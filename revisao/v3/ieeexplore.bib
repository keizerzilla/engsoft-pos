@INPROCEEDINGS{7087053, 
author={A. A. {Pawar} and N. N. {Patil}}, 
booktitle={2015 International Conference on Pervasive Computing (ICPC)}, 
title={Recognition of 3-D faces with missing parts and line scratch removal using new technique}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={This paper presents an implementation of face recognition, which is a very important task of human face identification. Line scratch detection in images is a highly challenging situation because of various characteristics of this defect. Few characteristics are considered with the different texture and geometry of images. We propose a useful algorithm for frame-by-frame line scratch detection in face image which deals with 3D approach and a filtering of detection. The temporary filtering algorithm can be used to remove false detection due to thin vertical structures by detecting the scratches on an image. Experimental evaluation can be detecting the lines and scratches on a face image and they used to solve this difficult approach. Our method is used with missing parts in an image. Three-dimensional face recognition is an extended method of facial recognition is considered according with the geometry and texture of a face. It has been elaborated that 3D face recognition methods can provide high accuracy as well as high detection with a comparison of 2D recognition. 3D avoids such mismatch effect of 2D face recognition algorithms. Additionally, most 3D scanners achieve both a 3D mesh and the texture of a face image. This allows combining the output of pure 3D matches with the more traditional algorithms of 2D face recognition, thus producing better performance.}, 
keywords={computational geometry;face recognition;filtering theory;image matching;image texture;mesh generation;object detection;3D face recognition;missing parts;line scratch removal;human face identification;frame-by-frame line scratch detection;image texture;image geometry;detection filtering;false detection removal;3D scanners;3D mesh;pure 3D matches;Face recognition;Three-dimensional displays;Transforms;Filtering;Image recognition;Noise;Films;3D Images;Adaptive detection;Face mask;Hough transforms;ICP algorithm;Line scratches;Missing parts;RANSAC;SIFT}, 
doi={10.1109/PERVASIVE.2015.7087053}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7292772, 
author={C. {Sindhuja} and K. {Mala}}, 
booktitle={2015 International Conference on Computing and Communications Technologies (ICCCT)}, 
title={Landmark identification in 3D image for facial expression recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={338-343}, 
abstract={Facial expression recognition plays a major role in non verbal communication. Recognition by machine is still a challenging problem. To automate the recognition for human machine interaction, a system is proposed in this paper. The proposed system uses shape descriptors to identify twelve land marks which mainly contribute to the facial expression recognition. From the location and the size or boundary of the land marks by matching with Facial Landmark Model (FLM), basic expressions are identified. The experimental results show that the shape descriptors and post processing correctly identifies landmarks automatically. The architectural distortion of action units is used to identify the basic facial expressions and tested on Bosphorous data set.}, 
keywords={emotion recognition;face recognition;landmark identification;3D image;facial expression recognition;nonverbal communication;human machine interaction;shape descriptors;facial landmark model;FLM;post processing;architectural distortion;facial expression identification;Bosphorous data set;Shape;Indexes;Face recognition;Nose;Mouth;Three-dimensional displays;Feature extraction;Landmark detection;Facial Landmark Model;Shape index}, 
doi={10.1109/ICCCT2.2015.7292772}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7558813, 
author={S. {Liu} and X. {Chen} and D. {Fan} and X. {Chen} and F. {Meng} and Q. {Huang}}, 
booktitle={2016 IEEE International Conference on Mechatronics and Automation}, 
title={3D smiling facial expression recognition based on SVM}, 
year={2016}, 
volume={}, 
number={}, 
pages={1661-1666}, 
abstract={Using Kinect acquired RGB-D image to obtain a face feature parameters and three-dimensional coordinates of the characteristic parameters, and to select the characteristic parameter Facial by Candide-3 model, and feature extraction and normalization. Smile face expression data collection through Kinect, SVM collected to smiley face data classify and output the result of recognition, and the results compared with two-dimensional image of smiling face expression recognition results. Experimental results show that three-dimensional image of smiling face expression recognition accuracy than the two-dimensional image of smiling face. This research has important significance for the research and application of facial expression recognition technology.}, 
keywords={emotion recognition;face recognition;feature extraction;image classification;interactive devices;support vector machines;3D smiling facial expression recognition;SVM;Kinect;RGB-D image;face feature parameters;three-dimensional coordinates;characteristic parameters;Candide-3 model;feature extraction;normalization;smile face expression data collection;smiley face data classification;smiling face expression recognition two-dimensional image;smiling face expression recognition three-dimensional image;facial expression recognition technology;Feature extraction;Face;Face recognition;Support vector machines;Image recognition;Training;Data mining;Facial Expression Recognition;Feature Extraction;Support Vector Machine;Kinect}, 
doi={10.1109/ICMA.2016.7558813}, 
ISSN={2152-744X}, 
month={Aug},}
@INPROCEEDINGS{7867242, 
author={ and and }, 
booktitle={2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)}, 
title={Research on the 3D face recognition based on multi-class classifier with depth and point cloud data}, 
year={2016}, 
volume={}, 
number={}, 
pages={398-402}, 
abstract={Human face recognition technology usually takes advantages of two-dimensional or three-dimensional data. Rising from 1980s, three-dimensional face recognition technology soon become one of the headed topic because of its admirable resistance to interference and more information compared with two-dimensional face recognition technology. The new 3D face model standardization algorithm presented in this article provides a solution to transfer the obtained face model to standardized CANDIDE-3 face model. The article also provides a new Bayesian classification model based on multi-class classifier, which could overcome the difficulty that ono-verse-one classifier has a low recognition rate when facing more than two people. The article conduct the comparison experiment based on the provided algorithm. According to the experiment, it could raise the face recognition rate efficiently when applying the standardization algorithm and training model.}, 
keywords={Bayes methods;face recognition;image classification;3D face recognition;multiclass classifier;human face recognition;three-dimensional face recognition;standardized CANDIDE-3 face model;Bayesian classification;ono-verse-one classifier;point cloud data;Face;Data models;Face recognition;Classification algorithms;Training;Three-dimensional displays;Standards;3D face recognition;depth data;point cloud data;CANDIDE-3;face model standardization;multi-class classifier}, 
doi={10.1109/IMCEC.2016.7867242}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8571963, 
author={S. {Yamada} and H. {Lu} and J. K. {Tan} and H. {Kim} and N. {Kimura} and T. {Okawachi} and E. {Nozoe} and N. {Nakamura}}, 
booktitle={2018 18th International Conference on Control, Automation and Systems (ICCAS)}, 
title={Extraction of Median Plane from Facial 3D Point Cloud Based on Symmetry Analysis Using ICP Algorithm}, 
year={2018}, 
volume={}, 
number={}, 
pages={1347-1350}, 
abstract={Cleft lip is a kind of congenital facial morphological abnormality. In the clinical field of cleft lip, it is necessary to analyze symmetric shape. However, there is no method to analyze the cleft lip technique based on symmetrical viewpoints. On the other hand, in our previous method to find a symmetric axis using a 2D image, since the middle line is extracted only from the front view of the face moire image. There was a problem that low accuracy was obtained by slight rotation of the face and it was not possible to consider 3D information. In this paper, we propose a method to extract the median plane of the face by analyzing based on bilateral symmetry by using 3D point cloud on the face of front. By extracting the median plane, we believe that not only surgical assistance of doctor be possible but also become a clue to development of simulation software which is the end goal.}, 
keywords={biomechanics;face recognition;feature extraction;image reconstruction;medical image processing;surgery;symmetric shape;clinical field;congenital facial morphological abnormality;ICP algorithm;symmetry analysis;facial 3D point cloud;bilateral symmetry;median plane;problem that low accuracy;face moire image;middle line;symmetric axis;symmetrical viewpoints;cleft lip technique;Three-dimensional displays;Lips;Face;Surgery;Iterative closest point algorithm;Two dimensional displays;Nose;Cleft lip;ICP algorithm;3D point cloud;Point Cloud Library;Facial median plane;Symmetry analysis.}, 
doi={}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7443802, 
author={S. {Arora} and S. {Chawla}}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={An intensified approach to face recognition through average half face}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Face recognition has broad excitement in the latest trend in image processing. Face recognition refers to identify a specific individual in digital image by analyzing and comparing patterns. It has numerous benefits which attract every sector but there are some issues such as more time consumption and lesser accuracy which degrade the user services. To solve this problem we proposed a highly accurate and fast method to reduce the execution time. The proposed method uses average half face approach because overall system's accuracy is better in it rather than using the original full face image. The proposed method can be used to recognize both 2D and 3D images. It mainly includes the average half face creation, feature detection, full face recognition through average half face using distance metrics and finally checking system's accuracy along with time consumption. The proposed method is based on eye, nose and mouth detection.}, 
keywords={face recognition;feature extraction;face recognition;average half face approach;digital image processing;half face creation;feature detection;distance metrics;eye detection;nose detection;mouth detection;Face;Face recognition;Nose;Databases;Mouth;Three-dimensional displays;Image processing;Face recognition;Image processing;Accuracy;Average half face;Distance metrics}, 
doi={10.1109/INDICON.2015.7443802}, 
ISSN={2325-9418}, 
month={Dec},}
@INPROCEEDINGS{8054955, 
author={M. {Jazouli} and A. {Majda} and A. {Zarghili}}, 
booktitle={2017 Intelligent Systems and Computer Vision (ISCV)}, 
title={A $P recognizer for automatic facial emotion recognition using Kinect sensor}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Autism is a developmental disorder involving qualitative impairments in social interaction. One source of those impairments are difficulties with facial expressions of emotion. Autistic people often have difficulty to recognize or to understand other people's emotions and feelings, or expressing their own. This work proposes a method to automatically recognize seven basic emotions among autistic children in real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and Neutral. The method uses the Microsoft Kinect sensor to track and identify points of interest from the 3D face model and it is based on the $P point-cloud recognizer to identify multi-stroke emotions as point-clouds. The experimental results show that our system can achieve above 94.28% recognition rate. Our study provides a novel clinical tool to help children with autism to assisting doctors in operating rooms.}, 
keywords={emotion recognition;face recognition;feature extraction;multistroke emotions;point-clouds;recognition rate;autism;$P recognizer;automatic facial emotion recognition;developmental disorder;qualitative impairments;social interaction;autistic people;autistic children;Microsoft Kinect sensor;3D face model;$P point-cloud recognizer;Face recognition;Emotion recognition;Face;Three-dimensional displays;Autism;Support vector machines;Algorithm design and analysis;ASD;Autism;emotion;face expression;Kinect;$P Recognizer}, 
doi={10.1109/ISACV.2017.8054955}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8269662, 
author={R. {Kaur} and D. {Sharma} and A. {Verma}}, 
booktitle={2017 4th International Conference on Signal Processing, Computing and Control (ISPCC)}, 
title={An advance 2D face recognition by feature extraction (ICA) and optimize multilayer architecture}, 
year={2017}, 
volume={}, 
number={}, 
pages={122-129}, 
abstract={Facial recognition has most significant real-life requests like investigation and access control. It is associated through the issue of appropriately verifying face pictures and transmit them person in a database. In a past years face study has been emerging active topic. Most of the face detector techniques could be classified into feature based methods and image based also. Feature based techniques adds low-level analysis, feature analysis, etc. Facial recognition is a system capable of verifying / identifying a human after 3D images. By evaluating selected facial unique features from the image and face dataset. Design from transformation method given vector dimensional illustration of individual face in a prepared set of images, Principle component analysis inclines to search a dimensional sub-space whose normal vector features correspond to the maximum variance direction in the real image space. The PCA algorithm evaluates the feature extraction, data, i.e. Eigen Values and vectors of the scatter matrix. In literature survey, Face recognition is a design recognition mission performed exactly on faces. It can be described as categorizing a facial either “known” or “unknown”, after comparing it with deposits known individuals. It is also necessary to need a system that has the capability of knowledge to recognize indefinite faces. Computational representations of facial recognition must statement various difficult issues. After existing work, we study the SIFT structures for the gratitude method. The novel technique is compared with well settled facial recognition methods, name component analysis and eigenvalues and vector. This algorithm is called PCA and ICA (Independent Component Analysis). In research work, we implement the novel approach to detect the face in minimum time and evaluate the better accuracy based on Back Propagation Neural Networks. We design the framework in face recognition using MATLAB 2013a simulation tool. Evaluate the performance parameters, i.e. the FAR (false acceptance rate), FRR (False rejection Rate) and Accuracy and compare the existing performance parameters i.e. accuracy.}, 
keywords={backpropagation;eigenvalues and eigenfunctions;face recognition;feature extraction;independent component analysis;principal component analysis;face recognition;feature extraction;appropriately verifying face pictures;face detector techniques;feature based techniques;low-level analysis;face dataset;individual face;image space;indefinite faces;settled facial recognition methods;independent component analysis;principle component analysis;vector dimensional illustration;normal vector features;backpropagation neural networks;Face recognition;Face;Feature extraction;Algorithm design and analysis;Signal processing algorithms;Videos;Databases;Face recognition;Features of face;Eigen values and Vectors;Neural Network}, 
doi={10.1109/ISPCC.2017.8269662}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8272703, 
author={H. {Li} and J. {Sun} and L. {Chen}}, 
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)}, 
title={Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition}, 
year={2017}, 
volume={}, 
number={}, 
pages={234-242}, 
abstract={This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01%, 97.60%, and 96.13% on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition.}, 
keywords={face recognition;feature extraction;geometry;image classification;image representation;pose estimation;location-sensitive sparse representation;expression-robust 3D face recognition approach;given raw 3D facial surfaces;3D face pre-processing pipeline;face region cropping;normalized 3D facial surface;facial surface normal components;deep face net;facial surface normals;different facial locations;location sensitive sparse representation classifier;different 3D faces;different normal components;BU-3DFE databases;massive 2D face images;Three-dimensional displays;Face;Face recognition;Solid modeling;Two dimensional displays;Shape;Deformable models}, 
doi={10.1109/BTAS.2017.8272703}, 
ISSN={2474-9699}, 
month={Oct},}
@INPROCEEDINGS{7443288, 
author={ and and S. {Tripathi}}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={Pose invariant method for emotion recognition from 3D images}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Information about the emotional state of a person can be inferred from facial expressions. Emotion recognition has become an active research area in recent years in various fields such as Human Robot Interaction (HRI), medicine, intelligent vehicle, etc., The challenges in emotion recognition from images with pose variations, motivates researchers to explore further. In this paper, we have proposed a method based on geometric features, considering images of 7 yaw angles (-45°,-30°,-15°,0°,+15°,+30°,+45°) from BU3DFE database. Most of the work that has been reported considered only positive yaw angles. In this work, we have included both positive and negative yaw angles. In the proposed method, feature extraction is carried out by concatenating distance and angle vectors between the feature points, and classification is performed using neural network. The results obtained for images with pose variations are encouraging and comparable with literature where work has been performed on pitch and yaw angles. Using our proposed method non-frontal views achieve similar accuracy when compared to frontal view thus making it pose invariant. The proposed method may be implemented for pitch and yaw angles in future.}, 
keywords={emotion recognition;feature extraction;image classification;neural nets;pose estimation;visual databases;pose invariant method;emotion recognition;3D image;person emotional state;facial expressions;pose variation;geometric features;BU3DFE database;positive yaw angles;negative yaw angles;feature extraction;concatenating distance;angle vectors;feature points;classification;neural network;nonfrontal views;Emotion recognition;Databases;Three-dimensional displays;Feature extraction;Eyebrows;Euclidean distance;Mouth;BU3DFE database;feature points;feature extraction;classification;neural network}, 
doi={10.1109/INDICON.2015.7443288}, 
ISSN={2325-9418}, 
month={Dec},}
@INPROCEEDINGS{8075548, 
author={T. {Frikha} and F. {Chaabane} and B. {Said} and H. {Drira} and M. {Abid} and C. {Ben Amar} and L. {Lille}}, 
booktitle={2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
title={Embedded approach for a Riemannian-based framework of analyzing 3D faces}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform.}, 
keywords={biometrics (access control);embedded systems;face recognition;feature extraction;field programmable gate arrays;hardware-software codesign;software architecture;stereo image processing;tensors;biometric facial recognition system;embedded systems;data completion;full face recognition;hardware architecture;adaptation approach;face detection;Riemannian-based framework;3D face analysis;multimedia embedded applications;face expressions recognition;mixed hardware software architecture;codesign technique;FPGA platform;Computer architecture;Shape;Face recognition;Multimedia communication;Three-dimensional displays;Embedded systems;Measurement;Facial analysis;face detection;Facial expressions;3D face recognition;embedded architecture;elastic analysis algorithm;Riemann geometry;Curve analysis}, 
doi={10.1109/ATSIP.2017.8075548}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7899838, 
author={ and and }, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Face verification with three-dimensional point cloud by using deep belief networks}, 
year={2016}, 
volume={}, 
number={}, 
pages={1430-1435}, 
abstract={Developing reliable and robust face verification systems has been a tough challenge in computer vision, for several decades. The variation in illumination and head pose may seriously inhibit the accuracy of two-dimensional face recognition. With the invention of a depth map sensor, more three-dimensional volume data can be processed to mitigate the problem associated with face verification. This paper presents a three-dimensional face verification approach that includes three phases. First, point cloud library is applied to estimate features such as normal vectors and principal curvatures of every point on a human face point cloud acquired from three-dimensional depth sensor. Next, we adopt deep belief networks to train the identification model using extracted features. Finally, face verification is accomplished by using the pre-trained deep belief networks to justify if new incoming face point cloud feature is the one we specified. The experimental results demonstrate that the proposed system performs exceptionally well with about 96.43% verification accuracy.}, 
keywords={belief networks;computer vision;face recognition;feature extraction;robust face verification systems;three-dimensional point cloud;deep belief networks;computer vision;two-dimensional face recognition;point cloud library;three-dimensional depth sensor;feature extraction;Three-dimensional displays;Face;Feature extraction;Face recognition;Estimation;Training;Neurons;face verification;3D point cloud;feature extraction;principal curvature estimation;deep belief networks}, 
doi={10.1109/ICPR.2016.7899838}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7910452, 
author={S. G. {Gunanto} and M. {Hariadi} and E. M. {Yuniarno}}, 
booktitle={2016 2nd International Conference of Industrial, Mechanical, Electrical, and Chemical Engineering (ICIMECE)}, 
title={Computer facial animation with synthesize marker on 3D faces surface}, 
year={2016}, 
volume={}, 
number={}, 
pages={260-263}, 
abstract={An animated character has its own characteristics and behaviour. The animator needs to be skilled enough to make a complex animation, especially on making face expression. a Well defined facial expression can represent the emotional condition and make the animation more expressing the mood. But most facial animation is done by manually, frame-by-frame. It is very time-consuming. This research proposed a combination methods for handling the facial animation based on marker location and face surface from the 3D character. The motion data captured based on the location and movement of the marker then implemented on 3D face model to generate the motion guidance. As a guidance, this marker data role as a centroid of a vertex cluster. The cluster provided by implementing segmentation fp-NN Clustering method based on surface and can visualize the deformation using linear blend skinning methods. The result from this research shows that this system can automatically generate facial animations based on the marker data and the surface segmentation. The visualization of deformation arranges accordingly to the motion captured data and organized sequentially.}, 
keywords={computer animation;data visualisation;face recognition;image segmentation;neural nets;pattern clustering;computer facial animation;synthesize marker;3D face surface;facial expression;3D character;motion data capture;motion guidance;vertex cluster;segmentation fp-NN Clustering method;linear blend skinning methods;Facial animation;Three-dimensional displays;Solid modeling;Motion segmentation;Surface treatment;Interpolation;facial animation;feature marker;surface}, 
doi={10.1109/ICIMECE.2016.7910452}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8226186, 
author={K. S. {Warke} and R. {Suralkar} and S. {Pawar} and P. {Sonawane} and S. {Wani}}, 
booktitle={2017 2nd International Conference for Convergence in Technology (I2CT)}, 
title={A real sense based multilevel security in cloud framework using face recognition and image processing}, 
year={2017}, 
volume={}, 
number={}, 
pages={531-533}, 
abstract={A 3D image can be used for the authentications as it gives more accuracy. We can also use the gestures for the authentication purpose. We can add multiple authentication levels together to make system more secure and login process more reliable. In our system we are going to provide three levels of authentication i.e. 1) Text Password:- This is first level in which user has to enter the text password which is OTP. OTP will be sent to the registered email-id or at the mobile number given. With The help of AES (advance encryption standard) algorithm the data will be encrypt and store at database server. 2) Hand Gesture Recognition:-This is second level in which user has to place this hand in front of camera. So camera can detect it, select one point out of 22 points on the hand whatever pattern user make in front of camera is saved as password at user's database. 3) Face Recognition:-This is last level in which user has to place face in front of camera. So camera can detect the face and select 78 landmark points. And those points are saved at the user's database. After passing all these stages user is authenticate and can upload or download the documents of his/her choice.}, 
keywords={authorisation;cloud computing;cryptography;face recognition;gesture recognition;image processing;gestures;authentication purpose;multiple authentication levels;text password;OTP;registered email;mobile number;AES;advance encryption standard;database server;pattern user;multilevel security;cloud framework;face recognition;Face recognition;Cloud computing;Cameras;Face;Authentication;Algorithm design and analysis;3D Gesture;Real Sense;Image Processing;Face Recognition;Cloud},
doi={10.1109/I2CT.2017.8226186}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7066811, 
author={G. {Betta} and D. {Capriglione} and M. {Corvino} and M. {Gasparetto} and E. {Zappa} and C. {Liguori} and A. {Paolillo}}, 
booktitle={2015 XVIII AISEM Annual Conference}, 
title={A proposal for improving the performance of face recognition systems based on 3d features}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={In this paper a suitable methodology for the improvement of the reliability of results in classification systems based on 3D images is proposed. More in detail, it is based on the knowledge of the uncertainty of the features constituting the 3D image (obtained processing a pair of two 2D stereoscopic images) and on a suitable statistical approach providing a confidence level to the classification result. These pieces of information are then managed in order to improve the classification performance in terms of correct classification and missed classification percentages. The experimental results, obtained applying the methodology on an Active Appearance Models algorithm, a popular method for face recognition based on 3D features, show that, compared with a traditional approach (which generally does not take into account the uncertainty on 3D features), the proposed methodology allows to significantly improve the classification performance even in scenarios characterized by a high uncertainty.}, 
keywords={face recognition;image classification;stereo image processing;face recognition systems;3D features;3D images;classification systems;2D stereoscopic image;classification performance;active appearance models;correct classification percentages;missed classification percentages;Uncertainty;Three-dimensional displays;Face recognition;Measurement uncertainty;Active appearance model;Classification algorithms;Databases;face recognition;measurement uncertainty;image classification;decision support systems;3D features}, 
doi={10.1109/AISEM.2015.7066811}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7378673, 
author={ and and }, 
booktitle={2015 23rd International Conference on Geoinformatics}, 
title={A method of extracting human facial feature points based on 3D laser scanning point cloud data}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-3}, 
abstract={Three-dimensional body measurement is determined by measuring the size of each part of the body and body shape, in order to study the morphological characteristics of the human body, and provide basic information for human industrial design, ergonomics, engineering design, anthropological research, and medicine. This paper presents a three-dimensional point cloud data extraction method of facial feature points by doing nose points with examples, using this method can accurately locate the feature points, and there is no specific stations toward requirements in the process of scanning point cloud of the human body, it is convenient and quick. The results show that this method can meet the ergonomics and other features to quickly locate and measure the body size requirements.}, 
keywords={computer graphics;face recognition;feature extraction;optical scanners;human facial feature point extraction;3D laser scanning point cloud data;three-dimensional body measurement;morphological characteristics;human body;human industrial design;ergonomics;engineering design;anthropological research;medicine;three-dimensional point cloud data extraction method;nose point;Atmospheric modeling;Ergonomics;Biomedical imaging;The face feature point;Nose point;Measurement;Three-dimensional point cloud;Three-dimensional laser scanning}, 
doi={10.1109/GEOINFORMATICS.2015.7378673}, 
ISSN={2161-024X}, 
month={June},}
@INPROCEEDINGS{8272716, 
author={F. {Liu} and J. {Hu} and J. {Sun} and Y. {Wang} and Q. {Zhao}}, 
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)}, 
title={Multi-dim: A multi-dimensional face database towards the application of 3D technology in real-world scenarios}, 
year={2017}, 
volume={}, 
number={}, 
pages={342-351}, 
abstract={Three-dimensional (3D) faces are increasingly utilized in many face-related tasks. Despite the promising improvement achieved by 3D face technology, it is still hard to thoroughly evaluate the performance and effect of 3D face technology in real-world applications where variations frequently occur in pose, illumination, expression and many other factors. This is due to the lack of benchmark databases that contain both high precision full-view 3D faces and their 2D face images/videos under different conditions. In this paper, we present such a multi-dimensional face database (namely Multi-Dim) of high precision 3D face scans, high definition photos, 2D still face images with varying pose and expression, low quality 2D surveillance video clips, along with ground truth annotations for them. Based on this Multi-Dim face database, extensive evaluation experiments have been done with state-of-the-art baseline methods for constructing 3D morphable model, reconstructing 3D faces from single images, 3D-assisted pose normalization for face verification, and 3D-rendered multiview gallery for face identification. Our results show that 3D face technology does help in improving unconstrained 2D face recognition when the probe 2D face images are of reasonable quality, whereas it deteriorates rather than improves the face recognition accuracy when the probe 2D face images are of poor quality. We will make Multi-Dim freely available to the community for the purpose of advancing the 3D-based unconstrained 2D face recognition and related techniques towards real-world applications.}, 
keywords={emotion recognition;face recognition;image reconstruction;pose estimation;solid modelling;video surveillance;probe 2D face images;unconstrained 2D face recognition;real-world applications;multidimensional face database;three-dimensional faces;face-related tasks;3D face technology;high precision full-view 3D faces;high precision 3D face scans;low quality 2D surveillance video clips;MultiDim face database;3D morphable model;face verification;face identification;face recognition accuracy;2D face image-video;Face;Three-dimensional displays;Two dimensional displays;Databases;Surveillance;Face recognition;Cameras}, 
doi={10.1109/BTAS.2017.8272716}, 
ISSN={2474-9699}, 
month={Oct},}
@INPROCEEDINGS{7351408, 
author={T. {Batabyal} and A. {Vaccari} and S. T. {Acton}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={UGraSP: A unified framework for activity recognition and person identification using graph signal processing}, 
year={2015}, 
volume={}, 
number={}, 
pages={3270-3274}, 
abstract={With the growing availability and wide distribution of low-cost, high-performance 3D imaging sensors, the image analysis community has witnessed an increased demand for solutions to the challenges of activity recognition and person identification. We propose an integrated framework, based on graph signal processing, that simultaneously performs both tasks using a single set of features. The novelty of our approach is based on the fact that the set of features used for activity recognition accommodates person identification without additional computation. The analysis is based on the extracted structure-invariant graph (skeleton). The Laplacian of the skeleton is used both to identify the person and recognize the performed activity. While person identification is achieved directly from the analysis of the Laplacian, activity recognition is obtained after transformation, into the graph spectral domain, of the vectorized form of the skeletal joints 3D coordinates. Feature vectors for activity recognition are then derived, in this domain, from the covariance matrices evaluated over fixed-length sequential video segments. Both classification tasks are implemented using linear support vector machines (SVM). When applied to real activity datasets, our approach shows an improved performance over the existing state-of-the-art.}, 
keywords={covariance matrices;feature extraction;graph theory;image classification;object detection;object recognition;support vector machines;video signal processing;UGraSP;unified framework;image analysis community;activity recognition;person identification;integrated framework;graph signal processing;feature tasks;structure-invariant graph extraction;graph skeleton;skeleton Laplacian;graph spectral domain;vectorized form;skeletal joints 3D coordinates;feature vectors;covariance matrices;fixed-length sequential video segments;classification tasks;linear support vector machines;SVM;real activity datasets;performance improvement;Laplace equations;Skeleton;Three-dimensional displays;Motion segmentation;Image recognition;Sensors;Support vector machines;Laplacian;Adjacency Matrix;Graph Signal Processing;Graph Fourier Transform;activity Recognition;Person Identification;Point cloud datasets}, 
doi={10.1109/ICIP.2015.7351408}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{7224078, 
author={L. {Spreeuwers}}, 
journal={IET Biometrics}, 
title={Breaking the 99% barrier: optimisation of three-dimensional face recognition}, 
year={2015}, 
volume={4}, 
number={3}, 
pages={169-178}, 
abstract={This study presents optimisations to a three-dimensional (3D) face recognition method the authors published in 2011. The optimisations concern handling and estimation of motion from a single 3D image using the symmetry of the face, fine registration by selection of the maximum score for small variations of the registration parameters and efficient training using automatic outlier removal where only part of the classifier is retrained. The optimisations lead to a staggering performance improvement: the verification rate on Face Recognition Grand Challenge (FRGC) v2 data at false accept rate = 0.1% increases from 94.6 to 99.3% and the identification rate increases from 99 to 99.4%. Both are, to the authors' knowledge, the best scores ever published on the FRGC data. In addition, the registration time was reduced from about 2.5 to 0.2-0.6 s and the number of comparisons has increased from about 11 000 to more than 50 000 per second. For slightly decreased performance, even millions of comparisons can be realised. The fast registration means near real-time recognition with 2-5 images is possible. The optimisations are not specific for this method, but can be beneficial for other 3D face recognition methods as well.}, 
keywords={face recognition;image classification;image registration;motion estimation;optimisation;3D face recognition;real-time image recognition;false accept rate;verification rate;FRGC v2 data;performance improvement;classifier training;automatic outlier removal;image registration parameters;3D image;motion handling;motion estimation;three-dimensional face recognition optimisation}, 
doi={10.1049/iet-bmt.2014.0017}, 
ISSN={2047-4938}, 
month={},}
@INPROCEEDINGS{8546094, 
author={J. {Liang} and F. {Liu} and H. {Tu} and Q. {Zhao} and A. K. {Jain}}, 
booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
title={On Mugshot-based Arbitrary View Face Recognition}, 
year={2018}, 
volume={}, 
number={}, 
pages={3126-3131}, 
abstract={Despite the wide usage of mugshot images in forensic applications, they are underutilized in existing automated face recognition systems. In this paper, we propose a novel mugshot-based arbitrary view face recognition method. Our approach reconstructs full 3D faces via cascaded regression in shape space with efficient seamless texture recovery. Unlike existing methods, it makes full use of the frontal and profile views available in mugshot images, and thus generates accurate and realistic 3D faces. Multi-view face images are synthesized from the reconstructed 3D faces to enlarge the gallery so that arbitrary view faces can be better recognized. Evaluation experiments were conducted on BFM and Multi-PIE databases by using state-of-the-art deep learning (DL) based face matchers. The results demonstrate the effectiveness of our proposed method and show that DL-based face matchers can benefit from mugshot images and the reconstructed 3D faces, especially for recognizing large off-angle faces.}, 
keywords={criminal law;face recognition;image matching;image reconstruction;image texture;learning (artificial intelligence);stereo image processing;forensic applications;multiPIE database;BFM database;deep learning based face matchers;mugshot-based arbitrary view face recognition method;seamless texture recovery;multiview face images;automated face recognition systems;off-angle faces;reconstructed 3D faces;mugshot images;DL-based face matchers;Face;Three-dimensional displays;Shape;Face recognition;Image reconstruction;Probes;Two dimensional displays}, 
doi={10.1109/ICPR.2018.8546094}, 
ISSN={1051-4651}, 
month={Aug},}
@INPROCEEDINGS{8389776, 
author={G. {Geetha} and M. {Safa} and C. {Fancy} and K. {Chittal}}, 
booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)}, 
title={3D face recognition using Hadoop}, 
year={2017}, 
volume={}, 
number={}, 
pages={1882-1885}, 
abstract={Face Recognition is one of the biometric technique to vestige the given faces. We present a 3D face recognition method using Hadoop to recognize 3D faces under varying expressions, lighting and different poses to overcome the challenges of the 2D face recognition. In this paper, a threshold facial region of an image is detected and preprocessing is done based through the image excellence. If the selected face is frontal face with good lighting, extract the prerequisite features and do the necessary comparison steps to recognize the faces. In case, if the selected face is in bad lighting, then perform histogram equalization and normalization to increase the contrast. Different Poses and expressions are the very challenging zones which require surplus pre-processing to improve the performance of the face recognition system. Hence an enhanced normalization method called 3D Morphable Model are used as a pre- processing technique to create a frontal view from a non-frontal view and also merge images with different views in to a single frontal view. Next to diminish the number of features used for recognition process; we emulate the linear discriminant analysis method for further classification. Eventually, we used an open-source Hadoop Image Processing Interface (HIPI) to act as an interface for MapReduce technology for recognition.}, 
keywords={biometrics (access control);face recognition;feature extraction;parallel processing;frontal face;biometric technique;3D face recognition method;histogram equalization;surplus preprocessing;enhanced normalization method;3D morphable model;single frontal view;nonfrontal view;linear discriminant analysis method;open-source Hadoop image processing interface;HIPI;MapReduce technology;Face;Face recognition;Three-dimensional displays;Lighting;Feature extraction;Solid modeling;Hadoop;Image Processing;Map Reduce;Linear Discriminant analysis}, 
doi={10.1109/ICECDS.2017.8389776}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7163090, 
author={X. {Yang} and D. {Huang} and Y. {Wang} and L. {Chen}}, 
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
title={Automatic 3D facial expression recognition using geometric scattering representation}, 
year={2015}, 
volume={1}, 
number={}, 
pages={1-6}, 
abstract={Facial Expression Recognition (FER) is one of the most active topics in the domain of computer vision and pattern recognition, and it has received increasing attention for its wide application potentials as well as attractive scientific challenges. In this paper, we present a novel method to automatic 3D FER based on geometric scattering representation. A set of maps of shape features in terms of multiple order differential quantities, i.e. the Normal Maps (NOM) and the Shape Index Maps (SIM), are first jointly adopted to comprehensively describe geometry attributes of the facial surface. The scattering operator is then introduced to further highlight expression related cues on these maps, thereby constructing geometric scattering representations of 3D faces for classification. The scattering descriptor not only encodes distinct local shape changes of various expressions as by several milestone descriptors, such as SIFT, HOG, etc., but also captures subtle information hidden in high frequencies, which is quite crucial to better distinguish expressions that are easily confused. We evaluate the proposed approach on the BU-3DFE database, and the performance is up to 84.8% and 82.7% with two commonly used protocols respectively which is superior to the state of the art ones.}, 
keywords={emotion recognition;face recognition;image classification;image representation;shape recognition;automatic 3D facial expression recognition;automatic 3D FER;BU-3DFE database;local shape changes;3D face classification;scattering operator;facial surface geometry attributes;SIM;shape index maps;NOM;normal maps;multiple order differential quantities;shape feature maps;geometric scattering representation;Three-dimensional displays;Shape;Scattering;Indexes;Support vector machines;Solid modeling;Feature extraction}, 
doi={10.1109/FG.2015.7163090}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8578635, 
author={S. {Cheng} and I. {Kotsia} and M. {Pantic} and S. {Zafeiriou}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={5117-5126}, 
abstract={The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.}, 
keywords={biometrics (access control);computer vision;emotion recognition;face recognition;image capture;image resolution;stereo image processing;visual databases;high-resolution 3D faces;4DFAB;facial behaviour;facial expression recognition;behavioural biometrics;computer vision applications;automatic face analysis;scale visual databases;face recognition;Databases;Three-dimensional displays;Face;Face recognition;Cameras;Two dimensional displays;Task analysis}, 
doi={10.1109/CVPR.2018.00537}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{7823978, 
author={S. {Naveen} and R. K. {Ahalya} and R. S. {Moni}}, 
booktitle={2016 International Conference on Communication Systems and Networks (ComNet)}, 
title={Multimodal face recognition using spectral transformation by LBP and polynomial coefficients}, 
year={2016}, 
volume={}, 
number={}, 
pages={13-17}, 
abstract={This paper presents a multimodal face recognition using spectral transformation by Local Binary Pattern (LBP) and Polynomial Coefficients. Here 2D image and 3D image are combined to get multimodal face recognition. In this method a novel feature extraction is done using LBP and Polynomial Coefficients. Then these features are spectrally transformed using Discrete Fourier Transform (DFT). These spectrally transformed features extracted from texture image using the two methods are combined at the score level. Similarly this is done in depth image. Finally feature information from texture and depth are combined at the score level which gives better results than the individual results.}, 
keywords={discrete Fourier transforms;face recognition;feature extraction;image texture;multimodal face recognition;spectral transformation;LBP;polynomial coefficients;local binary pattern;2D image;3D image;feature extraction;discrete Fourier transform;DFT;texture image;depth image;feature information;Feature extraction;Face recognition;Discrete Fourier transforms;Discrete cosine transforms;Databases;Face;Two dimensional displays;Texture;Depth;Local Binary Pattern (LBP);Polynomial Coefficients;Multimodal;Discrete Fourier Transform (DFT)}, 
doi={10.1109/CSN.2016.7823978}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8346387, 
author={H. M. R. {Afzal} and S. {Luo} and M. K. {Afzal}}, 
booktitle={2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)}, 
title={Reconstruction of 3D facial image using a single 2D image}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In many scenarios related to image processing, 3D face reconstruction is considered an essential part. A robust and fast method of 3D face reconstruction from a single 2D image is presented. This method consists of three main steps including extraction of features from single image, calculating the depth of image and adjustment of a 3D model on the direction of Z-axis. In the first step, the features of image are extracted by using supervised descent method (SDM). Using SDM, face regions like facial components (eyes, nose lips) and face contours are detected. Second step consists of depth prediction by implementation of multivariate Gaussian distribution. Finally, 3D face is constructed with the help of features and the depth information and 3D database. The proposed method has been verified by conducting several experiments depicted in evaluation section. Our method is robust in nature and gives good results even using a single image, comparing to other methods that use multiple images for reconstruction of 3D images.}, 
keywords={face recognition;feature extraction;Gaussian distribution;gradient methods;image reconstruction;stereo image processing;3D face reconstruction;supervised descent method;SDM;face regions;3D model;facial components;face contours;depth prediction;multivariate Gaussian distribution;image processing;single 2D image;3D facial image;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Shape;Two dimensional displays;Solid modeling;3D face reconstruction;features extraction;Gaussain distribution;facial modeling}, 
doi={10.1109/ICOMET.2018.8346387}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7853992, 
author={R. {Amin} and A. F. {Shams} and S. M. M. {Rahman} and D. {Hatzinakos}}, 
booktitle={2016 9th International Conference on Electrical and Computer Engineering (ICECE)}, 
title={Evaluation of discrimination power of facial parts from 3D point cloud data}, 
year={2016}, 
volume={}, 
number={}, 
pages={602-605}, 
abstract={Feature selection from facial regions is a well-known approach to increase the performance of 2D image-based face recognition systems. In case of 3D modality, the approach of region-based feature selection for face recognition is relatively new. In this context, this paper presents an approach to evaluate the discrimination power of different regions of a 3D facial surface for its potential use in face recognition systems. We propose the use of weighted average of unit normal vector on the facial surface as the feature for region-based face recognition from 3D point cloud data (PCD). The iterative closest point algorithm is employed for the registration of segmented regions of facial point clouds. A metric based on angular distance between normals is introduced to indicate the similarity between two surfaces of same facial region. Finally, the intra class correlation based discrimination score is formulated to find out the key facial regions such as the eyes, nose, and mouth that are significant while recognizing a person with facial surface PCD.}, 
keywords={computational geometry;correlation methods;face recognition;feature selection;image registration;image segmentation;discrimination power evaluation;facial parts;2D image-based face recognition systems;3D modality;region-based feature selection;3D facial surface;3D point cloud data;3D PCD;iterative closest point algorithm;segmented region registration;angular distance;intra class correlation;discrimination score;Three-dimensional displays;Face;Face recognition;Measurement;Two dimensional displays;Nose;Databases}, 
doi={10.1109/ICECE.2016.7853992}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8089906, 
author={L. {Fangmin} and C. {Ke} and L. {Xinhua}}, 
booktitle={2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA)}, 
title={3D Face Reconstruction Based on Convolutional Neural Network}, 
year={2017}, 
volume={}, 
number={}, 
pages={71-74}, 
abstract={Fast and robust 3D reconstruction of facial geometric structure from a single image is a challenging task with numerous applications, but there exist two problems when applied "in the wild": the 3D estimates are unstable for different photos of the same subject; the 3D estimates are over-regularized and generic. In response, a robust method for regressing discriminative 3D morphable face models(3DMM) is described to support face recognition and 3D mask printing. Combining the local data sets with the public data sets, improving the exiting 3DMM fitting method and then using a convolutional neural network(CNN) to improve reconstruction effect. The ground truth 3D faces of the CNN are the pooled 3DMM parameters extracted from the photos of the same subject. Using CNN to regress 3DMM shape and texture parameters directly from an input photo and offering a method for generating huge numbers of labeled examples. There are two key points of the paper: one is the training data generation for the model training; the other is the training of 3D reconstruction model. Experimental results and analysis show that this method costs much less time than traditional methods of 3D face modeling, and it is improved for different races on photos with any angles than the existing methods based on deep learning, and the system has better robustness.}, 
keywords={face recognition;image morphing;image reconstruction;image texture;learning (artificial intelligence);neural nets;regression analysis;shape recognition;3D face reconstruction;facial geometric structure;robust method;face recognition;3D mask printing;local data sets;public data sets;reconstruction effect;texture parameters;training data generation;3D reconstruction model;3D face modeling;convolutional neural network;discriminative 3D morphable face models;3DMM fitting method;CNN;Face;Three-dimensional displays;Solid modeling;Image reconstruction;Shape;Robustness;Data models;3D face reconstruction;convolutional neural network(CNN);3DMM;shape;texture}, 
doi={10.1109/ICICTA.2017.23}, 
ISSN={}, 
month={Oct},}
@ARTICLE{8353876, 
author={X. {Song} and Z. {Feng} and G. {Hu} and J. {Kittler} and X. {Wu}}, 
journal={IEEE Transactions on Information Forensics and Security}, 
title={Dictionary Integration Using 3D Morphable Face Models for Pose-Invariant Collaborative-Representation-Based Classification}, 
year={2018}, 
volume={13}, 
number={11}, 
pages={2734-2745}, 
abstract={The paper presents a dictionary integration algorithm using 3D morphable face models (3DMM) for pose-invariant collaborative-representation-based face classification. To this end, we first fit a 3DMM to the 2D face images of a dictionary to reconstruct the 3D shape and texture of each image. The 3D faces are used to render a number of virtual 2D face images with arbitrary pose variations to augment the training data, by merging the original and rendered virtual samples to create an extended dictionary. Second, to reduce the information redundancy of the extended dictionary and improve the sparsity of reconstruction coefficient vectors using collaborative-representation-based classification (CRC), we exploit an on-line class elimination scheme to optimise the extended dictionary by identifying the training samples of the most representative classes for a given query. The final goal is to perform pose-invariant face classification using the proposed dictionary integration method and the on-line pruning strategy under the CRC framework. Experimental results obtained for a set of well-known face data sets demonstrate the merits of the proposed method, especially its robustness to pose variations.}, 
keywords={face recognition;image classification;image representation;image texture;pose estimation;dictionary integration algorithm;3DMM;texture;virtual 2D face images;original rendered virtual samples;dictionary integration method;3D morphable face models;pose-invariant collaborative-representation-based face classification;online class elimination scheme;online pruning strategy;Face;Dictionaries;Training;Three-dimensional displays;Image reconstruction;Two dimensional displays;Solid modeling;Collaborative-representation-based classification;3D morphable face model;dictionary integration;face classification;virtual training samples}, 
doi={10.1109/TIFS.2018.2833052}, 
ISSN={1556-6013}, 
month={Nov},}
@INPROCEEDINGS{8687254, 
author={T. {Terada} and Y. {Chen} and R. {Kimura}}, 
booktitle={2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
title={3D Facial Landmark Detection Using Deep Convolutional Neural Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={390-393}, 
abstract={Various applications have been developed in facial research to aid the recognition of personal attributes, analysis of race, and personal authentication for the security industry and other research fields. Thus, it is possible to identify the differences in facial shape based on place of birth or country. The present study analyzes facial shape using scanned 3D facial images and investigates ways to extract facial landmarks from the 3D facial images. The detection of the facial landmark requires the normalization of the facial scale and position among in the 3D image data to analyze the facial shape. Therefore, it is difficult to obtain accurate facial landmarks from 3D facial images. Our method decomposes the task into the following three parts: (a) conversion of data from the 3D facial image to a 2D image, (b) extraction of facial landmarks from the 3D image using Convolutional Neural Network (CNN), (c) inversion of the identified facial landmarks from 2D to 3D images. In experiments, we compared the accuracy of this model for facial landmark detection.}, 
keywords={convolutional neural nets;face recognition;facial landmark detection;3D facial image;personal authentication;convolutional neural network;security industry;CNN;component;landmarks detection;3d facial image;point cloud;facial analysis;cnn}, 
doi={10.1109/FSKD.2018.8687254}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7442295, 
author={T. {Yamasaki} and I. {Nakamura} and K. {Aizawa}}, 
booktitle={2015 IEEE International Symposium on Multimedia (ISM)}, 
title={Fast Face Model Reconstruction and Synthesis Using an RGB-D Camera and Its Subjective Evaluation}, 
year={2015}, 
volume={}, 
number={}, 
pages={53-56}, 
abstract={It is difficult to show a frontal face in video chatting because there is a gap between a display and a camera. We propose a method for real-time face reorientation by creating a 2.5-D face model from a single RGB-D camera and synthesizing the rotated face model with the original face image. Our method uses two kinds face models complementarily: a point cloud based model and a generic face model fitted to the user. We conducted subjective evaluation and confirmed the validity of our proposed system.}, 
keywords={cameras;face recognition;image reconstruction;screens (display);video signal processing;fast face model reconstruction;fast face model synthesis;RGB-D camera;frontal face;video chatting;display;real-time face reorientation;2.5D face model;rotated face model synthesis;face image;point cloud-based model;generic face model;Decision support systems;Face;Active appearance model;Rendering (computer graphics);Multimedia communication;Cameras;Indexes;RGB-D camera;DIBR;real-time;gaze correction;face reorientation;virtual view synthesis}, 
doi={10.1109/ISM.2015.107}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7976644, 
author={S. {Sghaier} and C. {Souani} and H. {Faeidh} and K. {Besbes}}, 
booktitle={2016 Global Summit on Computer Information Technology (GSCIT)}, 
title={Novel Technique for 3D Face Segmentation and Landmarking}, 
year={2016}, 
volume={}, 
number={}, 
pages={27-31}, 
abstract={In this paper we propose a new technique to detect and extract any part that we need from a 3D face. The regions of interest of our approach are the eyes and the nose. Moreover, we are interested in the extraction of the salient landmarks in 3D face. Therefore, in this paper a new method is proposed to detect the nose tip and eye corners from a three-dimensional face range image. Our original technique allows fully automated processing, treating incomplete and noisy input data, and automatically rejecting non-facial areas. Besides, it is robust against holes in 3D image and insensitive to facial expressions. Besides, it is stable against translation and rotation of the face, and it is suitable for different resolutions of images. All the experiments have been performed on the GAVAB and FRAV 3D databases. After applying the proposed method to the 3D face, experimental results show that it is comparable to state-of-the-art methods in terms of its accuracy and flexibility.}, 
keywords={face recognition;image segmentation;visual databases;3D face segmentation;landmark extraction;nose tip detection;eye corner detection;three-dimensional face range image;automatic nonfacial area rejection;3D image;facial expressions;image resolutions;GAVAB 3D database;FRAV 3D database;Face;Three-dimensional displays;Nose;Information technology;Noise measurement;Robustness;Image resolution;3D face;segmentation;region of interest;anthropometric;landmarks}, 
doi={10.1109/GSCIT.2016.17}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7523133, 
author={G. {Torkhani} and A. {Ladgham} and M. N. {Mansouri} and A. {Sakly}}, 
booktitle={2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
title={Gabor-SVM applied to 3D-2D deformed mesh model}, 
year={2016}, 
volume={}, 
number={}, 
pages={447-452}, 
abstract={We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate.}, 
keywords={face recognition;feature extraction;Gabor filters;mesh generation;support vector machines;visual databases;wavelet transforms;3D-2D deformed mesh model;Gabor-SVM;3D face recognition;2D modeling;3D modeling;facial curvatures detection;3D image transformation;3D triangular mesh;2D space;2D smoother mesh;Gabor wavelets;surface curves;salient face feature detection;Gabor facial model;support vector machines;3D AJMAL face database;Three-dimensional displays;Solid modeling;Face recognition;Face;Deformable models;Feature extraction;Databases;3D face recognition;salient points;deformed mesh model;facial curvatures;Gabor wavelet;SVM}, 
doi={10.1109/ATSIP.2016.7523133}, 
ISSN={}, 
month={March},}
@ARTICLE{7361976, 
author={M. {Piccirilli} and G. {Doretto} and A. {Ross} and D. {Adjeroh}}, 
journal={IEEE Sensors Journal}, 
title={A Mobile Structured Light System for 3D Face Acquisition}, 
year={2016}, 
volume={16}, 
number={7}, 
pages={1854-1855}, 
abstract={A mobile sensor based on fringe projection techniques is developed with the goal of acquiring face 3D and color with a smartphone device. The system consists of a portable pico-projector and an Android-based smartphone. The data acquisition, pattern generation. and reconstruction of the final 3D point cloud are all driven by the smartphone. We present results on the root-mean-square error (RMSE) of the sensor and on 3D face matching.}, 
keywords={biometrics (access control);data acquisition;face recognition;image matching;image reconstruction;smart phones;mobile sensor;fringe projection;smartphone device;portable pico-projector;Android-based smartphone;data acquisition;pattern generation;3D point cloud reconstruction;root-mean-square error;3D face matching;mobile structured light system;3D biometrics;3D face acquisition;Three-dimensional displays;Face;Sensors;Cameras;Mobile communication;Image reconstruction;Lighting;Depth sensor;structured light;mobile device;Depth sensor;structured light;mobile device;3D face acquisition;3D face matching;3D biometrics}, 
doi={10.1109/JSEN.2015.2511064}, 
ISSN={1530-437X}, 
month={April},}
@INPROCEEDINGS{8314888, 
author={G. {Torkhani} and A. {Ladgham} and A. {Sakly}}, 
booktitle={2017 18th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering (STA)}, 
title={3D Gabor-Edge filters applied to face depth images}, 
year={2017}, 
volume={}, 
number={}, 
pages={578-582}, 
abstract={This manuscript introduces a novel 3D face authentication system inspired from the advantageous capacities of Gabor-Edge filters. The approach studies 3D face difficulties such as expression variety, different rotations and exposure to illuminations. The proposed systems starts by preprocessing the 3D face images to resolve acquisition problems. Then, a filtering process is performed by implanting our 3D Gabor-Edge technique extended based on the classic 3D Gabor masks. The next step is to achieve the classification of facial features from the edge saliency by the artificial Neural Network Classifier (NNC). The evaluation of the adopted system is achieved by exporting common datasets from GavabDB database. Experimental results are reported to prove the high accuracy rates of our method compared to the recent researches in the same biometric field.}, 
keywords={biometrics (access control);edge detection;face recognition;feature extraction;Gabor filters;image classification;neural nets;3D face difficulties;3D face images;3D Gabor-Edge technique;classic 3D Gabor masks;edge saliency;Gabor-edge filters;3D face authentication system;acquisition problems;facial feature classification;artificial neural network classifier;GavabDB database;biometric field;Three-dimensional displays;Face;Feature extraction;Authentication;Gabor filters;Face recognition;face authentication;Gabor filtering;3D images;saliency}, 
doi={10.1109/STA.2017.8314888}, 
ISSN={2573-539X}, 
month={Dec},}
@INPROCEEDINGS{7899769, 
author={G. {Tian} and T. {Mori} and Y. {Okuda}}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Spoofing detection for embedded face recognition system using a low cost stereo camera}, 
year={2016}, 
volume={}, 
number={}, 
pages={1017-1022}, 
abstract={Spoofing detection is essential for practical face recognition system. Based on the fact that genuine face has special geometric curvatures across surface, this paper brings forward an ultra-fast yet accurate spoofing detection approach using a low-cost stereo camera. To obtain curvatures, the three dimensional shapes of selected facial landmarks are analyzed, by fitting point cloud around each landmark to a specific partial face surface. Spoofing detection is then performed by evaluating curvatures of each landmark and integrating them together. Experiments verify that the approach is able to detect spoofed faces in printed photographs without or with various bending at FAR equal to 0.00%. Meanwhile, genuine faces have a trivial opportunity to be falsely rejected: FRR is 0.59% for near frontal faces and less than 5% for faces with large varying poses. Detection time is 51 milliseconds when executed on a single processor [1] running at a clock frequency of 266M Hz, this makes the detection very suitable for embedded face recognition system.}, 
keywords={face recognition;stereo image processing;spoofing detection;embedded face recognition system;low cost stereo camera;facial landmark 3D shapes;frequency 266 MHz;Face;Three-dimensional displays;Nose;Face recognition;Cameras;Surface fitting;Fitting;spoof detection;point cloud;surface fitting;stereo vision}, 
doi={10.1109/ICPR.2016.7899769}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8358484, 
author={K. M. {Swetha} and P. {Suja}}, 
booktitle={2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon)}, 
title={A geometric approach for recognizing emotions from 3D images with pose variations}, 
year={2017}, 
volume={}, 
number={}, 
pages={805-809}, 
abstract={Emotions are an incredibly important aspect of human life. Research on emotion recognition for the past few decades have resulted in development of several fields. In the current scenario, it is necessary that machines/robots need to identify human emotions and respond accordingly. Applications in this field can be seen in security, entertainment and Human Machine Interface/Human Robot Interface. Recent works on 3D images have gained importance due to its accuracy in real life applications as emotions can be recognised at different head poses. The intention of this work has been to develop an algorithm for recognition of emotion from facial expressions, which recognizes 6 basic emotions, which are anger, fear, happy, disgust, sad and surprise from 3D images in 7 yaw angles (+45° to -45°) and 3 pitch angles (+15°,0°, -15°). Most of the reported work considers + yaw angles. While in the current work, both positive as well as negative pitch and yaw angles are considered. BU3DFE database is used for the implementation. The proposed method resulted in improved accuracy and is comparable with the literature.}, 
keywords={emotion recognition;face recognition;geometry;pose estimation;BU3DFE database;geometric approach;pose variations;emotion recognition;human emotions;head poses;yaw angles;pitch angles;3D image;facial expressions;Basic emotions;feature points;BU3DFE database;classification}, 
doi={10.1109/SmartTechCon.2017.8358484}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7081271, 
author={M. C. {EL Rai} and N. {Werghi} and H. {Al Muhairi} and H. {Alsafar}}, 
booktitle={2015 International Conference on Communications, Signal Processing, and their Applications (ICCSPA'15)}, 
title={Using facial images for the diagnosis of genetic syndromes: A survey}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The analysis of facial appearance is significant to an early diagnosis of medical genetic diseases. The fast development of image processing and machine learning techniques facilitates the detection of facial dysmorphic features. This paper is a survey of the recent studies developed for the screening of genetic abnormalities across the facial features obtained from two dimensional and three dimensional images.}, 
keywords={diseases;face recognition;learning (artificial intelligence);medical image processing;patient diagnosis;facial images;genetic syndromes diagnosis;facial appearance analysis;medical genetic diseases;image processing;machine learning techniques;facial dysmorphic features;genetic abnormalities;facial features;two dimensional images;three dimensional images;Three-dimensional displays;Face;Genetics;Feature extraction;Principal component analysis;Accuracy;Databases;Facial images;2D imaging;3D imaging;Landmarks;Dysmorphology;Classification}, 
doi={10.1109/ICCSPA.2015.7081271}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8125644, 
author={A. V. {Kumar} and V. V. R. {Prasad} and K. M. {Bhurchandi} and V. R. {Satpute} and L. {Pious} and S. {Kar}}, 
booktitle={2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
title={Dense reconstruction of 3D human face using 5 images and no reference model}, 
year={2017}, 
volume={}, 
number={}, 
pages={1185-1190}, 
abstract={3D reconstruction of human faces is of high importance in applications like forensics, facial features based human tracking and realistic reconstruction of human faces in virtual reality applications. The published algorithms so far require either a large number of views of a human face or a 3D facial reference model. This paper proposes a technique for 3D human face reconstruction using only five views without any reference model unlike most of the contemporary facial reconstruction techniques. Face localization is done followed by facial feature point extraction in the facial images. The features are tracked in the other images using the Kanade-Lucas-Tomasi (KLT) object tracking algorithm. 3D location of each feature is calculated in all the images using triangulation and a 3D point cloud is formed. The Point cloud is processed to remove outliers and holes. The 3D face is then formed by interpolating and meshing the point cloud. This computationally efficient and low cost technique outperforms commercial and online available software and published algorithms.}, 
keywords={face recognition;feature extraction;image reconstruction;object tracking;solid modelling;virtual reality;dense reconstruction;facial features;human tracking;3D facial reference model;contemporary facial reconstruction techniques;face localization;facial feature point extraction;facial images;point cloud;3D human face;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Skin;Image color analysis;Cameras;3D Reconstruction;meshing;point cloud;tracking;triangulation}, 
doi={10.1109/CoDIT.2017.8125644}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7175983, 
author={W. {Gutfeter} and A. {Pacut}}, 
booktitle={2015 IEEE 2nd International Conference on Cybernetics (CYBCONF)}, 
title={Face 3D biometrics goes mobile: Searching for applications of portable depth sensor in face recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={489-494}, 
abstract={This paper presents an acquisition procedure and method of processing spatial images for face recognition with the use of a novel type of scanning device, namely mobile depth sensor Structure. Depth sensors, often called RGBD cameras, are able to deliver 3D images with a frame rate 30-60 frames per second, however they have relatively low resolution and a high level of noise. This kind of data is compared here with a high quality scans enrolled by the structural light scanner, for which the acquisition time is approximately 1.5 s for a single image, and which - because of its size - cannot be classified as a portable device. The purpose of this work was to find the method that will allow us to extract spatial features from mobile data sources analyzed here only in a static context. We transform the 3D data into local surface features and then into vectors of unified length by use of the Moving Least Squares method applied to a predefined grid of points on a reference cylinder. The feature matrices were calculated for various image features, and used in PCA analysis. Finally, the verification errors were calculated and compared to those obtained for stationary devices. The results show that single-image mobile sensor images lead to the results inferior to those of stationary sensors. However, we suggest a dynamic depth stream processing as the next step in the evolution of the described method. The presented results show that by including multi-frame processing into our method, it is likely to gain the accuracy similar to those obtained for a stationary device under controlled laboratory conditions.}, 
keywords={cameras;face recognition;feature extraction;image sensors;least squares approximations;principal component analysis;face 3D biometrics;portable depth sensor;acquisition procedure;spatial images processing;face recognition;scanning device;mobile depth sensor structure;RGBD cameras;3D images;frame rate;high quality scans;structural light scanner;acquisition time;spatial features extraction;mobile data sources;3D data;local surface features;vectors;moving least squares method;feature matrices;image features;PCA analysis;verification errors;stationary devices;single-image mobile sensor images;stationary sensors;dynamic depth stream processing;Face;Three-dimensional displays;Databases;Face recognition;Principal component analysis;Robot sensing systems;Approximation methods}, 
doi={10.1109/CYBConf.2015.7175983}, 
ISSN={}, 
month={June},}
@ARTICLE{8408720, 
author={R. S. {Siqueira} and G. R. {Alexandre} and J. M. {Soares} and G. A. P. {Thé}}, 
journal={IEEE Robotics and Automation Letters}, 
title={Triaxial Slicing for 3-D Face Recognition From Adapted Rotational Invariants Spatial Moments and Minimal Keypoints Dependence}, 
year={2018}, 
volume={3}, 
number={4}, 
pages={3513-3520}, 
abstract={This letter presents a multiple slicing model for threedimensional (3-D) images of human face, using the frontal, sagittal, and transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2-D spatial moments of Hu and 3-D spatial invariant rotation moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs nonneutral ROC I experiment, applying linear discriminant analysis as classifier and more than one sample for training, achieved 98.7% of verification rate at 0.1% of false acceptance rate. By using the support vector machine as classifier the rank1 experiment recognition rates of 99% and 95.4% have been achieved for a neutral vs neutral and for a neutral vs nonneutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when anger and disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity.}, 
keywords={face recognition;feature extraction;image classification;support vector machines;verification rate;false acceptance rate;support vector machine;rank1 experiment recognition rates;Bosphorus Database;3-D face recognition;multiple slicing model;human face;orthogonal planes;nose tip;facial recognition;3-D spatial invariant rotation moments;neutral vs nonneutral ROC;linear discriminant analysis;adapted rotational invariant spatial moments;minimal keypoint dependence;Three-dimensional displays;Face;Feature extraction;Nose;Two dimensional displays;Robustness;Iterative closest point algorithm;Computer vision for automation;recognition;surveillance systems}, 
doi={10.1109/LRA.2018.2854295}, 
ISSN={2377-3766}, 
month={Oct},}
@INPROCEEDINGS{8373915, 
author={W. {Tian} and F. {Liu} and Q. {Zhao}}, 
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)}, 
title={Landmark-Based 3D Face Reconstruction from an Arbitrary Number of Unconstrained Images}, 
year={2018}, 
volume={}, 
number={}, 
pages={774-779}, 
abstract={In this paper, we propose a novel method for reconstructing 3D faces from 2D images. The method is characterized in three aspects. (i) It utilizes only geometric cues in the input images, i.e., 2D facial landmarks. (ii) It works for an arbitrary number of unconstrained images, both single and multiple images. (iii) It can effectively exploit complementary information in multiple images of varying poses and expressions. The method is implemented based on cascaded regression in shape space. We have evaluated the method on three databases and observed from the experimental results that (i) the reconstruction error is reduced as more images of different poses are used, (ii) the proposed method can obtain comparable reconstruction results by using state-of-the-art automated methods to detect the 2D landmarks, and (iii) the proposed method is robust to variations in facial expressions and image qualities.}, 
keywords={face recognition;image reconstruction;regression analysis;stereo image processing;cascaded regression;image qualities;facial expressions;reconstruction error;2D facial landmarks;geometric cues;unconstrained images;3D face reconstruction;Conferences;Face;Gesture recognition;3D face reconstruction;unconstrained images;landmark based;cascade regression}, 
doi={10.1109/FG.2018.00122}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7852823, 
author={ and }, 
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={Joint subspace learning for reconstruction of 3D facial dynamic expression from single image}, 
year={2016}, 
volume={}, 
number={}, 
pages={820-824}, 
abstract={Recently, the synthesis of 3D dynamic expressions has become an important concern in computer graphics, facial recognition, etc. In this study, we propose a regression based joint subspace learning method for the automatic synthesis of 3D dynamic expression images. This method synthesizes 3D dynamic expression images from a single 2D facial image. We use two subspaces (the view subspace and the frame subspace) to synthesize a 3D image. First, we use the view subspace to estimate multi-view facial images from a front image. Next, we construct a 3D image using the estimated multi-view facial images. Finally, we estimate the 3D images in different frames by using the frame subspace to synthesis 3D dynamic expression images. This approach is unlike the conventional joint subspace learning in which, the coefficients estimated by the input image are directly used for synthesis. Furthermore, we propose using textural information to improve the accuracy of synthesized images.}, 
keywords={computer graphics;estimation theory;face recognition;image reconstruction;learning (artificial intelligence);regression analysis;stereo image processing;3D facial dynamic expression reconstruction;single image;computer graphics;facial recognition;regression based joint subspace learning;multiview facial image estimation;Three-dimensional displays;Shape;Image reconstruction;Two dimensional displays;Principal component analysis;Training;Joints;3D dynamic expressions;multi-view facial shape;joint subspace learning;regression}, 
doi={10.1109/CISP-BMEI.2016.7852823}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8518089, 
author={S. {Li} and L. {Su} and Y. {Liu} and Z. {He}}, 
booktitle={IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium}, 
title={Segmentation of Individual Trees Based on a Point Cloud Clustering Method Using Airborne Lidar Data}, 
year={2018}, 
volume={}, 
number={}, 
pages={7520-7523}, 
abstract={The objective of this paper was to develop a new algorithm to segment individual trees directly by using the three-dimensional space characteristic of airborne light detection and ranging point cloud data. The local maximum method was used in the initial segmentation and the error identification tree exclusion. On the basis of the point cloud spatial distribution of individual trees and the adjacent relationship with the other trees, a point cloud clustering method was developed to decide the points belonging to the individual trees. This algorithm was tested by 6 forest plots in the Genhe forestry reserve. The results showed that this algorithm could segment individual trees quickly and accurately, and the overall accuracy of this algorithm was 96.3%.}, 
keywords={forestry;geophysical image processing;image segmentation;optical radar;pattern clustering;remote sensing by laser beam;vegetation mapping;segment individual trees;point cloud clustering method;airborne lidar data;three-dimensional space characteristic;airborne light detection;ranging point cloud data;local maximum method;initial segmentation;error identification tree exclusion;point cloud spatial distribution;Genhe forestry reserve;Vegetation;Three-dimensional displays;Forestry;Laser radar;Clustering algorithms;Remote sensing;Lasers;LiDAR;segmentation;tree;clustering}, 
doi={10.1109/IGARSS.2018.8518089}, 
ISSN={2153-7003}, 
month={July},}
@INPROCEEDINGS{7797090, 
author={S. Z. {Gilani} and A. {Mian}}, 
booktitle={2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)}, 
title={Towards Large-Scale 3D Face Recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases.}, 
keywords={convolution;face recognition;feature extraction;image matching;learning (artificial intelligence);solid modelling;large-scale 3D face recognition;fully convolutional deep network;FCDN;seed points extraction;level set curves;single curvature dependent adaptive speed;dense correspondence;training set;surface matching;morphable model fitting;landmark localization results;FRGCv2 dataset;Bosphorus dataset;query face recognition;Three-dimensional displays;Face;Face recognition;Solid modeling;Databases;Two dimensional displays;Robustness}, 
doi={10.1109/DICTA.2016.7797090}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7428562, 
author={J. {Liu} and Q. {Zhang} and C. {Tang}}, 
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
title={Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information}, 
year={2015}, 
volume={}, 
number={}, 
pages={276-284}, 
abstract={3D facial landmarks play a significant role in many 3D facial studies. Due to the complex geometry of high resolution 3D human face, landmark detection remains to be a challenge problem. This paper proposes a novel method to detect landmarks automatically for high resolution 3D faces without learning or training, solely using geometric information. For high resolution 3D faces, geodesic remeshing is used to reduce the vertices number, then the remeshed 3D faces are parameterized and interpolated on a regular grid, which enable us to compute the differential geometric features more efficiently and accurately. To detect landmarks, we extract global and local constraints for each landmark by considering relative positions of landmarks and differential geometric features, then landmarks are detected by combine these constraints. We evaluate our method and compare it with other methods which are mostly related to ours in a large scale publicly available 3D face data set, BJUT-3D face database. The experimental results show that our method is robust and effective, and achieves better performance than existing methods.}, 
keywords={face recognition;feature extraction;image resolution;interpolation;mesh generation;object detection;automatic landmark detection;high resolution nonrigid 3D faces;geometric information;3D facial landmarks;geodesic remeshing;vertices number;parameterization;interpolation;regular grid;differential geometric features;global constraints extraction;local constraints extraction;Three-dimensional displays;Indexes;Shape;Training;Feature extraction;Facial animation;Mesh generation;3D faces;landmarks;geometric information;geodesic remeshing;differential geometric features}, 
doi={10.1109/IAEAC.2015.7428562}, 
ISSN={}, 
month={Dec},}
@ARTICLE{7312454, 
author={M. A. {de Jong} and A. {Wollstein} and C. {Ruff} and D. {Dunaway} and P. {Hysi} and T. {Spector} and F. {Liu} and W. {Niessen} and M. J. {Koudstaal} and M. {Kayser} and E. B. {Wolvius} and S. {Böhringer}}, 
journal={IEEE Transactions on Image Processing}, 
title={An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}, 
year={2016}, 
volume={25}, 
number={2}, 
pages={580-588}, 
abstract={In this paper, we present a novel approach to automatic 3D facial landmarking using 2D Gabor wavelets. Our algorithm considers the face to be a surface and uses map projections to derive 2D features from raw data. Extracted features include texture, relief map, and transformations thereof. We extend an established 2D landmarking method for simultaneous evaluation of these data. The method is validated by performing landmarking experiments on two data sets using 21 landmarks and compared with an active shape model implementation. On average, landmarking error for our method was 1.9 mm, whereas the active shape model resulted in an average landmarking error of 2.3 mm. A second study investigating facial shape heritability in related individuals concludes that automatic landmarking is on par with manual landmarking for some landmarks. Our algorithm can be trained in 30 min to automatically landmark 3D facial data sets of any size, and allows for fast and robust landmarking of 3D faces.}, 
keywords={face recognition;feature extraction;Gabor filters;wavelet transforms;automatic 3D facial landmarking algorithm;2D Gabor wavelets;map projections;feature extraction;data sets;active shape model;landmarking error;facial shape heritability;automatic landmarking;Three-dimensional displays;Face;Ellipsoids;Accuracy;Training;Solid modeling;Nose;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Algorithms;Anatomic Landmarks;Face;Humans;Imaging, Three-Dimensional;Pattern Recognition, Automated;Wavelet Analysis}, 
doi={10.1109/TIP.2015.2496183}, 
ISSN={1057-7149}, 
month={Feb},}
@INPROCEEDINGS{8525654, 
author={M. P. {Zapf} and A. {Gupta} and L. Y. {Morales Saiki} and M. {Kawanabe}}, 
booktitle={2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}, 
title={Data-Driven, 3-D Classification of Person-Object Relationships and Semantic Context Clustering for Robotics and AI Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={180-187}, 
abstract={We introduce a framework for detection and classification of spatio-temporal person-object interactions. Our method clusters similar semantic contexts from interactions detected from RGB-D data. 2-D object detection (YOLO) is run on RGB data from a Kinect v2 sensor on a mobile robot navigating an office and observing persons and desk spaces. Person and object detections are converted into 3-D point cloud time series via RGB-Depth co-registration and successive Euclidean and k-means spatial clustering. 3-D person and object point cloud streams are used to create time-series occupancy maps and person-object co-localization maps. From these maps, spatiotemporal correlations between persons and distinct objects are computed. Correlation patterns are clustered using k-means to obtain distinct human-object interactions, i.e. segment semantic context over time. We evaluated the performance of our approach to detect person-object correlations and cluster semantic context by recording 90 30-second RGB-D data episodes, with three persons handling representative objects (books, cups, bottles). Experimental results show that our framework is able to consistently assign semantic context to the same cluster in > 79% of cases (scene frames). Semantic contexts in visual scenes can be distinguished without the need to provide prior information, allowing mobile agents to learn and explore in new environments.}, 
keywords={feature extraction;image classification;image colour analysis;image motion analysis;image registration;image segmentation;mobile robots;object detection;path planning;pattern clustering;robot vision;time series;data-driven;3D classification;k-means spatial clustering;spatio-temporal person-object interaction detection;spatio-temporal person-object interaction classification;2D object detection;3D point cloud time series;RGB-depth coregistration;Euclidean clustering;person-object colocalization maps;correlation pattern clustering;semantic context segmentation;mobile agents;RGB-D data episodes;person-object correlations;human-object interactions;spatiotemporal correlations;time-series occupancy maps;object point cloud streams;mobile robot;Kinect v2 sensor;RGB data;semantic context clustering;person-object relationships;Three-dimensional displays;Robot sensing systems;Semantics;Object detection;Feature extraction;Correlation}, 
doi={10.1109/ROMAN.2018.8525654}, 
ISSN={1944-9437}, 
month={Aug},}
@INPROCEEDINGS{7163161, 
author={S. {Cheng} and I. {Marras} and S. {Zafeiriou} and M. {Pantic}}, 
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
title={Active nonrigid ICP algorithm}, 
year={2015}, 
volume={1}, 
number={}, 
pages={1-8}, 
abstract={The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15-20 years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region.}, 
keywords={face recognition;statistical analysis;facial surfaces;iterative closest point algorithm;nonrigid iterative closest point algorithms;3D face fitting;mouth region;statistical 3D model;3D facial deformation;single facial model;3D facial shape;3D mesh;3D facial model;active nonrigid ICP algorithm;Face;Three-dimensional displays;Iterative closest point algorithm;Solid modeling;Deformable models;Shape;Mouth}, 
doi={10.1109/FG.2015.7163161}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8035340, 
author={L. {Han} and Q. {Xiao} and X. {Liang}}, 
booktitle={2017 International Conference on Computer, Information and Telecommunication Systems (CITS)}, 
title={3D face reconstruction based on progressive cascade regression}, 
year={2017}, 
volume={}, 
number={}, 
pages={297-301}, 
abstract={In order to better learn the distributions of 2D and 3D faces and the mapping between them with limited training samples, a new 3D face reconstruction method based on progressive cascade regression is proposed. Firstly, it learns the mapping between 2D and 3D facial landmarks to estimate the initial 3D facial landmarks with a coupled space learning method. Secondly, a deformed space is constructed with the difference between the estimated initial landmarks and the ground truth of training samples; and more accurate 3D facial landmarks are reconstructed by modifying the initial 3D ones with shape compensations which are calculated by minimizing an objective function. Finally, the realistic 3D faces are reconstructed by a method that is based on a simple sparse regulation and shape deformation. The results on BJUT 3D face database demonstrate the effectiveness of the proposed method. In addition, compared with some typical methods, the method can get better subjective and objective results, especially in details.}, 
keywords={face recognition;image reconstruction;learning (artificial intelligence);regression analysis;solid modelling;3D face reconstruction;progressive cascade regression;training samples;facial landmark mapping;coupled space learning;deformed space construction;sparse regulation;shape deformation;Three-dimensional displays;Face;Training;Two dimensional displays;Image reconstruction;Solid modeling;Shape}, 
doi={10.1109/CITS.2017.8035340}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8099972, 
author={L. {Sheng} and J. {Cai} and T. {Cham} and V. {Pavlovic} and K. N. {Ngan}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Generative Model for Depth-Based Robust 3D Facial Pose Tracking}, 
year={2017}, 
volume={}, 
number={}, 
pages={4598-4607}, 
abstract={We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face models visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.}, 
keywords={face recognition;object tracking;pose estimation;ray tracing;statistical analysis;generative model;arbitrary facial expression variations;generative framework;face models visibility;depth-based robust 3D facial pose tracking;unconstrained scenarios;face model adaptation;statistical 3D face model;uncertainty prediction;distribution prediction;ray visibility constraint;Biwi datasets;ICT-3DHP datasets;Face;Three-dimensional displays;Solid modeling;Adaptation models;Robustness;Shape;Probabilistic logic}, 
doi={10.1109/CVPR.2017.489}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{7428566, 
author={J. {Liu} and Q. {Zhang} and C. {Tang}}, 
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
title={CoMES: A novel method for robust nose tip detection in face range images}, 
year={2015}, 
volume={}, 
number={}, 
pages={309-315}, 
abstract={As the most distinct feature point in facial landmarks, nose tip plays a significant role in 3D facial studies. Successful detection of nose tip can facilitate many 3D facial studies tasks. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, need not training, can handle large rotations and occlusions. We first remove small isolated connected regions and noise from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, we compute multi-angle energy of each point, then we use hierarchical clustering method to cluster the points whose multi-angle energies are larger than a threshold value. In the largest cluster, we can find one point with the largest multi-angle energy. For all scales of the scale-space, we get a series of such points and apply hierarchical clustering again for these points, nose tip will have the largest multi-angle energy in the largest cluster. We evaluate our method in FRGC v2.0 3D face database and BOSPHORUS 3D face database. The experimental results verify the robustness of our method with a high nose tip detection rate.}, 
keywords={edge detection;face recognition;pattern clustering;solid modelling;CoMES;robust nose tip detection;face range images;facial landmark;3D facial studies;scale-space;multiangle energy;hierarchical clustering method;FRGC v2.0 3D face database;BOSPHORUS 3D face database;Nose;Face;Three-dimensional displays;Training;Robustness;Feature extraction;Smoothing methods;nose tip;3D faces;scale-space;multi-angle energy;hierarchical clustering;range images}, 
doi={10.1109/IAEAC.2015.7428566}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7852696, 
author={F. {Li} and C. {Lai} and S. {Jin} and Y. {Peng}}, 
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={Automatic calibration of 3D human faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={135-139}, 
abstract={This paper presents a method for correcting the posture of 3D face with unknown position and posture based on the standard face pose. We define a standard posture of face which is not affected by the head gesture. Then the algorithms of principal component analysis and iterative closest point are used to achieve the preliminary adjustment of the target model. For the precise calibration, we use the method of depth values iterative searching to find the point of nose precisely, and propose an algorithm of searching along a straight line to find the ridge line of nose. Ultimately we determine the difference between the posture of the target model and standard posture to obtain accurate calculation of the target model posture.}, 
keywords={calibration;computer graphics;face recognition;principal component analysis;automatic calibration;3D human faces;standard face pose;standard posture;head gesture;principal component analysis;iterative closest point;precise calibration;iterative searching;target model posture;Three-dimensional displays;Calibration;Nose;Standards;Iterative closest point algorithm;Solid modeling;Principal component analysis;3D Faces;Posture Correction;Face Calibration;Facial Feature Points}, 
doi={10.1109/CISP-BMEI.2016.7852696}, 
ISSN={}, 
month={Oct},}
@ARTICLE{7973095, 
author={S. Z. {Gilani} and A. {Mian} and F. {Shafait} and I. {Reid}}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Dense 3D Face Correspondence}, 
year={2018}, 
volume={40}, 
number={7}, 
pages={1584-1598}, 
abstract={We present an algorithm that automatically establishes dense correspondences between a large number of 3D faces. Starting from automatically detected sparse correspondences on the outer boundary of 3D faces, the algorithm triangulates existing correspondences and expands them iteratively by matching points of distinctive surface curvature along the triangle edges. After exhausting keypoint matches, further correspondences are established by generating evenly distributed points within triangles by evolving level set geodesic curves from the centroids of large triangles. A deformable model (K3DM) is constructed from the dense corresponded faces and an algorithm is proposed for morphing the K3DM to fit unseen faces. This algorithm iterates between rigid alignment of an unseen face followed by regularized morphing of the deformable model. We have extensively evaluated the proposed algorithms on synthetic data and real 3D faces from the FRGCv2, Bosphorus, BU3DFE and UND Ear databases using quantitative and qualitative benchmarks. Our algorithm achieved dense correspondences with a mean localisation error of 1.28 mm on synthetic faces and detected 14 anthropometric landmarks on unseen real faces from the FRGCv2 database with 3 mm precision. Furthermore, our deformable model fitting algorithm achieved 98.5 percent face recognition accuracy on the FRGCv2 and 98.6 percent on Bosphorus database. Our dense model is also able to generalize to unseen datasets.}, 
keywords={face recognition;image matching;iterative methods;stereo image processing;automatically detected sparse correspondences;distinctive surface curvature;triangle edges;keypoint matches;K3DM;FRGCv2;deformable model fitting algorithm;face recognition;3D face correspondence;Face;Three-dimensional displays;Shape;Deformable models;Solid modeling;Databases;Face recognition;Dense correspondence;3D face;morphing;keypoint detection;level sets;geodesic curves;deformable model}, 
doi={10.1109/TPAMI.2017.2725279}, 
ISSN={0162-8828}, 
month={July},}
@INPROCEEDINGS{8490988, 
author={V. F. {Abrevaya} and S. {Wuhrer} and E. {Boyer}}, 
booktitle={2018 International Conference on 3D Vision (3DV)}, 
title={Spatiotemporal Modeling for Efficient Registration of Dynamic 3D Faces}, 
year={2018}, 
volume={}, 
number={}, 
pages={371-380}, 
abstract={We consider the registration of temporal sequences of 3D face scans. Face registration plays a central role in face analysis applications, for instance recognition or transfer tasks, among others. We propose an automatic approach that can register large sets of dynamic face scans without the need for landmarks or highly specialized acquisition setups. This allows for extended versatility among registered face shapes and deformations by enabling to leverage multiple datasets, a fundamental property when e.g. building statistical face models. Our approach is built upon a regression-based static registration method, which is improved by spatiotemporal modeling to exploit redundancies over both space and time. We experimentally demonstrate that accurate registrations can be obtained for varying data robustly and efficiently by applying our method to three standard dynamic face datasets.}, 
keywords={face recognition;image registration;image sequences;regression analysis;temporal sequences;3D face scans;face registration;face analysis applications;instance recognition;dynamic face scans;registered face shapes;regression-based static registration method;spatiotemporal modeling;standard dynamic face datasets;dynamic 3D face registration;statistical face models;Three-dimensional displays;Spatiotemporal phenomena;Registers;Solid modeling;Shape;Dynamics;Data models;Registration;3D Face;Spatiotemporal}, 
doi={10.1109/3DV.2018.00050}, 
ISSN={2475-7888}, 
month={Sep.},}
@INPROCEEDINGS{8100068, 
author={W. {Peng} and Z. {Feng} and C. {Xu} and Y. {Su}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Parametric T-Spline Face Morphable Model for Detailed Fitting in Shape Subspace}, 
year={2017}, 
volume={}, 
number={}, 
pages={5515-5523}, 
abstract={Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration for the synthesis of 3D faces by assuming that faces are in a linear class. However, the human face is in a nonlinear manifold, and a new test are always not in the pre-learnt subspace accurately because of the disparity brought by ethnicity, age, gender, etc. In the paper, we propose a parametric T-spline morphable model (T-splineMM) for 3D face representation, which has great advantages of fitting data from an unknown source accurately. In the model, we describe a face by C^2 T-spline surface, and divide the face surface into several shape units (SUs), according to facial action coding system (FACS), on T-mesh instead of on the surface directly. A fitting algorithm is proposed to optimize coefficients of T-spline control point components along pre-learnt identity and expression subspaces, as well as to optimize the details in refinement progress. As any pre-learnt subspace is not complete to handle the variety and details of faces and expressions, it covers a limited span of morphing. SUs division and detail refinement make the model fitting the facial muscle deformation in a larger span of morphing subspace. We conduct experiments on face scan data, kinect data as well as the space-time data to test the performance of detail fitting, robustness to missing data and noise, and to demonstrate the effectiveness of our model. Convincing results are illustrated to demonstrate the effectiveness of our model compared with the popular methods.}, 
keywords={face recognition;feature extraction;image morphing;image reconstruction;mesh generation;muscle;solid modelling;splines (mathematics);shape units;facial action coding system;fitting algorithm;T-spline control point components;expression subspaces;face scan data;parametric T-spline face morphable model;shape subspace pre-learnt subspace;human face;parametric T-spline morphable model for 3D;T-spline surface;face surface;kinect data;space-time data;Face;Three-dimensional displays;Shape;Solid modeling;Lips;Splines (mathematics);Computational modeling}, 
doi={10.1109/CVPR.2017.585}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{7391814, 
author={ and and and }, 
booktitle={2015 International Conference on 3D Imaging (IC3D)}, 
title={Robust nose tip detection for face range images based on local features in scale-space}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Being the most distinct feature point in 3D facial landmarks, nose tip plays a significant role in 3D facial studies such as face detection, face recognition, facial features extraction, face alignment, etc. Successful detection of nose tip can facilitate many tasks of 3D facial studies. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, needs not training, can handle large rotations and occlusions. To reduce computational cost, we first remove small isolated regions from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, the Multi-angle Energy (ME) of each point is computed and sorted in descending order. Then the first m points in the descending order list are obtained and hierarchical clustering method is used to cluster these points. In the first h largest clusters, we can find one point with the largest ME. For all scales of the scale-space, we get a series of such points which are treated as nose tip candidates. For these candidates, we apply hierarchical clustering again. In the obtained largest cluster, we compute the mean value of ME. The ME of nose tip will be closest to the mean value. We evaluate our method in two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The experimental results verify the robustness of our method with a high nose tip detection rate.}, 
keywords={face recognition;feature extraction;object detection;pattern clustering;nose tip detection;face range images;local features;3D facial landmarks;3D facial studies;multiangle energy;ME;range image smoothing;hierarchical clustering method;FRGC database;BOSPHORUS database;Nose;Three-dimensional displays;Face;Robustness;Training;Feature extraction;Smoothing methods;nose tip;3D faces;range images;robust smoothing;normal;scale-space;multi-angle energy;sphere fitting;least square;hierarchical clustering}, 
doi={10.1109/IC3D.2015.7391814}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7780900, 
author={T. {Bolkart} and S. {Wuhrer}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Robust Multilinear Model Learning Framework for 3D Faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={4911-4919}, 
abstract={Multilinear models are widely used to represent the statistical variations of 3D human faces as they decouple shape changes due to identity and expression. Existing methods to learn a multilinear face model degrade if not every person is captured in every expression, if face scans are noisy or partially occluded, if expressions are erroneously labeled, or if the vertex correspondence is inaccurate. These limitations impose requirements on the training data that disqualify large amounts of available 3D face data from being usable to learn a multilinear model. To overcome this, we introduce the first framework to robustly learn a multilinear model from 3D face databases with missing data, corrupt data, wrong semantic correspondence, and inaccurate vertex correspondence. To achieve this robustness to erroneous training data, our framework jointly learns a multilinear model and fixes the data. We evaluate our framework on two publicly available 3D face databases, and show that our framework achieves a data completion accuracy that is comparable to state-of-the-art tensor completion methods. Our method reconstructs corrupt data more accurately than state-of-the-art methods, and improves the quality of the learned model significantly for erroneously labeled expressions.}, 
keywords={computer graphics;face recognition;learning (artificial intelligence);visual databases;robust multilinear model learning framework;3D faces;statistical variations;3D human faces;multilinear face model;face scans;3D face databases;missing data;corrupt data;semantic correspondence;inaccurate vertex correspondence;erroneous training data;data completion accuracy;tensor completion methods;Data models;Computational modeling;Three-dimensional displays;Solid modeling;Semantics;Robustness;Shape}, 
doi={10.1109/CVPR.2016.531}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7888183, 
author={R. {Chellappa} and J. {Chen} and R. {Ranjan} and S. {Sankaranarayanan} and A. {Kumar} and V. M. {Patel} and C. D. {Castillo}}, 
booktitle={2016 Information Theory and Applications Workshop (ITA)}, 
title={Towards the design of an end-to-end automated system for image and video-based recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Over many decades, researchers working in object recognition have longed for an end-to-end automated system that will simply accept 2D or 3D image or videos as inputs and output the labels of objects in the input data. Computer vision methods that use representations derived based on geometric, radiometric and neural considerations and statistical and structural matchers and artificial neural network-based methods where a multi-layer network learns the mapping from inputs to class labels have provided competing approaches for image recognition problems. Over the last four years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements on object detection/recognition challenge problems. This has been made possible due to the availability of large annotated data, a better understanding of the non-linear mapping between image and class labels as well as the affordability of GPUs. In this paper, we present a brief history of developments in computer vision and artificial neural networks over the last forty years for the problem of image-based recognition. We then present the design details of a deep learning system for end-to-end unconstrained face verification/recognition. Some open issues regarding DCNNs for object recognition problems are then discussed. We caution the readers that the views expressed in this paper are from the authors and authors only!}, 
keywords={computer vision;convolution;face recognition;image representation;learning (artificial intelligence);neural nets;object detection;object recognition;video signal processing;end-to-end automated system;image-based recognition;video-based recognition;object recognition;computer vision;image representations;geometric considerations;radiometric considerations;neural considerations;statistical matchers;structural matchers;artificial neural network;multilayer network;deep convolutional neural networks;DCNNs;object detection;deep learning system;end-to-end unconstrained face recognition;Face;Computer vision;Object recognition;Machine learning;Videos;Image recognition;Feature extraction}, 
doi={10.1109/ITA.2016.7888183}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7899906, 
author={H. X. {Pham} and V. {Pavlovic} and and }, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Robust real-time performance-driven 3D face tracking}, 
year={2016}, 
volume={}, 
number={}, 
pages={1851-1856}, 
abstract={We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker capable of handling a wide range of target scene depths, whose performances are demonstrated in our extensive experiments better than those of the state-of-the-arts.}, 
keywords={cameras;computational geometry;face recognition;image colour analysis;object tracking;optimisation;solid modelling;stereo image processing;video signal processing;3D face tracking;RGBD video streams;head pose tracking;facial actions;cameras;point cloud;3D shape regressor;joint 2D+3D optimization;3D model reconstruction;Three-dimensional displays;Shape;Face;Training;Two dimensional displays;Optimization;Solid modeling}, 
doi={10.1109/ICPR.2016.7899906}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8074488, 
author={T. {Indumathi} and M. {Pushparani}}, 
booktitle={2017 World Congress on Computing and Communication Technologies (WCCCT)}, 
title={Multimodel Human Authentication by Matching 3D Skull and Gait}, 
year={2017}, 
volume={}, 
number={}, 
pages={38-42}, 
abstract={This research paper focuses multimodal human identifications that are captured in a web cam as images of face and walking style. The image can then be considered for further analyses of gait and skull characteristics as per Human Identification Systems. We propose to identify a skull by using a correlation measure between the 3D skull and 3D face in terms of morphology, and measure the correlation using Enhance Canonical Correlation Coefficient Analysis (ECCCA). We use the 3D skull data as the probe and 3D face geometric data as the gallery and match the skull with enrolled 3D faces by the correlation measure between the Probe and the Gallery. This paper proposes Uncorrelated Multilinear Discriminant Analysis (UMLDA) algorithm for the challenging problem of Gait Recognition. Finally, Neural Network for mat-lab tool is used for training and testing purpose. We have created different model of neural network based on hidden layer, selection of training algorithm and setting the different parameter for training. And then, we will test for the combination of NN+SVM, Knearest Neighbour Classification. Here all these experiments are done on CASIA gait database and input video.}, 
keywords={face recognition;gait analysis;image classification;image matching;learning (artificial intelligence);neural nets;support vector machines;3D skull;mat-lab tool;hidden layer;training algorithm;NN-SVM;Knearest neighbour classification;CASIA gait database;input video;neural network;Gait Recognition;Uncorrelated Multilinear Discriminant Analysis algorithm;gallery;3D face geometric data;Enhance Canonical Correlation Coefficient Analysis;Human Identification Systems;skull characteristics;walking style;web cam;multimodal human identifications;Face;Feature extraction;Training;Three-dimensional displays;Databases;Correlation;Algorithm design and analysis;Enhance Canonical Correlation Coefficient Analysis(ECCCA);Skull Identification;Gait identification;UMLDA;Neural Network}, 
doi={10.1109/WCCCT.2016.19}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8350702, 
author={R. {Siv} and I. {Ardiyanto} and R. {Hartanto}}, 
booktitle={2018 International Conference on Information and Communications Technology (ICOIACT)}, 
title={3D human face reconstruction using depth sensor of Kinect 2}, 
year={2018}, 
volume={}, 
number={}, 
pages={355-359}, 
abstract={3D Human Face Reconstruction or 3D Human Face Modeling is a construction of 3D geometry graphics of human face model which has been a very challenging research in both computer vision and computer graphics for over the last three decades. Researchers have proposed many methods for 3D human face reconstruction by using or not using 3D scanner. One of the affordable 3D scanner today is Microsoft Kinect which is well known as a motion sensor add-on for Xbox gaming console. By using the depth sensor of Kinect version 2, 3D human face reconstruction is possible to construct, yet the depth information from Kinect scanning alone may not produce a good quality of 3D model which could lose the details of the 3D face; therefore, the optimization and improvement are needed. This research is mainly aimed to reconstruct the 3D human face model by using a single shot of 3D depth information or point cloud from the depth sensor of Microsoft Kinect version 2, and then, reconstruct with the proposed method including Poisson Surface Reconstruction. The results come as expected and are measured by comparing to the result of the zoomed of original RGB point cloud which looks like a 3D model and to the result of Microsoft 3D applications.}, 
keywords={face recognition;geometry;image reconstruction;stochastic processes;3D human face reconstruction;depth sensor;3D Human Face Modeling;3D geometry graphics;Kinect 2;Poisson Surface Reconstruction;Three-dimensional displays;Face;Solid modeling;Image reconstruction;Surface reconstruction;Surface treatment;Computational modeling;3D Face Reconstruction;3D Face Modeling;Kinect;Surface Reconstruction}, 
doi={10.1109/ICOIACT.2018.8350702}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7550062, 
author={ and D. {Huang} and Y. {Wang} and J. {Sun}}, 
booktitle={2016 International Conference on Biometrics (ICB)}, 
title={Lock3DFace: A large-scale database of low-cost Kinect 3D faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future.}, 
keywords={emotion recognition;face recognition;image texture;pose estimation;video databases;video signal processing;large-scale database;low-cost Kinect 3D face videos;Lock3DFace database;3D face analysis;3D face recognition;3D FR;expression analysis;pose analysis;occlusion analysis;time lapse analysis;texture analysis;near infrared channels;lighting condition;radiation intensity;Three-dimensional displays;Databases;Videos;Solid modeling;Two dimensional displays;Lighting;Sensors}, 
doi={10.1109/ICB.2016.7550062}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8579077, 
author={J. {Li} and B. M. {Chen} and G. H. {Lee}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={SO-Net: Self-Organizing Network for Point Cloud Analysis}, 
year={2018}, 
volume={}, 
number={}, 
pages={9397-9406}, 
abstract={This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website1.}, 
keywords={computer vision;feature extraction;learning (artificial intelligence);nearest neighbour methods;self-organising feature maps;deep learning;orderless point clouds;SO-Net models;spatial distribution;individual points;SOM nodes;input point cloud;single feature vector;point cloud reconstruction;part segmentation;shape retrieval;point cloud recognition networks;point cloud analysis;permutation invariant architecture;self-organizing map;self-organizing network;point-to-node k nearest neighbor search;SO-Net;hierarchical feature extraction;Three-dimensional displays;Feature extraction;Training;Shape;Two dimensional displays;Graphical models}, 
doi={10.1109/CVPR.2018.00979}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{8634657, 
author={S. A. {Ali Shah} and M. {Bennamoun} and M. {Molton}}, 
booktitle={2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)}, 
title={A Fully Automatic Framework for Prediction of 3D Facial Rejuvenation}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={How will I look afterwards? is a common question asked by the patients undergoing a cosmetic procedure. Cosmetic practitioners at present can only offer subjective and descriptive replies. This subjective prediction is a serious concern for patients undergoing cosmetic treatment and therefore necessitates the development of automatic techniques for facial quantification. This paper proposes a novel machine learning approach to quantify and predict the outcome of 3D facial rejuvenation prior to actual cosmetic procedure. The facial rejuvenation prediction results are achieved by estimating the dermal filler volume in 3D faces. This involves estimation of structural changes in 3D face images and to learn underlying structural mapping. Our preliminary experimental results show that the proposed model achieves superior prediction accuracy on real world dataset compared to baseline methods. The computational time analysis shows that the proposed technique is very efficient (at test time) which makes it suitable for real time applications.}, 
keywords={cosmetics;face recognition;learning (artificial intelligence);superior prediction accuracy;fully automatic framework;3D facial rejuvenation;common question;cosmetic practitioners;cosmetic treatment;automatic techniques;facial quantification;actual cosmetic procedure;facial rejuvenation prediction results;3D face images;Three-dimensional displays;Prediction algorithms;Solid modeling;Predictive models;Training;Machine learning;Australia;Facial rejuvenation;Predictive Modeling;Machine Learning}, 
doi={10.1109/IVCNZ.2018.8634657}, 
ISSN={2151-2205}, 
month={Nov},}
@INPROCEEDINGS{8003592, 
author={A. S. {Asl} and M. A. {Oskoei}}, 
booktitle={2017 5th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)}, 
title={Depth dependent invariant features applied to person detection using 3D camera}, 
year={2017}, 
volume={}, 
number={}, 
pages={29-34}, 
abstract={This paper is about detection and tracking a person by mobile robots in in-door environments, such as shopping center and hospital. It uses vision based approaches to recognize texture of clothes. The paper proposes a method to use depth (distance) reference along with scale invariant features (SIFT) to recognize patterns in various orientation, distance and illumination. SIFT is an important feature detection algorithm that is robust against rotation, translation, and scaling in 2D images and to some extent against variations in lighting conditions. But it suffers inadequate performance for visual patterns rotated in 3D space. To overcome this issue, reference inputs given to the algorithm was extended to include images taken from different angles. The proposed algorithm showed considerably improved performance in detection for real-time applications.}, 
keywords={feature extraction;image sensors;image texture;mobile robots;object recognition;object tracking;robot vision;transforms;depth dependent invariant features;person detection;3D camera;person tracking;mobile robots;shopping center;hospital;vision based approaches;cloth texture recognition;depth reference;distance reference;scale invariant features;SIFT;pattern recognition;orientation;distance;illumination;feature detection algorithm;Feature extraction;Cameras;Three-dimensional displays;Robot kinematics;Detection algorithms;Robot sensing systems;Scale invariant features;SIFT;3D images;Person detection;mobile robot}, 
doi={10.1109/CFIS.2017.8003592}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7410768, 
author={T. {Bolkart} and S. {Wuhrer}}, 
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={A Groupwise Multilinear Correspondence Optimization for 3D Faces}, 
year={2015}, 
volume={}, 
number={}, 
pages={3604-3612}, 
abstract={Multilinear face models are widely used to model the space of human faces with expressions. For databases of 3D human faces of different identities performing multiple expressions, these statistical shape models decouple identity and expression variations. To compute a high-quality multilinear face model, the quality of the registration of the database of 3D face scans used for training is essential. Meanwhile, a multilinear face model can be used as an effective prior to register 3D face scans, which are typically noisy and incomplete. Inspired by the minimum description length approach, we propose the first method to jointly optimize a multilinear model and the registration of the 3D scans used for training. Given an initial registration, our approach fully automatically improves the registration by optimizing an objective function that measures the compactness of the multilinear model, resulting in a sparse model. We choose a continuous representation for each face shape that allows to use a quasi-Newton method in parameter space for optimization. We show that our approach is computationally significantly more efficient and leads to correspondences of higher quality than existing methods based on linear statistical models. This allows us to evaluate our approach on large standard 3D face databases and in the presence of noisy initializations.}, 
keywords={emotion recognition;face recognition;image registration;Newton method;statistical analysis;visual databases;groupwise multilinear correspondence optimization;multilinear face models;statistical shape models;identity variations;expression variations;high-quality multilinear face model;3D face scan registration;minimum description length approach;quasi-Newton method;parameter space;Three-dimensional displays;Computational modeling;Solid modeling;Shape;Optimization;Tensile stress;Principal component analysis}, 
doi={10.1109/ICCV.2015.411}, 
ISSN={2380-7504}, 
month={Dec},}
@INPROCEEDINGS{8545849, 
author={X. {Sun} and L. {Huang} and C. {Liu}}, 
booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
title={Multimodal Face Spoofing Detection via RGB-D Images}, 
year={2018}, 
volume={}, 
number={}, 
pages={2221-2226}, 
abstract={While it has been shown that using 3D information might significantly benefit face anti-spoofing systems, traditional color images are still generally used, due to several issues such as expensive hardware requirement, high time cost, or poor accessibility when obtaining and using true 3D images. Thus, we could use RGB-D images captured by relatively low cost sensors instead, e.g., Kinect cameras, to achieve better performance without consuming huge amount of time or money. This research presents a novel multimodal face anti-spoofing method, which makes full use of available information on RGB-D images and no manually chosen regions are needed. For every pair of RGB-D images, first of all, we calculate the correlation between color and depth images to detect multimodal properties; then, by analyzing the consistency of sub regions extracted from the depth image, we are able to distinguish flat spoofing faces from genuine human beings. Both anti-spoofing features are fused to make final anti-spoofing decisions. Experiments on both self-collected and pubic 3DMAD datasets show that our proposed approach is effective for intra-dataset and cross-dataset testing scenarios, and that our method could deal with different presentation attacks carried by photos, tablet screens, and face masks.}, 
keywords={face recognition;feature extraction;image colour analysis;image sensors;image texture;multimodal face anti-spoofing method;final anti-spoofing decisions;anti-spoofing features;flat spoofing;depth images;relatively low cost sensors;face anti-spoofing systems;RGB-D images;multimodal face spoofing detection;Face;Image color analysis;Correlation;Three-dimensional displays;Color;Skin;Entropy}, 
doi={10.1109/ICPR.2018.8545849}, 
ISSN={1051-4651}, 
month={Aug},}
@INPROCEEDINGS{7785124, 
author={F. {Maninchedda} and C. {Häne} and M. R. {Oswald} and M. {Pollefeys}}, 
booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
title={Face Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization}, 
year={2016}, 
volume={}, 
number={}, 
pages={489-498}, 
abstract={We present a system which is able to reconstruct human faces on mobile devices with only on-device processing using the sensors which are typically built into a current commodity smart phone. Such technology can for example be used for facial authentication purposes or as a fast preview for further post-processing. Our method uses recently proposed techniques which compute depth maps by passive multi-view stereo directly on the device. We propose an efficient method which recovers the geometry of the face from the typically noisy point cloud. First, we show that we can safely restrict the reconstruction to a 2.5D height map representation. Therefore we then propose a novel low dimensional height map shape model for faces which can be fitted to the input data efficiently even on a mobile phone. In order to be able to represent instance specific shape details, such as moles, we augment the reconstruction from the shape model with a distance map which can be regularized efficiently. We thoroughly evaluate our approach on synthetic and real data, thereby we use both high resolution depth data acquired using high quality multi-view stereo and depth data directly computed on mobile phones.}, 
keywords={face recognition;image reconstruction;image resolution;stereo image processing;face reconstruction;mobile devices;height map shape model;fast regularization;facial authentication;depth maps;passive multiview stereo;face geometry;noisy point cloud;low dimensional height map shape model;high resolution depth data;high quality multiview stereo;Face;Computational modeling;Cameras;Three-dimensional displays;Shape;Solid modeling;Image reconstruction}, 
doi={10.1109/3DV.2016.59}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7869954, 
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and N. {Werghi} and M. {Linguraru}}, 
booktitle={2016 IEEE 59th International Midwest Symposium on Circuits and Systems (MWSCAS)}, 
title={Facial landmarks detection using 3D constrained local model on mesh manifold}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={This paper proposes a novel 3D Constrained Local Models (CLM) approach applied for the detection of facial landmarks in 3D images. This approach capitalizes on the properties of Independent Component Analysis (ICA) to define appropriate priors of a face Point Distribution Model. Tailored to the mesh manifold modality, this approach address the limitations of the depth images which require pose normalization and suffer from the loss of the shape information caused by 2D projection. We validate this framework through a series of experiments conducted with the public Bosporus database, whereby it demonstrates a competitive performance compared to other state of the art methods.}, 
keywords={face recognition;independent component analysis;mesh generation;object detection;facial landmarks detection;3D constrained local model;CLM approach;3D images;independent component analysis;ICA;face point distribution model;mesh manifold modality;depth images;shape information loss;2D projection;public Bosporus database;pose normalization;Shape;Principal component analysis;Face;Three-dimensional displays;Deformable models;Integrated circuit modeling;Adaptation models}, 
doi={10.1109/MWSCAS.2016.7869954}, 
ISSN={1558-3899}, 
month={Oct},}
@ARTICLE{7050293, 
author={G. {Mills} and G. {Fotopoulos}}, 
journal={IEEE Geoscience and Remote Sensing Letters}, 
title={Rock Surface Classification in a Mine Drift Using Multiscale Geometric Features}, 
year={2015}, 
volume={12}, 
number={6}, 
pages={1322-1326}, 
abstract={Scale-dependent statistical depictions of surface morphology offer the potential to parameterize complex geometrical scaling relationships with greater detail than traditional fractal measures. Using multiscale operators, it is possible to identify points belonging to rough discontinuous surfaces in noisy point clouds solely on the basis of their local geometry. Many strategies for point cloud feature classification have been developed since the proliferation of laser scanning systems. Most of the techniques which are applicable to natural scenes employ external data sources such as hyperspectral imagery, return pulse intensity, and waveform data. In this letter, multiscale geometric parameters are used to identify individual point observations corresponding to rock surfaces in point clouds acquired by terrestrial laser scanning in scenes with man-made clutter and scanning artifacts. Three multiscale operators, namely, the approximate shape and density of a defined neighborhood and the distance of its mean point from its geometric center, are fused into a single feature vector. The procedure is demonstrated using real point cloud data acquired in a mine drift, with the goal of identifying points belonging to the rock face obscured by an overlying wire support mesh. Using the extra-trees classifier, extraneous returns caused by the mesh were excluded from the point cloud with a 97% success rate, while 87% of the desired surface points were retained.}, 
keywords={feature extraction;geometry;geophysical signal processing;mining;remote sensing by laser beam;rocks;surface morphology;rock surface classification;mine drift;multiscale geometric features;scale-dependent statistical depictions;surface morphology;complex geometrical scaling relationships;traditional fractal measures;multiscale operators;rough discontinuous surfaces;noisy point clouds;local geometry;point cloud feature classification;laser scanning systems;natural scenes;external data sources;hyperspectral imagery;return pulse intensity;waveform data;multiscale geometric parameters;terrestrial laser scanning;man-made clutter;scanning artifacts;defined neighborhood;single feature vector;real point cloud data;rock face;overlying wire support mesh;extra-trees classifier;Three-dimensional displays;Rocks;Rough surfaces;Surface roughness;Fractals;Surface morphology;Classification;geology;mining industry;point clouds;terrestrial LiDAR;Classification;geology;mining industry;point clouds;terrestrial LiDAR}, 
doi={10.1109/LGRS.2015.2398814}, 
ISSN={1545-598X}, 
month={June},}
@INPROCEEDINGS{7517246, 
author={P. {Azevedo} and T. O. {Dos Santos} and E. {De Aguiar}}, 
booktitle={2016 XVIII Symposium on Virtual and Augmented Reality (SVR)}, 
title={An Augmented Reality Virtual Glasses Try-On System}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper presents a virtual try-on system to correctly visualize 3D objects (e.g., glasses) in the face of a given user. By capturing the image and depth information of a user through a low-cost RGB-D camera, we apply a face tracking technique to detect specific landmarks in the facial image. These landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3D facial morphable model that fits as good as possible to the user's head and face. At the end, we deform the chosen 3D objects from its rest shape to a deformed shape matching the specific facial shape of the user. The last step projects and renders the 3D object into the original image, with enhanced precision and in proper scale, showing the selected object in the user's face. We validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. Our results demonstrate that, by fitting a facial model to the user's face, the rendered virtual 3D objects look more realistic.}, 
keywords={augmented reality;face recognition;image capture;image colour analysis;image matching;image reconstruction;image sensors;rendering (computer graphics);shape recognition;augmented reality virtual glass try-on system;3D object visualization;image capturing;depth information;low-cost RGB-D camera;facial image;point cloud reconstruction;3D facial morphable model;deformed shape matching;virtual 3D object rendering;Three-dimensional displays;Face;Solid modeling;Glass;Cameras;Shape;Augmented reality;Virtual reality;Computer graphics}, 
doi={10.1109/SVR.2016.12}, 
ISSN={}, 
month={June},}
@ARTICLE{8370652, 
author={Y. {Wang} and Z. {Li} and T. {Vu} and N. {Nyayapathi} and K. W. {Oh} and W. {Xu} and J. {Xia}}, 
journal={IEEE Sensors Journal}, 
title={A Robust and Secure Palm Vessel Biometric Sensing System Based on Photoacoustics}, 
year={2018}, 
volume={18}, 
number={14}, 
pages={5993-6000}, 
abstract={In this paper, we propose a new palm vessel biometric sensing system based on photoacoustic imaging, which is an emerging technique that allows high-resolution visualization of optical absorption in deep tissue. Our system consists of an ultrasound (US) linear transducer array, an US data acquisition system, and an Nd:YAG laser emitting 1064-nm wavelength. By scanning the array, we could get a 3-D image of palm vasculature. The 3-D image is further combined with our newly developed algorithm, Earth Mover's Distance-Radiographic Testing, to provide precise matching and robust recognition rate. Compared to conventional vein sensing techniques, our system demonstrates deeper imaging depth and better spatial resolution, offering securer biometric features to fight against counterfeits. In this paper, we imaged 20 different hands at various poses and quantified our system performance. We found that the usability and accuracy of our system are comparable to conventional biometric techniques, such as fingerprint imaging and face identification. Our technique can open up avenues for better liveness detection and biometric measurements.}, 
keywords={biological tissues;biomedical optical imaging;biomedical transducers;biomedical ultrasonics;biometrics (access control);feature extraction;fingerprint identification;image resolution;image sensors;laser applications in medicine;medical image processing;photoacoustic effect;ultrasonic transducer arrays;high-resolution visualization;linear transducer array;US data acquisition system;3-D image;palm vasculature;Earth Mover's Distance-Radiographic Testing;robust recognition rate;conventional vein sensing techniques;deeper imaging depth;securer biometric features;system performance;conventional biometric techniques;fingerprint imaging;face identification;biometric measurements;photoacoustics;palm vessel biometric sensing system;photoacoustic imaging;Optical imaging;Veins;Biomedical imaging;Three-dimensional displays;Optical scattering;Absorption;Photoacoustic imaging (PAI);high-resolution;three-dimensional (3D) image;imaging depth;precise;robust}, 
doi={10.1109/JSEN.2018.2843119}, 
ISSN={1530-437X}, 
month={July},}
@INPROCEEDINGS{7495382, 
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and H. A. {Safar} and N. {Werghi}}, 
booktitle={2016 18th Mediterranean Electrotechnical Conference (MELECON)}, 
title={Landmarks detection on 3D face scans using local histogram descriptors}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In this work, we exploit 3D Constrained Local Model (CLM) for facial landmark detection. Our approach integrates the geometric information of 3D face scans. The fast increase demand of 3D data invite to develop 3D image processing methods for many applications and especially for automatic landmark detection. The new step in this paper is the introduction of mesh histogram of gradients (meshHOG) as local descriptors around every landmark location. The proposed work is evaluated on the publicly available Bosphorus database. A comparison with the other descriptors mesh LBP and mesh SIFT are also depicted.}, 
keywords={computational geometry;face recognition;mesh generation;object detection;facial landmark detection;3D face scans;local histogram descriptors;3D constrained local model;geometric information;3D image processing methods;mesh histogram-of-gradients;meshHOG;local descriptors;publicly available Bosphorus database;Three-dimensional displays;Face;Histograms;Solid modeling;Shape;Deformable models;Mathematical model}, 
doi={10.1109/MELCON.2016.7495382}, 
ISSN={2158-8481}, 
month={April},}
@INPROCEEDINGS{7299095, 
author={S. Z. {Gilani} and F. {Shafait} and A. {Mian}}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Shape-based automatic detection of a large number of 3D facial landmarks}, 
year={2015}, 
volume={}, 
number={}, 
pages={4639-4648}, 
abstract={We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morphologically significant. The proposed algorithm evolves level set curves with adaptive geometric speed functions to automatically extract effective seed points for dense correspondence. Correspondences are established by minimizing the bending energy between patches around seed points of given faces to those of a reference face. Given its hierarchical structure, our algorithm is capable of establishing thousands of correspondences between a large number of faces. Finally, a morphable model based on the dense corresponding points is fitted to an unseen query face for transfer of correspondences and hence automatic detection of landmarks. The proposed algorithm can detect any number of pre-defined landmarks including subtle landmarks that are even difficult to detect manually. Extensive experimental comparison on two benchmark databases containing 6, 507 scans shows that our algorithm outperforms six state of the art algorithms.}, 
keywords={face recognition;feature extraction;geometry;minimisation;shape recognition;shape-based automatic detection;3D facial landmark;geometric speed function;bending energy minimization;Three-dimensional displays;Shape;Databases;Level set;Algorithm design and analysis;Feature extraction;Mathematical model}, 
doi={10.1109/CVPR.2015.7299095}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7351535, 
author={S. {Sankaranarayanan} and V. M. {Patel} and R. {Chellappa}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={3D facial model synthesis using coupled dictionaries}, 
year={2015}, 
volume={}, 
number={}, 
pages={3896-3900}, 
abstract={In this work, we propose a generative way of modeling faces, where the 3D shape of a face is generated by a supervised learning procedure involving coupled sparse feature learning. To learn dictionaries using the proposed method, we use the USF-HUMAN ID database [1]. We provide as input to our training system, paired correspondences of 2D and 3D images of individuals and aim to learn the low-level patches both in 2D and 3D domains that describe the corresponding subspaces in a sparse manner. We demonstrate the efficacy of our method by quantitative results on the 3D database and qualitative results on images drawn from the internet.}, 
keywords={face recognition;learning (artificial intelligence);3D facial model synthesis;coupled dictionaries;supervised learning procedure;coupled sparse feature learning;dictionary learning;USF-HUMAN ID database;2D images;3D images;Three-dimensional displays;Dictionaries;Solid modeling;Shape;Training;Databases;Encoding;3D Model;Face Synthesis;Coupled Sparse Coding;Cross-modal Learning}, 
doi={10.1109/ICIP.2015.7351535}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7943450, 
author={S. {Rajeev} and K. K. M. {Shreyas} and K. {Panetta} and S. {Agaian}}, 
booktitle={2017 IEEE International Symposium on Technologies for Homeland Security (HST)}, 
title={3-D palmprint modeling for biometric verification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Palmprint is a very unique and distinctive biometric trait because of features such as a person's inimitable principal lines, wrinkles, delta points, and minutiae. These constitute the main reasons why palmprint verification is considered as one of the most reliable personal identification methods. However, a clear majority of the research on palm-prints are concentrated on 2-D palmprint images irrespective of the fact that the human palm is a 3D-surface. While 2-D palmprint recognition has proved to be efficient in terms of verification rate, it has some essential downsides. These restrictions can adversely affect the performance and robustness of the palmprint recognition system. One of the possible solutions to resolve the limitations associated with 2-D palm print authentication systems is (i) to use a 3-D scanning system and to produce high quality 3-D images with depth information; (ii) to map 3-D palm-print images into 2-D images which may support the usage of 3-D images with both biometric palmprint 2-D image databases and 2-D palmprint recognition tools. The bloom of 3-D technologies has made it easier to capture and store 3-D images. The problem of a direct mapping approach is that a large section of the palm is hard-pressed on the scanner surface during 2-D based acquisition. This paper proposes a novel technique to unravel/map 3-D palm images to its equivalent 2-D palm-print image. This image can be then used to perform efficient and accurate 2-D identification/ verification. Experimental results and discussions will also be presented.}, 
keywords={authorisation;biometrics (access control);computer graphics;palmprint recognition;biometric verification;biometric trait;2-D palmprint images;2-D palmprint recognition;2-D palmprint authentication systems;3-D palmprint images;biometric palmprint 2-D image databases;2-D palmprint recognition tools;2-D identification-verification;Fingerprint recognition;Databases;Solid modeling;Mathematical model;Biomedical imaging;Feature extraction;biometric;palm-print;2-D;3-D;authentication;verification;identification;unrolling;mapping;modeling;security}, 
doi={10.1109/THS.2017.7943450}, 
ISSN={}, 
month={April},}
@ARTICLE{7029822, 
author={Y. {Li} and Y. {Wang} and B. {Wang} and L. {Sui}}, 
journal={IET Computer Vision}, 
title={Nose tip detection on three-dimensional faces using pose-invariant differential surface features}, 
year={2015}, 
volume={9}, 
number={1}, 
pages={75-84}, 
abstract={Three-dimensional (3D) facial data offer the potential to overcome the difficulties caused by the variation of head pose and illumination in 2D face recognition. In 3D face recognition, localisation of nose tip is essential to face normalisation, face registration and pose correction etc. Most of the existing methods of nose tip detection on 3D face deal mainly with frontal or near-frontal poses or are rotation sensitive. Many of them are training-based or model-based. In this study, a novel method of nose tip detection is proposed. Using pose-invariant differential surface features - high-order and low-order curvatures, it can detect nose tip on 3D faces under various poses automatically and accurately. Moreover, it does not require training and does not depend on any particular model. Experimental results on GavabDB verify the robustness and accuracy of the proposed method.}, 
keywords={face recognition;feature extraction;image registration;object detection;pose estimation;nose tip detection;pose invariant differential surface feature;3D face recognition;head pose variation;illumination;2D face recognition;nose tip localisation;face normalisation;face registration;pose correction;nearfrontal pose;low order curvature;high order curvature}, 
doi={10.1049/iet-cvi.2014.0070}, 
ISSN={1751-9632}, 
month={},}
@INPROCEEDINGS{7938174, 
author={Y. {Zhang} and X. {Shen} and Y. {Hu}}, 
booktitle={2016 International Conference on Virtual Reality and Visualization (ICVRV)}, 
title={Face Registration and Surgical Instrument Tracking for Image-Guided Surgical Navigation}, 
year={2016}, 
volume={}, 
number={}, 
pages={65-71}, 
abstract={This paper presents a face registration method and a surgical instrument tracking method for Image-guided Surgical Navigation System (ISNS). In ISNS, we start the process by reconstructing 3d point cloud of patient using a proactive structured light visual system and then accurately complete face registration based on Iterative Closest Point (ICP) algorithm. We utilize special markers to represent the position and attitude of surgical instrument. At a final step, we recognize special markers by a syncretic corner detection algorithm and recover the 3d coordinate to track surgical instrument. To further improve the accuracy and robustness of tracking, we also utilize kalman algorithm to filter out the noise in tracking. We have tested our system on 3d printing model with matched medical data, demonstrating its accuracy and robustness of registration and tracking.}, 
keywords={face recognition;image registration;iterative methods;medical image processing;surgery;face registration;surgical instrument tracking;image-guided surgical navigation system;ISNS;3D point cloud;patient treatment;proactive structured light visual system;iterative closest point algorithm;ICP algorithm;Surgery;Instruments;Three-dimensional displays;Image reconstruction;Aerospace electronics;Face;Iterative closest point algorithm;Surgical navigation;Registration;Surgical instrument tracking;Structured light}, 
doi={10.1109/ICVRV.2016.19}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8460605, 
author={W. J. {Beksi} and N. {Papanikolopoulos}}, 
booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Signature of Topologically Persistent Points for 3D Point Cloud Description}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={We present the Signature of Topologically Persistent Points (STPP), a global descriptor that encodes topological invariants of 3D point cloud data. These topological invariants include the zeroth and first homology groups and are computed using persistent homology, a method for finding the features of a topological space at different spatial resolutions. STPP is a competitive 3D point cloud descriptor when compared to the state of art and is resilient to noisy sensor data. We demonstrate experimentally on a publicly available RGB-D dataset that STPP can be used as a distinctive signature, thus allowing for 3D point cloud processing tasks such as object detection and classification.}, 
keywords={computer graphics;image resolution;solid modelling;object classification;object detection;3D point cloud processing tasks;RGB-D dataset;spatial resolutions;topological invariant encoding;global descriptor;topologically persistent point signature;homology groups;competitive 3D point cloud descriptor;topological space;3D point cloud data;STPP;time 3.0 d;Three-dimensional displays;Shape;Robot sensing systems;Generators;Histograms;Topology;Face}, 
doi={10.1109/ICRA.2018.8460605}, 
ISSN={2577-087X}, 
month={May},}
@INPROCEEDINGS{7550083, 
author={W. P. {Koppen} and W. J. {Christmas} and D. J. M. {Crouch} and W. F. {Bodmer} and J. V. {Kittler}}, 
booktitle={2016 International Conference on Biometrics (ICB)}, 
title={Extending non-negative matrix factorisation to 3D registered data}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The use of non-negative matrix factorisation (NMF) on 2D face images has been shown to result in sparse feature vectors that encode for local patches on the face, and thus provides a statistically justified approach to learning parts from wholes. However successful on 2D images, the method has so far not been extended to 3D images. The main reason for this is that 3D space is a continuum and so it is not apparent how to represent 3D coordinates in a non-negative fashion. This work compares different non-negative representations for spatial coordinates, and demonstrates that not all non-negative representations are suitable. We analyse the representational properties that make NMF a successful method to learn sparse 3D facial features. Using our proposed representation, the factorisation results in sparse and interpretable facial features.}, 
keywords={face recognition;feature extraction;image registration;image representation;learning (artificial intelligence);matrix decomposition;nonnegative matrix factorization;NMF;3D face image registration;representational property;3D facial feature learning;Face;Three-dimensional displays;Shape;Principal component analysis;Two dimensional displays;Facial features;Encoding}, 
doi={10.1109/ICB.2016.7550083}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7139616, 
author={T. {Linder} and S. {Wehner} and K. O. {Arras}}, 
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Real-time full-body human gender recognition in (RGB)-D data}, 
year={2015}, 
volume={}, 
number={}, 
pages={3039-3045}, 
abstract={Understanding social context is an important skill for robots that share a space with humans. In this paper, we address the problem of recognizing gender, a key piece of information when interacting with people and understanding human social relations and rules. Unlike previous work which typically considered faces or frontal body views in image data, we address the problem of recognizing gender in RGB-D data from side and back views as well. We present a large, gender-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the Kinect v1 and Kinect v2 sensor. We then learn and compare several classifiers on the Kinect v2 data using a HOG baseline, two state-of-the-art deep-learning methods, and a recent tessellation-based learning approach. Originally developed for person detection in 3D data, the latter is able to learn the best selection, location and scale of a set of simple point cloud features. We show that for gender recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 Hz.}, 
keywords={human-robot interaction;image classification;image sensors;learning (artificial intelligence);object detection;object recognition;real-time full-body human gender recognition;RGB-D data;human social relations;human social rules;image data;multiperspective RGB-D dataset;gender-balanced RGB-D dataset;annotated RGB-D dataset;Kinect v2 sensor;Kinect v1 sensor;HOG baseline;deep-learning methods;tessellation-based learning approach;person detection;point cloud features;Three-dimensional displays;Robot sensing systems;Training;Legged locomotion;Accuracy;Support vector machines}, 
doi={10.1109/ICRA.2015.7139616}, 
ISSN={1050-4729}, 
month={May},}
@ARTICLE{8024025, 
author={C. {Ye} and X. {Qian}}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired}, 
year={2018}, 
volume={26}, 
number={2}, 
pages={441-450}, 
abstract={This paper presents a 3-D object recognition method and its implementation on a robotic navigation aid to allow real-time detection of indoor structural objects for the navigation of a blind person. The method segments a point cloud into numerous planar patches and extracts their inter-plane relationships (IPRs).Based on the existing IPRs of the object models, the method defines six high level features (HLFs) and determines the HLFs for each patch. A Gaussian-mixture-model-based plane classifier is then devised to classify each planar patch into one belonging to a particular object model. Finally, a recursive plane clustering procedure is used to cluster the classified planes into the model objects. As the proposed method uses geometric context to detect an object, it is robust to the object's visual appearance change. As a result, it is ideal for detecting structural objects (e.g., stairways, doorways, and so on). In addition, it has high scalability and parallelism. The method is also capable of detecting some indoor non-structural objects. Experimental results demonstrate that the proposed method has a high success rate in object recognition.}, 
keywords={feature extraction;Gaussian processes;geometry;handicapped aids;image segmentation;mobile robots;object recognition;path planning;pattern clustering;robot vision;nonstructural objects;robotic navigation aid;real-time detection;indoor structural objects;numerous planar patches;object models;HLFs;Gaussian-mixture-model;plane classifier;planar patch;particular object model;recursive plane clustering procedure;point cloud;3D-object recognition;interplane relationships;Object recognition;Three-dimensional displays;Feature extraction;Visualization;Navigation;Cameras;RNA;Robotic navigation aid;visually impaired;3D object recognition;geometric context;Gaussian mixture model}, 
doi={10.1109/TNSRE.2017.2748419}, 
ISSN={1534-4320}, 
month={Feb},}
@INPROCEEDINGS{8578388, 
author={W. {Zhu} and Q. {Qiu} and J. {Huang} and R. {Calderbank} and G. {Sapiro} and I. {Daubechies}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={LDMNet: Low Dimensional Manifold Regularized Neural Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={2743-2751}, 
abstract={Deep neural networks have proved very successful on archetypal tasks for which large training sets are available, but when the training data are scarce, their performance suffers from overfitting. Many existing methods of reducing overfitting are data-independent. Data-dependent regularizations are mostly motivated by the observation that data of interest lie close to a manifold, which is typically hard to parametrize explicitly. These methods usually only focus on the geometry of the input data, and do not necessarily encourage the networks to produce geometrically meaningful features. To resolve this, we propose the Low-Dimensional-Manifold-regularized neural Network (LDMNet), which incorporates a feature regularization method that focuses on the geometry of both the input data and the output features. In LDMNet, we regularize the network by encouraging the combination of the input data and the output features to sample a collection of low dimensional manifolds, which are searched efficiently without explicit parametrization. To achieve this, we directly use the manifold dimension as a regularization term in a variational functional. The resulting Euler-Lagrange equation is a Laplace-Beltrami equation over a point cloud, which is solved by the point integral method without increasing the computational complexity. In the experiments, we show that LDMNet significantly outperforms widely-used regularizers. Moreover, LDMNet can extract common features of an object imaged via different modalities, which is very useful in real-world applications such as cross-spectral face recognition.}, 
keywords={computational complexity;face recognition;feature extraction;Laplace equations;learning (artificial intelligence);neural nets;LDMNet;Low Dimensional Manifold regularized neural networks;deep neural networks;data-dependent regularizations;feature regularization method;low dimensional manifolds;computational complexity;Laplace-Beltrami equation;cross-spectral face recognition;point cloud;Manifolds;Feature extraction;Training;Geometry;Neural networks;Training data;Three-dimensional displays}, 
doi={10.1109/CVPR.2018.00290}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{8317846, 
author={R. {Varga} and A. {Costea} and H. {Florea} and I. {Giosan} and S. {Nedevschi}}, 
booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)}, 
title={Super-sensor for 360-degree environment perception: Point cloud segmentation using image features}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={This paper describes a super-sensor that enables 360-degree environment perception for automated vehicles in urban traffic scenarios. We use four fisheye cameras, four 360-degree LIDARs and a GPS/IMU sensor mounted on an automated vehicle to build a super-sensor that offers an enhanced low-level representation of the environment by harmonizing all the available sensor measurements. Individual sensors cannot provide a robust 360-degree perception due to their limitations: field of view, range, orientation, number of scanning rays, etc. The novelty of this work consists of segmenting the 3D LIDAR point cloud by associating it with the 2D image semantic segmentation. Another contribution is the sensor configuration that enables 360-degree environment perception. The following steps are involved in the process: calibration, timestamp synchronization, fisheye image unwarping, motion correction of LIDAR points, point cloud projection onto the images and semantic segmentation of images. The enhanced low-level representation will improve the high-level perception environment tasks such as object detection, classification and tracking.}, 
keywords={image classification;image motion analysis;image representation;image resolution;image segmentation;image sensors;object detection;object tracking;optical radar;360-degree environment perception;point cloud segmentation;automated vehicle;360-degree LIDARs;GPS/IMU sensor;3D LIDAR point cloud;sensor measurements;image semantic segmentation;Conferences;Intelligent transportation systems;automated driving;environment perception;fisheye images;3D LIDAR points;360-degree perception;super-sensor}, 
doi={10.1109/ITSC.2017.8317846}, 
ISSN={2153-0017}, 
month={Oct},}
@INPROCEEDINGS{8633125, 
author={X. {Ju} and I. R. {Garcia Júnior} and L. {De Freitas Silva} and P. {Mossey} and D. {Al-Rudainy} and A. {Ayoub} and A. M. {De Mattos}}, 
booktitle={2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={3D Head Shape Analysis of Suspected Zika Infected Infants}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={The babies infected from Zika before they are born are at risk for problems with brain development and microcephaly. 3D head images of 43 Zika cases and 43 controls were collected aiming to extract shape characteristics for diagnosis purposes. Principal component analysis (PCA) has been applied on the vaults and faces of the collected 3D images and the scores on the second principal components of the vaults and faces showed significant differences between the control and Zika groups. The shape variations from -2σ to 2σ illustrated the typical characteristics of microcephaly of the Zika babies. Canonical correlation analysis (CCA) showed a significant correlation in the first CCA variates of face and vault which indicated the potential of 3D facial imaging for Zika surveillance. Further head circumferences and distances from ear to ear were measured from the 3D images and preliminary results showed the adding ear to ear distances for classifying control and Zika children strengthened the abilities of tested classification models.}, 
keywords={diseases;image classification;medical image processing;paediatrics;principal component analysis;shape recognition;stereo image processing;suspected Zika infected infants;microcephaly;Zika surveillance;3D facial imaging;canonical correlation analysis;Zika babies;shape variations;principal component analysis;3D head images;brain development;shape analysis;Zika children;3D imaging;shape analysis;Zika}, 
doi={10.1109/CISP-BMEI.2018.8633125}, 
ISSN={}, 
month={Oct},}
@ARTICLE{8440881, 
author={A. {Switonski} and T. {Krzeszowski} and H. {Josinski} and B. {Kwolek} and K. {Wojciechowski}}, 
journal={IET Biometrics}, 
title={Gait recognition on the basis of markerless motion tracking and DTW transform}, 
year={2018}, 
volume={7}, 
number={5}, 
pages={415-422}, 
abstract={In this study, a framework for view-invariant gait recognition on the basis of markerless motion tracking and dynamic time warping (DTW) transform is presented. The system consists of a proposed markerless motion capture system as well as introduced classification method of mocap data. The markerless system estimates the three-dimensional locations of skeleton driven joints. Such skeleton-driven point clouds represent poses over time. The authors align point clouds in every pair of frames by calculating the minimal sum of squared distances between the corresponding joints. A point cloud distance measure with temporal context has been utilised ink-nearest neighbours algorithm to compare time instants of motion sequences. To enhance the generalisation of the recognition and to shorten the processing time, for every individual a single multidimensional time series among several multidimensional time series describing the individual's gait is established. The correct classification rate has been determined on the basis of a real dataset of human gait. It contains 230 gait cycles of 22 subjects. The tracking results on the basis of markerless motion capture are referenced to Vicon system, whereas the achieved accuracies of recognition are compared with the ones obtained by DTW that is based on rotational data.}, 
keywords={gait analysis;image classification;image motion analysis;image sequences;learning (artificial intelligence);object tracking;time series;transforms;multidimensional time series;motion sequences;k-nearest neighbours algorithm;skeleton-driven point clouds;classification method;markerless motion capture system;dynamic time warping transform;view-invariant gait recognition;DTW transform;markerless motion tracking;gait recognition}, 
doi={10.1049/iet-bmt.2017.0134}, 
ISSN={2047-4938}, 
month={},}
@INPROCEEDINGS{7351660, 
author={R. J. {Arteaga} and S. J. {Ruuth}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={Laplace-Beltrami spectra for shape comparison of surfaces in 3D using the closest point method}, 
year={2015}, 
volume={}, 
number={}, 
pages={4511-4515}, 
abstract={The need to compare separate objects arises in a wide range of applications. In one approach for comparing objects, `Shape-DNA' is constructed to give a numerical fingerprint representing an individual object. Shape-DNA is a cropped set of eigenvalues of the Laplace-Beltrami operator for the surface of the object. In this paper, we compute the Shape-DNA of surfaces using the closest point method. Our approach may be applied to a variety of surface representations including triangulations, point clouds and certain analytical shapes. A 2D multidimensional scaling plot illustrates that similar objects form groups based on the Shape-DNAs. Our method has the benefit that it may be applied to surfaces defined by dense point clouds without requiring the construction of point connectivity.}, 
keywords={DNA;eigenvalues and eigenfunctions;fingerprint identification;image representation;Laplace transforms;shape recognition;Laplace-Beltrami spectra;shape comparison;closest point method;shape-DNA;numerical fingerprint representation;eigenvalues;surface representations;point clouds;2D multidimensional scaling plot;Shape;Three-dimensional displays;Eigenvalues and eigenfunctions;Standards;Interpolation;Rabbits;shape comparison;Laplace-Beltrami spectra;closest point method;point cloud}, 
doi={10.1109/ICIP.2015.7351660}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7457888, 
author={H. {Fan} and Y. {Zhou} and H. {Liang} and J. {Wang} and P. {Krebs} and D. {Lin} and J. {Su} and K. {Li} and H. {Chen} and X. {Wang} and J. {Zhou}}, 
booktitle={2015 Visual Communications and Image Processing (VCIP)}, 
title={Glasses-free 3D display with glasses-assisted quality: Key innovations for smart directional backlight autostereoscopy}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={A glasses-free 3D display with glasses-assisted quality is presented. Self-adaptive algorithm is employed to optimize system parameters, which is applied to design the micro structure of lens array and free form surface backlight units. Moiré contour map based on contrast sensitivity function is simulated and is manipulated by ameliorating the period ratio and the tilt angle of the superimposed periodical optical components. Directional transmissions of multi-users 3D images are realized with a finer dynamic synchronized backlight control and a face recognition technology. Comfortable viewings are demonstrated for two viewers, with full high definition for a single viewing channel. Minimum crosstalk as low as 2.3% is demonstrated over a large viewing volume.}, 
keywords={crosstalk;face recognition;stereo image processing;three-dimensional displays;crosstalk;single-viewing channel;full-high-definition;face recognition technology;dynamic synchronized backlight control;multiuser 3D image directional transmission;superimposed periodical optical components;period ratio;tilt angle;contrast sensitivity function;Moire contour map;free-form surface backlight units;lens array;self-adaptive algorithm;smart directional backlight autostereoscopy;glass-assisted quality;glass-free 3D display;Three-dimensional displays;Crosstalk;Lenses;Light emitting diodes;Bars;Synchronization;3D display;stereo vision;autostereoscopic;free-form surface design;crosstalk}, 
doi={10.1109/VCIP.2015.7457888}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7332643, 
author={C. {Stahlschmidt} and A. {Gavriilidis} and A. {Kummert}}, 
booktitle={2015 IEEE 9th International Workshop on Multidimensional (nD) Systems (nDS)}, 
title={Classification of ascending steps and stairs using Time-of-Flight sensor data}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper proposes a method to analyse human-made environments in order to verify the presence of ascending steps or stairs. Our system is intended to assist visually impaired people by providing acoustic information about the scene in front of a low-resolution Time-of-Flight (ToF) camera that is fixed to a mobile vehicle (rollator). Detailed instructions to the user about potentially hazardous situations are provided. This paper in particular deals with a fast approach on classification of ascending steps in 3D point clouds. This method is part of a system that aims on enhancing visually impaired persons understand the environment and help prevent collisions.}, 
keywords={computer graphics;image classification;image resolution;ascending step classification;time-of-flight sensor data;human-made environment;visually impaired people;acoustic information;low-resolution time-of-flight camera;ToF camera;mobile vehicle;rollator;hazardous situation;3D point cloud;visually impaired person;Three-dimensional displays;Cameras;Data mining;Algorithm design and analysis;Gray-scale;Acoustics;Mobile communication}, 
doi={10.1109/NDS.2015.7332643}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7523645, 
author={ and and and }, 
booktitle={2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)}, 
title={Development of autonomous laser toning system based on vision recognition and robot manipulator}, 
year={2016}, 
volume={}, 
number={}, 
pages={317-322}, 
abstract={In this paper, the design, implementation, and operation method of the autonomous laser toning system are proposed, which is called as MELON (Manipulator for Effective Laser tONing). The system can recognize the accurate treatment points from the 3D point cloud data obtained with the camera, and it is possible to emit the laser at the desired position and orientation repeatedly, precisely, and accurately using intuitive differential inverse kinematics of the robot manipulator. The feasibility test of the MELON is conducted by using a plaster cast of a woman's head, and then, we find that the manipulator has a workspace to cover the entire face of the human inductively and distribution of the laser emission is homogeneous on the face. Therefore, we find the possibility of the autonomous laser toning using MELON.}, 
keywords={computer graphics;manipulators;robot kinematics;robot vision;autonomous laser toning system;vision recognition;robot manipulator;MELON;manipulator for effective laser toning;3D point cloud;differential inverse kinematics;plaster cast;laser emission;Manipulators;Surface emitting lasers;Face;Cameras;Medical services}, 
doi={10.1109/BIOROB.2016.7523645}, 
ISSN={2155-1782}, 
month={June},}
@INPROCEEDINGS{7755218, 
author={A. M. {Intwala} and A. {Magikar}}, 
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, 
title={A review on process of 3D Model Reconstruction}, 
year={2016}, 
volume={}, 
number={}, 
pages={2851-2855}, 
abstract={3D Model Reconstruction is of the most important part in the field of Reverse Engineering. It has now become feasible to use this method to create a 3D model of existing product, component for CAD/CAM applications. Various phases of reverse engineering and 3D reconstruction are reviewed in details along the methodology involved within these stages. Data acquisition is the most crucial stage of 3d model reconstruction. Non touch and touch techniques of data acquisition are studied giving comparative advantages and disadvantages over each other. Face based and Edge based techniques are reviewed for segmentation of acquired data. The collected point data is obtained in Stereo Lithography (STL) format which is most popular in field of modern computer science. This collected data can be used to construct surfaces using modern CAD systems like CATIA, Pro/E etc. This paper also reviews some of the researches in the field of Reverse Engineering, The methods they implemented and their outcome and various technologies used with their limitations. It reviews latest advancement in the field of 3d Model Reconstruction.}, 
keywords={CAD/CAM;data acquisition;edge detection;face recognition;image reconstruction;reverse engineering;stereo image processing;stereolithography;3D model reconstruction process;reverse engineering;CAD-CAM applications;nontouch technique;touch technique;data acquisition;edge-based technique;face-based technique;stereo lithography;STL format;image processing;Three-dimensional displays;Solid modeling;Image reconstruction;Data models;Computational modeling;Reverse engineering;Measurement by laser beam;3D model reconstruction;Computer graphics;Image processing;Reverse Engineering;3d Point Cloud Data;CMM;3D Printer;Feature Extraction}, 
doi={10.1109/ICEEOT.2016.7755218}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8202220, 
author={D. L. {Rizzini}}, 
booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Place recognition of 3D landmarks based on geometric relations}, 
year={2017}, 
volume={}, 
number={}, 
pages={648-654}, 
abstract={Place recognition based on landmarks or features is an important problem occurring in localization, mapping, computer vision and point cloud processing. In this paper, we present GLAROT-3D, a translation and rotation invariant 3D signature based on geometric relations. The proposed method encodes into a histogram the pairwise relative positions of keypoint features extracted from 3D sensor data. Since it relies only on geometric properties and not on specific feature descriptors, it does not require any prior training or vocabulary construction and enables lightweight comparisons between landmark maps. The similarity of two point maps is computed as the distance between the corresponding rotated histograms to achieve rotation invariance. Histogram rotation is enabled by efficient orientation histogram based on sphere cubical projection. The performance of GLAROT has been assessed through experiments with standard benchmark datasets.}, 
keywords={feature extraction;image recognition;place recognition;geometric relations;keypoint features;3D sensor data;geometric properties;landmark maps;point maps;rotation invariance;histogram rotation;translation invariant 3D signature;pairwise relative positions;rotated histograms;orientation histogram;sphere cubical projection;Histograms;Three-dimensional displays;Feature extraction;Indexes;Face;Robot sensing systems;Computer vision}, 
doi={10.1109/IROS.2017.8202220}, 
ISSN={2153-0866}, 
month={Sep.},}
@INPROCEEDINGS{7301378, 
author={P. {Polewski} and W. {Yao} and M. {Heurich} and P. {Krzystek} and U. {Stilla}}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
title={Active learning approach to detecting standing dead trees from ALS point clouds combined with aerial infrared imagery}, 
year={2015}, 
volume={}, 
number={}, 
pages={10-18}, 
abstract={Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89% with less than 100 training examples, which corresponds to 10% of the pre-selected data pool.}, 
keywords={environmental factors;feature extraction;forestry;image classification;image colour analysis;infrared imaging;learning (artificial intelligence);matrix algebra;object detection;vegetation;active learning approach;standing dead tree detection;ALS point cloud;forest process;forest science;environmental science;aerial color infrared imagery;feature extraction;greedy approximation;kernelized feature matrix;error reduction;semisupervised classifier;Bavarian Forest National Park;Vegetation;Three-dimensional displays;Entropy;Training;Image segmentation;Logistics;Feature extraction}, 
doi={10.1109/CVPRW.2015.7301378}, 
ISSN={2160-7516}, 
month={June},}
@INPROCEEDINGS{7410401, 
author={A. {Sironi} and V. {Lepetit} and P. {Fua}}, 
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={Projection onto the Manifold of Elongated Structures for Accurate Extraction}, 
year={2015}, 
volume={}, 
number={}, 
pages={316-324}, 
abstract={Detection of elongated structures in 2D images and 3D image stacks is a critical prerequisite in many applications and Machine Learning-based approaches have recently been shown to deliver superior performance. However, these methods essentially classify individual locations and do not explicitly model the strong relationship that exists between neighboring ones. As a result, isolated erroneous responses, discontinuities, and topological errors are present in the resulting score maps. We solve this problem by projecting patches of the score map to their nearest neighbors in a set of ground truth training patches. Our algorithm induces global spatial consistency on the classifier score map and returns results that are provably geometrically consistent. We apply our algorithm to challenging datasets in four different domains and show that it compares favorably to state-of-the-art methods.}, 
keywords={feature extraction;image classification;learning (artificial intelligence);object detection;structural engineering computing;classifier score map;global spatial consistency;ground truth training patch;patch projection;score map;topological error;discontinuities;isolated erroneous response;individual location classification;machine learning-based approach;3D image stack;2D image;elongated structure detection;extraction;elongated structure manifold;Three-dimensional displays;Training;Manifolds;Biomembranes;Image segmentation;Feature extraction;Transforms}, 
doi={10.1109/ICCV.2015.44}, 
ISSN={2380-7504}, 
month={Dec},}
@INPROCEEDINGS{8123040, 
author={N. {Mohsin} and S. {Payandeh}}, 
booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={Localization and identification of body extremities based on data from multiple depth sensors}, 
year={2017}, 
volume={}, 
number={}, 
pages={2736-2741}, 
abstract={This paper explores the novel use of multiple depth sensors to overcome occlusions and improve localization and tracking of body extremities. The usage of data from only depth sensors not only overcomes visual challenges associated with RGB sensors under low illumination, but also protects the identity of surveyed person with high confidentiality. For integrating depth information from multiple sources, the paper presents first an overview of a novel calibration method for multiple depth sensors. In case of occlusion of any fiducial point in the primary sensor's depth image, co-ordinates of the point can be obtained from the frame of other sensors using the calibration parameters. To localize salient body parts such as hands, head and feet, a surface triangular mesh is applied on generated 3D point cloud from the primary sensor. The geodesic extrema from the mesh coincide with body extremities. The body extremities can be identified based on those relative geodesic distances between the extremities. Once the body parts are labelled, a portion of body can be targeted and evaluated for specific gait analysis and visualization. For the performance evaluation, our calibration method has fared well in comparison to other available techniques. Also, our proposed localization of salient body parts is able to successfully tag the specific body part i.e. the head region.}, 
keywords={calibration;data visualisation;feature extraction;gait analysis;image motion analysis;image sensors;mesh generation;object detection;sensor fusion;RGB sensors;salient body parts;depth information;body extremities localization;body extremities identification;multiple depth sensor data;body extremities tracking;illumination;identity confidentiality;specific body part tagging;fiducial point occlusion;primary sensor depth image;3D point cloud;mesh geodesic extrema;relative geodesic distances;gait analysis;visualization;Sensors;Three-dimensional displays;Calibration;Extremities;Image sensors;Image segmentation;Cameras;multiple depth sensors calibration;body extremities localization;Kinect II;geodesic distances}, 
doi={10.1109/SMC.2017.8123040}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7410516, 
author={S. {Katz} and A. {Tal}}, 
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={On the Visibility of Point Clouds}, 
year={2015}, 
volume={}, 
number={}, 
pages={1350-1358}, 
abstract={Is it possible to determine the visible subset of points directly from a given point cloud? Interestingly, it was shown that this is indeed the case - despite the fact that points cannot occlude each other, this task can be performed without surface reconstruction or normal estimation. The operator is very simple - it first transforms the points to a new domain and then constructs the convex hull in that domain. Points that lie on the convex hull of the transformed set of points are the images of the visible points. This operator found numerous applications in computer vision, including face reconstruction, keypoint detection, finding the best viewpoints, reduction of points, and many more. The current paper addresses a fundamental question: What properties should a transformation function satisfy, in order to be utilized in this operator? We show that three such properties are sufficient: the sign of the function, monotonicity, and a condition regarding the function's parameter. The correctness of an algorithm that satisfies these three properties is proved. Finally, we show an interesting application of the operator - assignment of visibility-confidence score. This feature is missing from previous approaches, where a binary yes/no visibility is determined. This score can be utilized in various applications, we illustrate its use in view-dependent curvature estimation.}, 
keywords={computer vision;convex programming;estimation theory;face recognition;image reconstruction;object detection;view-dependent curvature estimation;visibility-confidence score;transformation function;keypoint detection;computer vision;convex hull;surface reconstruction;point clouds;Kernel;Three-dimensional displays;Image reconstruction;Face;Surface reconstruction;Estimation;Computer vision}, 
doi={10.1109/ICCV.2015.159}, 
ISSN={2380-7504}, 
month={Dec},}
@INPROCEEDINGS{7780547, 
author={T. {Hackel} and J. D. {Wegner} and K. {Schindler}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Contour Detection in Unstructured 3D Point Clouds}, 
year={2016}, 
volume={}, 
number={}, 
pages={1610-1618}, 
abstract={We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds &gt; 107 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.}, 
keywords={edge detection;feature extraction;graph theory;image classification;image coding;image representation;learning (artificial intelligence);object detection;contour detection;unstructured 3D point clouds;surface orientation;large-scale outdoor point clouds;high-quality surface;solid models;unstructured-inhomogeneous point clouds;contour extraction;two-stage discriminative learning problem;contour score;binary classifier;feature extraction;contour scores;higher-order MRF;graph cliques;Canny-style edge detection;image representation;Three-dimensional displays;Feature extraction;Solid modeling;Image edge detection;Surface reconstruction;Surface topography;Solids}, 
doi={10.1109/CVPR.2016.178}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7298920, 
author={ and R. {Ji} and }, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery}, 
year={2015}, 
volume={}, 
number={}, 
pages={3013-3021}, 
abstract={Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.}, 
keywords={Boltzmann machines;feature extraction;image colour analysis;image matching;learning (artificial intelligence);object detection;3D object detection;bimodal deep Boltzmann machines;RGBD imagery;3D scenes;point clouds;3D training data labeling;cross-modality cues;cross-modality deep learning framework;3D scene object detection;cross-modality feature learning;3D detection window;3D point cloud;exemplar shape matching;3D CAD models;2D positive samples;R-CNNs;raw feature extraction;RGB domains;depth domains;RMRC dataset;bimodal based deep feature learning framework;Three-dimensional displays;Solid modeling;Training;Frequency modulation;Joints;Detectors;Feature extraction}, 
doi={10.1109/CVPR.2015.7298920}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{7331662, 
author={T. v. {Landesberger} and D. {Basgier} and M. {Becker}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Comparative Local Quality Assessment of 3D Medical Image Segmentations with Focus on Statistical Shape Model-Based Algorithms}, 
year={2016}, 
volume={22}, 
number={12}, 
pages={2537-2549}, 
abstract={The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.}, 
keywords={data analysis;data visualisation;ear;image segmentation;liver;medical image processing;neurophysiology;pattern clustering;statistical analysis;comparative local quality assessment;3D medical image segmentation;statistical shape model-based algorithm;automatic 3D medical segmentation algorithm;data analysis;data visualization;segmentation quality characteristics;hierarchical clustering algorithm;connectivity criterion;landmark correspondence;liver;cochlea;facial nerve;organ region identification;outlier instance detection;Image segmentation;Three-dimensional displays;Biomedical monitoring;Visual analytics;Medical diagnostic imaging;Systematics;Statistical analysis;Clustering;Visual analytics;3D medical image segmentation quality;comparison;clustering;statistical shape models}, 
doi={10.1109/TVCG.2015.2501813}, 
ISSN={1077-2626}, 
month={Dec},}
@INPROCEEDINGS{7328083, 
author={M. {Sano} and K. {Matsumoto} and B. H. {Thomas} and H. {Saito}}, 
booktitle={2015 IEEE International Symposium on Mixed and Augmented Reality}, 
title={[POSTER] Rubix: Dynamic Spatial Augmented Reality by Extraction of Plane Regions with a RGB-D Camera}, 
year={2015}, 
volume={}, 
number={}, 
pages={148-151}, 
abstract={Dynamic spatial augmented reality requires accurate real-time 3D pose information of the physical objects that are to be projected onto. Previous depth-based methods for tracking objects required strong features to enable recognition; making it difficult to estimate an accurate 6DOF pose for physical objects with a small set of recognizable features (such as a non-textured cube). We propose a more accurate method with fewer limitations for the pose estimation of a tangible object that has known planar faces and using depth data from an RGB-D camera only. In this paper, the physical object's shape is limited to cubes of different sizes. We apply this new tracking method to achieve dynamic projections onto these cubes. In our method, 3D points from an RGB-D camera are divided into a cluster of planar regions, and the point cloud inside each face of the object is fitted to an already-known geometric model of a cube. With the 6DOF pose of the physical object, SAR generated imagery is then projected correctly onto the physical object. The 6DOF tracking is designed to support tangible interactions with the physical object. We implemented example interactive applications with one or multiple cubes to show the capability of our method.}, 
keywords={augmented reality;cameras;feature extraction;object tracking;pose estimation;Rubix;plane region extraction;dynamic spatial augmented reality;tangible object pose estimation;RGB-D camera;tracking method;point cloud;SAR generated imagery;Three-dimensional displays;Cameras;Iterative closest point algorithm;Augmented reality;Real-time systems;Target tracking;Spatial Augmented Reality;Six Degree of Freedom Tracking;RGB-D Camera}, 
doi={10.1109/ISMAR.2015.43}, 
ISSN={}, 
month={Sep.},}
