@inproceedings{Nozawa:2015:FRS:2787626.2792634,
 author = {Nozawa, Naoki and Kuwahara, Daiki and Morishima, Shigeo},
 title = {3D Face Reconstruction from a Single Non-frontal Face Image},
 booktitle = {ACM SIGGRAPH 2015 Posters},
 series = {SIGGRAPH '15},
 year = {2015},
 isbn = {978-1-4503-3632-1},
 location = {Los Angeles, California},
 pages = {57:1--57:1},
 articleno = {57},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/2787626.2792634},
 doi = {10.1145/2787626.2792634},
 acmid = {2792634},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Demisse:2018:DFE:3190503.3176649,
 author = {Demisse, Girum G. and Aouada, Djamila and Ottersten, Bj\"{o}rn},
 title = {Deformation-Based 3D Facial Expression Representation},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 issue_date = {April 2018},
 volume = {14},
 number = {1s},
 month = mar,
 year = {2018},
 issn = {1551-6857},
 pages = {17:1--17:22},
 articleno = {17},
 numpages = {22},
 url = {http://doi.acm.org/10.1145/3176649},
 doi = {10.1145/3176649},
 acmid = {3176649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face deformation, 3D facial expression representation, expression modelling},
}

@article{Pala:2019:RFM:3309717.3287309,
 author = {Pala, Pietro and Berretti, Stefano},
 title = {Reconstructing 3D Face Models by Incremental Aggregation and Refinement of Depth Frames},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 issue_date = {February 2019},
 volume = {15},
 number = {1},
 month = jan,
 year = {2019},
 issn = {1551-6857},
 pages = {23:1--23:24},
 articleno = {23},
 numpages = {24},
 url = {http://doi.acm.org/10.1145/3287309},
 doi = {10.1145/3287309},
 acmid = {3287309},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face recognition, 3D reconstruction, Depth data, anisotropic error},
}

@inproceedings{Chen:2018:FRB:3207677.3278100,
 author = {Chen, Xing and Lu, Yinan and Fang, Ran},
 title = {3D Face Recognition Based on Empirical Mode Decomposition and Sparse Representation},
 booktitle = {Proceedings of the 2Nd International Conference on Computer Science and Application Engineering},
 series = {CSAE '18},
 year = {2018},
 isbn = {978-1-4503-6512-3},
 location = {Hohhot, China},
 pages = {78:1--78:5},
 articleno = {78},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3207677.3278100},
 doi = {10.1145/3207677.3278100},
 acmid = {3278100},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EMD, Face Recognition, SRC, meshSIFT},
}

@article{Berretti:2018:RAR:3190503.3182179,
 author = {Berretti, Stefano and Daoudi, Mohamed and Turaga, Pavan and Basu, Anup},
 title = {Representation, Analysis, and Recognition of 3D Humans: A Survey},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 issue_date = {April 2018},
 volume = {14},
 number = {1s},
 month = mar,
 year = {2018},
 issn = {1551-6857},
 pages = {16:1--16:36},
 articleno = {16},
 numpages = {36},
 url = {http://doi.acm.org/10.1145/3182179},
 doi = {10.1145/3182179},
 acmid = {3182179},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face and body analysis and retrieval, 3D face and body representation, 3D humans, 3D shape representation},
}

@inproceedings{Guo:2018:SVF:3240876.3240913,
 author = {Guo, Xingyan and Jin, Yi and Li, Yidong and Xing, Junliang and Lang, Congyan},
 title = {Stabilizing Video Facial Landmark Detection and Tracking via Global and Local Filtering},
 booktitle = {Proceedings of the 10th International Conference on Internet Multimedia Computing and Service},
 series = {ICIMCS '18},
 year = {2018},
 isbn = {978-1-4503-6520-8},
 location = {Nanjing, China},
 pages = {20:1--20:7},
 articleno = {20},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3240876.3240913},
 doi = {10.1145/3240876.3240913},
 acmid = {3240913},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face model, landmark detection, landmark smoothing},
}

@inproceedings{Abbad:2018:FRP:3177148.3180087,
 author = {Abbad, Abdelghafour and Abbad, Khalid and Tairi, Hamid},
 title = {3D Face Recognition in the Presence of Facial Expressions Based on Empirical Mode Decomposition},
 booktitle = {Proceedings of the 2Nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
 series = {MedPRAI '18},
 year = {2018},
 isbn = {978-1-4503-5290-1},
 location = {Rabat, Morocco},
 pages = {1--6},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3177148.3180087},
 doi = {10.1145/3177148.3180087},
 acmid = {3180087},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face recognition, EMD, expression, facial curves, geometric features, local features},
}

@inproceedings{Butler:2016:CFE:2851581.2892535,
 author = {Butler, Crystal and Subramanian, Lakshmi and Michalowicz, Stephanie},
 title = {Crowdsourced Facial Expression Mapping Using a 3D Avatar},
 booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
 series = {CHI EA '16},
 year = {2016},
 isbn = {978-1-4503-4082-3},
 location = {San Jose, California, USA},
 pages = {2798--2804},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/2851581.2892535},
 doi = {10.1145/2851581.2892535},
 acmid = {2892535},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D facial modeling, avatars, crowdsourcing, expression recognition, facial expressions, facs},
}

@inproceedings{Xie:2016:IFR:3028842.3028853,
 author = {Xie, Lanchi and Xu, Lei and Zhang, Ning and Guo, Jingjing and Yan, Yuwen and Li, Zhihui and Li, Zhigang and Xu, Xiaojing},
 title = {Improved Face Recognition Result Reranking Based on Shape Contexts},
 booktitle = {Proceedings of the 2016 International Conference on Intelligent Information Processing},
 series = {ICIIP '16},
 year = {2016},
 isbn = {978-1-4503-4799-0},
 location = {Wuhan, China},
 pages = {11:1--11:6},
 articleno = {11},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3028842.3028853},
 doi = {10.1145/3028842.3028853},
 acmid = {3028853},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {face recognition, reranking, shape contexts, shape matching, similarity calculation},
}

@inproceedings{Fischer:2015:AFI:2756601.2756619,
 author = {Fischer, Robert and Vielhauer, Claus},
 title = {Automated Firearm Identification: On Using a Novel Multiple-Slice-Shape (MSS) Approach for Comparison and Matching of Firing Pin Impression Topography},
 booktitle = {Proceedings of the 3rd ACM Workshop on Information Hiding and Multimedia Security},
 series = {IH\&\#38;MMSec '15},
 year = {2015},
 isbn = {978-1-4503-3587-4},
 location = {Portland, Oregon, USA},
 pages = {161--171},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/2756601.2756619},
 doi = {10.1145/2756601.2756619},
 acmid = {2756619},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {digital crime scene analysis, digitized forensics, firearm identification, firing pin shape matching, multiple slice shape, new forensic features, pattern classification, topography processing},
}

@inproceedings{Desai:2018:CSP:3204949.3204958,
 author = {Desai, Kevin and Prabhakaran, Balakrishnan and Raghuraman, Suraj},
 title = {Combining Skeletal Poses for 3D Human Model Generation Using Multiple Kinects},
 booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
 series = {MMSys '18},
 year = {2018},
 isbn = {978-1-4503-5192-8},
 location = {Amsterdam, Netherlands},
 pages = {40--51},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3204949.3204958},
 doi = {10.1145/3204949.3204958},
 acmid = {3204958},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D model, combined skeleton, interactive 3D tele-immersion, point cloud combination},
}

@article{Ramachandra:2017:PAD:3058791.3038924,
 author = {Ramachandra, Raghavendra and Busch, Christoph},
 title = {Presentation Attack Detection Methods for Face Recognition Systems: A Comprehensive Survey},
 journal = {ACM Comput. Surv.},
 issue_date = {April 2017},
 volume = {50},
 number = {1},
 month = mar,
 year = {2017},
 issn = {0360-0300},
 pages = {8:1--8:37},
 articleno = {8},
 numpages = {37},
 url = {http://doi.acm.org/10.1145/3038924},
 doi = {10.1145/3038924},
 acmid = {3038924},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Biometrics, antispoofing, attacks, countermeasure, face recognition, security},
}

@inproceedings{Liu:2018:MBE:3242969.3264989,
 author = {Liu, Chuanhe and Tang, Tianhao and Lv, Kui and Wang, Minghao},
 title = {Multi-Feature Based Emotion Recognition for Video Clips},
 booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
 series = {ICMI '18},
 year = {2018},
 isbn = {978-1-4503-5692-3},
 location = {Boulder, CO, USA},
 pages = {630--634},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3242969.3264989},
 doi = {10.1145/3242969.3264989},
 acmid = {3264989},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3d face landmark, deep learning, densenet, emotion recognition, emotiw 2018, inception net, lstm, soundnet},
}

@inproceedings{Nozawa:2016:FGR:2945078.2945102,
 author = {Nozawa, Tsukasa and Kato, Takuya and Savkin, Pavel A. and Nozawa, Naoki and Morishima, Shigeo},
 title = {3D Facial Geometry Reconstruction Using Patch Database},
 booktitle = {ACM SIGGRAPH 2016 Posters},
 series = {SIGGRAPH '16},
 year = {2016},
 isbn = {978-1-4503-4371-8},
 location = {Anaheim, California},
 pages = {24:1--24:2},
 articleno = {24},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2945078.2945102},
 doi = {10.1145/2945078.2945102},
 acmid = {2945102},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D reconstruction, shape from X, texture synthesis},
}

@article{Yuan:2018:GIH:3205271.3205279,
 author = {Yuan, Lin and Chen, Fanglin and Zeng, Ling-Li and Wang, Lubin and Hu, Dewen},
 title = {Gender Identification of Human Brain Image with A Novel 3D Descriptor},
 journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
 issue_date = {March 2018},
 volume = {15},
 number = {2},
 month = mar,
 year = {2018},
 issn = {1545-5963},
 pages = {551--561},
 numpages = {11},
 url = {https://doi.org/10.1109/TCBB.2015.2448081},
 doi = {10.1109/TCBB.2015.2448081},
 acmid = {3205279},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
}

@inproceedings{Yi:2017:RFR:3110224.3110230,
 author = {Yi, Zhang and Yuan, Zhou},
 title = {Research on 3D Face Recognition Based on Pose and Illumination Invariant},
 booktitle = {Proceedings of the 2017 International Conference on Computer Graphics and Digital Image Processing},
 series = {CGDIP '17},
 year = {2017},
 isbn = {978-1-4503-5236-9},
 location = {Prague, Czech Republic},
 pages = {6:1--6:5},
 articleno = {6},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3110224.3110230},
 doi = {10.1145/3110224.3110230},
 acmid = {3110230},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face recognition, illumination, pose, ratio image, spherical harmonic},
}

@article{Jin:2018:LFE:3295616.3200572,
 author = {Jin, Hai and Lian, Yuanfeng and Hua, Jing},
 title = {Learning Facial Expressions with 3D Mesh Convolutional Neural Network},
 journal = {ACM Trans. Intell. Syst. Technol.},
 issue_date = {January 2019},
 volume = {10},
 number = {1},
 month = nov,
 year = {2018},
 issn = {2157-6904},
 pages = {7:1--7:22},
 articleno = {7},
 numpages = {22},
 url = {http://doi.acm.org/10.1145/3200572},
 doi = {10.1145/3200572},
 acmid = {3200572},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D mesh convolutional neural networks, Facial expression analysis, visual analysis},
}

@inproceedings{CardiaNeto:2015:HFF:2695664.2695807,
 author = {Cardia Neto, Jo\~{a}o Baptista and Marana, Aparecido Nilceu},
 title = {3DLBP and HAOG Fusion for Face Recognition Utilizing Kinect As a 3D Scanner},
 booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
 series = {SAC '15},
 year = {2015},
 isbn = {978-1-4503-3196-8},
 location = {Salamanca, Spain},
 pages = {66--73},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2695664.2695807},
 doi = {10.1145/2695664.2695807},
 acmid = {2695807},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face recognition, 3DLBP, HAOG, Kinect, biometrics, computer vision, image processing, pattern recognition},
}

@inproceedings{Kopinski:2016:DLA:2994374.2994392,
 author = {Kopinski, Thomas and Sachara, Fabian and Handmann, Uwe},
 title = {A Deep Learning Approach to Mid-air Gesture Interaction for Mobile Devices from Time-of-Flight Data},
 booktitle = {Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
 series = {MOBIQUITOUS 2016},
 year = {2016},
 isbn = {978-1-4503-4750-1},
 location = {Hiroshima, Japan},
 pages = {1--9},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/2994374.2994392},
 doi = {10.1145/2994374.2994392},
 acmid = {2994392},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Deep Learning, Object Recognition, mid-air gestures},
}

@inproceedings{Amir:2016:DEV:3001773.3001804,
 author = {Amir, Mohd Hezri and Quek, Albert and Sulaiman, Nur Rasyid Bin and See, John},
 title = {DUKE: Enhancing Virtual Reality Based FPS Game with Full-body Interactions},
 booktitle = {Proceedings of the 13th International Conference on Advances in Computer Entertainment Technology},
 series = {ACE '16},
 year = {2016},
 isbn = {978-1-4503-4773-0},
 location = {Osaka, Japan},
 pages = {35:1--35:6},
 articleno = {35},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3001773.3001804},
 doi = {10.1145/3001773.3001804},
 acmid = {3001804},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {First-Person-Shooter, Gestures Recognition, Immersive Gameplay, Virtual Reality},
}

@inproceedings{Asteriadis:2015:SHA:2769493.2769569,
 author = {Asteriadis, Stylianos and Daras, Petros},
 title = {Skeleton-based Human Action Recognition Using Basis Vectors},
 booktitle = {Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
 series = {PETRA '15},
 year = {2015},
 isbn = {978-1-4503-3452-5},
 location = {Corfu, Greece},
 pages = {49:1--49:4},
 articleno = {49},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2769493.2769569},
 doi = {10.1145/2769493.2769569},
 acmid = {2769569},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {action recognition, gesture recognition, kinect data analysis},
}

@inproceedings{Cohen:2018:BSB:3177148.3180081,
 author = {Cohen, Fernand S. and Li, Chenxi},
 title = {3D Building Synthesis Based on Images and Affine Invariant Salient Features},
 booktitle = {Proceedings of the 2Nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
 series = {MedPRAI '18},
 year = {2018},
 isbn = {978-1-4503-5290-1},
 location = {Rabat, Morocco},
 pages = {44--51},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/3177148.3180081},
 doi = {10.1145/3177148.3180081},
 acmid = {3180081},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D building reconstruction, GPS, invariants, localization, salient features},
}

@inproceedings{Korn:2017:DAE:3064663.3064755,
 author = {Korn, Oliver and Stamm, Lukas and Moeckl, Gerd},
 title = {Designing Authentic Emotions for Non-Human Characters: A Study Evaluating Virtual Affective Behavior},
 booktitle = {Proceedings of the 2017 Conference on Designing Interactive Systems},
 series = {DIS '17},
 year = {2017},
 isbn = {978-1-4503-4922-2},
 location = {Edinburgh, United Kingdom},
 pages = {477--487},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3064663.3064755},
 doi = {10.1145/3064663.3064755},
 acmid = {3064755},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {affective, animation, body cues, design, emotion, emotion recognition, facial expressions, games, perception},
}

@inproceedings{Balint-Benczedi:2015:KAR:2772879.2773515,
 author = {B\'{a}lint-Bencz{\'e}di, Ferenc and Wiedemeyer, Thiemo and Tenorth, Moritz and Be\ssler, Daniel and Beetz, Michael},
 title = {A Knowledge-Based Approach to Robotic Perception Using Unstructured Information Management},
 booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
 series = {AAMAS '15},
 year = {2015},
 isbn = {978-1-4503-3413-6},
 location = {Istanbul, Turkey},
 pages = {1941--1942},
 numpages = {2},
 url = {http://dl.acm.org/citation.cfm?id=2772879.2773515},
 acmid = {2773515},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 keywords = {knowledge based reasoning, personal robotics, robot perception},
}

@inproceedings{Fang:2018:RCB:3197768.3201576,
 author = {Fang, Qinyuan and Kyrarini, Maria and Ristic-Durrant, Danijela and Gr\"{a}ser, Axel},
 title = {RGB-D Camera Based 3D Human Mouth Detection and Tracking Towards Robotic Feeding Assistance},
 booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
 series = {PETRA '18},
 year = {2018},
 isbn = {978-1-4503-6390-7},
 location = {Corfu, Greece},
 pages = {391--396},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3197768.3201576},
 doi = {10.1145/3197768.3201576},
 acmid = {3201576},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D Point Cloud, Assistive Robotics, Mouth Detection, RGB-D Camera, Robot Control},
}

@inproceedings{Conly:2015:IRS:2769493.2769534,
 author = {Conly, Christopher and Zhang, Zhong and Athitsos, Vassilis},
 title = {An Integrated RGB-D System for Looking Up the Meaning of Signs},
 booktitle = {Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
 series = {PETRA '15},
 year = {2015},
 isbn = {978-1-4503-3452-5},
 location = {Corfu, Greece},
 pages = {24:1--24:8},
 articleno = {24},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2769493.2769534},
 doi = {10.1145/2769493.2769534},
 acmid = {2769534},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Kinect, gesture recognition, hand location, tracking},
}

@inproceedings{Baig:2018:MDL:3240508.3241394,
 author = {Baig, Mohammed Habibullah and Varghese, Jibin Rajan and Wang, Zhangyang},
 title = {MusicMapp: A Deep Learning Based Solution for Music Exploration and Visual Interaction},
 booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
 series = {MM '18},
 year = {2018},
 isbn = {978-1-4503-5665-7},
 location = {Seoul, Republic of Korea},
 pages = {1253--1255},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/3240508.3241394},
 doi = {10.1145/3240508.3241394},
 acmid = {3241394},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustering, deep learning, music classification, visualization},
}

@inproceedings{Pollok:2018:NMD:3301506.3301542,
 author = {Pollok, Thomas},
 title = {A New Multi-Camera Dataset with Surveillance, Mobile and Stereo Cameras for Tracking, Situation Analysis and Crime Scene Investigation Applications},
 booktitle = {Proceedings of the 2018 the 2Nd International Conference on Video and Image Processing},
 series = {ICVIP 2018},
 year = {2018},
 isbn = {978-1-4503-6613-7},
 location = {Hong Kong, Hong Kong},
 pages = {171--175},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3301506.3301542},
 doi = {10.1145/3301506.3301542},
 acmid = {3301542},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Crime Scene Investigation, Dataset, Mobile Camera, Multi-Camera Calibration, Stereo, Surveillance Camera},
}

@inproceedings{Kheffache:2015:MNG:2820926.2820939,
 author = {Kheffache, Aghiles and Pantaleoni, Marco and Zhou, Bo and Durante, Paolo Berto},
 title = {Multiverse: A Next Generation Data Storage for Alembic},
 booktitle = {SIGGRAPH Asia 2015 Posters},
 series = {SA '15},
 year = {2015},
 isbn = {978-1-4503-3926-1},
 location = {Kobe, Japan},
 pages = {15:1--15:1},
 articleno = {15},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/2820926.2820939},
 doi = {10.1145/2820926.2820939},
 acmid = {2820939},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Zhang:2015:RTS:2787626.2792659,
 author = {Zhang, Kang and Yu, Wuyi and Manhein, Mary and Waggenspack, Warren and Li, Xin},
 title = {Reassembling 3D Thin Shells Using Integrated Template Guidance and Fracture Region Matching},
 booktitle = {ACM SIGGRAPH 2015 Posters},
 series = {SIGGRAPH '15},
 year = {2015},
 isbn = {978-1-4503-3632-1},
 location = {Los Angeles, California},
 pages = {88:1--88:1},
 articleno = {88},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/2787626.2792659},
 doi = {10.1145/2787626.2792659},
 acmid = {2792659},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Kempfle:2018:RRE:3266157.3266208,
 author = {Kempfle, Jochen and Van Laerhoven, Kristof},
 title = {Respiration Rate Estimation with Depth Cameras: An Evaluation of Parameters},
 booktitle = {Proceedings of the 5th International Workshop on Sensor-based Activity Recognition and Interaction},
 series = {iWOAR '18},
 year = {2018},
 isbn = {978-1-4503-6487-4},
 location = {Berlin, Germany},
 pages = {4:1--4:10},
 articleno = {4},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3266157.3266208},
 doi = {10.1145/3266157.3266208},
 acmid = {3266208},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Kinect v2, ToF sensing, non-contact measurement, respiration measurement, respiratory rate},
}

@inproceedings{Zeng:2019:PDG:3324320.3324370,
 author = {Zeng, Yingjie and Nie, Lanshun},
 title = {Poster: Deep Gait Recognition via Millimeter Wave},
 booktitle = {Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks},
 series = {EWSN ?19},
 year = {2019},
 isbn = {978-0-9949886-3-8},
 location = {Beijing, China},
 pages = {254--255},
 numpages = {2},
 url = {http://dl.acm.org/citation.cfm?id=3324320.3324370},
 acmid = {3324370},
 publisher = {Junction Publishing},
 address = {USA},
}

@inproceedings{Nugroho:2017:FVI:3162957.3163020,
 author = {Nugroho, Muhammad Adi and Kusumoputro, Benyamin},
 title = {Fuzzy Vector Implementation on Manifold Embedding for Head Pose Estimation with Degraded Images Using Fuzzy Nearest Distance},
 booktitle = {Proceedings of the 3rd International Conference on Communication and Information Processing},
 series = {ICCIP '17},
 year = {2017},
 isbn = {978-1-4503-5365-6},
 location = {Tokyo, Japan},
 pages = {454--457},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/3162957.3163020},
 doi = {10.1145/3162957.3163020},
 acmid = {3163020},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fuzzy line interpolation, fuzzy manifold, fuzzy nearest distance, fuzzy vector, image noises, pose estimation},
}

@inproceedings{Mitani:2018:OEB:3206025.3210489,
 author = {Mitani, Kohji},
 title = {The Ongoing Evolution of Broadcast Technology},
 booktitle = {Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval},
 series = {ICMR '18},
 year = {2018},
 isbn = {978-1-4503-5046-4},
 location = {Yokohama, Japan},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/3206025.3210489},
 doi = {10.1145/3206025.3210489},
 acmid = {3210489},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@INPROCEEDINGS{7087053, 
author={A. A. {Pawar} and N. N. {Patil}}, 
booktitle={2015 International Conference on Pervasive Computing (ICPC)}, 
title={Recognition of 3-D faces with missing parts and line scratch removal using new technique}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={This paper presents an implementation of face recognition, which is a very important task of human face identification. Line scratch detection in images is a highly challenging situation because of various characteristics of this defect. Few characteristics are considered with the different texture and geometry of images. We propose a useful algorithm for frame-by-frame line scratch detection in face image which deals with 3D approach and a filtering of detection. The temporary filtering algorithm can be used to remove false detection due to thin vertical structures by detecting the scratches on an image. Experimental evaluation can be detecting the lines and scratches on a face image and they used to solve this difficult approach. Our method is used with missing parts in an image. Three-dimensional face recognition is an extended method of facial recognition is considered according with the geometry and texture of a face. It has been elaborated that 3D face recognition methods can provide high accuracy as well as high detection with a comparison of 2D recognition. 3D avoids such mismatch effect of 2D face recognition algorithms. Additionally, most 3D scanners achieve both a 3D mesh and the texture of a face image. This allows combining the output of pure 3D matches with the more traditional algorithms of 2D face recognition, thus producing better performance.}, 
keywords={computational geometry;face recognition;filtering theory;image matching;image texture;mesh generation;object detection;3D face recognition;missing parts;line scratch removal;human face identification;frame-by-frame line scratch detection;image texture;image geometry;detection filtering;false detection removal;3D scanners;3D mesh;pure 3D matches;Face recognition;Three-dimensional displays;Transforms;Filtering;Image recognition;Noise;Films;3D Images;Adaptive detection;Face mask;Hough transforms;ICP algorithm;Line scratches;Missing parts;RANSAC;SIFT}, 
doi={10.1109/PERVASIVE.2015.7087053}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7292772, 
author={C. {Sindhuja} and K. {Mala}}, 
booktitle={2015 International Conference on Computing and Communications Technologies (ICCCT)}, 
title={Landmark identification in 3D image for facial expression recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={338-343}, 
abstract={Facial expression recognition plays a major role in non verbal communication. Recognition by machine is still a challenging problem. To automate the recognition for human machine interaction, a system is proposed in this paper. The proposed system uses shape descriptors to identify twelve land marks which mainly contribute to the facial expression recognition. From the location and the size or boundary of the land marks by matching with Facial Landmark Model (FLM), basic expressions are identified. The experimental results show that the shape descriptors and post processing correctly identifies landmarks automatically. The architectural distortion of action units is used to identify the basic facial expressions and tested on Bosphorous data set.}, 
keywords={emotion recognition;face recognition;landmark identification;3D image;facial expression recognition;nonverbal communication;human machine interaction;shape descriptors;facial landmark model;FLM;post processing;architectural distortion;facial expression identification;Bosphorous data set;Shape;Indexes;Face recognition;Nose;Mouth;Three-dimensional displays;Feature extraction;Landmark detection;Facial Landmark Model;Shape index}, 
doi={10.1109/ICCCT2.2015.7292772}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7558813, 
author={S. {Liu} and X. {Chen} and D. {Fan} and X. {Chen} and F. {Meng} and Q. {Huang}}, 
booktitle={2016 IEEE International Conference on Mechatronics and Automation}, 
title={3D smiling facial expression recognition based on SVM}, 
year={2016}, 
volume={}, 
number={}, 
pages={1661-1666}, 
abstract={Using Kinect acquired RGB-D image to obtain a face feature parameters and three-dimensional coordinates of the characteristic parameters, and to select the characteristic parameter Facial by Candide-3 model, and feature extraction and normalization. Smile face expression data collection through Kinect, SVM collected to smiley face data classify and output the result of recognition, and the results compared with two-dimensional image of smiling face expression recognition results. Experimental results show that three-dimensional image of smiling face expression recognition accuracy than the two-dimensional image of smiling face. This research has important significance for the research and application of facial expression recognition technology.}, 
keywords={emotion recognition;face recognition;feature extraction;image classification;interactive devices;support vector machines;3D smiling facial expression recognition;SVM;Kinect;RGB-D image;face feature parameters;three-dimensional coordinates;characteristic parameters;Candide-3 model;feature extraction;normalization;smile face expression data collection;smiley face data classification;smiling face expression recognition two-dimensional image;smiling face expression recognition three-dimensional image;facial expression recognition technology;Feature extraction;Face;Face recognition;Support vector machines;Image recognition;Training;Data mining;Facial Expression Recognition;Feature Extraction;Support Vector Machine;Kinect}, 
doi={10.1109/ICMA.2016.7558813}, 
ISSN={2152-744X}, 
month={Aug},}
@INPROCEEDINGS{7867242, 
author={ and and }, 
booktitle={2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)}, 
title={Research on the 3D face recognition based on multi-class classifier with depth and point cloud data}, 
year={2016}, 
volume={}, 
number={}, 
pages={398-402}, 
abstract={Human face recognition technology usually takes advantages of two-dimensional or three-dimensional data. Rising from 1980s, three-dimensional face recognition technology soon become one of the headed topic because of its admirable resistance to interference and more information compared with two-dimensional face recognition technology. The new 3D face model standardization algorithm presented in this article provides a solution to transfer the obtained face model to standardized CANDIDE-3 face model. The article also provides a new Bayesian classification model based on multi-class classifier, which could overcome the difficulty that ono-verse-one classifier has a low recognition rate when facing more than two people. The article conduct the comparison experiment based on the provided algorithm. According to the experiment, it could raise the face recognition rate efficiently when applying the standardization algorithm and training model.}, 
keywords={Bayes methods;face recognition;image classification;3D face recognition;multiclass classifier;human face recognition;three-dimensional face recognition;standardized CANDIDE-3 face model;Bayesian classification;ono-verse-one classifier;point cloud data;Face;Data models;Face recognition;Classification algorithms;Training;Three-dimensional displays;Standards;3D face recognition;depth data;point cloud data;CANDIDE-3;face model standardization;multi-class classifier}, 
doi={10.1109/IMCEC.2016.7867242}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8571963, 
author={S. {Yamada} and H. {Lu} and J. K. {Tan} and H. {Kim} and N. {Kimura} and T. {Okawachi} and E. {Nozoe} and N. {Nakamura}}, 
booktitle={2018 18th International Conference on Control, Automation and Systems (ICCAS)}, 
title={Extraction of Median Plane from Facial 3D Point Cloud Based on Symmetry Analysis Using ICP Algorithm}, 
year={2018}, 
volume={}, 
number={}, 
pages={1347-1350}, 
abstract={Cleft lip is a kind of congenital facial morphological abnormality. In the clinical field of cleft lip, it is necessary to analyze symmetric shape. However, there is no method to analyze the cleft lip technique based on symmetrical viewpoints. On the other hand, in our previous method to find a symmetric axis using a 2D image, since the middle line is extracted only from the front view of the face moire image. There was a problem that low accuracy was obtained by slight rotation of the face and it was not possible to consider 3D information. In this paper, we propose a method to extract the median plane of the face by analyzing based on bilateral symmetry by using 3D point cloud on the face of front. By extracting the median plane, we believe that not only surgical assistance of doctor be possible but also become a clue to development of simulation software which is the end goal.}, 
keywords={biomechanics;face recognition;feature extraction;image reconstruction;medical image processing;surgery;symmetric shape;clinical field;congenital facial morphological abnormality;ICP algorithm;symmetry analysis;facial 3D point cloud;bilateral symmetry;median plane;problem that low accuracy;face moire image;middle line;symmetric axis;symmetrical viewpoints;cleft lip technique;Three-dimensional displays;Lips;Face;Surgery;Iterative closest point algorithm;Two dimensional displays;Nose;Cleft lip;ICP algorithm;3D point cloud;Point Cloud Library;Facial median plane;Symmetry analysis.}, 
doi={}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7443802, 
author={S. {Arora} and S. {Chawla}}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={An intensified approach to face recognition through average half face}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Face recognition has broad excitement in the latest trend in image processing. Face recognition refers to identify a specific individual in digital image by analyzing and comparing patterns. It has numerous benefits which attract every sector but there are some issues such as more time consumption and lesser accuracy which degrade the user services. To solve this problem we proposed a highly accurate and fast method to reduce the execution time. The proposed method uses average half face approach because overall system's accuracy is better in it rather than using the original full face image. The proposed method can be used to recognize both 2D and 3D images. It mainly includes the average half face creation, feature detection, full face recognition through average half face using distance metrics and finally checking system's accuracy along with time consumption. The proposed method is based on eye, nose and mouth detection.}, 
keywords={face recognition;feature extraction;face recognition;average half face approach;digital image processing;half face creation;feature detection;distance metrics;eye detection;nose detection;mouth detection;Face;Face recognition;Nose;Databases;Mouth;Three-dimensional displays;Image processing;Face recognition;Image processing;Accuracy;Average half face;Distance metrics}, 
doi={10.1109/INDICON.2015.7443802}, 
ISSN={2325-9418}, 
month={Dec},}
@INPROCEEDINGS{8054955, 
author={M. {Jazouli} and A. {Majda} and A. {Zarghili}}, 
booktitle={2017 Intelligent Systems and Computer Vision (ISCV)}, 
title={A $P recognizer for automatic facial emotion recognition using Kinect sensor}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Autism is a developmental disorder involving qualitative impairments in social interaction. One source of those impairments are difficulties with facial expressions of emotion. Autistic people often have difficulty to recognize or to understand other people's emotions and feelings, or expressing their own. This work proposes a method to automatically recognize seven basic emotions among autistic children in real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and Neutral. The method uses the Microsoft Kinect sensor to track and identify points of interest from the 3D face model and it is based on the $P point-cloud recognizer to identify multi-stroke emotions as point-clouds. The experimental results show that our system can achieve above 94.28% recognition rate. Our study provides a novel clinical tool to help children with autism to assisting doctors in operating rooms.}, 
keywords={emotion recognition;face recognition;feature extraction;multistroke emotions;point-clouds;recognition rate;autism;$P recognizer;automatic facial emotion recognition;developmental disorder;qualitative impairments;social interaction;autistic people;autistic children;Microsoft Kinect sensor;3D face model;$P point-cloud recognizer;Face recognition;Emotion recognition;Face;Three-dimensional displays;Autism;Support vector machines;Algorithm design and analysis;ASD;Autism;emotion;face expression;Kinect;$P Recognizer}, 
doi={10.1109/ISACV.2017.8054955}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8269662, 
author={R. {Kaur} and D. {Sharma} and A. {Verma}}, 
booktitle={2017 4th International Conference on Signal Processing, Computing and Control (ISPCC)}, 
title={An advance 2D face recognition by feature extraction (ICA) and optimize multilayer architecture}, 
year={2017}, 
volume={}, 
number={}, 
pages={122-129}, 
abstract={Facial recognition has most significant real-life requests like investigation and access control. It is associated through the issue of appropriately verifying face pictures and transmit them person in a database. In a past years face study has been emerging active topic. Most of the face detector techniques could be classified into feature based methods and image based also. Feature based techniques adds low-level analysis, feature analysis, etc. Facial recognition is a system capable of verifying / identifying a human after 3D images. By evaluating selected facial unique features from the image and face dataset. Design from transformation method given vector dimensional illustration of individual face in a prepared set of images, Principle component analysis inclines to search a dimensional sub-space whose normal vector features correspond to the maximum variance direction in the real image space. The PCA algorithm evaluates the feature extraction, data, i.e. Eigen Values and vectors of the scatter matrix. In literature survey, Face recognition is a design recognition mission performed exactly on faces. It can be described as categorizing a facial either “known” or “unknown”, after comparing it with deposits known individuals. It is also necessary to need a system that has the capability of knowledge to recognize indefinite faces. Computational representations of facial recognition must statement various difficult issues. After existing work, we study the SIFT structures for the gratitude method. The novel technique is compared with well settled facial recognition methods, name component analysis and eigenvalues and vector. This algorithm is called PCA and ICA (Independent Component Analysis). In research work, we implement the novel approach to detect the face in minimum time and evaluate the better accuracy based on Back Propagation Neural Networks. We design the framework in face recognition using MATLAB 2013a simulation tool. Evaluate the performance parameters, i.e. the FAR (false acceptance rate), FRR (False rejection Rate) and Accuracy and compare the existing performance parameters i.e. accuracy.}, 
keywords={backpropagation;eigenvalues and eigenfunctions;face recognition;feature extraction;independent component analysis;principal component analysis;face recognition;feature extraction;appropriately verifying face pictures;face detector techniques;feature based techniques;low-level analysis;face dataset;individual face;image space;indefinite faces;settled facial recognition methods;independent component analysis;principle component analysis;vector dimensional illustration;normal vector features;backpropagation neural networks;Face recognition;Face;Feature extraction;Algorithm design and analysis;Signal processing algorithms;Videos;Databases;Face recognition;Features of face;Eigen values and Vectors;Neural Network}, 
doi={10.1109/ISPCC.2017.8269662}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8272703, 
author={H. {Li} and J. {Sun} and L. {Chen}}, 
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)}, 
title={Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition}, 
year={2017}, 
volume={}, 
number={}, 
pages={234-242}, 
abstract={This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01%, 97.60%, and 96.13% on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition.}, 
keywords={face recognition;feature extraction;geometry;image classification;image representation;pose estimation;location-sensitive sparse representation;expression-robust 3D face recognition approach;given raw 3D facial surfaces;3D face pre-processing pipeline;face region cropping;normalized 3D facial surface;facial surface normal components;deep face net;facial surface normals;different facial locations;location sensitive sparse representation classifier;different 3D faces;different normal components;BU-3DFE databases;massive 2D face images;Three-dimensional displays;Face;Face recognition;Solid modeling;Two dimensional displays;Shape;Deformable models}, 
doi={10.1109/BTAS.2017.8272703}, 
ISSN={2474-9699}, 
month={Oct},}
@INPROCEEDINGS{7443288, 
author={ and and S. {Tripathi}}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={Pose invariant method for emotion recognition from 3D images}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Information about the emotional state of a person can be inferred from facial expressions. Emotion recognition has become an active research area in recent years in various fields such as Human Robot Interaction (HRI), medicine, intelligent vehicle, etc., The challenges in emotion recognition from images with pose variations, motivates researchers to explore further. In this paper, we have proposed a method based on geometric features, considering images of 7 yaw angles (-45°,-30°,-15°,0°,+15°,+30°,+45°) from BU3DFE database. Most of the work that has been reported considered only positive yaw angles. In this work, we have included both positive and negative yaw angles. In the proposed method, feature extraction is carried out by concatenating distance and angle vectors between the feature points, and classification is performed using neural network. The results obtained for images with pose variations are encouraging and comparable with literature where work has been performed on pitch and yaw angles. Using our proposed method non-frontal views achieve similar accuracy when compared to frontal view thus making it pose invariant. The proposed method may be implemented for pitch and yaw angles in future.}, 
keywords={emotion recognition;feature extraction;image classification;neural nets;pose estimation;visual databases;pose invariant method;emotion recognition;3D image;person emotional state;facial expressions;pose variation;geometric features;BU3DFE database;positive yaw angles;negative yaw angles;feature extraction;concatenating distance;angle vectors;feature points;classification;neural network;nonfrontal views;Emotion recognition;Databases;Three-dimensional displays;Feature extraction;Eyebrows;Euclidean distance;Mouth;BU3DFE database;feature points;feature extraction;classification;neural network}, 
doi={10.1109/INDICON.2015.7443288}, 
ISSN={2325-9418}, 
month={Dec},}
@INPROCEEDINGS{8075548, 
author={T. {Frikha} and F. {Chaabane} and B. {Said} and H. {Drira} and M. {Abid} and C. {Ben Amar} and L. {Lille}}, 
booktitle={2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
title={Embedded approach for a Riemannian-based framework of analyzing 3D faces}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform.}, 
keywords={biometrics (access control);embedded systems;face recognition;feature extraction;field programmable gate arrays;hardware-software codesign;software architecture;stereo image processing;tensors;biometric facial recognition system;embedded systems;data completion;full face recognition;hardware architecture;adaptation approach;face detection;Riemannian-based framework;3D face analysis;multimedia embedded applications;face expressions recognition;mixed hardware software architecture;codesign technique;FPGA platform;Computer architecture;Shape;Face recognition;Multimedia communication;Three-dimensional displays;Embedded systems;Measurement;Facial analysis;face detection;Facial expressions;3D face recognition;embedded architecture;elastic analysis algorithm;Riemann geometry;Curve analysis}, 
doi={10.1109/ATSIP.2017.8075548}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7899838, 
author={ and and }, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Face verification with three-dimensional point cloud by using deep belief networks}, 
year={2016}, 
volume={}, 
number={}, 
pages={1430-1435}, 
abstract={Developing reliable and robust face verification systems has been a tough challenge in computer vision, for several decades. The variation in illumination and head pose may seriously inhibit the accuracy of two-dimensional face recognition. With the invention of a depth map sensor, more three-dimensional volume data can be processed to mitigate the problem associated with face verification. This paper presents a three-dimensional face verification approach that includes three phases. First, point cloud library is applied to estimate features such as normal vectors and principal curvatures of every point on a human face point cloud acquired from three-dimensional depth sensor. Next, we adopt deep belief networks to train the identification model using extracted features. Finally, face verification is accomplished by using the pre-trained deep belief networks to justify if new incoming face point cloud feature is the one we specified. The experimental results demonstrate that the proposed system performs exceptionally well with about 96.43% verification accuracy.}, 
keywords={belief networks;computer vision;face recognition;feature extraction;robust face verification systems;three-dimensional point cloud;deep belief networks;computer vision;two-dimensional face recognition;point cloud library;three-dimensional depth sensor;feature extraction;Three-dimensional displays;Face;Feature extraction;Face recognition;Estimation;Training;Neurons;face verification;3D point cloud;feature extraction;principal curvature estimation;deep belief networks}, 
doi={10.1109/ICPR.2016.7899838}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7910452, 
author={S. G. {Gunanto} and M. {Hariadi} and E. M. {Yuniarno}}, 
booktitle={2016 2nd International Conference of Industrial, Mechanical, Electrical, and Chemical Engineering (ICIMECE)}, 
title={Computer facial animation with synthesize marker on 3D faces surface}, 
year={2016}, 
volume={}, 
number={}, 
pages={260-263}, 
abstract={An animated character has its own characteristics and behaviour. The animator needs to be skilled enough to make a complex animation, especially on making face expression. a Well defined facial expression can represent the emotional condition and make the animation more expressing the mood. But most facial animation is done by manually, frame-by-frame. It is very time-consuming. This research proposed a combination methods for handling the facial animation based on marker location and face surface from the 3D character. The motion data captured based on the location and movement of the marker then implemented on 3D face model to generate the motion guidance. As a guidance, this marker data role as a centroid of a vertex cluster. The cluster provided by implementing segmentation fp-NN Clustering method based on surface and can visualize the deformation using linear blend skinning methods. The result from this research shows that this system can automatically generate facial animations based on the marker data and the surface segmentation. The visualization of deformation arranges accordingly to the motion captured data and organized sequentially.}, 
keywords={computer animation;data visualisation;face recognition;image segmentation;neural nets;pattern clustering;computer facial animation;synthesize marker;3D face surface;facial expression;3D character;motion data capture;motion guidance;vertex cluster;segmentation fp-NN Clustering method;linear blend skinning methods;Facial animation;Three-dimensional displays;Solid modeling;Motion segmentation;Surface treatment;Interpolation;facial animation;feature marker;surface}, 
doi={10.1109/ICIMECE.2016.7910452}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8226186, 
author={K. S. {Warke} and R. {Suralkar} and S. {Pawar} and P. {Sonawane} and S. {Wani}}, 
booktitle={2017 2nd International Conference for Convergence in Technology (I2CT)}, 
title={A real sense based multilevel security in cloud framework using face recognition and image processing}, 
year={2017}, 
volume={}, 
number={}, 
pages={531-533}, 
abstract={A 3D image can be used for the authentications as it gives more accuracy. We can also use the gestures for the authentication purpose. We can add multiple authentication levels together to make system more secure and login process more reliable. In our system we are going to provide three levels of authentication i.e. 1) Text Password:- This is first level in which user has to enter the text password which is OTP. OTP will be sent to the registered email-id or at the mobile number given. With The help of AES (advance encryption standard) algorithm the data will be encrypt and store at database server. 2) Hand Gesture Recognition:-This is second level in which user has to place this hand in front of camera. So camera can detect it, select one point out of 22 points on the hand whatever pattern user make in front of camera is saved as password at user's database. 3) Face Recognition:-This is last level in which user has to place face in front of camera. So camera can detect the face and select 78 landmark points. And those points are saved at the user's database. After passing all these stages user is authenticate and can upload or download the documents of his/her choice.}, 
keywords={authorisation;cloud computing;cryptography;face recognition;gesture recognition;image processing;gestures;authentication purpose;multiple authentication levels;text password;OTP;registered email;mobile number;AES;advance encryption standard;database server;pattern user;multilevel security;cloud framework;face recognition;Face recognition;Cloud computing;Cameras;Face;Authentication;Algorithm design and analysis;3D Gesture;Real Sense;Image Processing;Face Recognition;Cloud},
doi={10.1109/I2CT.2017.8226186}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7066811, 
author={G. {Betta} and D. {Capriglione} and M. {Corvino} and M. {Gasparetto} and E. {Zappa} and C. {Liguori} and A. {Paolillo}}, 
booktitle={2015 XVIII AISEM Annual Conference}, 
title={A proposal for improving the performance of face recognition systems based on 3d features}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={In this paper a suitable methodology for the improvement of the reliability of results in classification systems based on 3D images is proposed. More in detail, it is based on the knowledge of the uncertainty of the features constituting the 3D image (obtained processing a pair of two 2D stereoscopic images) and on a suitable statistical approach providing a confidence level to the classification result. These pieces of information are then managed in order to improve the classification performance in terms of correct classification and missed classification percentages. The experimental results, obtained applying the methodology on an Active Appearance Models algorithm, a popular method for face recognition based on 3D features, show that, compared with a traditional approach (which generally does not take into account the uncertainty on 3D features), the proposed methodology allows to significantly improve the classification performance even in scenarios characterized by a high uncertainty.}, 
keywords={face recognition;image classification;stereo image processing;face recognition systems;3D features;3D images;classification systems;2D stereoscopic image;classification performance;active appearance models;correct classification percentages;missed classification percentages;Uncertainty;Three-dimensional displays;Face recognition;Measurement uncertainty;Active appearance model;Classification algorithms;Databases;face recognition;measurement uncertainty;image classification;decision support systems;3D features}, 
doi={10.1109/AISEM.2015.7066811}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7378673, 
author={ and and }, 
booktitle={2015 23rd International Conference on Geoinformatics}, 
title={A method of extracting human facial feature points based on 3D laser scanning point cloud data}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-3}, 
abstract={Three-dimensional body measurement is determined by measuring the size of each part of the body and body shape, in order to study the morphological characteristics of the human body, and provide basic information for human industrial design, ergonomics, engineering design, anthropological research, and medicine. This paper presents a three-dimensional point cloud data extraction method of facial feature points by doing nose points with examples, using this method can accurately locate the feature points, and there is no specific stations toward requirements in the process of scanning point cloud of the human body, it is convenient and quick. The results show that this method can meet the ergonomics and other features to quickly locate and measure the body size requirements.}, 
keywords={computer graphics;face recognition;feature extraction;optical scanners;human facial feature point extraction;3D laser scanning point cloud data;three-dimensional body measurement;morphological characteristics;human body;human industrial design;ergonomics;engineering design;anthropological research;medicine;three-dimensional point cloud data extraction method;nose point;Atmospheric modeling;Ergonomics;Biomedical imaging;The face feature point;Nose point;Measurement;Three-dimensional point cloud;Three-dimensional laser scanning}, 
doi={10.1109/GEOINFORMATICS.2015.7378673}, 
ISSN={2161-024X}, 
month={June},}
@INPROCEEDINGS{8272716, 
author={F. {Liu} and J. {Hu} and J. {Sun} and Y. {Wang} and Q. {Zhao}}, 
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)}, 
title={Multi-dim: A multi-dimensional face database towards the application of 3D technology in real-world scenarios}, 
year={2017}, 
volume={}, 
number={}, 
pages={342-351}, 
abstract={Three-dimensional (3D) faces are increasingly utilized in many face-related tasks. Despite the promising improvement achieved by 3D face technology, it is still hard to thoroughly evaluate the performance and effect of 3D face technology in real-world applications where variations frequently occur in pose, illumination, expression and many other factors. This is due to the lack of benchmark databases that contain both high precision full-view 3D faces and their 2D face images/videos under different conditions. In this paper, we present such a multi-dimensional face database (namely Multi-Dim) of high precision 3D face scans, high definition photos, 2D still face images with varying pose and expression, low quality 2D surveillance video clips, along with ground truth annotations for them. Based on this Multi-Dim face database, extensive evaluation experiments have been done with state-of-the-art baseline methods for constructing 3D morphable model, reconstructing 3D faces from single images, 3D-assisted pose normalization for face verification, and 3D-rendered multiview gallery for face identification. Our results show that 3D face technology does help in improving unconstrained 2D face recognition when the probe 2D face images are of reasonable quality, whereas it deteriorates rather than improves the face recognition accuracy when the probe 2D face images are of poor quality. We will make Multi-Dim freely available to the community for the purpose of advancing the 3D-based unconstrained 2D face recognition and related techniques towards real-world applications.}, 
keywords={emotion recognition;face recognition;image reconstruction;pose estimation;solid modelling;video surveillance;probe 2D face images;unconstrained 2D face recognition;real-world applications;multidimensional face database;three-dimensional faces;face-related tasks;3D face technology;high precision full-view 3D faces;high precision 3D face scans;low quality 2D surveillance video clips;MultiDim face database;3D morphable model;face verification;face identification;face recognition accuracy;2D face image-video;Face;Three-dimensional displays;Two dimensional displays;Databases;Surveillance;Face recognition;Cameras}, 
doi={10.1109/BTAS.2017.8272716}, 
ISSN={2474-9699}, 
month={Oct},}
@INPROCEEDINGS{7351408, 
author={T. {Batabyal} and A. {Vaccari} and S. T. {Acton}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={UGraSP: A unified framework for activity recognition and person identification using graph signal processing}, 
year={2015}, 
volume={}, 
number={}, 
pages={3270-3274}, 
abstract={With the growing availability and wide distribution of low-cost, high-performance 3D imaging sensors, the image analysis community has witnessed an increased demand for solutions to the challenges of activity recognition and person identification. We propose an integrated framework, based on graph signal processing, that simultaneously performs both tasks using a single set of features. The novelty of our approach is based on the fact that the set of features used for activity recognition accommodates person identification without additional computation. The analysis is based on the extracted structure-invariant graph (skeleton). The Laplacian of the skeleton is used both to identify the person and recognize the performed activity. While person identification is achieved directly from the analysis of the Laplacian, activity recognition is obtained after transformation, into the graph spectral domain, of the vectorized form of the skeletal joints 3D coordinates. Feature vectors for activity recognition are then derived, in this domain, from the covariance matrices evaluated over fixed-length sequential video segments. Both classification tasks are implemented using linear support vector machines (SVM). When applied to real activity datasets, our approach shows an improved performance over the existing state-of-the-art.}, 
keywords={covariance matrices;feature extraction;graph theory;image classification;object detection;object recognition;support vector machines;video signal processing;UGraSP;unified framework;image analysis community;activity recognition;person identification;integrated framework;graph signal processing;feature tasks;structure-invariant graph extraction;graph skeleton;skeleton Laplacian;graph spectral domain;vectorized form;skeletal joints 3D coordinates;feature vectors;covariance matrices;fixed-length sequential video segments;classification tasks;linear support vector machines;SVM;real activity datasets;performance improvement;Laplace equations;Skeleton;Three-dimensional displays;Motion segmentation;Image recognition;Sensors;Support vector machines;Laplacian;Adjacency Matrix;Graph Signal Processing;Graph Fourier Transform;activity Recognition;Person Identification;Point cloud datasets}, 
doi={10.1109/ICIP.2015.7351408}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{7224078, 
author={L. {Spreeuwers}}, 
journal={IET Biometrics}, 
title={Breaking the 99% barrier: optimisation of three-dimensional face recognition}, 
year={2015}, 
volume={4}, 
number={3}, 
pages={169-178}, 
abstract={This study presents optimisations to a three-dimensional (3D) face recognition method the authors published in 2011. The optimisations concern handling and estimation of motion from a single 3D image using the symmetry of the face, fine registration by selection of the maximum score for small variations of the registration parameters and efficient training using automatic outlier removal where only part of the classifier is retrained. The optimisations lead to a staggering performance improvement: the verification rate on Face Recognition Grand Challenge (FRGC) v2 data at false accept rate = 0.1% increases from 94.6 to 99.3% and the identification rate increases from 99 to 99.4%. Both are, to the authors' knowledge, the best scores ever published on the FRGC data. In addition, the registration time was reduced from about 2.5 to 0.2-0.6 s and the number of comparisons has increased from about 11 000 to more than 50 000 per second. For slightly decreased performance, even millions of comparisons can be realised. The fast registration means near real-time recognition with 2-5 images is possible. The optimisations are not specific for this method, but can be beneficial for other 3D face recognition methods as well.}, 
keywords={face recognition;image classification;image registration;motion estimation;optimisation;3D face recognition;real-time image recognition;false accept rate;verification rate;FRGC v2 data;performance improvement;classifier training;automatic outlier removal;image registration parameters;3D image;motion handling;motion estimation;three-dimensional face recognition optimisation}, 
doi={10.1049/iet-bmt.2014.0017}, 
ISSN={2047-4938}, 
month={},}
@INPROCEEDINGS{8546094, 
author={J. {Liang} and F. {Liu} and H. {Tu} and Q. {Zhao} and A. K. {Jain}}, 
booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
title={On Mugshot-based Arbitrary View Face Recognition}, 
year={2018}, 
volume={}, 
number={}, 
pages={3126-3131}, 
abstract={Despite the wide usage of mugshot images in forensic applications, they are underutilized in existing automated face recognition systems. In this paper, we propose a novel mugshot-based arbitrary view face recognition method. Our approach reconstructs full 3D faces via cascaded regression in shape space with efficient seamless texture recovery. Unlike existing methods, it makes full use of the frontal and profile views available in mugshot images, and thus generates accurate and realistic 3D faces. Multi-view face images are synthesized from the reconstructed 3D faces to enlarge the gallery so that arbitrary view faces can be better recognized. Evaluation experiments were conducted on BFM and Multi-PIE databases by using state-of-the-art deep learning (DL) based face matchers. The results demonstrate the effectiveness of our proposed method and show that DL-based face matchers can benefit from mugshot images and the reconstructed 3D faces, especially for recognizing large off-angle faces.}, 
keywords={criminal law;face recognition;image matching;image reconstruction;image texture;learning (artificial intelligence);stereo image processing;forensic applications;multiPIE database;BFM database;deep learning based face matchers;mugshot-based arbitrary view face recognition method;seamless texture recovery;multiview face images;automated face recognition systems;off-angle faces;reconstructed 3D faces;mugshot images;DL-based face matchers;Face;Three-dimensional displays;Shape;Face recognition;Image reconstruction;Probes;Two dimensional displays}, 
doi={10.1109/ICPR.2018.8546094}, 
ISSN={1051-4651}, 
month={Aug},}
@INPROCEEDINGS{8389776, 
author={G. {Geetha} and M. {Safa} and C. {Fancy} and K. {Chittal}}, 
booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)}, 
title={3D face recognition using Hadoop}, 
year={2017}, 
volume={}, 
number={}, 
pages={1882-1885}, 
abstract={Face Recognition is one of the biometric technique to vestige the given faces. We present a 3D face recognition method using Hadoop to recognize 3D faces under varying expressions, lighting and different poses to overcome the challenges of the 2D face recognition. In this paper, a threshold facial region of an image is detected and preprocessing is done based through the image excellence. If the selected face is frontal face with good lighting, extract the prerequisite features and do the necessary comparison steps to recognize the faces. In case, if the selected face is in bad lighting, then perform histogram equalization and normalization to increase the contrast. Different Poses and expressions are the very challenging zones which require surplus pre-processing to improve the performance of the face recognition system. Hence an enhanced normalization method called 3D Morphable Model are used as a pre- processing technique to create a frontal view from a non-frontal view and also merge images with different views in to a single frontal view. Next to diminish the number of features used for recognition process; we emulate the linear discriminant analysis method for further classification. Eventually, we used an open-source Hadoop Image Processing Interface (HIPI) to act as an interface for MapReduce technology for recognition.}, 
keywords={biometrics (access control);face recognition;feature extraction;parallel processing;frontal face;biometric technique;3D face recognition method;histogram equalization;surplus preprocessing;enhanced normalization method;3D morphable model;single frontal view;nonfrontal view;linear discriminant analysis method;open-source Hadoop image processing interface;HIPI;MapReduce technology;Face;Face recognition;Three-dimensional displays;Lighting;Feature extraction;Solid modeling;Hadoop;Image Processing;Map Reduce;Linear Discriminant analysis}, 
doi={10.1109/ICECDS.2017.8389776}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7163090, 
author={X. {Yang} and D. {Huang} and Y. {Wang} and L. {Chen}}, 
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
title={Automatic 3D facial expression recognition using geometric scattering representation}, 
year={2015}, 
volume={1}, 
number={}, 
pages={1-6}, 
abstract={Facial Expression Recognition (FER) is one of the most active topics in the domain of computer vision and pattern recognition, and it has received increasing attention for its wide application potentials as well as attractive scientific challenges. In this paper, we present a novel method to automatic 3D FER based on geometric scattering representation. A set of maps of shape features in terms of multiple order differential quantities, i.e. the Normal Maps (NOM) and the Shape Index Maps (SIM), are first jointly adopted to comprehensively describe geometry attributes of the facial surface. The scattering operator is then introduced to further highlight expression related cues on these maps, thereby constructing geometric scattering representations of 3D faces for classification. The scattering descriptor not only encodes distinct local shape changes of various expressions as by several milestone descriptors, such as SIFT, HOG, etc., but also captures subtle information hidden in high frequencies, which is quite crucial to better distinguish expressions that are easily confused. We evaluate the proposed approach on the BU-3DFE database, and the performance is up to 84.8% and 82.7% with two commonly used protocols respectively which is superior to the state of the art ones.}, 
keywords={emotion recognition;face recognition;image classification;image representation;shape recognition;automatic 3D facial expression recognition;automatic 3D FER;BU-3DFE database;local shape changes;3D face classification;scattering operator;facial surface geometry attributes;SIM;shape index maps;NOM;normal maps;multiple order differential quantities;shape feature maps;geometric scattering representation;Three-dimensional displays;Shape;Scattering;Indexes;Support vector machines;Solid modeling;Feature extraction}, 
doi={10.1109/FG.2015.7163090}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8578635, 
author={S. {Cheng} and I. {Kotsia} and M. {Pantic} and S. {Zafeiriou}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={5117-5126}, 
abstract={The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.}, 
keywords={biometrics (access control);computer vision;emotion recognition;face recognition;image capture;image resolution;stereo image processing;visual databases;high-resolution 3D faces;4DFAB;facial behaviour;facial expression recognition;behavioural biometrics;computer vision applications;automatic face analysis;scale visual databases;face recognition;Databases;Three-dimensional displays;Face;Face recognition;Cameras;Two dimensional displays;Task analysis}, 
doi={10.1109/CVPR.2018.00537}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{7823978, 
author={S. {Naveen} and R. K. {Ahalya} and R. S. {Moni}}, 
booktitle={2016 International Conference on Communication Systems and Networks (ComNet)}, 
title={Multimodal face recognition using spectral transformation by LBP and polynomial coefficients}, 
year={2016}, 
volume={}, 
number={}, 
pages={13-17}, 
abstract={This paper presents a multimodal face recognition using spectral transformation by Local Binary Pattern (LBP) and Polynomial Coefficients. Here 2D image and 3D image are combined to get multimodal face recognition. In this method a novel feature extraction is done using LBP and Polynomial Coefficients. Then these features are spectrally transformed using Discrete Fourier Transform (DFT). These spectrally transformed features extracted from texture image using the two methods are combined at the score level. Similarly this is done in depth image. Finally feature information from texture and depth are combined at the score level which gives better results than the individual results.}, 
keywords={discrete Fourier transforms;face recognition;feature extraction;image texture;multimodal face recognition;spectral transformation;LBP;polynomial coefficients;local binary pattern;2D image;3D image;feature extraction;discrete Fourier transform;DFT;texture image;depth image;feature information;Feature extraction;Face recognition;Discrete Fourier transforms;Discrete cosine transforms;Databases;Face;Two dimensional displays;Texture;Depth;Local Binary Pattern (LBP);Polynomial Coefficients;Multimodal;Discrete Fourier Transform (DFT)}, 
doi={10.1109/CSN.2016.7823978}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8346387, 
author={H. M. R. {Afzal} and S. {Luo} and M. K. {Afzal}}, 
booktitle={2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)}, 
title={Reconstruction of 3D facial image using a single 2D image}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In many scenarios related to image processing, 3D face reconstruction is considered an essential part. A robust and fast method of 3D face reconstruction from a single 2D image is presented. This method consists of three main steps including extraction of features from single image, calculating the depth of image and adjustment of a 3D model on the direction of Z-axis. In the first step, the features of image are extracted by using supervised descent method (SDM). Using SDM, face regions like facial components (eyes, nose lips) and face contours are detected. Second step consists of depth prediction by implementation of multivariate Gaussian distribution. Finally, 3D face is constructed with the help of features and the depth information and 3D database. The proposed method has been verified by conducting several experiments depicted in evaluation section. Our method is robust in nature and gives good results even using a single image, comparing to other methods that use multiple images for reconstruction of 3D images.}, 
keywords={face recognition;feature extraction;Gaussian distribution;gradient methods;image reconstruction;stereo image processing;3D face reconstruction;supervised descent method;SDM;face regions;3D model;facial components;face contours;depth prediction;multivariate Gaussian distribution;image processing;single 2D image;3D facial image;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Shape;Two dimensional displays;Solid modeling;3D face reconstruction;features extraction;Gaussain distribution;facial modeling}, 
doi={10.1109/ICOMET.2018.8346387}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7853992, 
author={R. {Amin} and A. F. {Shams} and S. M. M. {Rahman} and D. {Hatzinakos}}, 
booktitle={2016 9th International Conference on Electrical and Computer Engineering (ICECE)}, 
title={Evaluation of discrimination power of facial parts from 3D point cloud data}, 
year={2016}, 
volume={}, 
number={}, 
pages={602-605}, 
abstract={Feature selection from facial regions is a well-known approach to increase the performance of 2D image-based face recognition systems. In case of 3D modality, the approach of region-based feature selection for face recognition is relatively new. In this context, this paper presents an approach to evaluate the discrimination power of different regions of a 3D facial surface for its potential use in face recognition systems. We propose the use of weighted average of unit normal vector on the facial surface as the feature for region-based face recognition from 3D point cloud data (PCD). The iterative closest point algorithm is employed for the registration of segmented regions of facial point clouds. A metric based on angular distance between normals is introduced to indicate the similarity between two surfaces of same facial region. Finally, the intra class correlation based discrimination score is formulated to find out the key facial regions such as the eyes, nose, and mouth that are significant while recognizing a person with facial surface PCD.}, 
keywords={computational geometry;correlation methods;face recognition;feature selection;image registration;image segmentation;discrimination power evaluation;facial parts;2D image-based face recognition systems;3D modality;region-based feature selection;3D facial surface;3D point cloud data;3D PCD;iterative closest point algorithm;segmented region registration;angular distance;intra class correlation;discrimination score;Three-dimensional displays;Face;Face recognition;Measurement;Two dimensional displays;Nose;Databases}, 
doi={10.1109/ICECE.2016.7853992}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8089906, 
author={L. {Fangmin} and C. {Ke} and L. {Xinhua}}, 
booktitle={2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA)}, 
title={3D Face Reconstruction Based on Convolutional Neural Network}, 
year={2017}, 
volume={}, 
number={}, 
pages={71-74}, 
abstract={Fast and robust 3D reconstruction of facial geometric structure from a single image is a challenging task with numerous applications, but there exist two problems when applied "in the wild": the 3D estimates are unstable for different photos of the same subject; the 3D estimates are over-regularized and generic. In response, a robust method for regressing discriminative 3D morphable face models(3DMM) is described to support face recognition and 3D mask printing. Combining the local data sets with the public data sets, improving the exiting 3DMM fitting method and then using a convolutional neural network(CNN) to improve reconstruction effect. The ground truth 3D faces of the CNN are the pooled 3DMM parameters extracted from the photos of the same subject. Using CNN to regress 3DMM shape and texture parameters directly from an input photo and offering a method for generating huge numbers of labeled examples. There are two key points of the paper: one is the training data generation for the model training; the other is the training of 3D reconstruction model. Experimental results and analysis show that this method costs much less time than traditional methods of 3D face modeling, and it is improved for different races on photos with any angles than the existing methods based on deep learning, and the system has better robustness.}, 
keywords={face recognition;image morphing;image reconstruction;image texture;learning (artificial intelligence);neural nets;regression analysis;shape recognition;3D face reconstruction;facial geometric structure;robust method;face recognition;3D mask printing;local data sets;public data sets;reconstruction effect;texture parameters;training data generation;3D reconstruction model;3D face modeling;convolutional neural network;discriminative 3D morphable face models;3DMM fitting method;CNN;Face;Three-dimensional displays;Solid modeling;Image reconstruction;Shape;Robustness;Data models;3D face reconstruction;convolutional neural network(CNN);3DMM;shape;texture}, 
doi={10.1109/ICICTA.2017.23}, 
ISSN={}, 
month={Oct},}
@ARTICLE{8353876, 
author={X. {Song} and Z. {Feng} and G. {Hu} and J. {Kittler} and X. {Wu}}, 
journal={IEEE Transactions on Information Forensics and Security}, 
title={Dictionary Integration Using 3D Morphable Face Models for Pose-Invariant Collaborative-Representation-Based Classification}, 
year={2018}, 
volume={13}, 
number={11}, 
pages={2734-2745}, 
abstract={The paper presents a dictionary integration algorithm using 3D morphable face models (3DMM) for pose-invariant collaborative-representation-based face classification. To this end, we first fit a 3DMM to the 2D face images of a dictionary to reconstruct the 3D shape and texture of each image. The 3D faces are used to render a number of virtual 2D face images with arbitrary pose variations to augment the training data, by merging the original and rendered virtual samples to create an extended dictionary. Second, to reduce the information redundancy of the extended dictionary and improve the sparsity of reconstruction coefficient vectors using collaborative-representation-based classification (CRC), we exploit an on-line class elimination scheme to optimise the extended dictionary by identifying the training samples of the most representative classes for a given query. The final goal is to perform pose-invariant face classification using the proposed dictionary integration method and the on-line pruning strategy under the CRC framework. Experimental results obtained for a set of well-known face data sets demonstrate the merits of the proposed method, especially its robustness to pose variations.}, 
keywords={face recognition;image classification;image representation;image texture;pose estimation;dictionary integration algorithm;3DMM;texture;virtual 2D face images;original rendered virtual samples;dictionary integration method;3D morphable face models;pose-invariant collaborative-representation-based face classification;online class elimination scheme;online pruning strategy;Face;Dictionaries;Training;Three-dimensional displays;Image reconstruction;Two dimensional displays;Solid modeling;Collaborative-representation-based classification;3D morphable face model;dictionary integration;face classification;virtual training samples}, 
doi={10.1109/TIFS.2018.2833052}, 
ISSN={1556-6013}, 
month={Nov},}
@INPROCEEDINGS{8687254, 
author={T. {Terada} and Y. {Chen} and R. {Kimura}}, 
booktitle={2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
title={3D Facial Landmark Detection Using Deep Convolutional Neural Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={390-393}, 
abstract={Various applications have been developed in facial research to aid the recognition of personal attributes, analysis of race, and personal authentication for the security industry and other research fields. Thus, it is possible to identify the differences in facial shape based on place of birth or country. The present study analyzes facial shape using scanned 3D facial images and investigates ways to extract facial landmarks from the 3D facial images. The detection of the facial landmark requires the normalization of the facial scale and position among in the 3D image data to analyze the facial shape. Therefore, it is difficult to obtain accurate facial landmarks from 3D facial images. Our method decomposes the task into the following three parts: (a) conversion of data from the 3D facial image to a 2D image, (b) extraction of facial landmarks from the 3D image using Convolutional Neural Network (CNN), (c) inversion of the identified facial landmarks from 2D to 3D images. In experiments, we compared the accuracy of this model for facial landmark detection.}, 
keywords={convolutional neural nets;face recognition;facial landmark detection;3D facial image;personal authentication;convolutional neural network;security industry;CNN;component;landmarks detection;3d facial image;point cloud;facial analysis;cnn}, 
doi={10.1109/FSKD.2018.8687254}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7442295, 
author={T. {Yamasaki} and I. {Nakamura} and K. {Aizawa}}, 
booktitle={2015 IEEE International Symposium on Multimedia (ISM)}, 
title={Fast Face Model Reconstruction and Synthesis Using an RGB-D Camera and Its Subjective Evaluation}, 
year={2015}, 
volume={}, 
number={}, 
pages={53-56}, 
abstract={It is difficult to show a frontal face in video chatting because there is a gap between a display and a camera. We propose a method for real-time face reorientation by creating a 2.5-D face model from a single RGB-D camera and synthesizing the rotated face model with the original face image. Our method uses two kinds face models complementarily: a point cloud based model and a generic face model fitted to the user. We conducted subjective evaluation and confirmed the validity of our proposed system.}, 
keywords={cameras;face recognition;image reconstruction;screens (display);video signal processing;fast face model reconstruction;fast face model synthesis;RGB-D camera;frontal face;video chatting;display;real-time face reorientation;2.5D face model;rotated face model synthesis;face image;point cloud-based model;generic face model;Decision support systems;Face;Active appearance model;Rendering (computer graphics);Multimedia communication;Cameras;Indexes;RGB-D camera;DIBR;real-time;gaze correction;face reorientation;virtual view synthesis}, 
doi={10.1109/ISM.2015.107}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7976644, 
author={S. {Sghaier} and C. {Souani} and H. {Faeidh} and K. {Besbes}}, 
booktitle={2016 Global Summit on Computer Information Technology (GSCIT)}, 
title={Novel Technique for 3D Face Segmentation and Landmarking}, 
year={2016}, 
volume={}, 
number={}, 
pages={27-31}, 
abstract={In this paper we propose a new technique to detect and extract any part that we need from a 3D face. The regions of interest of our approach are the eyes and the nose. Moreover, we are interested in the extraction of the salient landmarks in 3D face. Therefore, in this paper a new method is proposed to detect the nose tip and eye corners from a three-dimensional face range image. Our original technique allows fully automated processing, treating incomplete and noisy input data, and automatically rejecting non-facial areas. Besides, it is robust against holes in 3D image and insensitive to facial expressions. Besides, it is stable against translation and rotation of the face, and it is suitable for different resolutions of images. All the experiments have been performed on the GAVAB and FRAV 3D databases. After applying the proposed method to the 3D face, experimental results show that it is comparable to state-of-the-art methods in terms of its accuracy and flexibility.}, 
keywords={face recognition;image segmentation;visual databases;3D face segmentation;landmark extraction;nose tip detection;eye corner detection;three-dimensional face range image;automatic nonfacial area rejection;3D image;facial expressions;image resolutions;GAVAB 3D database;FRAV 3D database;Face;Three-dimensional displays;Nose;Information technology;Noise measurement;Robustness;Image resolution;3D face;segmentation;region of interest;anthropometric;landmarks}, 
doi={10.1109/GSCIT.2016.17}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7523133, 
author={G. {Torkhani} and A. {Ladgham} and M. N. {Mansouri} and A. {Sakly}}, 
booktitle={2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
title={Gabor-SVM applied to 3D-2D deformed mesh model}, 
year={2016}, 
volume={}, 
number={}, 
pages={447-452}, 
abstract={We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate.}, 
keywords={face recognition;feature extraction;Gabor filters;mesh generation;support vector machines;visual databases;wavelet transforms;3D-2D deformed mesh model;Gabor-SVM;3D face recognition;2D modeling;3D modeling;facial curvatures detection;3D image transformation;3D triangular mesh;2D space;2D smoother mesh;Gabor wavelets;surface curves;salient face feature detection;Gabor facial model;support vector machines;3D AJMAL face database;Three-dimensional displays;Solid modeling;Face recognition;Face;Deformable models;Feature extraction;Databases;3D face recognition;salient points;deformed mesh model;facial curvatures;Gabor wavelet;SVM}, 
doi={10.1109/ATSIP.2016.7523133}, 
ISSN={}, 
month={March},}
@ARTICLE{7361976, 
author={M. {Piccirilli} and G. {Doretto} and A. {Ross} and D. {Adjeroh}}, 
journal={IEEE Sensors Journal}, 
title={A Mobile Structured Light System for 3D Face Acquisition}, 
year={2016}, 
volume={16}, 
number={7}, 
pages={1854-1855}, 
abstract={A mobile sensor based on fringe projection techniques is developed with the goal of acquiring face 3D and color with a smartphone device. The system consists of a portable pico-projector and an Android-based smartphone. The data acquisition, pattern generation. and reconstruction of the final 3D point cloud are all driven by the smartphone. We present results on the root-mean-square error (RMSE) of the sensor and on 3D face matching.}, 
keywords={biometrics (access control);data acquisition;face recognition;image matching;image reconstruction;smart phones;mobile sensor;fringe projection;smartphone device;portable pico-projector;Android-based smartphone;data acquisition;pattern generation;3D point cloud reconstruction;root-mean-square error;3D face matching;mobile structured light system;3D biometrics;3D face acquisition;Three-dimensional displays;Face;Sensors;Cameras;Mobile communication;Image reconstruction;Lighting;Depth sensor;structured light;mobile device;Depth sensor;structured light;mobile device;3D face acquisition;3D face matching;3D biometrics}, 
doi={10.1109/JSEN.2015.2511064}, 
ISSN={1530-437X}, 
month={April},}
@INPROCEEDINGS{8314888, 
author={G. {Torkhani} and A. {Ladgham} and A. {Sakly}}, 
booktitle={2017 18th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering (STA)}, 
title={3D Gabor-Edge filters applied to face depth images}, 
year={2017}, 
volume={}, 
number={}, 
pages={578-582}, 
abstract={This manuscript introduces a novel 3D face authentication system inspired from the advantageous capacities of Gabor-Edge filters. The approach studies 3D face difficulties such as expression variety, different rotations and exposure to illuminations. The proposed systems starts by preprocessing the 3D face images to resolve acquisition problems. Then, a filtering process is performed by implanting our 3D Gabor-Edge technique extended based on the classic 3D Gabor masks. The next step is to achieve the classification of facial features from the edge saliency by the artificial Neural Network Classifier (NNC). The evaluation of the adopted system is achieved by exporting common datasets from GavabDB database. Experimental results are reported to prove the high accuracy rates of our method compared to the recent researches in the same biometric field.}, 
keywords={biometrics (access control);edge detection;face recognition;feature extraction;Gabor filters;image classification;neural nets;3D face difficulties;3D face images;3D Gabor-Edge technique;classic 3D Gabor masks;edge saliency;Gabor-edge filters;3D face authentication system;acquisition problems;facial feature classification;artificial neural network classifier;GavabDB database;biometric field;Three-dimensional displays;Face;Feature extraction;Authentication;Gabor filters;Face recognition;face authentication;Gabor filtering;3D images;saliency}, 
doi={10.1109/STA.2017.8314888}, 
ISSN={2573-539X}, 
month={Dec},}
@INPROCEEDINGS{7899769, 
author={G. {Tian} and T. {Mori} and Y. {Okuda}}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Spoofing detection for embedded face recognition system using a low cost stereo camera}, 
year={2016}, 
volume={}, 
number={}, 
pages={1017-1022}, 
abstract={Spoofing detection is essential for practical face recognition system. Based on the fact that genuine face has special geometric curvatures across surface, this paper brings forward an ultra-fast yet accurate spoofing detection approach using a low-cost stereo camera. To obtain curvatures, the three dimensional shapes of selected facial landmarks are analyzed, by fitting point cloud around each landmark to a specific partial face surface. Spoofing detection is then performed by evaluating curvatures of each landmark and integrating them together. Experiments verify that the approach is able to detect spoofed faces in printed photographs without or with various bending at FAR equal to 0.00%. Meanwhile, genuine faces have a trivial opportunity to be falsely rejected: FRR is 0.59% for near frontal faces and less than 5% for faces with large varying poses. Detection time is 51 milliseconds when executed on a single processor [1] running at a clock frequency of 266M Hz, this makes the detection very suitable for embedded face recognition system.}, 
keywords={face recognition;stereo image processing;spoofing detection;embedded face recognition system;low cost stereo camera;facial landmark 3D shapes;frequency 266 MHz;Face;Three-dimensional displays;Nose;Face recognition;Cameras;Surface fitting;Fitting;spoof detection;point cloud;surface fitting;stereo vision}, 
doi={10.1109/ICPR.2016.7899769}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8358484, 
author={K. M. {Swetha} and P. {Suja}}, 
booktitle={2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon)}, 
title={A geometric approach for recognizing emotions from 3D images with pose variations}, 
year={2017}, 
volume={}, 
number={}, 
pages={805-809}, 
abstract={Emotions are an incredibly important aspect of human life. Research on emotion recognition for the past few decades have resulted in development of several fields. In the current scenario, it is necessary that machines/robots need to identify human emotions and respond accordingly. Applications in this field can be seen in security, entertainment and Human Machine Interface/Human Robot Interface. Recent works on 3D images have gained importance due to its accuracy in real life applications as emotions can be recognised at different head poses. The intention of this work has been to develop an algorithm for recognition of emotion from facial expressions, which recognizes 6 basic emotions, which are anger, fear, happy, disgust, sad and surprise from 3D images in 7 yaw angles (+45° to -45°) and 3 pitch angles (+15°,0°, -15°). Most of the reported work considers + yaw angles. While in the current work, both positive as well as negative pitch and yaw angles are considered. BU3DFE database is used for the implementation. The proposed method resulted in improved accuracy and is comparable with the literature.}, 
keywords={emotion recognition;face recognition;geometry;pose estimation;BU3DFE database;geometric approach;pose variations;emotion recognition;human emotions;head poses;yaw angles;pitch angles;3D image;facial expressions;Basic emotions;feature points;BU3DFE database;classification}, 
doi={10.1109/SmartTechCon.2017.8358484}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7081271, 
author={M. C. {EL Rai} and N. {Werghi} and H. {Al Muhairi} and H. {Alsafar}}, 
booktitle={2015 International Conference on Communications, Signal Processing, and their Applications (ICCSPA'15)}, 
title={Using facial images for the diagnosis of genetic syndromes: A survey}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The analysis of facial appearance is significant to an early diagnosis of medical genetic diseases. The fast development of image processing and machine learning techniques facilitates the detection of facial dysmorphic features. This paper is a survey of the recent studies developed for the screening of genetic abnormalities across the facial features obtained from two dimensional and three dimensional images.}, 
keywords={diseases;face recognition;learning (artificial intelligence);medical image processing;patient diagnosis;facial images;genetic syndromes diagnosis;facial appearance analysis;medical genetic diseases;image processing;machine learning techniques;facial dysmorphic features;genetic abnormalities;facial features;two dimensional images;three dimensional images;Three-dimensional displays;Face;Genetics;Feature extraction;Principal component analysis;Accuracy;Databases;Facial images;2D imaging;3D imaging;Landmarks;Dysmorphology;Classification}, 
doi={10.1109/ICCSPA.2015.7081271}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8125644, 
author={A. V. {Kumar} and V. V. R. {Prasad} and K. M. {Bhurchandi} and V. R. {Satpute} and L. {Pious} and S. {Kar}}, 
booktitle={2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
title={Dense reconstruction of 3D human face using 5 images and no reference model}, 
year={2017}, 
volume={}, 
number={}, 
pages={1185-1190}, 
abstract={3D reconstruction of human faces is of high importance in applications like forensics, facial features based human tracking and realistic reconstruction of human faces in virtual reality applications. The published algorithms so far require either a large number of views of a human face or a 3D facial reference model. This paper proposes a technique for 3D human face reconstruction using only five views without any reference model unlike most of the contemporary facial reconstruction techniques. Face localization is done followed by facial feature point extraction in the facial images. The features are tracked in the other images using the Kanade-Lucas-Tomasi (KLT) object tracking algorithm. 3D location of each feature is calculated in all the images using triangulation and a 3D point cloud is formed. The Point cloud is processed to remove outliers and holes. The 3D face is then formed by interpolating and meshing the point cloud. This computationally efficient and low cost technique outperforms commercial and online available software and published algorithms.}, 
keywords={face recognition;feature extraction;image reconstruction;object tracking;solid modelling;virtual reality;dense reconstruction;facial features;human tracking;3D facial reference model;contemporary facial reconstruction techniques;face localization;facial feature point extraction;facial images;point cloud;3D human face;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Skin;Image color analysis;Cameras;3D Reconstruction;meshing;point cloud;tracking;triangulation}, 
doi={10.1109/CoDIT.2017.8125644}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7175983, 
author={W. {Gutfeter} and A. {Pacut}}, 
booktitle={2015 IEEE 2nd International Conference on Cybernetics (CYBCONF)}, 
title={Face 3D biometrics goes mobile: Searching for applications of portable depth sensor in face recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={489-494}, 
abstract={This paper presents an acquisition procedure and method of processing spatial images for face recognition with the use of a novel type of scanning device, namely mobile depth sensor Structure. Depth sensors, often called RGBD cameras, are able to deliver 3D images with a frame rate 30-60 frames per second, however they have relatively low resolution and a high level of noise. This kind of data is compared here with a high quality scans enrolled by the structural light scanner, for which the acquisition time is approximately 1.5 s for a single image, and which - because of its size - cannot be classified as a portable device. The purpose of this work was to find the method that will allow us to extract spatial features from mobile data sources analyzed here only in a static context. We transform the 3D data into local surface features and then into vectors of unified length by use of the Moving Least Squares method applied to a predefined grid of points on a reference cylinder. The feature matrices were calculated for various image features, and used in PCA analysis. Finally, the verification errors were calculated and compared to those obtained for stationary devices. The results show that single-image mobile sensor images lead to the results inferior to those of stationary sensors. However, we suggest a dynamic depth stream processing as the next step in the evolution of the described method. The presented results show that by including multi-frame processing into our method, it is likely to gain the accuracy similar to those obtained for a stationary device under controlled laboratory conditions.}, 
keywords={cameras;face recognition;feature extraction;image sensors;least squares approximations;principal component analysis;face 3D biometrics;portable depth sensor;acquisition procedure;spatial images processing;face recognition;scanning device;mobile depth sensor structure;RGBD cameras;3D images;frame rate;high quality scans;structural light scanner;acquisition time;spatial features extraction;mobile data sources;3D data;local surface features;vectors;moving least squares method;feature matrices;image features;PCA analysis;verification errors;stationary devices;single-image mobile sensor images;stationary sensors;dynamic depth stream processing;Face;Three-dimensional displays;Databases;Face recognition;Principal component analysis;Robot sensing systems;Approximation methods}, 
doi={10.1109/CYBConf.2015.7175983}, 
ISSN={}, 
month={June},}
@ARTICLE{8408720, 
author={R. S. {Siqueira} and G. R. {Alexandre} and J. M. {Soares} and G. A. P. {Thé}}, 
journal={IEEE Robotics and Automation Letters}, 
title={Triaxial Slicing for 3-D Face Recognition From Adapted Rotational Invariants Spatial Moments and Minimal Keypoints Dependence}, 
year={2018}, 
volume={3}, 
number={4}, 
pages={3513-3520}, 
abstract={This letter presents a multiple slicing model for threedimensional (3-D) images of human face, using the frontal, sagittal, and transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2-D spatial moments of Hu and 3-D spatial invariant rotation moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs nonneutral ROC I experiment, applying linear discriminant analysis as classifier and more than one sample for training, achieved 98.7% of verification rate at 0.1% of false acceptance rate. By using the support vector machine as classifier the rank1 experiment recognition rates of 99% and 95.4% have been achieved for a neutral vs neutral and for a neutral vs nonneutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when anger and disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity.}, 
keywords={face recognition;feature extraction;image classification;support vector machines;verification rate;false acceptance rate;support vector machine;rank1 experiment recognition rates;Bosphorus Database;3-D face recognition;multiple slicing model;human face;orthogonal planes;nose tip;facial recognition;3-D spatial invariant rotation moments;neutral vs nonneutral ROC;linear discriminant analysis;adapted rotational invariant spatial moments;minimal keypoint dependence;Three-dimensional displays;Face;Feature extraction;Nose;Two dimensional displays;Robustness;Iterative closest point algorithm;Computer vision for automation;recognition;surveillance systems}, 
doi={10.1109/LRA.2018.2854295}, 
ISSN={2377-3766}, 
month={Oct},}
@INPROCEEDINGS{8373915, 
author={W. {Tian} and F. {Liu} and Q. {Zhao}}, 
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)}, 
title={Landmark-Based 3D Face Reconstruction from an Arbitrary Number of Unconstrained Images}, 
year={2018}, 
volume={}, 
number={}, 
pages={774-779}, 
abstract={In this paper, we propose a novel method for reconstructing 3D faces from 2D images. The method is characterized in three aspects. (i) It utilizes only geometric cues in the input images, i.e., 2D facial landmarks. (ii) It works for an arbitrary number of unconstrained images, both single and multiple images. (iii) It can effectively exploit complementary information in multiple images of varying poses and expressions. The method is implemented based on cascaded regression in shape space. We have evaluated the method on three databases and observed from the experimental results that (i) the reconstruction error is reduced as more images of different poses are used, (ii) the proposed method can obtain comparable reconstruction results by using state-of-the-art automated methods to detect the 2D landmarks, and (iii) the proposed method is robust to variations in facial expressions and image qualities.}, 
keywords={face recognition;image reconstruction;regression analysis;stereo image processing;cascaded regression;image qualities;facial expressions;reconstruction error;2D facial landmarks;geometric cues;unconstrained images;3D face reconstruction;Conferences;Face;Gesture recognition;3D face reconstruction;unconstrained images;landmark based;cascade regression}, 
doi={10.1109/FG.2018.00122}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7852823, 
author={ and }, 
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={Joint subspace learning for reconstruction of 3D facial dynamic expression from single image}, 
year={2016}, 
volume={}, 
number={}, 
pages={820-824}, 
abstract={Recently, the synthesis of 3D dynamic expressions has become an important concern in computer graphics, facial recognition, etc. In this study, we propose a regression based joint subspace learning method for the automatic synthesis of 3D dynamic expression images. This method synthesizes 3D dynamic expression images from a single 2D facial image. We use two subspaces (the view subspace and the frame subspace) to synthesize a 3D image. First, we use the view subspace to estimate multi-view facial images from a front image. Next, we construct a 3D image using the estimated multi-view facial images. Finally, we estimate the 3D images in different frames by using the frame subspace to synthesis 3D dynamic expression images. This approach is unlike the conventional joint subspace learning in which, the coefficients estimated by the input image are directly used for synthesis. Furthermore, we propose using textural information to improve the accuracy of synthesized images.}, 
keywords={computer graphics;estimation theory;face recognition;image reconstruction;learning (artificial intelligence);regression analysis;stereo image processing;3D facial dynamic expression reconstruction;single image;computer graphics;facial recognition;regression based joint subspace learning;multiview facial image estimation;Three-dimensional displays;Shape;Image reconstruction;Two dimensional displays;Principal component analysis;Training;Joints;3D dynamic expressions;multi-view facial shape;joint subspace learning;regression}, 
doi={10.1109/CISP-BMEI.2016.7852823}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8518089, 
author={S. {Li} and L. {Su} and Y. {Liu} and Z. {He}}, 
booktitle={IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium}, 
title={Segmentation of Individual Trees Based on a Point Cloud Clustering Method Using Airborne Lidar Data}, 
year={2018}, 
volume={}, 
number={}, 
pages={7520-7523}, 
abstract={The objective of this paper was to develop a new algorithm to segment individual trees directly by using the three-dimensional space characteristic of airborne light detection and ranging point cloud data. The local maximum method was used in the initial segmentation and the error identification tree exclusion. On the basis of the point cloud spatial distribution of individual trees and the adjacent relationship with the other trees, a point cloud clustering method was developed to decide the points belonging to the individual trees. This algorithm was tested by 6 forest plots in the Genhe forestry reserve. The results showed that this algorithm could segment individual trees quickly and accurately, and the overall accuracy of this algorithm was 96.3%.}, 
keywords={forestry;geophysical image processing;image segmentation;optical radar;pattern clustering;remote sensing by laser beam;vegetation mapping;segment individual trees;point cloud clustering method;airborne lidar data;three-dimensional space characteristic;airborne light detection;ranging point cloud data;local maximum method;initial segmentation;error identification tree exclusion;point cloud spatial distribution;Genhe forestry reserve;Vegetation;Three-dimensional displays;Forestry;Laser radar;Clustering algorithms;Remote sensing;Lasers;LiDAR;segmentation;tree;clustering}, 
doi={10.1109/IGARSS.2018.8518089}, 
ISSN={2153-7003}, 
month={July},}
@INPROCEEDINGS{7797090, 
author={S. Z. {Gilani} and A. {Mian}}, 
booktitle={2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)}, 
title={Towards Large-Scale 3D Face Recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases.}, 
keywords={convolution;face recognition;feature extraction;image matching;learning (artificial intelligence);solid modelling;large-scale 3D face recognition;fully convolutional deep network;FCDN;seed points extraction;level set curves;single curvature dependent adaptive speed;dense correspondence;training set;surface matching;morphable model fitting;landmark localization results;FRGCv2 dataset;Bosphorus dataset;query face recognition;Three-dimensional displays;Face;Face recognition;Solid modeling;Databases;Two dimensional displays;Robustness}, 
doi={10.1109/DICTA.2016.7797090}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7428562, 
author={J. {Liu} and Q. {Zhang} and C. {Tang}}, 
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
title={Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information}, 
year={2015}, 
volume={}, 
number={}, 
pages={276-284}, 
abstract={3D facial landmarks play a significant role in many 3D facial studies. Due to the complex geometry of high resolution 3D human face, landmark detection remains to be a challenge problem. This paper proposes a novel method to detect landmarks automatically for high resolution 3D faces without learning or training, solely using geometric information. For high resolution 3D faces, geodesic remeshing is used to reduce the vertices number, then the remeshed 3D faces are parameterized and interpolated on a regular grid, which enable us to compute the differential geometric features more efficiently and accurately. To detect landmarks, we extract global and local constraints for each landmark by considering relative positions of landmarks and differential geometric features, then landmarks are detected by combine these constraints. We evaluate our method and compare it with other methods which are mostly related to ours in a large scale publicly available 3D face data set, BJUT-3D face database. The experimental results show that our method is robust and effective, and achieves better performance than existing methods.}, 
keywords={face recognition;feature extraction;image resolution;interpolation;mesh generation;object detection;automatic landmark detection;high resolution nonrigid 3D faces;geometric information;3D facial landmarks;geodesic remeshing;vertices number;parameterization;interpolation;regular grid;differential geometric features;global constraints extraction;local constraints extraction;Three-dimensional displays;Indexes;Shape;Training;Feature extraction;Facial animation;Mesh generation;3D faces;landmarks;geometric information;geodesic remeshing;differential geometric features}, 
doi={10.1109/IAEAC.2015.7428562}, 
ISSN={}, 
month={Dec},}
@ARTICLE{7312454, 
author={M. A. {de Jong} and A. {Wollstein} and C. {Ruff} and D. {Dunaway} and P. {Hysi} and T. {Spector} and F. {Liu} and W. {Niessen} and M. J. {Koudstaal} and M. {Kayser} and E. B. {Wolvius} and S. {Böhringer}}, 
journal={IEEE Transactions on Image Processing}, 
title={An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}, 
year={2016}, 
volume={25}, 
number={2}, 
pages={580-588}, 
abstract={In this paper, we present a novel approach to automatic 3D facial landmarking using 2D Gabor wavelets. Our algorithm considers the face to be a surface and uses map projections to derive 2D features from raw data. Extracted features include texture, relief map, and transformations thereof. We extend an established 2D landmarking method for simultaneous evaluation of these data. The method is validated by performing landmarking experiments on two data sets using 21 landmarks and compared with an active shape model implementation. On average, landmarking error for our method was 1.9 mm, whereas the active shape model resulted in an average landmarking error of 2.3 mm. A second study investigating facial shape heritability in related individuals concludes that automatic landmarking is on par with manual landmarking for some landmarks. Our algorithm can be trained in 30 min to automatically landmark 3D facial data sets of any size, and allows for fast and robust landmarking of 3D faces.}, 
keywords={face recognition;feature extraction;Gabor filters;wavelet transforms;automatic 3D facial landmarking algorithm;2D Gabor wavelets;map projections;feature extraction;data sets;active shape model;landmarking error;facial shape heritability;automatic landmarking;Three-dimensional displays;Face;Ellipsoids;Accuracy;Training;Solid modeling;Nose;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Algorithms;Anatomic Landmarks;Face;Humans;Imaging, Three-Dimensional;Pattern Recognition, Automated;Wavelet Analysis}, 
doi={10.1109/TIP.2015.2496183}, 
ISSN={1057-7149}, 
month={Feb},}
@INPROCEEDINGS{8525654, 
author={M. P. {Zapf} and A. {Gupta} and L. Y. {Morales Saiki} and M. {Kawanabe}}, 
booktitle={2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}, 
title={Data-Driven, 3-D Classification of Person-Object Relationships and Semantic Context Clustering for Robotics and AI Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={180-187}, 
abstract={We introduce a framework for detection and classification of spatio-temporal person-object interactions. Our method clusters similar semantic contexts from interactions detected from RGB-D data. 2-D object detection (YOLO) is run on RGB data from a Kinect v2 sensor on a mobile robot navigating an office and observing persons and desk spaces. Person and object detections are converted into 3-D point cloud time series via RGB-Depth co-registration and successive Euclidean and k-means spatial clustering. 3-D person and object point cloud streams are used to create time-series occupancy maps and person-object co-localization maps. From these maps, spatiotemporal correlations between persons and distinct objects are computed. Correlation patterns are clustered using k-means to obtain distinct human-object interactions, i.e. segment semantic context over time. We evaluated the performance of our approach to detect person-object correlations and cluster semantic context by recording 90 30-second RGB-D data episodes, with three persons handling representative objects (books, cups, bottles). Experimental results show that our framework is able to consistently assign semantic context to the same cluster in > 79% of cases (scene frames). Semantic contexts in visual scenes can be distinguished without the need to provide prior information, allowing mobile agents to learn and explore in new environments.}, 
keywords={feature extraction;image classification;image colour analysis;image motion analysis;image registration;image segmentation;mobile robots;object detection;path planning;pattern clustering;robot vision;time series;data-driven;3D classification;k-means spatial clustering;spatio-temporal person-object interaction detection;spatio-temporal person-object interaction classification;2D object detection;3D point cloud time series;RGB-depth coregistration;Euclidean clustering;person-object colocalization maps;correlation pattern clustering;semantic context segmentation;mobile agents;RGB-D data episodes;person-object correlations;human-object interactions;spatiotemporal correlations;time-series occupancy maps;object point cloud streams;mobile robot;Kinect v2 sensor;RGB data;semantic context clustering;person-object relationships;Three-dimensional displays;Robot sensing systems;Semantics;Object detection;Feature extraction;Correlation}, 
doi={10.1109/ROMAN.2018.8525654}, 
ISSN={1944-9437}, 
month={Aug},}
@INPROCEEDINGS{7163161, 
author={S. {Cheng} and I. {Marras} and S. {Zafeiriou} and M. {Pantic}}, 
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
title={Active nonrigid ICP algorithm}, 
year={2015}, 
volume={1}, 
number={}, 
pages={1-8}, 
abstract={The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15-20 years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region.}, 
keywords={face recognition;statistical analysis;facial surfaces;iterative closest point algorithm;nonrigid iterative closest point algorithms;3D face fitting;mouth region;statistical 3D model;3D facial deformation;single facial model;3D facial shape;3D mesh;3D facial model;active nonrigid ICP algorithm;Face;Three-dimensional displays;Iterative closest point algorithm;Solid modeling;Deformable models;Shape;Mouth}, 
doi={10.1109/FG.2015.7163161}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8035340, 
author={L. {Han} and Q. {Xiao} and X. {Liang}}, 
booktitle={2017 International Conference on Computer, Information and Telecommunication Systems (CITS)}, 
title={3D face reconstruction based on progressive cascade regression}, 
year={2017}, 
volume={}, 
number={}, 
pages={297-301}, 
abstract={In order to better learn the distributions of 2D and 3D faces and the mapping between them with limited training samples, a new 3D face reconstruction method based on progressive cascade regression is proposed. Firstly, it learns the mapping between 2D and 3D facial landmarks to estimate the initial 3D facial landmarks with a coupled space learning method. Secondly, a deformed space is constructed with the difference between the estimated initial landmarks and the ground truth of training samples; and more accurate 3D facial landmarks are reconstructed by modifying the initial 3D ones with shape compensations which are calculated by minimizing an objective function. Finally, the realistic 3D faces are reconstructed by a method that is based on a simple sparse regulation and shape deformation. The results on BJUT 3D face database demonstrate the effectiveness of the proposed method. In addition, compared with some typical methods, the method can get better subjective and objective results, especially in details.}, 
keywords={face recognition;image reconstruction;learning (artificial intelligence);regression analysis;solid modelling;3D face reconstruction;progressive cascade regression;training samples;facial landmark mapping;coupled space learning;deformed space construction;sparse regulation;shape deformation;Three-dimensional displays;Face;Training;Two dimensional displays;Image reconstruction;Solid modeling;Shape}, 
doi={10.1109/CITS.2017.8035340}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8099972, 
author={L. {Sheng} and J. {Cai} and T. {Cham} and V. {Pavlovic} and K. N. {Ngan}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Generative Model for Depth-Based Robust 3D Facial Pose Tracking}, 
year={2017}, 
volume={}, 
number={}, 
pages={4598-4607}, 
abstract={We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face models visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.}, 
keywords={face recognition;object tracking;pose estimation;ray tracing;statistical analysis;generative model;arbitrary facial expression variations;generative framework;face models visibility;depth-based robust 3D facial pose tracking;unconstrained scenarios;face model adaptation;statistical 3D face model;uncertainty prediction;distribution prediction;ray visibility constraint;Biwi datasets;ICT-3DHP datasets;Face;Three-dimensional displays;Solid modeling;Adaptation models;Robustness;Shape;Probabilistic logic}, 
doi={10.1109/CVPR.2017.489}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{7428566, 
author={J. {Liu} and Q. {Zhang} and C. {Tang}}, 
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
title={CoMES: A novel method for robust nose tip detection in face range images}, 
year={2015}, 
volume={}, 
number={}, 
pages={309-315}, 
abstract={As the most distinct feature point in facial landmarks, nose tip plays a significant role in 3D facial studies. Successful detection of nose tip can facilitate many 3D facial studies tasks. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, need not training, can handle large rotations and occlusions. We first remove small isolated connected regions and noise from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, we compute multi-angle energy of each point, then we use hierarchical clustering method to cluster the points whose multi-angle energies are larger than a threshold value. In the largest cluster, we can find one point with the largest multi-angle energy. For all scales of the scale-space, we get a series of such points and apply hierarchical clustering again for these points, nose tip will have the largest multi-angle energy in the largest cluster. We evaluate our method in FRGC v2.0 3D face database and BOSPHORUS 3D face database. The experimental results verify the robustness of our method with a high nose tip detection rate.}, 
keywords={edge detection;face recognition;pattern clustering;solid modelling;CoMES;robust nose tip detection;face range images;facial landmark;3D facial studies;scale-space;multiangle energy;hierarchical clustering method;FRGC v2.0 3D face database;BOSPHORUS 3D face database;Nose;Face;Three-dimensional displays;Training;Robustness;Feature extraction;Smoothing methods;nose tip;3D faces;scale-space;multi-angle energy;hierarchical clustering;range images}, 
doi={10.1109/IAEAC.2015.7428566}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7852696, 
author={F. {Li} and C. {Lai} and S. {Jin} and Y. {Peng}}, 
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={Automatic calibration of 3D human faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={135-139}, 
abstract={This paper presents a method for correcting the posture of 3D face with unknown position and posture based on the standard face pose. We define a standard posture of face which is not affected by the head gesture. Then the algorithms of principal component analysis and iterative closest point are used to achieve the preliminary adjustment of the target model. For the precise calibration, we use the method of depth values iterative searching to find the point of nose precisely, and propose an algorithm of searching along a straight line to find the ridge line of nose. Ultimately we determine the difference between the posture of the target model and standard posture to obtain accurate calculation of the target model posture.}, 
keywords={calibration;computer graphics;face recognition;principal component analysis;automatic calibration;3D human faces;standard face pose;standard posture;head gesture;principal component analysis;iterative closest point;precise calibration;iterative searching;target model posture;Three-dimensional displays;Calibration;Nose;Standards;Iterative closest point algorithm;Solid modeling;Principal component analysis;3D Faces;Posture Correction;Face Calibration;Facial Feature Points}, 
doi={10.1109/CISP-BMEI.2016.7852696}, 
ISSN={}, 
month={Oct},}
@ARTICLE{7973095, 
author={S. Z. {Gilani} and A. {Mian} and F. {Shafait} and I. {Reid}}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Dense 3D Face Correspondence}, 
year={2018}, 
volume={40}, 
number={7}, 
pages={1584-1598}, 
abstract={We present an algorithm that automatically establishes dense correspondences between a large number of 3D faces. Starting from automatically detected sparse correspondences on the outer boundary of 3D faces, the algorithm triangulates existing correspondences and expands them iteratively by matching points of distinctive surface curvature along the triangle edges. After exhausting keypoint matches, further correspondences are established by generating evenly distributed points within triangles by evolving level set geodesic curves from the centroids of large triangles. A deformable model (K3DM) is constructed from the dense corresponded faces and an algorithm is proposed for morphing the K3DM to fit unseen faces. This algorithm iterates between rigid alignment of an unseen face followed by regularized morphing of the deformable model. We have extensively evaluated the proposed algorithms on synthetic data and real 3D faces from the FRGCv2, Bosphorus, BU3DFE and UND Ear databases using quantitative and qualitative benchmarks. Our algorithm achieved dense correspondences with a mean localisation error of 1.28 mm on synthetic faces and detected 14 anthropometric landmarks on unseen real faces from the FRGCv2 database with 3 mm precision. Furthermore, our deformable model fitting algorithm achieved 98.5 percent face recognition accuracy on the FRGCv2 and 98.6 percent on Bosphorus database. Our dense model is also able to generalize to unseen datasets.}, 
keywords={face recognition;image matching;iterative methods;stereo image processing;automatically detected sparse correspondences;distinctive surface curvature;triangle edges;keypoint matches;K3DM;FRGCv2;deformable model fitting algorithm;face recognition;3D face correspondence;Face;Three-dimensional displays;Shape;Deformable models;Solid modeling;Databases;Face recognition;Dense correspondence;3D face;morphing;keypoint detection;level sets;geodesic curves;deformable model}, 
doi={10.1109/TPAMI.2017.2725279}, 
ISSN={0162-8828}, 
month={July},}
@INPROCEEDINGS{8490988, 
author={V. F. {Abrevaya} and S. {Wuhrer} and E. {Boyer}}, 
booktitle={2018 International Conference on 3D Vision (3DV)}, 
title={Spatiotemporal Modeling for Efficient Registration of Dynamic 3D Faces}, 
year={2018}, 
volume={}, 
number={}, 
pages={371-380}, 
abstract={We consider the registration of temporal sequences of 3D face scans. Face registration plays a central role in face analysis applications, for instance recognition or transfer tasks, among others. We propose an automatic approach that can register large sets of dynamic face scans without the need for landmarks or highly specialized acquisition setups. This allows for extended versatility among registered face shapes and deformations by enabling to leverage multiple datasets, a fundamental property when e.g. building statistical face models. Our approach is built upon a regression-based static registration method, which is improved by spatiotemporal modeling to exploit redundancies over both space and time. We experimentally demonstrate that accurate registrations can be obtained for varying data robustly and efficiently by applying our method to three standard dynamic face datasets.}, 
keywords={face recognition;image registration;image sequences;regression analysis;temporal sequences;3D face scans;face registration;face analysis applications;instance recognition;dynamic face scans;registered face shapes;regression-based static registration method;spatiotemporal modeling;standard dynamic face datasets;dynamic 3D face registration;statistical face models;Three-dimensional displays;Spatiotemporal phenomena;Registers;Solid modeling;Shape;Dynamics;Data models;Registration;3D Face;Spatiotemporal}, 
doi={10.1109/3DV.2018.00050}, 
ISSN={2475-7888}, 
month={Sep.},}
@INPROCEEDINGS{8100068, 
author={W. {Peng} and Z. {Feng} and C. {Xu} and Y. {Su}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Parametric T-Spline Face Morphable Model for Detailed Fitting in Shape Subspace}, 
year={2017}, 
volume={}, 
number={}, 
pages={5515-5523}, 
abstract={Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration for the synthesis of 3D faces by assuming that faces are in a linear class. However, the human face is in a nonlinear manifold, and a new test are always not in the pre-learnt subspace accurately because of the disparity brought by ethnicity, age, gender, etc. In the paper, we propose a parametric T-spline morphable model (T-splineMM) for 3D face representation, which has great advantages of fitting data from an unknown source accurately. In the model, we describe a face by C^2 T-spline surface, and divide the face surface into several shape units (SUs), according to facial action coding system (FACS), on T-mesh instead of on the surface directly. A fitting algorithm is proposed to optimize coefficients of T-spline control point components along pre-learnt identity and expression subspaces, as well as to optimize the details in refinement progress. As any pre-learnt subspace is not complete to handle the variety and details of faces and expressions, it covers a limited span of morphing. SUs division and detail refinement make the model fitting the facial muscle deformation in a larger span of morphing subspace. We conduct experiments on face scan data, kinect data as well as the space-time data to test the performance of detail fitting, robustness to missing data and noise, and to demonstrate the effectiveness of our model. Convincing results are illustrated to demonstrate the effectiveness of our model compared with the popular methods.}, 
keywords={face recognition;feature extraction;image morphing;image reconstruction;mesh generation;muscle;solid modelling;splines (mathematics);shape units;facial action coding system;fitting algorithm;T-spline control point components;expression subspaces;face scan data;parametric T-spline face morphable model;shape subspace pre-learnt subspace;human face;parametric T-spline morphable model for 3D;T-spline surface;face surface;kinect data;space-time data;Face;Three-dimensional displays;Shape;Solid modeling;Lips;Splines (mathematics);Computational modeling}, 
doi={10.1109/CVPR.2017.585}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{7391814, 
author={ and and and }, 
booktitle={2015 International Conference on 3D Imaging (IC3D)}, 
title={Robust nose tip detection for face range images based on local features in scale-space}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Being the most distinct feature point in 3D facial landmarks, nose tip plays a significant role in 3D facial studies such as face detection, face recognition, facial features extraction, face alignment, etc. Successful detection of nose tip can facilitate many tasks of 3D facial studies. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, needs not training, can handle large rotations and occlusions. To reduce computational cost, we first remove small isolated regions from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, the Multi-angle Energy (ME) of each point is computed and sorted in descending order. Then the first m points in the descending order list are obtained and hierarchical clustering method is used to cluster these points. In the first h largest clusters, we can find one point with the largest ME. For all scales of the scale-space, we get a series of such points which are treated as nose tip candidates. For these candidates, we apply hierarchical clustering again. In the obtained largest cluster, we compute the mean value of ME. The ME of nose tip will be closest to the mean value. We evaluate our method in two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The experimental results verify the robustness of our method with a high nose tip detection rate.}, 
keywords={face recognition;feature extraction;object detection;pattern clustering;nose tip detection;face range images;local features;3D facial landmarks;3D facial studies;multiangle energy;ME;range image smoothing;hierarchical clustering method;FRGC database;BOSPHORUS database;Nose;Three-dimensional displays;Face;Robustness;Training;Feature extraction;Smoothing methods;nose tip;3D faces;range images;robust smoothing;normal;scale-space;multi-angle energy;sphere fitting;least square;hierarchical clustering}, 
doi={10.1109/IC3D.2015.7391814}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7780900, 
author={T. {Bolkart} and S. {Wuhrer}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Robust Multilinear Model Learning Framework for 3D Faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={4911-4919}, 
abstract={Multilinear models are widely used to represent the statistical variations of 3D human faces as they decouple shape changes due to identity and expression. Existing methods to learn a multilinear face model degrade if not every person is captured in every expression, if face scans are noisy or partially occluded, if expressions are erroneously labeled, or if the vertex correspondence is inaccurate. These limitations impose requirements on the training data that disqualify large amounts of available 3D face data from being usable to learn a multilinear model. To overcome this, we introduce the first framework to robustly learn a multilinear model from 3D face databases with missing data, corrupt data, wrong semantic correspondence, and inaccurate vertex correspondence. To achieve this robustness to erroneous training data, our framework jointly learns a multilinear model and fixes the data. We evaluate our framework on two publicly available 3D face databases, and show that our framework achieves a data completion accuracy that is comparable to state-of-the-art tensor completion methods. Our method reconstructs corrupt data more accurately than state-of-the-art methods, and improves the quality of the learned model significantly for erroneously labeled expressions.}, 
keywords={computer graphics;face recognition;learning (artificial intelligence);visual databases;robust multilinear model learning framework;3D faces;statistical variations;3D human faces;multilinear face model;face scans;3D face databases;missing data;corrupt data;semantic correspondence;inaccurate vertex correspondence;erroneous training data;data completion accuracy;tensor completion methods;Data models;Computational modeling;Three-dimensional displays;Solid modeling;Semantics;Robustness;Shape}, 
doi={10.1109/CVPR.2016.531}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7888183, 
author={R. {Chellappa} and J. {Chen} and R. {Ranjan} and S. {Sankaranarayanan} and A. {Kumar} and V. M. {Patel} and C. D. {Castillo}}, 
booktitle={2016 Information Theory and Applications Workshop (ITA)}, 
title={Towards the design of an end-to-end automated system for image and video-based recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Over many decades, researchers working in object recognition have longed for an end-to-end automated system that will simply accept 2D or 3D image or videos as inputs and output the labels of objects in the input data. Computer vision methods that use representations derived based on geometric, radiometric and neural considerations and statistical and structural matchers and artificial neural network-based methods where a multi-layer network learns the mapping from inputs to class labels have provided competing approaches for image recognition problems. Over the last four years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements on object detection/recognition challenge problems. This has been made possible due to the availability of large annotated data, a better understanding of the non-linear mapping between image and class labels as well as the affordability of GPUs. In this paper, we present a brief history of developments in computer vision and artificial neural networks over the last forty years for the problem of image-based recognition. We then present the design details of a deep learning system for end-to-end unconstrained face verification/recognition. Some open issues regarding DCNNs for object recognition problems are then discussed. We caution the readers that the views expressed in this paper are from the authors and authors only!}, 
keywords={computer vision;convolution;face recognition;image representation;learning (artificial intelligence);neural nets;object detection;object recognition;video signal processing;end-to-end automated system;image-based recognition;video-based recognition;object recognition;computer vision;image representations;geometric considerations;radiometric considerations;neural considerations;statistical matchers;structural matchers;artificial neural network;multilayer network;deep convolutional neural networks;DCNNs;object detection;deep learning system;end-to-end unconstrained face recognition;Face;Computer vision;Object recognition;Machine learning;Videos;Image recognition;Feature extraction}, 
doi={10.1109/ITA.2016.7888183}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7899906, 
author={H. X. {Pham} and V. {Pavlovic} and and }, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Robust real-time performance-driven 3D face tracking}, 
year={2016}, 
volume={}, 
number={}, 
pages={1851-1856}, 
abstract={We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker capable of handling a wide range of target scene depths, whose performances are demonstrated in our extensive experiments better than those of the state-of-the-arts.}, 
keywords={cameras;computational geometry;face recognition;image colour analysis;object tracking;optimisation;solid modelling;stereo image processing;video signal processing;3D face tracking;RGBD video streams;head pose tracking;facial actions;cameras;point cloud;3D shape regressor;joint 2D+3D optimization;3D model reconstruction;Three-dimensional displays;Shape;Face;Training;Two dimensional displays;Optimization;Solid modeling}, 
doi={10.1109/ICPR.2016.7899906}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8074488, 
author={T. {Indumathi} and M. {Pushparani}}, 
booktitle={2017 World Congress on Computing and Communication Technologies (WCCCT)}, 
title={Multimodel Human Authentication by Matching 3D Skull and Gait}, 
year={2017}, 
volume={}, 
number={}, 
pages={38-42}, 
abstract={This research paper focuses multimodal human identifications that are captured in a web cam as images of face and walking style. The image can then be considered for further analyses of gait and skull characteristics as per Human Identification Systems. We propose to identify a skull by using a correlation measure between the 3D skull and 3D face in terms of morphology, and measure the correlation using Enhance Canonical Correlation Coefficient Analysis (ECCCA). We use the 3D skull data as the probe and 3D face geometric data as the gallery and match the skull with enrolled 3D faces by the correlation measure between the Probe and the Gallery. This paper proposes Uncorrelated Multilinear Discriminant Analysis (UMLDA) algorithm for the challenging problem of Gait Recognition. Finally, Neural Network for mat-lab tool is used for training and testing purpose. We have created different model of neural network based on hidden layer, selection of training algorithm and setting the different parameter for training. And then, we will test for the combination of NN+SVM, Knearest Neighbour Classification. Here all these experiments are done on CASIA gait database and input video.}, 
keywords={face recognition;gait analysis;image classification;image matching;learning (artificial intelligence);neural nets;support vector machines;3D skull;mat-lab tool;hidden layer;training algorithm;NN-SVM;Knearest neighbour classification;CASIA gait database;input video;neural network;Gait Recognition;Uncorrelated Multilinear Discriminant Analysis algorithm;gallery;3D face geometric data;Enhance Canonical Correlation Coefficient Analysis;Human Identification Systems;skull characteristics;walking style;web cam;multimodal human identifications;Face;Feature extraction;Training;Three-dimensional displays;Databases;Correlation;Algorithm design and analysis;Enhance Canonical Correlation Coefficient Analysis(ECCCA);Skull Identification;Gait identification;UMLDA;Neural Network}, 
doi={10.1109/WCCCT.2016.19}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8350702, 
author={R. {Siv} and I. {Ardiyanto} and R. {Hartanto}}, 
booktitle={2018 International Conference on Information and Communications Technology (ICOIACT)}, 
title={3D human face reconstruction using depth sensor of Kinect 2}, 
year={2018}, 
volume={}, 
number={}, 
pages={355-359}, 
abstract={3D Human Face Reconstruction or 3D Human Face Modeling is a construction of 3D geometry graphics of human face model which has been a very challenging research in both computer vision and computer graphics for over the last three decades. Researchers have proposed many methods for 3D human face reconstruction by using or not using 3D scanner. One of the affordable 3D scanner today is Microsoft Kinect which is well known as a motion sensor add-on for Xbox gaming console. By using the depth sensor of Kinect version 2, 3D human face reconstruction is possible to construct, yet the depth information from Kinect scanning alone may not produce a good quality of 3D model which could lose the details of the 3D face; therefore, the optimization and improvement are needed. This research is mainly aimed to reconstruct the 3D human face model by using a single shot of 3D depth information or point cloud from the depth sensor of Microsoft Kinect version 2, and then, reconstruct with the proposed method including Poisson Surface Reconstruction. The results come as expected and are measured by comparing to the result of the zoomed of original RGB point cloud which looks like a 3D model and to the result of Microsoft 3D applications.}, 
keywords={face recognition;geometry;image reconstruction;stochastic processes;3D human face reconstruction;depth sensor;3D Human Face Modeling;3D geometry graphics;Kinect 2;Poisson Surface Reconstruction;Three-dimensional displays;Face;Solid modeling;Image reconstruction;Surface reconstruction;Surface treatment;Computational modeling;3D Face Reconstruction;3D Face Modeling;Kinect;Surface Reconstruction}, 
doi={10.1109/ICOIACT.2018.8350702}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7550062, 
author={ and D. {Huang} and Y. {Wang} and J. {Sun}}, 
booktitle={2016 International Conference on Biometrics (ICB)}, 
title={Lock3DFace: A large-scale database of low-cost Kinect 3D faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future.}, 
keywords={emotion recognition;face recognition;image texture;pose estimation;video databases;video signal processing;large-scale database;low-cost Kinect 3D face videos;Lock3DFace database;3D face analysis;3D face recognition;3D FR;expression analysis;pose analysis;occlusion analysis;time lapse analysis;texture analysis;near infrared channels;lighting condition;radiation intensity;Three-dimensional displays;Databases;Videos;Solid modeling;Two dimensional displays;Lighting;Sensors}, 
doi={10.1109/ICB.2016.7550062}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8579077, 
author={J. {Li} and B. M. {Chen} and G. H. {Lee}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={SO-Net: Self-Organizing Network for Point Cloud Analysis}, 
year={2018}, 
volume={}, 
number={}, 
pages={9397-9406}, 
abstract={This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website1.}, 
keywords={computer vision;feature extraction;learning (artificial intelligence);nearest neighbour methods;self-organising feature maps;deep learning;orderless point clouds;SO-Net models;spatial distribution;individual points;SOM nodes;input point cloud;single feature vector;point cloud reconstruction;part segmentation;shape retrieval;point cloud recognition networks;point cloud analysis;permutation invariant architecture;self-organizing map;self-organizing network;point-to-node k nearest neighbor search;SO-Net;hierarchical feature extraction;Three-dimensional displays;Feature extraction;Training;Shape;Two dimensional displays;Graphical models}, 
doi={10.1109/CVPR.2018.00979}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{8634657, 
author={S. A. {Ali Shah} and M. {Bennamoun} and M. {Molton}}, 
booktitle={2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)}, 
title={A Fully Automatic Framework for Prediction of 3D Facial Rejuvenation}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={How will I look afterwards? is a common question asked by the patients undergoing a cosmetic procedure. Cosmetic practitioners at present can only offer subjective and descriptive replies. This subjective prediction is a serious concern for patients undergoing cosmetic treatment and therefore necessitates the development of automatic techniques for facial quantification. This paper proposes a novel machine learning approach to quantify and predict the outcome of 3D facial rejuvenation prior to actual cosmetic procedure. The facial rejuvenation prediction results are achieved by estimating the dermal filler volume in 3D faces. This involves estimation of structural changes in 3D face images and to learn underlying structural mapping. Our preliminary experimental results show that the proposed model achieves superior prediction accuracy on real world dataset compared to baseline methods. The computational time analysis shows that the proposed technique is very efficient (at test time) which makes it suitable for real time applications.}, 
keywords={cosmetics;face recognition;learning (artificial intelligence);superior prediction accuracy;fully automatic framework;3D facial rejuvenation;common question;cosmetic practitioners;cosmetic treatment;automatic techniques;facial quantification;actual cosmetic procedure;facial rejuvenation prediction results;3D face images;Three-dimensional displays;Prediction algorithms;Solid modeling;Predictive models;Training;Machine learning;Australia;Facial rejuvenation;Predictive Modeling;Machine Learning}, 
doi={10.1109/IVCNZ.2018.8634657}, 
ISSN={2151-2205}, 
month={Nov},}
@INPROCEEDINGS{8003592, 
author={A. S. {Asl} and M. A. {Oskoei}}, 
booktitle={2017 5th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)}, 
title={Depth dependent invariant features applied to person detection using 3D camera}, 
year={2017}, 
volume={}, 
number={}, 
pages={29-34}, 
abstract={This paper is about detection and tracking a person by mobile robots in in-door environments, such as shopping center and hospital. It uses vision based approaches to recognize texture of clothes. The paper proposes a method to use depth (distance) reference along with scale invariant features (SIFT) to recognize patterns in various orientation, distance and illumination. SIFT is an important feature detection algorithm that is robust against rotation, translation, and scaling in 2D images and to some extent against variations in lighting conditions. But it suffers inadequate performance for visual patterns rotated in 3D space. To overcome this issue, reference inputs given to the algorithm was extended to include images taken from different angles. The proposed algorithm showed considerably improved performance in detection for real-time applications.}, 
keywords={feature extraction;image sensors;image texture;mobile robots;object recognition;object tracking;robot vision;transforms;depth dependent invariant features;person detection;3D camera;person tracking;mobile robots;shopping center;hospital;vision based approaches;cloth texture recognition;depth reference;distance reference;scale invariant features;SIFT;pattern recognition;orientation;distance;illumination;feature detection algorithm;Feature extraction;Cameras;Three-dimensional displays;Robot kinematics;Detection algorithms;Robot sensing systems;Scale invariant features;SIFT;3D images;Person detection;mobile robot}, 
doi={10.1109/CFIS.2017.8003592}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7410768, 
author={T. {Bolkart} and S. {Wuhrer}}, 
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={A Groupwise Multilinear Correspondence Optimization for 3D Faces}, 
year={2015}, 
volume={}, 
number={}, 
pages={3604-3612}, 
abstract={Multilinear face models are widely used to model the space of human faces with expressions. For databases of 3D human faces of different identities performing multiple expressions, these statistical shape models decouple identity and expression variations. To compute a high-quality multilinear face model, the quality of the registration of the database of 3D face scans used for training is essential. Meanwhile, a multilinear face model can be used as an effective prior to register 3D face scans, which are typically noisy and incomplete. Inspired by the minimum description length approach, we propose the first method to jointly optimize a multilinear model and the registration of the 3D scans used for training. Given an initial registration, our approach fully automatically improves the registration by optimizing an objective function that measures the compactness of the multilinear model, resulting in a sparse model. We choose a continuous representation for each face shape that allows to use a quasi-Newton method in parameter space for optimization. We show that our approach is computationally significantly more efficient and leads to correspondences of higher quality than existing methods based on linear statistical models. This allows us to evaluate our approach on large standard 3D face databases and in the presence of noisy initializations.}, 
keywords={emotion recognition;face recognition;image registration;Newton method;statistical analysis;visual databases;groupwise multilinear correspondence optimization;multilinear face models;statistical shape models;identity variations;expression variations;high-quality multilinear face model;3D face scan registration;minimum description length approach;quasi-Newton method;parameter space;Three-dimensional displays;Computational modeling;Solid modeling;Shape;Optimization;Tensile stress;Principal component analysis}, 
doi={10.1109/ICCV.2015.411}, 
ISSN={2380-7504}, 
month={Dec},}
@INPROCEEDINGS{8545849, 
author={X. {Sun} and L. {Huang} and C. {Liu}}, 
booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
title={Multimodal Face Spoofing Detection via RGB-D Images}, 
year={2018}, 
volume={}, 
number={}, 
pages={2221-2226}, 
abstract={While it has been shown that using 3D information might significantly benefit face anti-spoofing systems, traditional color images are still generally used, due to several issues such as expensive hardware requirement, high time cost, or poor accessibility when obtaining and using true 3D images. Thus, we could use RGB-D images captured by relatively low cost sensors instead, e.g., Kinect cameras, to achieve better performance without consuming huge amount of time or money. This research presents a novel multimodal face anti-spoofing method, which makes full use of available information on RGB-D images and no manually chosen regions are needed. For every pair of RGB-D images, first of all, we calculate the correlation between color and depth images to detect multimodal properties; then, by analyzing the consistency of sub regions extracted from the depth image, we are able to distinguish flat spoofing faces from genuine human beings. Both anti-spoofing features are fused to make final anti-spoofing decisions. Experiments on both self-collected and pubic 3DMAD datasets show that our proposed approach is effective for intra-dataset and cross-dataset testing scenarios, and that our method could deal with different presentation attacks carried by photos, tablet screens, and face masks.}, 
keywords={face recognition;feature extraction;image colour analysis;image sensors;image texture;multimodal face anti-spoofing method;final anti-spoofing decisions;anti-spoofing features;flat spoofing;depth images;relatively low cost sensors;face anti-spoofing systems;RGB-D images;multimodal face spoofing detection;Face;Image color analysis;Correlation;Three-dimensional displays;Color;Skin;Entropy}, 
doi={10.1109/ICPR.2018.8545849}, 
ISSN={1051-4651}, 
month={Aug},}
@INPROCEEDINGS{7785124, 
author={F. {Maninchedda} and C. {Häne} and M. R. {Oswald} and M. {Pollefeys}}, 
booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
title={Face Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization}, 
year={2016}, 
volume={}, 
number={}, 
pages={489-498}, 
abstract={We present a system which is able to reconstruct human faces on mobile devices with only on-device processing using the sensors which are typically built into a current commodity smart phone. Such technology can for example be used for facial authentication purposes or as a fast preview for further post-processing. Our method uses recently proposed techniques which compute depth maps by passive multi-view stereo directly on the device. We propose an efficient method which recovers the geometry of the face from the typically noisy point cloud. First, we show that we can safely restrict the reconstruction to a 2.5D height map representation. Therefore we then propose a novel low dimensional height map shape model for faces which can be fitted to the input data efficiently even on a mobile phone. In order to be able to represent instance specific shape details, such as moles, we augment the reconstruction from the shape model with a distance map which can be regularized efficiently. We thoroughly evaluate our approach on synthetic and real data, thereby we use both high resolution depth data acquired using high quality multi-view stereo and depth data directly computed on mobile phones.}, 
keywords={face recognition;image reconstruction;image resolution;stereo image processing;face reconstruction;mobile devices;height map shape model;fast regularization;facial authentication;depth maps;passive multiview stereo;face geometry;noisy point cloud;low dimensional height map shape model;high resolution depth data;high quality multiview stereo;Face;Computational modeling;Cameras;Three-dimensional displays;Shape;Solid modeling;Image reconstruction}, 
doi={10.1109/3DV.2016.59}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7869954, 
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and N. {Werghi} and M. {Linguraru}}, 
booktitle={2016 IEEE 59th International Midwest Symposium on Circuits and Systems (MWSCAS)}, 
title={Facial landmarks detection using 3D constrained local model on mesh manifold}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={This paper proposes a novel 3D Constrained Local Models (CLM) approach applied for the detection of facial landmarks in 3D images. This approach capitalizes on the properties of Independent Component Analysis (ICA) to define appropriate priors of a face Point Distribution Model. Tailored to the mesh manifold modality, this approach address the limitations of the depth images which require pose normalization and suffer from the loss of the shape information caused by 2D projection. We validate this framework through a series of experiments conducted with the public Bosporus database, whereby it demonstrates a competitive performance compared to other state of the art methods.}, 
keywords={face recognition;independent component analysis;mesh generation;object detection;facial landmarks detection;3D constrained local model;CLM approach;3D images;independent component analysis;ICA;face point distribution model;mesh manifold modality;depth images;shape information loss;2D projection;public Bosporus database;pose normalization;Shape;Principal component analysis;Face;Three-dimensional displays;Deformable models;Integrated circuit modeling;Adaptation models}, 
doi={10.1109/MWSCAS.2016.7869954}, 
ISSN={1558-3899}, 
month={Oct},}
@ARTICLE{7050293, 
author={G. {Mills} and G. {Fotopoulos}}, 
journal={IEEE Geoscience and Remote Sensing Letters}, 
title={Rock Surface Classification in a Mine Drift Using Multiscale Geometric Features}, 
year={2015}, 
volume={12}, 
number={6}, 
pages={1322-1326}, 
abstract={Scale-dependent statistical depictions of surface morphology offer the potential to parameterize complex geometrical scaling relationships with greater detail than traditional fractal measures. Using multiscale operators, it is possible to identify points belonging to rough discontinuous surfaces in noisy point clouds solely on the basis of their local geometry. Many strategies for point cloud feature classification have been developed since the proliferation of laser scanning systems. Most of the techniques which are applicable to natural scenes employ external data sources such as hyperspectral imagery, return pulse intensity, and waveform data. In this letter, multiscale geometric parameters are used to identify individual point observations corresponding to rock surfaces in point clouds acquired by terrestrial laser scanning in scenes with man-made clutter and scanning artifacts. Three multiscale operators, namely, the approximate shape and density of a defined neighborhood and the distance of its mean point from its geometric center, are fused into a single feature vector. The procedure is demonstrated using real point cloud data acquired in a mine drift, with the goal of identifying points belonging to the rock face obscured by an overlying wire support mesh. Using the extra-trees classifier, extraneous returns caused by the mesh were excluded from the point cloud with a 97% success rate, while 87% of the desired surface points were retained.}, 
keywords={feature extraction;geometry;geophysical signal processing;mining;remote sensing by laser beam;rocks;surface morphology;rock surface classification;mine drift;multiscale geometric features;scale-dependent statistical depictions;surface morphology;complex geometrical scaling relationships;traditional fractal measures;multiscale operators;rough discontinuous surfaces;noisy point clouds;local geometry;point cloud feature classification;laser scanning systems;natural scenes;external data sources;hyperspectral imagery;return pulse intensity;waveform data;multiscale geometric parameters;terrestrial laser scanning;man-made clutter;scanning artifacts;defined neighborhood;single feature vector;real point cloud data;rock face;overlying wire support mesh;extra-trees classifier;Three-dimensional displays;Rocks;Rough surfaces;Surface roughness;Fractals;Surface morphology;Classification;geology;mining industry;point clouds;terrestrial LiDAR;Classification;geology;mining industry;point clouds;terrestrial LiDAR}, 
doi={10.1109/LGRS.2015.2398814}, 
ISSN={1545-598X}, 
month={June},}
@INPROCEEDINGS{7517246, 
author={P. {Azevedo} and T. O. {Dos Santos} and E. {De Aguiar}}, 
booktitle={2016 XVIII Symposium on Virtual and Augmented Reality (SVR)}, 
title={An Augmented Reality Virtual Glasses Try-On System}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper presents a virtual try-on system to correctly visualize 3D objects (e.g., glasses) in the face of a given user. By capturing the image and depth information of a user through a low-cost RGB-D camera, we apply a face tracking technique to detect specific landmarks in the facial image. These landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3D facial morphable model that fits as good as possible to the user's head and face. At the end, we deform the chosen 3D objects from its rest shape to a deformed shape matching the specific facial shape of the user. The last step projects and renders the 3D object into the original image, with enhanced precision and in proper scale, showing the selected object in the user's face. We validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. Our results demonstrate that, by fitting a facial model to the user's face, the rendered virtual 3D objects look more realistic.}, 
keywords={augmented reality;face recognition;image capture;image colour analysis;image matching;image reconstruction;image sensors;rendering (computer graphics);shape recognition;augmented reality virtual glass try-on system;3D object visualization;image capturing;depth information;low-cost RGB-D camera;facial image;point cloud reconstruction;3D facial morphable model;deformed shape matching;virtual 3D object rendering;Three-dimensional displays;Face;Solid modeling;Glass;Cameras;Shape;Augmented reality;Virtual reality;Computer graphics}, 
doi={10.1109/SVR.2016.12}, 
ISSN={}, 
month={June},}
@ARTICLE{8370652, 
author={Y. {Wang} and Z. {Li} and T. {Vu} and N. {Nyayapathi} and K. W. {Oh} and W. {Xu} and J. {Xia}}, 
journal={IEEE Sensors Journal}, 
title={A Robust and Secure Palm Vessel Biometric Sensing System Based on Photoacoustics}, 
year={2018}, 
volume={18}, 
number={14}, 
pages={5993-6000}, 
abstract={In this paper, we propose a new palm vessel biometric sensing system based on photoacoustic imaging, which is an emerging technique that allows high-resolution visualization of optical absorption in deep tissue. Our system consists of an ultrasound (US) linear transducer array, an US data acquisition system, and an Nd:YAG laser emitting 1064-nm wavelength. By scanning the array, we could get a 3-D image of palm vasculature. The 3-D image is further combined with our newly developed algorithm, Earth Mover's Distance-Radiographic Testing, to provide precise matching and robust recognition rate. Compared to conventional vein sensing techniques, our system demonstrates deeper imaging depth and better spatial resolution, offering securer biometric features to fight against counterfeits. In this paper, we imaged 20 different hands at various poses and quantified our system performance. We found that the usability and accuracy of our system are comparable to conventional biometric techniques, such as fingerprint imaging and face identification. Our technique can open up avenues for better liveness detection and biometric measurements.}, 
keywords={biological tissues;biomedical optical imaging;biomedical transducers;biomedical ultrasonics;biometrics (access control);feature extraction;fingerprint identification;image resolution;image sensors;laser applications in medicine;medical image processing;photoacoustic effect;ultrasonic transducer arrays;high-resolution visualization;linear transducer array;US data acquisition system;3-D image;palm vasculature;Earth Mover's Distance-Radiographic Testing;robust recognition rate;conventional vein sensing techniques;deeper imaging depth;securer biometric features;system performance;conventional biometric techniques;fingerprint imaging;face identification;biometric measurements;photoacoustics;palm vessel biometric sensing system;photoacoustic imaging;Optical imaging;Veins;Biomedical imaging;Three-dimensional displays;Optical scattering;Absorption;Photoacoustic imaging (PAI);high-resolution;three-dimensional (3D) image;imaging depth;precise;robust}, 
doi={10.1109/JSEN.2018.2843119}, 
ISSN={1530-437X}, 
month={July},}
@INPROCEEDINGS{7495382, 
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and H. A. {Safar} and N. {Werghi}}, 
booktitle={2016 18th Mediterranean Electrotechnical Conference (MELECON)}, 
title={Landmarks detection on 3D face scans using local histogram descriptors}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In this work, we exploit 3D Constrained Local Model (CLM) for facial landmark detection. Our approach integrates the geometric information of 3D face scans. The fast increase demand of 3D data invite to develop 3D image processing methods for many applications and especially for automatic landmark detection. The new step in this paper is the introduction of mesh histogram of gradients (meshHOG) as local descriptors around every landmark location. The proposed work is evaluated on the publicly available Bosphorus database. A comparison with the other descriptors mesh LBP and mesh SIFT are also depicted.}, 
keywords={computational geometry;face recognition;mesh generation;object detection;facial landmark detection;3D face scans;local histogram descriptors;3D constrained local model;geometric information;3D image processing methods;mesh histogram-of-gradients;meshHOG;local descriptors;publicly available Bosphorus database;Three-dimensional displays;Face;Histograms;Solid modeling;Shape;Deformable models;Mathematical model}, 
doi={10.1109/MELCON.2016.7495382}, 
ISSN={2158-8481}, 
month={April},}
@INPROCEEDINGS{7299095, 
author={S. Z. {Gilani} and F. {Shafait} and A. {Mian}}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Shape-based automatic detection of a large number of 3D facial landmarks}, 
year={2015}, 
volume={}, 
number={}, 
pages={4639-4648}, 
abstract={We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morphologically significant. The proposed algorithm evolves level set curves with adaptive geometric speed functions to automatically extract effective seed points for dense correspondence. Correspondences are established by minimizing the bending energy between patches around seed points of given faces to those of a reference face. Given its hierarchical structure, our algorithm is capable of establishing thousands of correspondences between a large number of faces. Finally, a morphable model based on the dense corresponding points is fitted to an unseen query face for transfer of correspondences and hence automatic detection of landmarks. The proposed algorithm can detect any number of pre-defined landmarks including subtle landmarks that are even difficult to detect manually. Extensive experimental comparison on two benchmark databases containing 6, 507 scans shows that our algorithm outperforms six state of the art algorithms.}, 
keywords={face recognition;feature extraction;geometry;minimisation;shape recognition;shape-based automatic detection;3D facial landmark;geometric speed function;bending energy minimization;Three-dimensional displays;Shape;Databases;Level set;Algorithm design and analysis;Feature extraction;Mathematical model}, 
doi={10.1109/CVPR.2015.7299095}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7351535, 
author={S. {Sankaranarayanan} and V. M. {Patel} and R. {Chellappa}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={3D facial model synthesis using coupled dictionaries}, 
year={2015}, 
volume={}, 
number={}, 
pages={3896-3900}, 
abstract={In this work, we propose a generative way of modeling faces, where the 3D shape of a face is generated by a supervised learning procedure involving coupled sparse feature learning. To learn dictionaries using the proposed method, we use the USF-HUMAN ID database [1]. We provide as input to our training system, paired correspondences of 2D and 3D images of individuals and aim to learn the low-level patches both in 2D and 3D domains that describe the corresponding subspaces in a sparse manner. We demonstrate the efficacy of our method by quantitative results on the 3D database and qualitative results on images drawn from the internet.}, 
keywords={face recognition;learning (artificial intelligence);3D facial model synthesis;coupled dictionaries;supervised learning procedure;coupled sparse feature learning;dictionary learning;USF-HUMAN ID database;2D images;3D images;Three-dimensional displays;Dictionaries;Solid modeling;Shape;Training;Databases;Encoding;3D Model;Face Synthesis;Coupled Sparse Coding;Cross-modal Learning}, 
doi={10.1109/ICIP.2015.7351535}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7943450, 
author={S. {Rajeev} and K. K. M. {Shreyas} and K. {Panetta} and S. {Agaian}}, 
booktitle={2017 IEEE International Symposium on Technologies for Homeland Security (HST)}, 
title={3-D palmprint modeling for biometric verification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Palmprint is a very unique and distinctive biometric trait because of features such as a person's inimitable principal lines, wrinkles, delta points, and minutiae. These constitute the main reasons why palmprint verification is considered as one of the most reliable personal identification methods. However, a clear majority of the research on palm-prints are concentrated on 2-D palmprint images irrespective of the fact that the human palm is a 3D-surface. While 2-D palmprint recognition has proved to be efficient in terms of verification rate, it has some essential downsides. These restrictions can adversely affect the performance and robustness of the palmprint recognition system. One of the possible solutions to resolve the limitations associated with 2-D palm print authentication systems is (i) to use a 3-D scanning system and to produce high quality 3-D images with depth information; (ii) to map 3-D palm-print images into 2-D images which may support the usage of 3-D images with both biometric palmprint 2-D image databases and 2-D palmprint recognition tools. The bloom of 3-D technologies has made it easier to capture and store 3-D images. The problem of a direct mapping approach is that a large section of the palm is hard-pressed on the scanner surface during 2-D based acquisition. This paper proposes a novel technique to unravel/map 3-D palm images to its equivalent 2-D palm-print image. This image can be then used to perform efficient and accurate 2-D identification/ verification. Experimental results and discussions will also be presented.}, 
keywords={authorisation;biometrics (access control);computer graphics;palmprint recognition;biometric verification;biometric trait;2-D palmprint images;2-D palmprint recognition;2-D palmprint authentication systems;3-D palmprint images;biometric palmprint 2-D image databases;2-D palmprint recognition tools;2-D identification-verification;Fingerprint recognition;Databases;Solid modeling;Mathematical model;Biomedical imaging;Feature extraction;biometric;palm-print;2-D;3-D;authentication;verification;identification;unrolling;mapping;modeling;security}, 
doi={10.1109/THS.2017.7943450}, 
ISSN={}, 
month={April},}
@ARTICLE{7029822, 
author={Y. {Li} and Y. {Wang} and B. {Wang} and L. {Sui}}, 
journal={IET Computer Vision}, 
title={Nose tip detection on three-dimensional faces using pose-invariant differential surface features}, 
year={2015}, 
volume={9}, 
number={1}, 
pages={75-84}, 
abstract={Three-dimensional (3D) facial data offer the potential to overcome the difficulties caused by the variation of head pose and illumination in 2D face recognition. In 3D face recognition, localisation of nose tip is essential to face normalisation, face registration and pose correction etc. Most of the existing methods of nose tip detection on 3D face deal mainly with frontal or near-frontal poses or are rotation sensitive. Many of them are training-based or model-based. In this study, a novel method of nose tip detection is proposed. Using pose-invariant differential surface features - high-order and low-order curvatures, it can detect nose tip on 3D faces under various poses automatically and accurately. Moreover, it does not require training and does not depend on any particular model. Experimental results on GavabDB verify the robustness and accuracy of the proposed method.}, 
keywords={face recognition;feature extraction;image registration;object detection;pose estimation;nose tip detection;pose invariant differential surface feature;3D face recognition;head pose variation;illumination;2D face recognition;nose tip localisation;face normalisation;face registration;pose correction;nearfrontal pose;low order curvature;high order curvature}, 
doi={10.1049/iet-cvi.2014.0070}, 
ISSN={1751-9632}, 
month={},}
@INPROCEEDINGS{7938174, 
author={Y. {Zhang} and X. {Shen} and Y. {Hu}}, 
booktitle={2016 International Conference on Virtual Reality and Visualization (ICVRV)}, 
title={Face Registration and Surgical Instrument Tracking for Image-Guided Surgical Navigation}, 
year={2016}, 
volume={}, 
number={}, 
pages={65-71}, 
abstract={This paper presents a face registration method and a surgical instrument tracking method for Image-guided Surgical Navigation System (ISNS). In ISNS, we start the process by reconstructing 3d point cloud of patient using a proactive structured light visual system and then accurately complete face registration based on Iterative Closest Point (ICP) algorithm. We utilize special markers to represent the position and attitude of surgical instrument. At a final step, we recognize special markers by a syncretic corner detection algorithm and recover the 3d coordinate to track surgical instrument. To further improve the accuracy and robustness of tracking, we also utilize kalman algorithm to filter out the noise in tracking. We have tested our system on 3d printing model with matched medical data, demonstrating its accuracy and robustness of registration and tracking.}, 
keywords={face recognition;image registration;iterative methods;medical image processing;surgery;face registration;surgical instrument tracking;image-guided surgical navigation system;ISNS;3D point cloud;patient treatment;proactive structured light visual system;iterative closest point algorithm;ICP algorithm;Surgery;Instruments;Three-dimensional displays;Image reconstruction;Aerospace electronics;Face;Iterative closest point algorithm;Surgical navigation;Registration;Surgical instrument tracking;Structured light}, 
doi={10.1109/ICVRV.2016.19}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8460605, 
author={W. J. {Beksi} and N. {Papanikolopoulos}}, 
booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Signature of Topologically Persistent Points for 3D Point Cloud Description}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={We present the Signature of Topologically Persistent Points (STPP), a global descriptor that encodes topological invariants of 3D point cloud data. These topological invariants include the zeroth and first homology groups and are computed using persistent homology, a method for finding the features of a topological space at different spatial resolutions. STPP is a competitive 3D point cloud descriptor when compared to the state of art and is resilient to noisy sensor data. We demonstrate experimentally on a publicly available RGB-D dataset that STPP can be used as a distinctive signature, thus allowing for 3D point cloud processing tasks such as object detection and classification.}, 
keywords={computer graphics;image resolution;solid modelling;object classification;object detection;3D point cloud processing tasks;RGB-D dataset;spatial resolutions;topological invariant encoding;global descriptor;topologically persistent point signature;homology groups;competitive 3D point cloud descriptor;topological space;3D point cloud data;STPP;time 3.0 d;Three-dimensional displays;Shape;Robot sensing systems;Generators;Histograms;Topology;Face}, 
doi={10.1109/ICRA.2018.8460605}, 
ISSN={2577-087X}, 
month={May},}
@INPROCEEDINGS{7550083, 
author={W. P. {Koppen} and W. J. {Christmas} and D. J. M. {Crouch} and W. F. {Bodmer} and J. V. {Kittler}}, 
booktitle={2016 International Conference on Biometrics (ICB)}, 
title={Extending non-negative matrix factorisation to 3D registered data}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The use of non-negative matrix factorisation (NMF) on 2D face images has been shown to result in sparse feature vectors that encode for local patches on the face, and thus provides a statistically justified approach to learning parts from wholes. However successful on 2D images, the method has so far not been extended to 3D images. The main reason for this is that 3D space is a continuum and so it is not apparent how to represent 3D coordinates in a non-negative fashion. This work compares different non-negative representations for spatial coordinates, and demonstrates that not all non-negative representations are suitable. We analyse the representational properties that make NMF a successful method to learn sparse 3D facial features. Using our proposed representation, the factorisation results in sparse and interpretable facial features.}, 
keywords={face recognition;feature extraction;image registration;image representation;learning (artificial intelligence);matrix decomposition;nonnegative matrix factorization;NMF;3D face image registration;representational property;3D facial feature learning;Face;Three-dimensional displays;Shape;Principal component analysis;Two dimensional displays;Facial features;Encoding}, 
doi={10.1109/ICB.2016.7550083}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7139616, 
author={T. {Linder} and S. {Wehner} and K. O. {Arras}}, 
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Real-time full-body human gender recognition in (RGB)-D data}, 
year={2015}, 
volume={}, 
number={}, 
pages={3039-3045}, 
abstract={Understanding social context is an important skill for robots that share a space with humans. In this paper, we address the problem of recognizing gender, a key piece of information when interacting with people and understanding human social relations and rules. Unlike previous work which typically considered faces or frontal body views in image data, we address the problem of recognizing gender in RGB-D data from side and back views as well. We present a large, gender-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the Kinect v1 and Kinect v2 sensor. We then learn and compare several classifiers on the Kinect v2 data using a HOG baseline, two state-of-the-art deep-learning methods, and a recent tessellation-based learning approach. Originally developed for person detection in 3D data, the latter is able to learn the best selection, location and scale of a set of simple point cloud features. We show that for gender recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 Hz.}, 
keywords={human-robot interaction;image classification;image sensors;learning (artificial intelligence);object detection;object recognition;real-time full-body human gender recognition;RGB-D data;human social relations;human social rules;image data;multiperspective RGB-D dataset;gender-balanced RGB-D dataset;annotated RGB-D dataset;Kinect v2 sensor;Kinect v1 sensor;HOG baseline;deep-learning methods;tessellation-based learning approach;person detection;point cloud features;Three-dimensional displays;Robot sensing systems;Training;Legged locomotion;Accuracy;Support vector machines}, 
doi={10.1109/ICRA.2015.7139616}, 
ISSN={1050-4729}, 
month={May},}
@ARTICLE{8024025, 
author={C. {Ye} and X. {Qian}}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired}, 
year={2018}, 
volume={26}, 
number={2}, 
pages={441-450}, 
abstract={This paper presents a 3-D object recognition method and its implementation on a robotic navigation aid to allow real-time detection of indoor structural objects for the navigation of a blind person. The method segments a point cloud into numerous planar patches and extracts their inter-plane relationships (IPRs).Based on the existing IPRs of the object models, the method defines six high level features (HLFs) and determines the HLFs for each patch. A Gaussian-mixture-model-based plane classifier is then devised to classify each planar patch into one belonging to a particular object model. Finally, a recursive plane clustering procedure is used to cluster the classified planes into the model objects. As the proposed method uses geometric context to detect an object, it is robust to the object's visual appearance change. As a result, it is ideal for detecting structural objects (e.g., stairways, doorways, and so on). In addition, it has high scalability and parallelism. The method is also capable of detecting some indoor non-structural objects. Experimental results demonstrate that the proposed method has a high success rate in object recognition.}, 
keywords={feature extraction;Gaussian processes;geometry;handicapped aids;image segmentation;mobile robots;object recognition;path planning;pattern clustering;robot vision;nonstructural objects;robotic navigation aid;real-time detection;indoor structural objects;numerous planar patches;object models;HLFs;Gaussian-mixture-model;plane classifier;planar patch;particular object model;recursive plane clustering procedure;point cloud;3D-object recognition;interplane relationships;Object recognition;Three-dimensional displays;Feature extraction;Visualization;Navigation;Cameras;RNA;Robotic navigation aid;visually impaired;3D object recognition;geometric context;Gaussian mixture model}, 
doi={10.1109/TNSRE.2017.2748419}, 
ISSN={1534-4320}, 
month={Feb},}
@INPROCEEDINGS{8578388, 
author={W. {Zhu} and Q. {Qiu} and J. {Huang} and R. {Calderbank} and G. {Sapiro} and I. {Daubechies}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={LDMNet: Low Dimensional Manifold Regularized Neural Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={2743-2751}, 
abstract={Deep neural networks have proved very successful on archetypal tasks for which large training sets are available, but when the training data are scarce, their performance suffers from overfitting. Many existing methods of reducing overfitting are data-independent. Data-dependent regularizations are mostly motivated by the observation that data of interest lie close to a manifold, which is typically hard to parametrize explicitly. These methods usually only focus on the geometry of the input data, and do not necessarily encourage the networks to produce geometrically meaningful features. To resolve this, we propose the Low-Dimensional-Manifold-regularized neural Network (LDMNet), which incorporates a feature regularization method that focuses on the geometry of both the input data and the output features. In LDMNet, we regularize the network by encouraging the combination of the input data and the output features to sample a collection of low dimensional manifolds, which are searched efficiently without explicit parametrization. To achieve this, we directly use the manifold dimension as a regularization term in a variational functional. The resulting Euler-Lagrange equation is a Laplace-Beltrami equation over a point cloud, which is solved by the point integral method without increasing the computational complexity. In the experiments, we show that LDMNet significantly outperforms widely-used regularizers. Moreover, LDMNet can extract common features of an object imaged via different modalities, which is very useful in real-world applications such as cross-spectral face recognition.}, 
keywords={computational complexity;face recognition;feature extraction;Laplace equations;learning (artificial intelligence);neural nets;LDMNet;Low Dimensional Manifold regularized neural networks;deep neural networks;data-dependent regularizations;feature regularization method;low dimensional manifolds;computational complexity;Laplace-Beltrami equation;cross-spectral face recognition;point cloud;Manifolds;Feature extraction;Training;Geometry;Neural networks;Training data;Three-dimensional displays}, 
doi={10.1109/CVPR.2018.00290}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{8317846, 
author={R. {Varga} and A. {Costea} and H. {Florea} and I. {Giosan} and S. {Nedevschi}}, 
booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)}, 
title={Super-sensor for 360-degree environment perception: Point cloud segmentation using image features}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={This paper describes a super-sensor that enables 360-degree environment perception for automated vehicles in urban traffic scenarios. We use four fisheye cameras, four 360-degree LIDARs and a GPS/IMU sensor mounted on an automated vehicle to build a super-sensor that offers an enhanced low-level representation of the environment by harmonizing all the available sensor measurements. Individual sensors cannot provide a robust 360-degree perception due to their limitations: field of view, range, orientation, number of scanning rays, etc. The novelty of this work consists of segmenting the 3D LIDAR point cloud by associating it with the 2D image semantic segmentation. Another contribution is the sensor configuration that enables 360-degree environment perception. The following steps are involved in the process: calibration, timestamp synchronization, fisheye image unwarping, motion correction of LIDAR points, point cloud projection onto the images and semantic segmentation of images. The enhanced low-level representation will improve the high-level perception environment tasks such as object detection, classification and tracking.}, 
keywords={image classification;image motion analysis;image representation;image resolution;image segmentation;image sensors;object detection;object tracking;optical radar;360-degree environment perception;point cloud segmentation;automated vehicle;360-degree LIDARs;GPS/IMU sensor;3D LIDAR point cloud;sensor measurements;image semantic segmentation;Conferences;Intelligent transportation systems;automated driving;environment perception;fisheye images;3D LIDAR points;360-degree perception;super-sensor}, 
doi={10.1109/ITSC.2017.8317846}, 
ISSN={2153-0017}, 
month={Oct},}
@INPROCEEDINGS{8633125, 
author={X. {Ju} and I. R. {Garcia Júnior} and L. {De Freitas Silva} and P. {Mossey} and D. {Al-Rudainy} and A. {Ayoub} and A. M. {De Mattos}}, 
booktitle={2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={3D Head Shape Analysis of Suspected Zika Infected Infants}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={The babies infected from Zika before they are born are at risk for problems with brain development and microcephaly. 3D head images of 43 Zika cases and 43 controls were collected aiming to extract shape characteristics for diagnosis purposes. Principal component analysis (PCA) has been applied on the vaults and faces of the collected 3D images and the scores on the second principal components of the vaults and faces showed significant differences between the control and Zika groups. The shape variations from -2σ to 2σ illustrated the typical characteristics of microcephaly of the Zika babies. Canonical correlation analysis (CCA) showed a significant correlation in the first CCA variates of face and vault which indicated the potential of 3D facial imaging for Zika surveillance. Further head circumferences and distances from ear to ear were measured from the 3D images and preliminary results showed the adding ear to ear distances for classifying control and Zika children strengthened the abilities of tested classification models.}, 
keywords={diseases;image classification;medical image processing;paediatrics;principal component analysis;shape recognition;stereo image processing;suspected Zika infected infants;microcephaly;Zika surveillance;3D facial imaging;canonical correlation analysis;Zika babies;shape variations;principal component analysis;3D head images;brain development;shape analysis;Zika children;3D imaging;shape analysis;Zika}, 
doi={10.1109/CISP-BMEI.2018.8633125}, 
ISSN={}, 
month={Oct},}
@ARTICLE{8440881, 
author={A. {Switonski} and T. {Krzeszowski} and H. {Josinski} and B. {Kwolek} and K. {Wojciechowski}}, 
journal={IET Biometrics}, 
title={Gait recognition on the basis of markerless motion tracking and DTW transform}, 
year={2018}, 
volume={7}, 
number={5}, 
pages={415-422}, 
abstract={In this study, a framework for view-invariant gait recognition on the basis of markerless motion tracking and dynamic time warping (DTW) transform is presented. The system consists of a proposed markerless motion capture system as well as introduced classification method of mocap data. The markerless system estimates the three-dimensional locations of skeleton driven joints. Such skeleton-driven point clouds represent poses over time. The authors align point clouds in every pair of frames by calculating the minimal sum of squared distances between the corresponding joints. A point cloud distance measure with temporal context has been utilised ink-nearest neighbours algorithm to compare time instants of motion sequences. To enhance the generalisation of the recognition and to shorten the processing time, for every individual a single multidimensional time series among several multidimensional time series describing the individual's gait is established. The correct classification rate has been determined on the basis of a real dataset of human gait. It contains 230 gait cycles of 22 subjects. The tracking results on the basis of markerless motion capture are referenced to Vicon system, whereas the achieved accuracies of recognition are compared with the ones obtained by DTW that is based on rotational data.}, 
keywords={gait analysis;image classification;image motion analysis;image sequences;learning (artificial intelligence);object tracking;time series;transforms;multidimensional time series;motion sequences;k-nearest neighbours algorithm;skeleton-driven point clouds;classification method;markerless motion capture system;dynamic time warping transform;view-invariant gait recognition;DTW transform;markerless motion tracking;gait recognition}, 
doi={10.1049/iet-bmt.2017.0134}, 
ISSN={2047-4938}, 
month={},}
@INPROCEEDINGS{7351660, 
author={R. J. {Arteaga} and S. J. {Ruuth}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={Laplace-Beltrami spectra for shape comparison of surfaces in 3D using the closest point method}, 
year={2015}, 
volume={}, 
number={}, 
pages={4511-4515}, 
abstract={The need to compare separate objects arises in a wide range of applications. In one approach for comparing objects, `Shape-DNA' is constructed to give a numerical fingerprint representing an individual object. Shape-DNA is a cropped set of eigenvalues of the Laplace-Beltrami operator for the surface of the object. In this paper, we compute the Shape-DNA of surfaces using the closest point method. Our approach may be applied to a variety of surface representations including triangulations, point clouds and certain analytical shapes. A 2D multidimensional scaling plot illustrates that similar objects form groups based on the Shape-DNAs. Our method has the benefit that it may be applied to surfaces defined by dense point clouds without requiring the construction of point connectivity.}, 
keywords={DNA;eigenvalues and eigenfunctions;fingerprint identification;image representation;Laplace transforms;shape recognition;Laplace-Beltrami spectra;shape comparison;closest point method;shape-DNA;numerical fingerprint representation;eigenvalues;surface representations;point clouds;2D multidimensional scaling plot;Shape;Three-dimensional displays;Eigenvalues and eigenfunctions;Standards;Interpolation;Rabbits;shape comparison;Laplace-Beltrami spectra;closest point method;point cloud}, 
doi={10.1109/ICIP.2015.7351660}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7457888, 
author={H. {Fan} and Y. {Zhou} and H. {Liang} and J. {Wang} and P. {Krebs} and D. {Lin} and J. {Su} and K. {Li} and H. {Chen} and X. {Wang} and J. {Zhou}}, 
booktitle={2015 Visual Communications and Image Processing (VCIP)}, 
title={Glasses-free 3D display with glasses-assisted quality: Key innovations for smart directional backlight autostereoscopy}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={A glasses-free 3D display with glasses-assisted quality is presented. Self-adaptive algorithm is employed to optimize system parameters, which is applied to design the micro structure of lens array and free form surface backlight units. Moiré contour map based on contrast sensitivity function is simulated and is manipulated by ameliorating the period ratio and the tilt angle of the superimposed periodical optical components. Directional transmissions of multi-users 3D images are realized with a finer dynamic synchronized backlight control and a face recognition technology. Comfortable viewings are demonstrated for two viewers, with full high definition for a single viewing channel. Minimum crosstalk as low as 2.3% is demonstrated over a large viewing volume.}, 
keywords={crosstalk;face recognition;stereo image processing;three-dimensional displays;crosstalk;single-viewing channel;full-high-definition;face recognition technology;dynamic synchronized backlight control;multiuser 3D image directional transmission;superimposed periodical optical components;period ratio;tilt angle;contrast sensitivity function;Moire contour map;free-form surface backlight units;lens array;self-adaptive algorithm;smart directional backlight autostereoscopy;glass-assisted quality;glass-free 3D display;Three-dimensional displays;Crosstalk;Lenses;Light emitting diodes;Bars;Synchronization;3D display;stereo vision;autostereoscopic;free-form surface design;crosstalk}, 
doi={10.1109/VCIP.2015.7457888}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7332643, 
author={C. {Stahlschmidt} and A. {Gavriilidis} and A. {Kummert}}, 
booktitle={2015 IEEE 9th International Workshop on Multidimensional (nD) Systems (nDS)}, 
title={Classification of ascending steps and stairs using Time-of-Flight sensor data}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper proposes a method to analyse human-made environments in order to verify the presence of ascending steps or stairs. Our system is intended to assist visually impaired people by providing acoustic information about the scene in front of a low-resolution Time-of-Flight (ToF) camera that is fixed to a mobile vehicle (rollator). Detailed instructions to the user about potentially hazardous situations are provided. This paper in particular deals with a fast approach on classification of ascending steps in 3D point clouds. This method is part of a system that aims on enhancing visually impaired persons understand the environment and help prevent collisions.}, 
keywords={computer graphics;image classification;image resolution;ascending step classification;time-of-flight sensor data;human-made environment;visually impaired people;acoustic information;low-resolution time-of-flight camera;ToF camera;mobile vehicle;rollator;hazardous situation;3D point cloud;visually impaired person;Three-dimensional displays;Cameras;Data mining;Algorithm design and analysis;Gray-scale;Acoustics;Mobile communication}, 
doi={10.1109/NDS.2015.7332643}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7523645, 
author={ and and and }, 
booktitle={2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)}, 
title={Development of autonomous laser toning system based on vision recognition and robot manipulator}, 
year={2016}, 
volume={}, 
number={}, 
pages={317-322}, 
abstract={In this paper, the design, implementation, and operation method of the autonomous laser toning system are proposed, which is called as MELON (Manipulator for Effective Laser tONing). The system can recognize the accurate treatment points from the 3D point cloud data obtained with the camera, and it is possible to emit the laser at the desired position and orientation repeatedly, precisely, and accurately using intuitive differential inverse kinematics of the robot manipulator. The feasibility test of the MELON is conducted by using a plaster cast of a woman's head, and then, we find that the manipulator has a workspace to cover the entire face of the human inductively and distribution of the laser emission is homogeneous on the face. Therefore, we find the possibility of the autonomous laser toning using MELON.}, 
keywords={computer graphics;manipulators;robot kinematics;robot vision;autonomous laser toning system;vision recognition;robot manipulator;MELON;manipulator for effective laser toning;3D point cloud;differential inverse kinematics;plaster cast;laser emission;Manipulators;Surface emitting lasers;Face;Cameras;Medical services}, 
doi={10.1109/BIOROB.2016.7523645}, 
ISSN={2155-1782}, 
month={June},}
@INPROCEEDINGS{7755218, 
author={A. M. {Intwala} and A. {Magikar}}, 
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, 
title={A review on process of 3D Model Reconstruction}, 
year={2016}, 
volume={}, 
number={}, 
pages={2851-2855}, 
abstract={3D Model Reconstruction is of the most important part in the field of Reverse Engineering. It has now become feasible to use this method to create a 3D model of existing product, component for CAD/CAM applications. Various phases of reverse engineering and 3D reconstruction are reviewed in details along the methodology involved within these stages. Data acquisition is the most crucial stage of 3d model reconstruction. Non touch and touch techniques of data acquisition are studied giving comparative advantages and disadvantages over each other. Face based and Edge based techniques are reviewed for segmentation of acquired data. The collected point data is obtained in Stereo Lithography (STL) format which is most popular in field of modern computer science. This collected data can be used to construct surfaces using modern CAD systems like CATIA, Pro/E etc. This paper also reviews some of the researches in the field of Reverse Engineering, The methods they implemented and their outcome and various technologies used with their limitations. It reviews latest advancement in the field of 3d Model Reconstruction.}, 
keywords={CAD/CAM;data acquisition;edge detection;face recognition;image reconstruction;reverse engineering;stereo image processing;stereolithography;3D model reconstruction process;reverse engineering;CAD-CAM applications;nontouch technique;touch technique;data acquisition;edge-based technique;face-based technique;stereo lithography;STL format;image processing;Three-dimensional displays;Solid modeling;Image reconstruction;Data models;Computational modeling;Reverse engineering;Measurement by laser beam;3D model reconstruction;Computer graphics;Image processing;Reverse Engineering;3d Point Cloud Data;CMM;3D Printer;Feature Extraction}, 
doi={10.1109/ICEEOT.2016.7755218}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8202220, 
author={D. L. {Rizzini}}, 
booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Place recognition of 3D landmarks based on geometric relations}, 
year={2017}, 
volume={}, 
number={}, 
pages={648-654}, 
abstract={Place recognition based on landmarks or features is an important problem occurring in localization, mapping, computer vision and point cloud processing. In this paper, we present GLAROT-3D, a translation and rotation invariant 3D signature based on geometric relations. The proposed method encodes into a histogram the pairwise relative positions of keypoint features extracted from 3D sensor data. Since it relies only on geometric properties and not on specific feature descriptors, it does not require any prior training or vocabulary construction and enables lightweight comparisons between landmark maps. The similarity of two point maps is computed as the distance between the corresponding rotated histograms to achieve rotation invariance. Histogram rotation is enabled by efficient orientation histogram based on sphere cubical projection. The performance of GLAROT has been assessed through experiments with standard benchmark datasets.}, 
keywords={feature extraction;image recognition;place recognition;geometric relations;keypoint features;3D sensor data;geometric properties;landmark maps;point maps;rotation invariance;histogram rotation;translation invariant 3D signature;pairwise relative positions;rotated histograms;orientation histogram;sphere cubical projection;Histograms;Three-dimensional displays;Feature extraction;Indexes;Face;Robot sensing systems;Computer vision}, 
doi={10.1109/IROS.2017.8202220}, 
ISSN={2153-0866}, 
month={Sep.},}
@INPROCEEDINGS{7301378, 
author={P. {Polewski} and W. {Yao} and M. {Heurich} and P. {Krzystek} and U. {Stilla}}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
title={Active learning approach to detecting standing dead trees from ALS point clouds combined with aerial infrared imagery}, 
year={2015}, 
volume={}, 
number={}, 
pages={10-18}, 
abstract={Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89% with less than 100 training examples, which corresponds to 10% of the pre-selected data pool.}, 
keywords={environmental factors;feature extraction;forestry;image classification;image colour analysis;infrared imaging;learning (artificial intelligence);matrix algebra;object detection;vegetation;active learning approach;standing dead tree detection;ALS point cloud;forest process;forest science;environmental science;aerial color infrared imagery;feature extraction;greedy approximation;kernelized feature matrix;error reduction;semisupervised classifier;Bavarian Forest National Park;Vegetation;Three-dimensional displays;Entropy;Training;Image segmentation;Logistics;Feature extraction}, 
doi={10.1109/CVPRW.2015.7301378}, 
ISSN={2160-7516}, 
month={June},}
@INPROCEEDINGS{7410401, 
author={A. {Sironi} and V. {Lepetit} and P. {Fua}}, 
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={Projection onto the Manifold of Elongated Structures for Accurate Extraction}, 
year={2015}, 
volume={}, 
number={}, 
pages={316-324}, 
abstract={Detection of elongated structures in 2D images and 3D image stacks is a critical prerequisite in many applications and Machine Learning-based approaches have recently been shown to deliver superior performance. However, these methods essentially classify individual locations and do not explicitly model the strong relationship that exists between neighboring ones. As a result, isolated erroneous responses, discontinuities, and topological errors are present in the resulting score maps. We solve this problem by projecting patches of the score map to their nearest neighbors in a set of ground truth training patches. Our algorithm induces global spatial consistency on the classifier score map and returns results that are provably geometrically consistent. We apply our algorithm to challenging datasets in four different domains and show that it compares favorably to state-of-the-art methods.}, 
keywords={feature extraction;image classification;learning (artificial intelligence);object detection;structural engineering computing;classifier score map;global spatial consistency;ground truth training patch;patch projection;score map;topological error;discontinuities;isolated erroneous response;individual location classification;machine learning-based approach;3D image stack;2D image;elongated structure detection;extraction;elongated structure manifold;Three-dimensional displays;Training;Manifolds;Biomembranes;Image segmentation;Feature extraction;Transforms}, 
doi={10.1109/ICCV.2015.44}, 
ISSN={2380-7504}, 
month={Dec},}
@INPROCEEDINGS{8123040, 
author={N. {Mohsin} and S. {Payandeh}}, 
booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={Localization and identification of body extremities based on data from multiple depth sensors}, 
year={2017}, 
volume={}, 
number={}, 
pages={2736-2741}, 
abstract={This paper explores the novel use of multiple depth sensors to overcome occlusions and improve localization and tracking of body extremities. The usage of data from only depth sensors not only overcomes visual challenges associated with RGB sensors under low illumination, but also protects the identity of surveyed person with high confidentiality. For integrating depth information from multiple sources, the paper presents first an overview of a novel calibration method for multiple depth sensors. In case of occlusion of any fiducial point in the primary sensor's depth image, co-ordinates of the point can be obtained from the frame of other sensors using the calibration parameters. To localize salient body parts such as hands, head and feet, a surface triangular mesh is applied on generated 3D point cloud from the primary sensor. The geodesic extrema from the mesh coincide with body extremities. The body extremities can be identified based on those relative geodesic distances between the extremities. Once the body parts are labelled, a portion of body can be targeted and evaluated for specific gait analysis and visualization. For the performance evaluation, our calibration method has fared well in comparison to other available techniques. Also, our proposed localization of salient body parts is able to successfully tag the specific body part i.e. the head region.}, 
keywords={calibration;data visualisation;feature extraction;gait analysis;image motion analysis;image sensors;mesh generation;object detection;sensor fusion;RGB sensors;salient body parts;depth information;body extremities localization;body extremities identification;multiple depth sensor data;body extremities tracking;illumination;identity confidentiality;specific body part tagging;fiducial point occlusion;primary sensor depth image;3D point cloud;mesh geodesic extrema;relative geodesic distances;gait analysis;visualization;Sensors;Three-dimensional displays;Calibration;Extremities;Image sensors;Image segmentation;Cameras;multiple depth sensors calibration;body extremities localization;Kinect II;geodesic distances}, 
doi={10.1109/SMC.2017.8123040}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7410516, 
author={S. {Katz} and A. {Tal}}, 
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={On the Visibility of Point Clouds}, 
year={2015}, 
volume={}, 
number={}, 
pages={1350-1358}, 
abstract={Is it possible to determine the visible subset of points directly from a given point cloud? Interestingly, it was shown that this is indeed the case - despite the fact that points cannot occlude each other, this task can be performed without surface reconstruction or normal estimation. The operator is very simple - it first transforms the points to a new domain and then constructs the convex hull in that domain. Points that lie on the convex hull of the transformed set of points are the images of the visible points. This operator found numerous applications in computer vision, including face reconstruction, keypoint detection, finding the best viewpoints, reduction of points, and many more. The current paper addresses a fundamental question: What properties should a transformation function satisfy, in order to be utilized in this operator? We show that three such properties are sufficient: the sign of the function, monotonicity, and a condition regarding the function's parameter. The correctness of an algorithm that satisfies these three properties is proved. Finally, we show an interesting application of the operator - assignment of visibility-confidence score. This feature is missing from previous approaches, where a binary yes/no visibility is determined. This score can be utilized in various applications, we illustrate its use in view-dependent curvature estimation.}, 
keywords={computer vision;convex programming;estimation theory;face recognition;image reconstruction;object detection;view-dependent curvature estimation;visibility-confidence score;transformation function;keypoint detection;computer vision;convex hull;surface reconstruction;point clouds;Kernel;Three-dimensional displays;Image reconstruction;Face;Surface reconstruction;Estimation;Computer vision}, 
doi={10.1109/ICCV.2015.159}, 
ISSN={2380-7504}, 
month={Dec},}
@INPROCEEDINGS{7780547, 
author={T. {Hackel} and J. D. {Wegner} and K. {Schindler}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Contour Detection in Unstructured 3D Point Clouds}, 
year={2016}, 
volume={}, 
number={}, 
pages={1610-1618}, 
abstract={We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds &gt; 107 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.}, 
keywords={edge detection;feature extraction;graph theory;image classification;image coding;image representation;learning (artificial intelligence);object detection;contour detection;unstructured 3D point clouds;surface orientation;large-scale outdoor point clouds;high-quality surface;solid models;unstructured-inhomogeneous point clouds;contour extraction;two-stage discriminative learning problem;contour score;binary classifier;feature extraction;contour scores;higher-order MRF;graph cliques;Canny-style edge detection;image representation;Three-dimensional displays;Feature extraction;Solid modeling;Image edge detection;Surface reconstruction;Surface topography;Solids}, 
doi={10.1109/CVPR.2016.178}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7298920, 
author={ and R. {Ji} and }, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery}, 
year={2015}, 
volume={}, 
number={}, 
pages={3013-3021}, 
abstract={Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.}, 
keywords={Boltzmann machines;feature extraction;image colour analysis;image matching;learning (artificial intelligence);object detection;3D object detection;bimodal deep Boltzmann machines;RGBD imagery;3D scenes;point clouds;3D training data labeling;cross-modality cues;cross-modality deep learning framework;3D scene object detection;cross-modality feature learning;3D detection window;3D point cloud;exemplar shape matching;3D CAD models;2D positive samples;R-CNNs;raw feature extraction;RGB domains;depth domains;RMRC dataset;bimodal based deep feature learning framework;Three-dimensional displays;Solid modeling;Training;Frequency modulation;Joints;Detectors;Feature extraction}, 
doi={10.1109/CVPR.2015.7298920}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{7331662, 
author={T. v. {Landesberger} and D. {Basgier} and M. {Becker}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Comparative Local Quality Assessment of 3D Medical Image Segmentations with Focus on Statistical Shape Model-Based Algorithms}, 
year={2016}, 
volume={22}, 
number={12}, 
pages={2537-2549}, 
abstract={The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.}, 
keywords={data analysis;data visualisation;ear;image segmentation;liver;medical image processing;neurophysiology;pattern clustering;statistical analysis;comparative local quality assessment;3D medical image segmentation;statistical shape model-based algorithm;automatic 3D medical segmentation algorithm;data analysis;data visualization;segmentation quality characteristics;hierarchical clustering algorithm;connectivity criterion;landmark correspondence;liver;cochlea;facial nerve;organ region identification;outlier instance detection;Image segmentation;Three-dimensional displays;Biomedical monitoring;Visual analytics;Medical diagnostic imaging;Systematics;Statistical analysis;Clustering;Visual analytics;3D medical image segmentation quality;comparison;clustering;statistical shape models}, 
doi={10.1109/TVCG.2015.2501813}, 
ISSN={1077-2626}, 
month={Dec},}
@INPROCEEDINGS{7328083, 
author={M. {Sano} and K. {Matsumoto} and B. H. {Thomas} and H. {Saito}}, 
booktitle={2015 IEEE International Symposium on Mixed and Augmented Reality}, 
title={[POSTER] Rubix: Dynamic Spatial Augmented Reality by Extraction of Plane Regions with a RGB-D Camera}, 
year={2015}, 
volume={}, 
number={}, 
pages={148-151}, 
abstract={Dynamic spatial augmented reality requires accurate real-time 3D pose information of the physical objects that are to be projected onto. Previous depth-based methods for tracking objects required strong features to enable recognition; making it difficult to estimate an accurate 6DOF pose for physical objects with a small set of recognizable features (such as a non-textured cube). We propose a more accurate method with fewer limitations for the pose estimation of a tangible object that has known planar faces and using depth data from an RGB-D camera only. In this paper, the physical object's shape is limited to cubes of different sizes. We apply this new tracking method to achieve dynamic projections onto these cubes. In our method, 3D points from an RGB-D camera are divided into a cluster of planar regions, and the point cloud inside each face of the object is fitted to an already-known geometric model of a cube. With the 6DOF pose of the physical object, SAR generated imagery is then projected correctly onto the physical object. The 6DOF tracking is designed to support tangible interactions with the physical object. We implemented example interactive applications with one or multiple cubes to show the capability of our method.}, 
keywords={augmented reality;cameras;feature extraction;object tracking;pose estimation;Rubix;plane region extraction;dynamic spatial augmented reality;tangible object pose estimation;RGB-D camera;tracking method;point cloud;SAR generated imagery;Three-dimensional displays;Cameras;Iterative closest point algorithm;Augmented reality;Real-time systems;Target tracking;Spatial Augmented Reality;Six Degree of Freedom Tracking;RGB-D Camera}, 
doi={10.1109/ISMAR.2015.43}, 
ISSN={}, 
month={Sep.},}

@ARTICLE{Yu20191917,
author={Yu, Y. and Da, F. and Guo, Y.},
title={Sparse ICP with Resampling and Denoising for 3D Face Verification},
journal={IEEE Transactions on Information Forensics and Security},
year={2019},
volume={14},
number={7},
pages={1917-1927},
doi={10.1109/TIFS.2018.2889255},
art_number={8585047},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059045736&doi=10.1109%2fTIFS.2018.2889255&partnerID=40&md5=3d2e20cdc9894c8986e4bdf7a1fdd2d0},
affiliation={School of Automation, Southeast University, Nanjing, 210096, China},
abstract={Three-dimensional face recognition has shown its potential to obtain higher recognition accuracy than 2D methods. Among numerous face recognition methods, registration of two faces is comparatively intuitive. We propose a rigid registration method using surface resampling and denoising, which lowers the impact on registration residuals caused by sampling difference and noise, significantly improving the accuracy. While sparsityinducing norms reduce sensitivity to outliers and missing data, with preprocessing and region segmentation methods, our registration method is applied to face verification. Without datadriven learning or training, only residuals of rigid registration are utilized, and verification rates at 0.1% FAR are as follows: 100% for n versus n, 96.9% for n versus all, and 98.6% for ROC III experiment on FRGC v2.0 database, and 100% for n versus n and 95.7% for n versus all on Bosphorus database. Experiments show that the proposed algorithm outperforms the state-of-theart algorithms and is preferable in a verification scenario. © 2018 IEEE.},
author_keywords={3D face recognition;  3D face verification;  ICP},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xue2019,
author={Xue, J. and Zhang, Q. and Li, C. and Lang, W. and Wang, M. and Hu, Y.},
title={3D face profilometry based on galvanometer scanner with infrared fringe projection in high speed},
journal={Applied Sciences (Switzerland)},
year={2019},
volume={9},
number={7},
doi={10.3390/app9071458},
art_number={1458},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064123307&doi=10.3390%2fapp9071458&partnerID=40&md5=eb750afb2ce9eab954e95d4d1239d456},
affiliation={School of Aeronautics and Astronautics, Sichuan University, Chengdu, 610065, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, 610065, China; Artificial Intelligence Key Laboratory of Sichuan Province, Sichuan University of Science and Engineering, Zigong, 643000, China},
abstract={Structured light 3D shape metrology has become a very important technique and one of the hot research topics in 3D face recognition. However, it is still very challenging to use the digital light projector (DLP) in a 3D scanner and achieve high-speed, low-cost, small-size, and infrared-illuminated measurements. Instead of using a DLP, this paper proposes to use a galvanometer scanner to project phase-shifted fringes with a projection speed of infrared fringes up to 500 fps. Moreover, the measurement accuracy of multi-frequency (hierarchical) and multi-wavelength (heterodyne) temporal phase unwrapping approaches implemented in this system is analyzed. The measurement accuracy of the two methods is better than 0.2 mm. Comparisons are made between this method and the classical DLP approach. This method can achieve a similar accuracy and repeatability compared to the classical DLP method when a face mask is measured. The experiments on real human face indicate that this proposed method can improve the field of 3D scanning applications at a lower cost. © 2019 by the authors.},
author_keywords={3D face recognition;  Galvanometer scanner;  Infrared fringe projection;  Low-cost projector;  Optical 3D measurement},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yang2019,
author={Yang, F. and Shao, Q. and Cai, Z.},
title={Joint Identification Method Research of Access System Base on RFID and 3D Face Recognition},
journal={IOP Conference Series: Earth and Environmental Science},
year={2019},
volume={234},
number={1},
doi={10.1088/1755-1315/234/1/012099},
art_number={012099},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063400103&doi=10.1088%2f1755-1315%2f234%2f1%2f012099&partnerID=40&md5=578f666082074af503fded62a0165b27},
affiliation={School of Electrical Information Engineering, Foshan University, Foshan, 528000, China},
abstract={In this paper, the main contents of radio frequency identification, 3D(three-dimensional) face recognition, associative recognition algorithm based on radio frequency and 3D face recognition. The associative recognition method combined with radio frequency identification and 3D face recognition proposed in the paper, It can combine the advantages of stable radio frequency identification and high recognition rate and the advantages of 3D face recognition identity security to achieve the purpose of security, high recognition rate and strong operability of the entrance guard system. Therefore, it has broad application prospects in the access control system. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shi2019455,
author={Shi, B. and Zang, H. and Zheng, R. and Zhan, S.},
title={An efficient 3D face recognition approach using Frenet feature of iso-geodesic curves},
journal={Journal of Visual Communication and Image Representation},
year={2019},
volume={59},
pages={455-460},
doi={10.1016/j.jvcir.2019.02.002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061129042&doi=10.1016%2fj.jvcir.2019.02.002&partnerID=40&md5=8f0fe3cd51ff2049c71bfe756a9bf6d6},
affiliation={School of Computer and Information, Hefei University of Technology, Hefei, 230009, China},
abstract={Extracting efficient features from the large volume of 3D facial data directly is extremely difficult in 3D face recognition (3D-FR) with the latest methods, which mostly require heavy computations and manual processing steps. This paper presents a computationally efficient 3D-FR system based on a novel Frenet frame-based feature that is derived from the 3D facial iso-geodesic curves. In terms of the evaluation of the proposed method, we conducted a number of experiments on the CASIA 3D face database, and a superior recognition performance has been achieved. The performance evaluation suggests that the pose invariance attribute of the features relieves the need of an expensive 3D face registration in the face preprocessing procedure, where we take less time to process conversely. Our experiments further demonstrate that the proposed method not only achieves competitive recognition performance when compared with some existing techniques for 3D-FR, but also is computationally efficient. © 2019 Elsevier Inc.},
author_keywords={3D face recognition;  Facial curves;  Frenet framework;  Iso-geodesic;  Pose invariant},
document_type={Article},
source={Scopus},
}

@ARTICLE{Neto2019594,
author={Neto, J.B.C. and Marana, A.N.},
title={3D face recognition with reconstructed faces from a collection of 2D images},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11401 LNCS},
pages={594-601},
doi={10.1007/978-3-030-13469-3_69},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063066921&doi=10.1007%2f978-3-030-13469-3_69&partnerID=40&md5=ae38ed3a54a47df05e22f1ec3db64720},
affiliation={São Carlos Federal University - UFSCAR, São Carlos, SP  13565-905, Brazil; UNESP - São Paulo State University, Bauru, SP  17033-360, Brazil},
abstract={Nowadays, there is an increasing need for systems that can accurately and quickly identify a person. Traditional identification methods utilize something a person knows or something a person has. This kind of methods has several drawbacks, being the main one the fact that it is impossible to detect an imposter who uses genuine credentials to pass as a genuine person. One way to solve these kinds of problems is to utilize biometric identification. The face is one of the biometric features that best suits the covert identification. However, in general, biometric systems based on 2D face recognition perform very poorly in unconstrained environments, common in covert identification scenarios, since the input images present variations in pose, illumination, and facial expressions. One way to mitigate this problem is to use 3D face data, but the current 3D scanners are expensive and require a lot of cooperation from people being identified. Therefore, in this work, we propose an approach based on local descriptors for 3D Face Recognition based on 3D face models reconstructed from collections of 2D images. Initial results show 95% in a subset of the LFW Face dataset. © Springer Nature Switzerland AG 2019.},
author_keywords={3D face recognition;  3DLBP;  Biometrics;  Face reconstruction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hajati2019936,
author={Hajati, F. and Cheraghian, A. and Ameri Sianaki, O. and Zeinali, B. and Gheisari, S.},
title={Polar Topographic Derivatives for 3D Face Recognition: Application to Internet of Things Security},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={927},
pages={936-945},
doi={10.1007/978-3-030-15035-8_92},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064875340&doi=10.1007%2f978-3-030-15035-8_92&partnerID=40&md5=a1997efd234042ace42c04b05607ab05},
affiliation={College of Engineering and Science, Victoria University Sydney, Sydney, Australia; College of Engineering and Computer Science, The Australian National University, Canberra, Australia; Electrical Engineering Department, Iran University of Science and Technology, Tehran, Iran; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, Australia},
abstract={We propose Polar Topographic Derivatives (PTD) to fuse the shape and texture information of a facial surface for 3D face recognition. Polar Average Absolute Deviations (PAADs) of the Gabor topography maps are extracted as features. High-order polar derivative patterns are obtained by encoding texture variations in a polar neighborhood. By using the and Bosphorus 3D face database, our method shows that it is robust to expression and pose variations comparing to existing state-of-the-art benchmark approaches. © 2019, Springer Nature Switzerland AG.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Peter201977,
author={Peter, M. and Minoi, J.-L. and Hipiny, I.H.M.},
title={3D face recognition using kernel-based PCA approach},
journal={Lecture Notes in Electrical Engineering},
year={2019},
volume={481},
pages={77-86},
doi={10.1007/978-981-13-2622-6_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053242189&doi=10.1007%2f978-981-13-2622-6_8&partnerID=40&md5=c4b90626b4453034107d227cbd63ef3d},
affiliation={Faculty of Computer Science and Information Technology, Universiti Malaysia SarawakSarawak, Malaysia},
abstract={Face recognition is commonly used for biometric security purposes in video surveillance and user authentications. The nature of face exhibits non-linear shapes due to appearance deformations, and face variations presented by facial expressions. Recognizing faces reliably across changes in facial expression has proved to be a more difficult problem leading to low recognition rates in many face recognition experiments. This is mainly due to the tens degree-of-freedom in a non-linear space. Recently, non-linear PCA has been revived as it posed a significant advantage for data representation in high dimensionality space. In this paper, we experimented the use of non-linear kernel approach in 3D face recognition and the results of the recognition rates have shown that the kernel method outperformed the standard PCA. © Springer Nature Singapore Pte Ltd. 2019.},
author_keywords={3D face;  Facial recognition;  Kernel PCA},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dutta2019175,
author={Dutta, K. and Bhattacharjee, D. and Nasipuri, M. and Poddar, A.},
title={3D face recognition based on volumetric representation of range image},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={883},
pages={175-189},
doi={10.1007/978-981-13-3702-4_11},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061157145&doi=10.1007%2f978-981-13-3702-4_11&partnerID=40&md5=2543d63ceec9074d39578ea3320cdfbd},
affiliation={Department of Computer Science and Engineering, Jadavpur University, Kolkata, West Bengal, India; Computer Science Engineering, Birla Institute of Technology, Mesra, Ranchi, India},
abstract={In this paper, a 3D face recognition system has been developed based on the volumetric representation of 3D range image. The main approach to build this system is to calculate volume on some distinct region of 3D range face data. The system has mainly three steps. In the very first step, seven significant facial landmarks are identified on the face. Secondly, six distinct triangular regions A to F are created on the face using any three individual landmarks where nose tip is common to all regions. Further 3D volumes of all the respective triangular regions have been calculated based on plane fitting on the input range images. Finally, KNN and SVM classifiers are considered for classification. Initially, the classification and recognition are carried out on the different volumetric region, and a further combination of all the regions is considered. The proposed approach is tested on three useful challenging databases, namely Frav3D, Bosphorous, and GavabDB. © Springer Nature Singapore Pte Ltd. 2019.},
author_keywords={3D range image;  Classification;  Facial landmark;  Plane fitting;  Volumetric representation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Feng2019123,
author={Feng, J. and Guo, Q. and Guan, Y. and Wu, M. and Zhang, X. and Ti, C.},
title={3D face recognition method based on deep convolutional neural network},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={670},
pages={123-130},
doi={10.1007/978-981-10-8971-8_12},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050385395&doi=10.1007%2f978-981-10-8971-8_12&partnerID=40&md5=09db62184dc8c481f3e4cfdd015290df},
affiliation={School of Electronics and Information Engineering, Harbin Institute of Technology, Xidazhi Street. 92, Harbin, 150001, China; Beijing Automation Control Equipment Research Institute, Beijing, 100074, China; School of Information Engineering Branch, Yangling Vocational & Technical College, Weihuilu. 24, Yangling, Shanxi  712100, China},
abstract={In 2D face recognition, result may suffer from the impact of varying pose, expression, and illumination conditions. However, 3D face recognition utilizes depth information to enhance systematic robustness. Thus, an improved deep convolutional neural network (DCNN) combined with softmax classifier to identify face is trained. First, the preprocessing of color image and depth map is different in removing redundant information. Then, the feature extraction networks for 2D face image and depth map are, respectively, build with the principle of recognition rate maximization, and parameters about neural networks reset by a series of tests, in order to acquire higher recognition rate. At last, the fusion of two feature layers is the final input of artificial neural network (ANN) recognition system, which is followed by a 64-way softmax output. Experimental results demonstrate that it is effective in improving recognition rate. © Springer Nature Singapore Pte Ltd 2019.},
author_keywords={3D face recognition;  Deep convolutional neural network;  Depth map;  Feature extraction;  Feature fusion},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cirrincione2019253,
author={Cirrincione, G. and Marcolin, F. and Spada, S. and Vezzetti, E.},
title={Intelligent quality assessment of geometrical features for 3D face recognition},
journal={Smart Innovation, Systems and Technologies},
year={2019},
volume={103},
pages={253-264},
doi={10.1007/978-3-319-95095-2_24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051862957&doi=10.1007%2f978-3-319-95095-2_24&partnerID=40&md5=3eb73efa9d500b19b1f7a6291f6d10d3},
affiliation={DIGEP, Politecnico di Torino, Turin, Italy; Laboratory of LTI, Université de Picardie Jules Verne, Amiens, France; University of South Pacific, Suva, Fiji},
abstract={This paper proposes a methodology to assess the discriminative capabilities of geometrical descriptors referring to the public Bosphorus 3D facial database as testing dataset. The investigated descriptors include histogram versions of Shape Index and Curvedness, Euclidean and geodesic distances between facial soft-tissue landmarks. The discriminability of these features is evaluated through the analysis of single block of features and their meanings with different techniques. Multilayer perceptron neural network methodology is adopted to evaluate the relevance of the features, examined in different test combinations. Principle component analysis (PCA) is applied for dimensionality reduction. © Springer International Publishing AG, part of Springer Nature 2019.},
author_keywords={3D face recognition;  Dimensionality reduction;  Geometrical descriptors;  Neural network;  Principal component analysis},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Cirrincione2019153,
author={Cirrincione, G. and Marcolin, F. and Spada, S. and Vezzetti, E.},
title={Intelligent quality assessment of geometrical features for 3D face recognition},
journal={Smart Innovation, Systems and Technologies},
year={2019},
volume={102},
pages={153-164},
doi={10.1007/978-3-319-95098-3_14},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050746846&doi=10.1007%2f978-3-319-95098-3_14&partnerID=40&md5=19f63a5d1f6f7e44a5e0130cf4ebeaf0},
affiliation={DIGEP, Politecnico Di Torino, Turin, Italy; Laboratory LTI, Université de Picardie Jules Verne, Amiens, France; University of South Pacific, Suva, Fiji},
abstract={This paper proposes a methodology to assess the discriminative capabilities of geometrical descriptors referring to the public Bosphorus 3D facial database as testing dataset. The investigated descriptors include histogram versions of Shape Index and Curvedness, Euclidean and geodesic distances between facial soft-tissue landmarks. The discriminability of these features is evaluated through the analysis of single block of features and their meanings with different techniques. Multilayer perceptron neural network methodology is adopted to evaluate the relevance of the features, examined in different test combinations. Principle Component Analysis (PCA) is applied for dimensionality reduction. © 2019, Springer International Publishing AG, part of Springer Nature.},
author_keywords={3D face recognition;  Dimensionality reduction;  Geometrical descriptors;  Neural network;  Principal component analysis},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Pala2019,
author={Pala, P. and Berretti, S.},
title={Reconstructing 3D face models by incremental aggregation and refinement of depth frames},
journal={ACM Transactions on Multimedia Computing, Communications and Applications},
year={2019},
volume={15},
number={1},
doi={10.1145/3287309},
art_number={23},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061189013&doi=10.1145%2f3287309&partnerID=40&md5=b14174ca209c1c0dde3f8e3b04c0bfad},
affiliation={University of Florence, Via Santa Marta 3, Florence, 50139, Italy},
abstract={Face recognition from two-dimensional (2D) still images and videos is quite successful even with "in the wild" conditions. Instead, less consolidated results are available for the cases in which face data come from non-conventional cameras, such as infrared or depth. In this article, we investigate this latter scenario assuming that a low-resolution depth camera is used to perform face recognition in an uncooperative context. To this end, we propose, first, to automatically select a set of frames from the depth sequence of the camera because they provide a good view of the face in terms of pose and distance. Then, we design a progressive refinement approach to reconstruct a higher-resolution model from the selected low-resolution frames. This process accounts for the anisotropic error of the existing points in the current 3D model and the points in a newly acquired frame so that the refinement step can progressively adjust the point positions in the model using a Kalman-like estimation. The quality of the reconstructed model is evaluated by considering the error between the reconstructed models and their corresponding high-resolution scans used as ground truth. In addition, we performed face recognition using the reconstructed models as probes against a gallery of reconstructed models and a gallery with high-resolution scans. The obtained results confirm the possibility to effectively use the reconstructed models for the face recognition task. © 2019 Copyright held by the owner/author(s).},
author_keywords={3D face recognition;  3D reconstruction;  Anisotropic error;  Depth data},
document_type={Article},
source={Scopus},
}

@CONFERENCE{ZulqarnainGilani20181896,
author={Zulqarnain Gilani, S. and Mian, A.},
title={Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2018},
pages={1896-1905},
doi={10.1109/CVPR.2018.00203},
art_number={8578301},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060207182&doi=10.1109%2fCVPR.2018.00203&partnerID=40&md5=7a2227c884147feae86f1d25240e9a7a},
affiliation={Computer Science and Software Engineering, University of Western Australia, Australia},
abstract={Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhou2018,
author={Zhou, S. and Xiao, S.},
title={3D face recognition: a survey},
journal={Human-centric Computing and Information Sciences},
year={2018},
volume={8},
number={1},
doi={10.1186/s13673-018-0157-2},
art_number={35},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057339588&doi=10.1186%2fs13673-018-0157-2&partnerID=40&md5=b87148e43a6001284a758168a66ede05},
affiliation={College of Computer Science and Electronic Engineering, Hunan University, South Lushan Road, Yuelu District, Changsha, 410082, China; National Collaborative Innovation Center for High Performance Computing, Beijing, China},
abstract={3D face recognition has become a trending research direction in both industry and academia. It inherits advantages from traditional 2D face recognition, such as the natural recognition process and a wide range of applications. Moreover, 3D face recognition systems could accurately recognize human faces even under dim lights and with variant facial positions and expressions, in such conditions 2D face recognition systems would have immense difficulty to operate. This paper summarizes the history and the most recent progresses in 3D face recognition research domain. The frontier research results are introduced in three categories: pose-invariant recognition, expression-invariant recognition, and occlusion-invariant recognition. To promote future research, this paper collects information about publicly available 3D face databases. This paper also lists important open problems. © 2018, The Author(s).},
author_keywords={3D face database;  3D face recognition;  Expression-invariant face recognition;  Occlusion-invariant face recognition;  Pose-invariant face recognition},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Reji2018,
author={Reji, R. and Sojanlal, P.},
title={Region Based 3D Face Recognition},
journal={2017 IEEE International Conference on Computational Intelligence and Computing Research, ICCIC 2017},
year={2018},
doi={10.1109/ICCIC.2017.8524581},
art_number={8524581},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057946157&doi=10.1109%2fICCIC.2017.8524581&partnerID=40&md5=92c092fa4e2838ab444fcc0356fcdb75},
affiliation={Mahatma Gandhi University, School of Computer Sciences, Kottayam, Kerala, India; Mar-Baselious Institute of Technology Science, Kothamangalam, Kerala, India},
abstract={This paper focuses on a region based methodology for expression in sensitive 3D face recognition process. Considering facial regions that are comparatively unchanging during expressions, results shows that using fifteen sub regions on the face can attain high 3D face recognition. We use a modified face recognition algorithm along with hierarchical contour based image registration for finding the similarity score. Our method operates in two modes: verification mode and confirmation mode. Crop 100 mm of frontal face region, apply preprocessing and automatically detect nose tip, translate the face image to origin and crop fifteen sub regions. The cropped sub regions are defined by cuboids which occupy more volumetric data, Nose Tip is the most projecting point of the face with the highest value along Z-axis so consider it as origin. The modified face recognition algorithm reduces the effects caused by facial expressions and artifacts. Finally a Hierarchical contour based image registration technique is applied which yields better results. The approach is applied on Bosphorus 3D datasets and achieved a verification rate of 95.3% at 0.1% false acceptance rate. In the identification scenario 99.3% rank one recognition is achieved. © 2017 IEEE.},
author_keywords={3D face recognition;  Biometrics;  Contour based image registration;  MFRA;  Rank based Score},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2018,
author={Chen, X. and Lu, Y. and Fang, R.},
title={3D face recognition based on empirical mode decomposition and sparse representation},
journal={ACM International Conference Proceeding Series},
year={2018},
doi={10.1145/3207677.3278100},
art_number={a78},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056823589&doi=10.1145%2f3207677.3278100&partnerID=40&md5=f9c74fb6ca6e983205b3b33030481b84},
affiliation={Jilin University, Changchun, China},
abstract={With 1 the research interest increasing in 3D face recognition, many methods for 3D face recognition have emerged in recent years. In this paper, a novel method named as decomposition-based face classification (DFC) is proposed to recognize a 3D face by using empirical mode decomposition (EMD) and sparse representation. A 3D face scan is firstly decomposed into several surfaces by a 3D EMD algorithm, then for each decomposed surface, meshSIFT is used for detecting salient points and extracting the local descriptor, and the dictionary is constructed by concatenating all the local descriptors. Finally, the identification of a probe face can be determined by the multitask sparse representation classification. Experimental results demonstrate that our decomposition-based method can achieve state-of-the-art performance on two benchmark databases, Bosphorus and FRGC2.0. © 2018 Association for Computing Machinery. ACM.},
author_keywords={EMD;  Face Recognition;  MeshSIFT;  SRC},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Siqueira20183513,
author={Siqueira, R.S. and Alexandre, G.R. and Soares, J.M. and The, G.A.P.},
title={Triaxial slicing for 3-D face recognition from adapted rotational invariants spatial moments and minimal keypoints dependence},
journal={IEEE Robotics and Automation Letters},
year={2018},
volume={3},
number={4},
pages={3513-3520},
doi={10.1109/LRA.2018.2854295},
art_number={8408720},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063307357&doi=10.1109%2fLRA.2018.2854295&partnerID=40&md5=5205595c202573ef77808eaf4aee1626},
affiliation={Faculty of Electrical Engineering, Instituto Federal de Educação, Ciência e Tecnologia Do Ceará, Fortaleza, CE 60410-42, Brazil; Department of Teleinformatics Engineering, Universidade Federal Do Ceará, Fortaleza, CE 60020-181, Brazil},
abstract={This letter presents a multiple slicing model for three-dimensional (3-D) images of human face, using the frontal, sagittal, and transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2-D spatial moments of Hu and 3-D spatial invariant rotation moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs nonneutral ROC I experiment, applying linear discriminant analysis as classifier and more than one sample for training, achieved 98.7% of verification rate at 0.1% of false acceptance rate. By using the support vector machine as classifier the rank1 experiment recognition rates of 99% and 95.4% have been achieved for a neutral vs neutral and for a neutral vs non-neutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when anger and disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity. © 2016 IEEE.},
author_keywords={Computer vision for automation;  recognition;  surveillance systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fei2018139,
author={Fei, H. and Tu, B. and Chen, Q. and He, D. and Zhou, C. and Peng, Y.},
title={An overview of face-related technologies},
journal={Journal of Visual Communication and Image Representation},
year={2018},
volume={56},
pages={139-143},
doi={10.1016/j.jvcir.2018.09.012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053774931&doi=10.1016%2fj.jvcir.2018.09.012&partnerID=40&md5=ddfc4618812dd5bf104f59915ffc5af0},
affiliation={College of Information and Communication Engineering, School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Computer and Information, Hefei University of Technology, Hefei, China},
abstract={In recent years, information technology is developing continuously and set off a burst of artificial intelligence boom in the field of science. The development of advanced technologies such as unmanned driving and AI chips, is the extensive application of artificial intelligence. Face-related technologies have a wide range of applications because of intuitive results and good concealment. Since 3D face information can provide more comprehensive facial information than 2D face information, and it can solve many difficulties that cannot be solved in 2D face recognition. Therefore, more and more researchers have studied 3D face recognition in recent years. Under the new circumstances, the research on face are experiencing all kinds of challenges. With the tireless of many scientists, the new technology is also making a constant progress, and in the development of many technologies it still maintained its leading position. In this paper, we simply sort out the present development process of facial correlation technology, and the general evolution of this technology is outlined. Finally, the practical significance of this technology development is briefly discussed. © 2018},
author_keywords={3D face reconstruction;  Deep learning;  Face enhancement;  Face recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abbad2018525,
author={Abbad, A. and Abbad, K. and Tairi, H.},
title={3D face recognition: Multi-scale strategy based on geometric and local descriptors},
journal={Computers and Electrical Engineering},
year={2018},
volume={70},
pages={525-537},
doi={10.1016/j.compeleceng.2017.08.017},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028606046&doi=10.1016%2fj.compeleceng.2017.08.017&partnerID=40&md5=5414b01946f345f018c49654dff50957},
affiliation={LIIAN, Department of Computer Science, Faculty of Sciences Dhar El Mahraz University Sidi Mohamed Ben Abdelah, Fez, Morocco; LISA, Department of Computer Science, Faculty of Science and Technology University Sidi Mohamed Ben Abdelah, Fez, Morocco},
abstract={Most human expression variations cause a non-rigid deformation of face scans, which is a challenge today. In this article, we present a novel framework for 3D face recognition that uses a geometry and local shape descriptor in a matching process to overcome the distortions caused by expressions in faces. This algorithm consists of four major components. First, the 3D face model is presented at different scales. Second, isometric-invariant features on each scale are extracted. Third, the geometric information is obtained on the 3D surface in terms of radial and level facial curves. Fourth, the feature vectors on each scale are concatenated with their corresponding geometric information. We conducted a number of experiments using two well-known and challenging datasets, namely, the GavabDB and Bosphorus datasets, and superior recognition performance has been achieved. The new system displays an overall rank-1 identification rate of 98.9% for all faces with neutral and non-neutral expressions on the GavabDB database. © 2017 Elsevier Ltd},
author_keywords={3D face recognition;  Expression;  Facial curves;  Geometric features;  Local features},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kumar201811,
author={Kumar, S. and Tripathi, B.K.},
title={On the root-power mean aggregation based neuron in quaternionic domain},
journal={International Journal of Intelligent Systems and Applications},
year={2018},
volume={10},
number={7},
pages={11-26},
doi={10.5815/ijisa.2018.07.02},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049396089&doi=10.5815%2fijisa.2018.07.02&partnerID=40&md5=b120b1db1c45a14ead152da69668d3b1},
affiliation={Harcourt Butler Technical University, Department of Computer Science and Engineering, Kanpur, 208002, India},
abstract={This paper illustrates the new structure of artificial neuron based on root-power means (RPM) for quaternionic-valued signals and also presented an efficient learning process of neural networks with quaternionic-valued root-power means neurons (H-RPMN). The main aim of this neuron is to present the potential capability of a nonlinear aggregation operation on the quaternionic-valued signals in neuron cell. A wide spectrum of aggregation ability of RPM in between minima and maxima has a beautiful property of changing its degree of compensation in the natural way which emulates the various existing neuron models as its special cases. Further, the quaternionic resilient propagation algorithm (H-RPROP) with error-dependent weight backtracking step significantly accelerates the training speed and exhibits better approximation accuracy. The wide spectrums of benchmark problems are considered to evaluate the performance of proposed quaternionic root-power mean neuron with H-RPROP learning algorithm. © 2018 MECS.},
author_keywords={3D face recognition;  Quasi-arithmetic means;  Quaternionic resilient propagation;  Quaternionic-valued backpropagation;  Quaternionic-valued multilayer perceptron;  Root-power means in quaternionic domain (H)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao2018120,
author={Gao, J. and Evans, A.N.},
title={Expression robust 3D face landmarking using thresholded surface normals},
journal={Pattern Recognition},
year={2018},
volume={78},
pages={120-132},
doi={10.1016/j.patcog.2018.01.011},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042375361&doi=10.1016%2fj.patcog.2018.01.011&partnerID=40&md5=a93105edc5d4f49296e9c43d397e9898},
affiliation={Department of Medical Biochemistry and Microbiology, Uppsala University, Uppsala, Sweden; Department of Electronic and Electrical Engineering, University of Bath, Bath, United Kingdom},
abstract={3D face recognition is an increasing popular modality for biometric authentication, for example in the iPhoneX. Landmarking plays a significant role in region based face recognition algorithms. The accuracy and consistency of the landmarking will directly determine the effectiveness of feature extraction and hence the overall recognition performance. While surface normals have been shown to provide high performing features for face recognition, their use in landmarking has not been widely explored. To this end, a new 3D facial landmarking algorithm based on thresholded surface normal maps is proposed, which is applicable to widely used 3D face databases. The benefits of employing surface normals are demonstrated for both facial roll and yaw rotation calibration and nasal landmarks localization. Results on the Bosphorus, FRGC and BU-3DFE databases show that the detected landmarks possess high within-class consistency and accuracy under different expressions. For several key landmarks the performance achieved surpasses that of state-of-the-art techniques and is also training free and computationally efficient. The use of surface normals therefore provides a useful representation of the 3D surface and the proposed landmarking algorithm provides an effective approach to localising the key nasal landmarks. © 2018 Elsevier Ltd},
author_keywords={3D face landmarking;  Surface normals},
document_type={Article},
source={Scopus},
}

@ARTICLE{Truong2018119,
author={Truong, K.T. and Le, T.H.},
title={Video-based face recognition using shape and texture information in 3D Morphable Model},
journal={JP Journal of Heat and Mass Transfer},
year={2018},
volume={15},
number={Special Issue 1},
pages={119-124},
doi={10.17654/HMSI118119},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050083363&doi=10.17654%2fHMSI118119&partnerID=40&md5=0d768b750e9373900b4773eec844cfbf},
affiliation={Department of Computer Science, University of Science, VNU-HCM, Ho Chi Minh City, Viet Nam},
abstract={In this paper, a novel approach, based on combining 3D Morphable Model (3DMM) to shape vector and texture variance, is proposed for face recognition in video (called 3DMM-S-TV). In detail, the system fits a face video to a 3DMM, then utilizes shape fitting coefficients and texture info to recognize face. For this purpose: (1) apply 3DMM to reconstruct 3D face; (2) form a shape vector to present each face in video; (3) calculate texture variance of each face; (4) use shape vector to estimate a face gallery in training data similar to faces in test data; (5) use minimum texture variance to identify objects from the gallery. Proposed methods are evaluated on two face video databases (YouTube Celebrities and FAMED). © 2018 Pushpa Publishing House, Allahabad, India.},
author_keywords={3D face recognition;  Face recognition using 3DMM},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ahdid201873,
author={Ahdid, R. and Taifi, K. and Said, S. and Fakir, M. and Manaut, B.},
title={Automatic face recognition system using iso-geodesic curves in riemanian manifold},
journal={Proceedings - 2017 14th International Conference on Computer Graphics, Imaging and Visualization, CGiV 2017},
year={2018},
pages={73-78},
doi={10.1109/CGiV.2017.25},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048323921&doi=10.1109%2fCGiV.2017.25&partnerID=40&md5=73b9e65d367cee00cdcb28e7b5ef55cb},
affiliation={Sultan Moulay Slimane University, Beni Mellal, Morocco},
abstract={In this paper, we present an automatic 3D face recognition system. This system is based on the representation of human faces surfaces as collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two facial surfaces, we compute a geodesic distance between a pair of facial curves using a Riemannian geometry. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008). © 2017 IEEE.},
author_keywords={3D face recognition;  facial surfaces;  geodesic distance;  iso-geodesic curves;  Riemannian geometry},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ahdid201844,
author={Ahdid, R. and Said, S. and Fakir, M. and Manaut, B. and Ouadid, Y. and Taifi, K.},
title={Three dimensional face surface recognition by geodesic distance using jacobi iterations},
journal={Proceedings - 2017 14th International Conference on Computer Graphics, Imaging and Visualization, CGiV 2017},
year={2018},
pages={44-48},
doi={10.1109/CGiV.2017.28},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048323143&doi=10.1109%2fCGiV.2017.28&partnerID=40&md5=325b8a88194db0d808a9ef00987e1535},
affiliation={Sultan Moulay Slimane University, Beni Mellal, Morocco},
abstract={In this paper, we present an automatic application of 3D face recognition system using geodesic distance in Riemannian geometry. We consider, in this approach, the three dimensional face images as residing in Riemannian manifold and we compute the geodesic distance using the Jacobi iterations as a solution of the Eikonal equation. The problem of solving the Eikonal equation, unstructured simplified meshes of 3D face surface, such as tetrahedral and triangles are important for accurately modeling material interfaces and curved domains, which are approximations to curved surfaces in R3. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008). © 2017 IEEE.},
author_keywords={3D face recognition;  Eikonal equation;  geodesic distance;  Jacobi iterations;  Riemannian geometry},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ramalingam201825,
author={Ramalingam, S.},
title={Fuzzy interval-valued multi criteria based decision making for ranking features in multi-modal 3D face recognition},
journal={Fuzzy Sets and Systems},
year={2018},
volume={337},
pages={25-51},
doi={10.1016/j.fss.2017.06.002},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024108581&doi=10.1016%2fj.fss.2017.06.002&partnerID=40&md5=903332f6c0cb447ada7cd083cb5492cd},
affiliation={Division of Electronics and Communications Engineering, School of Engineering and Technology, University of Hertfordshire, Hatfield, WD6 4UB, United Kingdom},
abstract={This paper describes an application of multi-criteria decision making (MCDM) for multi-modal fusion of features in a 3D face recognition system. A decision making process is outlined that is based on the performance of multi-modal features in a face recognition task involving a set of 3D face databases. In particular, the fuzzy interval valued MCDM technique called TOPSIS is applied for ranking and deciding on the best choice of multi-modal features at the decision stage. It provides a formal mechanism of benchmarking their performances against a set of criteria. The technique demonstrates its ability in scaling up the multi-modal features. © 2017},
author_keywords={3D face recognition;  Evidential reasoning under uncertainty;  Fuzzy fusion;  Interval values;  MCDM;  Multi-modal features;  TOPSIS},
document_type={Article},
source={Scopus},
}

@ARTICLE{NourbakhshKaashki201866,
author={Nourbakhsh Kaashki, N. and Safabakhsh, R.},
title={RGB-D face recognition under various conditions via 3D constrained local model},
journal={Journal of Visual Communication and Image Representation},
year={2018},
volume={52},
pages={66-85},
doi={10.1016/j.jvcir.2018.02.003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042322207&doi=10.1016%2fj.jvcir.2018.02.003&partnerID=40&md5=2b91afeff8a19c987036cf8a30b610dc},
affiliation={Department of Computer Engineering, Amirkabir University of Technology, Tehran, Iran},
abstract={This research proposes a method for 3D face recognition in various conditions using 3D constrained local model (CLM-Z). In this method, a combination of 2D images (RGBs) and depth images (Ds) captured by Kinect has been used. After detecting the face and smoothing the depth image, CLM-Z model has been used to model and detect the important points of the face. These points are described using Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), and 3D Local Binary Patterns (3DLBP). Finally, each face is recognized by a Support Vector Machine (SVM). The challenging situations are changes of lighting, facial expression and head pose. The results on CurtinFaces and IIIT-D datasets demonstrate that the proposed method outperformed state-of-the-art methods under illumination, expression and pitch pose conditions and comparable results were obtained in other cases. Additionally, our proposed method is robust even when the training data has not been carefully collected. © 2018 Elsevier Inc.},
author_keywords={3D constrained local model;  3D face recognition;  Depth image;  Face model;  Facial expression;  Feature descriptor;  Head pose;  Kinect;  Lighting},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Abbad20181,
author={Abbad, A. and Abbad, K. and Tairi, H.},
title={3D face recognition in the presence of facial expressions based on empirical mode decomposition},
journal={ACM International Conference Proceeding Series},
year={2018},
volume={2018-March},
pages={1-6},
doi={10.1145/3177148.3180087},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047112949&doi=10.1145%2f3177148.3180087&partnerID=40&md5=93db67c347fc7dadc31256f7eb69e983},
affiliation={Laboratory LIIAN, Faculty of Science, Dhar EL Mahraz, Morocco; Laboratory ISA, Faculty of Science and Technology, Morocco},
abstract={This paper presents an efficient 3D face recognition method to handle facial expression. The proposed method uses the Surfaces Empirical Mode Decomposition (SEMD), facial curves and local shape descriptor in a matching process to overcome the distortions caused by expressions in faces. The basic idea is that, the face is presented at different scales by SEMD. Then the isometric-invariant features on each scale are extracted. After that, the geometric information is obtained on the 3D surface in terms of radial and level facial curves. Finally, the feature vectors on each scale are associated with their corresponding geometric information. The presented method is validated on GavabDB database resulting a rank 1 recognition rate (RR) of 98.9% for all faces with neutral and non-neutral expressions. This result outperforms other 3D expression-invariant face recognition methods on the same database. © 2018 Association for Computing Machinery.},
author_keywords={3D face recognition;  EMD;  Expression;  Facial curves;  Geometric features;  Local features},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Guo2018,
author={Guo, Y. and Wei, R. and Liu, Y.},
title={Weighted gradient feature extraction based on multiscale sub-blocks for 3d facial recognition in bimodal images},
journal={Information (Switzerland)},
year={2018},
volume={9},
number={3},
doi={10.3390/info9030048},
art_number={48},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042711391&doi=10.3390%2finfo9030048&partnerID=40&md5=fe75121af64143005ff6ec84435ed59d},
affiliation={School of Computer Science and Engineering, Hebei University of Technology, Tianjin, 300400, China},
abstract={In this paper, we propose a bimodal 3D facial recognition method aimed at increasing the recognition rate and reducing the effect of illumination, pose, expression, ages, and occlusion on facial recognition. There are two features extracted from the multiscale sub-blocks in both the 3D mode depth map and 2D mode intensity map, which are the local gradient pattern (LGP) feature and the weighted histogram of gradient orientation (WHGO) feature. LGP and WHGO features are cascaded to form the 3D facial feature vector LGP-WHGO, and are further trained and identified by the support vector machine (SVM). Experiments on the CASIA database, FRGC v2.0 database, and Bosphorus database show that, the proposed method can efficiently extract the structure information and texture information of the facial image, and have a robustness to illumination, expression, occlusion and pose. © 2018 by the authors.},
author_keywords={3D face recognition;  Bimodal;  Depth map;  Intensitymap;  LGP-WHGO;  Multiscale sub-blocks},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Soltanpour20182811,
author={Soltanpour, S. and Wu, Q.M.J.},
title={High-order local normal derivative pattern (LNDP) for 3D face recognition},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2018},
volume={2017-September},
pages={2811-2815},
doi={10.1109/ICIP.2017.8296795},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045292850&doi=10.1109%2fICIP.2017.8296795&partnerID=40&md5=12c33512c432220af4c4c49f5116433e},
affiliation={University of Windsor, Department of Electrical and Computer Engineering, Windsor, Canada},
abstract={This paper proposes a novel descriptor based on the local derivative pattern (LDP) for 3D face recognition. Compared to the local binary pattern (LBP), LDP can capture more detailed information by encoding directional pattern features. It is based on the local derivative variations that extract high-order local information. We propose a novel discriminative facial shape descriptor, local normal derivative pattern (LNDP) that extracts LDP from the surface normal. Using surface normal, the orientation of a surface at each point is determined as a first-order surface differential. Three normal component images are extracted by estimating three components of normal vectors in x, y, and z channels. Each normal component is divided into several patches and encoded using LDP. The final descriptor is created by concatenating histograms of the LNDP on each patch. Experimental results on two famous 3D face databases, FRGC v2.0 and Bosphorus illustrate the effectiveness of the proposed descriptor. © 2017 IEEE.},
author_keywords={3D face;  High-order local pattern;  Local derivative pattern;  Surface normal},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li20181295,
author={Li, Y. and Wang, Y. and Liu, J. and Hao, W.},
title={Expression-insensitive 3D face recognition by the fusion of multiple subject-specific curves},
journal={Neurocomputing},
year={2018},
volume={275},
pages={1295-1307},
doi={10.1016/j.neucom.2017.09.070},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040674183&doi=10.1016%2fj.neucom.2017.09.070&partnerID=40&md5=ecfea39a1b4d26ab1e33c34b50b87558},
affiliation={Institute of Computer Science & Engineering, Xi'an University of Technology, No.5 South Jinhua Road, Xi'an, 710048, China},
abstract={This study proposes a 3D face recognition method using multiple subject-specific curves insensitive to intra-subject distortions caused by expression variations. Considering that most sharp variances in facial convex regions are closely related to the bone structure, the convex crest curves are first extracted as the most vital subject-specific facial curves based on the principal curvature extrema in convex local surfaces. Then, the central profile curve and the horizontal contour curve passing through the nose tip are detected by using the precise localization of the nose tip and symmetry plane. Based on their discriminative power and robustness to expression changes, the three types of curves are fused with appropriate weights at the feature-level and used for matching 3D faces with the iterative closest point algorithm. The combination of multiple expression-insensitive curves is complementary and provides sufficient and stable facial surface features for face recognition. In addition, for each convex crest curve, an expression-irrelevant factor is assigned as the adaptive weight to improve the face matching performance. The results of experiments using two public 3D databases, GavabDB and BU-3DFE, demonstrate the effectiveness of the proposed method, and its recognition rates on both databases reflect an encouraging performance. © 2017 Elsevier B.V.},
author_keywords={3D face recognition;  Expression-insensitive;  Feature-level;  Fusion;  Subject-specific curve},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kim2018133,
author={Kim, D. and Hernandez, M. and Choi, J. and Medioni, G.},
title={Deep 3D face identification},
journal={IEEE International Joint Conference on Biometrics, IJCB 2017},
year={2018},
volume={2018-January},
pages={133-142},
doi={10.1109/BTAS.2017.8272691},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046247920&doi=10.1109%2fBTAS.2017.8272691&partnerID=40&md5=9660021125b86249b9768912252c19ff},
affiliation={USC Institute for Robotics and Intelligent Systems (IRIS), University of Southern California, United States},
abstract={We propose a novel 3D face recognition algorithm using a deep convolutional neural network (DCNN) and a 3D face expression augmentation technique. The performance of 2D face recognition algorithms has significantly increased by leveraging the representational power of deep neural networks and the use of large-scale labeled training data. In this paper, we show that transfer learning from a CNN trained on 2D face images can effectively work for 3D face recognition by fine-tuning the CNN with an extremely small number of 3D facial scans. We also propose a 3D face expression augmentation technique which synthesizes a number of different facial expressions from a single 3D face scan. Our proposed method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets without using hand-crafted features. The 3D face identification using our deep features also scales well for large databases. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2018234,
author={Li, H. and Sun, J. and Chen, L.},
title={Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition},
journal={IEEE International Joint Conference on Biometrics, IJCB 2017},
year={2018},
volume={2018-January},
pages={234-242},
doi={10.1109/BTAS.2017.8272703},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046291057&doi=10.1109%2fBTAS.2017.8272703&partnerID=40&md5=e5c4065442f8bc899597efdc443ed01c},
affiliation={School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, 710049, China; Department of Mathematics and Informatics, Ecole Centrale de Lyon, Lyon, 69134, France},
abstract={This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01%, 97.60%, and 96.13% on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhao2018207,
author={Zhao, J.-L. and Wu, Z.-K. and Pan, Z.-K. and Duan, F.-Q. and Li, J.-H. and Lv, Z.-H. and Wang, K. and Chen, Y.-C.},
title={3D Face Similarity Measure by Fréchet Distances of Geodesics},
journal={Journal of Computer Science and Technology},
year={2018},
volume={33},
number={1},
pages={207-222},
doi={10.1007/s11390-018-1814-7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041342736&doi=10.1007%2fs11390-018-1814-7&partnerID=40&md5=8478bdecdc0eeeb6cb4dda3bb1ecda52},
affiliation={School of Data Science and Software Engineering, Qingdao University, Qingdao, 266071, China; College of Automation and Electrical Engineering, Qingdao University, Qingdao, 266071, China; College of Information Science and Technology, Beijing Normal University, Beijing, 100087, China; College of Computer Science and Technology, Qingdao University, Qingdao, 266071, China; School of Management, Capital Normal University, Beijing, 100048, China},
abstract={3D face similarity is a critical issue in computer vision, computer graphics and face recognition and so on. Since Fréchet distance is an effective metric for measuring curve similarity, a novel 3D face similarity measure method based on Fréchet distances of geodesics is proposed in this paper. In our method, the surface similarity between two 3D faces is measured by the similarity between two sets of 3D curves on them. Due to the intrinsic property of geodesics, we select geodesics as the comparison curves. Firstly, the geodesics on each 3D facial model emanating from the nose tip point are extracted in the same initial direction with equal angular increment. Secondly, the Fréchet distances between the two sets of geodesics on the two compared facial models are computed. At last, the similarity between the two facial models is computed based on the Fréchet distances of the geodesics obtained in the second step. We verify our method both theoretically and practically. In theory, we prove that the similarity of our method satisfies three properties: reflexivity, symmetry, and triangle inequality. And in practice, experiments are conducted on the open 3D face database GavaDB, Texas 3D Face Recognition database, and our 3D face database. After the comparison with iso-geodesic and Hausdorff distance method, the results illustrate that our method has good discrimination ability and can not only identify the facial models of the same person, but also distinguish the facial models of any two different persons. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={3D face;  Fréchet distance;  geodesic;  similarity measure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lv2018,
author={Lv, C. and Zhao, J.},
title={3D Face Recognition based on Local Conformal Parameterization and Iso-Geodesic Stripes Analysis},
journal={Mathematical Problems in Engineering},
year={2018},
volume={2018},
doi={10.1155/2018/4707954},
art_number={4707954},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056267138&doi=10.1155%2f2018%2f4707954&partnerID=40&md5=6b9608dc984f786b68071250cc5e66cb},
affiliation={College of Information Science and Technology, Beijing Normal University, Beijing, China; Engineering Research Center of Virtual Reality and Applications, Ministry of Education, Beijing Key Laboratory of Digital Preservation, Beijing, 100875, China; School of Data Science and Software Engineering, Qingdao University, Qingdao, China; College of Automation and Electrical Engineering, Qingdao University, Qingdao, China},
abstract={3D face recognition is an important topic in the field of pattern recognition and computer graphic. We propose a novel approach for 3D face recognition using local conformal parameterization and iso-geodesic stripes. In our framework, the 3D facial surface is considered as a Riemannian 2-manifold. The surface is mapped into the 2D circle parameter domain using local conformal parameterization. In the parameter domain, the geometric features are extracted from the iso-geodesic stripes. Combining the relative position measure, Chain 2D Weighted Walkthroughs (C2DWW), the 3D face matching results can be obtained. The geometric features from iso-geodesic stripes in parameter domain are robust in terms of head poses, facial expressions, and some occlusions. In the experiments, our method achieves a high recognition accuracy of 3D facial data from the Texas3D and Bosphorus3D face database. © 2018 Chenlei Lv and Junli Zhao.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2018141,
author={Wang, X.},
title={3D face recognition based on regional shape maps},
journal={Journal of Advanced Computational Intelligence and Intelligent Informatics},
year={2018},
volume={22},
number={1},
pages={141-146},
doi={10.20965/jaciii.2018.p0141},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041116479&doi=10.20965%2fjaciii.2018.p0141&partnerID=40&md5=6ea7a6070e2cb83b72ff696c4d5c6c18},
affiliation={School of Applied Science, Beijing Information Science and Technology University, No. 12, Qing He Xiao Ying East Road, Haidian District, Beijing, China},
abstract={This study proposes an iterative closest shape point (ICSP) registration method based on regional shape maps for 3D face recognition. A neutral expression image randomly selected from a face database is considered as the reference face. The point-to-point correspondences between the input face and the reference face are achieved by constructing the points' regional shape maps. The distance between corresponding point pairs is then minimized by iterating through the correspondence findings and coordinate transformations. The vectors composed of the closest shape points obtained in the last iteration are regarded as the feature vectors of the input face. These 3D face feature vectors are finally used for both training and recognition using the Fisherface method. Experiments are conducted using the 3D face database maintained by the Chinese Academy of Science Institute of Automation (CASIA). The results show that the proposed method can effectively improve 3D face recognition performance.},
author_keywords={3D face recognition;  Corresponding point;  Iterative closest shape point;  Regional shape map},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ratyal20184903,
author={Ratyal, N. and Taj, I. and Bajwa, U. and Sajid, M.},
title={Pose and expression invariant alignment based multi-view 3d face recognition},
journal={KSII Transactions on Internet and Information Systems},
year={2018},
volume={12},
number={10},
pages={4903-4929},
doi={10.3837/tiis.2018.10.016},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057280833&doi=10.3837%2ftiis.2018.10.016&partnerID=40&md5=b7937933dbf1206dd6e5a81101a545c0},
affiliation={Vision and Pattern Recognition Systems Research Group, Capital University of Science and Technology (CUST), Islamabad, Pakistan; Department of Computer Science, COMSATS Institute of Information Technology, Lahore, Pakistan},
abstract={In this study, a fully automatic pose and expression invariant 3D face alignment algorithm is proposed to handle frontal and profile face images which is based on a two pass course to fine alignment strategy. The first pass of the algorithm coarsely aligns the face images to an intrinsic coordinate system (ICS) through a single 3D rotation and the second pass aligns them at fine level using a minimum nose tip-scanner distance (MNSD) approach. For facial recognition, multi-view faces are synthesized to exploit real 3D information and test the efficacy of the proposed system. Due to optimal separating hyper plane (OSH), Support Vector Machine (SVM) is employed in multi-view face verification (FV) task. In addition, a multi stage unified classifier based face identification (FI) algorithm is employed which combines results from seven base classifiers, two parallel face recognition algorithms and an exponential rank combiner, all in a hierarchical manner. The performance figures of the proposed methodology are corroborated by extensive experiments performed on four benchmark datasets: GavabDB, Bosphorus, UMB-DB and FRGC v2.0. Results show mark improvement in alignment accuracy and recognition rates. Moreover, a computational complexity analysis has been carried out for the proposed algorithm which reveals its superiority in terms of computational efficiency as well. © 2018 KSII.},
author_keywords={3D alignment;  3D FR;  Profile face;  SVM;  Unified classifier},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khan2018220,
author={Khan, M.S. and Jehanzeb, M. and Babar, M.I. and Faisal, S. and Ullah, Z. and Amin, S.Z.B.M.},
title={Face recognition analysis using 3D model},
journal={Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
year={2018},
volume={200},
pages={220-236},
doi={10.1007/978-3-319-95450-9_19},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051123902&doi=10.1007%2f978-3-319-95450-9_19&partnerID=40&md5=3b4f204837bae6e24de550e8aa4fd707},
affiliation={College of Computer Science, Sichuan University, Chengdu, 610065, China; Department of Computer Science, Army Public College of Management & Sciences (APCOMS), Rawalpindi, Punjab, Pakistan; Department of Computer Science, University of Haripur, Hattar Road Near Swat Chowk, Haripur, Khyber Pakhtunkhwa  22620, Pakistan; Federation University Australia, Mt Helen, Ballarat, VIC  3350, Australia; Western China Earthquake and Hazards Mitigation Research Centre, College of Architecture and Environment, Sichuan University, Chengdu, 610065, China},
abstract={Facial Recognition is a commonly used technology in security-related applications. It has been thoroughly studied and scrutinized for its number of practical real-world applications. On the road ahead of understanding this technology, there remain several obstacles. In this paper, methods of 3D face recognition are examined by measuring quantifiable applications and results. In facial recognition, three Dimensional Morphable Model (3DMM) techniques have attracted more and more attention as effectiveness in use increases over time. 3DMM provides automation and more accurate image rendering when compared to other traditional techniques. The accuracy in image rendering comes at a cost; as 3DMM requires more focus on texture estimation, shape-controlling limits, and extrinsic variations, accurately matching fitting models, feature tracking and precision identification. We have underlined different issues in comparison based on these methods. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018.},
author_keywords={3D model;  Morphable model;  Recognition;  Reconstruction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{CardiaNeto2018135,
author={Cardia Neto, J.B. and Marana, A.N.},
title={Utilizing deep learning and 3DLBP for 3D Face recognition},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10657 LNCS},
pages={135-142},
doi={10.1007/978-3-319-75193-1_17},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042219158&doi=10.1007%2f978-3-319-75193-1_17&partnerID=40&md5=62334ac9f49d30280414a4356fe20dc0},
affiliation={São Carlos Federal University - UFSCAR, São Carlos, SP  13565-905, Brazil; UNESP - São Paulo State University, Bauru, SP  17033-360, Brazil},
abstract={Methods based on biometrics can help prevent frauds and do personal identification in day-to-day activities. Automated Face Recognition is one of the most popular research subjects since it has several important properties, such as universality, acceptability, low costs, and covert identification. In constrained environments methods based on 2D features can outperform the human capacity for face recognition but, once occlusion and other types of challenges are presented, the aforementioned methods do not perform so well. To deal with such problems 3D data and deep learning based methods can be a solution. In this paper we propose the utilization of Convolutional Neural Networks (CNN) with low-level 3D local features (3DLBP) for face recognition. The 3D local features are extracted from depth maps captured by a Kinect sensor. Experimental results on Eurecom database show that this proposal is promising, since, in average, almost 90% of the faces were correctly recognized. © Springer International Publishing AG, part of Springer Nature 2018.},
author_keywords={3D face recognition;  3D local features;  Biometrics;  Convolutional neural networks;  Deep learning;  Depth maps;  Kinect},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Banita20182325,
author={Banita and Tanwar, P.},
title={Evaluation of 3d facial paralysis using fuzzy logic},
journal={International Journal of Engineering and Technology(UAE)},
year={2018},
volume={7},
number={4},
pages={2325-2331},
doi={10.14419/ijet.v7i4.13619},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053901681&doi=10.14419%2fijet.v7i4.13619&partnerID=40&md5=f68480ef298793b35ad707b17c64ab59},
affiliation={Lingaya's University, Faridabad, India; Manav Rachana University, Faridabad, India},
abstract={Face recognition are of great interest to researchers in terms of Image processing and Computer Graphics. In recent years, various factors become popular which clearly affect the face model. Which are ageing, universal facial expressions, and muscle movement. Similarly in terms of medical terminology the facial paralysis can be peripheral or central depending on the level of motor neuron lesion which can be below the nucleus of the nerve or supra nuclear. The various medical therapy used for facial paralysis are electroaccupunture, electro-therapy, laser acupuncture, manual acupuncture which is a traditional form of acupuncture. Imaging plays a great role in evaluation of degree of paralysis and also for faces recognition. There is a wide research in terms of facial expressions and facial recognition but lim-ited research work is available in facial paralysis. House- Brackmann Grading system is one of the simplest and easiest method to evalu-ate the degree of facial paralysis. During evaluation common facial expressions are recorded and are further evaluated by considering the focal points of the left or the right side of the face. This paper presents the classification of face recognition and its respective fuzzy rules to remove uncertainty in the result after evaluation of facial paralysis. © 2018 Banita, Dr. Poonam Tanwar.},
author_keywords={3D face recognition;  CNN;  Evaluation of facial paralysis;  MAMDANI model;  Stages of face recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kalake2018226,
author={Kalake, L. and Yoshida, C.},
title={Designing an Electronic Health Security System Framework for Authentication with Wi-Fi, Smartphone and 3D Face Recognition Technology},
journal={Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
year={2018},
volume={206},
pages={226-232},
doi={10.1007/978-3-319-67837-5_22},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032688156&doi=10.1007%2f978-3-319-67837-5_22&partnerID=40&md5=9b0b8b2e9d152fca97568c24ab9ca0b6},
affiliation={Graduate School of Technology, Kobe Institute of Computing, Kano-cho 2-7-7, Chuo-ku, Kobe, 6500001, Japan},
abstract={Information technology for development is the tool that has been around for ages and it is now mainly focusing on making people lives easy including of those in a health sector. However, health practitioners and patients are somehow had not fully experienced this benefits due to sensitive information distribution and security concerns around the distribution of electronic health records. There have been various issues and challenges on security breaches, leakage of confidential patient records and computer attacks which have been raised on security and privacy concerns in electronic health records. The unauthorized access, denial of services, lack of standardization of the system increases mistrust on electronic health record system and makes it very difficult for the parties involved in handling and transmission of patients’ record. Therefore the aim of this paper is to propose an efficient and cost-effective face recognition security framework through Wi-Fi to enable the monitoring and access control on patient record in developing countries. © 2018, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.},
author_keywords={3D face recognition;  Authentication and security;  Biometric;  International mobile station equipment identity;  Mac address;  Mobile device encryption;  Patient electronic health record;  Serial number;  Wi-Fi},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hu2017133,
author={Hu, H. and Shah, S.A.A. and Bennamoun, M. and Molton, M.},
title={2D and 3D face recognition using convolutional neural network},
journal={IEEE Region 10 Annual International Conference, Proceedings/TENCON},
year={2017},
volume={2017-December},
pages={133-138},
doi={10.1109/TENCON.2017.8227850},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044210743&doi=10.1109%2fTENCON.2017.8227850&partnerID=40&md5=12f531513eddb3716404e891cd002a29},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA  6009, Australia},
abstract={Face recognition remains a challenge today as recognition performance is strongly affected by variability such as illumination, expressions and poses. In this work we apply Convolutional Neural Networks (CNNs) on the challenging task of both 2D and 3D face recognition. We constructed two CNN models, namely CNN-1 (two convolutional layers) and CNN-2 (one convolutional layer) for testing on 2D and 3D dataset. A comprehensive parametric study of two CNN models on face recognition is represented in which different combinations of activation function, learning rate and filter size are investigated. We find that CNN-2 has a better accuracy performance on both 2D and 3D face recognition. Our experimental results show that an accuracy of 85.15% was accomplished using CNN-2 on depth images with FRGCv2.0 dataset (4950 images with 557 objectives). An accuracy of 95% was achieved using CNN-2 on 2D raw image with the AT&T dataset (400 images with 40 objectives). The results indicate that the proposed CNN model is capable to handle complex information from facial images in different dimensions. These results provide valuable insights into further application of CNN on 3D face recognition. © 2017 IEEE.},
author_keywords={Convolutional Neural Networks;  Depth Image;  Face Recognition},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Soltanpour2017391,
author={Soltanpour, S. and Boufama, B. and Jonathan Wu, Q.M.},
title={A survey of local feature methods for 3D face recognition},
journal={Pattern Recognition},
year={2017},
volume={72},
pages={391-406},
doi={10.1016/j.patcog.2017.08.003},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027498785&doi=10.1016%2fj.patcog.2017.08.003&partnerID=40&md5=170ec9f0f1db0dc82a019bf4eb78253f},
affiliation={Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, N9B 3P4, Canada; School of Computer Science, University of Windsor, Windsor, ON, N9B 3P4, Canada},
abstract={One of the main modules in a face recognition system is feature extraction, which has a significant effect on the whole system performance. In the past decades, various types of feature extractors and descriptors have been proposed for 3D face recognition. Although several literature reviews have been carried out on 3D face recognition algorithms, only a few studies have been performed on feature extraction methods. The latter have a vital role to overcome degradation conditions, such as face expression variations and occlusions. Depending on the types of features used in 3D face recognition, these methods can be divided into two categories: global and local feature-based methods. Local feature-based methods have been effectively applied in the literature, as they are more robust to occlusions and missing data. This survey presents a state-of-the-art for 3D face recognition using local features, with the main focus being the extraction of these features. © 2017},
author_keywords={3-D;  Face recognition;  Feature extraction;  Local features;  Survey},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Soltanpour2017560,
author={Soltanpour, S. and Wu, Q.M.J.},
title={Multiscale depth local derivative pattern for sparse representation based 3D face recognition},
journal={2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017},
year={2017},
volume={2017-January},
pages={560-565},
doi={10.1109/SMC.2017.8122665},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044201680&doi=10.1109%2fSMC.2017.8122665&partnerID=40&md5=5918c06bbe0b6cc90d03ba306b0d1258},
affiliation={Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada},
abstract={3D face recognition is a popular research area due to its vast application in biometrics and security. Local featurebased methods gain importance in the recent years due to their robustness under degradation conditions. In this paper, a novel high-order local pattern descriptor in combination with sparse representation based classifier (SRC) is proposed for expression robust 3D face recognition. 3D point clouds are converted to depth maps after preprocessing. Multi-directional derivatives are applied in spatial space to encode the depth maps based on the local derivative pattern (LDP) scheme. Directional pattern features are calculated according to local derivative variations. Since LDP computes spatial relationship of neighbors in a local region, it extracts distinct information from the depth map. Multiscale depth-LDP is presented as a novel descriptor for 3D face recognition. The descriptor is employed along with the SRC to increase the range data distinctiveness. A histogram on the derivative pattern creates a spatial feature descriptor that represents the distinctive micro-patterns from 3D data. We evaluate the proposed algorithm on two famous 3D face databases, FRGC v2.0 and Bosphorus. The experimental results demonstrate that the proposed approach achieves acceptable performance under facial expression. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2017,
author={Liu, Z. and Cohen, F.},
title={Synthesis and identification of three-dimensional faces from image(s) and three-dimensional generic models},
journal={Journal of Electronic Imaging},
year={2017},
volume={26},
number={6},
doi={10.1117/1.JEI.26.6.063005},
art_number={063005},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034836161&doi=10.1117%2f1.JEI.26.6.063005&partnerID=40&md5=cbf0b712fa254c1cccec3850f657b1dd},
affiliation={Kepler Group LLC, New York, NY, United States; Drexel University, Electrical and Computer Engineering, Philadelphia, PA, United States},
abstract={We describe an approach for synthesizing a three-dimensional (3-D) face structure from an image or images of a human face taken at a priori unknown poses using gender and ethnicity specific 3-D generic models. The synthesis process starts with a generic model, which is personalized as images of the person become available using preselected landmark points that are tessellated to form a high-resolution triangular mesh. From a single image, two of the three coordinates of the model are reconstructed in accordance with the given image of the person, while the third coordinate is sampled from the generic model, and the appearance is made in accordance with the image. With multiple images, all coordinates and appearance are reconstructed in accordance with the observed images. This method allows for accurate pose estimation as well as face identification in 3-D rendering of a difficult two-dimensional (2-D) face recognition problem into a much simpler 3-D surface matching problem. The estimation of the unknown pose is achieved using the Levenberg-Marquardt optimization process. Encouraging experimental results are obtained in a controlled environment with high-resolution images under a good illumination condition, as well as for images taken in an uncontrolled environment under arbitrary illumination with low-resolution cameras. © 2017 SPIE and IS&T.},
author_keywords={3-D face recognition;  3-D face synthesis;  iterative personalization;  pose estimation;  ray tracing;  subdivision},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Frikha2017,
author={Frikha, T. and Chaabane, F. and Said, B. and Drira, H. and Abid, M. and Ben Amar, C. and Lille, L.},
title={Embedded approach for a Riemannian-based framework of analyzing 3D faces},
journal={Proceedings - 3rd International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2017},
year={2017},
doi={10.1109/ATSIP.2017.8075548},
art_number={8075548},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035335463&doi=10.1109%2fATSIP.2017.8075548&partnerID=40&md5=f9c92873ac16b26563e98ecaba5f6bba},
affiliation={REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, ENIS, BP 1173, Gabes University, Higher Institute of Computer Science and Multimedia of Gabes, Sfax, 3038, Tunisia; REGIM-Lab, Sfax, 3038, Tunisia; Multimedia of Gabes, Sfax, 3038, Tunisia; CES-Laboratory Sfax Sud University, National Engineering School of Sfax, Sfax, Tunisia},
abstract={Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform. © 2017 IEEE.},
author_keywords={3D face recognition;  Curve analysis;  elastic analysis algorithm;  embedded architecture;  face detection;  Facial analysis;  Facial expressions;  Riemann geometry},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sui201719575,
author={Sui, D. and Hou, D. and Duan, X.},
title={An interpolation algorithm fitted for dynamic 3D face reconstruction},
journal={Multimedia Tools and Applications},
year={2017},
volume={76},
number={19},
pages={19575-19589},
doi={10.1007/s11042-015-3233-x},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954425638&doi=10.1007%2fs11042-015-3233-x&partnerID=40&md5=78e6cc2a30fcda0f0eb7e4d7fd616c46},
affiliation={School of Information Science and Technology, Wuhan University of Technology, Hubei, 430070, China; School of Software Engineering, Anyang Normal University, Anyang, Henan  455000, China; Department of Computer, College of Science California State Polytecnic University-Pomona, California, United States},
abstract={In order to solve the problem of low recognition accuracy in later period which is caused by the too few extracted parameters in the 3D face recognition, and the incapable formation of completed point cloud structure. An automatic iterative interpolation algorithm is proposed. The new and more accurate 3D face data points are obtained by automatic iteration. This algorithm can be used to restore the data point cloud information of 3D facial feature in 2D images by means of facial three-legged structure formed by 3D face and automatic interpolation. Thus, it can realize to shape the 3D facial dynamic model which can be recognized and has high saturability. Experimental results show that the interpolation algorithm can achieve the complete the construction of facial feature based on the facial feature after 3D dynamic reconstruction, and the validity is higher. © 2016, Springer Science+Business Media New York.},
author_keywords={3D face dynamic recognition;  Iterative interpolation algorithm;  Point cloud structure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Deng20171305,
author={Deng, X. and Da, F. and Shao, H.},
title={Adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition},
journal={Signal, Image and Video Processing},
year={2017},
volume={11},
number={7},
pages={1305-1312},
doi={10.1007/s11760-017-1087-6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017095710&doi=10.1007%2fs11760-017-1087-6&partnerID=40&md5=fc06247cc3ca7870221563a085266269},
affiliation={School of Automation, Southeast University, Nanjing, Jiangsu  210096, China; Key Laboratory of Measurement and Control for Complex System, Ministry of Education, Southeast University, Nanjing, Jiangsu  210096, China},
abstract={A novel adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition is proposed in this paper. Firstly, the novel facial coarse-to-fine landmarks localization method based on Active Shape Model and Gabor wavelets transformation is proposed to exactly and automatically locate facial landmarks in range image. Secondly, the multi-scale fusion of the pyramid local binary patterns (F-PLBP) based on the irregular segmentation associated with the located landmarks is proposed to extract the discriminative feature. Thirdly, a sparse representation-based classifier based on the adaptive feature selection (A-SRC) using the distribution of the reconstruction residual is presented to select the expression-robust feature and identify the faces. Finally, the experimental evaluation based on FRGC v2.0 indicates that the adaptive feature selection method using F-PLBP combined with the A-SRC can obtain the high recognition accuracy by performing the higher discriminative power to overcome the influence from the facial expression variations. © 2017, Springer-Verlag London.},
author_keywords={3D face recognition;  Adaptive feature selection;  Facial landmark localization;  Multi-scale fusion},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liang201784,
author={Liang, Y. and Zhang, Y. and Zeng, X.-X.},
title={Pose-invariant 3D face recognition using half face},
journal={Signal Processing: Image Communication},
year={2017},
volume={57},
pages={84-90},
doi={10.1016/j.image.2017.05.004},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020037210&doi=10.1016%2fj.image.2017.05.004&partnerID=40&md5=88180b3e1c893d79c59e39647f3ee0a2},
affiliation={Guangdong University of Technology, School of Automation, No.100, Waihuan Xi Road, Guangzhou Higher Education Mega Centre, Guangzhou, 510006, China; South China Normal University, School of Software, Nanhai Information Technology Park, Foshan, 528225, China},
abstract={Pose variations are still challenging problems in 3D face recognition because large pose variations will cause self-occlusion and result in missing data. In this paper, a new method for pose-invariant 3D face recognition is proposed to handle significant pose variations. For pose estimation and registration, a coarse-to-fine strategy is proposed to detect landmarks under large yaw variations. At the coarse search step, candidate landmarks are detected using HK curvature analysis and subdivided according to a facial geometrical structure-based classification strategy. At the fine search step, candidate landmarks are identified and labeled by comparing with a Facial Landmark Model. By using the half face matching, we perform the matching step with respect to frontal scans and side scans. Experiments carried out on the Bosphorus and UND/FRGC v2.0 databases show that our method has high accuracy and robustness to pose variations. © 2017 Elsevier B.V.},
author_keywords={3D face recognition;  Facial landmark localization;  Half face;  Iso-geodesic stripes;  Pose variation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang20171141,
author={Wang, X.-Q. and Yuan, J.-Z. and Li, Q.},
title={3D face recognition using spherical vector norms map},
journal={Journal of Information Science and Engineering},
year={2017},
volume={33},
number={5},
pages={1141-1161},
doi={10.6688/JISE.2017.33.5.3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049875700&doi=10.6688%2fJISE.2017.33.5.3&partnerID=40&md5=cd82489caa82b30e6bb8b05ea1dbaae9},
affiliation={Beijing Key Laboratory of Information Service Engineering, China; Computer Technology Institute, Beijing Union University, Beijing, 100101, China; Beijing Advanced Innovation Center for Imaging Technology Capital Normal University, Beijing, 100048, China},
abstract={In this paper, we introduce a novel, automatic method for 3D face recognition. A new feature called a spherical vector norms map of a 3D face is created using the normal vector of each point. This feature contains more detailed information than the original depth image in regions such as the eyes and nose. For certain flat areas of 3D face, such as the forehead and cheeks, this map could increase the distinguishability of different points. In addition, this feature is robust to facial expression due to an adjustment that is made in the mouth region. Then, the facial representations, which are based on Histograms of Oriented Gradients, are extracted from the spherical vector norms map and the original depth image. A new partitioning strategy is proposed to produce the histogram of eight patches of a given image, in which all of the pixels are binned based on the magnitude and direction of their gradients. In this study, SVNs map and depth image are represented compactly with two histograms of oriented gradients; this approach is completed by Linear Discriminant Analysis and a Nearest Neighbor classifier. © 2017 Institute of Information Science. All Rights Reserved.},
author_keywords={3D face recognition;  Face recognition grand challenge database;  Histograms of oriented gradients;  Linear discriminant analysis;  Spherical vector norms map},
document_type={Article},
source={Scopus},
}

@ARTICLE{Deng201781,
author={Deng, X. and Da, F. and Shao, H.},
title={Efficient 3D face recognition using local covariance descriptor and Riemannian kernel sparse coding},
journal={Computers and Electrical Engineering},
year={2017},
volume={62},
pages={81-91},
doi={10.1016/j.compeleceng.2017.01.028},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011588375&doi=10.1016%2fj.compeleceng.2017.01.028&partnerID=40&md5=0a8b76ccd56a100449319171e3925d83},
affiliation={School of Automation, Southeast University, Nanjing 210096, China; Key Laboratory of Measurement and Control for Complex System, Ministry of Education, Southeast University, Nanjing 210096, China},
abstract={This paper proposes a novel 3D face recognition method using the local covariance descriptor and Riemannian kernel sparse coding in order to accurately evaluate the intrinsic correlation of the extracted features and further improve the 3D face recognition accuracy. Firstly, the keypoints are detected by the farthest point sampling method, and the corresponding keypoint neighborhood is extracted by the specified radius associated with geodesic distance. Then, different types of the efficient features are selected to construct the local covariance descriptor with inherent property. Finally, the appropriate Riemannian kernel sparse coding is used to identify the faces in probe. Experimental evaluation has been performed on two challenging 3D face datasets, FRGC v2.0 and Bosphorus, which indicates that the proposed approach can significantly improve the identification accuracy comparing with other state-of-the-art methods. © 2017 Elsevier Ltd},
author_keywords={3D face recognition;  Local covariance descriptor;  Riemannian kernel sparse coding;  Symmetric positive define matrix},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kim2017477,
author={Kim, J. and Han, D. and Hwang, W. and Kim, J.},
title={3D face recognition via discriminative keypoint selection},
journal={2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2017},
year={2017},
pages={477-480},
doi={10.1109/URAI.2017.7992781},
art_number={7992781},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034232492&doi=10.1109%2fURAI.2017.7992781&partnerID=40&md5=5fe4994d9645b41dca322b1421c30da3},
affiliation={School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Dept. of Software and Computer Engineering, Ajou University, Suwon, South Korea},
abstract={In this paper, we propose a discriminative keypoint selection-based 3D face recognition method that is superior to prevalent techniques in terms of both computational complexity and performance. We use the average face model (AFM) for face registration to efficiently locate the axis of symmetry in the rotated face mesh and recover a full frontal face from a 3D face model commonly corrupted due to pose variances. Instead of using the keypoint detection method, we use the feature selection algorithm to find the most discriminant keypoints for face identification and reduce computational time for not only feature extraction but also keypoint matching. The results of the experiments conducted on the Bosphorus database and the UMB-DB show that our algorithm can improve rank-1 identification accuracy, thus confirming its robustness against pose variances, expressions, and occlusions. © 2017 IEEE.},
author_keywords={Face recognition;  feature selection;  sparse representation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yi2017,
author={Yi, Z. and Yuan, Z.},
title={Research on 3D face recognition based on pose and illumination invariant},
journal={ACM International Conference Proceeding Series},
year={2017},
volume={Part F130523},
doi={10.1145/3110224.3110230},
art_number={a6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030531368&doi=10.1145%2f3110224.3110230&partnerID=40&md5=aa474ad08228f7cca6c22bbea758dde7},
affiliation={College of Software Engineering, Lanzhou Institute of Technology, Lanzhou, 730050, China; School of Chemical and Biological Engineering, Lanzhou Jiaotong University, Lanzhou, 730070, China},
abstract={In order to solve the problem of error rate caused by pose and illumination variation, the sparse statistical deformable model is used to reconstruct the 3D face shape. By using the method of spherical harmonic quotient image, estimation of the original input image light properties, the generated model standardized posture and illumination reconstruction, the original input image due to pose and illumination recognition error caused by the different solution. Experimental results show that the proposed method can effectively improve the recognition rate of 3D face. © 2017 Association for Computing Machinery.},
author_keywords={3D face recognition;  Illumination;  Pose;  Ratio image;  Spherical harmonic},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Torkhani2017969,
author={Torkhani, G. and Ladgham, A. and Sakly, A. and Mansouri, M.N.},
title={A 3D–2D face recognition method based on extended Gabor wavelet combining curvature and edge detection},
journal={Signal, Image and Video Processing},
year={2017},
volume={11},
number={5},
pages={969-976},
doi={10.1007/s11760-016-1046-7},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008237723&doi=10.1007%2fs11760-016-1046-7&partnerID=40&md5=bc47ad9c58f9b47b4c97513603061854},
affiliation={Laboratory EμE, Faculty of Sciences of Monastir, Monastir, Tunisia; Department of Electrical Engineering, National Engineering School of Monastir (ENIM), Monastir, Tunisia; Reseach Unit, Industrial Systems Study and Renewable Energy (ESIER), ENIM, Monastir, Tunisia},
abstract={The main limitation in 3D face recognition (FR) systems is their susceptibility to scanning difficulties and uncontrolled environments such as pose, illumination and expression variety. This paper proposes a new FR framework based on 3D to 2D mesh deforming and combined Gabor curvature and edge maps. The advantage of this method comes from the powerful saliency distribution achieved from applying extended Gabor wavelets to 2D projected face meshes. The extracted feature vectors are classified using the outstanding robustness of the support vector machine. Experiments carried out on common databases proved that valid accuracy rates can be accomplished by the proposed approach comparing to other existing methods. © 2017, Springer-Verlag London.},
author_keywords={Feature extraction;  Gabor curvature and edge maps;  Gabor wavelets;  Recognition rate},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tang2017589,
author={Tang, Y. and Chen, L.},
title={3D Facial Geometric Attributes Based Anti-Spoofing Approach against Mask Attacks},
journal={Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017},
year={2017},
pages={589-595},
doi={10.1109/FG.2017.74},
art_number={7961795},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026323809&doi=10.1109%2fFG.2017.74&partnerID=40&md5=ed89e52bd051c85dbee8198cd1d7344e},
affiliation={Universite de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, Lyon, 69134, France},
abstract={3D scanning and 3D printing techniques, as the technical impetus of 3D face recognition, also boost unconsciously the security threat against it from the spoofing attacks via manufactured mask. In order to improve the robustness of 3D face recognition system, several countermeasures against mask attacks based on photometric features have been reported in recent years. However, the anti-spoofing approach involving 3D meshed face scan and the related 3D facial features have not been studied yet. For filling this gap, in this paper, we propose to exploit the anti-spoofing performance of geometric attributes based 3D facial description. It synthesises the advantages of the selected geometric attributes, named principal curvature measures, and the meshSIFT-based feature descriptor. Specifically, the estimation of geometric attributes is coherent to the property of discrete surface, and the feature related to them can accurately describe the shape of facial surface. These characteristics are beneficial to discovering the geometry-based dissimilarity between genuine face and fraud mask. In the experiment part, the baselines of verification and anti-spoofing performance are evaluated on the Morpho database. Furthermore, for simulating a real-world scenario and testing the corresponding anti-spoofing performance, the size of genuine face set is massively extended by uniting the Morpho database and the FRGC v2.0 database to increase the ratio of genuine faces to fraud masks. The evaluation results prove that the proposed 3D face verification system can guarantee competitive verification accuracy for genuine faces and promising anti-spoofing performance against mask attacks. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yahia2017328,
author={Yahia, S. and Ben Salem, Y. and Abdelkrim, M.N.},
title={3D face recognition using local binary pattern and grey level co-occurrence matrix},
journal={2016 17th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering, STA 2016 - Proceedings},
year={2017},
pages={328-338},
doi={10.1109/STA.2016.7952047},
art_number={7952047},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024382159&doi=10.1109%2fSTA.2016.7952047&partnerID=40&md5=6f4d58a55714353c995acbaca835313b},
affiliation={Electric Department, ENIG, MACS Laboratory, Gabès, Tunisia},
abstract={In this paper, we aim to study the problem of 3D face recognition. This problem becomes complex when we consider the human face expressions and different face occlusions. In this context is situated our work, we use the 3D UMB database to evaluate two powerful methods: The 3D Local Binary Pattern (3D-LBP) and the 3D Grey Level Co-occurrence Matrix (3D-GLCM). We tested also the combination of these two approaches. A training phase is necessary, to pass to the test phase which is followed by classification. We use a multiclass Support Vector Machines (SVM) for classification. The experimental results prove that in this classification problem 3D-LBP is the more suitable. © 2016 IEEE.},
author_keywords={3D face recognition;  3D-GLCM;  3D-LBP;  classification;  SVM},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Saleh2017116,
author={Saleh, Y. and Edirisinghe, E.},
title={3D face reconstruction and recognition using the overfeat network},
journal={2017 8th International Conference on Information and Communication Systems, ICICS 2017},
year={2017},
pages={116-119},
doi={10.1109/IACS.2017.7921956},
art_number={7921956},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020175736&doi=10.1109%2fIACS.2017.7921956&partnerID=40&md5=909441cac89ba044d64659e5f0349f22},
affiliation={Department Computer Science, Loughborough University, Loughborough, LE11 3TU, United Kingdom},
abstract={Although face recognition is considered a popular area of research and study, it still has few unresolved challenges, and with the appearance of devices such as the Microsoft Kinect, new possibilities for researchers were uncovered. With the goal of enhancing face recognition techniques, this paper presents a novel way to reconstruct face images in different angles, through the use of the data of one front image captured by the Kinect, using faster techniques than ever before, also, this paper utilizes a deep learning network called Overfeat, where it functioned as a feature extractor that was used on normal images and on the new 3D created images, which introduced a new application for the network. To check the capabilities of the new created images, they were used as a testing set in three main experiments. Finally, results of the experiments are presented to prove the ability of the created images to function as new data sets for face recognition; also, proving the capability of the Overfeat network, working with computer generated face images. © 2017 IEEE.},
author_keywords={3D Face recognition;  Deep Learning;  Kinect;  Overfeat},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Emambakhsh2017995,
author={Emambakhsh, M. and Evans, A.},
title={Nasal Patches and Curves for Expression-Robust 3D Face Recognition},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2017},
volume={39},
number={5},
pages={995-1007},
doi={10.1109/TPAMI.2016.2565473},
art_number={7467565},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018498024&doi=10.1109%2fTPAMI.2016.2565473&partnerID=40&md5=e1afb77517c631cf7da8f8d394318b02},
affiliation={Institute of Sensors, Signals and Systems, Heriot-Watt University, Edinburgh, United Kingdom; Department of Electronic and Electrical Engineering, University of Bath, Bath, United Kingdom},
abstract={The potential of the nasal region for expression robust 3D face recognition is thoroughly investigated by a novel five-step algorithm. First, the nose tip location is coarsely detected and the face is segmented, aligned and the nasal region cropped. Then, a very accurate and consistent nasal landmarking algorithm detects seven keypoints on the nasal region. In the third step, a feature extraction algorithm based on the surface normals of Gabor-wavelet filtered depth maps is utilised and, then, a set of spherical patches and curves are localised over the nasal region to provide the feature descriptors. The last step applies a genetic algorithm-based feature selector to detect the most stable patches and curves over different facial expressions. The algorithm provides the highest reported nasal region-based recognition ranks on the FRGC, Bosphorus and BU-3DFE datasets. The results are comparable with, and in many cases better than, many state-of-the-art 3D face recognition algorithms, which use the whole facial domain. The proposed method does not rely on sophisticated alignment or denoising steps, is very robust when only one sample per subject is used in the gallery, and does not require a training step for the landmarking algorithm. © 2016 IEEE.},
author_keywords={Face recognition;  facial landmarking;  feature selection;  Gabor wavelets;  nose region;  surface normals},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yu2017296,
author={Yu, X. and Gao, Y. and Zhou, J.},
title={Sparse 3D directional vertices vs continuous 3D curves: Efficient 3D surface matching and its application for single model face recognition},
journal={Pattern Recognition},
year={2017},
volume={65},
pages={296-306},
doi={10.1016/j.patcog.2016.12.009},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010678622&doi=10.1016%2fj.patcog.2016.12.009&partnerID=40&md5=a32f68ce5d21826e871243d030db3868},
affiliation={School of Engineering, Griffith UniversityQLD  4111, Australia; School of Information and Communication Technology, Griffith UniversityQLD  4111, Australia},
abstract={Traditionally, point clouds and meshes are used to represent and match 3D shapes, which often cannot meet the computational speed and storage space requirements in many 3D data matching and retrieval applications. In this paper, we present a novel 3D directional vertices (3D2V) approach to efficiently represent and match 3D surfaces by much fewer sparsely distributed structured vertices that carry structural information transferred from their deleted neighbouring points. A 3D2V conversion and similarity measurement method is developed to compute the distance between two different 3D2Vs. The performance of the proposed method is evaluated on 3D face recognition using Face Recognition Grand Challenge v2.0 (FRGC v2.0) and GavabDB databases and compared with the curve-based benchmark method. The experimental results demonstrate that the proposed 3D2V method can significantly reduce the data storage requirement and computation time with a moderate increase of accuracy at the same time. It provides a new tool for developing fast 3D surface matching algorithms for large scale 3D data classification and retrieval. © 2016},
author_keywords={3D curve;  3D directional vertex;  3D face recognition;  3D surface matching;  Fast 3D matching;  Hausdorff distance;  Iterative closest points;  Storage space},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wu2017398,
author={Wu, Z. and Hou, Z. and Zhang, J.},
title={Research on the 3D face recognition based on multi-class classifier with depth and point cloud data},
journal={Proceedings of 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference, IMCEC 2016},
year={2017},
pages={398-402},
doi={10.1109/IMCEC.2016.7867242},
art_number={7867242},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016763964&doi=10.1109%2fIMCEC.2016.7867242&partnerID=40&md5=9f0eedd2b08b943e738a503f0aa393bc},
affiliation={Changzhou University, Jiangsu Province, China},
abstract={Human face recognition technology usually takes advantages of two-dimensional or three-dimensional data. Rising from 1980s, three-dimensional face recognition technology soon become one of the headed topic because of its admirable resistance to interference and more information compared with two-dimensional face recognition technology. The new 3D face model standardization algorithm presented in this article provides a solution to transfer the obtained face model to standardized CANDIDE-3 face model. The article also provides a new Bayesian classification model based on multi-class classifier, which could overcome the difficulty that ono-verse-one classifier has a low recognition rate when facing more than two people. The article conduct the comparison experiment based on the provided algorithm. According to the experiment, it could raise the face recognition rate efficiently when applying the standardization algorithm and training model. © 2016 IEEE.},
author_keywords={3D face recognition;  CANDIDE-3;  Depth data;  Face model standardization;  Multi-class classifier;  Point cloud data},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bobulski2017121,
author={Bobulski, J.},
title={Multimodal face recognition method with two-dimensional hidden Markov model},
journal={Bulletin of the Polish Academy of Sciences: Technical Sciences},
year={2017},
volume={65},
number={1},
pages={121-128},
doi={10.1515/bpasts-2017-0015},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013807265&doi=10.1515%2fbpasts-2017-0015&partnerID=40&md5=0560b8de533c2b1b9017479cf483d622},
affiliation={Institute of Computer and Information Science, Czestochowa University of Technology, 73 Dabrowskiego St., Czestochowa, 42-201, Poland},
abstract={The paper presents a new solution for the face recognition based on two-dimensional hidden Markov models. The traditional HMM uses one-dimensional data vectors, which is a drawback in the case of 2D and 3D image processing, because part of the information is lost during the conversion to one-dimensional features vector. The paper presents a concept of the full ergodic 2DHMM, which can be used in 2D and 3D face recognition. The experimental results demonstrate that the system based on two dimensional hidden Markov models is able to achieve a good recognition rate for 2D, 3D and multimodal (2D+3D) face images recognition, and is faster than ICP method.},
author_keywords={3D face recognition;  Biometrics;  Hidden Markov model;  Pattern recognition},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Naveen2017112,
author={Naveen, S. and Rugmini, K.P. and Moni, R.S.},
title={3D face reconstruction by pose correction, patch cloning and texture wrapping},
journal={2016 International Conference on Communication Systems and Networks, ComNet 2016},
year={2017},
pages={112-116},
doi={10.1109/CSN.2016.7823997},
art_number={7823997},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014911379&doi=10.1109%2fCSN.2016.7823997&partnerID=40&md5=d83e42a8548567d921eea89900252c10},
affiliation={Dept. of ECE, LBSITW, Thiruvananthapuram Kerala, India; Dept. of ECE Marian Engineering College, Thiruvananthapuram Kerala, India},
abstract={Face is being considered as one of the most commonly used biometric modality. The inaccuracy in two dimensional face recognition systems is mainly due to pose variations, occlusions, illumination etc. Among this, changes in illumination condition do not affect 3D face recognition systems. But pose variation drastically changes the appearance of face images. To solve the problems with depth map and texture images corrupted by head pose variations and the occlusions generated due to these pose variations, a reconstruction method is proposed which consist of three stages. In the first stage, the pose correction is done by Iterative Closest Point (ICP) algorithm and in the second stage the occluded region of the face is reconstructed by a resurfacing method called patch cloning. It is followed by the wrapping of reconstructed depth map by its texture to generate a 3D model. The statistical error between the original face and the reconstructed face is also evaluated. In this work, facial symmetry is used as a prior knowledge. Experiments are done with the FRAV3D database. © 2016 IEEE.},
author_keywords={Face recognition;  Face Resurfacing;  ICP algorithm;  Patch Cloning;  Pose Correction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ramalingam2017,
author={Ramalingam, S. and Maheswari, U.},
title={A fuzzy interval valued fusion technique for multi-modal 3D face recognition},
journal={Proceedings - International Carnahan Conference on Security Technology},
year={2017},
doi={10.1109/CCST.2016.7815709},
art_number={7815709},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011685180&doi=10.1109%2fCCST.2016.7815709&partnerID=40&md5=028f8cd51bd05382872e10cdaf8fcc8a},
affiliation={School of Engineering and Technology, University of Hertfordshire, Hatfield, AL10 9AB, United Kingdom; Department of Mathematics, Kamaraj College of Engineering and Technology, Virudhunagar, India},
abstract={This paper proposes a fuzzy interval valued multicriteria decision making (MCDM) technique that aggregates information from multi-modal feature sets during decision making in a 3D face recognition system. In this paper, an interval valued fuzzy TOPSIS technique is applied to a 3D face recognition system that is benchmarked against a set of databases. Such a system is shown to be useful in decision making when the choice of alternatives of the feature sets is combinatorial and complex. © 2016 IEEE.},
author_keywords={3D Face Recognition;  Disparity Maps;  fuzzy interval MCDM;  Range Data;  TOPSIS},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bobulski201753,
author={Bobulski, J.},
title={Face recognition with 3D face asymmetry},
journal={Advances in Intelligent Systems and Computing},
year={2017},
volume={525},
pages={53-60},
doi={10.1007/978-3-319-47274-4_6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994876936&doi=10.1007%2f978-3-319-47274-4_6&partnerID=40&md5=c7adf1f39551d024cb6732e2715c3479},
affiliation={Institute of Computer and Information Sciences, Czestochowa University of Technology, Dabrowskiego 73, Czestochowa, 42-200, Poland},
abstract={Using of 3D images for the identification was in a field of the interest of many researchers which developed a few methods offering good results. However, there are few techniques exploiting the 3D asymmetry amongst these methods. We propose fast algorithm for rough extraction face asymmetry that is used to 3D face recognition with hidden Markov models. This paper presents conception of fast method for determine 3D face asymmetry. The research results indicate that face recognition with 3D face asymmetry may be used in biometrics systems. © Springer International Publishing AG 2017.},
author_keywords={Face asymmetry;  Face recognition;  Hidden Markov models;  Identity verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hariri2017187,
author={Hariri, W. and Tabia, H. and Farah, N. and Declercq, D. and Benouareth, A.},
title={Geometrical and visual feature quantization for 3D face recognition},
journal={VISIGRAPP 2017 - Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
year={2017},
volume={5},
pages={187-193},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013991858&partnerID=40&md5=fd026eef10dcc4922fe7269db34feb6c},
affiliation={ETIS, ENSEA, University of Cergy-Pontoise, CNRS, UMR 8051, Cergy-Pontoise, France; Labged Laboratory, Computer Science Department, Badji Mokhtar Annaba University, Annaba, Algeria},
abstract={In this paper, we present an efficient method for 3D face recognition based on vector quantization of both geometrical and visual proprieties of the face. The method starts by describing each 3D face using a set of orderless features, and use then the Bag-of-Features paradigm to construct the face signature. We analyze the performance of three well-known classifiers: the Naïve Bayes, the Multilayer perceptron and the Random forests. The results reported on the FRGCv2 dataset show the effectiveness of our approach and prove that the method is robust to facial expression. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Bag-of-Features;  Codebook;  Depth image;  HoS;  LBP;  Term vector},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2017633,
author={Zhang, T. and Mu, Z. and Li, Y. and Liu, Q. and Zhang, Y.},
title={3D face and ear recognition based on partial mars map},
journal={ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
year={2017},
volume={2017-January},
pages={633-637},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049438191&partnerID=40&md5=142925806e53f82e08f8cdf1fe8539c6},
affiliation={School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China},
abstract={This paper proposes a 3D face recognition approach based on facial pose estimation, which is robust to large pose variations in the unconstrained scene. Deep learning method is used to facial pose estimation, and the generation of partial MARS (Multimodal fAce and eaR Spherical) map reduces the probability of feature points appearing in the deformed region. Then we extract the features from the depth and texture maps. Finally, the matching scores from two types of maps should be calculated by Bayes decision to generate the final result. In the large pose variations, the recognition rate of the method in this paper is 94.6%. The experimental results show that our approach has superior performance than the existing methods used on the MARS map, and has potential to deal with 3D face recognition in unconstrained scene. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={3d face recognition;  Deep learning;  Head pose estimation;  Partial mars map},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Deng201713,
author={Deng, X. and Da, F. and Shao, H.},
title={Expression-robust 3D face recognition based on feature-level fusion and feature-region fusion},
journal={Multimedia Tools and Applications},
year={2017},
volume={76},
number={1},
pages={13-31},
doi={10.1007/s11042-015-3012-8},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945315051&doi=10.1007%2fs11042-015-3012-8&partnerID=40&md5=5f6b0d57d4d091b3769e974b26751540},
affiliation={Department of Automation, Southeast University, Nanjing, Jiangsu  210096, China; Key Laboratory of Measurement and Control for Complex System of Ministry of Education, Southeast University, Nanjing, Jiangsu  210096, China},
abstract={3D face shape is essentially a non-rigid free-form surface, which will produce non-rigid deformation under expression variations. In terms of that problem, a promising solution named Coherent Point Drift (CPD) non-rigid registration for the non-rigid region is applied to eliminate the influence from the facial expression while guarantees 3D surface topology. In order to take full advantage of the extracted discriminative feature of the whole face under facial expression variations, the novel expression-robust 3D face recognition method using feature-level fusion and feature-region fusion is proposed. Furthermore, the Principal Component Analysis and Linear Discriminant Analysis in combination with Rotated Sparse Regression (PL-RSR) dimensionality reduction method is presented to promote the computational efficiency and provide a solution to the curse of dimensionality problem, which benefit the performance optimization. The experimental evaluation indicates that the proposed strategy has achieved the rank-1 recognition rate of 97.91 % and 96.71 % based on Face Recognition Grand Challenge (FRGC) v2.0 and Bosphorus respectively, which means the proposed approach outperforms state-of-the-art approach. © 2015, Springer Science+Business Media New York.},
author_keywords={3D face recognition;  Dimensionality reduction;  Feature-level fusion;  Feature-region fusion;  Non-rigid point set registration},
document_type={Article},
source={Scopus},
}

@ARTICLE{Keshwani2017333,
author={Keshwani, L. and Pete, D.},
title={Comparative analysis of frontal face recognition using radial curves and back propagation neural network},
journal={Advances in Intelligent Systems and Computing},
year={2017},
volume={469},
pages={333-344},
doi={10.1007/978-981-10-1678-3_32},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984944723&doi=10.1007%2f978-981-10-1678-3_32&partnerID=40&md5=8786b9edc61be7625d9311c127d87030},
affiliation={Datta Meghe College of Engineering, Airoli, Navi Mumbai, India; Electronics and Telecommunication Department, Datta Meghe College of Engineering, Airoli, Navi Mumbai, India},
abstract={Person identification using face as a cue is one of the most prominent and robust technique. This paper presents 3D face recognition system using Radial curves and Back Propagation Neural Networks (BPNN). The face images used for experimentation are under various challenges like illumination, pose variation, expression and occlusions. The features of images are extracted using Eigen vectors. These features are compared using radial curves on the face starting from center of the face to the end of the face. Each corresponding curve is matched using Euclidean Distance classifier. The BPNN is used to train the features for face matching. The proposed algorithms are tested on ORL and DMCE database. The performance analysis is based on recognition rate accuracy of the system. The proposed radial curve system yields recognition rate accuracy of 100 % for images from the ORL database and 98 % for the images from DMCE database. © Springer Science+Business Media Singapore 2017.},
author_keywords={Back propagation neural networks;  Face recognition;  ORL database;  Radial curves},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tortorici201756,
author={Tortorici, C. and Werghi, N.},
title={Early features fusion over 3D face for face recognition},
journal={Communications in Computer and Information Science},
year={2017},
volume={684},
pages={56-64},
doi={10.1007/978-3-319-60654-5_5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025144868&doi=10.1007%2f978-3-319-60654-5_5&partnerID=40&md5=3b06dd7bdc4134f54389a82b8b2fcda0},
affiliation={Khalifa University, Abu Dhabi, United Arab Emirates},
abstract={In this paper, a novel approach for fusing shape and texture Local Binary Patterns (LBP) for 3D Face Recognition is presented. Using the recently proposed mesh-LBP [23], it is now possible to compute LBP directly on a mesh manifold, allowing Early Feature Fusion to enhance face description power. Compared to its depth image counterparts, the proposed method (a) inherits the intrinsic advantages of mesh surfaces, (such as preservation of full geometry), (b) does not require face registration, (c) can accommodate partial or rotation matching, and (d) natively allows early-level fusion of texture and shape descriptors. The advantages of early-fusion is presented together with an experimentation of two merging schemes tested on the Bosphorus database. © Springer International Publishing AG 2017.},
author_keywords={3D face recognition;  Early feature-fusion;  LBP;  Local Binary Pattern},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{SilvaMata20173,
author={Silva Mata, F.J. and Castellanos, E.G. and Muñoz-Briseño, A. and Talavera-Bustamante, I. and Berretti, S.},
title={3D Face Recognition in Continuous Spaces},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10485 LNCS},
pages={3-13},
doi={10.1007/978-3-319-68548-9_1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032464388&doi=10.1007%2f978-3-319-68548-9_1&partnerID=40&md5=4288e99914c770d9d22c7b028ccbc05a},
affiliation={CENATAV, Havana, Cuba; University of Florence, Florence, Italy},
abstract={This work introduces a new approach for face recognition based on 3D scans. The main idea of the proposed method is that of converting the 3D face scans into a functional representation, performing all the subsequent processing in the continuous space. To this end, a model alignment problem is first solved by combining graph matching and clustering. Fiducial points of the face are initially detected by analysis of continuous functions computed on the surface. Then, the alignment is performed by transforming the geometric graphs whose nodes are the critical points of the representative function of the surface in previously determined subspaces. A clustering step is finally applied to correct small displacement in the models. The 3D face representation is then obtained on the aligned models by functions carefully selected according to mathematical and computational criteria. In particular, the face is divided into regions, which are treated as independent domains where a set of functions is determined by fitting the surface data using the least squares method. Experimental results demonstrate the feasibility of the method. © 2017, Springer International Publishing AG.},
author_keywords={3D face recognition;  Functional representation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tang201741,
author={Tang, Y. and Chen, L.},
title={Shape analysis based anti-spoofing 3D face recognition with mask attacks},
journal={Communications in Computer and Information Science},
year={2017},
volume={684},
pages={41-55},
doi={10.1007/978-3-319-60654-5_4},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025122871&doi=10.1007%2f978-3-319-60654-5_4&partnerID=40&md5=ee21d18278533c54c55ae227daf6a772},
affiliation={Université de Lyon, Ecole Centrale de Lyon, LIRIS laboratory, UMR CNRS 5205, Lyon, 69134, France},
abstract={With the growth of face recognition, the spoofing mask attacks attract more attention in biometrics research area. In recent years, the countermeasures based on the texture and depth image against spoofing mask attacks have been reported, but the research based on 3D meshed sample has not been studied yet. In this paper, we propose to apply 3D shape analysis based on principal curvature measures to describe the meshed facial surface. Meanwhile, a verification protocol based on this feature descriptor is designed to verify person identity and to evaluate the anti-spoofing performance on Morpho database. Furthermore, for simulating a real-life testing scenario, FRGCv2 database is enrolled as an extension of face scans to augment the ratio of genuine face samples to fraud mask samples. The experimental results show that our system can guarantee a high verification rate for genuine faces and the satisfactory anti-spoofing performance against spoofing mask attacks in parallel. © Springer International Publishing AG 2017.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wei201766,
author={Wei, X. and Li, H. and Gu, X.D.},
title={Three Dimensional Face Recognition via Surface Harmonic Mapping and Deep Learning},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10568 LNCS},
pages={66-76},
doi={10.1007/978-3-319-69923-3_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032656852&doi=10.1007%2f978-3-319-69923-3_8&partnerID=40&md5=598d2aabb96d1b2d7f9963542a2fcddf},
affiliation={School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Department of Computer Science and Department of Mathematics, State University of New York at Stony Brook, New York, United States},
abstract={In this paper, we propose a general 3D face recognition framework by combining the idea of surface harmonic mapping and deep learning. In particular, given a 3D face scan, we first run the pre-processing pipeline and detect three main facial landmarks (i.e., nose tip and two inner eye corners). Then, harmonic mapping is employed to map the 3D coordinates and differential geometry quantities (e.g., normal vectors, curvatures) of each 3D face scan to a 2D unit disc domain, generating a group of 2D harmonic shape images (HSI). The 2D rotation of the harmonic shape images are removed by using the three detected landmarks. All these pose normalized harmonic shape images are fed into a pre-trained deep convolutional neural network (DCNN) to generate their deep representations. Finally, sparse representation classifier with score-level fusion is used for face similarity measurement and the final decision. The advantage of our method is twofold: (i) it is a general framework and can be easily extended to other surface mapping and deep learning algorithms. (ii) it is registration-free and only needs three landmarks. The effectiveness of the proposed framework was demonstrated on the BU-3DFE database, and reporting a rank-one recognition rate of 89.38% on the whole database. © 2017, Springer International Publishing AG.},
author_keywords={3D face recognition;  Deep learning;  Surface harmonic mapping},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lee2017105,
author={Lee, D. and Krim, H.},
title={3D face recognition in the Fourier domain using deformed circular curves},
journal={Multidimensional Systems and Signal Processing},
year={2017},
volume={28},
number={1},
pages={105-127},
doi={10.1007/s11045-015-0334-7},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931003254&doi=10.1007%2fs11045-015-0334-7&partnerID=40&md5=e10f2e06f41d7b10795343f6f7dad231},
affiliation={Division of Mobile Communications, Samsung Electronics Co. Ltd, Suwon, Gyeonggi, South Korea; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC  27695, United States},
abstract={One of the most significant problems in image and vision applications is the efficient representation of a target image containing a large amount of data with high complexity. The ability to analyze high dimensional signals in a lower dimension without losing their information, has been crucial in the field of image processing. This paper proposes an approach to 3D face recognition using dimensionality reduction based on deformed circular curves, on the shortest geodesic distances, and on the properties of the Fourier Transform. Measured geodesic distances information generates a matrix whose entities are geodesic distances between the reference point and an arbitrary point on a 3D object, and an one-dimensional vector is generated by reshaping the matrix without losing the original properties of the target object. Following the property of the Fourier Transform, symmetry of the magnitude response, the original signal can be analyzed in the lower dimensional space without loss of inherent characteristics. This paper mainly deal with the efficient representation and recognition algorithm using deformed circular curves and the simulation shows promising result for recognition of geometric face information. © 2015, Springer Science+Business Media New York.},
author_keywords={Classification;  Deformed circular curves;  Dimensionality reduction;  Face recognition;  Fourier Transform;  Geodesic distance},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pitteri2017925,
author={Pitteri, G. and Munaro, M. and Menegatti, E.},
title={Depth-based frontal view generation for pose invariant face recognition with consumer RGB-D sensors},
journal={Advances in Intelligent Systems and Computing},
year={2017},
volume={531},
pages={925-937},
doi={10.1007/978-3-319-48036-7_67},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013040754&doi=10.1007%2f978-3-319-48036-7_67&partnerID=40&md5=9cd77bba58f0cd9c12593505c307634b},
affiliation={Department of Information Engineering, University of Padova, Via Gradenigo 6/A, Padova, 35131, Italy},
abstract={In this work, we propose to exploit depth information to build a pose-invariant face recognition algorithm from RGB-D data. Our approach first estimates the head pose and then generates a frontal view for those faces that are rotated with respect to the frame of the camera. Then, some interest points of the face are detected by means of a Random Forest applied to the RGB image and they are used as keypoints where to compute feature descriptors. Around these points and their 3D counterpart, we extract both 2D and 3D local descriptors, which are then concatenated and classified by means of a Support Vector Machine trained in “one-versus-all” fashion. In order to validate the accuracy of the system with data from consumer RGB-D sensors, we created the IAS-Lab RGB-D Face Dataset, a new public dataset in which RGB-D data are acquired with a second generation Microsoft Kinect. The reported experiments show that the depth-aided approach we propose allows to improve the recognition rate up to 50 %. © Springer International Publishing AG 2017.},
author_keywords={3D face recognition;  Frontalization;  Pose-invariance;  RGB-D},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu2016,
author={Yu, X. and Gao, Y. and Zhou, J.},
title={Boosting Radial Strings for 3D Face Recognition with Expressions and Occlusions},
journal={2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
year={2016},
doi={10.1109/DICTA.2016.7797014},
art_number={7797014},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011088593&doi=10.1109%2fDICTA.2016.7797014&partnerID=40&md5=b22aac1e03b48565af8c230b46a10822},
affiliation={School of Engineering, Griffith University, Nathan, QLD, Australia; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia},
abstract={In this paper, we present a new radial string representation and matching approach for 3D face recognition under expression variations and partial occlusions. The radial strings are an indexed collection of strings emanating from the nose tip of a face scan. The matching between two radial strings is conducted through a dynamic programming process, in which a partial matching mechanism is established to effectively find those un-occluded substrings. Moreover, the most discriminative and stable radial strings are selected optimally by the well-known AdaBoost algorithm to achieve a composite classifier for 3D face recognition under facial expression changes. Experimental results on the GavabDB and the Bosphorus databases show that the proposed approach achieves promising results for human face recognition with expressions and occlusions. © 2016 IEEE.},
author_keywords={face recognition;  facial curves;  feature selection;  machine learning;  string matching},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gilani2016,
author={Gilani, S.Z. and Mian, A.},
title={Towards Large-Scale 3D Face Recognition},
journal={2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
year={2016},
doi={10.1109/DICTA.2016.7797090},
art_number={7797090},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011072173&doi=10.1109%2fDICTA.2016.7797090&partnerID=40&md5=8e7de072eb2e5794e1830d7699cecbb6},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, Australia},
abstract={3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Manjani2016,
author={Manjani, I. and Sumerkan, H. and Flynn, P.J. and Bowyer, K.W.},
title={Template aging in 3D and 2D face recognition},
journal={2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
year={2016},
doi={10.1109/BTAS.2016.7791202},
art_number={7791202},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011292291&doi=10.1109%2fBTAS.2016.7791202&partnerID=40&md5=489de6a99b54a846a691ee770e40a082},
affiliation={Department of Computer Science and Engineering, IIIT-Delhi, Delhi, 110020, India; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN  46556, United States},
abstract={This is the first work to explore template aging in 3D face recognition. We use a dataset of images representing 16 subjects with 3D and 2D face images, and compare short-term and long-term time-lapse matching accuracy. We find that an ensemble-of-regions approach to 3D face matching has much greater accuracy than whole-face 3D matching, or than a commercial 2D matcher. We observe a drop in accuracies with increased time lapse, most with whole-face 3D matching followed by 2D matching and the 3D ensemble of regions approach. Finally, we determine whether the difference in match quality arising with an increased time lapse is statistically significant. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Das2016243,
author={Das, N. and Mandal, D. and Biswas, S.},
title={Simultaneous Semi-Coupled Dictionary Learning for Matching RGBD Data},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2016},
pages={243-251},
doi={10.1109/CVPRW.2016.37},
art_number={7789527},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010203549&doi=10.1109%2fCVPRW.2016.37&partnerID=40&md5=aa825a3ef4e099d932800d5621327839},
affiliation={Indian Institute of Science, Bangalore, India},
abstract={Matching with hidden information which is available only during training and not during testing has recently become an important research problem. Matching data from two different modalities, known as cross-modal matching is another challenging problem due to the large variations in the data coming from different modalities. Often, these are treated as two independent problems. But for applications like matching RGBD data, when only one modality is available during testing, it can reduce to either of the two problems. In this work, we propose a framework which can handle both these scenarios seamlessly with applications to matching RGBD data of Lambertian objects. The proposed approach jointly uses the RGB and depth data to learn an illumination invariant canonical version of the objects. Dictionaries are learnt for the RGB, depth and the canonical data, such that the transformed sparse coefficients of the RGB and the depth data is equal to that of the canonical data. Given RGB or depth data, their sparse coefficients corresponding to their canonical version is computed which can be directly used for matching using a Mahalanobis metric. Extensive experiments on three datasets, EURECOM, VAP RGB-D-T and Texas 3D Face Recognition database show the effectiveness of the proposed framework. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shah2016,
author={Shah, S.A.A. and Bennamoun, M. and Boussaid, F.},
title={Automatic 3D face landmark localization based on 3D vector field analysis},
journal={International Conference Image and Vision Computing New Zealand},
year={2016},
volume={2016-November},
doi={10.1109/IVCNZ.2015.7761526},
art_number={7761526},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006942604&doi=10.1109%2fIVCNZ.2015.7761526&partnerID=40&md5=5da7d80624be2691571e8fafab5b9dbf},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Perth, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, 35 Stirling Highway, Perth, WA, Australia},
abstract={In applications such as 3D face synthesis and animation, a prominent face landmark is required to enable 3D face normalization, pose correction, 3D face recognition and reconstruction. Due to variations in facial expressions, automatic 3D face landmark localization remains a challenge. Nose tip is one of the salient landmarks in a human face. In this paper, a novel nose tip localization technique is proposed. In the proposed approach, the rotation of the 3D vector field is analyzed for robust and efficient nose tip localization. The proposed technique has the following three characteristics: (1) it does not require any training; (2) it does not rely on any particular model; (3) it is very efficient, requiring an average time of only 1.9s for nose tip detection. We tested the proposed technique on BU3DFE and Shrec'10 datasets. Experimental results show that the proposed technique is robust to variations in facial expressions, achieving a 100% detection rate on these publicly available 3D face datasets. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li201693,
author={Li, B.Y.L. and Xue, M. and Mian, A. and Liu, W. and Krishna, A.},
title={Robust RGB-D face recognition using Kinect sensor},
journal={Neurocomputing},
year={2016},
volume={214},
pages={93-108},
doi={10.1016/j.neucom.2016.06.012},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977478475&doi=10.1016%2fj.neucom.2016.06.012&partnerID=40&md5=df18014ab4a68d40273fdbb72a7a85ae},
affiliation={Department of Computing, Curtin University, Kent Street, Perth, WA  6102, Australia; Dalian Key Lab of Digital Technology for National Culture, Dalian Minzu University, Liaoning, Dalian  116600, China; Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA  6009, Australia},
abstract={In this paper we propose a robust face recognition algorithm for low resolution RGB-D Kinect data. Many techniques are proposed for image preprocessing due to the noisy depth data. First, facial symmetry is exploited based on the 3D point cloud to obtain a canonical frontal view image irrespective of the initial pose and then depth data is converted to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are then used to project RGB to DCS (Discriminant Color Space) and normal maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust Sparse Coding method is proposed that codes the multiple channels (DCS or DNM) of a test image as a sparse combination of training samples with different pixel weighting. Weights are calculated dynamically in an iterative process to achieve robustness against variations in pose, illumination, facial expressions and disguise. In contrast to existing techniques, our multi-channel approach is more robust to variations. Reconstruction errors of the test image (DCS and DNM) are normalized and fused to decide its identity. The proposed algorithm is evaluated on four public databases. It achieves 98.4% identification rate on CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects. Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2 databases, the proposed algorithm achieves 97.6%, 95.6% and 95.2% identification rates respectively. To the best of our knowledge, these are the highest identification rates reported so far for the first three databases. © 2016 Elsevier B.V.},
author_keywords={3D face recognition;  Kinect;  Multi-channel discriminant transform;  Sparse coding},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo2016403,
author={Guo, Y. and Lei, Y. and Liu, L. and Wang, Y. and Bennamoun, M. and Sohel, F.},
title={EI3D: Expression-invariant 3D face recognition based on feature and shape matching},
journal={Pattern Recognition Letters},
year={2016},
volume={83},
pages={403-412},
doi={10.1016/j.patrec.2016.04.003},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966659227&doi=10.1016%2fj.patrec.2016.04.003&partnerID=40&md5=b09b19b7a5436b53278c02d001e93910},
affiliation={College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan  410073, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan  610065, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan  410073, China; College of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Computer Science and Software Engineering, The University of Western Australia, Perth, WA  6009, Australia; School of Engineering and Information Technology, Murdoch University, Perth, WA  6150, Australia},
abstract={This paper presents a local feature based shape matching algorithm for expression-invariant 3D face recognition. Each 3D face is first automatically detected from a raw 3D data and normalized to achieve pose invariance. The 3D face is then represented by a set of keypoints and their associated local feature descriptors to achieve robustness to expression variations. During face recognition, a probe face is compared against each gallery face using both local feature matching and 3D point cloud registration. The number of feature matches, the average distance of matched features, and the number of closest point pairs after registration are used to measure the similarity between two 3D faces. These similarity metrics are then fused to obtain the final results. The proposed algorithm has been tested on the FRGC v2 benchmark and a high recognition performance has been achieved. It obtained the state-of-the-art results by achieving an overall rank-1 identification rate of 97.0% and an average verification rate of 99.01% at 0.001 false acceptance rate for all faces with neutral and non-neutral expressions. Further, the robustness of our algorithm under different occlusions has been demonstrated on the Bosphorus dataset. © 2016 Elsevier B.V.},
author_keywords={3D face recognition;  Face identification;  Facial expression;  Keypoint detection;  Local feature;  Shape matching},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lei2016994,
author={Lei, Y. and Feng, S. and Zhou, X. and Guo, Y.},
title={An efficient 3D partial face recognition approach with single sample},
journal={Proceedings of the 2016 IEEE 11th Conference on Industrial Electronics and Applications, ICIEA 2016},
year={2016},
pages={994-999},
doi={10.1109/ICIEA.2016.7603727},
art_number={7603727},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997335714&doi=10.1109%2fICIEA.2016.7603727&partnerID=40&md5=05e7f7f4b11d4caaaace67a1698a150c},
affiliation={College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan, 610065, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, 410073, China},
abstract={3D partial face recognition under missing parts, occlusions and data corruptions is a major challenge for the practical application of the techniques of 3D face recognition. Moreover, one individual can only provide one sample for training in most practical scenarios, and thus the face recognition with single sample problem is another highly challenging task. We propose an efficient framework for 3D partial face recognition with single sample addressing both of the two problems. First, we represent a facial scan with a set of keypoint based local geometrical descriptors, which gains sufficient robustness to partial facial data along with expression/pose variations. Then, a two-step modified collaborative representation classification scheme is proposed to address the single sample recognition problem. A class-based probability estimation is given during the first classification step, and the obtained result is then incorporated into the modified collaborative representation classification as a locality constraint to improve its classification performance. Extensive experiments on the Bosphorus and FRGC v2.0 datasets demonstrate the efficiency of the proposed approach when addressing the problem of 3D partial face recognition with single sample. © 2016 IEEE.},
author_keywords={3D facial representation;  3D partial face recognition;  collaborative representation;  locality constraint;  single sample problem},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2016663,
author={Zhang, J. and Hou, Z. and Wu, Z. and Chen, Y. and Li, W.},
title={Research of 3D face recognition algorithm based on deep learning stacked denoising autoencoder theory},
journal={Proceedings of 2016 8th IEEE International Conference on Communication Software and Networks, ICCSN 2016},
year={2016},
pages={663-667},
doi={10.1109/ICCSN.2016.7586606},
art_number={7586606},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994495941&doi=10.1109%2fICCSN.2016.7586606&partnerID=40&md5=9eed5623ef21575da3fdcb6dfd818fc2},
affiliation={College of Information, Mathematics and Physics, Changzhou University, Changzhou, China},
abstract={This electronic Due to the fact that the 3D face depth data have more information, the 3D face recognition is attracting more and more attention in the machine learning area. Firstly, this paper selects 30 feature points from the 113 feature points of Candide-3 face model to characterize face, which improves the efficiency of recognition algorithm obviously without affecting the recognition accuracy. With the significant advantage of the characterization of essential features by learning a deep nonlinear network, this paper presents a stacked denoising autoencoder algorithm model based on deep learning which improves neural networks model. This algorithm conducts the unsupervised preliminary training of face depth data and the supervised training to fine-tuning the network which is better than neural network's random initialization. The experiment indicates that compared with real face data, the reconstruction face model has a small matching error by using SDAE algorithm and it achieves an excellent face recognition effect. © 2016 IEEE.},
author_keywords={3D face depth data;  deep learning;  neural networks;  stacked denoising autoencoder;  unsupervised preliminary training},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Ganguly2016933,
author={Ganguly, S. and Bhattacharjee, D. and Nasipuri, M.},
title={3D image acquisition and analysis of range face images for registration and recognition},
journal={Biometrics: Concepts, Methodologies, Tools, and Applications},
year={2016},
pages={933-968},
doi={10.4018/978-1-5225-0983-7.ch037},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015971759&doi=10.4018%2f978-1-5225-0983-7.ch037&partnerID=40&md5=8615e03e322fd512c4ff348631cff16a},
affiliation={Jadavpur University, India},
abstract={Although, automatic face recognition has been studied for more than four decades; there are still some challenging issues due to different variations in face images. There are mainly two categories of face recognition based on acquisition procedure. One technology that deals with video based face recognition and another approach where different sensors are used for acquisition purpose of different stationary face images, for instance: optical image, infra-red image and 3D image. In this context, researchers have focused only on 3D face images. 3D face images convey a series of advantages over 2D i.e. video frame, optical as well as infra-red face images. In this chapter, a detailed study of acquisition, visualization, detail about 3D images, analyzing it with some fundamental image processing techniques and application in the field of biometric through face registration and recognition are discussed. This chapter also gives a brief idea of the state of the art about the research methodologies of 3D face recognition and its applications. © 2017, IGI Global. All rights reserved.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Zhang2016,
author={Zhang, J. and Huang, D. and Wang, Y. and Sun, J.},
title={Lock3DFace: A large-scale database of low-cost Kinect 3D faces},
journal={2016 International Conference on Biometrics, ICB 2016},
year={2016},
doi={10.1109/ICB.2016.7550062},
art_number={7550062},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988432113&doi=10.1109%2fICB.2016.7550062&partnerID=40&md5=b25394bb9ffda22d92409c2cd0a74cfa},
affiliation={IRIP Lab, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China},
abstract={In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kim20163011,
author={Kim, D. and Choi, J. and Leksut, J.T. and Medioni, G.},
title={Accurate 3D face modeling and recognition from RGB-D stream in the presence of large pose changes},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2016},
volume={2016-August},
pages={3011-3015},
doi={10.1109/ICIP.2016.7532912},
art_number={7532912},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006789474&doi=10.1109%2fICIP.2016.7532912&partnerID=40&md5=132fb651cb89c0d50239c14c3a7104ae},
affiliation={Institute for Robotics and Intelligent Systems, University of Southern California, 3737 Watt way PHE 101, Los Angeles, CA  90089, United States},
abstract={We propose a 3D face modeling and recognition system using an RGB-D stream in the presence of large pose changes. In the previous work, all facial data points are registered with a reference to improve the accuracy of 3D face model from a low-resolution depth sequence. This registration often fails when applied to non-frontal faces. It causes inaccurate 3D face models and poor performance of matching. We address this problem by pre-aligning each input face ('frontalization') before the registration, which avoids registration failures. For each frame, our method estimates the 3D face pose, assesses the quality of data, segments the facial region, frontalizes it, and performs an accurate registration with the previous 3D model. The 3D-3D recognition system using accurate 3D models from our method outperforms other face recognition systems and shows 100% rank 1 recognition accuracy on a dataset with 30 subjects. © 2016 IEEE.},
author_keywords={3D Face Modeling;  3D Face Recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu20163016,
author={Yu, X. and Gao, Y. and Zhou, J.},
title={3D face recognition under partial occlusions using radial strings},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2016},
volume={2016-August},
pages={3016-3020},
doi={10.1109/ICIP.2016.7532913},
art_number={7532913},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006713653&doi=10.1109%2fICIP.2016.7532913&partnerID=40&md5=dd468caaf8cc8593dad5e05e0c040b9c},
affiliation={School of Engineering, Griffith University, Nathan, QLD, Australia; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia},
abstract={3D face recognition with partial occlusions is a highly challenging problem. In this paper, we propose a novel radial string representation and matching approach to recognize 3D facial scans in the presence of partial occlusions. Here we encode 3D facial surfaces into an indexed collection of radial strings emanating from the nosetips and Dynamic Programming (DP) is then used to measure the similarity between two radial strings. In order to address the recognition problems with partial occlusions, a partial matching mechanism is established in our approach that effectively eliminates those occluded parts and finds the most discriminative parts during the matching process. Experimental results on the Bosphorus database demonstrate that the proposed approach yields superior performance on partially occluded data. © 2016 IEEE.},
author_keywords={3D face recognition;  Partial occlusions;  Radial string matching;  Structural recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Torkhani2016447,
author={Torkhani, G. and Ladgham, A. and Mansouri, M.N. and Sakly, A.},
title={Gabor-SVM applied to 3D-2D deformed mesh model},
journal={2nd International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2016},
year={2016},
pages={447-452},
doi={10.1109/ATSIP.2016.7523133},
art_number={7523133},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984607903&doi=10.1109%2fATSIP.2016.7523133&partnerID=40&md5=a94aa03e640ea81c789823dfdf4c35ed},
affiliation={CSR Research Unit, E e Laboratory, National School of Engineering, Ibn Eljazzar, Monastir, 5019, Tunisia; Electrical Departement, National School of Engineering, Ibn Eljazzar, Monastir, 5019, Tunisia},
abstract={We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate. © 2016 IEEE.},
author_keywords={3D face recognition;  deformed mesh model;  facial curvatures;  Gabor wavelet;  salient points;  SVM},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hariri20161,
author={Hariri, W. and Tabia, H. and Farah, N. and Benouareth, A. and Declercq, D.},
title={3D face recognition using covariance based descriptors},
journal={Pattern Recognition Letters},
year={2016},
volume={78},
pages={1-7},
doi={10.1016/j.patrec.2016.03.028},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964930100&doi=10.1016%2fj.patrec.2016.03.028&partnerID=40&md5=bca65280157410a38ef712a15de825bc},
affiliation={ETIS, ENSEA, Univ Cergy-Pontoise, CNRS, UMR-8051, Cergy-Pontoise Cedex, 95014, France; Labged Laboratory, Computer Science Department, Badji Mokhtar Annaba University, B.P.12, Annaba, 23000, Algeria},
abstract={In this paper, we propose a new 3D face recognition method based on covariance descriptors. Unlike feature-based vectors, covariance-based descriptors enable the fusion and the encoding of different types of features and modalities into a compact representation. The covariance descriptors are symmetric positive definite matrices which can be viewed as an inner product on the tangent space of (Symd+) the manifold of Symmetric Positive Definite (SPD) matrices. In this article, we study geodesic distances on the Symd+ manifold and use them as metrics for 3D face matching and recognition. We evaluate the performance of the proposed method on the FRGCv2 and the GAVAB databases and demonstrate its superiority compared to other state of the art methods. © 2016 Elsevier B.V. All rights reserved.},
author_keywords={Covariance matrix;  Face matching;  Geodesic distances},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ratyal2016294,
author={Ratyal, N.I. and Taj, I.A. and Bajwa, U.I. and Sajid, M. and Baig, M.J.A. and Butt, F.M.},
title={3D face recognition based on region ensemble and hybrid features},
journal={2016 International Conference on Computing, Electronic and Electrical Engineering, ICE Cube 2016 - Proceedings},
year={2016},
pages={294-300},
doi={10.1109/ICECUBE.2016.7495241},
art_number={7495241},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979619459&doi=10.1109%2fICECUBE.2016.7495241&partnerID=40&md5=92ad69daeb0afd442983127ab6cf1e01},
affiliation={Electrical Engineering Department, Vision and Pattern Recognition Systems Research Group, Capital University of Science and Technology, Islamabad, Pakistan; Department of Computer Science, COMSATS Institute of Information Technology, Lahore, Pakistan; Electrical (Power) Engineering Department, Mirpur University of Science and Technology (AJK), Pakistan; Electrical Engineering Department, University of Azad Jammu and Kashmir, Muzaffarabad (AJK), Pakistan},
abstract={In this paper, we present a novel pose and expression invariant 3D face recognition approach based on intrinsic coordinate system (ICS) alignment. Motivated by the fact that a single classifier cannot be generally efficient against all face regions, a two tier region ensemble based classification approach is presented which employs three parallel face recognition algorithms using Mahalanobis Cosine (MahCos) matching score. The parallel face recognition algorithms employ Principal Component Analysis (PCA) based holistic features, Local Binary Patterns (LBP) based local features and modified Borda Count (MBC) based fusion technique respectively to classify the face images. The results obtained from the parallel algorithms are combined using an exponential rank reordering approach. The performance of the proposed methodology is corroborated by extensive experiments performed on FRGC v2.0 3D database. The results confirm that fusion strategies can be effectively used to construct a single classifier for improved performance. © 2016 IEEE.},
author_keywords={3D face recognition;  FRGC v2.0;  fusion;  region ensemble},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Galbally201683,
author={Galbally, J. and Satta, R.},
title={Three-dimensional and two-and-a-halfdimensional face recognition spoofing using three-dimensional printed models},
journal={IET Biometrics},
year={2016},
volume={5},
number={2},
pages={83-91},
doi={10.1049/iet-bmt.2014.0075},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971441283&doi=10.1049%2fiet-bmt.2014.0075&partnerID=40&md5=428d278db100f600f4c059cb0568c416},
affiliation={European Commission, Joint Research Centre (JRC), Institute for Protection and Security of Citizen (IPSC), via Enrico Fermi 2749, Ispra, VA  21027, Italy},
abstract={The vulnerability of biometric systems to external attacks using a physical artefact in order to impersonate the legitimate user has become a major concern over the last decade. Such a threat, commonly known as 'spoofing', poses a serious risk to the integrity of biometric systems. The usual low-complexity and low-cost characteristics of these attacks make them accessible to the general public, rendering each user a potential intruder. The present study addresses the spoofing issue analysing the feasibility to perform low-cost attacks with self-manufactured three-dimensional (3D) printed models to 2.5D and 3D face recognition systems. A new database with 2D, 2.5D and 3D real and fake data from 26 subjects was acquired for the experiments. Results showed the high vulnerability of the three tested systems, including a commercial solution, to the attacks. © The Institution of Engineering and Technology 2016.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Werghi2016964,
author={Werghi, N. and Tortorici, C. and Berretti, S. and Del Bimbo, A.},
title={Boosting 3D LBP-Based Face Recognition by Fusing Shape and Texture Descriptors on the Mesh},
journal={IEEE Transactions on Information Forensics and Security},
year={2016},
volume={11},
number={5},
pages={964-979},
doi={10.1109/TIFS.2016.2515505},
art_number={7373633},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963974403&doi=10.1109%2fTIFS.2016.2515505&partnerID=40&md5=376f43714248cb873d28dcb663ac1bb5},
affiliation={Electrical and Computer Engineering Department, Khalifa University, Abu Dhabi, 127788, United Arab Emirates; Department of Information Engineering, University of Florence, Florence, 50139, Italy},
abstract={In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBPs) on a mesh for 3D face recognition. Using a recently proposed framework, we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared with its depth-image counterpart, our approach is distinguished by the following features: 1) inherits the intrinsic advantages of mesh surface (e.g., preservation of the full geometry); 2) does not require normalization; and 3) can accommodate partial matching. In addition, it allows early level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data, also in comparison to the state-of-the-art solutions. © 2016 IEEE.},
author_keywords={3D face recognition;  feature and score fusion;  mesh-LBP},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sun2016,
author={Sun, Y.},
title={Expression invariant 3D face recognition based on GMDS},
journal={2015 10th International Conference on Information, Communications and Signal Processing, ICICS 2015},
year={2016},
doi={10.1109/ICICS.2015.7459832},
art_number={7459832},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973640839&doi=10.1109%2fICICS.2015.7459832&partnerID=40&md5=e564c632dda19325c177c7574b194f1b},
affiliation={School of Information Engineering, Guangdong University of Technology, Guangzhou, China},
abstract={In this paper, we propose an efficient expression-invariant 3D face recognition algorithm to compute the minimum-distortion mapping between two 3D face by the Generalized MultiDimensional Scaling (GMDS). Both full and partial parts matching are computed for finding the least distortion embedding of one 3D face into another during GMDS. The problem of expression-invariant three-dimensional face recognition turns to be isometry-invariant matching of surfaces by the means of GMDS. The FRGC v2.0 dataset is conducted for covering face recognition under expression changes. The experimental results demonstrate that the proposed method provides a very encouraging new solution for 3D face recognition. © 2015 IEEE.},
author_keywords={3D face recognition;  Generalized MultiDimensional Scaling(GMDS)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ganguly2016275,
author={Ganguly, S. and Bhattachaijee, D. and Nasipuri, M.},
title={3D face recognition from complement component range face images},
journal={2015 IEEE International Conference on Computer Graphics, Vision and Information Security, CGVIS 2015},
year={2016},
pages={275-278},
doi={10.1109/CGVIS.2015.7449936},
art_number={7449936},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966632732&doi=10.1109%2fCGVIS.2015.7449936&partnerID=40&md5=ac5698f1d98fa8b29941ffc837f84a7b},
affiliation={Department of Computer Science and Engineering, Jadavpur University, Kolkata-32, India},
abstract={Face and facial attributes represent meaningful definition about a variety of information to discriminate an individual from others and for developing a computational model for automatic face recognition purpose. However, in this work, selection of relevant features from newly created face space is the pivotal contribution of the authors. Here, authors have demonstrated a new face space 'Complement Component' that have been used to extract the four basic components along X, and Y axes in four directions. Later, authors have experimented the discriminative attributes from these face spaces for recognition purpose. Here, comparison of the proposed method has been reported by examining its success on two well accepted 3D face databases, namely: Frav3D and Texas3D. In case of 2D face images, it does not contain depth like information i.e. Z-values in X-Y plane through intensity values. Therefore, it has not been undertaken during this investigation. © 2015 IEEE.},
author_keywords={3D face image;  Complement Component;  Face recognition;  range face image},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lei2016218,
author={Lei, Y. and Guo, Y. and Hayat, M. and Bennamoun, M. and Zhou, X.},
title={A Two-Phase Weighted Collaborative Representation for 3D partial face recognition with single sample},
journal={Pattern Recognition},
year={2016},
volume={52},
pages={218-237},
doi={10.1016/j.patcog.2015.09.035},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947306090&doi=10.1016%2fj.patcog.2015.09.035&partnerID=40&md5=50ca73115b78f6746739042c1f293a0e},
affiliation={College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China; School of Computer Science and Software Engineering, University of Western Australia, Crawley, WA, Australia; IBM Research Australia, Carlton, VIC, Australia},
abstract={3D face recognition with the availability of only partial data (missing parts, occlusions and data corruptions) and single training sample is a highly challenging task. This paper presents an efficient 3D face recognition approach to address this challenge. We represent a facial scan with a set of local Keypoint-based Multiple Triangle Statistics (KMTS), which is robust to partial facial data, large facial expressions and pose variations. To address the single sample problem, we then propose a Two-Phase Weighted Collaborative Representation Classification (TPWCRC) framework. A class-based probability estimation is first calculated based on the extracted local descriptors as a prior knowledge. The resulting class-based probability estimation is then incorporated into the proposed classification framework as a locality constraint to further enhance its discriminating power. Experimental results on six challenging 3D facial datasets show that the proposed KMTS-TPWCRC framework achieves promising results for human face recognition with missing parts, occlusions, data corruptions, expressions and pose variations. © 2015 Elsevier Ltd. All rights reserved.},
author_keywords={3D face recognition;  3D representation;  Partial facial data;  Single sample problem;  Sparse representation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Echeagaray-Patron2016843,
author={Echeagaray-Patron, B.A. and Miramontes-Jaramillo, D. and Kober, V.},
title={Conformal parameterization and curvature analysis for 3D facial recognition},
journal={Proceedings - 2015 International Conference on Computational Science and Computational Intelligence, CSCI 2015},
year={2016},
pages={843-844},
doi={10.1109/CSCI.2015.133},
art_number={7424213},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964412211&doi=10.1109%2fCSCI.2015.133&partnerID=40&md5=6a3fc9e7dac95a6f25aacc425aa40af5},
affiliation={Department of Computer Science, CICESE, Ensenada, B.C., 22860, Mexico; Department of Mathematics, Chelyabinsk State University, Russian Federation},
abstract={This work proposes a new algorithm for 3D face recognition. The algorithm uses 3D shape data without color or texture information and exploits local curvature information which is a measure with high discriminant capability and robust to deformations such as rotation and scaling. In order to reduce high dimensionality of typical face surfaces our approach uses a conformal parameterization, preserving angles of original faces and simplifies the correspondence problem. Experimental results are presented and discussed using CASIA and Gavab databases. © 2015 IEEE.},
author_keywords={3D face recognition;  Conformal parameterization;  Curvature analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhou2016109,
author={Zhou, W. and Chen, J.-X. and Wang, L.},
title={A RGB-D face recognition approach without confronting the camera},
journal={Proceedings of 2015 IEEE International Conference on Computer and Communications, ICCC 2015},
year={2016},
pages={109-114},
doi={10.1109/CompComm.2015.7387550},
art_number={7387550},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963938690&doi=10.1109%2fCompComm.2015.7387550&partnerID=40&md5=d5bfca843f857fa39877a526fa1c2108},
affiliation={Key Lab of Broadband Wireless Communication and Sensor Network Technology, Ministry of Education, Nanjing, 210003, China},
abstract={Face recognition research mainly focuses on traditional 2D color images, which is extremely susceptible to be affected by external factors such as various viewpoints and has limited recognition accuracy. In order to achieve improved recognition performance, as well as the 3D face holds more abundant information than 2D, we present a 3D human face recognition algorithm using the Microsoft's Kinect. The proposed approach integrates the depth data with the RGB data to generate 3D face raw data and then extracts feature points, identifies the target via a two-level cascade classifier. Also, we build a 3D-face database including 16 individuals captured exclusively using Kinect. The experimental results indicate that the introduced algorithm can not only achieve better recognition accuracy in comparison to existing 2D and 3D face recognition algorithms when the probe face is exactly in front of Kinect sensor, but also can increase 9.3% of recognition accuracy compared to the PCA-3D algorithm when it is not confronting the camera. © 2015 IEEE.},
author_keywords={3D face recognition;  classifier;  Kinect;  RGB-D images;  XML file},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bagchi2016,
author={Bagchi, P. and Bhattacharjee, D. and Nasipuri, M.},
title={3D Face Recognition using surface normals},
journal={IEEE Region 10 Annual International Conference, Proceedings/TENCON},
year={2016},
volume={2016-January},
doi={10.1109/TENCON.2015.7372819},
art_number={7372819},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962199727&doi=10.1109%2fTENCON.2015.7372819&partnerID=40&md5=8d29f7cc34b9d49495eea2e33fa5af0b},
affiliation={Dept of CSE, RCC Institute of Information Technology, Beliaghata, Kolkata, India; Dept of CSE, Jadavpur University, Kolkata, India},
abstract={In this proposed work, a fully automatic 3D Face Recognition system across pose is presented, which works successfully on three modern databases namely the Frav3D, GavabDB and the Bosphorus databases. Poses handled in the system are yaw, pitch and roll varying from 0° to ±30° as well as expressions. The feature extraction is by depth face images with variation in depth values of the surface normals and also by KPCA. The system gives high recognition rate of 96.92% in case of GavabDB database, 96.25% in case of Bosphorus face database and 92.25% in case of Frav3D database by surface normals extraction. © 2015 IEEE.},
author_keywords={Gaussian kernel;  ICP;  Kernel-PCA;  Range Images},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{AdrianaEcheagaray-Patrón2016,
author={Adriana Echeagaray-Patrón, B. and Kober, V.},
title={Face recognition based on matching of local features on 3D dynamic range sequences},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9971},
doi={10.1117/12.2236355},
art_number={997131},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006856443&doi=10.1117%2f12.2236355&partnerID=40&md5=12be001842b19c746ae9a723c01aed88},
affiliation={Department of Computer Science, CICESE, Ensenada, B.C., 22860, Mexico; Department of Mathematics, Chelyabinsk State University, Russian Federation},
abstract={3D face recognition has attracted attention in the last decade due to improvement of technology of 3D image acquisition and its wide range of applications such as access control, surveillance, human-computer interaction and biometric identification systems. Most research on 3D face recognition has focused on analysis of 3D still data. In this work, a new method for face recognition using dynamic 3D range sequences is proposed. Experimental results are presented and discussed using 3D sequences in the presence of pose variation. The performance of the proposed method is compared with that of conventional face recognition algorithms based on descriptors. © 2016 SPIE.},
author_keywords={3D face recognition;  Dynamic 3D range sequences;  HOG;  LBP;  SURF},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Starczewski2016210,
author={Starczewski, J.T. and Pabiasz, S. and Vladymyrska, N. and Marvuglia, A. and Napoli, C. and Wózniak, M.},
title={Self organizing maps for 3D face understanding},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9693},
pages={210-217},
doi={10.1007/978-3-319-39384-1_19},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977471207&doi=10.1007%2f978-3-319-39384-1_19&partnerID=40&md5=7f993d82b7b04337f5466c3ed735ff72},
affiliation={Institute of Computational Intelligence, Czestochowa University of Technology, Czestochowa, Poland; Radom Academy of Economics, Radom, Poland; Environmental Research and Innovation Department, Luxembourg Institute of Science and Technology, Esch-sur-Alzette, Luxembourg; Department of Mathematics and Informatics, University of Catania, Catania, Italy; Institute of Mathematics, Silesian University of Technology, Gliwice, Poland},
abstract={Landmarks are unique points that can be located on every face. Facial landmarks typically recognized by people are correlated with anthropomorphic points. Our purpose is to employ in 3D face recognition such landmarks that are easy to interpret. Face understanding is construed as identification of face characteristic points with automatic labeling of them. In this paper, we apply methods based on Self Organizing Maps to understand 3D faces. © Springer International Publishing Switzerland 2016.},
author_keywords={3D face recognition;  Self organizing maps;  Understanding of images},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Galbally2016199,
author={Galbally, J. and Satta, R.},
title={Biometric sensor interoperability: A case study in 3D face recognition},
journal={ICPRAM 2016 - Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods},
year={2016},
pages={199-204},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969988967&partnerID=40&md5=80af1e7c3fa3ef4e303e144e6927af47},
affiliation={European Commission - Joint Research Centre, IPSC, Via Enrico Fermi 2749, Ispra, 21027, Italy},
abstract={Biometric systems typically suffer a significant loss of performance when the acquisition sensor is changed between enrolment and authentication. Such a problem, commonly known as sensor interoperability, poses a serious challenge to the accuracy of matching algorithms. The present work addresses for the first time the sensor interoperability issue in 3D face recognition systems, analysing the performance of two popular and well known techniques for 3D facial authentication. For this purpose, a new gender-balanced database comprising 3D data of 26 subjects has been acquired using two devices belonging to the new generation of low-cost 3D sensors. The results show the high sensor-dependency of the tested systems and the need to develop matching algorithms robust to the variation in the sensor resolution. © Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={3D face database;  3D face recognition;  Interoperability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Song2016,
author={Song, D. and Luo, J. and Zi, C. and Tian, H.},
title={3D Face Recognition Using Anthropometric and Curvelet Features Fusion},
journal={Journal of Sensors},
year={2016},
volume={2016},
doi={10.1155/2016/6859364},
art_number={6859364},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954447014&doi=10.1155%2f2016%2f6859364&partnerID=40&md5=145d972d12039ce0cfdea217aaa57214},
affiliation={College of Electrical Engineering and Automation, Tianjin Polytechnic University, Tianjin, 300387, China; Key Laboratory of Advanced Electrical Engineering and Energy Technology, Tianjin, 300387, China; School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, Sydney, NSW  2522, Australia},
abstract={Curvelet transform can describe the signal by multiple scales, and multiple directions. In order to improve the performance of 3D face recognition algorithm, we proposed an Anthropometric and Curvelet features fusion-based algorithm for 3D face recognition (Anthropometric Curvelet Fusion Face Recognition, ACFFR). First, the eyes, nose, and mouth feature regions are extracted by the Anthropometric characteristics and curvature features of the human face. Second, Curvelet energy features of the facial feature regions at different scales and different directions are extracted by Curvelet transform. At last, Euclidean distance is used as the similarity between template and objectives. To verify the performance, the proposed algorithm is compared with Anthroface3D and Curveletface3D on the Texas 3D FR database. The experimental results have shown that the proposed algorithm performs well, with equal error rate of 1.75% and accuracy of 97.0%. The algorithm we proposed in this paper has better robustness to expression and light changes than Anthroface3D and Curveletface3D. © 2016 Dan Song et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bellil2016365,
author={Bellil, W. and Brahim, H. and Ben Amar, C.},
title={Gappy wavelet neural network for 3D occluded faces: detection and recognition},
journal={Multimedia Tools and Applications},
year={2016},
volume={75},
number={1},
pages={365-380},
doi={10.1007/s11042-014-2294-6},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953638822&doi=10.1007%2fs11042-014-2294-6&partnerID=40&md5=e5d7d42e44edd21d1de8b6e4cc7bdd48},
affiliation={REGIM: REsearch Groups on Intelligent Machines, University of Sfax, National Engineering School of Sfax (ENIS), Sfax, Tunisia},
abstract={The first handicap in 3D faces recognizing under unconstrained problem is the largest variability of the visual aspect when we use various sources. This great variability complicates the task of identifying persons from their 3D facial scans and it is the most reason that bring to face detection and recognition of the major problems in pattern recognition fields, biometrics and computer vision. We propose a new 3D face identification and recognition method based on Gappy Wavelet Neural Network (GWNN) that is able to provide better accuracy in the presence of facial occlusions. The proposed approach consists of three steps: the first step is face detection. The second step is to identify and remove occlusions. Occluded regions detection is done by considering that occlusions can be defined as local face deformations. These deformations are detected by a comparison between the input facial test wavelet coefficients and wavelet coefficients of generic face model formed by the mean data base faces. They are beneficial for neighborhood relationships between pixels rotation, dilation and translation invariant. Then, occluded regions are refined by removing wavelet coefficient above a certain threshold. Finally, the last stage of processing and retrieving is made based on wavelet neural network to recognize and to restore 3D occluded regions that gathers the most. The experimental results on this challenging database demonstrate that the proposed approach improves recognition rate performance from 93.57 to 99.45 % which represents a competitive result compared to the state of the art. © 2014, Springer Science+Business Media New York.},
author_keywords={3D face recognition; Wavelets;  Gappy data;  Occlusion detection;  Wavelet neural network},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gaonkar201615,
author={Gaonkar, A.A. and Gad, M.D. and Vetrekar, N.T. and Tilve, V.S. and Gad, R.S.},
title={Experimental evaluation of 3D kinect face database},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={10481 LNCS},
pages={15-26},
doi={10.1007/978-3-319-68124-5_2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057824711&doi=10.1007%2f978-3-319-68124-5_2&partnerID=40&md5=3a104665aebdb39c053c8b83ce35c8fb},
affiliation={Department of Electronics, Goa University, Taleigao Plateau, Goa, India; Goa Engineering College, Farmagudi, Goa, India; School of Earth and Space Exploration, Arizona State University, Tempe, United States},
abstract={3D face recognition has gain a paramount importance over 2D due to its potential to address the limitations of 2D face recognition against the variation in facial poses, angles, occlusions etc. Research in 3D face recognition has accelerated in recent years due to the development of low cost 3D Kinect camera sensor. This has leads to the development of few RGB-D database across the world. Here in this paper we introduce the base results of our 3D facial database (GU-RGBD database) comprising variation in pose (0°, 45°, 90°, −45°, −90°), expression (smile, eyes closed), occlusion (half face covered with paper) and illumination variation using Kinect. We present a proposed noise removal non-linear interpolation filter for the patches present in the depth images. The results were obtained on three face recognition algorithms and fusion at matching score level for recognition and verification rate. The obtained results indicated that the performance with our proposed filter shows improvement over pose with score level fusion using sum rule. © Springer International Publishing AG 2017.},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Tistarelli2016305,
author={Tistarelli, M. and Cadoni, M. and Lagorio, A. and Grosso, E.},
title={Blending 2D and 3D face recognition},
journal={Face Recognition Across the Imaging Spectrum},
year={2016},
pages={305-331},
doi={10.1007/978-3-319-28501-6_13},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014098769&doi=10.1007%2f978-3-319-28501-6_13&partnerID=40&md5=44e32cb05fff607d64356a67b85a0d29},
affiliation={University of Sassari, Sassari, Italy},
abstract={Over the last decade, performance of face recognition algorithms systematically improved. This is particularly impressive when considering very large or challenging datasets such as the FRGC v2 or Labelled Faces in the Wild. A better analysis of the structure of the facial texture and shape is one of the main reasons of improvement in recognition performance. Hybrid face recognition methods, combining holistic and feature-based approaches, also allowed to increase efficiency and robustness. Both photometric information and shape information allow to extract facial features which can be exploited for recognition. However, both sources, grey levels of image pixels and 3D data, are affected by several noise sources which may impair the recognition performance. One of the main difficulties in matching 3D faces is the detection and localization of distinctive and stable points in 3D scans. Moreover, the large amount of data (tens of thousands of points) to be processed make the direct one-to-one matching a very time-consuming process. On the other hand, matching algorithms based on the analysis of 2D data alone are very sensitive to variations in illumination, expression and pose. Algorithms, based on the face shape information alone, are instead relatively insensitive to these sources of noise. These mutually exclusive features of 2D- and 3D-based face recognition algorithm call for a cooperative scheme which may take advantage of the strengths of both, while coping for their weaknesses. We envisage many real and practical applications where 2D data can be used to improve 3D matching and vice versa. Towards this end, this chapter highlights both the advantages and disadvantages of 2D- and 3D-based face recognition algorithms. It also explores the advantages of blending 2D- and 3D data-based techniques, also proposing a novel approach for a fast and robust matching. Several experimental results, obtained from publicly available datasets, currently at the state of the art, demonstrate the effectiveness of the proposed approach. © Springer International Publishing Switzerland 2016.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Boukamcha2015,
author={Boukamcha, H. and Elhallek, M. and Atri, M. and Smach, F.},
title={3D face landmark auto detection},
journal={2015 World Symposium on Computer Networks and Information Security, WSCNIS 2015},
year={2015},
doi={10.1109/WSCNIS.2015.7368276},
art_number={7368276},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962430025&doi=10.1109%2fWSCNIS.2015.7368276&partnerID=40&md5=13ae6691c8afd0ea13bb469882ec0aea},
affiliation={University of Sciences of Monastir, Monastir, Tunisia; Université of Bourgogne, France},
abstract={This paper presents our methodology for Landmark Point detection to improve 3D face recognition in a presence of variant facial expression. The objective was to develop an automatic process for distinguishing and segmenting to be embedded in a 3D face recognition system using only 3D Point Distribution Model (PDM) as input. The approach used hydride method to extract this features from the surface curvature information. Landmark Localization is done on the segmented face via finding the change that decreases the deviation of the model from the mean profile. Face registering is achieved using previous anthropometric information and the localized landmarks. The results confirm that the method used is accurate and robust for the proposed application. © 2015 IEEE.},
author_keywords={3D Face;  Graph Matching;  Labelling;  Registration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lv20153635,
author={Lv, S. and Da, F. and Deng, X.},
title={A 3D face recognition method using region-based extended local binary pattern},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2015},
volume={2015-December},
pages={3635-3639},
doi={10.1109/ICIP.2015.7351482},
art_number={7351482},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956668987&doi=10.1109%2fICIP.2015.7351482&partnerID=40&md5=2c0948e184ab3751c8c447b235e50e0a},
affiliation={School of Automation, Southeast University, Key Laboratory of Measurement and Control of CSE, Ministry of Education, Nanjing, 210096, China},
abstract={A 3D face recognition method using region-based extended local binary pattern (eLBP) is proposed. First, the depth image converted from the preprocessed 3D pointclouds is normalized. Then, different regions according to their distortions under facial expressions are extracted by binary masks and represented by the uniform pattern of extended LBP. Finally, sparse representation classifier (SRC) is adopted for classification on the single region. Feature-level and score-level fusion with weight-sparse representation classifier (W-SRC) are also tested and compared, and the latter has better performance. The experiments on FRGC v2.0 database demonstrate that the proposed method is robust and efficient. © 2015 IEEE.},
author_keywords={3D face recognition;  binary mask;  depth image;  extended local binary pattern;  weight-sparse representation classifier},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tortorici20152670,
author={Tortorici, C. and Werghi, N. and Berretti, S.},
title={Boosting 3D LBP-based face recognition by fusing shape and texture descriptors on the mesh},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2015},
volume={2015-December},
pages={2670-2674},
doi={10.1109/ICIP.2015.7351287},
art_number={7351287},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956639911&doi=10.1109%2fICIP.2015.7351287&partnerID=40&md5=6ea1b56b061b463d4fc6e850dc9c85e6},
affiliation={Khalifa University, Abu Dhabi, United Arab Emirates; University of Florence, Florence, Italy},
abstract={In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBP) for 3D face recognition. Using the framework proposed in [1], we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared to its depth-image counterpart, our approach is distinguished by the following features: a) inherits the intrinsic advantages of mesh surface; b) does not require normalization; c) can accommodate partial matching. In addition, it allows early-level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data. © 2015 IEEE.},
author_keywords={3D face recognition;  fusion;  mesh-LBP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Patil2015393,
author={Patil, H. and Kothari, A. and Bhurchandi, K.},
title={3-D face recognition: features, databases, algorithms and challenges},
journal={Artificial Intelligence Review},
year={2015},
volume={44},
number={3},
pages={393-441},
doi={10.1007/s10462-015-9431-0},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941419993&doi=10.1007%2fs10462-015-9431-0&partnerID=40&md5=c90b8821f623bcb40665d0010a316ef6},
affiliation={Visvesvaraya National Institute of Technology, Nagpur, India},
abstract={Face recognition is being widely accepted as a biometric technique because of its non-intrusive nature. Despite extensive research on 2-D face recognition, it suffers from poor recognition rate due to pose, illumination, expression, ageing, makeup variations and occlusions. In recent years, the research focus has shifted toward face recognition using 3-D facial surface and shape which represent more discriminating features by the virtue of increased dimensionality. This paper presents an extensive survey of recent 3-D face recognition techniques in terms of feature detection, classifiers as well as published algorithms that address expression and occlusion variation challenges followed by our critical comments on the published work. It also summarizes remarkable 3-D face databases and their features used for performance evaluation. Finally we suggest vital steps of a robust 3-D face recognition system based on the surveyed work and identify a few possible directions for research in this area. © 2015, Springer Science+Business Media Dordrecht.},
author_keywords={3-D Face databases;  3-D faces;  Biometrics;  Classifiers;  Face matching;  Face recognition;  Feature extraction},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Werghi20152521,
author={Werghi, N. and Tortorici, C. and Berretti, S. and Del Bimbo, A.},
title={Representing 3D texture on mesh manifolds for retrieval and recognition applications},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2015},
volume={07-12-June-2015},
pages={2521-2530},
doi={10.1109/CVPR.2015.7298867},
art_number={7298867},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959190332&doi=10.1109%2fCVPR.2015.7298867&partnerID=40&md5=f250cc4afb5eff7b0a9ce39449bd5844},
affiliation={Khalifa University of Science, Technology and Research, Sharjah, United Arab Emirates; University of Florence, Florence, Italy},
abstract={In this paper, we present and experiment a novel approach for representing texture of 3D mesh manifolds using local binary patterns (LBP). Using a recently proposed framework [37], we compute LBP directly on the mesh surface, either using geometric or photometric appearance. Compared to its depth-image counterpart, our approach is distinguished by the following features: a) inherits the intrinsic advantages of mesh surface (e.g., preservation of the full geometry); b) does not require normalization; c) can accommodate partial matching. In addition, it allows early-level fusion of the geometry and photometric texture modalities. Through experiments conducted on two application scenarios, namely, 3D texture retrieval and 3D face recognition, we assess the effectiveness of the proposed solution with respect to state of the art approaches. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liang2015,
author={Liang, Y. and Zhang, Y.},
title={Expression-invariant face recognition using three-dimensional weighted walkthrough and centroid distance},
journal={Journal of Electronic Imaging},
year={2015},
volume={24},
number={5},
doi={10.1117/1.JEI.24.5.053007},
art_number={053007},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941344580&doi=10.1117%2f1.JEI.24.5.053007&partnerID=40&md5=cf2a7dde7737084919b6a8b4c738d263},
affiliation={Guangdong University of Technology, School of Automation, Guangzhou Higher Education Mega Centre, No. 100, Waihuan Xi Road, Guangzhou, 510006, China; South China Normal University, School of Software, Nanhai Information Technology Park, Foshan, 528225, China},
abstract={Three-dimensional (3-D) face recognition provides a potential to handle challenges caused by illumination and pose variations. However, extreme expression variations still complicate the task of recognition. An accurate and robust method for expression-invariant 3-D face recognition is proposed. A 3-D face is partitioned into a set of isogeodesic stripes and the spatial relationships of the stripes are described by 3-D weighted walkthrough and the centroid distance. Moreover, the method of the similarity measure is given. Experiments are performed on the CASIA dataset and the FRGC v2.0 dataset. The results show that our method has advantages for recognition performance despite large expression variations. © 2015 SPIE and IS&T.},
author_keywords={3-D face recognition;  3-D weighted walkthrough;  centroid distance;  isogeodesic stripes},
document_type={Article},
source={Scopus},
}

@ARTICLE{Deng20155509,
author={Deng, X. and Da, F. and Shao, H.},
title={Expression-robust 3D face recognition using region-based multiscale wavelet feature fusion},
journal={Journal of Computational Information Systems},
year={2015},
volume={11},
number={15},
pages={5509-5517},
doi={10.12733/jcis14953},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950271521&doi=10.12733%2fjcis14953&partnerID=40&md5=da8d93edfc030808fb309b097e409e94},
affiliation={Department of Automation, Southeast University, Nanjing, 210096, China; Key Laboratory of Measurement and Control for Complex System of Ministry of Education, Southeast University, Nanjing, 210096, China},
abstract={In order to eliminate the impact of facial expressions and improve the efficiency of calculation, this paper proposes a novel expression-robust 3D face recognition algorithm using region-based feature fusion technique based on multiscale wavelet transformations. The discrete wavelet transformation is applied to extract frequency component features of geometric image based on the semi-rigid face region as well as the non-rigid face region in order to reduce the influence from the facial expression using the Coherent Point Drift non-rigid point set registration. The dimensionality reduction methods are utilized to promote the computational efficiency, and the experimental results show that our algorithm outperforms state-of-the-art methods based on FRGC v2.0. Copyright © 2015 Binary Information Press.},
author_keywords={3D face recognition;  Dimensionality reduction;  Multiscale wavelet transform;  Non-rigid point set registration},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mracek2015,
author={Mracek, S. and Dvorak, R. and Vana, J. and Novotny, T. and Drahansky, M.},
title={3D face recognition utilizing a low-cost depth sensor},
journal={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
year={2015},
doi={10.1109/FG.2015.7163169},
art_number={7163169},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944930257&doi=10.1109%2fFG.2015.7163169&partnerID=40&md5=195536ecc9cf6a2730e8b680cec47b46},
affiliation={Faculty of Information Technology, Brno University of Technology, Bozetechova 1/2, Brno, 612 66, Czech Republic},
abstract={This demo shows a working prototype of the 3D face recognition biometric device utilizing a low-cost depth sensor, namely SoftKinetic DS325. It is based on the Intel Celeron board for embedded PCs, the sensor, and a touch screen. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao2015,
author={Gao, J. and Evans, A.N.},
title={Expression robust 3D face recognition by matching multi-component local shape descriptors on the nasal and adjoining cheek regions},
journal={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
year={2015},
doi={10.1109/FG.2015.7163144},
art_number={7163144},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944930906&doi=10.1109%2fFG.2015.7163144&partnerID=40&md5=f40854840a153023accb0e1bb1ac740d},
affiliation={Department of Electronic and Electrical Engineering, University of Bath, Bath, United Kingdom},
abstract={This paper proposes a novel local depth and surface normals descriptor to explore the discriminative features on the nasal surface and the adjoining cheek regions for expression robust 3D face recognition. After preprocessing the 3D face data, landmarks located on the perimeter of a triangular region covering the nose and adjoining parts of the cheeks are accurately detected. Inspired by Local Binary Patterns, local shape differences for 3D points on a set of horizontal curves joining selected landmarks provide a novel representation of the local shape information. A further analysis of the discriminatory power of each patch shows that the adjoining regions have the potential to produce good recognition performance. Using the FRGC and Bosphorus databases, the performance of the proposed descriptor is evaluated on diverse patches, scales and for four components, one from the depth and three from the surface normals. Results show that the new local shape descriptor performs well at representing the shape information on a relatively large scale. On the basis of this descriptor, a relatively small set of features extracted from the nasal and adjoining cheek regions produce a R1RR of 97.76% and an EER of 1.32%. The adjoining cheek regions demonstrate a high discriminatory power and provide a useful new addition to 3D face biometrics. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xuechun2015587,
author={Xuechun, W. and Zhaoping, W.},
title={Design of a three-dimensional face recognition system},
journal={Open Automation and Control Systems Journal},
year={2015},
volume={7},
number={1},
pages={587-590},
doi={10.2174/1874444301507010587},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958249364&doi=10.2174%2f1874444301507010587&partnerID=40&md5=2acfca5b28422aff2b32e0652b175519},
affiliation={School of Information Engineering, Huanghe Science and Technology College, Zhengzhou, Henan  450006, China},
abstract={With the development of computer vision and computer graphics, face recognition technology has gradually developed from 2D to 3D. Using in-depth information of faces, 3D face recognition solves and overcomes the environ- ment and changes in the expression that 2D face recognition encounters. In this paper, the principles of laser triangulation measurement applied in 3D detection and modeling of faces are studied. A 3D face recognition system is designed on this basis. It not only has universal configuration and simple algorithm as well as other characteristics, but also has certain practicality. © Xuechun and Zhaoping.},
author_keywords={3D modeling;  Computer vision;  Face recognition;  Principles of triangulation measurement},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2015128,
author={Li, H. and Huang, D. and Morvan, J.-M. and Wang, Y. and Chen, L.},
title={Towards 3D Face Recognition in the Real: A Registration-Free Approach Using Fine-Grained Matching of 3D Keypoint Descriptors},
journal={International Journal of Computer Vision},
year={2015},
volume={113},
number={2},
pages={128-142},
doi={10.1007/s11263-014-0785-6},
note={cited By 51},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939949330&doi=10.1007%2fs11263-014-0785-6&partnerID=40&md5=e89019b8bd090159f9b7875702ccf3e6},
affiliation={School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi, 710049, China; Beijing Center for Mathematics and Information Interdisciplinary Sciences (BCMIIS), Beijing, China; Laboratory of Intelligent Recognition and Image Processing, School of Computer Science and Engineering, Beihang University, Beijing, 10091, China; Département de Mathématiques, Université Claude Bernard Lyon 1, Lyon, 69622, France; Geometric Modeling and Scientific Visualization Center, King Abdullah University of Science and Technology, Makkah, Saudi Arabia; Département de Mathématiques et Informatique, UMR CNRS 5205, Ecole Centrale Lyon, Lyon, 69134, France},
abstract={Registration algorithms performed on point clouds or range images of face scans have been successfully used for automatic 3D face recognition under expression variations, but have rarely been investigated to solve pose changes and occlusions mainly since that the basic landmarks to initialize coarse alignment are not always available. Recently, local feature-based SIFT-like matching proves competent to handle all such variations without registration. In this paper, towards 3D face recognition for real-life biometric applications, we significantly extend the SIFT-like matching framework to mesh data and propose a novel approach using fine-grained matching of 3D keypoint descriptors. First, two principal curvature-based 3D keypoint detectors are provided, which can repeatedly identify complementary locations on a face scan where local curvatures are high. Then, a robust 3D local coordinate system is built at each keypoint, which allows extraction of pose-invariant features. Three keypoint descriptors, corresponding to three surface differential quantities, are designed, and their feature-level fusion is employed to comprehensively describe local shapes of detected keypoints. Finally, we propose a multi-task sparse representation based fine-grained matching algorithm, which accounts for the average reconstruction error of probe face descriptors sparsely represented by a large dictionary of gallery descriptors in identification. Our approach is evaluated on the Bosphorus database and achieves rank-one recognition rates of 96.56, 98.82, 91.14, and 99.21 % on the entire database, and the expression, pose, and occlusion subsets, respectively. To the best of our knowledge, these are the best results reported so far on this database. Additionally, good generalization ability is also exhibited by the experiments on the FRGC v2.0 database. © 2014, Springer Science+Business Media New York.},
author_keywords={3D keypoint descriptors;  Expression, pose and occlusion;  Fine-grained matching;  Registration-free 3D face recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20151817,
author={Zhang, C. and Gu, Y.-Z. and Hu, K.-L. and Wang, Y.-G.},
title={Face recognition using SIFT features under 3D meshes},
journal={Journal of Central South University},
year={2015},
volume={22},
number={5},
pages={1817-1825},
doi={10.1007/s11771-015-2700-x},
art_number={2700},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930008100&doi=10.1007%2fs11771-015-2700-x&partnerID=40&md5=569ac008f1ef201b7fcbffd9e0faa163},
affiliation={Key Laboratory of Wireless Sensor Network & Communication, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai, 201800, China; Department of Computer Science and Engineering, Shaoxing University, Shaoxing, 312000, China},
abstract={Expression, occlusion, and pose variations are three main challenges for 3D face recognition. A novel method is presented to address 3D face recognition using scale-invariant feature transform (SIFT) features on 3D meshes. After preprocessing, shape index extrema on the 3D facial surface are selected as keypoints in the difference scale space and the unstable keypoints are removed after two screening steps. Then, a local coordinate system for each keypoint is established by principal component analysis (PCA). Next, two local geometric features are extracted around each keypoint through the local coordinate system. Additionally, the features are augmented by the symmetrization according to the approximate left-right symmetry in human face. The proposed method is evaluated on the Bosphorus, BU-3DFE, and Gavab databases, respectively. Good results are achieved on these three datasets. As a result, the proposed method proves robust to facial expression variations, partial external occlusions and large pose changes. © 2015, Central South University Press and Springer-Verlag Berlin Heidelberg.},
author_keywords={3D face recognition;  3D meshes;  expression;  large pose changes;  occlusion;  scale-invariant feature transform (SIFT)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen20159,
author={Chen, H.-Z. and Hung, P.-S.},
title={3D Point Clouds Face Recognition by BP Neural Networks},
journal={Chung Cheng Ling Hsueh Pao/Journal of Chung Cheng Institute of Technology},
year={2015},
volume={44},
number={1},
pages={9-24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959360389&partnerID=40&md5=a7ab39b6b4cfc96adbb4c77ed8a96d41},
affiliation={Institute of Civil and Hydraulic Engineering, Feng Chia University, Taiwan; Land Management, Feng Chia University, Taiwan},
abstract={In this paper, coordinates of 3D point clouds data are used directly for face recognition. 3D face point clouds database were built from neutral expressions and variant poses (left, frontal, and right). We are currently concentrating on a procedure of the coarse and fine transformation adjustment for face recognition. The transformation adjustment is based on the spatial fitted line to the point clouds data, centered at the nose tip, along longitudinal and cross-section profiles. Our 3D face point clouds data are combinations of 69 same-person pairs and 19,531 different-person pairs. We present an artificial neural networks with nose tip/root alignment geometry concept as network inputs for face recognition. The experiment presents that the success rates of same-person pairs is 100%. An appropriate amount for different-person pairs are 820 and their success rates can reach 99.3%. Compared to other related works, the framework has following highlights: (1) a coordinate transformation has to be done before face recognition alignment stage; (2) face recognition is directly completed by using point clouds without building 3D modality; (3) artificial intelligence neural networks for face recognition only with the feature points of nose tip/ root have illustrated that the proposed system is feasible.},
author_keywords={3D face point clouds database;  3D face recognition;  Artificial Intelligence neural networks},
document_type={Article},
source={Scopus},
}

@CONFERENCE{CardiaNeto201566,
author={Cardia Neto, J.B. and Marana, A.N.},
title={3DLBP and HAOG fusion for face recognition utilizing kinect as a 3D scanner},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2015},
volume={13-17-April-2015},
pages={66-73},
doi={10.1145/2695664.2695807},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955478290&doi=10.1145%2f2695664.2695807&partnerID=40&md5=2159132826aeae8225527d924694aade},
affiliation={Graduate Program in Computer Science, UNESP - São Paulo State University, Bauru, São Paulo, 17018-820, Brazil; Department of Computing, Faculty of Sciences, UNESP - São Paulo State University, Bauru, São Paulo, 17018-820, Brazil},
abstract={Pose and illumination variability are two major problems with 2D face recognition. Since 3D data is less sensible to illumination changes and can be used to adjust pose variations, it has been adopted to improve performance on face recognition systems. The main problem with utilizing 3D data is the high cost of the traditional 3D scanners. The Kinect is a low cost device that can be used to obtain the 3D data from an environment in a fast manner, but with lower accuracy than the traditional scanners. Recently, a 3D Local Binary Pattern (3DLBP) method was proposed for 3D face recognition by using high resolution scanners. The main goal of this work is to assess the performance of 3DLBP method, fused with Histogram of Averaged Oriented Gradients (HAOG) face descriptor method, for face recognition when Kinect is used as the 3D face scanner. Another goal is to compare the 3DLBP method, fused with HAOG descriptor, with other methods proposed in the literature for face recognition by using Kinect. Experimental results on EURECOM face dataset showed that the data generated by Kinect are discriminative enough to allow face recognition and that 3DLBP performs better than the other methods. Copyright is held by the owner/author(s).},
author_keywords={3D face recognition;  3DLBP;  HAOG;  Kinect},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Beumier20152291,
author={Beumier, C.},
title={Design of coded structured light pattern for 3D facial surface capture},
journal={European Signal Processing Conference},
year={2015},
volume={06-10-September-2004},
pages={2291-2294},
art_number={7079884},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979948896&partnerID=40&md5=b09f05fe749fab292462602ba77b999e},
affiliation={Signal and Image Centre, Royal Military Academy, Avenue de la Renaissance, 30, Brussels, B-1000, Belgium},
abstract={In the context of 3D face recognition, facial surfaces are advantageously captured by a structured light acquisition system, which is typically quick, low cost and uses off-the-shelve components. The light pattern projected, a key aspect of the structured light approach, makes the major difference between developed systems. In most of them, elements of the light pattern must be identified by a property such as element thickness or colour. We present in this paper the design of projected patterns that led to the realisation of three 3D acquisition prototypes. © 2004 EUSIPCO.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Elaiwat20151235,
author={Elaiwat, S. and Bennamoun, M. and Boussaid, F. and El-Sallam, A.},
title={A Curvelet-based approach for textured 3D face recognition},
journal={Pattern Recognition},
year={2015},
volume={48},
number={4},
pages={1235-1246},
doi={10.1016/j.patcog.2014.10.013},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920743471&doi=10.1016%2fj.patcog.2014.10.013&partnerID=40&md5=dde3d88e6412c83672a1a30bbf49b936},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia; School of Sport Science, Exercise and Health, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia},
abstract={In this paper, we present a fully automated multimodal Curvelet-based approach for textured 3D face recognition. The proposed approach relies on a novel multimodal keypoint detector capable of repeatably identifying keypoints on textured 3D face surfaces. Unique local surface descriptors are then constructed around each detected keypoint by integrating Curvelet elements of different orientations, resulting in highly descriptive rotation invariant features. Unlike previously reported Curvelet-based face recognition algorithms which extract global features from textured faces only, our algorithm extracts both texture and 3D local features. In addition, this is achieved across a number of frequency bands to achieve robust and accurate recognition under varying illumination conditions and facial expressions. The proposed algorithm was evaluated using three well-known and challenging datasets, namely FRGC v2, BU-3DFE and Bosphorus datasets. Reported results show superior performance compared to prior art, with 99.2%, 95.1% and 91% verification rates at 0.001 FAR for FRGC v2, BU-3DFE and Bosphorus datasets, respectively. © 2014 Elsevier Ltd. All rights reserved.},
author_keywords={Digital Curvelet transform;  Face recognition;  Keypoint detection;  Local features},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li201575,
author={Li, Y. and Wang, Y. and Wang, B. and Sui, L.},
title={Nose tip detection on three-dimensional faces using pose-invariant differential surface features},
journal={IET Computer Vision},
year={2015},
volume={9},
number={1},
pages={75-84},
doi={10.1049/iet-cvi.2014.0070},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988269675&doi=10.1049%2fiet-cvi.2014.0070&partnerID=40&md5=9b44b1684d50589e30f8aa93abe2e4b3},
affiliation={School of Computer Science and Engineering, Xi'an University of Technology, 5 South Jinhua Road, Xi'an, 710048, China},
abstract={Three-dimensional (3D) facial data offer the potential to overcome the difficulties caused by the variation of head pose and illumination in 2D face recognition. In 3D face recognition, localisation of nose tip is essential to face normalisation, face registration and pose correction etc. Most of the existing methods of nose tip detection on 3D face deal mainly with frontal or near-frontal poses or are rotation sensitive. Many of them are training-based or model-based. In this study, a novel method of nose tip detection is proposed. Using pose-invariant differential surface features - high-order and low-order curvatures, it can detect nose tip on 3D faces under various poses automatically and accurately. Moreover, it does not require training and does not depend on any particular model. Experimental results on GavabDB verify the robustness and accuracy of the proposed method. © The Institution of Engineering and Technology 2015.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Betta2015,
author={Betta, G. and Capriglione, D. and Corvino, M. and Gasparetto, M. and Liguori, C. and Paolillo, A. and Zappa, E.},
title={Metrological performance comparison of biometric system architectures for 3D face recognition},
journal={XXI IMEKO World Congress "Measurement in Research and Industry"},
year={2015},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951072890&partnerID=40&md5=f575fdc74c6576579e3881b3aeb3a78c},
affiliation={DIEI, University of Cassino and of Southern Lazio Cassino (FR), Italy; DME, Politecnico di Milano, Via La Masa, 1, Milano, Italy; DIIn, University of Salerno, Via Giovanni Paolo II, 132, Fisciano (SA), Italy},
abstract={Different biometric system architectures based on 3D images can be realized for face recognition. In such systems the triangulation of images provided by a couple of 2D cameras is employed to achieve the 3D features of a face. To setting up the system, different positioning of cameras as well as both kind and resolution of camera could be considered. These parameters can affect the correct decision rate of the system in classifying the input face, especially in presence of image uncertainty. In previous papers, the authors proposed an original approach for the estimation of the confidence level of results provided by classification systems for face recognition. Such approach is here adopted in order to compare several 3D architectures differing in camera specifications and geometrical positioning.},
author_keywords={3D image features;  Classifier;  Decision support systems;  Face recognition;  Measurement uncertainty},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liang2015406,
author={Liang, R. and Shen, W. and Li, X.-X. and Wang, H.},
title={Bayesian multi-distribution-based discriminative feature extraction for 3D face recognition},
journal={Information Sciences},
year={2015},
volume={320},
pages={406-417},
doi={10.1016/j.ins.2015.03.063},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937521792&doi=10.1016%2fj.ins.2015.03.063&partnerID=40&md5=ea6a6994deb813d32a7a1d39602f0d94},
affiliation={College of Information Engineering, Zhejiang University of TechnologyHangzhou, China},
abstract={Due to the difficulties associated with the collection of 3D samples, 3D face recognition technologies often have to work with smaller than desirable sample sizes. With the aim of enlarging the training number for each subject, we divide each training image into several patches. However, this immediately introduces two further problems for 3D models: high computational cost and dispersive features caused by the divided 3D image patches. We therefore first map 3D face images into 2D depth images, which greatly reduces the dimension of the samples. Though the depth images retain most of the robust features of 3D images, such as pose and illumination invariance, they lose many discriminative features of the original 3D samples. In this study, we propose a Bayesian learning framework to extract the discriminative features from the depth images. Specifically, we concentrate the features of the intra-class patches to a mean feature by maximizing the multivariate Gaussian likelihood function, and, simultaneously, enlarge the distances between the inter-class mean features by maximizing the exponential priori distribution of the mean features. For classification, we use the nearest neighbor classifier combined with the Mahalanobis distance to calculate the distance between the features of the test image and items in the training set. Experiments on two widely-used 3D face databases demonstrate the efficiency and accuracy of our proposed method compared to relevant state-of-the-art methods. © 2015 Elsevier Inc. All rights reserved.},
author_keywords={3D face recognition;  Bayesian learning;  Depth image;  Single training sample per person},
document_type={Article},
source={Scopus},
}

@ARTICLE{Quan2015199,
author={Quan, W. and Matuszewski, B.J. and Shark, L.-K.},
title={3-d face recognition using geodesic-map representation and statistical shape modelling},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9493},
pages={199-212},
doi={10.1007/978-3-319-27677-9_13},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955314982&doi=10.1007%2f978-3-319-27677-9_13&partnerID=40&md5=e85737d15347f5ea197969c838869be9},
affiliation={Applied Digital Signal and Image Processing (ADSIP) Research Centre, University of Central Lancashire, Preston, PR1 2HE, United Kingdom},
abstract={3-D face recognition research has received significant attention in the past two decades because of the rapid development in imaging technology and ever increasing security demand of modern society. One of its challenges is to cope with non-rigid deformation among faces, which is often caused by the changes of appearance and facial expression. Popular solutions to deal with this problem are to detect the deformable parts of the face and exclude them, or to represent a face in terms of sparse signature points, curves or patterns that are invariant to deformation. Such approaches, however, may lead to loss of information which is important for classification. In this paper, we propose a new geodesic-map representation with statistical shape modelling for handling the non-rigid deformation challenge in face recognition. The proposed representation captures all geometrical information from the entire 3-D face and provides a compact and expression-free map that preserves intrinsic geometrical information. As a result, the search for dense points correspondence in the face recognition task can be speeded up by using a simple image-based method instead of time-consuming, recursive closest distance search in 3-D space. An experimental investigation was conducted on 3-D face scans using publicly available databases and compared with the benchmark approaches. The experimental results demonstrate that the proposed scheme provides a highly competitive new solution for 3-D face recognition. © Springer International Publishing Switzerland 2015.},
author_keywords={3-D face recognition;  Geodesic-map representation;  Non-rigid deformation;  Shape modeling},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang201527,
author={Wang, H. and Mu, Z. and Zeng, H. and Huang, M.},
title={3D face recognition using local features matching on sphere depth representation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9428},
pages={27-34},
doi={10.1007/978-3-319-25417-3_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950245926&doi=10.1007%2f978-3-319-25417-3_4&partnerID=40&md5=a7ea7b117fa5089c7a6e2e77750229fd},
affiliation={School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China},
abstract={This paper proposes a 3D face recognition approach using sphere depth image, which is robust to pose variations in unconstrained environments. The input 3D face point clouds is first transformed into sphere depth images, and then represented as a 3DLBP image to enhance the distinctiveness of smooth and similar facial depth images. An improved SIFT algorithm is applied in the following matching process. The improved SIFT algorithm employs the learning to rank approach to select the keypoints with higher stability and repeatability instead of manually rule-based method used by the original SIFT algorithm. The proposed face recognition method is evaluated on CASIA 3D face database. And the experimental results show our approach has superior performance than many existing methods for 3D face recognition and handles pose variations quite well. © Springer International Publishing Switzerland 2015.},
author_keywords={3D face recognition;  Learning to rank;  Local binary patterns;  Sphere depth image},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ming2015352,
author={Ming, Y. and Jin, Y.},
title={Robust 3D local SIFT features for 3D face recognition},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9246},
pages={352-359},
doi={10.1007/978-3-319-22873-0_31},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985029352&doi=10.1007%2f978-3-319-22873-0_31&partnerID=40&md5=4448fcb45dc3bae2a2c99810502b1ce7},
affiliation={School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, 100044, China},
abstract={In this paper, a robust 3D local SIFT feature is proposed for 3D face recognition. For preprocessing the original 3D face data, facial regional segmentation is first employed by fusing curvature characteristics and shape band mechanism. Then, we design a new local descriptor for the extracted regions, called 3D local Scale-Invariant Feature Transform (3D LSIFT). The key point detection based on 3D LSIFT can effectively reflect the geometric characteristic of 3D facial surface by encoding the gray and depth information captured by 3D face data. Then, 3D LSIFT descriptor extends to describe the discrimination on 3D faces. Experimental results based on the common international 3D face databases demonstrate the higher-qualified performance of our proposed algorithm with effectiveness, robustness, and universality. © Springer International Publishing Switzerland 2015.},
author_keywords={3D face recognition;  3D local Scale-Invariant feature transform;  Depth information;  Facial region segmentation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ming201514,
author={Ming, Y.},
title={Robust regional bounding spherical descriptor for 3D face recognition and emotion analysis},
journal={Image and Vision Computing},
year={2015},
volume={35},
pages={14-22},
doi={10.1016/j.imavis.2014.12.003},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921669860&doi=10.1016%2fj.imavis.2014.12.003&partnerID=40&md5=fe785e43fe879b32d41aadd4abc61953},
affiliation={School of Electronic Engineering, Beijing Key Laboratory of Work Safety Intelligent Monitoring, Beijing University of Posts and Telecommunications, Beijing, 100876, China},
abstract={3D face recognition and emotion analysis play important roles in many fields of communication and edutainment. An effective facial descriptor, with higher discriminating capability for face recognition and higher descriptiveness for facial emotion analysis, is a challenging issue. However, in the practical applications, the descriptiveness and discrimination are independent and contradictory to each other. 3D facial data provide a promising way to balance these two aspects. In this paper, a robust regional bounding spherical descriptor (RBSR) is proposed to facilitate 3D face recognition and emotion analysis. In our framework, we first segment a group of regions on each 3D facial point cloud by shape index and spherical bands on the human face. Then the corresponding facial areas are projected to regional bounding spheres to obtain our regional descriptor. Finally, a regional and global regression mapping (RGRM) technique is employed to the weighted regional descriptor for boosting the classification accuracy. Three largest available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the performance comparison and the experimental results show a consistently better performance for 3D face recognition and emotion analysis. © 2015 Elsevier B.V. All rights reserved.},
author_keywords={3D face recognition;  Emotion analysis;  Kullback-Leiber divergence (KLD);  Regional and global regression;  Regional bounding spherical descriptor},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Echeagaray-Patrón2015,
author={Echeagaray-Patrón, B.A. and Kober, V.},
title={3D face recognition based on matching of facial surfaces},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9598},
doi={10.1117/12.2186695},
art_number={95980V},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951325808&doi=10.1117%2f12.2186695&partnerID=40&md5=cb5a757e08c6a66683fc0706f09faf95},
affiliation={Department of Computer Science, CICESE, Ensenada, B.C.22860, Mexico; Department of Mathematics, Chelyabinsk State University, Russian Federation},
abstract={Face recognition is an important task in pattern recognition and computer vision. In this work a method for 3D face recognition in the presence of facial expression and poses variations is proposed. The method uses 3D shape data without color or texture information. A new matching algorithm based on conformal mapping of original facial surfaces onto a Riemannian manifold followed by comparison of conformal and isometric invariants computed in the manifold is suggested. Experimental results are presented using common 3D face databases that contain significant amount of expression and pose variations. © 2015 SPIE.},
author_keywords={3D face recognition;  3D facial shape analysis;  Conformal mapping},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo2015,
author={Guo, Z. and Liu, S. and Wang, Y. and Lei, T.},
title={Learning deformation model for expression-robust 3D face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9817},
doi={10.1117/12.2228002},
art_number={98170O},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028295582&doi=10.1117%2f12.2228002&partnerID=40&md5=42fcb3ae1a1a8506c08003ac3fd73bc1},
affiliation={School of Electronics and Information, Northwestern Polytechnical University, Xi'an, 710072, China; School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, 730070, China},
abstract={Expression change is the major cause of local plastic deformation of the facial surface. The intra-class differences with large expression change somehow are larger than the inter-class differences as it's difficult to distinguish the same individual with facial expression change. In this paper, an expression-robust 3D face recognition method is proposed by learning expression deformation model. The expression of the individuals on the training set is modeled by principal component analysis, the main components are retained to construct the facial deformation model. For the test 3D face, the shape difference between the test and the neutral face in training set is used for reconstructing the expression change by the constructed deformation model. The reconstruction residual error is used for face recognition. The average recognition rate on GavabDB and self-built database reaches 85.1% and 83%, respectively, which shows strong robustness for expression changes. © 2015 SPIE.},
author_keywords={3D face recognition;  Facial deformation model;  Principal component analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Luo2015,
author={Luo, J. and Geng, S.Z. and Xiao, Z.X. and Xiu, C.B.},
title={A review of recent advances in 3d face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9443},
doi={10.1117/12.2178750},
art_number={944303},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925431201&doi=10.1117%2f12.2178750&partnerID=40&md5=f8e793b50759be140f6e5bf45667a121},
affiliation={Key Laboratory of Advanced of Electrical Engineering and Energy Technology, Tianjin Polytechnic University, Tianjin, 300387, China; College of Electrical Engineering and Automation, Tianjin Polytechnic University, Tianjin, 300387, China},
abstract={Face recognition based on machine vision has achieved great advances and been widely used in the various fields. However, there are some challenges on the face recognition, such as facial pose, variations in illumination, and facial expression. So, this paper gives the recent advances in 3D face recognition. 3D face recognition approaches are categorized into four groups: minutiae approach, space transform approach, geometric features approach, model approach. Several typical approaches are compared in detail, including feature extraction, recognition algorithm, and the performance of the algorithm. Finally, this paper summarized the challenge existing in 3D face recognition and the future trend. This paper aims to help the researches majoring on face recognition. © 2015 SPIE.},
author_keywords={3D face recognition;  geometric features;  minutiae approach.;  space transform},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Belghini2015317,
author={Belghini, N. and Ezghari, S. and Zahi, A.},
title={3D face recognition using facial curves, sparse random projection and fuzzy similarity measure},
journal={Colloquium in Information Science and Technology, CIST},
year={2015},
volume={2015-January},
number={January},
pages={317-322},
doi={10.1109/CIST.2014.7016639},
art_number={7016639},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938075627&doi=10.1109%2fCIST.2014.7016639&partnerID=40&md5=bca04cb8b40640821dbbc277a0cf6e71},
affiliation={System Intelligent and Application Laboratory (SIA) FST, Fez, Morocco},
abstract={In this paper, we propose a fuzzy similarity based classification approach for 3D face recognition. In the feature extraction method, we exploit curve concept to represent the 3D facial data, two types of curves was considered: depth-level and depth-radial curves. As the dimension of the obtained features is high, the problem 'curse of dimensionality' appears. To solve this problem, the Random Projection (RP) method was used. The proposed classifier performs Fuzzification operation using triangular membership functions for input data and ordered weighted averaging operators to measure similarity. Experiment was conducted using vrml files from 3D Database considering only one training sample per person. The obtained results are very promising for depth-level and depth-radial curves, besides the recognition rates are higher than 98%. © 2014 IEEE.},
author_keywords={3D face recognition;  facial curves;  fuzzy logic;  OWA operator;  similarity measure;  sparse random projection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ratyal2015241,
author={Ratyal, N.I. and Taj, I.A. and Bajwa, U.I. and Sajid, M.},
title={3D face recognition based on pose and expression invariant alignment},
journal={Computers and Electrical Engineering},
year={2015},
volume={46},
pages={241-255},
doi={10.1016/j.compeleceng.2015.06.007},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931030581&doi=10.1016%2fj.compeleceng.2015.06.007&partnerID=40&md5=a95f41db3d492d423152cf1d2907956a},
affiliation={Vision and Pattern Recognition Systems Research Group, Mohammad Ali Jinnah University, Islamabad, Pakistan; Department of Computer Science, COMSATS Institute of Information Technology, Lahore, Pakistan},
abstract={In this paper we present a novel pose and expression invariant approach for 3D face registration based on intrinsic coordinate system characterized by nose tip, horizontal nose plane and vertical symmetry plane of the face. It is observed that distance of nose tip from 3D scanner is reduced after pose correction which is presented as a quantifying heuristic for proposed registration scheme. In addition, motivated by the fact that a single classifier cannot be generally efficient against all face regions, a two tier ensemble classifier based 3D face recognition approach is presented which employs Principal Component Analysis (PCA) for feature extraction and Mahalanobis Cosine (MahCos) matching score for classification of facial regions with weighted Borda Count (WBC) based combination and a re-ranking stage. The performance of proposed approach is corroborated by extensive experiments performed on two databases: GavabDB and FRGC v2.0, confirming effectiveness of fusion strategies to improve performance. © 2015 Elsevier Ltd. All rights reserved.},
author_keywords={3D face recognition;  3D registration;  Ensemble classifier;  Fusion;  Intrinsic coordinate system},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wang2015192,
author={Wang, X. and Ruan, Q. and Jin, Y. and An, G.},
title={3D face recognition using closest point coordinates and spherical vector norms},
journal={IET Conference Publications},
year={2015},
volume={2015},
number={CP681},
pages={192-196},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983122121&partnerID=40&md5=4a26023e54f7fb05eaa6d68668c88d60},
affiliation={Institution of Information Science, Beijing Jiaotong University, Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, 100044, China},
abstract={In this paper, we introduce a new feature named spherical vector norms for 3D face recognition. The proposed feature is efficient, insensitive to facial expression and contains discriminatory information of 3D face. The feature extraction method is firstly finding a set of the points with the closest distance to the standard face, denoted as closest point coordinates, and then extracting the spherical vector norms of these points. This paper combines point coordinates and spherical vector norms for improving recognition. Finally this approach is finished by Linear Discriminant Analysis (LDA) and Nearest Neighbor classifier. We have performed different experiments on the Face Recognition Grand Challenge database. It achieves the verification rate of 97.11% on All vs. All experiment at 0.1% FAR and 96.64% verification rate on Neutral vs. Expression experiment.},
author_keywords={3D face recognition;  Linear discriminant analysis;  Spherical vector norms},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tang2015466,
author={Tang, Y. and Sun, X. and Huang, D. and Morvan, J.-M. and Wang, Y. and Chen, L.},
title={3D face recognition with asymptotic cones based principal curvatures},
journal={Proceedings of 2015 International Conference on Biometrics, ICB 2015},
year={2015},
pages={466-472},
doi={10.1109/ICB.2015.7139111},
art_number={7139111},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943279293&doi=10.1109%2fICB.2015.7139111&partnerID=40&md5=bd52f57fa0f04d14b00dab8a5d4e64e2},
affiliation={Université de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, Lyon, 69134, France; King Abdullah University of Science and Technology, V.C.C. Research Center, Thuwal, 23955-6900, Saudi Arabia; IRIP, School of Computer Science and Engineering, Beihang Universtiy, Beijing, 100191, China; Université de Lyon, CNRS, Université Claude Bernard Lyon 1, ICJ UMR 5208, Villeurbanne, F-69622, France},
abstract={The classical curvatures of smooth surfaces (Gaussian, mean and principal curvatures) have been widely used in 3D face recognition (FR). However, facial surfaces resulting from 3D sensors are discrete meshes. In this paper, we present a general framework and define three principal curvatures on discrete surfaces for the purpose of 3D FR. These principal curvatures are derived from the construction of asymptotic cones associated to any Borel subset of the discrete surface. They describe the local geometry of the underlying mesh. First two of them correspond to the classical principal curvatures in the smooth case. We isolate the third principal curvature that carries out meaningful geometric shape information. The three principal curvatures in different Borel subsets scales give multi-scale local facial surface descriptors. We combine the proposed principal curvatures with the LNP-based facial descriptor and SRC for recognition. The identification and verification experiments demonstrate the practicability and accuracy of the third principal curvature and the fusion of multi-scale Borel subset descriptors on 3D face from FRGC v2.0. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hafez2015373,
author={Hafez, S.F. and Selim, M.M. and Zayed, H.H.},
title={3D face recognition based on normal map features using selected Gabor filters and linear discriminant analysis},
journal={International Journal of Biometrics},
year={2015},
volume={7},
number={4},
pages={373-390},
doi={10.1504/IJBM.2015.076138},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969234799&doi=10.1504%2fIJBM.2015.076138&partnerID=40&md5=4c091fd618afea54f7840fe4998e06bf},
affiliation={Faculty of Engineering, Benha University, Benha, Egypt; Department of Computer Science, Faculty of Computers and Informatics, Benha University, Benha, Egypt},
abstract={In this paper, we present a new approach to enhance and improve the performance of automatic 3D face recognition system. The proposed method has been implemented through a preprocessing technique to align and normalise all images in the database based on eyes centres localisation using 2D normalised cross-correlation (2DNCC). Preprocessing 3D face data has been implemented using depth map representation of the 3D data. The detected eyes centres and eyes distance (ED) have been used to segment and align 3D face images to produce a cropped face region of interest (ROI). The proposed approach extracted 3D face features using a set of selected orthogonal Gabor filters applied to normal map representation of the 3D face model. This approach minimises the feature vector extracted compared to systems that use complete Gabor filters bank. A further compression to the extracted features has been accomplished using linear discriminant analysis (LDA) before the classification stage. Experimental results show that the proposed system is effective for both dimensionality reduction and good recognition performance when compared to current systems. The system has been tested using CASIA and Gavab 3D face images databases and achieved 98.35% and 85% recognition rates, respectively. © 2015 Inderscience Enterprises Ltd.},
author_keywords={Depth maps;  Face recognition;  LDA;  Linear discriminant analysis;  Normal maps;  Preprocessing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zou2015898,
author={Zou, H. and Da, F. and Wang, Z.},
title={A novel 3D face feature based on geometry image vertical shape information},
journal={Optik},
year={2015},
volume={126},
number={9-10},
pages={898-902},
doi={10.1016/j.ijleo.2015.02.083},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928986749&doi=10.1016%2fj.ijleo.2015.02.083&partnerID=40&md5=2eba158b312d6844869e98302c0480fa},
affiliation={School of Automation, Southeast University, Nanjing, Jiangsu, 210096, China; School of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, 210037, China},
abstract={A novel and efficient face feature is proposed in this paper. 3D faces from the database are preprocessed and mapped to 2D geometry images. Then the geometry images are decomposed into wavelet responses by multi-scale Gabor transforms. According to analyses and experiments, responses that represent vertical shape information are figured out to be face feature for recognition. Moreover, the feature extracted by multi-scale Haar transforms also obtains satisfying experiment results, which prove that the feature is free from the extraction methods. Extensive experiments conducted on FRGC(Face Recognition Grand Challenge) v2.0 show a satisfactory performance compared with existing popular methods. It is also approved that the vertical shape information is promising for dealing with face expressions in 3D face recognition. © 2015 Elsevier GmbH. All rights reserved.},
author_keywords={3D face feature;  Geometry image;  Vertical shape information},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20153357,
author={Zhang, C. and Gu, Y. and Wang, Y. and Li, F. and Zhan, Y. and Pi, J. and Qu, L.},
title={Adaptive multiple regions matching for 3D face recognition under expression and pose variations},
journal={Journal of Computational Information Systems},
year={2015},
volume={11},
number={9},
pages={3357-3369},
doi={10.12733/jcis14297},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938320850&doi=10.12733%2fjcis14297&partnerID=40&md5=af76d7b96bb6eca2db8bc3e1babc780a},
affiliation={Key Laboratory of Wireless Sensor Network & Communication, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai, 200050, China; University of Chinese Academy of Sciences, Beijing, 100049, China; Shanghai Internet of Things Co., Ltd., Shanghai, 201800, China},
abstract={Expression and pose variations are two major challenges for 3D face recognition. This paper presents a method to cope with these two challenges by fusing the matching results of adaptive multiple regions on the 3D face. First, one approach is proposed for pose correction of 3D face based on three landmark points: nose tip, nasion, and subnasale. Then multiple regions are adaptively chosen from the facial surface, which include nose, left and right eye-forehead regions, left and right cheeks, and mouth-chin region. Next, a least trimmed square Hausdorff distance method is applied for region matching. Moreover, to obtain a better overall performance, several score-level and rank-level fusion schemes are used to fuse the contribution of each region. The proposed approach is evaluated on the Bosphorus and the BU-3DFE databases, and yields good results. The study shows that the proposed algorithm is robust to expression and pose changes. ©, 2015, Binary Information Press. All right reserved.},
author_keywords={3D face recognition;  Expression;  Pose correction;  Pose rotation;  Region matching},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Xue2015,
author={Xue, J. and Su, X. and Zhang, Q.},
title={High-speed 3D face measurement based on color speckle projection},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9302},
doi={10.1117/12.2076458},
art_number={93022Y},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924939644&doi=10.1117%2f12.2076458&partnerID=40&md5=6eb59f4ca5a72e8c2457421e42cda527},
affiliation={School of Aeronautics and Astronautics, Sichuan University, Chengdu, 610065, China; Department of Opto-Electronics Science and Technology, Sichuan University, Chengdu, 610065, China},
abstract={Nowadays, 3D face recognition has become a subject of considerable interest in the security field due to its unique advantages in domestic and international. However, acquiring color-textured 3D faces data in a fast and accurate manner is still highly challenging. In this paper, a new approach based on color speckle projection for 3D face data dynamic acquisition is proposed. Firstly, the projector-camera color crosstalk matrix that indicates how much each projector channel influences each camera channel is measured. Secondly, the reference-speckle-sets images are acquired with CCD, and then three gray sets are separated from the color sets using the crosstalk matrix and are saved. Finally, the color speckle image which is modulated by face is captured, and it is split three gray channels. We measure the 3D face using multi-sets of speckle correlation methods with color speckle image in high-speed similar as one-shot, which greatly improves the measurement accuracy and stability. The suggested approach has been implemented and the results are supported by experiments. © 2015 SPIE.},
author_keywords={3D face measurement;  Color crosstalk;  Color speckle projection;  High-speed measurement},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chouchane2015,
author={Chouchane, A. and Belahcene, M. and Ouamane, A. and Bourennane, S.},
title={3D face recognition based on histograms of local descriptors},
journal={2014 4th International Conference on Image Processing Theory, Tools and Applications, IPTA 2014},
year={2015},
doi={10.1109/IPTA.2014.7001925},
art_number={7001925},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921721830&doi=10.1109%2fIPTA.2014.7001925&partnerID=40&md5=4bbf52130cd2484d05cce4611c376687},
affiliation={LMSE, University of Biskra, Biskra, R.P.07000, Algeria; Centre de Développement des Technologies Avancées, ASM Alger, Algeria; Institut Fresnel, UMR CNRS 7249, Ecole Centrale Marseille, France},
abstract={Face recognition in an uncontrolled condition such as illumination and expression variations is a challenging task. Local descriptor is one of the most efficient methods used to deal with these problems. In this paper, we present an automatic 3D face recognition approach based on three local descriptors, local phase quantization (LPQ), Three-Patch Local Binary Patterns (TPLBP) and Four-Patch Local Binary Patterns (TPLBP). Facial images are passing through one of the three descriptors and divided into sub-regions or rectangular blocks. The histogram of each sub-region is extracted and concatenated into a single feature vector. PCA (Principal Component Analysis) and EFM (Enhanced Fisher linear discriminant Model) are used to reduce the dimensionality of the resulting feature vectors. Finally, these vectors are sent to the classification step, when we use two methods; SVM (Support Victor Machine) and similarity measures. CASIA 3D face database is introduced to experimental evaluation. The experimental results illustrate a high recognition performance of the proposed approach. © 2014 IEEE.},
author_keywords={3D face recognition;  FPLBP;  Local phase quantization;  Locale descriptors;  Support vector machines;  TPLBP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chouchane201550,
author={Chouchane, A. and Belahcene, M. and Bourennane, S.},
title={3D and 2D face recognition using integral projection curves based depth and intensity images},
journal={International Journal of Intelligent Systems Technologies and Applications},
year={2015},
volume={14},
number={1},
pages={50-69},
doi={10.1504/IJISTA.2015.072219},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964744543&doi=10.1504%2fIJISTA.2015.072219&partnerID=40&md5=e5008a255876319ef73de7c015360bc8},
affiliation={Faculty of Science and Technology, Department of Electrical Engineering, University of Mohamed Khider, Biskra, BP 145 RP, Biskra, 07000, Algeria; Institut Fresnel, UMR CNRS 7249, Ecole Centrale Marseille, France},
abstract={This paper presents an automatic face recognition system in the presence of illumination, expressions and pose variations based on depth and intensity information. At first, the registration of 3D faces is achieved using iterative closest point (ICP). Nose tip point must be located using Maximum Intensity Method. This point usually has the largest depth value; however there is a problem with some unnecessary data such as: shoulders, hair, neck and parts of clothes; to cope with this issue, we propose the integral projection curves (IPC)-based facial area segmentation to extract the facial area. After that, the combined method principal component analysis (PCA) with enhanced fisher model (EFM) is used to obtain the feature matrix vectors. Finally, the classification is performed using distance measurement and support vector machine (SVM). The experiments are implemented on two face databases CASIA3D and GavabDB; our results show that the proposed method achieves a high recognition performance. Copyright © 2015 Inderscience Enterprises Ltd.},
author_keywords={2D and 3D face recognition;  EFM;  Enhanced fisher model;  IPC-based facial area segmentation;  Nose tip;  PCA;  Principal component analysis;  Support vector machine;  SVM},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lagorio2015,
author={Lagorio, A. and Cadoni, M. and Grosso, E. and Tistarelli, M.},
title={A 3D algorithm for unsupervised face identification},
journal={3rd International Workshop on Biometrics and Forensics, IWBF 2015},
year={2015},
doi={10.1109/IWBF.2015.7110239},
art_number={7110239},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936101701&doi=10.1109%2fIWBF.2015.7110239&partnerID=40&md5=65b942c6e6bd6ec99d853a5a632cd56f},
affiliation={VisionLab - Computer Vision Laboratory, United Kingdom},
abstract={With the increasing availability of low-cost 3D data acquisition devices, the use of 3D face data for the recognition of individuals is becoming more appealing and computationally feasible. This paper proposes a completely automatic algorithm for face registration and matching. The algorithm is based on the extraction of stable 3D facial features characterizing the face and the subsequent construction of a signature manifold. The facial features are extracted by performing a continuous-to-discrete scale-space analysis. Registration is driven from the matching of triplets of feature points and the registration error is computed as shape matching score. Conversely to most techniques in the literature, a major advantage of the proposed method is that no data pre-processing is required. Therefore all presented results have been obtained exclusively from the raw data available from the 3D acquisition device. The method has been tested on the Bosphorus 3D face database and the performances compared to the ICP baseline algorithm. Even in presence of noise in the data, the algorithm proved to be very robust and reported identification performances which are aligned to the current state of the art, but without requiring any pre-processing of the raw data. © 2015 IEEE.},
author_keywords={3D Face recognition;  Face recognition},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu20152759,
author={Liu, X. and Yang, G. and Sun, X.},
title={A new strategy for 3D face recognition},
journal={Journal of Information and Computational Science},
year={2015},
volume={12},
number={7},
pages={2759-2768},
doi={10.12733/jics20105863},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929754542&doi=10.12733%2fjics20105863&partnerID=40&md5=2f790a80a714665fcf2c18c84c9d3346},
affiliation={Department of Computer, Shandong Institute of Business and Technology, Yantai, 264005, China; Engineering Training Center, Yantai University, Yantai, 264005, China},
abstract={This paper discusses the improved method for 3D face recognition. The novel method is introduced in detail. By the Thin Plate Spline (TPS) transformation, the negative effects caused by expressions and postures are weakened. In order to reduce the time for registration, the template face is used for the process of registration. After a series of registration process, the point set distance is calculated. The face that with the smallest point set distance to the query face is selected as the final recognition result. The method for the temp template face acquiring and the method for registration are improved. To reserve the original face feature to a large extent, the temp template face is selected from the original face set, and some registration is directly processed between the template face and original face set. Experiment results show that the new method of this paper has better identification rate and higher operating efficiency. Copyright © 2015 Binary Information Press.},
author_keywords={Face recognition;  Improved;  Recognition;  Registration;  Template face},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wu2015,
author={Wu, Y. and Cheng, Y. and Yang, N.},
title={3D face recognition algorithm of alignment and fitting},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9631},
doi={10.1117/12.2197158},
art_number={96311A},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946042352&doi=10.1117%2f12.2197158&partnerID=40&md5=a23ce1e3c95831e97dd1ba3873a690aa},
affiliation={Department of Command and Automation, Dalian Air Force Communication NCO Academy, Dalian Liaoning, 116100, China},
abstract={For 3D face recognition area, Design a algorithm follows a morphable model approach. To align the scan with a model, ICP and spin-images are used also referred to as registration. Deformation is done by a nonrigid ICP algorithm to fit the model with the scan. From the fitted model a geometry image and a normal image is generated. The developed algorithm is tested by several measurements. From the results of these measurements, it could be concluded that the algorithm is robust and reliable. © 2015 SPIE.},
author_keywords={Eigenfaces;  Face recognition;  Spin-image;  Transformation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Said2015,
author={Said, S. and Jemai, O. and Zaied, M. and Ben Amar, C.},
title={3D fast wavelet network model-assisted 3D face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9875},
doi={10.1117/12.2228368},
art_number={98750E},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958206236&doi=10.1117%2f12.2228368&partnerID=40&md5=e95e90acc2067f0a5a38e384b37d87d2},
affiliation={REsearch Groups in Intelligent Machines (REGIM-Lab), University of Sfax, National Engineering School of Sfax, BP 1173, Sfax, 3038, Tunisia},
abstract={In last years, the emergence of 3D shape in face recognition is due to its robustness to pose and illumination changes. These attractive benefits are not all the challenges to achieve satisfactory recognition rate. Other challenges such as facial expressions and computing time of matching algorithms remain to be explored. In this context, we propose our 3D face recognition approach using 3D wavelet networks. Our approach contains two stages: learning stage and recognition stage. For the training we propose a novel algorithm based on 3D fast wavelet transform. From 3D coordinates of the face (x,y,z), we proceed to voxelization to get a 3D volume which will be decomposed by 3D fast wavelet transform and modeled after that with a wavelet network, then their associated weights are considered as vector features to represent each training face. For the recognition stage, an unknown identity face is projected on all the training WN to obtain a new vector features after every projection. A similarity score is computed between the old and the obtained vector features. To show the efficiency of our approach, experimental results were performed on all the FRGC v.2 benchmark. © 2015 SPIE.},
author_keywords={3D face recognition;  Fast wavelet transform;  Wavelet network},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Soltanpour2015,
author={Soltanpour, S. and Wu, Q.M.J. and Anvaripour, M.},
title={Multimodal 2D-3D face recognition using structural context and pyramidal shape index},
journal={IET Seminar Digest},
year={2015},
volume={2015},
number={5},
doi={10.1049/ic.2015.0100},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946097401&doi=10.1049%2fic.2015.0100&partnerID=40&md5=a74cf5dc0ae25ab330b63f02f8787fde},
affiliation={Department of Electrical and Computer Engineering, University of Windsor, Canada},
abstract={Combination of 2D and 3D face recognition approaches intensifies recognition accuracy. In this paper, we propose a new algorithm for face recognition by applying hybrid approach, structural context and pyramidal shape index. Proposed pyramidal local shape index descriptors are extracted in each level or scale of the Gaussian pyramid of range image. In this way, we can extract high contrast and reliable 3D face features. We extract Scale Invariant Feature Transform on pyramidal shape index image and histogram of structural context is used to find matched key points. A local descriptor structural context represents the structure of the image using SIFT. Structural context histogram is applied in both texture and range images to find SIFT matched points as 2D and 3D matching score respectively. Score level fusion using sum rule is applied to get final matching score. Experimental results on Face Recognition Grand Challenge (FRGC v2.0) database illustrate detection rate 98.8% and 98.5% at 0.1% false acceptance rate for All vs. All and ROC III experiments respectively. Comparing to the state of the art, these are the best results.},
author_keywords={3D face recognition;  Gaussian pyramid;  Shape index;  SIFT;  Structural context},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Quy2015117,
author={Quy, N.H. and Quoc, N.H. and Anh, N.T.L. and Yang, H.-J. and Bao, P.T.},
title={3D human face recognition using sift descriptors of face’s feature regions},
journal={Studies in Computational Intelligence},
year={2015},
volume={572},
pages={117-126},
doi={10.1007/978-3-319-10774-5_11},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921639289&doi=10.1007%2f978-3-319-10774-5_11&partnerID=40&md5=d996f0febf2256df0de13d2ef642f8a2},
affiliation={University of Science, Ho Chi Minh City, Viet Nam; Department of Computer Engineering, Chonnam National University, South Korea},
abstract={Many researches in 3D face recognition problem have been studied because of adverse effects of human’s age, emotions, and environmental conditions on 2D models. In this paper, we propose a novel method for recognizing 3D faces. First, a 3D human face is normalized and determined regions of interest (ROI). Second, SIFT algorithm is applied to these ROIs for detecting invariant feature points. Finally, this descriptor, extracted from a training image, will be stored and later used to identify the face in a test image. For performing reliable recognition, we also adjust parameters of SIFT algorithm to fit own characteristics of the template database. In our experiments, the proposed method produces promising performance up to 84.6% of accuracy when using 3D Notre Dame biometric data-TEC. © Springer International Publishing Switzerland 2015.},
author_keywords={3D face recognition;  Range images;  SIFT descriptors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2015151,
author={Liu, S. and Mu, Z. and Huang, H.},
title={3D face recognition fusing spherical depth map and spherical texture map},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9428},
pages={151-159},
doi={10.1007/978-3-319-25417-3_19},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950277767&doi=10.1007%2f978-3-319-25417-3_19&partnerID=40&md5=828cf646ebe1b2dd0a70ea3a6fdb0939},
affiliation={School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Computing Center, Beijing Information Science and Technology University, Beijing, 100192, China},
abstract={Face recognition in unconstrained environments is often influenced by pose variations. And the problem is basically the identification that uses partial data. In this paper, a method fusing structure and texture information is proposed to solve the problem. In the register phase, the approximate 180 degree information of face is acquired, and the data used to identify individual is obtained from a random single view. Pure face is extracted from 3D data first, then convert the original data to the form of spherical depth map (SDM) and spherical texture map (STM), which are invariant to out-plane rotation, subsequently facilitating the successive alignment-free identification that is robust to pose variations. We make identification through sparse representation for its well performance with the two maps. Experiments show that our proposed method gets a high recognition rate with pose and expression variations. © Springer International Publishing Switzerland 2015.},
author_keywords={Face recognition;  Sparse representation;  Spherical Depth Map;  Spherical Texture Map},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schimbinschi2015180,
author={Schimbinschi, F. and Schomaker, L. and Wiering, M.},
title={Ensemble methods for robust 3D face recognition using commodity depth sensors},
journal={Proceedings - 2015 IEEE Symposium Series on Computational Intelligence, SSCI 2015},
year={2015},
pages={180-187},
doi={10.1109/SSCI.2015.36},
art_number={7376609},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964959816&doi=10.1109%2fSSCI.2015.36&partnerID=40&md5=fd84168d29ae9b46a6cbb844ab04e5cf},
affiliation={Department of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia; Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, Groningen, Netherlands},
abstract={In this paper we introduce a new dataset and pose invariant sampling method and describe the ensemble methods used for recognizing faces in 3D scenes, captured using commodity depth sensors. We use the 3D SIFT key point detector to take advantage of the similarities between faces, which leads to a set of points of interest based on the curvature of the face. For all key points, features are extracted using a 3D feature descriptor. Then, a variable-sized amount of features are generated per each 3D face image. The first ensemble method we constructed uses a K-nearest neighbors classifier to classify each key point-sampled feature vector as belonging to one of the subjects recorded in our dataset. All votes over all key points are combined. In the second ensemble technique, the key points are clustered with K-means, using the feature vectors and approximated sampling positions relative to the face. This leads to a set of experts that specialize for a specific region. Then a K-nearest neighbors classifier is trained on the examples falling in each expert's specialized region. Finally, for a new 3D face image, votes from all experts are combined in a sum ensemble technique to categorize the 3D face. We also introduce 6 new "real world" datasets with different variances: 3 types of 3D rotations, distance to sensor, expressions, and an all-in-one dataset. The results show very high cross validation accuracies for the same type of variance. In addition, 36 variance specific pair-Tests in which the system is trained on one dataset and tested on a completely different dataset also show encouraging results. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Svoboda2015452,
author={Svoboda, J. and Bronstein, M.M. and Drahansky, M.},
title={Contactless biometric hand geometry recognition using a low-cost 3D camera},
journal={Proceedings of 2015 International Conference on Biometrics, ICB 2015},
year={2015},
pages={452-457},
doi={10.1109/ICB.2015.7139109},
art_number={7139109},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943329089&doi=10.1109%2fICB.2015.7139109&partnerID=40&md5=088b980bbb44eef1cfc61d3c229e1b06},
affiliation={Faculty of Informatics, Universita della Svizzera Italiana, Lugano, Switzerland; Faculty of Information Technology, Brno University of Technology, Brno, Czech Republic},
abstract={In the past decade, the interest in using 3D data for biometric person authentication has increased significantly, propelled by the availability of affordable 3D sensors. The adoption of 3D features has been especially successful in face recognition applications, leading to several commercial 3D face recognition products. In other biometric modalities such as hand recognition, several studies have shown the potential advantage of using 3D geometric information, however, no commercial-grade systems are currently available. In this paper, we present a contactless 3D hand recognition system based on the novel Intel RealSense camera, the first mass-produced embeddable 3D sensor. The small form factor and low cost make this sensor especially appealing for commercial biometric applications, however, they come at the price of lower resolution compared to more expensive 3D scanners used in previous research. We analyze the robustness of several existing 2D and 3D features that can be extracted from the images captured by the RealSense camera and study the use of metric learning for their fusion. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vijayalakshmi20151,
author={Vijayalakshmi, G.V. and Joseph Raj, A.N. and Ashok Varma, S.V.S.K.},
title={Optimum selection of features for 2D (color) and 3D (depth) face recognition using modified PCA (2D)},
journal={2014 IEEE International Conference on "Smart Structures and Systems", ICSSS 2014},
year={2015},
pages={1-7},
doi={10.1109/ICSSS.2014.7006175},
art_number={7006175},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922421135&doi=10.1109%2fICSSS.2014.7006175&partnerID=40&md5=1ebe3cb224d86125718e03677dc69b96},
affiliation={SENSE, VIT University, Vellore, India},
abstract={The paper proposes a Modified Principal Component Analysis coined as 2DPCA to compare 2D and 3D face recognition. In 2DPCA a covariance matrix of image is obtained directly from the original image and is used to find the eigenvectors for image feature extraction. Here the Texas 3D +AFs-1+AF0- face recognition database was considered, which has 1149 pairs of high resolution, preprocessed and pose normalized color and range images. These images are pixel-to-pixel registered and of resolution of 751×501 pixels. The experiment performed using the images reconstructed from feature vectors demonstrated that depth information was beneficial in representing and recognizing the face with least number of principal components. © 2014 IEEE.},
author_keywords={2D(RGB) image;  2DPCA;  3D (Depth) image;  eigen faces;  eigen vectors;  face recognition;  feature extraction;  Principal Component Analysis (PCA)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tian2015499,
author={Tian, L. and Fan, C. and Ming, Y. and Shi, J.},
title={SRDANet: An efficient deep learning algorithm for face analysis},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9244},
pages={499-510},
doi={10.1007/978-3-319-22879-2_46},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985006693&doi=10.1007%2f978-3-319-22879-2_46&partnerID=40&md5=677312dc329735134faf0a8467c2ea50},
affiliation={Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China},
abstract={In this work, we take advantage of the superiority of Spectral Graph Theory in classification application and propose a novel deep learning framework for face analysis which is called Spectral Regression Discriminant Analysis Network (SRDANet). Our SRDANet model shares the same basic architecture of Convolutional Neural Network (CNN),which comprises three basic components: convolutional filter layer, nonlinear processing layer and feature pooling layer. While it is different from traditional deep learning network that in our convolutional layer, we extract the leading eigenvectors from patches in facial image which are used as filter kernels instead of randomly initializing kernels and update them by stochastic gradient descent (SGD). And the output of all cascaded convolutional filter layers is used as the input of nonlinear processing layer. In the following nonlinear processing layer, we use hashing method for nonlinear processing. In feature pooling layer, the block-based histograms are employed to pooling output features instead of max-pooling technique. At last, the output of feature pooling layer is considered as one final feature output of our model. Different from the previous single-task research for face analysis, our proposed approach demonstrates an excellent performance in face recognition and expression recognition with 2D/3D facial images simultaneously. Extensive experiments conducted on many different face analysis databases demonstrate the efficiency of our proposed SRDANet model. Databases such as Extended Yale B, PIE, ORL are used for 2D face recognition, FRGC v2 is used for 3D face recognition and BU-3DFE is used for 3D expression recognition. © Springer International Publishing Switzerland 2015.},
author_keywords={Deep learning;  Expression recognition;  Face recognition;  Spectral regression discriminant analysis;  SRDA network},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{MatíasdiMartino2015176,
author={Matías di Martino, J. and Fernández, A. and Ferrari, J.},
title={One-shot 3D-gradient method applied to face recognition},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9423},
pages={176-183},
doi={10.1007/978-3-319-25751-8_22},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983485990&doi=10.1007%2f978-3-319-25751-8_22&partnerID=40&md5=d1ca3b0d2eb47528de1f16f2734dc918},
affiliation={Facultad de Ingeniería, Universidad de la República, Montevideo, Uruguay},
abstract={In this work we describe a novel one-shot face recognition setup. Instead of using a 3D scanner to reconstruct the face, we acquire a single photo of the face of a person while a rectangular pattern is been projected over it. Using this unique image, it is possible to extract 3D low-level geometrical features without the explicit 3D reconstruction. To handle expression variations and occlusions that may occur (e.g. wearing a scarf or a bonnet), we extract information just from the eyes-forehead and nose regions which tend to be less influenced by facial expressions. Once features are extracted, SVM hyper-planes are obtained from each subject on the database (one vs all approach), then new instances can be classified according to its distance to each of those hyper-planes. The advantage of our method with respect to other ones published in the literature, is that we do not need and explicit 3D reconstruction. Experiments with the Texas 3D Database and with new acquired data are presented, which shows the potential of the presented framework to handle different illumination conditions, pose and facial expressions. © Springer International Publishing Switzerland 2015.},
author_keywords={3D face recognition;  Differential 3D reconstruction},
document_type={Conference Paper},
source={Scopus},
}
﻿
@article{ ISI:000463151400027,
Author = {Rasouli, Maryam S. D. and Payandeh, Shahram},
Title = {{A novel depth image analysis for sleep posture estimation}},
Journal = {{JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING}},
Year = {{2019}},
Volume = {{10}},
Number = {{5, SI}},
Pages = {{1999-2014}},
Month = {{MAY}},
Abstract = {{Recognition of sleep posture and its changes are related to information
   monitoring in a number of health-related applications such as apnea
   prevention and elderly care. This paper uses a less privacy-invading
   approach to classify sleep postures of a person in various
   configurations including side and supine postures. In order to
   accomplish this, a single depth sensor has been utilized to collect
   selective depth signals and populated a dataset associated with the
   depth data. The data is then analyzed by a novel frequency-based feature
   selection approach. These extracted features were then correlated in
   order to rank their information content in various 2D scans from the 3D
   point cloud in order to train a support vector machine (SVM). The data
   of subjects are collected under two conditions. First when they were
   covered with a thin blanket and second without any blanket. In order to
   reduce the dimensionality of the feature space, a T-test approach is
   employed to determine the most dominant set of features in the frequency
   domain. The proposed recognition approach based on the frequency domain
   is also compared with an approach using feature vector defined based on
   skeleton joints. The comparative studies are performed given various
   scenarios and by a variety of datasets. Through our study, it is shown
   that our proposed method offers better performance to that of the
   joint-based method.}},
DOI = {{10.1007/s12652-018-0796-1}},
ISSN = {{1868-5137}},
EISSN = {{1868-5145}},
Unique-ID = {{ISI:000463151400027}},
}

@article{ ISI:000458711300008,
Author = {Patruno, Cosimo and Marani, Roberto and Cicirelli, Grazia and Stella,
   Ettore and D'Orazio, Tiziana},
Title = {{People re-identification using skeleton standard posture and color
   descriptors from RGB-D data}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{89}},
Pages = {{77-90}},
Month = {{MAY}},
Abstract = {{This paper tackles the problem of people re-identification by using soft
   biometrics features. The method works on RGB-D data (color point clouds)
   to determine the best matching among a database of possible users. For
   each subject under testing, skeletal information in three-dimensions is
   used to regularize the pose and to create a skeleton standard posture
   (SSP). A partition grid, whose sizes depend on the SSP, groups the
   samples of the point cloud accordingly to their position. Every group is
   then studied to build the person signature. The same grid is then used
   for the other subjects of the database to preserve information about
   possible shape differences among users. The effectiveness of this novel
   method has been tested on three public datasets. Numerical experiments
   demonstrate an improvement of results with reference to the current
   state-of-the-art, with recognition rates of 97.84\% (on a partition of
   BIWI RGBD-ID), 61.97\% (KinectREID) and 89.71\% (RGBD-ID), respectively.
   (C) 2019 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2019.01.003}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{D'Orazio, Tiziana/H-5032-2019}},
ORCID-Numbers = {{D'Orazio, Tiziana/0000-0003-1473-7110}},
Unique-ID = {{ISI:000458711300008}},
}

@article{ ISI:000457666900036,
Author = {Lv, Chenlei and Wu, Zhongke and Wang, Xingce and Zhou, Mingquan and Toh,
   Kar-Ann},
Title = {{Nasal similarity measure of 3D faces based on curve shape space}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{88}},
Pages = {{458-469}},
Month = {{APR}},
Abstract = {{We propose a novel method for measuring the nasal similarity among 3D
   faces. Firstly, we construct a representation for the nose shape, which
   is composed of a set of geodesic curves, each crosses the bridge of the
   nose. Next, using these geodesic curves, we formulate a similarity
   measure to compare among noses in the curve shape space. Under the
   Riemannian framework, the shape space is a quotient space for which the
   scaling, translation and rotation are removed. Since the nose similarity
   measure is based on the shape comparison, the proposed method has the
   following advantages: (1) the similarity measure is robust to facial
   expressions since the nose is not affected by facial expressions; (2)
   the geometric features of the nose shape match well with the human
   perception; (3) the similarity measure is independent of the mesh grid
   because the chosen nose curves are not sensitive to the triangular mesh
   model. We construct a nasal hierarchical structure for noses
   organization which is based on nose similarity measure results. In our
   experiments, we evaluate the performance of the proposed method and
   compare it with competing methods on three public face databases namely,
   FRGC2.0, Texas3D and BosphorusDB. The results show superiority of the
   proposed method in terms of both the speed and the accuracy when the
   nasal measurements are processed in the nasal hierarchical structure and
   the nasal samples with low sampling rate (5\%-25\% of original point
   cloud). (C) 2018 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.12.006}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000457666900036}},
}

@article{ ISI:000457666900042,
Author = {Pribanic, Tomislav and Petkovic, Tomislav and Donlic, Matea},
Title = {{3D registration based on the direction sensor measurements}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{88}},
Pages = {{532-546}},
Month = {{APR}},
Abstract = {{3D registration is a very active topic, spanning research areas such as
   computational geometry, computer graphics and pattern recognition. It
   aims to solve spatial transformation that aligns two point clouds. In
   this work we propose the use of a single direction sensor, such as an
   accelerometer or a magnetometer, commonly available on contemporary
   mobile platforms, such as tablets and smartphones. Both sensors have
   been heavily investigated earlier, but only for joint use with other
   sensors, such as gyroscopes and GPS. We show a time-efficient and
   accurate 3D registration method that takes advantage of only either an
   accelerometer or a magnetometer. We demonstrate a 3D reconstruction of
   individual point clouds and the proposed 3D registration method on a
   tablet equipped with an accelerometer or a magnetometer. However, we
   point out that the proposed method is not restricted to mobile
   platforms. Indeed, it can easily be applied in any 3D measurement system
   that is upgradable with some ubiquitous direction sensor, for example by
   adding a smartphone equipped with either an accelerometer or a
   magnetometer. We compare the proposed method against several
   state-of-the-art methods implemented in the open source Point Cloud
   Library (PCL). The proposed method outperforms the PCL methods tested,
   both in terms of processing time and accuracy. (C) 2018 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.12.008}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Petkovic, Tomislav/0000-0002-3054-002X
   Donlic, Matea/0000-0001-5165-6438}},
Unique-ID = {{ISI:000457666900042}},
}

@article{ ISI:000461603400031,
Author = {Yi, Tang-Tang},
Title = {{Face Recognition Algorithm Based on 3D Point Cloud Acquired by Mixed
   Image Sensor}},
Journal = {{JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT
   INFORMATICS}},
Year = {{2019}},
Volume = {{23}},
Number = {{2}},
Pages = {{366-369}},
Month = {{MAR}},
Abstract = {{In order to solve the problem of low recognition accuracy in recognition
   of 3D face images collected by traditional sensors, a face recognition
   algorithm for 3D point cloud collected by mixed image sensors is
   proposed. The algorithm first uses the 3D wheelbase to expand the face
   image edge. According to the 3D wheelbase, the noise of extended image
   is detected, and median filtering is used to eliminate the detected
   noise. Secondly, the priority of the boundary pixels to recognize the
   face image in the denoising image recognition process is determined, and
   the key parts such as the illuminance line are analyzed, so that the
   recognition of the 3D point cloud face image is completed. Experiments
   show that the proposed algorithm improves the recognition accuracy of 3D
   face images, which recognition time is lower than that of the
   traditional algorithm by about 4 times, and the recognition efficiency
   is high.}},
DOI = {{10.20965/jaciii.2019.p0366}},
ISSN = {{1343-0130}},
EISSN = {{1883-8014}},
Unique-ID = {{ISI:000461603400031}},
}

@article{ ISI:000460046500034,
Author = {Alameda-Hernandez, Pedro and El Hamdouni, Rachid and Irigaray, Clemente
   and Chacon, Jose},
Title = {{Weak foliated rock slope stability analysis with ultra-close-range
   terrestrial digital photogrammetry}},
Journal = {{BULLETIN OF ENGINEERING GEOLOGY AND THE ENVIRONMENT}},
Year = {{2019}},
Volume = {{78}},
Number = {{2}},
Pages = {{1157-1171}},
Month = {{MAR}},
Abstract = {{This paper presents a review of the data acquisition procedures of
   geotechnical parameters for rock slope stability assessment and the
   proposal of some new improvements. For this purpose, a piece of research
   based on the slope mass rating classification system using close-range
   terrestrial digital photogrammetry (CR-TDP) has led to improvements in
   quality and timing of discontinuity data acquisition, and analyzes the
   suitability of each one of the parameters when applied to weak foliated
   rocks. TDP allows rapid 3D image acquisition of a rock slope, which can
   be analyzed using software to determine the geometrical parameters that
   affect stability. A fast procedure to perform the photogrammetric,
   non-contact survey in order to obtain the 3D images is shown in this
   paper. Being a rapid and single-person task, this procedure provides
   enough precision to be applied to weak foliated rock slopes with
   non-well-defined geometry. Furthermore, the study has focused on highly
   foliated rock outcrops, in which high resolution in the 3D images is
   very desirable. This research was applied to mountain road cuts, in
   which the use of TDP with a very close range was necessary. Through an
   application on weak rocks in the Alpujarras (Andalusia, Spain), this
   work analyzes the bias when applying TDP to materials such as these,
   under progressive weathering processes.}},
DOI = {{10.1007/s10064-017-1119-z}},
ISSN = {{1435-9529}},
EISSN = {{1435-9537}},
ResearcherID-Numbers = {{Hamdouni, Rachid El/L-1672-2017
   }},
ORCID-Numbers = {{Hamdouni, Rachid El/0000-0002-1271-3839
   Alameda Hernandez, Pedro Manuel/0000-0003-1830-1912}},
Unique-ID = {{ISI:000460046500034}},
}

@article{ ISI:000457692800013,
Author = {Abu, Arpah and Ngo, Chee Guan and Abu-Hassan, Nur Idayu Adira and
   Othman, Siti Adibah},
Title = {{Automated craniofacial landmarks detection on 3D image using geometry
   characteristics information}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2019}},
Volume = {{19}},
Number = {{13}},
Month = {{FEB 4}},
Note = {{17th International Conference on Bioinformatics (InCoB), Jawaharlal
   Nehru Univ, New Delhi, INDIA, SEP 26-28, 2018}},
Abstract = {{BackgroundIndirect anthropometry (IA) is one of the craniofacial
   anthropometry methods to perform the measurements on the digital facial
   images. In order to get the linear measurements, a few definable points
   on the structures of individual facial images have to be plotted as
   landmark points. Currently, most anthropometric studies use landmark
   points that are manually plotted on a 3D facial image by the examiner.
   This method is time-consuming and leads to human biases, which will vary
   from intra-examiners to inter-examiners when involving large data sets.
   Biased judgment also leads to a wider gap in measurement error. Thus,
   this work aims to automate the process of landmarks detection to help in
   enhancing the accuracy of measurement. In this work, automated
   craniofacial landmarks (ACL) on a 3D facial image system was developed
   using geometry characteristics information to identify the nasion (n),
   pronasale (prn), subnasale (sn), alare (al), labiale superius (ls),
   stomion (sto), labiale inferius (li), and chelion (ch). These landmarks
   were detected on the 3D facial image in .obj file format. The IA was
   also performed by manually plotting the craniofacial landmarks using
   Mirror software. In both methods, once all landmarks were detected, the
   eight linear measurements were then extracted. Paired t-test was
   performed to check the validity of ACL (i) between the subjects and (ii)
   between the two methods, by comparing the linear measurements extracted
   from both ACL and AI. The tests were performed on 60 subjects (30 males
   and 30 females).ResultsThe results on the validity of the ACL against IA
   between the subjects show accurate detection of n, sn, prn, sto, ls and
   li landmarks. The paired t-test showed that the seven linear
   measurements were statistically significant when p<0.05. As for the
   results on the validity of the ACL against IA between the methods, ACL
   is more accurate when p approximate to 0.03.ConclusionsIn conclusion,
   ACL has been validated with the eight landmarks and is suitable for
   automated facial recognition. ACL has proved its validity and
   demonstrated the practicability to be used as an alternative for IA, as
   it is time-saving and free from human biases.}},
DOI = {{10.1186/s12859-018-2548-9}},
Article-Number = {{548}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{OTHMAN, SITI ADIBAH BINTI/B-9784-2010
   Management Center, Dental Research/C-2478-2013}},
ORCID-Numbers = {{OTHMAN, SITI ADIBAH BINTI/0000-0002-3846-7701
   }},
Unique-ID = {{ISI:000457692800013}},
}

@article{ ISI:000460829200061,
Author = {Che, Erzhuo and Jung, Jaehoon and Olsen, Michael J.},
Title = {{Object Recognition, Segmentation, and Classification of Mobile Laser
   Scanning Point Clouds: A State of the Art Review}},
Journal = {{SENSORS}},
Year = {{2019}},
Volume = {{19}},
Number = {{4}},
Month = {{FEB 2}},
Abstract = {{Mobile Laser Scanning (MLS) is a versatile remote sensing technology
   based on Light Detection and Ranging (lidar) technology that has been
   utilized for a wide range of applications. Several previous reviews
   focused on applications or characteristics of these systems exist in the
   literature, however, reviews of the many innovative data processing
   strategies described in the literature have not been conducted in
   sufficient depth. To this end, we review and summarize the state of the
   art for MLS data processing approaches, including feature extraction,
   segmentation, object recognition, and classification. In this review, we
   first discuss the impact of the scene type to the development of an MLS
   data processing method. Then, where appropriate, we describe relevant
   generalized algorithms for feature extraction and segmentation that are
   applicable to and implemented in many processing approaches. The methods
   for object recognition and point cloud classification are further
   reviewed including both the general concepts as well as technical
   details. In addition, available benchmark datasets for object
   recognition and classification are summarized. Further, the current
   limitations and challenges that a significant portion of point cloud
   processing techniques face are discussed. This review concludes with our
   future outlook of the trends and opportunities of MLS data processing
   algorithms and applications.}},
DOI = {{10.3390/s19040810}},
Article-Number = {{810}},
ISSN = {{1424-8220}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000460829200061}},
}

@article{ ISI:000459798800005,
Author = {Sun, Jia and Huang, Di and Wang, Yunhong and Chen, Liming},
Title = {{Expression Robust 3D Facial Landmarking via Progressive Coarse-to-Fine
   Tuning}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2019}},
Volume = {{15}},
Number = {{1}},
Month = {{FEB}},
Abstract = {{Facial landmarking is a fundamental task in automatic machine-based face
   analysis. The majority of existing techniques for such a problem are
   based on 2D images; however, they suffer from illumination and pose
   variations that may largely degrade landmarking performance. The
   emergence of 3D data theoretically provides an alternative to overcome
   these weaknesses in the 2D domain. This article proposes a novel
   approach to 3D facial landmarking, which combines both the advantages of
   feature-based methods as well as model-based ones in a progressive
   three-stage coarse-to-fine manner (initial, intermediate, and fine
   stages). For the initial stage, a few fiducial landmarks (i.e., the nose
   tip and two inner eye corners) are robustly detected through curvature
   analysis, and these points are further exploited to initialize the
   subsequent stage. For the intermediate stage, a statistical model is
   learned in the feature space of three normal components of the facial
   point-cloud rather than the smooth original coordinates, namely Active
   Normal Model (ANM). For the fine stage, cascaded regression is employed
   to locally refine the landmarks according to their geometry attributes.
   The proposed approach can accurately localize dozens of fiducial points
   on each 3D face scan, greatly surpassing the feature-based ones, and it
   also improves the state of the art of the model-based ones in two
   aspects: sensitivity to initialization and deficiency in discrimination.
   The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE
   databases, and competitive results are achieved in comparison with
   counterparts in the literature, clearly demonstrating its effectiveness.}},
DOI = {{10.1145/3282833}},
Article-Number = {{21}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
Unique-ID = {{ISI:000459798800005}},
}

@article{ ISI:000456899900003,
Author = {Zhou, Zixiang and Gong, Jie and Hu, Xuan},
Title = {{Community-scale multi-level post-hurricane damage assessment of
   residential buildings using multi-temporal airborne LiDAR data}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2019}},
Volume = {{98}},
Pages = {{30-45}},
Month = {{FEB}},
Abstract = {{Building damage assessment is a critical task following major hurricane
   events. Use of remotely sensed data to support building damage
   assessment is a logical choice considering the difficulty of gaining
   ground access to the impacted areas immediately after hurricane events.
   However, a remote sensing based damage assessment approach is often only
   capable of detecting severely damaged buildings. In this study, an
   airborne LiDAR based approach is proposed to assess multi-level
   hurricane damage at the community scale. In the proposed approach,
   building clusters are first extracted using a density-based algorithm. A
   novel cluster matching algorithm is proposed to robustly match
   post-event and pre-event building clusters. Multiple features including
   roof area and volume, roof orientation, and roof shape are computed as
   building damage indicators. A hierarchical determination process is then
   employed to identify the extent of damage to each building object. The
   results of this study suggest that our proposed approach is capable of
   1) recognizing building objects, 2) extracting damage features, and 3)
   characterizing the extent of damage to individual building properties.}},
DOI = {{10.1016/j.autcon.2018.10.018}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
Unique-ID = {{ISI:000456899900003}},
}

@article{ ISI:000457939400106,
Author = {Zhang, Wuming and Wan, Peng and Wang, Tiejun and Cai, Shangshu and Chen,
   Yiming and Jin, Xiuliang and Yan, Guangjian},
Title = {{A Novel Approach for the Detection of Standing Tree Stems from
   Plot-Level Terrestrial Laser Scanning Data}},
Journal = {{REMOTE SENSING}},
Year = {{2019}},
Volume = {{11}},
Number = {{2}},
Month = {{JAN 2}},
Abstract = {{Tree stem detection is a key step toward retrieving detailed stem
   attributes from terrestrial laser scanning (TLS) data. Various
   point-based methods have been proposed for the stem point extraction at
   both individual tree and plot levels. The main limitation of the
   point-based methods is their high computing demand when dealing with
   plot-level TLS data. Although segment-based methods can reduce the
   computational burden and uncertainties of point cloud classification,
   its application is largely limited to urban scenes due to the complexity
   of the algorithm, as well as the conditions of natural forests. Here we
   propose a novel and simple segment-based method for efficient stem
   detection at the plot level, which is based on the curvature feature of
   the points and connected component segmentation. We tested our method
   using a public TLS dataset with six forest plots that were collected for
   the international TLS benchmarking project in Evo, Finland. Results
   showed that the mean accuracies of the stem point extraction were
   comparable to the state-of-art methods (>95\%). The accuracies of the
   stem mappings were also comparable to the methods tested in the
   international TLS benchmarking project. Additionally, our method was
   applicable to a wide range of stem forms. In short, the proposed method
   is accurate and simple; it is a sensible solution for the stem detection
   of standing trees using TLS data.}},
DOI = {{10.3390/rs11020211}},
Article-Number = {{211}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{Yan, Guangjian/0000-0001-5030-748X
   Wang, Tiejun/0000-0002-1138-8464}},
Unique-ID = {{ISI:000457939400106}},
}

@article{ ISI:000457037300020,
Author = {Shao, Gang and Shao, Guofan and Fei, Songlin},
Title = {{Delineation of individual deciduous trees in plantations with
   low-density LiDAR data}},
Journal = {{INTERNATIONAL JOURNAL OF REMOTE SENSING}},
Year = {{2019}},
Volume = {{40}},
Number = {{1}},
Pages = {{346-363}},
Month = {{JAN 2}},
Abstract = {{Delineation of individual deciduous trees with Light Detection and
   Ranging (LiDAR) data has long been sought for accurate forest inventory
   in temperate forests. Previous attempts mainly focused on high-density
   LiDAR data to obtain reliable delineation results, which may have
   limited applications due to the high cost and low availability of such
   data. Here, the feasibility of individual deciduous tree delineation
   with low-density LiDAR data was examined using a point-density-based
   algorithm. First a high-resolution point density model (PDM) was
   developed from low-density LiDAR point cloud to locate individual trees
   through the horizontal spatial distribution of LiDAR points. Then,
   individual tree crowns and associated attributes were delineated with a
   2D marker-controlled watershed segmentation. Additionally, the PDM-based
   approach was compared with a conventional canopy height model (CHM)
   based delineation. The results demonstrated that the PDM-based approach
   produced an 89\% detection accuracy to identify deciduous trees in our
   study area. The tree attributes derived from the PDM-based algorithm
   explained 81\% and 83\% of tree height and crown width variations of
   forest stands, respectively. The conventional CHM-based tree attributes,
   on the other hand, could explain only 71\% and 66\% of tree height and
   crown width, respectively. Our results suggest that the application of
   the PDM-based individual tree identification in deciduous forests with
   low-density LiDAR data is feasible and has relatively high accuracy to
   predict tree height and crown width, which are highly desired in
   large-scale forest inventory and analysis.}},
DOI = {{10.1080/01431161.2018.1513664}},
ISSN = {{0143-1161}},
EISSN = {{1366-5901}},
ORCID-Numbers = {{Shao, Gang/0000-0003-3198-966X}},
Unique-ID = {{ISI:000457037300020}},
}

@article{ ISI:000457664300030,
Author = {Teza, Giordano and Pesci, Arianna},
Title = {{Evaluation of the temperature pattern of a complex body from thermal
   imaging and 3D information: A method and its MATLAB implementation}},
Journal = {{INFRARED PHYSICS \& TECHNOLOGY}},
Year = {{2019}},
Volume = {{96}},
Pages = {{228-237}},
Month = {{JAN}},
Abstract = {{The standard setting of a camera used in Infrared thermography (IRT) is
   based on the choice of the same values of emissivity and distance for
   all pixels of a thermal image even if the emissivity depends on the
   relative position of camera and observed surface. Often this is not a
   problem. However, the resulting temperature pattern could be inadequate
   if a body having a complex shape is observed from strongly constrained
   positions. In order to face this issue, a procedure aimed at providing a
   correct temperature pattern by using 3D information related to a point
   cloud is proposed together with its MATLAB implementation (COMAP3
   toolbox). For each pixel of a thermal image, the relative position of
   camera and observed surface is estimated, leading to pixel-specific
   values of emissivity and distance. The temperature obtained in this way
   is also mapped onto the point cloud. The effectiveness of the procedure
   in recognizing areas characterized by peculiar thermal behavior is shown
   in the case of a historic cylindrical masonry bell tower (Caorle's bell
   tower, Venice, Italy).}},
DOI = {{10.1016/j.infrared.2018.11.029}},
ISSN = {{1350-4495}},
EISSN = {{1879-0275}},
ORCID-Numbers = {{PESCI, Arianna/0000-0003-1863-3132
   Teza, Giordano/0000-0002-6902-5033}},
Unique-ID = {{ISI:000457664300030}},
}

@article{ ISI:000457692900001,
Author = {Jazouli, Maha and Majda, Aicha and Merad, Djamal and Aalouane, Rachid
   and Zarghili, Arsalane},
Title = {{Automatic detection of stereotyped movements in autistic children using
   the Kinect sensor}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMEDICAL ENGINEERING AND TECHNOLOGY}},
Year = {{2019}},
Volume = {{29}},
Number = {{3}},
Pages = {{201-220}},
Abstract = {{Autism Spectrum Disorders (ASD) is a developmental disorder that affects
   communications, social skills or behaviours that can occur in some
   people. Children or adults with ASD often have repetitive motor
   movements or unusual behaviours. The objective of this work is to
   automatically detect stereotypical motor movements in real time using
   Kinect sensor. The approach is based on the \$P Point-Cloud Recogniser
   to identify multi-stroke gestures as point clouds. This paper presents
   new methodology to automatically detect five stereotypical motor
   movements: body rocking, hand flapping, fingers flapping, hand on the
   face and hands behind back. With many ASD-children, our proposed system
   gives us satisfactory results. This can help to implement a smart video
   surveillance system and then helps clinicians in the diagnosing ASD.}},
DOI = {{10.1504/IJBET.2019.097621}},
ISSN = {{1752-6418}},
EISSN = {{1752-6426}},
Unique-ID = {{ISI:000457692900001}},
}

@article{ ISI:000454963800023,
Author = {Garcia-Luna, Ramiro and Senent, Salvador and Jurado-Pina, Rafael and
   Jimenez, Rafael},
Title = {{Structure from Motion photogrammetry to characterize underground rock
   masses: Experiences from two real tunnels}},
Journal = {{TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY}},
Year = {{2019}},
Volume = {{83}},
Pages = {{262-273}},
Month = {{JAN}},
Abstract = {{A new methodology to identify discontinuity sets at the tunnel face
   based on the Structure from Motion (SfM) photogrammetric technique is
   proposed. The work focuses on the performance of this technique when
   employed to characterize the ground mass under real tunneling
   conditions, illustrating its possibilities and analyzing several aspects
   that affect the quality of the obtained results. By means of a set of
   overlapping photographs from the tunnel face, SfM constructs a 3D point
   cloud model, from which discontinuities are identified using a
   discontinuity set extractor software. To orientate and scale the digital
   model, an easy-to-use ``portable orientation template{''}, specifically
   developed for this work, is employed. The proposed methodology is
   applied to two real tunnels under construction in Northern Spain. Its
   results are compared with those obtained with a traditional analysis
   based on manual compass measurements. Results show that the SfM
   methodology provides an adequate characterization of the structure of
   the rock mass, identifying the same number of discontinuity sets as the
   compass measurements approach and with differences in orientation that
   are within the uncertainty range associated to manual measurements. Only
   one sub-horizontal set presented higher orientation differences, but
   this is mainly due to the presence of shotcrete at the face. In addition
   to the advantages of a ``distant{''} measurement technique-e.g., health
   and safety advantages, capability to characterize unreachable areas,
   etc.-, as well as to the advantage of its reduced cost, the proposed SfM
   methodology and its associated tools allow one to represent planes
   associated to each discontinuity set back into the original 3D digital
   point model, and to perform detailed analyses that clarify and improve
   the obtained results. Finally, an analysis about the minimum number of
   photographs needed to adequately characterize the tunnel face is
   conducted, with results showing that around 15 good quality photographs
   are enough for tunnel faces with excavated areas of about 50 m(2).}},
DOI = {{10.1016/j.tust.2018.09.026}},
ISSN = {{0886-7798}},
Unique-ID = {{ISI:000454963800023}},
}

@article{ ISI:000450379200003,
Author = {Kaminska, Agnieszka and Lisiewicz, Maciej and Sterenczak, Krzysztof and
   Kraszewski, Bartlomiej and Sadkowski, Rafal},
Title = {{Species-related single dead tree detection using multi-temporal ALS data
   and CIR imagery}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{219}},
Pages = {{31-43}},
Month = {{DEC 15}},
Abstract = {{The assessment of the health conditions of trees in forests is extremely
   important for biodiversity, forest management, global environment
   monitoring, and carbon dynamics. There is a vast amount of research
   using remote sensing (RS) techniques for the assessment of the current
   condition of a forest, but only a small number of these are concerned
   with detection and classification of dead trees. Among the available RS
   techniques, only the airborne laser scanner (ALS) enables dead tree
   detection at the single tree level with high accuracy.
   The main objective of the study was to identify spruce, pine and
   deciduous trees by alive or dead classifications. Three RS data sets
   including ALS (leaf-on and leaf-off) and color-infrared (CIR) imagery
   (leaf-on) were used for the study. We used intensity and structural
   variables from the ALS data and spectral information derived from aerial
   imagery for the classification procedure. Additionally, we tested the
   differences in the classification accuracy of all variants contained in
   the data integration. In the study, the random forest (RF) classifier
   was used. The study was carried out in the Polish part of the Bialowieia
   Forest (BF).
   In general, we can state that all classifications, with different
   combinations of ALS features and CIR, resulted in high overall accuracy
   (OA >= 90\%) and Kappa (kappa > 0.86). For the best variant
   (CIR\_ALS(WSn-FH)), the mean values of overall accuracy and Kappa were
   equal to 94.3\% and 0.93, respectively. The leaf -on point cloud
   features alone produced the lowest accuracies (OA = 75-81\% and x =
   0.68-0.76). Improvements of 0-0.04 in the Kappa coefficient and 0-3.1\%
   in the overall classification accuracy were found after the point cloud
   normalization for all variants. Full -height point cloud features (F)
   produced lower accuracies than the results based on features calculated
   for half of the tree height point clouds (H) and combined FH.
   The importance of each of the predictors for different data sets for
   tree species classification provided by the RF algorithm was
   investigated. The lists of top features were the same, independent of
   intensity normalization. For the classification based on both of the
   point clouds (leaf on and leaf-off), three structural features (a
   proportion of first returns for both half -height and full -height
   variants and the canopy relief ratio of points) and two intensity
   features from first returns and half -height variant (the coefficient of
   variation and skewness) were rated as the most important. In the
   classification based on the point cloud with CIR features, two image
   features were among the most important (the NDVI and mean value of
   reflectance in the green band).}},
DOI = {{10.1016/j.rse.2018.10.005}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Lisiewicz, Maciej/Y-3668-2018
   Kraszewski, Bartlomiej/U-9482-2017}},
ORCID-Numbers = {{Lisiewicz, Maciej/0000-0003-0676-8291
   Sterenczak, Krzysztof/0000-0002-9556-0144
   Kraszewski, Bartlomiej/0000-0001-6161-7619}},
Unique-ID = {{ISI:000450379200003}},
}

@article{ ISI:000460570100008,
Author = {Nguyen Duy Thanh and Khachumov, V. M.},
Title = {{Models and Methods for Matching Images in the Problem of Face
   Recognition}},
Journal = {{SCIENTIFIC AND TECHNICAL INFORMATION PROCESSING}},
Year = {{2018}},
Volume = {{45}},
Number = {{5}},
Pages = {{360-367}},
Month = {{DEC}},
Abstract = {{A subject area is analyzed and the topicality of the problem of face
   recognition is shown. Methods for matching images by applying position
   lines, convolutions and invariants to the group of affine transformation
   for 2D and 3D images are examined. Proper comparison is a necessary
   condition for solving the recognition problem. Proper use of the
   position-line method for reducing face images to the normalized form is
   shown.}},
DOI = {{10.3103/S0147688218050088}},
ISSN = {{0147-6882}},
EISSN = {{1934-8118}},
Unique-ID = {{ISI:000460570100008}},
}

@article{ ISI:000455637600091,
Author = {Zhou, Tan and Popescu, Sorin and Malambo, Lonesome and Zhao, Kaiguang
   and Krause, Keith},
Title = {{From LiDAR Waveforms to Hyper Point Clouds: A Novel Data Product to
   Characterize Vegetation Structure}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Full waveform (FW) LiDAR holds great potential for retrieving vegetation
   structure parameters at a high level of detail, but this prospect is
   constrained by practical factors such as the lack of available handy
   processing tools and the technical intricacy of waveform processing.
   This study introduces a new product named the Hyper Point Cloud (HPC),
   derived from FW LiDAR data, and explores its potential applications,
   such as tree crown delineation using the HPC-based intensity and
   percentile height (PH) surfaces, which shows promise as a solution to
   the constraints of using FW LiDAR data. The results of the HPC present a
   new direction for handling FW LiDAR data and offer prospects for
   studying the mid-story and understory of vegetation with high point
   density (similar to 182 points/m(2)). The intensity-derived digital
   surface model (DSM) generated from the HPC shows that the ground region
   has higher maximum intensity (MAXI) and mean intensity (MI) than the
   vegetation region, while having lower total intensity (TI) and number of
   intensities (NI) at a given grid cell. Our analysis of intensity
   distribution contours at the individual tree level exhibit similar
   patterns, indicating that the MAXI and MI decrease from the tree crown
   center to the tree boundary, while a rising trend is observed for TI and
   NI. These intensity variable contours provide a theoretical
   justification for using HPC-based intensity surfaces to segment tree
   crowns and exploit their potential for extracting tree attributes. The
   HPC-based intensity surfaces and the HPC-based PH Canopy Height Models
   (CHM) demonstrate promising tree segmentation results comparable to the
   LiDAR-derived CHM for estimating tree attributes such as tree locations,
   crown widths and tree heights. We envision that products such as the HPC
   and the HPC-based intensity and height surfaces introduced in this study
   can open new perspectives for the use of FW LiDAR data and alleviate the
   technical barrier of exploring FW LiDAR data for detailed vegetation
   structure characterization.}},
DOI = {{10.3390/rs10121949}},
Article-Number = {{1949}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Popescu, Sorin C/D-5981-2015
   Malambo, Lonesome/A-5693-2015}},
ORCID-Numbers = {{Popescu, Sorin C/0000-0002-8155-8801
   Zhou, Tan/0000-0002-9193-5113
   Malambo, Lonesome/0000-0002-8102-3700}},
Unique-ID = {{ISI:000455637600091}},
}

@article{ ISI:000455069600031,
Author = {Jaafar, Wan Shafrina Wan Mohd and Woodhouse, Iain Hector and Silva,
   Carlos Alberto and Omar, Hamdan and Maulud, Khairul Nizam Abdul and
   Hudak, Andrew Thomas and Klauberg, Carine and Cardil, Adrian and Mohan,
   Midhun},
Title = {{Improving Individual Tree Crown Delineation and Attributes Estimation of
   Tropical Forests Using Airborne LiDAR Data}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Individual tree crown (ITC) segmentation is an approach to isolate
   individual tree from the background vegetation and delineate precisely
   the crown boundaries for forest management and inventory purposes. ITC
   detection and delineation have been commonly generated from canopy
   height model (CHM) derived from light detection and ranging (LiDAR)
   data. Existing ITC segmentation methods, however, are limited in their
   efficiency for characterizing closed canopies, especially in tropical
   forests, due to the overlapping structure and irregular shape of tree
   crowns. Furthermore, the potential of 3-dimensional (3D) LiDAR data is
   not fully realized by existing CHM-based methods. Thus, the aim of this
   study was to develop an efficient framework for ITC segmentation in
   tropical forests using LiDAR-derived CHM and 3D point cloud data in
   order to accurately estimate tree attributes such as the tree height,
   mean crown width and aboveground biomass (AGB). The proposed framework
   entails five major steps: (1) automatically identifying dominant tree
   crowns by implementing semi-variogram statistics and morphological
   analysis; (2) generating initial tree segments using a watershed
   algorithm based on mathematical morphology; (3) identifying problematic
   segments based on predetermined set of rules; (4) tuning the problematic
   segments using a modified distance-based algorithm (DBA); and (5)
   segmenting and counting the number of individual trees based on the 3D
   LiDAR point clouds within each of the identified segment. This approach
   was developed in a way such that the 3D LiDAR points were only examined
   on problematic segments identified for further evaluations. 209
   reference trees with diameter at breast height (DBH) 10 cm were selected
   in the field in two study areas in order to validate ITC detection and
   delineation results of the proposed framework. We computed tree crown
   metrics (e.g., maximum crown height and mean crown width) to estimate
   aboveground biomass (AGB) at tree level using previously published
   allometric equations. Accuracy assessment was performed to calculate
   percentage of correctly detected trees, omission and commission errors.
   Our method correctly identified individual tree crowns with detection
   accuracy exceeding 80 percent at both forest sites. Also, our results
   showed high agreement (R-2 > 0.64) in terms of AGB estimates using 3D
   LiDAR metrics and variables measured in the field, for both sites. The
   findings from our study demonstrate the efficacy of the proposed
   framework in delineating tree crowns, even in high canopy density areas
   such as tropical rainforests, where, usually the traditional algorithms
   are limited in their performances. Moreover, the high tree delineation
   accuracy in the two study areas emphasizes the potential robustness and
   transferability of our approach to other densely forested areas across
   the globe.}},
DOI = {{10.3390/f9120759}},
Article-Number = {{759}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{wan mohd jaafar, wan shafrina/P-3916-2018
   abdul maulud, khairul nizam/J-4136-2015
   }},
ORCID-Numbers = {{Silva, Carlos Alberto/0000-0002-7844-3560
   wan mohd jaafar, wan shafrina/0000-0002-7813-088X
   abdul maulud, khairul nizam/0000-0002-9215-2778
   Hudak, Andrew/0000-0001-7480-1458}},
Unique-ID = {{ISI:000455069600031}},
}

@article{ ISI:000451733800040,
Author = {Shen, Yueqian and Lindenbergh, Roderik and Wang, Jinguo and Ferreira,
   Vagner G.},
Title = {{Extracting Individual Bricks from a Laser Scan Point Cloud of an
   Unorganized Pile of Bricks}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Bricks are the vital component of most masonry structures. Their
   maintenance is critical to the protection of masonry buildings.
   Terrestrial Light Detection and Ranging (TLidar) systems provide massive
   point cloud data in an accurate and fast way. TLidar enables us to
   sample and store the state of a brick surface in a practical way. This
   article aims to extract individual bricks from an unorganized pile of
   bricks sampled by a dense point cloud. The method automatically segments
   and models the individual bricks. The methodology is divided into five
   main steps: Filter needless points, brick boundary points removal,
   coarse segmentation using 3D component analysis, planar segmentation and
   grouping, and brick reconstruction. A novel voting scheme is used to
   segment the planar patches in an effective way. Brick reconstruction is
   based on the geometry of single brick and its corresponding nominal size
   (length, width and height). The number of bricks reconstructed is around
   75\%. An accuracy assessment is performed by comparing 3D coordinates of
   the reconstructed vertices to the manually picked vertices. The standard
   deviations of differences along x, y and z axes are 4.55 mm, 4.53 mm and
   4.60 mm, respectively. The comparison results indicate that the accuracy
   of reconstruction based on the introduced methodology is high and
   reliable. The work presented in this paper provides a theoretical basis
   and reference for large scene applications in brick-like structures.
   Meanwhile, the high-accuracy brick reconstruction lays the foundation
   for further brick displacement estimation.}},
DOI = {{10.3390/rs10111709}},
Article-Number = {{1709}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{Lindenbergh, Roderik/0000-0001-8655-5266
   Shen, Yueqian/0000-0003-2455-9012}},
Unique-ID = {{ISI:000451733800040}},
}

@article{ ISI:000433909100002,
Author = {Song, Xiaoning and Feng, Zhen-Hua and Hu, Guosheng and Kittler, Josef
   and Wu, Xiao-Jun},
Title = {{Dictionary Integration Using 3D Morphable Face Models for Pose-Invariant
   Collaborative-Representation-Based Classification}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2018}},
Volume = {{13}},
Number = {{11}},
Pages = {{2734-2745}},
Month = {{NOV}},
Abstract = {{The paper presents a dictionary integration algorithm using 3D morphable
   face models (3DMM) for pose-invariant collaborative-representation-based
   face classification. To this end, we first fit a 3DMM to the 2D face
   images of a dictionary to reconstruct the 3D shape and texture of each
   image. The 3D faces are used to render a number of virtual 2D face
   images with arbitrary pose variations to augment the training data, by
   merging the original and rendered virtual samples to create an extended
   dictionary. Second, to reduce the information redundancy of the extended
   dictionary and improve the sparsity of reconstruction coefficient
   vectors using collaborative-representation-based classification (CRC),
   we exploit an on-line class elimination scheme to optimise the extended
   dictionary by identifying the training samples of the most
   representative classes for a given query. The final goal is to perform
   pose-invariant face classification using the proposed dictionary
   integration method and the on-line pruning strategy under the CRC
   framework. Experimental results obtained for a set of well-known face
   data sets demonstrate the merits of the proposed method, especially its
   robustness to pose variations.}},
DOI = {{10.1109/TIFS.2018.2833052}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ORCID-Numbers = {{Kittler, Josef/0000-0002-8110-9205
   Feng, Zhenhua/0000-0002-4485-4249}},
Unique-ID = {{ISI:000433909100002}},
}

@article{ ISI:000447286200001,
Author = {Song, Wei and Zou, Shuanghui and Tian, Yifei and Fong, Simon and Cho,
   Kyungeun},
Title = {{Classifying 3D objects in LiDAR point clouds with a back-propagation
   neural network}},
Journal = {{HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES}},
Year = {{2018}},
Volume = {{8}},
Month = {{OCT 12}},
Abstract = {{Due to object recognition accuracy limitations, unmanned ground vehicles
   (UGVs) must perceive their environments for local path planning and
   object avoidance. To gather high-precision information about the UGV's
   surroundings, Light Detection and Ranging (LiDAR) is frequently used to
   collect large-scale point clouds. However, the complex spatial features
   of these clouds, such as being unstructured, diffuse, and disordered,
   make it difficult to segment and recognize individual objects. This
   paper therefore develops an object feature extraction and classification
   system that uses LiDAR point clouds to classify 3D objects in urban
   environments. After eliminating the ground points via a height threshold
   method, this describes the 3D objects in terms of their geometrical
   features, namely their volume, density, and eigenvalues. A
   back-propagation neural network (BPNN) model is trained (over the course
   of many iterations) to use these extracted features to classify objects
   into five types. During the training period, the parameters in each
   layer of the BPNN model are continually changed and modified via
   back-propagation using a non-linear sigmoid function. In the system, the
   object segmentation process supports obstacle detection for autonomous
   driving, and the object recognition method provides an environment
   perception function for terrain modeling. Our experimental results
   indicate that the object recognition accuracy achieve 91.5\% in outdoor
   environment.}},
DOI = {{10.1186/s13673-018-0152-7}},
Article-Number = {{29}},
ISSN = {{2192-1962}},
ORCID-Numbers = {{Wei, Song/0000-0002-5909-9661}},
Unique-ID = {{ISI:000447286200001}},
}

@article{ ISI:000445398200037,
Author = {Thierens, Laurent A. M. and De Roo, Noemi M. C. and De Pauw, Guy A. M.
   and Brusselaers, Nele},
Title = {{Quantifying Soft Tissue Changes in Cleft Lip and Palate Using
   Nonionizing Three-Dimensional Imaging: A Systematic Review}},
Journal = {{JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{76}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{Purpose: The use of nonionizing 3-dimensional (3D) imaging in cleft lip
   and palate (CLP) research is well-established; however, general
   guidelines concerning the assessment of these images are lacking. The
   aim of the present study was to review the methods for quantification of
   soft tissue changes on 3D surface images acquired before and after an
   orthopedic or surgical intervention in CLP patients.
   Materials and Methods: A systematic literature search was performed
   using the databases MEDLINE (through PubMed), CENTRAL, Web of Science,
   and EMBASE. The literature search and eligibility assessment were
   performed by 2 independent reviewers in a nonblinded standardized
   manner. Only longitudinal studies reporting the assessment of pre- and
   postoperative 3D surface images and at least 10 CLP patients were
   considered eligible.
   Results: Fifteen unique studies (reported from 1996 to 2017) were
   identified after an eligibility assessment. The assessment of the 3D
   images was performed with landmark-dependent analyses, mostly supported
   by superimposition of the pre- and postoperative images. A wide spectrum
   of superimposition techniques has been reported. The reliability of
   these assessment methods was often not reported or was insufficiently
   reported.
   Conclusions: Soft tissue changes subsequent to a surgical or an
   orthopedic intervention can be quantified on 3D surface images using
   assessment methods that are primarily based on landmark identification,
   whether or not followed by superimposition. Operator bias is inherently
   enclosed in landmark-dependent analyses. The reliability of these
   methods has been insufficiently reported. (C) 2018 American Association
   of Oral and Maxillofacial Surgeons}},
DOI = {{10.1016/j.joms.2018.05.020}},
ISSN = {{0278-2391}},
EISSN = {{1531-5053}},
Unique-ID = {{ISI:000445398200037}},
}

@article{ ISI:000442764500004,
Author = {Cardoso, Maria Joao and Vrieling, Conny and Cardoso, Jaime S. and
   Oliveira, Helder P. and Williams, Norman R. and Dixon, J. M. and PICTURE
   Project Clinical Trial Tea and PICTURE Project Delphi Panel},
Title = {{The value of 3D images in the aesthetic evaluation of breast cancer
   conservative treatment. Results from a prospective multicentric clinical
   trial}},
Journal = {{BREAST}},
Year = {{2018}},
Volume = {{41}},
Pages = {{19-24}},
Month = {{OCT}},
Abstract = {{Purpose: BCCT.core (Breast Cancer Conservative Treatment. cosmetic
   results) is a software created for the objective evaluation of aesthetic
   result of breast cancer conservative treatment using a single patient
   frontal photography. The lack of volume information has been one
   criticism, as the use of 3D information might improve accuracy in
   aesthetic evaluation. In this study, we have evaluated the added value
   of 3D information to two methods of aesthetic evaluation: a panel of
   experts; and an augmented version of the computational model -
   BCCT.core3d.
   Material and methods: Within the scope of EU Seventh Framework Programme
   Project PICTURE, 2D and 3D images from 106 patients from three clinical
   centres were evaluated by a panel of 17 experts and the BCCT.core.
   Agreement between all methods was calculated using the kappa (K) and
   weighted kappa (wK) statistics.
   Results: Subjective agreement between 2D and 3D individual evaluation
   was fair to moderate. The agreement between the expert classification
   and the BCCT.core software with both 2D and 3D features was also fair to
   moderate.
   Conclusions: The inclusion of 3D images did not add significant
   information to the aesthetic evaluation either by the panel or the
   software. Evaluation of aesthetic outcome can be performed using of the
   BCCT.core software, with a single frontal image. (C) 2018 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.breast.2018.06.008}},
ISSN = {{0960-9776}},
EISSN = {{1532-3080}},
ResearcherID-Numbers = {{WILLIAMS, Norman/C-2002-2008
   Cardoso, Jaime/I-3286-2013
   Oliveira, Helder/M-9956-2017}},
ORCID-Numbers = {{WILLIAMS, Norman/0000-0001-6496-312X
   Cardoso, Jaime/0000-0002-3760-2473
   Cardoso, Maria Joao/0000-0002-8137-3700
   Oliveira, Helder/0000-0002-6193-8540}},
Unique-ID = {{ISI:000442764500004}},
}

@article{ ISI:000440851800017,
Author = {Siqueira, Robson S. and Alexandre, Gilderlane R. and Soares, Jose M. and
   The, George A. P.},
Title = {{Triaxial Slicing for 3-D Face Recognition From Adapted Rotational
   Invariants Spatial Moments and Minimal Keypoints Dependence}},
Journal = {{IEEE ROBOTICS AND AUTOMATION LETTERS}},
Year = {{2018}},
Volume = {{3}},
Number = {{4}},
Pages = {{3513-3520}},
Month = {{OCT}},
Abstract = {{This letter presents a multiple slicing model for three-dimensional
   (3-D) images of human face, using the frontal, sagittal, and transverse
   orthogonal planes. The definition of the segments depends on just one
   key point, the nose tip, which makes it simple and independent of the
   detection of several key points. For facial recognition, attributes
   based on adapted 2-D spatial moments of Hu and 3-D spatial invariant
   rotation moments are extracted from each segment. Tests with the
   proposed model using the Bosphorus Database for neutral vs nonneutral
   ROC I experiment, applying linear discriminant analysis as classifier
   and more than one sample for training, achieved 98.7\% of verification
   rate at 0.1\% of false acceptance rate. By using the support vector
   machine as classifier the rank1 experiment recognition rates of 99\% and
   95.4\% have been achieved for a neutral vs neutral and for a neutral vs
   non neutral, respectively. These results approach the state-of-the-art
   using Bosphorus Database and even surpasses it when anger and disgust
   expressions are evaluated. In addition, we also evaluate the
   generalization of our method using the FRGC v2.0 database and achieve
   competitive results, making the technique promising, especially for its
   simplicity.}},
DOI = {{10.1109/LRA.2018.2854295}},
ISSN = {{2377-3766}},
ORCID-Numbers = {{Marques Soares, Jose/0000-0002-5111-5794
   Alexandre, Gilderlane/0000-0002-8778-5351
   The, George/0000-0002-8064-8901}},
Unique-ID = {{ISI:000440851800017}},
}

@article{ ISI:000449993800083,
Author = {Wu, Jianwei and Yao, Wei and Polewski, Przemyslaw},
Title = {{Mapping Individual Tree Species and Vitality along Urban Road Corridors
   with LiDAR and Imaging Sensors: Point Density versus View Perspective}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{To meet a growing demand for accurate high-fidelity vegetation cover
   mapping in urban areas toward biodiversity conservation and assessing
   the impact of climate change, this paper proposes a complete approach to
   species and vitality classification at single tree level by synergistic
   use of multimodality 3D remote sensing data. So far, airborne laser
   scanning system (ALS or airborne LiDAR) has shown promising results in
   tree cover mapping for urban areas. This paper analyzes the potential of
   mobile laser scanning system/mobile mapping system (MLS/MMS)-based
   methods for recognition of urban plant species and characterization of
   growth conditions using ultra-dense LiDAR point clouds and provides an
   objective comparison with the ALS-based methods. Firstly, to solve the
   extremely intensive computational burden caused by the classification of
   ultra-dense MLS data, a new method for the semantic labeling of LiDAR
   data in the urban road environment is developed based on combining a
   conditional random field (CRF) for the context-based classification of
   3D point clouds with shape priors. These priors encode geometric
   primitives found in the scene through sample consensus segmentation.
   Then, single trees are segmented from the labelled tree points using the
   3D graph cuts algorithm. Multinomial logistic regression classifiers are
   used to determine the fine deciduous urban tree species of conversation
   concern and their growth vitality. Finally, the weight-of-evidence
   (WofE) based decision fusion method is applied to combine the
   probability outputs of classification results from the MLS and ALS data.
   The experiment results obtained in city road corridors demonstrated that
   point cloud data acquired from the airborne platform achieved even
   slightly better results in terms of tree detection rate, tree species
   and vitality classification accuracy, although the tree vitality
   distribution in the test site is less balanced compared to the species
   distribution. When combined with MLS data, overall accuracies of 78\%
   and 74\% for tree species and vitality classification can be achieved,
   which has improved by 5.7\% and 4.64\% respectively compared to the
   usage of airborne data only.}},
DOI = {{10.3390/rs10091403}},
Article-Number = {{1403}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Yao, Wei/E-8520-2017}},
ORCID-Numbers = {{Yao, Wei/0000-0001-7704-0615}},
Unique-ID = {{ISI:000449993800083}},
}

@article{ ISI:000448227000069,
Author = {Ahmadi, Bahar and Javidi, Bahram and Shahbazmohamadi, Sina},
Title = {{Automated detection of counterfeit ICs using machine learning}},
Journal = {{MICROELECTRONICS RELIABILITY}},
Year = {{2018}},
Volume = {{88-90}},
Number = {{SI}},
Pages = {{371-377}},
Month = {{SEP}},
Note = {{29th European Symposium on the Reliability of Electron Devices, Failure
   Physics and Analysis (ESREF), Aalborg, DENMARK, OCT 01-05, 2018}},
Abstract = {{The electronic industry has been experiencing a growing counterfeit
   market, resulting in electronic supply chains in other industries to be
   prone to counterfeit parts as well. Over the past few years, several
   methods have been developed for evaluating the reliability of an IC and
   distinguishing them as counterfeit or authentic. Trained experts offer
   services for evaluating an IC based on destructive or non-destructive
   methods. However, defect detection and recognition are mostly dependent
   on human decision, and therefore are vulnerable to error. In this paper,
   we propose a method to automatically detect and identify die-face
   delamination on an IC die. Die-face delamination is a predominant
   internal defect in recycled ICs but can be easily missed during defect
   detection. Here, we have acquired the 3D image of an IC
   non-destructively using X-ray computed tomography and applied image
   processing techniques and machine learning algorithms on the 3D image to
   detect die-face delamination in the forms of thermally induced cracks
   and damaged surfaces.}},
DOI = {{10.1016/j.microrel.2018.06.083}},
ISSN = {{0026-2714}},
Unique-ID = {{ISI:000448227000069}},
}

@article{ ISI:000445436100006,
Author = {Halik, Lukasz and Smaczynski, Maciej},
Title = {{Geovisualisation of Relief in a Virtual Reality System on the Basis of
   Low-Level Aerial Imagery}},
Journal = {{PURE AND APPLIED GEOPHYSICS}},
Year = {{2018}},
Volume = {{175}},
Number = {{9}},
Pages = {{3209-3221}},
Month = {{SEP}},
Abstract = {{The aim of the following paper was to present the geomatic process of
   transforming low-level aerial imagery obtained with unmanned aerial
   vehicles (UAV) into a digital terrain model (DTM) and implementing the
   model into a virtual reality system (VR). The object of the study was a
   natural aggretage heap of an irregular shape and denivelations up to 11
   m. Based on the obtained photos, three point clouds (varying in the
   level of detail) were generated for the 20,000-m(2)-area. For further
   analyses, the researchers selected the point cloud with the best ratio
   of accuracy to output file size. This choice was made based on seven
   control points of the heap surveyed in the field and the corresponding
   points in the generated 3D model. The obtained several-centimetre
   differences between the control points in the field and the ones from
   the model might testify to the usefulness of the described algorithm for
   creating large-scale DTMs for engineering purposes. Finally, the chosen
   model was implemented into the VR system, which enables the most
   lifelike exploration of 3D terrain plasticity in real time, thanks to
   the first person view mode (FPV). In this mode, the user observes an
   object with the aid of a Head- mounted display (HMD), experiencing the
   geovisualisation from the inside, and virtually analysing the terrain as
   a direct animator of the observations.}},
DOI = {{10.1007/s00024-017-1755-z}},
ISSN = {{0033-4553}},
EISSN = {{1420-9136}},
Unique-ID = {{ISI:000445436100006}},
}

@article{ ISI:000445204800002,
Author = {Wang, Jinhu and Lindenbergh, Roderik and Menenti, Massimo},
Title = {{Scalable individual tree delineation in 3D point clouds}},
Journal = {{PHOTOGRAMMETRIC RECORD}},
Year = {{2018}},
Volume = {{33}},
Number = {{163}},
Pages = {{315-340}},
Month = {{SEP}},
Abstract = {{Manually monitoring and documenting trees is labour intensive. Lidar
   provides a possible solution for automatic tree-inventory generation.
   Existing approaches for segmenting trees from original point cloud data
   lack scalable and efficient methods that separate individual trees
   sampled by different laser-scanning systems with sufficient quality
   under all circumstances. In this study a new algorithm for efficient
   individual tree delineation from lidar point clouds is presented and
   validated. The proposed algorithm first resamples the points using
   cuboid (modified voxel) cells. Consecutively connected cells are
   accumulated by vertically traversing cell layers. Trees in close
   proximity are identified, based on a novel cell-adjacency analysis. The
   scalable performance of this algorithm is validated on airborne, mobile
   and terrestrial laser-scanning point clouds. Validation against ground
   truth demonstrates an improvement from 89\% to 94\% relative to a
   state-of-the-art method while computation time is similar.
   Resume La detection et la documentation manuelle des arbres est une
   tache fastidieuse. Le lidar offre une solution possible pour
   l'inventaire automatique des arbres. Les approches existantes pour la
   segmentation des arbres dans des nuages bruts de points ne proposent pas
   de methodes efficaces et adaptees a toutes les echelles pour separer des
   arbres individuels echantillonnes par differents systemes lidar avec une
   qualite acceptable en toute circonstance. Cette etude propose et valide
   un nouvel algorithme pour la delimitation efficace d'arbres individuels
   a partir de nuages de points lidar. L'algorithme propose commence par
   reechantillonner les points dans des cellules cubiques (voxels), puis
   regroupe les cellules connexes en traversant verticalement les couches
   de cellules. Les arbres proches sont identifies grace a une nouvelle
   analyse d'adjacence de cellules. La performance de cetalgorithme en
   termes d'adaptabilite au changement d'echelle est validee a partir de
   nuages de points issus de systemes laser a balayage aerien, mobile et
   terrestre. Une validation basee sur des donnees de terrain de reference
   fait etat d'une amelioration de 89\% a 94\% par rapport a des methodes
   connues pour un temps de calcul comparable.
   Zusammenfassung Eine uberwachung und Dokumentation von Baumen ist sehr
   arbeitsaufwandig. Lidar bietet das Potential fur automatische
   Bauminventur. Es gibt Ansatze zur Segmentierung von Baumen aus
   Punktwolken, die allerdings noch nicht in der Lage sind, einzelne Baume
   in Punktwolken verschiedener Lidar-Systeme zuverlassig und mit
   ausreichender Qualitat unter vielfaltigen realen Bedingungen zu
   separieren. Diese Studie stellt einen neuen Algorithmus zur effizienten
   Erfassung von Baumen in Lidar-Punktwolken dar. Der Algorithmus bildet
   Punkte mit Hilfe von quaderformigen (Voxel) Zellen um. Nacheinander
   verbundene Zellen werden durch vertikale Traverse der Zellschichten
   akkumuliert. Baume in nachster Nachbarschaft werden durch eine neuartige
   Zellanalyse identifiziert. Der Vorteil der Skalierbarkeit des
   Algorithmus wird an flugzeuggestutzten, mobilen und terrestrischen
   Laserscanpunktwolken validiert. Anhand von Solldaten ist festzustellen,
   dass bei gleicher Rechenzeit, eine Verbesserung von 89\% bis 94\% im
   Vergleich zu aktuellen Verfahren erzielt werden kann.
   Resumen Monitorizar y documentar manualmente arboles es un trabajo
   intensivo. El lidar proporciona una posible solucion para la generacion
   automatica del inventario de arboles. Los enfoques existentes para
   segmentar arboles a partir originalmente de nubes de puntos lidar
   carecen de metodos escalables y eficientes que separen arboles
   individuales muestreados por diferentes sistemas lidar con calidad
   suficiente bajo todas las circunstancias. En este estudio, se presenta y
   valida un algoritmo nuevo para la delimitacion eficiente de arboles
   individuales a partir de nubes de puntos lidar. El algoritmo propuesto
   primero remuestrea los puntos usando celulas cuboides (voxels). Los
   voxels adyacentes se acumulan atravesando verticalmente las capas de
   voxels. Basados en un nuevo analisis de adyacencia de voxels se
   identifican arboles que estan proximos. El rendimiento escalable de este
   algoritmo se valida con nubes de puntos lidar aerotransportados, moviles
   y terrestres. La validacion con verdad terreno demuestra una mejora del
   89\% al 94\% en comparacion con un metodo de vanguardia, mientras que el
   tiempo de calculo es similar.}},
DOI = {{10.1111/phor.12247}},
ISSN = {{0031-868X}},
EISSN = {{1477-9730}},
ORCID-Numbers = {{Lindenbergh, Roderik/0000-0001-8655-5266}},
Unique-ID = {{ISI:000445204800002}},
}

@article{ ISI:000442238900004,
Author = {Switonski, Adam and Krzeszowski, Tomasz and Josinski, Henryk and Kwolek,
   Bogdan and Wojciechowski, Konrad},
Title = {{Gait recognition on the basis of markerless motion tracking and DTW
   transform}},
Journal = {{IET BIOMETRICS}},
Year = {{2018}},
Volume = {{7}},
Number = {{5}},
Pages = {{415-422}},
Month = {{SEP}},
Abstract = {{In this study, a framework for view-invariant gait recognition on the
   basis of markerless motion tracking and dynamic time warping (DTW)
   transform is presented. The system consists of a proposed markerless
   motion capture system as well as introduced classification method of
   mocap data. The markerless system estimates the three-dimensional
   locations of skeleton driven joints. Such skeleton-driven point clouds
   represent poses over time. The authors align point clouds in every pair
   of frames by calculating the minimal sum of squared distances between
   the corresponding joints. A point cloud distance measure with temporal
   context has been utilised in k-nearest neighbours algorithm to compare
   time instants of motion sequences. To enhance the generalisation of the
   recognition and to shorten the processing time, for every individual a
   single multidimensional time series among several multidimensional time
   series describing the individual's gait is established. The correct
   classification rate has been determined on the basis of a real dataset
   of human gait. It contains 230 gait cycles of 22 subjects. The tracking
   results on the basis of markerless motion capture are referenced to
   Vicon system, whereas the achieved accuracies of recognition are
   compared with the ones obtained by DTW that is based on rotational data.}},
DOI = {{10.1049/iet-bmt.2017.0134}},
ISSN = {{2047-4938}},
EISSN = {{2047-4946}},
ResearcherID-Numbers = {{Krzeszowski, Tomasz/H-7717-2019}},
ORCID-Numbers = {{Krzeszowski, Tomasz/0000-0001-7359-4637}},
Unique-ID = {{ISI:000442238900004}},
}

@article{ ISI:000440350800018,
Author = {van Veen, Martinus M. and Korteweg, Steven F. S. and Dijkstra, Pieter U.
   and Werker, Paul M. N.},
Title = {{Keeping the fat on the right spot prevents contour deformity in
   temporalis muscle transposition}},
Journal = {{JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY}},
Year = {{2018}},
Volume = {{71}},
Number = {{8}},
Pages = {{1181-1187}},
Month = {{AUG}},
Abstract = {{The temporalis muscle transposition is a reliable, one-stage reanimation
   technique for longstanding facial paralysis. In the variation described
   by Rubin, the muscle is released from the temporal bone and folded over
   the zygomatic arch towards the modiolus. This results in unsightly
   temporal hollowing and zygomatic bulging. We present a modification of
   this technique, which preserves the temporal fat pad in its anatomical
   location as well as conceals temporal hollowing and prevents zygomatic
   bulging.
   The data of 23 patients treated with this modification were analysed.
   May classification was used for evaluation of mouth reanimation. Experts
   and patients scored visibility of the contour deformity on a 100-mm
   visual analogue scale (VAS) (score 0 = poor/100 = best). 3D images of
   the face were used to measure temporal hollowing and zygomatic bulging.
   3D images were compared to those of controls with a similar gender and
   age distribution.
   After a median follow-up of 5.7 years, all patients achieved symmetry at
   rest. Eleven patients achieved symmetry while smiling with closed lips
   (May classification ``Good{''}). A median (interquartile range {[}IQR])
   VAS score of 19 (6; 41) was given by experts and 25 (5; 59) by patients
   themselves. 3D volumes of zygomatic bulging differed from those of
   control subjects, although all volume differences were small (median
   <3.3 ml) and temporal hollowing did not differ significantly.
   On the basis of our results, we conclude that our modified Rubin
   temporalis transposition technique provides an elegant way to conceal
   bulging over the zygomatic arch and prevents temporal hollowing, without
   the need for fascial extensions to reach the modiolus. (C) 2018 British
   Association of Plastic, Reconstructive and Aesthetic Surgeons. Published
   by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.bjps.2018.04.007}},
ISSN = {{1748-6815}},
EISSN = {{1878-0539}},
ResearcherID-Numbers = {{Werker, Paul/L-6293-2019}},
Unique-ID = {{ISI:000440350800018}},
}

@article{ ISI:000439703500027,
Author = {Gibelli, Daniele and Pucciarelli, Valentina and Cappella, Annalisa and
   Dolci, Claudia and Sforza, Chiarella},
Title = {{Are Portable Stereophotogrammetric Devices Reliable in Facial Imaging? A
   Validation Study of VECTRA H1 Device}},
Journal = {{JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{76}},
Number = {{8}},
Pages = {{1772-1784}},
Month = {{AUG}},
Abstract = {{Purpose: Modern 3-dimensional (3D) image acquisition systems represent a
   crucial technologic development in facial anatomy because of their
   accuracy and precision. The recently introduced portable devices can
   improve facial databases by increasing the number of applications. In
   the present study, the VECTRA H1 portable stereophotogrammetric device
   was validated to verify its applicability to 3D facial analysis.
   Materials and Methods: Fifty volunteers underwent 4 facial scans using
   portable VECTRA H1 and static VECTRA M3 devices (2 for each instrument).
   Repeatability of linear, angular, surface area, and volume measurements
   was verified within the device and between devices using the
   Bland-Altman test and the calculation of absolute and relative technical
   errors of measurement (TEM and rTEM, respectively). In addition, the 2
   scans obtained by the same device and the 2 scans obtained by different
   devices were registered and superimposed to calculate the root mean
   square (RMS; point-to-point) distance between the 2 surfaces.
   Results: Most linear, angular, and surface area measurements had high
   repeatability in M3 versus M3, H1 versus H1, and M3 versus H1
   comparisons (range, 82.2 to 98.7\%; TEM range, 0.3 to 2.0 mm, 0.4
   degrees to 1.8 degrees; rTEM range, 0.2 to 3.1\%). In contrast, volumes
   and RMS distances showed evident differences in M3 versus M3 and H1
   versus H1 comparisons and reached the maximum when scans from the 2
   different devices were compared.
   Conclusion: The portable VECTRA H1 device proved reliable for assessing
   linear measurements, angles, and surface areas; conversely, the
   influence of involuntary facial movements on volumes and RMS distances
   was more important compared with the static device. (C) 2018 American
   Association of Oral and Maxillofacial Surgeons}},
DOI = {{10.1016/j.joms.2018.01.021}},
ISSN = {{0278-2391}},
EISSN = {{1531-5053}},
ResearcherID-Numbers = {{Cappella, Annalisa/V-5586-2017
   Sforza, Chiarella/C-3008-2015}},
ORCID-Numbers = {{Sforza, Chiarella/0000-0001-6532-6464}},
Unique-ID = {{ISI:000439703500027}},
}

@article{ ISI:000435048200005,
Author = {Reiser, David and Vazquez-Arellano, Manuel and Paraforos, Dimitris S.
   and Garrido-Izard, Miguel and Griepentrog, Hans W.},
Title = {{Iterative individual plant clustering in maize with assembled 2D LiDAR
   data}},
Journal = {{COMPUTERS IN INDUSTRY}},
Year = {{2018}},
Volume = {{99}},
Pages = {{42-52}},
Month = {{AUG}},
Abstract = {{A two dimensional (2D) laser scanner was mounted at the front part of a
   small 4-wheel autonomous robot with differential steering, at an angle
   of 30 degrees pointing downwards. The machine was able to drive between
   maize rows and collect concurrent time-stamped data. A robotic total
   station tracked the position of a prism mounted on the vehicle. The
   total station and laser scanner data were fused to generate a three
   dimensional (3D) point cloud. This 3D representation was used to detect
   individual plant positions, which are of particular interest for
   applications such as phenotyping, individual plant treatment and
   precision weeding. Two different methodologies were applied to the 3D
   point cloud to estimate the position of the individual plants. The first
   methodology used the Euclidian Clustering on the entire point cloud. The
   second methodology utilised the position of an initial plant and the
   fixed plant spacing to search iteratively for the best clusters. The two
   algorithms were applied at three different plant growth stages. For the
   first method, results indicated a detection rate up to 73.7\% with a
   root mean square error of 3.6 cm. The second method was able to detect
   all plants (100\% detection rate) with an accuracy of 2.7-3.0 cm, taking
   the plant spacing of 13 cm into account.}},
DOI = {{10.1016/j.compind.2018.03.023}},
ISSN = {{0166-3615}},
EISSN = {{1872-6194}},
ORCID-Numbers = {{Paraforos, Dimitrios S./0000-0001-8275-8840
   Reiser, David/0000-0003-0158-6456}},
Unique-ID = {{ISI:000435048200005}},
}

@article{ ISI:000437024600042,
Author = {Wang, Yuehang and Li, Zhengxiong and Vu, Tri and Nyayapathi, Nikhila and
   Oh, Kwang W. and Xu, Wenyao and Xia, Jun},
Title = {{A Robust and Secure Palm Vessel Biometric Sensing System Based on
   Photoacoustics}},
Journal = {{IEEE SENSORS JOURNAL}},
Year = {{2018}},
Volume = {{18}},
Number = {{14}},
Pages = {{5993-6000}},
Month = {{JUL 15}},
Abstract = {{In this paper, we propose a new palm vessel bio-metric sensing system
   based on photoacoustic imaging, which is an emerging technique that
   allows high-resolution visualization of optical absorption in deep
   tissue. Our system consists of an ultrasound (US) linear transducer
   array, an US data acquisition system, and an Nd:YAG laser emitting
   1064-nm wavelength. By scanning the array, we could get a 3-D image of
   palm vasculature. The 3-D image is further combined with our newly
   developed algorithm, Earth Mover's Distance-Radiographic Testing, to
   provide precise matching and robust recognition rate. Compared to
   conventional vein sensing techniques, our system demonstrates deeper
   imaging depth and better spatial resolution, offering securer biometric
   features to fight against counterfeits. In this paper, we imaged 20
   different hands at various poses and quantified our system performance.
   We found that the usability and accuracy of our system are comparable to
   conventional biometric techniques, such as fingerprint imaging and face
   identification. Our technique can open up avenues for better liveness
   detection and biometric measurements.}},
DOI = {{10.1109/JSEN.2018.2843119}},
ISSN = {{1530-437X}},
EISSN = {{1558-1748}},
ORCID-Numbers = {{Nyayapathi, Nikhila/0000-0002-3711-797X}},
Unique-ID = {{ISI:000437024600042}},
}

@article{ ISI:000451673800001,
Author = {Chen, Ying and Haerdie, Wolfgang K. and He, Qiang and Majer, Piotr},
Title = {{Risk related brain regions detection and individual risk classification
   with 3D image FPCA}},
Journal = {{STATISTICS \& RISK MODELING}},
Year = {{2018}},
Volume = {{35}},
Number = {{3-4}},
Pages = {{89-110}},
Month = {{JUL}},
Abstract = {{Understanding how people make decisions from risky choices has attracted
   increasing attention of researchers in economics, psychology and
   neuroscience. While economists try to evaluate individual's risk
   preference through mathematical modeling, neuroscientists answer the
   question by exploring the neural activities of the brain. We propose a
   model-free method, 3-dimensional image functional principal component
   analysis (3DIF), to provide a connection between active risk related
   brain region detection and individual's risk preference. The 3DIF
   methodology is directly applicable to 3-dimensional image data without
   artificial vectorization or mapping and simultaneously guarantees the
   contiguity of risk related brain regions rather than discrete voxels.
   Simulation study evidences an accurate and reasonable region detection
   using the 3DIF method. In real data analysis, five important risk
   related brain regions are detected, including parietal cortex (PC),
   ventrolateral prefrontal cortex (VLPFC), lateral orbifrontal cortex
   (IOFC), anterior insula (aINS) and dorsolateral prefrontal cortex
   (DLPFC), while the alternative methods only identify limited risk
   related regions. Moreover, the 3DIF method is useful for extraction of
   subjective specific signature scores that carry explanatory power for
   individual's risk attitude. In particular, the 3DIF method perfectly
   classifies both strongly and weakly risk averse subjects for in-sample
   analysis. In out-of-sample experiment, it achieves 73\%-88\% overall
   accuracy, among which 90\%-100\% strongly risk averse subjects and
   49\%-71\% weakly risk averse subjects are correctly classified with
   leave-k-out cross validations.}},
DOI = {{10.1515/strm-2017-0011}},
ISSN = {{2193-1402}},
EISSN = {{2196-7040}},
ORCID-Numbers = {{Chen, Ying/0000-0002-2577-7348}},
Unique-ID = {{ISI:000451673800001}},
}

@article{ ISI:000441334300307,
Author = {Rymarczyk, Tomasz and Klosowski, Grzegorz and Kozlowski, Edward},
Title = {{A Non-Destructive System Based on Electrical Tomography and Machine
   Learning to Analyze the Moisture of Buildings}},
Journal = {{SENSORS}},
Year = {{2018}},
Volume = {{18}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{This article presents the results of research on a new method of spatial
   analysis of walls and buildings moisture. Due to the fact that
   destructive methods are not suitable for historical buildings of great
   architectural significance, a non-destructive method based on electrical
   tomography has been adopted. A hybrid tomograph with special sensors was
   developed for the measurements. This device enables the acquisition of
   data, which are then reconstructed by appropriately developed methods
   enabling spatial analysis of wet buildings. Special electrodes that
   ensure good contact with the surface of porous building materials such
   as bricks and cement were introduced. During the research, a group of
   algorithms enabling supervised machine learning was analyzed. They have
   been used in the process of converting input electrical values into
   conductance depicted by the output image pixels. The conductance values
   of individual pixels of the output vector made it possible to obtain
   images of the interior of building walls as both flat intersections (2D)
   and spatial (3D) images. The presented group of algorithms has a high
   application value. The main advantages of the new methods are: high
   accuracy of imaging, low costs, high processing speed, ease of
   application to walls of various thickness and irregular surface. By
   comparing the results of tomographic reconstructions, the most efficient
   algorithms were identified.}},
DOI = {{10.3390/s18072285}},
Article-Number = {{2285}},
ISSN = {{1424-8220}},
ResearcherID-Numbers = {{Klosowski, Grzegorz/B-8899-2017
   Kozlowski, Edward/A-6882-2013
   Rymarczyk, Tomasz/D-6177-2015}},
ORCID-Numbers = {{Klosowski, Grzegorz/0000-0001-7927-3674
   Kozlowski, Edward/0000-0002-7147-4903
   Rymarczyk, Tomasz/0000-0002-3524-9151}},
Unique-ID = {{ISI:000441334300307}},
}

@article{ ISI:000440122900009,
Author = {Huang, Delin and Du, Shichang and Li, Guilong and Zhao, Chen and Deng,
   Yafei},
Title = {{Detection and monitoring of defects on three-dimensional curved surfaces
   based on high-density point cloud data}},
Journal = {{PRECISION ENGINEERING-JOURNAL OF THE INTERNATIONAL SOCIETIES FOR
   PRECISION ENGINEERING AND NANOTECHNOLOGY}},
Year = {{2018}},
Volume = {{53}},
Pages = {{79-95}},
Month = {{JUL}},
Abstract = {{The surface quality of three-dimensional (3-D) curved surfaces is one of
   the most important factors that can directly influence the performance
   of the final product. This paper presents a systematic approach for
   detection and monitoring of defects on 3-D curved surfaces based on
   high-density point cloud data. Firstly, an algorithm to remove outliers
   and a boundary recognition algorithm are proposed to divide the entire
   3-D curved surface including millions of measured points into multiple
   sub-regions. Secondly, two new evaluation indexes based on wavelet
   packet entropy and normal vector are explored to represent the features
   of the multiple sub-regions to determine whether the sub-regions are
   out-of-limit (OOL) of specifications. Thirdly, three quality parameters
   representing quality characteristics of a curved surface are presented
   and their values are calculated based on the clusters of OOL
   sub-regions. Finally, three individual control charts are presented to
   monitor the three quality parameters. As long as any quality parameter
   is out of the control range, the manufacturing process of the curved
   surface is determined to be out-of-control (OOC). The results of a case
   study show that the proposed approach can effectively identify the OOC
   manufacturing process and detect defects on 3-D curved surfaces.}},
DOI = {{10.1016/j.precisioneng.2018.03.001}},
ISSN = {{0141-6359}},
EISSN = {{1873-2372}},
Unique-ID = {{ISI:000440122900009}},
}

@article{ ISI:000434294800004,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal and Shafait, Faisal and Reid,
   Ian},
Title = {{Dense 3D Face Correspondence}},
Journal = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}},
Year = {{2018}},
Volume = {{40}},
Number = {{7}},
Pages = {{1584-1598}},
Month = {{JUL}},
Abstract = {{We present an algorithm that automatically establishes dense
   correspondences between a large number of 3D faces. Starting from
   automatically detected sparse correspondences on the outer boundary of
   3D faces, the algorithm triangulates existing correspondences and
   expands them iteratively by matching points of distinctive surface
   curvature along the triangle edges. After exhausting keypoint matches,
   further correspondences are established by generating evenly distributed
   points within triangles by evolving level set geodesic curves from the
   centroids of large triangles. A deformable model (K3DM) is constructed
   from the dense corresponded faces and an algorithm is proposed for
   morphing the K3DM to fit unseen faces. This algorithm iterates between
   rigid alignment of an unseen face followed by regularized morphing of
   the deformable model. We have extensively evaluated the proposed
   algorithms on synthetic data and real 3D faces from the FRGCv2,
   Bosphorus, BU3DFE and UND Ear databases using quantitative and
   qualitative benchmarks. Our algorithm achieved dense correspondences
   with a mean localisation error of 1.28 mm on synthetic faces and
   detected 14 anthropometric landmarks on unseen real faces from the
   FRGCv2 database with 3 mm precision. Furthermore, our deformable model
   fitting algorithm achieved 98.5 percent face recognition accuracy on the
   FRGCv2 and 98.6 percent on Bosphorus database. Our dense model is also
   able to generalize to unseen datasets.}},
DOI = {{10.1109/TPAMI.2017.2725279}},
ISSN = {{0162-8828}},
EISSN = {{1939-3539}},
ORCID-Numbers = {{Reid, Ian/0000-0001-7790-6423
   Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000434294800004}},
}

@article{ ISI:000435372100001,
Author = {Burkus, Mate and Schlegl, Adam Tibor and O'Sullivan, Ian and Markus,
   Istvan and Vermes, Csaba and Tunyogi-Csapo, Miklos},
Title = {{Sagittal plane assessment of spino-pelvic complex in a Central European
   population with adolescent idiopathic scoliosis: a case control study}},
Journal = {{SCOLIOSIS AND SPINAL DISORDERS}},
Year = {{2018}},
Volume = {{13}},
Month = {{JUN 14}},
Abstract = {{Background: Scoliosis is a complex three-dimensional deformity. While
   the frontal profile is well understood, increasing attention has turned
   to balance in the sagittal plane. The present study evaluated changes in
   sagittal spino-pelvic parameters in a large Hungarian population with
   adolescent idiopathic scoliosis.
   Methods: EOS 2D/3D images of 458 scoliotic and 69 control cases were
   analyzed. After performing 3D reconstructions, the sagittal parameters
   were assessed as a whole and by curve type using independent sample t
   test and linear regression analysis.
   Results: Patients with scoliosis had significantly decreased thoracic
   kyphosis (p < 0.001) with values T1-T12, 34.1 +/- 17.1 degrees vs. 43.4
   +/- 12.7 degrees in control; T4-T12, 27.1 +/- 18.8 degrees vs. 37.7 +/-
   15.1 degrees in control; and T5-T12, 24.9 +/- 15.8 degrees vs. 32.9 +/-
   15. 0 degrees in control. Changes in thoracic kyphosis correlated with
   magnitude of the Cobb angle (p < 0.001). No significant change was found
   in lumbar lordosis and the pelvic parameters. After substratification
   according to the Lenke classification and individually evaluating
   subgroups, results were similar with a significant decrease in only the
   thoracic kyphosis. A strong correlation was seen between sacral slope,
   pelvic incidence, and lumbar lordosis, and between pelvic version and
   thoracic kyphosis in control and scoliotic groups, whereas pelvic
   incidence was also seen to be correlated with thoracic kyphosis in
   scoliosis patients.
   Conclusion: Adolescent idiopathic scoliosis patients showed a
   significant decrease in thoracic kyphosis, and the magnitude of the
   decrease was directly related to the Cobb angle. Changes in pelvic
   incidence were minimal but were also significantly correlated with
   thoracic changes. Changes were similar though not identical to those
   seen in other Caucasian studies and differed from those in other
   ethnicities. Scoliotic curves and their effect on pelvic balance must
   still be regarded as individual to each patient, necessitating
   individual assessment, although changes perhaps can be predicted by
   patient ethnicity.}},
DOI = {{10.1186/s13013-018-0156-0}},
Article-Number = {{10}},
ISSN = {{2397-1789}},
ORCID-Numbers = {{Schlegl, Adam Tibor/0000-0003-0349-2525}},
Unique-ID = {{ISI:000435372100001}},
}

@article{ ISI:000440272800007,
Author = {Kwiek, Bartlomiej and Ambroziak, Marcin and Osipowicz, Katarzyna and
   Kowalewski, Cezary and Rozalski, Michal},
Title = {{Treatment of Previously Treated Facial Capillary Malformations: Results
   of Single-Center Retrospective Objective 3-Dimensional Analysis of the
   Efficacy of Large Spot 532 nm Lasers}},
Journal = {{DERMATOLOGIC SURGERY}},
Year = {{2018}},
Volume = {{44}},
Number = {{6}},
Pages = {{803-813}},
Month = {{JUN}},
Abstract = {{BACKGROUND Current treatment of facial capillary malformations (CM) has
   limited efficacy.
   OBJECTIVE To assess the efficacy of large spot 532 nm lasers for the
   treatment of previously treated facial CM with the use of 3-dimensional
   (3D) image analysis.
   PATIENTS AND METHODS Forty-three white patients aged 6 to 59 were
   included in this study. Patients had 3D photography performed before and
   after treatment with a 532 nm Nd:YAG laser with large spot and contact
   cooling. Objective analysis of percentage improvement based on 3D
   digital assessment of combined color and area improvement (global
   clearance effect {[}GCE]) were performed.
   RESULTS The median maximal improvement achieved during the treatment
   (GCE(max)) was 59.1\%. The mean number of laser procedures required to
   achieve this improvement was 6.2 (range 1-16). Improvement of minimum
   25\% (GCE25) was achieved by 88.4\% of patients, a minimum of 50\%
   (GCE50) by 61.1\%, a minimum of 75\% (GCE75) by 25.6\%, and a minimum of
   90\% (GCE90) by 4.6\%. Patients previously treated with pulsed dye
   lasers had a significantly less response than those treated with other
   modalities (GCE (max) 37.3\% vs 61.8\%, respectively).
   CONCLUSION A large spot 532 nm laser is effective in previously treated
   patients with facial CM.}},
DOI = {{10.1097/DSS.0000000000001447}},
ISSN = {{1076-0512}},
EISSN = {{1524-4725}},
ORCID-Numbers = {{Kowalewski, Cezary/0000-0002-6608-9066}},
Unique-ID = {{ISI:000440272800007}},
}

@article{ ISI:000435193700027,
Author = {Wang, Di and Brunner, Jasmin and Ma, Zhenyu and Lu, Hao and Hollaus,
   Markus and Pang, Yong and Pfeifer, Norbert},
Title = {{Separating Tree Photosynthetic and Non-Photosynthetic Components from
   Point Cloud Data Using Dynamic Segment Merging}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{5}},
Month = {{MAY}},
Abstract = {{Many biophysical forest properties such as wood volume and leaf area
   index (LAI) require prior knowledge on either photosynthetic or
   non-photosynthetic components. Laser scanning appears to be a helpful
   technique in nondestructively quantifying forest structures, as it can
   acquire an accurate three-dimensional point cloud of objects. In this
   study, we propose an unsupervised geometry-based method named Dynamic
   Segment Merging (DSM) to identify non-photosynthetic components of trees
   by semantically segmenting tree point clouds, and examining the linear
   shape prior of each resulting segment. We tested our method using one
   single tree dataset and four plot-level datasets, and compared our
   results to a supervised machine learning method. We further demonstrated
   that by using an optimal neighborhood selection method that involves
   multi-scale analysis, the results were improved. Our results showed that
   the overall accuracy ranged from 81.8\% to 92.0\% with an average value
   of 87.7\%. The supervised machine learning method had an average overall
   accuracy of 86.4\% for all datasets, on account of a collection of
   manually delineated representative training data. Our study indicates
   that separating tree photosynthetic and non-photosynthetic components
   from laser scanning data can be achieved in a fully unsupervised manner
   without the need of training data and user intervention.}},
DOI = {{10.3390/f9050252}},
Article-Number = {{252}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Wang, Di/T-2571-2018
   }},
ORCID-Numbers = {{Wang, Di/0000-0003-0232-8862
   Pang, Yong/0000-0002-9760-6580
   Pfeifer, Norbert/0000-0002-2348-7929}},
Unique-ID = {{ISI:000435193700027}},
}

@article{ ISI:000430653400001,
Author = {Lopez, R. and Gantet, P. and Julian, A. and Hitzel, A. and
   Herbault-Barres, B. and Alshehri, S. and Payoux, P.},
Title = {{Value of PET/CT 3D visualization of head and neck squamous cell
   carcinoma extended to mandible}},
Journal = {{JOURNAL OF CRANIO-MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{46}},
Number = {{5}},
Pages = {{743-748}},
Month = {{MAY}},
Abstract = {{Purpose: To study an original 3D visualization of head and neck squamous
   cell carcinoma extending to the mandible by using {[}18F]-NaF PET/CT and
   {[}18F]-FDG PET/CT imaging along with a new innovative FDG and NaF image
   analysis using dedicated software. The main interest of the 3D
   evaluation is to have a better visualization of bone extension in such
   cancers and that could also avoid unsatisfying surgical treatment later
   on. Patients and methods: A prospective study was carried out from
   November 2016 to September 2017. Twenty patients with head and neck
   squamous cell carcinoma extending to the mandible (stage 4 in the UICC
   classification) underwent {[}18F]-NaF and {[}18F]-FDG PET/CT. We
   compared the delineation of 3D quantification obtained with {[}18F]-NaF
   and {[}18F]-FDG PET/CT. In order to carry out this comparison, a method
   of visualisation and quantification of PET images was developed. This
   new approach was based on a process of quantification of radioactive
   activity within the mandibular bone that objectively defined the
   significant limits of this activity on PET images and on a 3D
   visualization. Furthermore, the spatial limits obtained by analysis of
   the PET/CT 3D images were compared to those obtained by
   histopathological examination of mandibular resection which confirmed
   intraosseous extension to the mandible. Results: The {[}18F]-NaF PET/CT
   imaging confirmed the mandibular extension in 85\% of cases and was not
   shown in {[}18F]-FDG PET/CT imaging. The {[}18F]-NaF PET/CT was
   significantly more accurate than {[}18F]-FDG PET/CT in 3D assessment of
   intraosseous extension of head and neck squamous cell carcinoma. This
   new 3D information shows the importance in the imaging approach of
   cancers. All cases of mandibular extension suspected on {[}18F]-NaF
   PET/CT imaging were confirmed based on histopathological results as a
   reference. Conclusions: The {[}18F]-NaF PET/CT 3D visualization should
   be included in the pre-treatment workups of head and neck cancers. With
   the use of a dedicated software which enables objective delineation of
   radioactive activity within the bone, it gives a very encouraging
   results. The {[}18F]-FDG PET/CT appears insufficient to confirm
   mandibular extension. This new 3D simulation management is expected to
   avoid under treatment of patients with intraosseous mandibular extension
   of head and neck cancers. However, there is also a need for a further
   study that will compare the interest of PET/CT and PET/MRI in this
   indication. (C) 2018 European Association for Cranio-Maxillo-Facial
   Surgery. Published by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jcms.2018.02.007}},
ISSN = {{1010-5182}},
EISSN = {{1878-4119}},
Unique-ID = {{ISI:000430653400001}},
}

@article{ ISI:000429200400012,
Author = {Majka, Piotr and Chlodzinska, Natalia and Turlejski, Krzysztof and
   Banasik, Tomasz and Djavadian, Ruzanna L. and Weglarz, Wladyslaw P. and
   Wojcik, Daniel K.},
Title = {{A three-dimensional stereotaxic atlas of the gray short-tailed opossum
   (Monodelphis domestica) brain}},
Journal = {{BRAIN STRUCTURE \& FUNCTION}},
Year = {{2018}},
Volume = {{223}},
Number = {{4}},
Pages = {{1779-1795}},
Month = {{MAY}},
Abstract = {{The gray short-tailed opossum (Monodelphis domestica) is a small
   marsupial gaining recognition as a laboratory animal in biomedical
   research. Despite numerous studies on opossum neuroanatomy, a consistent
   and comprehensive neuroanatomical reference for this species is still
   missing. Here we present the first three-dimensional, multimodal atlas
   of the Monodelphis opossum brain. It is based on four complementary
   imaging modalities: high resolution ex vivo magnetic resonance images,
   micro-computed tomography scans of the cranium, images of the face of
   the cutting block, and series of sections stained with the Nissl method
   and for myelinated fibers. Individual imaging modalities were
   reconstructed into a three-dimensional form and then registered to the
   MR image by means of affine and deformable registration routines. Based
   on a superimposition of the 3D images, 113 anatomical structures were
   demarcated and the volumes of individual regions were measured. The
   stereotaxic coordinate system was defined using a set of cranial
   landmarks: interaural line, bregma, and lambda, which allows for easy
   expression of any location within the brain with respect to the skull.
   The atlas is released under the Creative Commons license and available
   through various digital atlasing web services.}},
DOI = {{10.1007/s00429-017-1540-x}},
ISSN = {{1863-2653}},
EISSN = {{1863-2661}},
ResearcherID-Numbers = {{Weglarz, Wladyslaw/W-5770-2018
   Wojcik, Daniel K/C-6334-2008
   }},
ORCID-Numbers = {{Weglarz, Wladyslaw/0000-0002-3390-3615
   Majka, Piotr/0000-0002-9055-8686
   Wojcik, Daniel K/0000-0003-0812-9872
   Krzysztof, Turlejski/0000-0001-6708-7815
   Djavadian, Ruzanna/0000-0002-0416-0234}},
Unique-ID = {{ISI:000429200400012}},
}

@article{ ISI:000425652100003,
Author = {Li, Zhan and Schaefer, Michael and Strahler, Alan and Schaaf, Crystal
   and Jupp, David},
Title = {{On the utilization of novel spectral laser scanning for
   three-dimensional classification of vegetation elements}},
Journal = {{INTERFACE FOCUS}},
Year = {{2018}},
Volume = {{8}},
Number = {{2}},
Month = {{APR 6}},
Abstract = {{The Dual-Wavelength Echidna Lidar (DWEL), a full waveform terrestrial
   laser scanner (TLS), has been used to scan a variety of forested and
   agricultural environments. From these scanning campaigns, we summarize
   the benefits and challenges given by DWEL's novel coaxial
   dual-wavelength scanning technology, particularly for the
   three-dimensional (3D) classification of vegetation elements.
   Simultaneous scanning at both 1064 nm and 1548 nm by DWEL instruments
   provides a new spectral dimension to TLS data that joins the 3D spatial
   dimension of lidar as an information source. Our point cloud
   classification algorithm explores the utilization of both spectral and
   spatial attributes of individual points from DWEL scans and highlights
   the strengths and weaknesses of each attribute domain. The spectral and
   spatial attributes for vegetation element classification each perform
   better in different parts of vegetation (canopy interior, fine branches,
   coarse trunks, etc.) and under different vegetation conditions (dead or
   live, leaf-on or leaf-off, water content, etc.). These environmental
   characteristics of vegetation, convolved with the lidar instrument
   specifications and lidar data quality, result in the actual capabilities
   of spectral and spatial attributes to classify vegetation elements in 3D
   space. The spectral and spatial information domains thus complement each
   other in the classification process. The joint use of both not only
   enhances the classification accuracy but also reduces its variance
   across the multiple vegetation types we have examined, highlighting the
   value of the DWEL as a new source of 3D spectral information. Wider
   deployment of the DWEL instruments is in practice currently held back by
   challenges in instrument development and the demands of data processing
   required by coaxial dual-or multi-wavelength scanning. But the
   simultaneous 3D acquisition of both spectral and spatial features,
   offered by new multispectral scanning instruments such as the DWEL,
   opens doors to study biophysical and biochemical properties of forested
   and agricultural ecosystems at more detailed scales.}},
DOI = {{10.1098/rsfs.2017.0039}},
Article-Number = {{20170039}},
ISSN = {{2042-8898}},
EISSN = {{2042-8901}},
ORCID-Numbers = {{Schaefer, Michael/0000-0001-6584-9521
   Li, Zhan/0000-0001-6307-5200}},
Unique-ID = {{ISI:000425652100003}},
}

@article{ ISI:000433517100003,
Author = {Demisse, Girum G. and Aouada, Djamila and Ottersten, Bjorn},
Title = {{Deformation-Based 3D Facial Expression Representation}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2018}},
Volume = {{14}},
Number = {{1, S}},
Month = {{APR}},
Abstract = {{We propose a deformation-based representation for analyzing expressions
   fromthree-dimensional (3D) faces. A point cloud of a 3D face is
   decomposed into an ordered deformable set of curves that start from a
   fixed point. Subsequently, a mapping function is defined to identify the
   set of curves with an element of a high-dimensional matrix Lie group,
   specifically the direct product of SE(3). Representing 3D faces as an
   element of a high-dimensional Lie group has two main advantages. First,
   using the group structure, facial expressions can be decoupled from a
   neutral face. Second, an underlying non-linear facial expression
   manifold can be captured with the Lie group and mapped to a linear
   space, Lie algebra of the group. This opens up the possibility of
   classifying facial expressions with linear models without compromising
   the underlying manifold. Alternatively, linear combinations of
   linearised facial expressions can be mapped back from the Lie algebra to
   the Lie group. The approach is tested on the Binghamton University 3D
   Facial Expression (BU-3DFE) and the Bosphorus datasets. The results show
   that the proposed approach performed comparably, on the BU-3DFE dataset,
   without using features or extensive landmark points.}},
DOI = {{10.1145/3176649}},
Article-Number = {{17}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
ResearcherID-Numbers = {{Ottersten, Bjorn/G-1005-2011}},
ORCID-Numbers = {{Ottersten, Bjorn/0000-0003-2298-6774}},
Unique-ID = {{ISI:000433517100003}},
}

@article{ ISI:000424962000005,
Author = {Czerniawski, T. and Sankaran, B. and Nahangi, M. and Haas, C. and Leite,
   F.},
Title = {{6D DBSCAN-based segmentation of building point clouds for planar object
   classification}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2018}},
Volume = {{88}},
Pages = {{44-58}},
Month = {{APR}},
Abstract = {{Due to constraints in manufacturing and construction, buildings and many
   of the manmade objects within them are often rectangular and composed of
   planar parts. Detection and analysis of planes is, therefore, central to
   processing point clouds captured in these spaces. This paper presents a
   study of the semantic information stored in the planar objects of noisy
   building point clouds. The dataset considered is the Scene Meshes
   Dataset with aNNotations (SceneNN), a collection of over 100 indoor
   scenes captured by consumer-grade depth cameras. All planar objects
   within the dataset are detected using a new point cloud segmentation
   method that applies Density Based Spatial Clustering of Applications
   with Noise (DBSCAN) in a six dimensional clustering space. With all
   planes isolated, an extensive list of features describing the planes is
   extracted and studied using feature selection. Then dimensionality
   reduction and unsupervised learning are used to explore the
   discriminative ability of the final feature set as well as emergent
   class groupings. Finally, we train a bagged decision tree classifier
   that achieves 71.2\% accuracy in predicting the object class from which
   individual planes originate.}},
DOI = {{10.1016/j.autcon.2017.12.029}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
ORCID-Numbers = {{Czerniawski, Thomas/0000-0002-7310-6522}},
Unique-ID = {{ISI:000424962000005}},
}

@article{ ISI:000430067700001,
Author = {Landschoff, Jannes and Du Plessis, Anton and Griffiths, Charles L.},
Title = {{A micro X-ray computed tomography dataset of South African hermit crabs
   (Crustacea: Decapoda: Anomura: Paguroidea) containing scans of two rare
   specimens and three recently described species}},
Journal = {{GIGASCIENCE}},
Year = {{2018}},
Volume = {{7}},
Number = {{4}},
Month = {{MAR 14}},
Abstract = {{Background: Along with the conventional deposition of physical types at
   natural history museums, the deposition of 3-dimensional (3D) image data
   has been proposed for rare and valuable museum specimens, such as
   irrepla ceable type material. Findings: Micro computed tomography (mu
   CT) scan data of 5 hermit crab species from South Africa, including rare
   specimens and type material, depicted main identification
   characteristics of calcified body parts. However, low-image contrasts,
   especially in larger (> 50 mm total length) specimens, did not allow
   sufficient 3D reconstructions of weakly calcified and fine
   characteristics, such as soft tissue of the pleon, mouthparts, gills,
   and setation. Reconstructions of soft tissue were sometimes possible,
   depending on individual sample and scanning characteristics. The raw
   data of seven scans are publicly available for download from the GigaDB
   repository. Conclusions: Calcified body parts visualized from mu CT data
   can aid taxonomic validation and provide additional, virtual deposition
   of rare specimens. The use of a nondestructive, nonstaining mu CT
   approach for taxonomy, reconstructions of soft tissue structures,
   microscopic spines, and setae depend on species characteristics.
   Constrained to these limitations, the presented dataset can be used for
   future morphological studies. However, our virtual specimens will be
   most valuable to taxonomists who can download a digital avatar for 3D
   examination. Simultaneously, in the event of physical damage to or loss
   of the original physical specimen, this dataset serves as a vital
   insurance policy.}},
DOI = {{10.1093/gigascience/giy022}},
ISSN = {{2047-217X}},
ORCID-Numbers = {{du Plessis, Anton/0000-0002-4370-8661}},
Unique-ID = {{ISI:000430067700001}},
}

@article{ ISI:000428508200022,
Author = {Alonzo, Michael and Andersen, Hans-Erik and Morton, Douglas C. and Cook,
   Bruce D.},
Title = {{Quantifying Boreal Forest Structure and Composition Using UAV Structure
   from Motion}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{The vast extent and inaccessibility of boreal forest ecosystems are
   barriers to routine monitoring of forest structure and composition. In
   this research, we bridge the scale gap between intensive but sparse plot
   measurements and extensive remote sensing studies by collecting forest
   inventory variables at the plot scale using an unmanned aerial vehicle
   (UAV) and a structure from motion (SfM) approach. At 20 Forest Inventory
   and Analysis (FIA) subplots in interior Alaska, we acquired overlapping
   imagery and generated dense, 3D, RGB (red, green, blue) point clouds. We
   used these data to model forest type at the individual crown scale as
   well as subplot-scale tree density (TD), basal area (BA), and
   aboveground biomass (AGB). We achieved 85\% cross-validation accuracy
   for five species at the crown level. Classification accuracy was
   maximized using three variables representing crown height, form, and
   color. Consistent with previous UAV-based studies, SfM point cloud data
   generated robust models of TD (r(2) = 0.91), BA (r(2) = 0.79), and AGB
   (r(2) = 0.92), using a mix of plot-and crown-scale information. Precise
   estimation of TD required either segment counts or species information
   to differentiate black spruce from mixed white spruce plots. The
   accuracy of species-specific estimates of TD, BA, and AGB at the plot
   scale was somewhat variable, ranging from accurate estimates of black
   spruce TD (+/1\%) and aspen BA (-2\%) to misallocation of aspen AGB
   (+118\%) and white spruce AGB (-50\%). These results convey the
   potential utility of SfM data for forest type discrimination in FIA
   plots and the remaining challenges to develop classification approaches
   for species-specific estimates at the plot scale that are more robust to
   segmentation error.}},
DOI = {{10.3390/f9030119}},
Article-Number = {{119}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Morton, Douglas/D-5044-2012}},
Unique-ID = {{ISI:000428508200022}},
}

@article{ ISI:000428936900023,
Author = {Yuan, Lin and Chen, Fanglin and Zeng, Ling-Li and Wang, Lubin and Hu,
   Dewen},
Title = {{Gender Identification of Human Brain Image with A Novel 3D Descriptor}},
Journal = {{IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS}},
Year = {{2018}},
Volume = {{15}},
Number = {{2}},
Pages = {{551-561}},
Month = {{MAR-APR}},
Abstract = {{Determining gender by examining the human brain is not a simple task
   because the spatial structure of the human brain is complex, and no
   obvious differences can be seen by the naked eyes. In this paper, we
   propose a novel three-dimensional feature descriptor, the
   three-dimensional weighted histogram of gradient orientation (3D WHGO)
   to describe this complex spatial structure. The descriptor combines
   local information for signal intensity and global three-dimensional
   spatial information for the whole brain. We also improve a framework to
   address the classification of three-dimensional images based on MRI.
   This framework, three-dimensional spatial pyramid, uses additional
   information regarding the spatial relationship between features. The
   proposed method can be used to distinguish gender at the individual
   level. We examine our method by using the gender identification of
   individual magnetic resonance imaging (MRI) scans of a large sample of
   healthy adults across four research sites, resulting in up to
   individual-level accuracies under the optimized parameters for
   distinguishing between females and males. Compared with previous
   methods, the proposed method obtains higher accuracy, which suggests
   that this technology has higher discriminative power. With its improved
   performance in gender identification, the proposed method may have the
   potential to inform clinical practice and aid in research on
   neurological and psychiatric disorders.}},
DOI = {{10.1109/TCBB.2015.2448081}},
ISSN = {{1545-5963}},
EISSN = {{1557-9964}},
ResearcherID-Numbers = {{Hu, Dewen/D-1978-2015}},
ORCID-Numbers = {{Hu, Dewen/0000-0001-7357-0053}},
Unique-ID = {{ISI:000428936900023}},
}

@article{ ISI:000427548400003,
Author = {Hu, Guiqing and Taylor, Dianne W. and Liu, Jun and Taylor, Kenneth A.},
Title = {{Identification of interfaces involved in weak interactions with
   application to F-actin-aldolase rafts}},
Journal = {{JOURNAL OF STRUCTURAL BIOLOGY}},
Year = {{2018}},
Volume = {{201}},
Number = {{3}},
Pages = {{199-209}},
Month = {{MAR}},
Abstract = {{Macromolecular interactions occur with widely varying affinities. Strong
   interactions form well defined interfaces but weak interactions are more
   dynamic and variable. Weak interactions can collectively lead to large
   structures such as microvilli via cooperativity and are often the
   precursors of much stronger interactions, e.g. the initial actin-myosin
   interaction during muscle contraction. Electron tomography combined with
   subvolume alignment and classification is an ideal method for the study
   of weak interactions because a 3-D image is obtained for the individual
   interactions, which subsequently are characterized collectively. Here we
   describe a method to characterize heterogeneous F-actin-aldolase
   interactions in 2-D rafts using electron tomography. By forming separate
   averages of the two constituents and fitting an atomic structure to each
   average, together with the alignment information which relates the raw
   motif to the average, an atomic model of each crosslink is determined
   and a frequency map of contact residues is computed. The approach should
   be applicable to any large structure composed of constituents that
   interact weakly and heterogeneously.}},
DOI = {{10.1016/j.jsb.2017.11.005}},
ISSN = {{1047-8477}},
EISSN = {{1095-8657}},
Unique-ID = {{ISI:000427548400003}},
}

@article{ ISI:000427313700006,
Author = {Herfort, Benjamin and Hoefle, Bernhard and Klonner, Carolin},
Title = {{3D micro-mapping: Towards assessing the quality of crowdsourcing to
   support 3D point cloud analysis}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2018}},
Volume = {{137}},
Pages = {{73-83}},
Month = {{MAR}},
Abstract = {{In this paper, we propose a method to crowdsource the task of complex
   three-dimensional information extraction from 3D point clouds. We design
   web-based 3D micro tasks tailored to assess segmented LiDAR point clouds
   of urban trees and investigate the quality of the approach in an
   empirical user study. Our results for three different experiments with
   increasing complexity indicate that a single crowd sourcing task can be
   solved in a very short time of less than five seconds on average.
   Furthermore, the results of our empirical case study reveal that the
   accuracy, sensitivity and precision of 3D crowdsourcing are high for
   most information extraction problems. For our first experiment (binary
   classification with single answer) we obtain an accuracy of 91\%, a
   sensitivity of 95\% and a precision of 92\%. For the more complex tasks
   of the second Experiment 2 (multiple answer classification) the accuracy
   ranges from 65\% to 99\% depending on the label class. Regarding the
   third experiment - the determination of the crown base height of
   individual trees - our study highlights that crowdsourcing can be a tool
   to obtain values with even higher accuracy in comparison to an automated
   computer-based approach. Finally, we found out that the accuracy of the
   crowdsourced results for all experiments is hardly influenced by
   characteristics of the input point cloud data and of the users.
   Importantly, the results' accuracy can be estimated using agreement
   among volunteers as an intrinsic indicator, which makes a broad
   application of 3D micro-mapping very promising. (C) 2018 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2018.01.009}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Hofle, Bernhard/A-4702-2010
   }},
ORCID-Numbers = {{Hofle, Bernhard/0000-0001-5849-1461
   Klonner, Carolin/0000-0003-1981-2204}},
Unique-ID = {{ISI:000427313700006}},
}

@article{ ISI:000418312200002,
Author = {Kukunda, Collins B. and Duque-Lazo, Joaquin and Gonzalez-Ferreiro,
   Eduardo and Thaden, Hauke and Kleinn, Christoph},
Title = {{Ensemble classification of individual Pinus crowns from multispectral
   satellite imagery and airborne LiDAR}},
Journal = {{INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION}},
Year = {{2018}},
Volume = {{65}},
Pages = {{12-23}},
Month = {{MAR}},
Abstract = {{Distinguishing tree species is relevant in many contexts of remote
   sensing assisted forest inventory. Accurate tree species maps support
   management and conservation planning, pest and disease control and
   biomass estimation. This study evaluated the performance of applying
   ensemble techniques with the goal of automatically distinguishing Pinus
   sylvestris L. and Pinus uncinata Mill. Ex Mirb within a 1.3 km(2)
   mountainous area in Barcelonnette (France). Three modelling schemes were
   examined, based on: (1) high-density LiDAR data (160 returns m(-2)), (2)
   Worldview-2 multispectral imagery, and (3) Worldview-2 and LiDAR in
   combination. Variables related to the crown structure and height of
   individual trees were extracted from the normalized LiDAR point cloud at
   individual-tree level, after performing individual tree crown (ITC)
   delineation. Vegetation indices and the Haralick texture indices were
   derived from Worldview-2 images and served as independent spectral
   variables. Selection of the best predictor subset was done after a
   comparison of three variable selection procedures: (1) Random Forests
   with cross validation (AUCREcv), (2) Akaike Information Criterion (AIC)
   and (3) Bayesian Information Criterion (BIC). To classify the species, 9
   regression techniques were combined using ensemble models. Predictions
   were evaluated using cross validation and an independent dataset.
   Integration of datasets and models improved individual tree species
   classification (True Skills Statistic, TSS; from 0.67 to 0.81) over
   individual techniques and maintained strong predictive power (Relative
   Operating Characteristic, ROC = 0.91). Assemblage of regression models
   and integration of the datasets provided more reliable species
   distribution maps and associated tree-scale mapping uncertainties. Our
   study highlights the potential of model and data assemblage at improving
   species classifications needed in present-day forest planning and
   management.}},
DOI = {{10.1016/j.jag.2017.09.016}},
ISSN = {{0303-2434}},
ResearcherID-Numbers = {{Duque-Lazo, Joaquin/L-3722-2019
   Gonzalez-Ferreiro, Eduardo/Q-9709-2016}},
ORCID-Numbers = {{Duque-Lazo, Joaquin/0000-0003-4223-5070
   Gonzalez-Ferreiro, Eduardo/0000-0002-4565-2155}},
Unique-ID = {{ISI:000418312200002}},
}

@article{ ISI:000427009100019,
Author = {Ye, Cang and Qian, Xiangfei},
Title = {{3-D Object Recognition of a Robotic Navigation Aid for the Visually
   Impaired}},
Journal = {{IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING}},
Year = {{2018}},
Volume = {{26}},
Number = {{2}},
Pages = {{441-450}},
Month = {{FEB}},
Abstract = {{This paper presents a 3-D object recognition method and its
   implementation on a robotic navigation aid to allow real-time detection
   of indoor structural objects for the navigation of a blind person. The
   method segments a point cloud into numerous planar patches and extracts
   their inter-plane relationships (IPRs). Based on the existing IPRs of
   the object models, the method defines six high level features (HLFs) and
   determines the HLFs for each patch. A Gaussian-mixture-model-based plane
   classifier is then devised to classify each planar patch into one
   belonging to a particular object model. Finally, a recursive plane
   clustering procedure is used to cluster the classified planes into the
   model objects. As the proposed method uses geometric context to detect
   an object, it is robust to the object's visual appearance change. As a
   result, it is ideal for detecting structural objects (e.g., stairways,
   doorways, and so on). In addition, it has high scalability and
   parallelism. The method is also capable of detecting some indoor
   nonstructural objects. Experimental results demonstrate that the
   proposed method has a high success rate in object recognition.}},
DOI = {{10.1109/TNSRE.2017.2748419}},
ISSN = {{1534-4320}},
EISSN = {{1558-0210}},
Unique-ID = {{ISI:000427009100019}},
}

@article{ ISI:000418370200123,
Author = {Li, Ye and Wang, YingHui and Liu, Jing and Hao, Wen},
Title = {{Expression-insensitive 3D face recognition by the fusion of multiple
   subject-specific curves}},
Journal = {{NEUROCOMPUTING}},
Year = {{2018}},
Volume = {{275}},
Pages = {{1295-1307}},
Month = {{JAN 31}},
Abstract = {{This study proposes a 3D face recognition method using multiple
   subject-specific curves insensitive to intra-subject distortions caused
   by expression variations. Considering that most sharp variances in
   facial convex regions are closely related to the bone structure, the
   convex crest curves are first extracted as the most vital
   subject-specific facial curves based on the principal curvature extrema
   in convex local surfaces. Then, the central profile curve and the
   horizontal contour curve passing through the nose tip are detected by
   using the precise localization of the nose tip and symmetry plane. Based
   on their discriminative power and robustness to expression changes, the
   three types of curves are fused with appropriate weights at the
   feature-level and used for matching 3D faces with the iterative closest
   point algorithm. The combination of multiple expression-insensitive
   curves is complementary and provides sufficient and stable facial
   surface features for face recognition. In addition, for each convex
   crest curve, an expression-irrelevant factor is assigned as the adaptive
   weight to improve the face matching performance. The results of
   experiments using two public 3D databases, GavabDB and BU-3DFE,
   demonstrate the effectiveness of the proposed method, and its
   recognition rates on both databases reflect an encouraging performance.
   (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2017.09.070}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000418370200123}},
}

@article{ ISI:000423097800017,
Author = {Crouch, Daniel J. M. and Winney, Bruce and Koppen, Willem P. and
   Christmas, William J. and Hutnik, Katarzyna and Day, Tammy and Meena,
   Devendra and Boumertit, Abdelhamid and Hysi, Pirro and Nessa, Ayrun and
   Spector, Tim D. and Kittler, Josef and Bodmer, Walter F.},
Title = {{Genetics of the human face: Identification of large-effect single gene
   variants}},
Journal = {{PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA}},
Year = {{2018}},
Volume = {{115}},
Number = {{4}},
Pages = {{E676-E685}},
Month = {{JAN 23}},
Abstract = {{To discover specific variants with relatively large effects on the human
   face, we have devised an approach to identifying facial features with
   high heritability. This is based on using twin data to estimate the
   additive genetic value of each point on a face, as provided by a 3D
   camera system. In addition, we have used the ethnic difference between
   East Asian and European faces as a further source of face genetic
   variation. We use principal components (PCs) analysis to provide a fine
   definition of the surface features of human faces around the eyes and of
   the profile, and chose upper and lower 10\% extremes of the most
   heritable PCs for looking for genetic associations. Using this strategy
   for the analysis of 3D images of 1,832 unique volunteers from the
   well-characterized People of the British Isles study and 1,567 unique
   twin images from the TwinsUK cohort, together with genetic data for
   500,000 SNPs, we have identified three specific genetic variants with
   notable effects on facial profiles and eyes.}},
DOI = {{10.1073/pnas.1708207114}},
ISSN = {{0027-8424}},
Unique-ID = {{ISI:000423097800017}},
}

@inproceedings{ ISI:000462163500103,
Author = {Ju, Xiangyang and Garcia Junior, Idelmo Rangel and Silva, Leonardo de
   Freitas and Mossey, Peter and Al-Rudainy, Dhelal and Ayoub, Ashraf and
   de Mattos, Adriana Marques},
Editor = {{Li, W and Li, Q and Wang, L}},
Title = {{3D Head Shape Analysis of Suspected Zika Infected Infants}},
Booktitle = {{2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018)}},
Year = {{2018}},
Note = {{11th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI), Beijing, PEOPLES R CHINA, OCT
   13-15, 2018}},
Organization = {{IEEE; IEEE Engn Med \& Biol Soc; Beijing Univ Chem Technol; Beijing Inst
   Technol; E China Normal Univ}},
Abstract = {{The babies infected from Zika before they are born are at risk for
   problems with brain development and microcephaly. 3D head images of 43
   Zika cases and 43 controls were collected aiming to extract shape
   characteristics for diagnosis purposes. Principal component analysis
   (PCA) has been applied on the vaults and faces of the collected 3D
   images and the scores on the second principal components of the vaults
   and faces showed significant differences between the control and Zika
   groups. The shape variations from -2s to 2s illustrated the typical
   characteristics of microcephaly of the Zika babies. Canonical
   correlation analysis (CCA) showed a significant correlation in the first
   CCA variates of face and vault which indicated the potential of 3D
   facial imaging for Zika surveillance. Further head circumferences and
   distances from ear to ear were measured from the 3D images and
   preliminary results showed the adding ear to ear distances for
   classifying control and Zika children strengthened the abilities of
   tested classification models.}},
ISBN = {{978-1-5386-7604-2}},
Unique-ID = {{ISI:000462163500103}},
}

@inproceedings{ ISI:000460950900069,
Author = {Khan, Jihas and Raj, Jayakrishna and Pradeep, R.},
Editor = {{Zelinka, I and Senkerik, R and Panda, G and Kanthan, PSL}},
Title = {{Modeling of an Automotive Grade LIDAR Sensor}},
Booktitle = {{SOFT COMPUTING SYSTEMS, ICSCS 2018}},
Series = {{Communications in Computer and Information Science}},
Year = {{2018}},
Volume = {{837}},
Pages = {{676-686}},
Note = {{2nd International Conference on Soft Computing Systems (ICSCS), Baselios
   Mathews II Coll Engn, Sasthamcotta, INDIA, APR 19-20, 2018}},
Abstract = {{Automobiles use LIDAR sensor to detect different objects around the
   vehicle. For system analysis and study, this paper is proposing a LIDAR
   sensor model, which takes into consideration the impact of the real
   world information. Impact of the environment on the LIDAR sensor is
   modeled and all the possible parameters of the LIDAR are modeled as
   configurable parameters. Option to import 3D objects of any shape, type
   or dimension via FBX file format is also incorporated. 3D objects in FBX
   format shall be converted to a set of triangles first, which approximate
   the surface mesh of the 3D object. Ray cast modeling is then used to
   detect whether in a vertical distribution of LIDAR beams, an
   intersection occurs between LIDAR beam and any of the triangular face.
   If there is a collision, the collision point shall be saved as the point
   cloud data. This will be repeated around the sensor, and all such point
   cloud data points shall be appended to the final point cloud data. These
   point cloud data is then subjected to segmentation and object detection
   using belief theory. Either the processed point cloud data in object
   information format or the unprocessed raw point cloud data can be
   produced as the output by the proposed LIDAR sensor model.}},
DOI = {{10.1007/978-981-13-1936-5\_69}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-981-13-1936-5; 978-981-13-1935-8}},
Unique-ID = {{ISI:000460950900069}},
}

@inproceedings{ ISI:000457881301017,
Author = {Akita, Tokihiko and Yamauchi, Yuji and Fujiyoshi, Hironobu},
Book-Group-Author = {{IEEE}},
Title = {{Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye
   Cameras}},
Booktitle = {{2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)}},
Series = {{IEEE International Conference on Intelligent Transportation Systems-ITSC}},
Year = {{2018}},
Pages = {{1103-1108}},
Note = {{21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC), Maui, HI, NOV 04-07, 2018}},
Organization = {{IEEE; IEEE Intelligent Transportat Syst Soc}},
Abstract = {{Recently, automated emergency brake systems for pedestrian have been
   commercialized. However, they cannot detect crossing pedestrians when
   turning at intersections because the field of view is not wide enough.
   Thus, we propose to utilize a surround view camera system becoming
   popular by making it into stereo vision which is robust for the
   pedestrian recognition. However, conventional stereo camera technologies
   cannot be applied due to fisheye cameras and uncalibrated camera poses.
   Thus we have created the new method to absorb difference of the
   pedestrian appearance between cameras by machine learning for the stereo
   vision. The method of stereo matching between image patches in each
   camera image was designed by combining D-Brief and NCC with SVM. Good
   generalization performance was achieved by it compared with individual
   conventional algorithms. Furthermore, feature amounts of the point cloud
   reconstructed by the stereo pairs are utilized with Random Forest to
   discriminate pedestrians. The algorithm was evaluated for the actual
   camera images of crossing pedestrians at various intersections, and
   96.0\% of pedestrian tracking rate with high position detection accuracy
   was achieved. They were compared with Faster R-CNN as the best pattern
   recognition technique, and our proposed method indicated better
   detection performance.}},
ISSN = {{2153-0009}},
ISBN = {{978-1-7281-0323-5}},
Unique-ID = {{ISI:000457881301017}},
}

@inproceedings{ ISI:000457843602090,
Author = {Zhu, Wei and Qiu, Qiang and Huang, Jiaji and Calderbank, Robert and
   Sapiro, Guillermo and Daubechies, Ingrid},
Book-Group-Author = {{IEEE}},
Title = {{LDMNet: Low Dimensional Manifold Regularized Neural Networks}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{2743-2751}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{Deep neural networks have proved very successful on archetypal tasks for
   which large training sets are available, but when the training data are
   scarce, their performance suffers from overfitting. Many existing
   methods of reducing overfitting are data-independent. Data-dependent
   regularizations are mostly motivated by the observation that data of
   interest lie close to a manifold, which is typically hard to parametrize
   explicitly. These methods usually only focus on the geometry of the
   input data, and do not necessarily encourage the networks to produce
   geometrically meaningful features. To resolve this, we propose the
   Low-Dimensional-Manifold-regularized neural Network (LDMNet), which
   incorporates a feature regularization method that focuses on the
   geometry of both the input data and the output features. In LDMNet, we
   regularize the network by encouraging the combination of the input data
   and the output features to sample a collection of low dimensional
   manifolds, which are searched efficiently without explicit
   parametrization. To achieve this, we directly use the manifold dimension
   as a regularization term in a variational functional. The resulting
   Euler-Lagrange equation is a Laplace-Beltrami equation over a point
   cloud, which is solved by the point integral method without increasing
   the computational complexity. In the experiments, we show that LDMNet
   significantly outperforms widely-used regularizers. Moreover, LDMNet can
   extract common features of an object imaged via different modalities,
   which is very useful in real-world applications such as cross-spectral
   face recognition.}},
DOI = {{10.1109/CVPR.2018.00290}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843602090}},
}

@inproceedings{ ISI:000457843605028,
Author = {Cheng, Shiyang and Kotsia, Irene and Pantic, Maja and Zafeiriou,
   Stefanos},
Book-Group-Author = {{IEEE}},
Title = {{4DFAB: A Large Scale 4D Database for Facial Expression Analysis and
   Biometric Applications}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{5117-5126}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{The progress we are currently witnessing in many computer vision
   applications, including automatic face analysis, would not be made
   possible without tremendous efforts in collecting and annotating large
   scale visual databases. To this end, we propose 4DFAB, a new large scale
   database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes).
   4DFAB contains recordings of 180 subjects captured in four different
   sessions spanning over a five-year period. It contains 4D videos of
   subjects displaying both spontaneous and posed facial behaviours. The
   database can be used for both face and facial expression recognition, as
   well as behavioural biometrics. It can also be used to learn very
   powerful blendshapes for parametrising facial behaviour. In this paper,
   we conduct several experiments and demonstrate the usefulness of the
   database for various applications. The database will be made publicly
   available for research purposes.}},
DOI = {{10.1109/CVPR.2018.00537}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843605028}},
}

@inproceedings{ ISI:000457843609059,
Author = {Li, Jiaxin and Chen, Ben M. and Lee, Gim Hee},
Book-Group-Author = {{IEEE}},
Title = {{SO-Net: Self-Organizing Network for Point Cloud Analysis}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{9397-9406}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{This paper presents SO-Net, a permutation invariant architecture for
   deep learning with orderless point clouds. The SO-Net models the spatial
   distribution of point cloud by building a Self-Organizing Map (SOM).
   Based on the SOM, SO-Net performs hierarchical feature extraction on
   individual points and SOM nodes, and ultimately represents the input
   point cloud by a single feature vector. The receptive field of the
   network can be systematically adjusted by conducting point-to-node k
   nearest neighbor search. In recognition tasks such as point cloud
   reconstruction, classification, object part segmentation and shape
   retrieval, our proposed network demonstrates performance that is similar
   with or better than state-of-the-art approaches. In addition, the
   training speed is significantly faster than existing point cloud
   recognition networks because of the parallelizability and simplicity of
   the proposed architecture. Our code is available at the project
   website.(1)}},
DOI = {{10.1109/CVPR.2018.00979}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843609059}},
}

@inproceedings{ ISI:000455784500063,
Author = {Ye, Jing and Zhou, Changhe and Li, Chao and Miao, Chaofeng},
Editor = {{Sheng, Y and Yu, C and Zhou, C}},
Title = {{High-precision 3D Shape Measurement Based on Time-resolved VCSEL}},
Booktitle = {{HOLOGRAPHY, DIFFRACTIVE OPTICS, AND APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2018}},
Volume = {{10818}},
Note = {{Conference on Holography, Diffractive Optics, and Applications VIII,
   Beijing, PEOPLES R CHINA, OCT 11-13, 2018}},
Organization = {{SPIE; Chinese Opt Soc}},
Abstract = {{High-speed and high-precision human face 3D shape measurement plays a
   very important role in diverse applications such as human-computer
   interaction, 3D face recognition, Virtual Reality. This paper introduces
   a structured light system based on VCSEL(Vertical Cavity Surface
   Emitting Laser) with one simulated projectors and two camera for human
   face 3D shape measurement. Large-scale production cost of VCSEL is low,
   because of the manufacturing process compatible with LED. VCSEL has the
   advantages of projecting a large area of diffractive structure light and
   easy to integrate into lens array internally. The process of VCSEL
   projecting the structural light that changes over time to human face is
   simulated by computer. The ICP algorithm is used to match the image of
   single frame structure light from the right camera to the left camera. A
   single frame image of three-dimensional face point cloud is obtained by
   using binocular stereo vision principle. The multi-frame images of point
   cloud that change along time series are superposed to obtain higher
   density point cloud data and improve the measurement accuracy. This 3D
   measurement based on VCSEL has advantages of low cost, high precision,
   and small size and should be useful for practical applications.}},
DOI = {{10.1117/12.2502417}},
Article-Number = {{UNSP 1081829}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-2235-7}},
Unique-ID = {{ISI:000455784500063}},
}

@inproceedings{ ISI:000455146803022,
Author = {Liang, Jie and Liu, Feng and Tu, Huan and Zhao, Qijun and Jain, Anil K.},
Book-Group-Author = {{IEEE}},
Title = {{On Mugshot-based Arbitrary View Face Recognition}},
Booktitle = {{2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2018}},
Pages = {{3126-3131}},
Note = {{24th International Conference on Pattern Recognition (ICPR), Chinese
   Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA, AUG 20-24, 2018}},
Organization = {{Int Assoc Pattern Recognit; Chinese Assoc Automat}},
Abstract = {{Despite the wide usage of mugshot images in forensic applications, they
   are underutilized in existing automated face recognition systems. In
   this paper, we propose a novel mugshot-based arbitrary view face
   recognition method. Our approach reconstructs full 3D faces via cascaded
   regression in shape space with efficient seamless texture recovery.
   Unlike existing methods, it makes full use of the frontal and profile
   views available in mugshot images, and thus generates accurate and
   realistic 3D faces. Multi-view face images are synthesized from the
   reconstructed 3D faces to enlarge the gallery so that arbitrary view
   faces can be better recognized. Evaluation experiments were conducted on
   BFM and Multi-PIE databases by using state-of-the-art deep learning (DL)
   based face matchers. The results demonstrate the effectiveness of our
   proposed method and show that DL-based face matchers can benefit from
   mugshot images and the reconstructed 3D faces, especially for
   recognizing large off-angle faces.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5386-3788-3}},
Unique-ID = {{ISI:000455146803022}},
}

@inproceedings{ ISI:000455305000011,
Author = {Harikumar, A. and Paris, C. and Bovolo, F. and Bruzzone, L.},
Editor = {{Bruzzone, L and Bovolo, F}},
Title = {{A novel data-driven approach to tree species classification using high
   density multireturn airborne lidar data}},
Booktitle = {{IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXIV}},
Series = {{Proceedings of SPIE}},
Year = {{2018}},
Volume = {{10789}},
Note = {{Conference on Image and Signal Processing for Remote Sensing XXIV,
   Berlin, GERMANY, SEP 10-12, 2018}},
Organization = {{SPIE}},
Abstract = {{Tree species information is crucial for accurate forest parameter
   estimation. Small footprint high density multi-return Light Detection
   and Ranging (LiDAR) data contain a large amount of structural details
   for modelling and thus distinguishing individual tree species. To fully
   exploit the potential of these data, we propose a data-driven tree
   species classification approach based on a volumetric analysis of
   single-tree-point-cloud that extracts features that are able to
   characterize both the internal and the external crown structure. The
   method captures the spatial distribution of the LiDAR points within the
   crown by generating a feature vector representing the three-dimensional
   (3D) crown information. Each element in the feature vector uniquely
   corresponds to an Elementary Quantization Volume (EQV) of the crown.
   Three strategies have been defined to generate unique EQVs that model
   different representations of the crown components. The classification is
   performed by using a Support Vector Machines (C-SVM) classifier using
   the histogram intersection kernel that has the enhanced ability to give
   maximum preference to the key features in high dimensional feature
   space. All the experiments were performed on a set of 200 trees
   belonging to Norway Spruce, European Larch, Swiss Pine, and Silver Fir
   (i.e., 50 trees per species). The classifier is trained using 120 trees
   and tested on an independent set of 80 trees. The proposed method
   outperforms the classification performance of the state-of-the-art
   method used for comparison.}},
DOI = {{10.1117/12.2325634}},
Article-Number = {{UNSP 107890E}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-2162-6}},
ORCID-Numbers = {{Paris, Claudia/0000-0002-7189-6268}},
Unique-ID = {{ISI:000455305000011}},
}

@inproceedings{ ISI:000455343100004,
Author = {Desai, Kevin and Prabhakaran, Balakrishnan and Raghuraman, Suraj},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{Combining Skeletal Poses for 3D Human Model Generation using Multiple
   Kinects}},
Booktitle = {{PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18)}},
Year = {{2018}},
Pages = {{40-51}},
Note = {{9th ACM Multimedia Systems Conference (MMSys), Centrum Wiskunde \&
   Informatica Amsterdam, Amsterdam, NETHERLANDS, JUN 12-15, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGMM; ACM SIGOPS; ACM SIGMOBILE; ACM SIGCOMM}},
Abstract = {{RGB-D cameras, such as the Microsoft Kinect, provide us with the 3D
   information, color and depth, associated with the scene. Interactive 3D
   Tele-Immersion (i3DTI) systems use such RGB-D cameras to capture the
   person present in the scene in order to collaborate with other remote
   users and interact with the virtual objects present in the environment.
   Using a single camera, it becomes difficult to estimate an accurate
   skeletal pose and complete 3D model of the person, especially when the
   person is not in the complete view of the camera. With multiple cameras,
   even with partial views, it is possible to get a more accurate estimate
   of the skeleton of the person leading to a better and complete 3D model.
   In this paper, we present a real-time skeletal pose identification
   approach that leverages on the inaccurate skeletons of the individual
   Kinects, and provides a combined optimized skeleton. We estimate the
   Probability of an Accurate Joint (PAJ) for each joint from all of the
   Kinect skeletons. We determine the correct direction of the person and
   assign the correct joint sides for each skeleton. We then use a greedy
   consensus approach to combine the highly probable and accurate joints to
   estimate the combined skeleton. Using the individual skeletons, we
   segment the point clouds from all the cameras. We use the already
   computed PAJ values to obtain the Probability of an Accurate Bone (PAB).
   The individual point clouds are then combined one segment after another
   using the calculated PAB values. The generated combined point cloud is a
   complete and accurate 3D representation of the person present in the
   scene. We validate our estimated skeleton against two well-known methods
   by computing the error distance between the best view Kinect skeleton
   and the estimated skeleton. An exhaustive analysis is performed by using
   around 500000 skeletal frames in total, captured using 7 users and 7
   cameras. Visual analysis is performed by checking whether the estimated
   skeleton is completely present within the human model. We also develop a
   3D Holo-Bubble game to showcase the real-time performance of the
   combined skeleton and point cloud. Our results show that our method
   performs better than the state-of-the-art approaches that use multiple
   Kinects, in terms of objective error, visual quality and real-time user
   performance.}},
DOI = {{10.1145/3204949.3204958}},
ISBN = {{978-1-4503-5192-8}},
ORCID-Numbers = {{Desai, Kevin/0000-0002-2964-8981}},
Unique-ID = {{ISI:000455343100004}},
}

@article{ ISI:000455069200012,
Author = {Wang, Kaishi and Jiang, Yi and Zhang, Zhifei and Lu, Yongtian and Ni,
   Yusu},
Title = {{Extension of the Clinical Significance of the ``Cog{''}}},
Journal = {{ORL-JOURNAL FOR OTO-RHINO-LARYNGOLOGY HEAD AND NECK SURGERY}},
Year = {{2018}},
Volume = {{80}},
Number = {{5-6}},
Pages = {{317-325}},
Abstract = {{Objective: To study the clinical anatomy of the epitympanum, the attic,
   and its medial wall, to try to discover a new clinical operation-related
   anatomical landmark, and to investigate the adjacent anatomical
   relationship with this landmark. Materials and Methods: Eight donor
   temporal bone specimens were dissected endoscopically. For 29 healthy
   persons (17 males and 12 females), CT images of the temporal bone (57
   ears) were taken, 3-dimensional (3-D) reconstruction and
   multidimensional plane reconstruction were performed, and identification
   and assessment of 3-D spatial relationships between any 2 of these
   complex structures were done. Results: 3-D images of the temporal bone
   structures including the facial nerve, the cochlea, the semicircular
   canal, and the brain plate were reconstructed and shown in detail. We
   discovered a new clinical surgery-related anatomical landmark (the
   ``cog{''} tangent and the trailing edge of the cog). Based on the
   tangent and the trailing edge of the cog, we quantified the anatomical
   relationship between it and its neighboring important structures.
   Conclusion: Based on endoscopic anatomy and the temporal bone spiral CT
   3-D structure reconstruction of the epitympanum, the attic, and the
   adjacent structures, we found an extension of the clinical significance
   the cog. Quantification of the adjacent anatomical relationship of this
   landmark is very important for otology microsurgical operation. (c) 2018
   S. Karger AG, Basel}},
DOI = {{10.1159/000493012}},
ISSN = {{0301-1569}},
EISSN = {{1423-0275}},
Unique-ID = {{ISI:000455069200012}},
}

@inproceedings{ ISI:000451039807048,
Author = {Li, Shihua and Su, Lian and Liu, Yuhan and He, Ze},
Book-Group-Author = {{IEEE}},
Title = {{SEGMENTATION OF INDIVIDUAL TREES BASED ON A POINT CLOUD CLUSTERING
   METHOD USING AIRBORNE LIDAR DATA}},
Booktitle = {{IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING
   SYMPOSIUM}},
Series = {{IEEE International Symposium on Geoscience and Remote Sensing IGARSS}},
Year = {{2018}},
Pages = {{7520-7523}},
Note = {{38th IEEE International Geoscience and Remote Sensing Symposium
   (IGARSS), Valencia, SPAIN, JUL 22-27, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Geoscience
   \& Remote Sensing Soc; European Space Agcy}},
Abstract = {{The objective of this paper was to develop a new algorithm to segment
   individual trees directly by using the three-dimensional space
   characteristic of airborne light detection and ranging point cloud data.
   The local maximum method was used in the initial segmentation and the
   error identification tree exclusion. On the basis of the point cloud
   spatial distribution of individual trees and the adjacent relationship
   with the other trees, a point cloud clustering method was developed to
   decide the points belonging to the individual trees. This algorithm was
   tested by 6 forest plots in the Genhe forestry reserve. The results
   showed that this algorithm could segment individual trees quickly and
   accurately, and the overall accuracy of this algorithm was 96.3\%.}},
ISSN = {{2153-6996}},
ISBN = {{978-1-5386-7150-4}},
Unique-ID = {{ISI:000451039807048}},
}

@inproceedings{ ISI:000449774200039,
Author = {Abrevaya, Victoria Fernandez and Wuhrer, Stefanie and Boyer, Edmond},
Book-Group-Author = {{IEEE}},
Title = {{Spatiotemporal Modeling for Efficient Registration of Dynamic 3D Faces}},
Booktitle = {{2018 INTERNATIONAL CONFERENCE ON 3D VISION (3DV)}},
Series = {{International Conference on 3D Vision}},
Year = {{2018}},
Pages = {{371-380}},
Note = {{6th International Conference on 3D Vision (3DV), Verona, ITALY, SEP
   05-08, 2018}},
Organization = {{Agisoft; Aquifi; Google; Microsoft; Microtec; Nvidia; 3DFlow;
   Digitalviews}},
Abstract = {{We consider the registration of temporal sequences of 3D face scans.
   Face registration plays a central role in face analysis applications,
   for instance recognition or transfer tasks, among others. We propose an
   automatic approach that can register large sets of dynamic face scans
   without the need for landmarks or highly specialized acquisition setups.
   This allows for extended versatility among registered face shapes and
   deformations by enabling to leverage multiple datasets, a fundamental
   property when e.g. building statistical face models. Our approach is
   built upon a regression-based static registration method, which is
   improved by spatiotemporal modeling to exploit redundancies over both
   space and time. We experimentally demonstrate that accurate
   registrations can be obtained for varying data robustly and efficiently
   by applying our method to three standard dynamic face datasets.}},
DOI = {{10.1109/3DV.2018.00050}},
ISBN = {{978-1-5386-8425-2}},
Unique-ID = {{ISI:000449774200039}},
}

@inproceedings{ ISI:000446394502083,
Author = {Mitash, Chaitanya and Boularias, Abdeslam and Bekris, Kostas E.},
Book-Group-Author = {{IEEE}},
Title = {{Improving 6D Pose Estimation of Objects in Clutter via Physics-aware
   Monte Carlo Tree Search}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)}},
Series = {{IEEE International Conference on Robotics and Automation ICRA}},
Year = {{2018}},
Pages = {{3331-3338}},
Note = {{IEEE International Conference on Robotics and Automation (ICRA),
   Brisbane, AUSTRALIA, MAY 21-25, 2018}},
Organization = {{IEEE; CSIRO; Australian Govt, Dept Def Sci \& Technol; DJI; Queensland
   Univ Technol; Woodside; Baidu; Bosch; Houston Mechatron; Kinova Robot;
   KUKA; Hit Robot Grp; Honda Res Inst; iRobot; Mathworks; NuTonomy;
   Ouster; Uber}},
Abstract = {{This work proposes a process for efficiently searching over combinations
   of individual object 6D pose hypotheses in cluttered scenes, especially
   in cases involving occlusions and objects resting on each other. The
   initial set of candidate object poses is generated from state-of-the-art
   object detection and global point cloud registration techniques. The
   best scored pose per object by using these techniques may not be
   accurate due to overlaps and occlusions. Nevertheless, experimental
   indications provided in this work show that object poses with lower
   ranks may be closer to the real poses than ones with high ranks
   according to registration techniques. This motivates a global
   optimization process for improving these poses by taking into account
   scene-level physical interactions between objects. It also implies that
   the Cartesian product of candidate poses for interacting objects must be
   searched so as to identify the best scene-level hypothesis. To perform
   the search efficiently, the candidate poses for each object are
   clustered so as to reduce their number but still keep a sufficient
   diversity. Then, searching over the combinations of candidate object
   poses is performed through a Monte Carlo Tree Search (MCTS) process that
   uses the similarity between the observed depth image of the scene and a
   rendering of the scene given the hypothesized pose as a score that
   guides the search procedure. MCTS handles in a principled way the
   tradeoff between fine-tuning the most promising poses and exploring new
   ones, by using the Upper Confidence Bound (UCB) technique. Experimental
   results indicate that this process is able to quickly identify in
   cluttered scenes physically-consistent object poses that are
   significantly closer to ground truth compared to poses found by point
   cloud registration methods.}},
ISSN = {{1050-4729}},
ISBN = {{978-1-5386-3081-5}},
Unique-ID = {{ISI:000446394502083}},
}

@article{ ISI:000445394200001,
Author = {Hentz, Angela M. K. and Silva, Carlos A. and Dalla Corte, Ana P. and
   Netto, Sylvio P. and Stager, Michael P. and Klauberg, Carine},
Title = {{Estimating forest uniformity in Eucalyptus spp. and Pinus taeda L.
   stands using field measurements and structure from motion point clouds
   generated from unmanned aerial vehicle (UAV) data collection}},
Journal = {{FOREST SYSTEMS}},
Year = {{2018}},
Volume = {{27}},
Number = {{2}},
Abstract = {{Aim of study: In this study we applied 3D point clouds generated by
   images obtained from an Unmanned Aerial Vehicle (UAV) to evaluate the
   uniformity of young forest stands.
   Area of study: Two commercial forest stands were selected, with two
   plots each. The forest species studied were Eucalyptus spp. and Pinus
   taeda L. and the trees had an age of 1.5 years.
   Material and methods: The individual trees were detected based on
   watershed segmentation and local maxima, using the spectral values
   stored in the point cloud. After the tree detection, the heights were
   calculated using two approaches, in the first one using the Digital
   Surface Model (DSM) and a Digital Terrain Model, and in the second using
   only the DSM. We used the UAV-derived heights to estimate an uniformity
   index.
   Main results: The trees were detected with a maximum 6\% of error.
   However, the height was underestimated in all cases, in an average of 1
   and 0.7 m for Pinus and Eucalyptus stands. We proposed to use the models
   built herein to estimate tree height, but the regression models did not
   explain the variably within the data satisfactorily. Therefore, the
   uniformity index calculated using the direct UAV-height values presented
   results close to the field inventory, reaching better results when using
   the second height approach (error ranging 2.8-7.8\%).
   Research highlights: The uniformity index using the UAV-derived height
   from the proposed methods was close to the values obtained in field. We
   noted the potential for using UAV imagery in forest monitoring.}},
DOI = {{10.5424/fs/2018272-11713}},
Article-Number = {{UNSP e005}},
ISSN = {{2171-5068}},
EISSN = {{2171-9845}},
Unique-ID = {{ISI:000445394200001}},
}

@article{ ISI:000432842000001,
Author = {Petkovic, Uros and Korez, Robert and Parent, Stefan and Kadoury, Samuel
   and Vrtovec, Tomaz},
Title = {{Semi-automated 3D Cobb Angle Measurements from Scoliotic Mesh Models}},
Journal = {{ELEKTROTEHNISKI VESTNIK-ELECTROCHEMICAL REVIEW}},
Year = {{2018}},
Volume = {{85}},
Number = {{1-2}},
Pages = {{1-6}},
Abstract = {{The Cobb angle, which is the main diagnostic parameter for the
   evaluation of spinal deformities, is usually measured on two-dimensional
   coronal radiographic (X-ray) images. To provide more accurate
   measurements, there is a tendency for measuring the Cobb angle from
   three-dimensional (3D) images. In this paper we propose a semi-automated
   method for the evaluation of the 3D Cobb angle from 3D spine mesh
   models. From the manually selected upper-end and lower-end vertebra mesh
   models, we first identify the vertebral body centers, and then label
   faces of the superior and inferior endplates of the mesh model, which
   define the planes used for the measurement of the 3D Cobb angle. The
   results obtained on 60 mesh models of scoliotic spines at 17 different
   face-vertex densities indicate that the method is robust and accurate at
   the face-edge lengths below 6 mm with the corresponding mean absolute
   error of 3.0 degrees and standard deviation of 2.2 degrees when compared
   to reference measurements.}},
ISSN = {{0013-5852}},
EISSN = {{2232-3228}},
Unique-ID = {{ISI:000432842000001}},
}

@article{ ISI:000425828200076,
Author = {Anderson, Kyle E. and Glenn, Nancy F. and Spaete, Lucas P. and
   Shinneman, Douglas J. and Pilliod, David S. and Arkle, Robert S. and
   McIlroy, Susan K. and Derryberry, DeWayne R.},
Title = {{Estimating vegetation biomass and cover across large plots in shrub and
   grass dominated drylands using terrestrial lidar and machine learning}},
Journal = {{ECOLOGICAL INDICATORS}},
Year = {{2018}},
Volume = {{84}},
Pages = {{793-802}},
Month = {{JAN}},
Abstract = {{Terrestrial laser scanning (TLS) has been shown to enable an efficient,
   precise, and non-destructive inventory of vegetation structure at ranges
   up to hundreds of meters. We developed a method that leverages TLS
   collections with machine learning techniques to model and map canopy
   cover and biomass of several classes of short-stature vegetation across
   large plots. We collected high-definition TLS scans of 26 1-ha plots in
   desert grasslands and big sagebrush shrublands in southwest Idaho, USA.
   We used the Random Forests machine learning algorithm to develop
   decision tree models predicting the biomass and canopy cover of several
   vegetation classes from statistical descriptors of the aboveground
   heights of TLS points. Manual measurements of vegetation characteristics
   collected within each plot served as training and validation data.
   Models based on five or fewer TLS descriptors of vegetation heights were
   developed to predict the canopy cover fraction of shrubs (R-2 = 0.77,
   RMSE = 7\%), annual grasses (R-2 = 0.70, RMSE = 21\%), perennial grasses
   (R-2 = 0.36, RMSE = 12\%), forbs (R-2 = 0.52, RMSE = 6\%), bare earth or
   litter (R-2 = 0.49, RMSE = 19\%), and the biomass of shrubs (R-2 = 0.71,
   RMSE = 175 g) and herbaceous vegetation (R-2 = 0.61, RMSE = 99 g) (all
   values reported are out-of-bag). Our models explained much of the
   variability between predictions and manual measurements, and yet we
   expect that future applications could produce even better results by
   reducing some of the methodological sources of error that we
   encountered. Our work demonstrates how TLS can be used efficiently to
   extend manual measurement of vegetation characteristics from small to
   large plots in grasslands and shrublands, with potential application to
   other similarly structured ecosystems. Our method shows that vegetation
   structural characteristics can be modeled without classifying and
   delineating individual plants, a challenging and time-consuming step
   common in previous methods applying TLS to vegetation inventory.
   Improving application of TLS to studies of shrub steppe ecosystems will
   serve immediate management needs by enhancing vegetation inventories,
   environmental modeling studies, and the ability to train broader
   datasets collected from air and space.}},
DOI = {{10.1016/j.ecolind.2017.09.034}},
ISSN = {{1470-160X}},
EISSN = {{1872-7034}},
Unique-ID = {{ISI:000425828200076}},
}

@article{ ISI:000424092300038,
Author = {Zhou, Tan and Popescu, Sorin C. and Lawing, A. Michelle and Eriksson,
   Marian and Strimbu, Bogdan M. and Buerkner, Paul C.},
Title = {{Bayesian and Classical Machine Learning Methods: A Comparison for Tree
   Species Classification with LiDAR Waveform Signatures}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{A plethora of information contained in full-waveform (FW) Light
   Detection and Ranging (LiDAR) data offers prospects for characterizing
   vegetation structures. This study aims to investigate the capacity of FW
   LiDAR data alone for tree species identification through the integration
   of waveform metrics with machine learning methods and Bayesian
   inference. Specifically, we first conducted automatic tree segmentation
   based on the waveform-based canopy height model (CHM) using three
   approaches including TreeVaW, watershed algorithms and the combination
   of TreeVaW and watershed (TW) algorithms. Subsequently, the Random
   forests (RF) and Conditional inference forests (CF) models were employed
   to identify important tree-level waveform metrics derived from three
   distinct sources, such as raw waveforms, composite waveforms, the
   waveform-based point cloud and the combined variables from these three
   sources. Further, we discriminated tree (gray pine, blue oak, interior
   live oak) and shrub species through the RF, CF and Bayesian multinomial
   logistic regression (BMLR) using important waveform metrics identified
   in this study. Results of the tree segmentation demonstrated that the TW
   algorithms outperformed other algorithms for delineating individual tree
   crowns. The CF model overcomes waveform metrics selection bias caused by
   the RF model which favors correlated metrics and enhances the accuracy
   of subsequent classification. We also found that composite waveforms are
   more informative than raw waveforms and waveform-based point cloud for
   characterizing tree species in our study area. Both classical machine
   learning methods (the RF and CF) and the BMLR generated satisfactory
   average overall accuracy (74\% for the RF, 77\% for the CF and 81\% for
   the BMLR) and the BMLR slightly outperformed the other two methods.
   However, these three methods suffered from low individual classification
   accuracy for the blue oak which is prone to being misclassified as the
   interior live oak due to the similar characteristics of blue oak and
   interior live oak. Uncertainty estimates from the BMLR method compensate
   for this downside by providing classification results in a probabilistic
   sense and rendering users with more confidence in interpreting and
   applying classification results to real-world tasks such as forest
   inventory. Overall, this study recommends the CF method for feature
   selection and suggests that BMLR could be a superior alternative to
   classical machining learning methods.}},
DOI = {{10.3390/rs10010039}},
Article-Number = {{39}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Lawing, Michelle/F-7453-2019
   Popescu, Sorin C/D-5981-2015}},
ORCID-Numbers = {{Lawing, Michelle/0000-0003-4041-6177
   Popescu, Sorin C/0000-0002-8155-8801}},
Unique-ID = {{ISI:000424092300038}},
}

@article{ ISI:000423587100013,
Author = {Zhao, Jun-Li and Wu, Zhong-Ke and Pan, Zhen-Kuan and Duan, Fu-Qing and
   Li, Jin-Hua and Lv, Zhi-Han and Wang, Kang and Chen, Yu-Cong},
Title = {{3D Face Similarity Measure by Fr,chet Distances of Geodesics}},
Journal = {{JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY}},
Year = {{2018}},
Volume = {{33}},
Number = {{1}},
Pages = {{207-222}},
Month = {{JAN}},
Abstract = {{3D face similarity is a critical issue in computer vision, computer
   graphics and face recognition and so on. Since Fr,chet distance is an
   effective metric for measuring curve similarity, a novel 3D face
   similarity measure method based on Fr,chet distances of geodesics is
   proposed in this paper. In our method, the surface similarity between
   two 3D faces is measured by the similarity between two sets of 3D curves
   on them. Due to the intrinsic property of geodesics, we select geodesics
   as the comparison curves. Firstly, the geodesics on each 3D facial model
   emanating from the nose tip point are extracted in the same initial
   direction with equal angular increment. Secondly, the Fr,chet distances
   between the two sets of geodesics on the two compared facial models are
   computed. At last, the similarity between the two facial models is
   computed based on the Fr,chet distances of the geodesics obtained in the
   second step. We verify our method both theoretically and practically. In
   theory, we prove that the similarity of our method satisfies three
   properties: reflexivity, symmetry, and triangle inequality. And in
   practice, experiments are conducted on the open 3D face database GavaDB,
   Texas 3D Face Recognition database, and our 3D face database. After the
   comparison with iso-geodesic and Hausdorff distance method, the results
   illustrate that our method has good discrimination ability and can not
   only identify the facial models of the same person, but also distinguish
   the facial models of any two different persons.}},
DOI = {{10.1007/s11390-018-1814-7}},
ISSN = {{1000-9000}},
EISSN = {{1860-4749}},
Unique-ID = {{ISI:000423587100013}},
}

@article{ ISI:000422943700008,
Author = {Benedek, Csaba and Galai, Bence and Nagy, Balazs and Janko, Zsolt},
Title = {{Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance
   System}},
Journal = {{IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY}},
Year = {{2018}},
Volume = {{28}},
Number = {{1}},
Pages = {{101-113}},
Month = {{JAN}},
Abstract = {{This paper presents new approaches for gait and activity analysis based
   on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed
   algorithms are embedded into an integrated 4D vision and visualization
   system, which is able to analyze and interactively display real
   scenarios in natural outdoor environments with walking pedestrians. The
   main focus of the investigations is gait-based person reidentification
   during tracking and recognition of specific activity patterns, such as
   bending, waving, making phone calls, and checking the time looking at
   wristwatches. The descriptors for training and recognition are observed
   and extracted from realistic outdoor surveillance scenarios, where
   multiple pedestrians are walking in the field of interest following
   possibly intersecting trajectories; thus, the observations might often
   be affected by occlusions or background noise. Since there is no public
   database available for such scenarios, we created and published a new
   Lidar-based outdoor gait and activity data set on our website that
   contains point cloud sequences of 28 different persons extracted and
   aggregated from 35-min-long measurements. The presented results confirm
   that both efficient gait-based identification and activity recognition
   are achievable in the sparse point clouds of a single RMB Lidar sensor.
   After extracting the people trajectories, we synthesized a
   free-viewpoint video, in which moving avatar models follow the
   trajectories of the observed pedestrians in real time, ensuring that the
   leg movements of the animated avatars are synchronized with the real
   gait cycles observed in the Lidar stream.}},
DOI = {{10.1109/TCSVT.2016.2595331}},
ISSN = {{1051-8215}},
EISSN = {{1558-2205}},
ORCID-Numbers = {{Benedek, Csaba/0000-0003-3203-0741}},
Unique-ID = {{ISI:000422943700008}},
}

@article{ ISI:000418513500004,
Author = {Sghaier, Souhir and Farhat, Wajdi and Souani, Chokri},
Title = {{Novel Technique for 3D Face Recognition Using Anthropometric Methodology}},
Journal = {{INTERNATIONAL JOURNAL OF AMBIENT COMPUTING AND INTELLIGENCE}},
Year = {{2018}},
Volume = {{9}},
Number = {{1}},
Pages = {{60-77}},
Month = {{JAN-MAR}},
Abstract = {{This manuscript presents an improved system research that can detect and
   recognize the person in 3D space automatically and without the
   interaction of the people's faces. This system is based not only on a
   quantum computation and measurements to extract the vector features in
   the phase of characterization but also on learning algorithm (using SVM)
   to classify and recognize the person. This research presents an improved
   technique for automatic 3D face recognition using anthropometric
   proportions and measurement to detect and extract the area of interest
   which is unaffected by facial expression. This approach is able to treat
   incomplete and noisy images and reject the non-facial areas
   automatically. Moreover, it can deal with the presence of holes in the
   meshed and textured 3D image. It is also stable against small
   translation and rotation of the face. All the experimental tests have
   been done with two 3D face datasets FRAV 3D and GAVAB. Therefore, the
   test's results of the proposed approach are promising because they
   showed that it is competitive comparable to similar approaches in terms
   of accuracy, robustness, and flexibility. It achieves a high recognition
   performance rate of 95.35\% for faces with neutral and non-neutral
   expressions for the identification and 98.36\% for the authentification
   with GAVAB and 100\% with some gallery of FRAV 3D datasets.}},
DOI = {{10.4018/IJACI.2018010104}},
ISSN = {{1941-6237}},
EISSN = {{1941-6245}},
ResearcherID-Numbers = {{Farhat, Wajdi/N-5341-2015
   Souani, Chokri/B-1853-2015}},
ORCID-Numbers = {{Farhat, Wajdi/0000-0003-3647-8316
   Souani, Chokri/0000-0002-8987-3582}},
Unique-ID = {{ISI:000418513500004}},
}

@article{ ISI:000413384000029,
Author = {Barnes, Chloe and Balzter, Heiko and Barrett, Kirsten and Eddy, James
   and Milrier, Sam and Suarez, Juan C.},
Title = {{Airborne laser scanning and tree crown fragmentation metrics for the
   assessment of Phytophthora ramorum infected larch forest stands}},
Journal = {{FOREST ECOLOGY AND MANAGEMENT}},
Year = {{2017}},
Volume = {{404}},
Pages = {{294-305}},
Month = {{NOV 15}},
Abstract = {{The invasive phytopathogen Phytophthora ramorum has caused extensive
   infection of larch forest across areas of the UK, particularly in
   Southwest England, South Wales and Southwest Scotland. At present,
   landscape level assessment of the disease in these areas is conducted
   manually by tree health surveyors during helicopter surveys. Airborne
   laser scanning (ALS), also known as LiDAR, has previously been applied
   to the segmentation of larch tree crowns infected by P. ramorum
   infection and the detection of insect pests in coniferous tree species.
   This study evaluates metrics from high-density discrete ALS point clouds
   (24 points/m(2)) and canopy height models (CHMs) to identify individual
   trees infected with P. ramorum and to discriminate between four disease
   severity categories (NI: not infected, 1: light, 2: moderate, 3: heavy).
   The metrics derived from ALS point clouds include canopy cover,
   skewness, and bicentiles (B60, B70, B80 and B90) calculated using both a
   static (1 m) and a variable (50\% of tree height) cut-off height.
   Significant differences are found between all disease severity
   categories, except in the case of healthy individuals (NI) and those in
   the early stages of infection (category 1). In addition, fragmentation
   metrics are shown to identify the increased patchiness and infra-crown
   height irregularities of CHMs associated with individual trees subject
   to heavy infection (category 3) of P. ramorum. Classifications using a
   k-nearest neighbour (k-NN) classifier and ALS point cloud metrics to
   classify disease presence/absence and severity yielded overall
   accuracies of 72\% and 65\% respectively. The results indicate that ALS
   can be used to identify individual tree crowns subject to moderate and
   heavy P. ramorum infection in larch forests. This information
   demonstrates the potential applications of ALS for the development of a
   targeted phytosanitary approach for the management of P. ramorum.}},
DOI = {{10.1016/j.foreco.2017.08.052}},
ISSN = {{0378-1127}},
EISSN = {{1872-7042}},
ResearcherID-Numbers = {{Balzter, Heiko/B-5976-2008}},
ORCID-Numbers = {{Balzter, Heiko/0000-0002-9053-4684}},
Unique-ID = {{ISI:000413384000029}},
}

@article{ ISI:000416554100095,
Author = {Shen, Xin and Cao, Lin},
Title = {{Tree-Species Classification in Subtropical Forests Using Airborne
   Hyperspectral and LiDAR Data}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Accurate classification of tree-species is essential for sustainably
   managing forest resources and effectively monitoring species diversity.
   In this study, we used simultaneously acquired hyperspectral and LiDAR
   data from LiCHy (Hyperspectral, LiDAR and CCD) airborne system to
   classify tree-species in subtropical forests of southeast China. First,
   each individual tree crown was extracted using the LiDAR data by a point
   cloud segmentation algorithm (PCS) and the sunlit portion of each crown
   was selected using the hyperspectral data. Second, different suites of
   hyperspectral and LiDAR metrics were extracted and selected by the
   indices of Principal Component Analysis (PCA) and the mean decrease in
   Gini index (MDG) from Random Forest (RF). Finally, both hyperspectral
   metrics (based on whole crown and sunlit crown) and LiDAR metrics were
   assessed and used as inputs to Random Forest classifier to discriminate
   five tree-species at two levels of classification. The results showed
   that the tree delineation approach (point cloud segmentation algorithm)
   was suitable for detecting individual tree in this study (overall
   accuracy = 82.9\%). The classification approach provided a relatively
   high accuracy (overall accuracy > 85.4\%) for classifying five
   tree-species in the study site. The classification using both
   hyperspectral and LiDAR metrics resulted in higher accuracies than only
   hyperspectral metrics (the improvement of overall accuracies =
   0.4-5.6\%). In addition, compared with the classification using whole
   crown metrics (overall accuracies = 85.4-89.3\%), using sunlit crown
   metrics (overall accuracies = 87.1-91.5\%) improved the overall
   accuracies of 2.3\%. The results also suggested that fewer of the most
   important metrics can be used to classify tree-species effectively
   (overall accuracies = 85.8-91.0\%).}},
DOI = {{10.3390/rs9111180}},
Article-Number = {{1180}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000416554100095}},
}

@article{ ISI:000409180500015,
Author = {Sui, Dan and Hou, Deheng and Duan, Xinyu},
Title = {{An interpolation algorithm fitted for dynamic 3D face reconstruction}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2017}},
Volume = {{76}},
Number = {{19}},
Pages = {{19575-19589}},
Month = {{OCT}},
Abstract = {{In order to solve the problem of low recognition accuracy in later
   period which is caused by the too few extracted parameters in the 3D
   face recognition, and the incapable formation of completed point cloud
   structure. An automatic iterative interpolation algorithm is proposed.
   The new and more accurate 3D face data points are obtained by automatic
   iteration. This algorithm can be used to restore the data point cloud
   information of 3D facial feature in 2D images by means of facial
   three-legged structure formed by 3D face and automatic interpolation.
   Thus, it can realize to shape the 3D facial dynamic model which can be
   recognized and has high saturability. Experimental results show that the
   interpolation algorithm can achieve the complete the construction of
   facial feature based on the facial feature after 3D dynamic
   reconstruction, and the validity is higher.}},
DOI = {{10.1007/s11042-015-3233-x}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000409180500015}},
}

@article{ ISI:000404319800008,
Author = {Zhang, Jian and Li, Ke and Liang, Yun and Li, Na},
Title = {{Learning 3D faces from 2D images via Stacked Contractive Autoencoder}},
Journal = {{NEUROCOMPUTING}},
Year = {{2017}},
Volume = {{257}},
Pages = {{67-78}},
Month = {{SEP 27}},
Abstract = {{3D face reconstruction from a 2D face image has been found important to
   various applications such as face detection and recognition because a 3D
   face provides more semantic information than 2D image. This paper
   proposes a deep learning framework for 3D face reconstruction. The
   framework is designed to compute subspace feature of arbitrary face
   image, then map the feature to its counterpart in another subspace
   learned with 3D faces, and reconstruct the 3D face using the counterpart
   feature. During the course of training, we learn 2D and 3D subspaces
   through Stacked Contractive Autoencoders (SCAE), use a one-layer fully
   connected neural network to learn the mapping, and use the pre-trained
   parameters of the SCAEs and the one-layer network to initialize a deep
   feedforward neural network whose input are face images and output are 3D
   faces. The network is optimized by gradient descent algorithm with
   back-propagation. Extensive experimental results on various data sets
   indicate the effectiveness of the proposed SCAE-based 3D face
   reconstruction method. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2016.11.062}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000404319800008}},
}

@article{ ISI:000411153300086,
Author = {Liu, Ting and Lv, Jun and Qin, Yutao},
Title = {{Standardized tumor volume: an independent prognostic factor in advanced
   nasopharyngeal carcinoma}},
Journal = {{ONCOTARGET}},
Year = {{2017}},
Volume = {{8}},
Number = {{41}},
Pages = {{70299-70309}},
Month = {{SEP 19}},
Abstract = {{The study evaluated the prognostic effect of standardized tumor volume
   in patients with advanced nasopharyngeal carcinoma (NPC) treated with
   concurrent chemoradiotherapy. Between Jan 1, 2009 and December 30, 2012,
   143 patients diagnosed with NPC in UICC stage III-IVb by histopathology
   were enrolled in the study. These patients underwent intensity-modulated
   radiotherapy combined with concurrent chemotherapy. The
   three-dimensional images of tumor volume were reconstructed
   automatically by the treatment planning system. SGTVnx was calculated
   based on GTVnx/person's volume. SGTVnd was calculated based on
   GTVnd/person's volume. SGTVnx was significantly associated with the
   5-year overall survival (OS), disease-free survival (DFS), DMFS, and
   LRFS rates in univariate and multivariate analyses. Although SGTVnd was
   associated with the 5-year OS, DFS, and DMFS rates, it was not an
   independent prognostic factor for LRFS. In receiver operating
   characteristic (ROC) curve analysis, 1.091 and 0.273 were determined as
   the cut-off points for SGTVnx and SGTVnd, respectively. The 5-year OS,
   DFS, DMFS, and LRFS rates for patients with a SGTVnx > 1.091 vs. SGTVnx
   <= 1.091 was 65.4\% vs. 93.4\% (P < 0.001), 65.2\% vs. 94.8\% (P <
   0.001), 71.4\% vs. 97.4\% (P < 0.001), and 84.8\% vs. 97.3\% (P =
   0.003), respectively, for SGTVnd > 0.273 vs. SGTVnd <= 0.273 was 70.3\%
   vs. 96.5\% (P < 0.001), 70.1\% vs. 94.8\% (P < 0.001), 77.5\% vs. 98.2\%
   (P < 0.001), and 88.5\% vs. 96.6\% (P = 0.049), respectively. UICC stage
   grouping, T classification, N classification, and sex were not found to
   be independent prognostic factors for NPC. Standardized tumor volume was
   an independent prognostic factor for NPC that might improve the current
   NPC TNM classification system and provide new clinical evidence for
   personalized treatment strategies.}},
DOI = {{10.18632/oncotarget.20313}},
ISSN = {{1949-2553}},
Unique-ID = {{ISI:000411153300086}},
}

@article{ ISI:000410059200037,
Author = {Quint, S. and Christ, A. F. and Guckenberger, A. and Himbert, S. and
   Kaestner, L. and Gekle, S. and Wagner, C.},
Title = {{3D tomography of cells in micro-channels}},
Journal = {{APPLIED PHYSICS LETTERS}},
Year = {{2017}},
Volume = {{111}},
Number = {{10}},
Month = {{SEP 4}},
Abstract = {{We combine confocal imaging, microfluidics, and image analysis to record
   3D-images of cells in flow. This enables us to recover the full 3D
   representation of several hundred living cells per minute. Whereas 3D
   confocal imaging has thus far been limited to steady specimens, we
   overcome this restriction and present a method to access the 3D shape of
   moving objects. The key of our principle is a tilted arrangement of the
   micro-channel with respect to the focal plane of the microscope. This
   forces cells to traverse the focal plane in an inclined manner. As a
   consequence, individual layers of passing cells are recorded, which can
   then be assembled to obtain the volumetric representation. The full 3D
   information allows for a detailed comparison with theoretical and
   numerical predictions unfeasible with, e.g., 2D imaging. Our technique
   is exemplified by studying flowing red blood cells in a micro-channel
   reflecting the conditions prevailing in the microvasculature. We observe
   two very different types of shapes: ``croissants{''} and ``slippers.{''}
   Additionally, we perform 3D numerical simulations of our experiment to
   confirm the observations. Since 3D confocal imaging of cells in flow has
   not yet been realized, we see high potential in the field of flow
   cytometry where cell classification thus far mostly relies on 1D
   scattering and fluorescence signals. Published by AIP Publishing.}},
DOI = {{10.1063/1.4986392}},
Article-Number = {{103701}},
ISSN = {{0003-6951}},
EISSN = {{1077-3118}},
ResearcherID-Numbers = {{Wagner, Christian/A-1307-2009
   Gekle, Stephan/I-9695-2014}},
ORCID-Numbers = {{Wagner, Christian/0000-0001-7788-4594
   Gekle, Stephan/0000-0001-5597-1160}},
Unique-ID = {{ISI:000410059200037}},
}

@article{ ISI:000412378800003,
Author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Benouareth, Abdallah
   and Declercq, David},
Title = {{3D facial expression recognition using kernel methods on Riemannian
   manifold}},
Journal = {{ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE}},
Year = {{2017}},
Volume = {{64}},
Pages = {{25-32}},
Month = {{SEP}},
Abstract = {{Automatic human Facial Expressions Recognition (FER) is becoming of
   increased interest. FER finds its applications in many emerging areas
   such as affective computing and intelligent human computer interaction.
   Most of the existing work on FER has been done using 2D data which
   suffers from inherent problems of illumination changes and pose
   variations. With the development of 3D image capturing technologies, the
   acquisition of 3D data is becoming a more feasible task. The 3D data
   brings a more effective solution in addressing the issues raised by its
   2D counterpart. State-of-the-art 3D FER methods are often based on a
   single descriptor which may fail to handle the large inter-class and
   intra-class variability of the human facial expressions. In this work,
   we explore, for the first time, the usage of covariance matrices of
   descriptors, instead of the descriptors themselves, in 3D FER. Since
   covariance matrices are elements of the non-linear manifold of Symmetric
   Positive Definite (SPD) matrices, we particularly look at the
   application of manifold-based classification to the problem of 3D FER.
   We evaluate the performance of the proposed framework on the BU-3DFE and
   the Bosphorus datasets, and demonstrate its superiority compared to the
   state-of-the-art methods. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.engappai.2017.05.009}},
ISSN = {{0952-1976}},
EISSN = {{1873-6769}},
Unique-ID = {{ISI:000412378800003}},
}

@article{ ISI:000411548000002,
Author = {Jimenez-Pique, E. and Turon-Vinas, M. and Chen, H. and Trifonov, T. and
   Fair, J. and Tarres, E. and Llanes, L.},
Title = {{Focused ion beam tomography of WC-Co cemented carbides}},
Journal = {{INTERNATIONAL JOURNAL OF REFRACTORY METALS \& HARD MATERIALS}},
Year = {{2017}},
Volume = {{67}},
Pages = {{9-17}},
Month = {{SEP}},
Abstract = {{The microstructure of three different grades of WC-Co cemented carbides
   (hardmetals) has been reconstructed in three dimensions after sequential
   images obtained by focused ion beam. The three dimensional
   microstructual parameters are compared against the well-known two
   dimensional parameters of grain size, phase percentages and mean free
   path. Results show good agreement with the exception of individual grain
   recognition, which could not be univocally segmented. In the case of
   mean free path, the three-dimensional image depicts a more realistic
   description of the metal interconnections in the composite. Aiming for a
   simple example of direct application of these FIB tomography outcomes,
   reconstructed real microstructure for the coarser hardmetal grade
   studied was translated in a finite element modelling mesh, and elastic
   residual stresses were estimated from sintering to room temperature.
   Calculated thermal stresses agree with experimental results and show
   significant local variations in their value due to the complex
   microstructure of cemented carbides.}},
DOI = {{10.1016/j.ijrmhm.2017.04.007}},
ISSN = {{0263-4368}},
ResearcherID-Numbers = {{LLANES, LUIS/H-9761-2015
   }},
ORCID-Numbers = {{LLANES, LUIS/0000-0003-1054-1073
   Jimenez-Pique, Emilio/0000-0002-6950-611X}},
Unique-ID = {{ISI:000411548000002}},
}

@article{ ISI:000408398200010,
Author = {Eng, Z. H. D. and Yick, Y. Y. and Guo, Y. and Xu, H. and Reiner, M. and
   Cham, T. J. and Chen, S. H. A.},
Title = {{3D faces are recognized more accurately and faster than 2D faces, but
   with similar inversion effects}},
Journal = {{VISION RESEARCH}},
Year = {{2017}},
Volume = {{138}},
Pages = {{78-85}},
Month = {{SEP}},
Abstract = {{Recognition of faces typically occurs via holistic processing where
   individual features are combined to provide an overall facial
   representation. However, when faces are inverted, there is greater
   reliance on featural processing where faces are recognized based on
   their individual features. These findings are based on a substantial
   number of studies using 2-dimensional (2D) faces and it is unknown
   whether these results can be extended to 3-dimensional (3D) faces, which
   have more depth information that is absent in the typical 2D stimuli
   used in face recognition literature. The current study used the face
   inversion paradigm as a means to investigate how holistic and featural
   processing are differentially influenced by 2D and 3D faces. Twenty-five
   participants completed a delayed face-matching task consisting of
   upright and inverted faces that were presented as both 2D and 3D
   stereoscopic images. Recognition accuracy was significantly higher for
   3D upright faces compared to 2D upright faces, providing support that
   the enriched visual information in 3D stereoscopic images facilitates
   holistic processing that is essential for the recognition of upright
   faces. Typical face inversion effects were also obtained, regardless of
   whether the faces were presented in 2D or 3D. Moreover, recognition
   performances for 2D inverted and 3D inverted faces did not differ. Taken
   together, these results demonstrated that 3D stereoscopic effects
   influence face recognition during holistic processing but not during
   featural processing. Our findings therefore provide a novel perspective
   that furthers our understanding of face recognition mechanisms, shedding
   light on how the integration of stereoscopic information in 3D faces
   influences face recognition processes. (c) 2017 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.visres.2017.06.004}},
ISSN = {{0042-6989}},
EISSN = {{1878-5646}},
ResearcherID-Numbers = {{Chen, SH Annabel/F-3742-2011
   }},
ORCID-Numbers = {{Chen, SH Annabel/0000-0002-1540-5516
   Xu, Hong/0000-0003-1389-5408}},
Unique-ID = {{ISI:000408398200010}},
}

@article{ ISI:000407961700001,
Author = {Damon, James and Gasparovic, Ellen},
Title = {{Modeling Multi-object Configurations via Medial/Skeletal Linking
   Structures}},
Journal = {{INTERNATIONAL JOURNAL OF COMPUTER VISION}},
Year = {{2017}},
Volume = {{124}},
Number = {{3}},
Pages = {{255-272}},
Month = {{SEP}},
Abstract = {{We introduce a method for modeling a configuration of objects in 2D or
   3D images using a mathematical ``skeletal linking structure{''} which
   will simultaneously capture the individual shape features of the objects
   and their positional information relative to one another. The objects
   may either have smooth boundaries and be disjoint from the others or
   share common portions of their boundaries with other objects in a
   piecewise smooth manner. These structures include a special class of
   ``Blum medial linking structures{''}, which are intrinsically associated
   to the configuration and build upon the Blum medial axes of the
   individual objects. We give a classification of the properties of Blum
   linking structures for generic configurations. The skeletal linking
   structures add increased flexibility for modeling configurations of
   objects by relaxing the Blum conditions and they extend in a minimal way
   the individual ``skeletal structures{''} which have been previously used
   for modeling individual objects and capturing their geometric
   properties. This allows for the mathematical methods introduced for
   single objects to be significantly extended to the entire configuration
   of objects. These methods not only capture the internal shape structures
   of the individual objects but also the external structure of the
   neighboring regions of the objects. In the subsequent second paper
   (Damon and Gasparovic in Shape and positional geometry of multi-object
   configurations) we use these structures to identify specific external
   regions which capture positional information about neighboring objects,
   and we develop numerical measures for closeness of portions of objects
   and their significance for the configuration. This allows us to use the
   same mathematical structures to simultaneously analyze both the shape
   properties of the individual objects and positional properties of the
   configuration. This provides a framework for analyzing the statistical
   properties of collections of similar configurations such as for
   applications to medical imaging.}},
DOI = {{10.1007/s11263-017-1019-5}},
ISSN = {{0920-5691}},
EISSN = {{1573-1405}},
Unique-ID = {{ISI:000407961700001}},
}

@article{ ISI:000403135200018,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal and Eastwood, Peter},
Title = {{Deep, dense and accurate 3D face correspondence for generating
   population specific deformable models}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2017}},
Volume = {{69}},
Pages = {{238-250}},
Month = {{SEP}},
Abstract = {{We present a multilinear algorithm to automatically establish dense
   point-to-point correspondence over an arbitrarily large number of
   population specific 3D faces across identities, facial expressions and
   poses. The algorithm is initialized with a subset of anthropometric
   landmarks detected by our proposed Deep Landmark Identification Network
   which is trained on synthetic images. The landmarks are used to segment
   the 3D face into Voronoi regions by evolving geodesic level set curves.
   Exploiting the intrinsic features of these regions, we extract
   discriminative keypoints on the facial manifold to elastically match the
   regions across faces for establishing dense correspondence. Finally, we
   generate a Region based 3D Deformable Model which is fitted to unseen
   faces to transfer the correspondences. We evaluate our algorithm on the
   tasks of facial landmark detection and recognition using two benchmark
   datasets. Comparison with thirteen state-of-the-art techniques shows the
   efficacy of our algorithm. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2017.04.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Eastwood, Peter/0000-0002-4490-4138
   Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000403135200018}},
}

@article{ ISI:000413881700007,
Author = {Ouamane, Abdelmalik and Boutellaa, Elhocine and Bengherabi, Messaoud and
   Taleb-Ahmed, Abdelmalik and Hadid, Abdenour},
Title = {{A novel statistical and multiscale local binary feature for 2D and 3D
   face verification}},
Journal = {{COMPUTERS \& ELECTRICAL ENGINEERING}},
Year = {{2017}},
Volume = {{62}},
Pages = {{68-80}},
Month = {{AUG}},
Abstract = {{In this paper, we propose a face verification framework using 2D and 3D
   images. We first introduce a novel face descriptor based on the local
   statistics of the 2D and 3D images. In the proposed framework, the novel
   descriptor is combined with three other popular and effective local
   descriptors, namely, Local Binary Patterns (LBP), Local Phase
   Quantization (LPQ) and Binarized Statistical Image Features (BSIF). The
   multiscale variants of these four descriptors are investigated seeking
   better performance. To reduce the feature vector dimensionality and
   mitigate the class intra-variability, we use Exponential Discriminant
   Analysis (EDA) and Within Class Covariance Normalization (WCCN),
   respectively. Finally, a score level fusion scheme is adopted to combine
   different face descriptors and modalities. An extensive evaluation of
   the proposed framework is carried out on two publicly available and
   largely used 2D+3D face databases, namely FRGC v2.0 and CAISA 3D.
   Promising results that favorably compare to the state of the art are
   obtained. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.compeleceng.2017.01.001}},
ISSN = {{0045-7906}},
EISSN = {{1879-0755}},
Unique-ID = {{ISI:000413881700007}},
}

@article{ ISI:000410870200001,
Author = {Jo, Jaeik and Kim, Hyunjun and Kim, Jaihie},
Title = {{3D facial shape reconstruction using macro- and micro-level features
   from high resolution facial images}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{64}},
Pages = {{1-9}},
Month = {{AUG}},
Abstract = {{Three-dimensional (3D) facial modeling and stereo matching-based methods
   are widely used for 3D facial reconstruction from 2D single-view and
   multiple-view images. However, these methods cannot realistically
   reconstruct 3D faces because they use insufficient numbers of
   macro-level Facial Feature Points (FFPs). This paper proposes an
   accurate and person-specific 3D facial reconstruction method that uses
   ample numbers of macro and micro-level FFPs to enable coverage of all
   facial regions of high resolution facial images. Comparisons of 3D
   facial images reconstructed using the proposed method for ground-truth
   3D facial images from the Bosphorus 3D database show that the method is
   superior to a conventional Active Appearance Model-Structure from Motion
   (AAM + SfM)-based method in terms of average 3D root mean square error
   between the reconstructed and ground-truth 3D faces. Further, the
   proposed method achieved outstanding accuracy in local facial regions
   such as the cheek areas where extraction of FFPs is difficult for
   existing methods. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/jimavis.2017.05.001}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000410870200001}},
}

@article{ ISI:000410870200008,
Author = {Xia, Baiqiang and Ben Amor, Boulbaba and Daoudi, Mohamed},
Title = {{= Joint gender, ethnicity and age estimation from 3D faces An
   experimental illustration of their correlations}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{64}},
Pages = {{90-102}},
Month = {{AUG}},
Abstract = {{Humans present clear demographic traits which allow their peers to
   recognize their gender and ethnic groups as well as estimate their age.
   Abundant literature has investigated the problem of automated gender,
   ethnicity and age recognition from facial images. However, despite the
   co-existence of these traits, most of the studies have addressed them
   separately, very little attention has been given to their correlations.
   In this work, we address the problem of joint demographic estimation and
   investigate the correlation through the morphological differences in 3D
   facial shapes. To this end, a set of facial features are extracted to
   capture the 3D shape differences among the demographic groups. Then, a
   correlation-based feature selection is applied to highlight salient
   features and remove redundancy. These features are later fed to Random
   Forest for gender and ethnicity classification, and age estimation.
   Extensive experiments conducted on FRGCv2 dataset, under
   Expression-Dependent and Expression-Independent settings, demonstrate
   the effectiveness of the proposed approaches for the three traits, and
   also show the accuracy improvement when considering their correlations.
   To the best of our knowledge, this is the first study exploring the
   correlations of these facial soft-biometric traits using 3D faces. This
   is also the first work which studies the problem of age estimation from
   3D Faces.(1) (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2017.06.004}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305}},
Unique-ID = {{ISI:000410870200008}},
}

@article{ ISI:000403860600008,
Author = {Zhu, Qing and Li, Yuan and Hu, Han and Wu, Bo},
Title = {{Robust point cloud classification based on multi-level semantic
   relationships for urban scenes}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{129}},
Pages = {{86-102}},
Month = {{JUL}},
Abstract = {{The semantic classification of point clouds is a fundamental part of
   three-dimensional urban reconstruction. For datasets with high spatial
   resolution but significantly more noises, a general trend is to exploit
   more contexture information to surmount the decrease of discrimination
   of features for classification. However, previous works on adoption of
   contexture information are either too restrictive or only in a small
   region and in this paper, we propose a point cloud classification method
   based on multi-level semantic relationships, including
   point-homogeneity, supervoxel-adjacency and class-knowledge constraints,
   which is more versatile and incrementally propagate the classification
   cues from individual points to the object level and formulate them as a
   graphical model. The point-homogeneity constraint clusters points with
   similar geometric and radiometric properties into regular-shaped
   supervoxels that correspond to the vertices in the graphical model. The
   supervoxel-adjacency constraint contributes to the pairwise interactions
   by providing explicit adjacent relationships between supervoxels. The
   class knowledge constraint operates at the object level based on
   semantic rules, guaranteeing the classification correctness of
   supervoxel clusters at that level. International Society of
   Photogrammetry and Remote Sensing (ISPRS) benchmark tests have shown
   that the proposed method achieves state-of-the-art performance with an
   average per-area completeness and correctness of 93.88\% and 95.78\%,
   respectively. The evaluation of classification of photogrammetric point
   clouds and DSM generated from aerial imagery confirms the method's
   reliability in several challenging urban scenes. (C) 2017 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2017.04.022}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Wu, Bo/J-6177-2012
   Hu, Han/V-5068-2018}},
ORCID-Numbers = {{Wu, Bo/0000-0001-9530-3044
   Hu, Han/0000-0003-1137-2208}},
Unique-ID = {{ISI:000403860600008}},
}

@article{ ISI:000403031400006,
Author = {Milenkovic, Milutin and Wagner, Wolfgang and Quast, Raphael and Hollaus,
   Markus and Ressl, Camillo and Pfeifer, Norbert},
Title = {{Total canopy transmittance estimated from small-footprint, full-waveform
   airborne LiDAR}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{128}},
Pages = {{61-72}},
Month = {{JUN}},
Abstract = {{Canopy transmittance is a directional and wavelength-specific physical
   parameter that quantifies the amount of radiation attenuated when
   passing through a vegetation layer. The parameter has been estimated
   from LiDAR data in many different ways over the years. While early LiDAR
   methods treated each returned echo equally or weighted the echoes
   according to their return order, recent methods have focused more on the
   echo energy. In this study, we suggest a new method of estimating the
   total canopy transmittance considering only the energy of ground echoes.
   Therefore, this method does not require assumptions for the reflectance
   or absorption behavior of vegetation. As the oblique looking geometry of
   LiDAR is explicitly considered, canopy transmittance can be derived for
   individual laser beams and can be mapped spatially. The method was
   applied on a contemporary full-waveform LiDAR data set collected under
   leaf-off conditions and over a study site that contains two sub regions:
   one with a mixed (coniferous and deciduous) forest and another that is
   predominantly a deciduous forest in an alluvial plain. The resulting
   canopy transmittance map was analyzed for both sub regions and compared
   to aerial photos and the well-known fractional cover method. A visual
   comparison with aerial photos showed that even single trees and small
   canopy openings are visible in the canopy transmittance map. In
   comparison with the fractional cover method, the canopy transmittance
   map showed no saturation, i.e., there was better separability between
   patches with different vegetation structure. (C) 2017 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2017.03.008}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Quast, Raphael/0000-0003-0419-4546
   Milenkovic, Milutin/0000-0003-3256-6669
   Pfeifer, Norbert/0000-0002-2348-7929}},
Unique-ID = {{ISI:000403031400006}},
}

@article{ ISI:000401888600006,
Author = {Coomes, David A. and Dalponte, Michele and Jucker, Tommaso and Asner,
   Gregory P. and Banin, Lindsay F. and Burslem, David F. R. P. and Lewis,
   Simon L. and Nilus, Reuben and Phillips, Oliver L. and Phua, Mui-How and
   Qie, Lan},
Title = {{Area-based vs tree-centric approaches to mapping forest carbon in
   Southeast Asian forests from airborne laser scanning data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2017}},
Volume = {{194}},
Pages = {{77-88}},
Month = {{JUN 1}},
Abstract = {{Tropical forests are a key component of the global carbon cycle, and
   mapping their carbon density is essential for understanding human
   influences on climate and for ecosystem-service-based payments for
   forest protection. Discrete-return airborne laser scanning (ALS) is
   increasingly recognised as a high-quality technology for mapping
   tropical forest carbon, because it generates 3D point clouds of forest
   structure from which aboveground carbon density (ACD) can be estimated.
   Area-based models are state of the art when it comes to estimating ACD
   from ALS data, but discard tree-level information contained within the
   ALS point cloud. This paper compares area based and tree-centric models
   for estimating ACD in lowland old-growth forests in Sabah, Malaysia.
   These forests are challenging to map because of their immense height. We
   compare the performance of (a) an area-based model developed by Asner
   and Mascaro (2014), and used primarily in the neotropics hitherto, with
   (b) a tree-centric approach that uses a new algorithm (itcSegment) to
   locate trees within the ALS canopy height model, measures their heights
   and crown widths, and calculates biomass from these dimensions. We find
   that Asner and Mascaro's model needed regional calibration, reflecting
   the distinctive structure of Southeast Asian forests. We also discover
   that forest basal area is closely related to canopy gap fraction
   measured by ALS, and use this finding to refine Asner and Mascaro's
   model. Finally, we show that our tree-centric approach is less accurate
   at estimating ACD than the best-performing area-based model (RMSE 18\%
   vs 13\%). Tree-centric modelling is appealing because it is based on
   summing the biomass of individual trees, but until algorithms can detect
   understory trees reliably and estimate biomass from crown dimensions
   precisely, areas-based modelling will remain the method of choice. (C)
   2017 The Authors. Published by Elsevier Inc.}},
DOI = {{10.1016/j.rse.2017.03.017}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Burslem, David FRP/F-1204-2019
   Jucker, Tommaso/S-4724-2017
   Phillips, Oliver L/A-1523-2011
   Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Burslem, David FRP/0000-0001-6033-0990
   Jucker, Tommaso/0000-0002-0751-6312
   Phillips, Oliver L/0000-0002-8993-6168
   Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000401888600006}},
}

@article{ ISI:000402350200003,
Author = {Eskandari, A. H. and Arjmand, N. and Shirazi-Adl, A. and Farahmand, F.},
Title = {{Subject-specific 2D/3D image registration and kinematics-driven
   musculoskeletal model of the spine}},
Journal = {{JOURNAL OF BIOMECHANICS}},
Year = {{2017}},
Volume = {{57}},
Pages = {{18-26}},
Month = {{MAY}},
Abstract = {{An essential input to the musculoskeletal (MS) trunk models that
   estimate muscle and spine forces is kinematics of the thorax, pelvis,
   and lumbar vertebrae. While thorax and pelvis kinematics are usually
   measured via skin motion capture devices (with inherent errors on the
   proper identification of the underlying bony landmarks and the relative
   skin-sensor-bone movements), those of the intervening lumbar vertebrae
   are commonly approximated at fixed proportions based on the
   thorax-pelvis kinematics. This study proposes an image-based kinematics
   measurement approach to drive subject-specific (musculature, geometry,
   mass, and center of masses) MS models. Kinematics of the thorax, pelvis,
   and individual lumbar vertebrae as well as disc inclinations, gravity
   loading, and musculature were all measured via different imaging
   techniques. The model estimated muscle and lumbar forces in various
   upright and flexed postures in which kinematics were obtained using
   upright fluoroscopy via 2D/3D image registration. Predictions of this
   novel image-kinematics-driven model (Img-KD) were compared with those of
   the traditional kinematics-driven (T-KD) model in which individual
   lumbar vertebral rotations were assumed based on thorax-pelvis
   orientations. Results indicated that while differences between Img-KD
   and T-KD models remained small for the force in the global muscles
   (attached to the thoracic cage) (<15\%), L4-S1 compression (<15\%), and
   shear (<20\%) forces in average for all the simulated tasks, they were
   relatively larger for the force in the local muscles (attached to the
   lumbar vertebrae). Assuming that the skin-based measurements of thorax
   and pelvis kinematics are accurate enough, the T-KD model predictions of
   spinal forces remain reliable. (C) 2017 Elsevier Ltd. All rights
   reserved,}},
DOI = {{10.1016/j.jbiomech.2017.03.011}},
ISSN = {{0021-9290}},
EISSN = {{1873-2380}},
ResearcherID-Numbers = {{Farahmand, Farzam/B-3921-2011}},
ORCID-Numbers = {{Farahmand, Farzam/0000-0001-8900-7003}},
Unique-ID = {{ISI:000402350200003}},
}

@article{ ISI:000390884300006,
Author = {Jones, Scott P. and Dwyer, Dominic M. and Lewis, Michael B.},
Title = {{The utility of multiple synthesized views in the recognition of
   unfamiliar faces}},
Journal = {{QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY}},
Year = {{2017}},
Volume = {{70}},
Number = {{5}},
Pages = {{906-918}},
Month = {{MAY}},
Abstract = {{The ability to recognize an unfamiliar individual on the basis of prior
   exposure to a photograph is notoriously poor and prone to errors, but
   recognition accuracy is improved when multiple photographs are
   available. In applied situations, when only limited real images are
   available (e.g., from a mugshot or CCTV image), the generation of new
   images might provide a technological prosthesis for otherwise fallible
   human recognition. We report two experiments examining the effects of
   providing computer-generated additional views of a target face. In
   Experiment 1, provision of computer-generated views supported better
   target face recognition than exposure to the target image alone and
   equivalent performance to that for exposure of multiple photograph
   views. Experiment 2 replicated the advantage of providing generated
   views, but also indicated an advantage for multiple viewings of the
   single target photograph. These results strengthen the claim that
   identifying a target face can be improved by providing multiple
   synthesized views based on a single target image. In addition, our
   results suggest that the degree of advantage provided by synthesized
   views may be affected by the quality of synthesized material.}},
DOI = {{10.1080/17470218.2016.1158302}},
ISSN = {{1747-0218}},
EISSN = {{1747-0226}},
ResearcherID-Numbers = {{Dwyer, Dominic Michael/D-1498-2009
   }},
ORCID-Numbers = {{Dwyer, Dominic Michael/0000-0001-8069-5508
   Lewis, Michael/0000-0002-5735-5318
   Jones, Scott/0000-0001-5516-4385}},
Unique-ID = {{ISI:000390884300006}},
}

@article{ ISI:000400214700018,
Author = {Dunham, Lisa and Wartman, Joseph and Olsen, Michael J. and O'Banion,
   Matthew and Cunningham, Keith},
Title = {{Rockfall Activity Index (RAI): A lidar-derived, morphology-based method
   for hazard assessment}},
Journal = {{ENGINEERING GEOLOGY}},
Year = {{2017}},
Volume = {{221}},
Pages = {{184-192}},
Month = {{APR 20}},
Abstract = {{In this paper, we introduce the Rockfall Activity Index (RAI), a point
   cloud-derived, high-resolution, morphology based approach for assessing
   rockfall hazards. With the RAI methodology, rockfall hazards are
   evaluated in a two-step procedure. First, morphological indices (local
   slope and roughness) are used to classify mass wasting processes acting
   on a rock-slope. These classifications are then used with estimated
   instability rates to map rockfall activity across an entire slope face.
   The rockfall hazard is quantified as the estimated annual kinetic energy
   produced by rockfall along 1-m length segments of a slope face. Field
   assessment of the RAI method at multiple study sites indicates that the
   morphology-derived classification and hazard assessment routines provide
   results that closely match the observed behavior and performance of rock
   slopes. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.enggeo.2017.03.009}},
ISSN = {{0013-7952}},
EISSN = {{1872-6917}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000400214700018}},
}

@article{ ISI:000425868600001,
Author = {Wang, Lamei and Chen, Wenfeng and Li, Hong},
Title = {{Use of 3D faces facilitates facial expression recognition in children}},
Journal = {{SCIENTIFIC REPORTS}},
Year = {{2017}},
Volume = {{7}},
Month = {{APR 3}},
Abstract = {{This study assessed whether presenting 3D face stimuli could facilitate
   children's facial expression recognition. Seventy-one children aged
   between 3 and 6 participated in the study. Their task was to judge
   whether a face presented in each trial showed a happy or fearful
   expression. Half of the face stimuli were shown with 3D representations,
   whereas the other half of the images were shown as 2D pictures. We
   compared expression recognition under these conditions. The results
   showed that the use of 3D faces improved the speed of facial expression
   recognition in both boys and girls. Moreover, 3D faces improved boys'
   recognition accuracy for fearful expressions. Since fear is the most
   difficult facial expression for children to recognize, the facilitation
   effect of 3D faces has important practical implications for children
   with difficulties in facial expression recognition. The potential
   benefits of 3D representation for other expressions also have
   implications for developing more realistic assessments of children's
   expression recognition.}},
DOI = {{10.1038/srep45464}},
Article-Number = {{45464}},
ISSN = {{2045-2322}},
ResearcherID-Numbers = {{Chen, Wenfeng/H-2424-2012
   }},
ORCID-Numbers = {{Chen, Wenfeng/0000-0002-4271-8366
   wang, la mei/0000-0002-9203-5539}},
Unique-ID = {{ISI:000425868600001}},
}

@article{ ISI:000401097300027,
Author = {Santoro, Valeria and Lubelli, Sergio and De Donno, Antonio and
   Inchingolo, Alessio and Lavecchia, Fulvio and Introna, Francesco},
Title = {{Photogrammetric 3D skull/photo superimposition: A pilot study}},
Journal = {{FORENSIC SCIENCE INTERNATIONAL}},
Year = {{2017}},
Volume = {{273}},
Pages = {{168-174}},
Month = {{APR}},
Abstract = {{The identification of bodies through the examination of skeletal remains
   holds a prominent place in the field of forensic investigations.
   Technological advancements in 3D facial acquisition techniques have led
   to the proposal of a new body identification technique that involves a
   combination of craniofacial superimposition and photogrammetry. The aim
   of this study was to test the method by superimposing various
   computerized 3D images of skulls onto various photographs of missing
   people taken while they were still alive in cases when there was a
   suspicion that the skulls in question belonged to them. The technique is
   divided into four phases: preparatory phase, 3d acquisition phase,
   superimposition phase, and metric image analysis 3d.
   The actual superimposition of the images was carried out in the fourth
   step. and was done so by comparing the skull images with the selected
   photos.
   Using a specific software, the two images (i.e. the 3D avatar and the
   photo of the missing person) were superimposed. Cross-comparisons of 5
   skulls discovered in a mass grave, and of 2 skulls retrieved in the
   crawlspace of a house were performed. The morphologyc phase reveals a
   full overlap between skulls and photos of disappeared persons. Metric
   phase reveals that correlation coefficients of this values, higher than
   0.998-0,997 allow to confirm identification hypothesis. (C) 2017
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.forsciint.2017.02.006}},
ISSN = {{0379-0738}},
EISSN = {{1872-6283}},
Unique-ID = {{ISI:000401097300027}},
}

@article{ ISI:000398720100091,
Author = {Weinmann, Martin and Weinmann, Michael and Mallet, Clement and Bredif,
   Mathieu},
Title = {{A Classification-Segmentation Framework for the Detection of Individual
   Trees in Dense MMS Point Cloud Data Acquired in Urban Areas}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{In this paper, we present a novel framework for detecting individual
   trees in densely sampled 3D point cloud data acquired in urban areas.
   Given a 3D point cloud, the objective is to assign point-wise labels
   that are both class-aware and instance-aware, a task that is known as
   instance-level segmentation. To achieve this, our framework addresses
   two successive steps. The first step of our framework is given by the
   use of geometric features for a binary point-wise semantic
   classification with the objective of assigning semantic class labels to
   irregularly distributed 3D points, whereby the labels are defined as
   ``tree points{''} and ``other points{''}. The second step of our
   framework is given by a semantic segmentation with the objective of
   separating individual trees within the ``tree points{''}. This is
   achieved by applying an efficient adaptation of the mean shift algorithm
   and a subsequent segment-based shape analysis relying on semantic rules
   to only retain plausible tree segments. We demonstrate the performance
   of our framework on a publicly available benchmark dataset, which has
   been acquired with a mobile mapping system in the city of Delft in the
   Netherlands. This dataset contains 10.13 M labeled 3D points among which
   17.6\% are labeled as ``tree points{''}. The derived results clearly
   reveal a semantic classification of high accuracy (up to 90.77\%) and an
   instance-level segmentation of high plausibility, while the simplicity,
   applicability and efficiency of the involved methods even allow applying
   the complete framework on a standard laptop computer with a reasonable
   processing time (less than 2.5 h).}},
DOI = {{10.3390/rs9030277}},
Article-Number = {{277}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Bredif, Mathieu/T-3029-2018
   }},
ORCID-Numbers = {{Bredif, Mathieu/0000-0003-0228-1232
   Mallet, Clement/0000-0002-2675-165X
   Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000398720100091}},
}

@article{ ISI:000399162400002,
Author = {Lindberg, Eva and Holmgren, Johan},
Title = {{Individual Tree Crown Methods for 3D Data from Remote Sensing}},
Journal = {{CURRENT FORESTRY REPORTS}},
Year = {{2017}},
Volume = {{3}},
Number = {{1}},
Pages = {{19-31}},
Month = {{MAR}},
Abstract = {{Purpose of Review The rapid development of remote sensing technology
   hasmade dense 3D data available from airborne laser scanning and
   recently also photogrammetric point clouds. This paper reviews methods
   for extraction of individual trees from 3D data and their applications
   in forestry and ecology.
   Recent Findings Methods for analysis of 3D data at tree level have been
   developed since the turn of the century. The first algorithms were based
   on 2D surface models of the upper contours of tree crowns. These methods
   are robust and provide information about the trees in the top-most
   canopy. There are also methods that use the complete 3D data. However,
   development of these 3D methods is still needed to include use of
   geometric properties. To detect a large fraction of the tallest trees, a
   surface model method generally gives the best results, but detection of
   smaller trees below the top-most canopy requires methods utilizing the
   whole point cloud. Several new sensors are now available with capability
   to describe the upper part of the canopy, which can be used to
   frequently update vegetation maps. Highly sensitive laser photo
   detectors have become available for civilian applications, which will
   enable acquisition of high-resolution 3D laser data for large areas to
   much lower costs.
   Summary Methods for ITC delineation from 3D data provide information
   about a large fraction of the trees, but there is still a challenge to
   make optimal use of the information from whole point cloud. Newly
   developed sensors might make ITC methods cheaper and feasible for large
   areas.}},
DOI = {{10.1007/s40725-017-0051-6}},
ISSN = {{2198-6436}},
Unique-ID = {{ISI:000399162400002}},
}

@article{ ISI:000398720100002,
Author = {Nevalainen, Olli and Honkavaara, Eija and Tuominen, Sakari and Viljanen,
   Niko and Hakala, Teemu and Yu, Xiaowei and Hyyppa, Juha and Saari,
   Heikki and Polonen, Ilkka and Imai, Nilton N. and Tommaselli, Antonio M.
   G.},
Title = {{Individual Tree Detection and Classification with UAV-Based
   Photogrammetric Point Clouds and Hyperspectral Imaging}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{Small unmanned aerial vehicle (UAV) based remote sensing is a rapidly
   evolving technology. Novel sensors and methods are entering the market,
   offering completely new possibilities to carry out remote sensing tasks.
   Three-dimensional (3D) hyperspectral remote sensing is a novel and
   powerful technology that has recently become available to small UAVs.
   This study investigated the performance of UAV-based photogrammetry and
   hyperspectral imaging in individual tree detection and tree species
   classification in boreal forests. Eleven test sites with 4151 reference
   trees representing various tree species and developmental stages were
   collected in June 2014 using a UAV remote sensing system equipped with a
   frame format hyperspectral camera and an RGB camera in highly variable
   weather conditions. Dense point clouds were measured photogrammetrically
   by automatic image matching using high resolution RGB images with a 5 cm
   point interval. Spectral features were obtained from the hyperspectral
   image blocks, the large radiometric variation of which was compensated
   for by using a novel approach based on radiometric block adjustment with
   the support of in-flight irradiance observations. Spectral and 3D point
   cloud features were used in the classification experiment with various
   classifiers. The best results were obtained with Random Forest and
   Multilayer Perceptron (MLP) which both gave 95\% overall accuracies and
   an F-score of 0.93. Accuracy of individual tree identification from the
   photogrammetric point clouds varied between 40\% and 95\%, depending on
   the characteristics of the area. Challenges in reference measurements
   might also have reduced these numbers. Results were promising,
   indicating that hyperspectral 3D remote sensing was operational from a
   UAV platform even in very difficult conditions. These novel methods are
   expected to provide a powerful tool for automating various environmental
   close-range remote sensing tasks in the very near future.}},
DOI = {{10.3390/rs9030185}},
Article-Number = {{185}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Imai, Nilton/O-8909-2018
   }},
ORCID-Numbers = {{Imai, Nilton/0000-0003-0516-0567
   Nevalainen, Olli/0000-0002-4826-2929
   Honkavaara, Eija/0000-0002-7236-2145}},
Unique-ID = {{ISI:000398720100002}},
}

@article{ ISI:000398720100102,
Author = {Ni, Huan and Lin, Xiangguo and Zhang, Jixian},
Title = {{Classification of ALS Point Cloud with Improved Point Cloud Segmentation
   and Random Forests}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{This paper presents an automated and effective framework for classifying
   airborne laser scanning (ALS) point clouds. The framework is composed of
   four stages: (i) step-wise point cloud segmentation, (ii) feature
   extraction, (iii) Random Forests (RF) based feature selection and
   classification, and (iv) post-processing. First, a step-wise point cloud
   segmentation method is proposed to extract three kinds of segments,
   including planar, smooth and rough surfaces. Second, a segment, rather
   than an individual point, is taken as the basic processing unit to
   extract features. Third, RF is employed to select features and classify
   these segments. Finally, semantic rules are employed to optimize the
   classification result. Three datasets provided by Open Topography are
   utilized to test the proposed method. Experiments show that our method
   achieves a superior classification result with an overall classification
   accuracy larger than 91.17\%, and kappa coefficient larger than 83.79\%.}},
DOI = {{10.3390/rs9030288}},
Article-Number = {{288}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000398720100102}},
}

@article{ ISI:000397032500012,
Author = {Yao, Fei and Wang, Jian and Yao, Ju and Hang, Fangrong and Lei, Xu and
   Cao, Yongke},
Title = {{Three-dimensional image reconstruction with free open-source OsiriX
   software in video-assisted thoracoscopic lobectomy and segmentectomy}},
Journal = {{INTERNATIONAL JOURNAL OF SURGERY}},
Year = {{2017}},
Volume = {{39}},
Pages = {{16-22}},
Month = {{MAR}},
Abstract = {{Objective: The aim of this retrospective study was to evaluate the
   practice and the feasibility of Osirix, a free and open-source medical
   imaging software, in performing accurate video-assisted thoracoscopic
   lobectomy and segmentectomy.
   Methods: From July 2014 to April 2016, 63 patients received anatomical
   video-assisted thoracoscopic surgery (VATS), either lobectomy or
   segmentectomy, in our department. Three-dimensional (3D) reconstruction
   images of 61 (96.8\%) patients were preoperatively obtained with
   contrast-enhanced computed tomography (CT). Preoperative resection
   simulations were accomplished with patient-individual reconstructed 3D
   images. For lobectomy, pulmonary lobar veins, arteries and bronchi were
   identified meticulously by carefully reviewing the 3D images on the
   display. For segmentectomy, the intrasegmental veins in the affected
   segment for division and the intersegmental veins to be preserved were
   identified on the 3D images. Patient preoperative characteristics,
   surgical outcomes and postoperative data were reviewed from a
   prospective database.
   Results: The study cohort of 63 patients included 33 (52.4\%) men and 30
   (47.6\%) women, of whom 46 (73.0\%) underwent VATS lobectomy and 17
   (27.0\%) underwent VATS segmentectomy. There was 1 conversion from VATS
   lobectomy to open thoracotomy because of fibrocalcified lymph nodes. A
   VATS lobectomy was performed in 1 case after completing the
   segmentectomy because invasive adenocarcinoma was detected by
   intraoperative frozen-section analysis. There were no 30-day or 90-day
   operative mortalities
   Conclusions: The free, simple, and user-friendly software program Osirix
   can provide a 3D anatomic structure of pulmonary vessels and a clear
   vision into the space between the lesion and adjacent tissues, which
   allows surgeons to make preoperative simulations and improve the
   accuracy and safety of actual surgery. (C) 2017 IJS Publishing Group
   Ltd. Published by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.ijsu.2017.01.079}},
ISSN = {{1743-9191}},
EISSN = {{1743-9159}},
Unique-ID = {{ISI:000397032500012}},
}

@article{ ISI:000395034500015,
Author = {Gibelli, Daniele and De Angelis, Danilo and Poppa, Pasquale and Sforza,
   Chiarella and Cattaneo, Cristina},
Title = {{An Assessment of How Facial Mimicry Can Change Facial Morphology:
   Implications for Identification}},
Journal = {{JOURNAL OF FORENSIC SCIENCES}},
Year = {{2017}},
Volume = {{62}},
Number = {{2}},
Pages = {{405-410}},
Month = {{MAR}},
Abstract = {{The assessment of facial mimicry is important in forensic anthropology;
   in addition, the application of modern 3D image acquisition systems may
   help for the analysis of facial surfaces. This study aimed at exposing a
   novel method for comparing 3D profiles in different facial expressions.
   Ten male adults, aged between 30 and 40 years, underwent acquisitions by
   stereophotogrammetry (VECTRA-3D (R)) with different expressions
   (neutral, happy, sad, angry, surprised). The acquisition of each
   individual was then superimposed on the neutral one according to nine
   landmarks, and the root mean square (RMS) value between the two
   expressions was calculated. The highest difference in comparison with
   the neutral standard was shown by the happy expression (RMS 4.11 mm),
   followed by the surprised (RMS 2.74 mm), sad (RMS 1.3 mm), and angry
   ones (RMS 1.21 mm). This pilot study shows that the 3D-3D
   superimposition may provide reliable results concerning facial
   alteration due to mimicry.}},
DOI = {{10.1111/1556-4029.13295}},
ISSN = {{0022-1198}},
EISSN = {{1556-4029}},
ORCID-Numbers = {{De Angelis, Danilo/0000-0001-6388-9415}},
Unique-ID = {{ISI:000395034500015}},
}

@article{ ISI:000395034500021,
Author = {Gibelli, Daniele and De Angelis, Danilo and Poppa, Pasquale and Sforza,
   Chiarella and Cattaneo, Cristina},
Title = {{A View to the Future: A Novel Approach for 3D-3D Superimposition and
   Quantification of Differences for Identification from Next-Generation
   Video Surveillance Systems}},
Journal = {{JOURNAL OF FORENSIC SCIENCES}},
Year = {{2017}},
Volume = {{62}},
Number = {{2}},
Pages = {{457-461}},
Month = {{MAR}},
Abstract = {{Techniques of 2D-3D superimposition are widely used in cases of personal
   identification from video surveillance systems. However, the progressive
   improvement of 3D image acquisition technology will enable operators to
   perform also 3D-3D facial superimposition. This study aims at analyzing
   the possible applications of 3D-3D superimposition to personal
   identification, although from a theoretical point of view. Twenty
   subjects underwent a facial 3D scan by stereophotogrammetry twice at
   different time periods. Scans were superimposed two by two according to
   nine landmarks, and root-mean-square (RMS) value of point-to-point
   distances was calculated. When the two superimposed models belonged to
   the same individual, RMS value was 2.10 mm, while it was 4.47 mm in
   mismatches with a statistically significant difference (p < 0.0001).
   This experiment shows the potential of 3D-3D superimposition: Further
   studies are needed to ascertain technical limits which may occur in
   practice and to improve methods useful in the forensic practice.}},
DOI = {{10.1111/1556-4029.13290}},
ISSN = {{0022-1198}},
EISSN = {{1556-4029}},
ResearcherID-Numbers = {{Sforza, Chiarella/C-3008-2015
   }},
ORCID-Numbers = {{Sforza, Chiarella/0000-0001-6532-6464
   De Angelis, Danilo/0000-0001-6388-9415}},
Unique-ID = {{ISI:000395034500021}},
}

@article{ ISI:000395521200012,
Author = {Chen, Jingdao and Fang, Yihai and Cho, Yong K. and Kim, Changwan},
Title = {{Principal Axes Descriptor for Automated Construction-Equipment
   Classification from Point Clouds}},
Journal = {{JOURNAL OF COMPUTING IN CIVIL ENGINEERING}},
Year = {{2017}},
Volume = {{31}},
Number = {{2}},
Month = {{MAR}},
Abstract = {{Recognizing construction assets (e.g.,materials, equipment, labor) from
   point cloud data of construction environments provides essential
   information for engineering and management applications including
   progress monitoring, safety management, supply-chain management, and
   quality control. This study introduces a novel principal axes descriptor
   (PAD) for construction-equipment classification from point cloud data.
   Scattered as-is point clouds are first processed with downsampling,
   segmentation, and clustering steps to obtain individual instances of
   construction equipment. A geometric descriptor consisting of dimensional
   variation, occupancy distribution, shape profile, and plane counting
   features is then calculated to encode three-dimensional (3D)
   characteristics of each equipment category. Using the derived features,
   machine learning methods such as k-nearest neighbors and support vector
   machine are employed to determine class membership among major
   construction-equipment categories such as backhoe loader, bulldozer,
   dump truck, excavator, and front loader. Construction-equipment
   classification with the proposed PAD was validated using computer-aided
   design (CAD)-generated point clouds as training data and laser-scanned
   point clouds from an equipment yard as testing data. The recognition
   performance was further evaluated using point clouds from a construction
   site as well as a pose variation data set. PAD was shown to achieve a
   higher recall rate and lower computation time compared to competing 3D
   descriptors. The results indicate that the proposed descriptor is a
   viable solution for construction-equipment classification from point
   cloud data. (C) 2016 American Society of Civil Engineers.}},
DOI = {{10.1061/(ASCE)CP.1943-5487.0000628}},
Article-Number = {{04016058}},
ISSN = {{0887-3801}},
EISSN = {{1943-5487}},
ORCID-Numbers = {{Fang, Yihai/0000-0002-9451-4947}},
Unique-ID = {{ISI:000395521200012}},
}

@article{ ISI:000397013700050,
Author = {Hu, Xingbo and Chen, Wei and Xu, Weiyang},
Title = {{Adaptive Mean Shift-Based Identification of Individual Trees Using
   Airborne LiDAR Data}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Identifying individual trees and delineating their canopy structures
   from the forest point cloud data acquired by an airborne LiDAR (Light
   Detection And Ranging) has significant implications in forestry
   inventory. Once accurately identified, tree structural attributes such
   as tree height, crown diameter, canopy based height and diameter at
   breast height can be derived. This paper focuses on a novel
   computationally efficient method to adaptively calibrate the kernel
   bandwidth of a computational scheme based on mean shift-a non-parametric
   probability density-based clustering technique-to segment the 3D
   (three-dimensional) forest point clouds and identify individual tree
   crowns. The basic concept of this method is to partition the 3D space
   over each test plot into small vertical units (irregular columns
   containing 3D spatial features from one or more trees) first, by using a
   fixed bandwidth mean shift procedure and a small square grouping
   technique, and then rough estimation of crown sizes for distinct trees
   within a unit, based on an original 2D (two-dimensional) incremental
   grid projection technique, is applied to provide a basis for dynamical
   calibration of the kernel bandwidth for an adaptive mean shift procedure
   performed in each partition. The adaptive mean shift-based scheme, which
   incorporates our proposed bandwidth calibration method, is validated on
   10 test plots of a dense, multi-layered evergreen broad-leaved forest
   located in South China. Experimental results reveal that this approach
   can work effectively and when compared to the conventional point-based
   approaches (e.g., region growing, k-means clustering, fixed bandwidth or
   multi-scale mean shift), its accuracies are relatively high: it detects
   86 percent of the trees ({''}recall{''}) and 92 percent of the
   identified trees are correct ({''}precision{''}), showing good potential
   for use in the area of forest inventory.}},
DOI = {{10.3390/rs9020148}},
Article-Number = {{148}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{xu, weiyang/0000-0002-8980-6005}},
Unique-ID = {{ISI:000397013700050}},
}

@article{ ISI:000397013700010,
Author = {Yu, Xiaowei and Hyyppa, Juha and Litkey, Paula and Kaartinen, Harri and
   Vastaranta, Mikko and Holopainen, Markus},
Title = {{Single-Sensor Solution to Tree Species Classification Using
   Multispectral Airborne Laser Scanning}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{This paper investigated the potential of multispectral airborne laser
   scanning (ALS) data for individual tree detection and tree species
   classification. The aim was to develop a single-sensor solution for
   forest mapping that is capable of providing species-specific
   information, required for forest management and planning purposes.
   Experiments were conducted using 1903 ground measured trees from 22
   sample plots and multispectral ALS data, acquired with an Optech Titan
   scanner over a boreal forest, mainly consisting of Scots pine (Pinus
   Sylvestris), Norway spruce (Picea Abies), and birch (Betula sp.), in
   southern Finland. ALS-features used as predictors for tree species were
   extracted from segmented tree objects and used in random forest
   classification. Different combinations of features, including point
   cloud features, and intensity features of single and multiple channels,
   were tested. Among the field-measured trees, 61.3\% were correctly
   detected. The best overall accuracy (OA) of tree species classification
   achieved for correctly-detected trees was 85.9\% (Kappa = 0.75), using a
   point cloud and single-channel intensity features combination, which was
   not significantly different from the ones that were obtained either
   using all features (OA = 85.6\%, Kappa = 0.75), or single-channel
   intensity features alone (OA = 85.4\%, Kappa = 0.75). Point cloud
   features alone achieved the lowest accuracy, with an OA of 76.0\%.
   Field-measured trees were also divided into four categories. An
   examination of the classification accuracy for four categories of trees
   showed that isolated and dominant trees can be detected with a detection
   rate of 91.9\%, and classified with a high overall accuracy of 90.5\%.
   The corresponding detection rate and accuracy were 81.5\% and 89.8\% for
   a group of trees, 26.4\% and 79.1\% for trees next to a larger tree, and
   7.2\% and 53.9\% for trees situated under a larger tree, respectively.
   The results suggest that Channel 2 (1064 nm) contains more information
   for separating pine, spruce, and birch, followed by channel 1 (1550 nm)
   and channel 3 (532 nm) with an overall accuracy of 81.9\%, 78.3\%, and
   69.1\%, respectively. Our results indicate that the use of multispectral
   ALS data has great potential to lead to a single-sensor solution for
   forest mapping.}},
DOI = {{10.3390/rs9020108}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Kaartinen, Harri/B-1474-2015
   Vastaranta, Mikko/K-9656-2018}},
ORCID-Numbers = {{Vastaranta, Mikko/0000-0001-6552-9122}},
Unique-ID = {{ISI:000397013700010}},
}

@article{ ISI:000394522400015,
Author = {Bobulski, J.},
Title = {{Multimodal face recognition method with two-dimensional hidden Markov
   model}},
Journal = {{BULLETIN OF THE POLISH ACADEMY OF SCIENCES-TECHNICAL SCIENCES}},
Year = {{2017}},
Volume = {{65}},
Number = {{1}},
Pages = {{121-128}},
Month = {{FEB}},
Abstract = {{The paper presents a new solution for the face recognition based on
   two-dimensional hidden Markov models. The traditional HMM uses
   one-dimensional data vectors, which is a drawback in the case of 2D and
   3D image processing, because part of the information is lost during the
   conversion to one-dimensional features vector. The paper presents a
   concept of the full ergodic 2DHMM, which can be used in 2D and 3D face
   recognition. The experimental results demonstrate that the system based
   on two dimensional hidden Markov models is able to achieve a good
   recognition rate for 2D, 3D and multimodal (2D+3D) face images
   recognition, and is faster than ICP method.}},
DOI = {{10.1515/bpasts-2017-0015}},
ISSN = {{0239-7528}},
EISSN = {{2300-1917}},
ResearcherID-Numbers = {{Bobulski, Janusz/C-4998-2014}},
ORCID-Numbers = {{Bobulski, Janusz/0000-0003-3345-604X}},
Unique-ID = {{ISI:000394522400015}},
}

@article{ ISI:000395844700002,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Title = {{Statistical non-rigid ICP algorithm and its application to 3D face
   alignment}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{58}},
Pages = {{3-12}},
Month = {{FEB}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means Of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region. (C) 2016 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.imavis.2016.10.007}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000395844700002}},
}

@article{ ISI:000391965900001,
Author = {Dinh-Cuong Hoang and Liang-Chia Chen and Thanh-Hung Nguyen},
Title = {{Sub-OBB based object recognition and localization algorithm using range
   images}},
Journal = {{MEASUREMENT SCIENCE AND TECHNOLOGY}},
Year = {{2017}},
Volume = {{28}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{This paper presents a novel approach to recognize and estimate pose of
   the 3D objects in cluttered range images. The key technical breakthrough
   of the developed approach can enable robust object recognition and
   localization under undesirable condition such as environmental
   illumination variation as well as optical occlusion to viewing the
   object partially. First, the acquired point clouds are segmented into
   individual object point clouds based on the developed 3D object
   segmentation for randomly stacked objects. Second, an efficient
   shape-matching algorithm called Sub-OBB based object recognition by
   using the proposed oriented bounding box (OBB) regional area-based
   descriptor is performed to reliably recognize the object. Then, the 3D
   position and orientation of the object can be roughly estimated by
   aligning the OBB of segmented object point cloud with OBB of matched
   point cloud in a database generated from CAD model and 3D virtual
   camera. To detect accurate pose of the object, the iterative closest
   point (ICP) algorithm is used to match the object model with the
   segmented point clouds. From the feasibility test of several scenarios,
   the developed approach is verified to be feasible for object pose
   recognition and localization.}},
DOI = {{10.1088/1361-6501/aa513a}},
Article-Number = {{025401}},
ISSN = {{0957-0233}},
EISSN = {{1361-6501}},
Unique-ID = {{ISI:000391965900001}},
}

@inproceedings{ ISI:000453217100010,
Author = {Balding, Steven and Davis, Darryl N.},
Editor = {{Gao, Y and Fallah, S and Jin, Y and Lekakou, C}},
Title = {{Combining Depth and Intensity Images to Produce Enhanced Object
   Detection for Use in a Robotic Colony}},
Booktitle = {{TOWARDS AUTONOMOUS ROBOTIC SYSTEMS (TAROS 2017)}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10454}},
Pages = {{115-125}},
Note = {{18th Annual Conference on Towards Autonomous Robotics (TAROS), Univ
   Surrey, Guildford, ENGLAND, JUL 19-21, 2017}},
Organization = {{Airbus Defence \& Space; Chinese Acad Sci; IET; Springer; UK Robot
   Autonomous Syst Network}},
Abstract = {{Robotic colonies that can communicate with each other and interact with
   their ambient environments can be utilized for a wide range of research
   and industrial applications. However amongst the problems that these
   colonies face is that of the isolating objects within an environment.
   Robotic colonies that can isolate objects within the environment can not
   only map that environment in detail, but interact with that ambient
   space. Many object recognition techniques exist, however these are often
   complex and computationally expensive, leading to overly complex
   implementations. In this paper a simple model is proposed to isolate
   objects, these can then be recognize and tagged. The model will be using
   2D and 3D perspectives of the perceptual data to produce a probability
   map of the outline of an object, therefore addressing the defects that
   exist with 2D and 3D image techniques. Some of the defects that will be
   addressed are; low level illumination and objects at similar depths.
   These issues may not be completely solved, however, the model provided
   will provide results confident enough for use in a robotic colony.}},
DOI = {{10.1007/978-3-319-64107-2\_10}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-64107-2; 978-3-319-64106-5}},
ORCID-Numbers = {{Davis, Darryl/0000-0001-5236-6141}},
Unique-ID = {{ISI:000453217100010}},
}

@inproceedings{ ISI:000425498402048,
Author = {Liu, Liu and Li, Hongdong and Dai, Yuchao},
Book-Group-Author = {{IEEE}},
Title = {{Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale
   3D Map}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)}},
Series = {{IEEE International Conference on Computer Vision}},
Year = {{2017}},
Pages = {{2391-2400}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{Given an image of a street scene in a city, this paper develops a new
   method that can quickly and precisely pinpoint at which location (as
   well as viewing direction) the image was taken, against a pre-stored
   large-scale 3D point-cloud map of the city. We adopt the recently
   developed 2D-3D direct feature matching framework for this task
   {[}23,31,32,42-44]. This is a challenging task especially for
   large-scale problems. As the map size grows bigger, many 3D points in
   the wider geographical area can be visually very similar-or even
   identical-causing severe ambiguities in 2D-3D feature matching. The key
   is to quickly and unambiguously find the correct matches between a query
   image and the large 3D map. Existing methods solve this problem mainly
   via comparing individual features' visual similarities in a local and
   per feature manner, thus only local solutions can be found, inadequate
   for large-scale applications.
   In this paper, we introduce a global method which harnesses global
   contextual information exhibited both within the query image and among
   all the 3D points in the map. This is achieved by a novel global ranking
   algorithm, applied to a Markov network built upon the 3D map, which
   takes account of not only visual similarities between individual 2D-3D
   matches, but also their global compatibilities (as measured by
   co-visibility) among all matching pairs found in the scene. Tests on
   standard benchmark datasets show that our method achieved both higher
   precision and comparable recall, compared with the state-of-the-art.}},
DOI = {{10.1109/ICCV.2017.260}},
ISSN = {{1550-5499}},
ISBN = {{978-1-5386-1032-9}},
ORCID-Numbers = {{Liu, Liu/0000-0002-5880-5974}},
Unique-ID = {{ISI:000425498402048}},
}

@article{ ISI:000397995100002,
Author = {Ahmed, Oumer S. and Shemrock, Adam and Chabot, Dominique and Dillon,
   Chris and Williams, Griffin and Wasson, Rachel and Franklin, Steven E.},
Title = {{Hierarchical land cover and vegetation classification using
   multispectral data acquired from an unmanned aerial vehicle}},
Journal = {{INTERNATIONAL JOURNAL OF REMOTE SENSING}},
Year = {{2017}},
Volume = {{38}},
Number = {{8-10}},
Pages = {{2037-2052}},
Note = {{Conference on Small Unmanned Aerial Systems (sUAS) for Environmental
   Research, Univ Worcester, Worcester, ENGLAND, JUN, 2016}},
Abstract = {{The use of multispectral cameras deployed on unmanned aerial vehicles
   (UAVs) in land cover and vegetation mapping applications continues to
   improve and receive increasing recognition and adoption by resource
   management and forest survey practitioners. Comparisons of different
   camera data and platform performance characteristics are an important
   contribution in understanding the role and operational capability of
   this technology. In this article, object-based classification accuracies
   for different cover types and vegetation species of interest in central
   Ontario were examined using data from three UAV-based multispectral
   cameras. Five land-cover classes (forest, shrub, herbaceous, bare soil,
   and built-up) were determined to be up to 95\% correct overall with
   calibrated multispectral Parrot Sequoia digital camera data compared to
   independent field observations. The levels of classification accuracy
   decreased approximately 10-15\% when spectrally less capable
   consumer-grade RGB sensors were used. Multispectral Parrot Sequoia
   classification accuracy was approximately 89\% when more detailed
   vegetation classes, including individual deciduous tree species, shrub
   communities and agricultural crops, were analysed. Additional work is
   suggested in the use of such UAV multispectral and point cloud data in
   ash tree discrimination to support emerald ash borer infestation
   detection and management, and in analysis of functional and structural
   vegetation characteristics (e.g. leaf area index).}},
DOI = {{10.1080/01431161.2017.1294781}},
ISSN = {{0143-1161}},
EISSN = {{1366-5901}},
Unique-ID = {{ISI:000397995100002}},
}

@inproceedings{ ISI:000446968900006,
Author = {Rihani, Amal and Jribi, Majdi and Ghorbel, Faouzi},
Editor = {{BenAmor, B and Chaieb, F and Ghorbel, F}},
Title = {{Enhancing 3D Face Recognition by a Robust Version of ICP Based on the
   Three Polar Representation}},
Booktitle = {{REPRESENTATIONS, ANALYSIS AND RECOGNITION OF SHAPE AND MOTION FROM
   IMAGING DATA}},
Series = {{Communications in Computer and Information Science}},
Year = {{2017}},
Volume = {{684}},
Pages = {{65-74}},
Note = {{6th International Workshop on Representations, Analysis and Recognition
   of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
   OCT 27-29, 2016}},
Abstract = {{In this paper, we intend to propose a framework for the description and
   the matching of three dimensional faces. Our starting point is the
   representation of the 3D face by an invariant description under the M(3)
   group of translations and rotations. This representation is materialized
   by the points of the arc-length reparametrization of all the level
   curves of the three polar representation. These points are indexed by
   their level curve number and their position in each level. With this
   type of description we need a step of registration to align 3D faces
   with different expressions. Therefore, we propose to use a robust
   version of the iterative closest point algorithm (ICP) adopted to 3D
   face recognition context. We test the accuracy of our approach on a part
   of the BU-3DFE database of 3D faces. The obtained results for many
   protocols of the identification scenario show the performance of such
   framework.}},
DOI = {{10.1007/978-3-319-60654-5\_6}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-319-60654-5; 978-3-319-60653-8}},
Unique-ID = {{ISI:000446968900006}},
}

@inproceedings{ ISI:000438669700055,
Author = {Secanj, Marin and Arbanas, Snjezana Mihalic and Kordic, Branko and
   Krkac, Martin and Gazibara, Sanja Bernat},
Editor = {{Mikos, M and Vilimek, V and Yin, Y and Sassa, K}},
Title = {{Identification of Rock Fall Prone Areas on the Steep Slopes Above the
   Town of Omis, Croatia}},
Booktitle = {{ADVANCING CULTURE OF LIVING WITH LANDSLIDES, VOL 5: LANDSLIDES IN
   DIFFERENT ENVIRONMENTS}},
Year = {{2017}},
Pages = {{481-487}},
Note = {{4th World Landslide Forum, Ljubljana, SLOVENIA, MAY 29-JUN 02, 2017}},
Organization = {{Int Consortium Landslides; Int Programme Landslides, Global Promot Comm;
   Geol Survey Slovenia Ljubljana; Univ Ljubljana; Republ Slovenia, Minist
   Environm \& Spatial Planning; Republic Slovenia, Minist Infrastructure;
   Slovenian Natl Platform Disaster Risk Reduct; Int Programme Landslides;
   Slovenian Chamber Engineers; Int Assoc Hydrogeologists Slovene Comm;
   Water Management Soc Slovenia; Geomorphol Assoc Slovenia; Inst Water
   Republ Slovenia; Slovenian Geol Soc; Slovenian Geotechn Soc; IHP UNESCO,
   Slovenian Natl Comm; Slovenian Assoc Geodesy \& Geophys}},
Abstract = {{The aim of this paper was identification of rock fall prone areas above
   the historical town of Omis, located at the Adriatic coast in Croatia.
   Unstable areas were identified by kinematic analysis performed based on
   relative orientations of discontinuities and slope face. Input data was
   extracted from the surface model created from the high-resolution point
   cloud. The town of Omis is threatened by rock falls, because of its
   specific location just at the toe of Mt. Omiska Dinara. Rock fall risk
   is even higher due to rich cultural and historical heritage of the town.
   Collection of spatial data was performed by Time of Flight and
   phase-shift terrestrial laser scanners in order to derivate high
   resolution point cloud necessary for derivation of surface model.
   Split-FX software was used to extract discontinuity surfaces were
   semi-automatically from the point cloud data. Spatial kinematic analysis
   was performed for each triangle of TIN surface model of the investigated
   slopes to identify locations of possible instability mechanism. From the
   results of the spatial kinematic analysis, the most critical parts of
   the slope have identified for planar and wedge failure and flexural and
   block toppling. Verification of identified rock fall areas was performed
   by visual inspection of hazardous blocks at the surface model.
   Identified rock fall prone areas, unstable blocks and probable
   instability mechanisms on the steep slopes above the town Omis, present
   the input data for risk reduction by efficient design of
   countermeasures.}},
DOI = {{10.1007/978-3-319-53483-1\_57}},
ISBN = {{978-3-319-53483-1; 978-3-319-53482-4}},
ORCID-Numbers = {{Secanj, Marin/0000-0002-9818-9731}},
Unique-ID = {{ISI:000438669700055}},
}

@inproceedings{ ISI:000434278900149,
Author = {Swetha, K. M. and Suja, P.},
Editor = {{Niranjan, SK and Manvi, SS and Kodabagi, MM and Hulipalled, VR}},
Title = {{A Geometric Approach for Recognizing Emotions From 3D Images with Pose
   Variations}},
Booktitle = {{PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES
   FOR SMART NATION (SMARTTECHCON)}},
Year = {{2017}},
Pages = {{805-809}},
Note = {{International Conference On Smart Technologies For Smart Nation
   (SmartTechCon), REVA Univ, Bengaluru, INDIA, AUG 17-19, 2017}},
Organization = {{IEEE; IEEE Bangalore Sect; IEEE Computat Intelligence Soc, Bangalore
   Chapter; CSIR}},
Abstract = {{Emotions are an incredibly important aspect of human life. Research on
   emotion recognition for the past few decades have resulted in
   development of several fields. In the current scenario, it is necessary
   that machines/robots need to identify human emotions and respond
   accordingly. Applications in this field can be seen in security,
   entertainment and Human Machine Interface/Human Robot Interface. Recent
   works on 3D images have gained importance due to its accuracy in real
   life applications as emotions can be recognised at different head poses.
   The intention of this work has been to develop an algorithm for
   recognition of emotion from facial expressions, which recognizes 6 basic
   emotions, which are anger, fear, happy, disgust, sad and surprise from
   3D images in 7 yaw angles (+45 degrees to -45 degrees) and 3 pitch
   angles (+15 degrees, 0 degrees, -15 degrees). Most of the reported work
   considers + yaw angles. While in the current work, both positive as well
   as negative pitch and yaw angles are considered. BU3DFE database is used
   for the implementation. The proposed method resulted in improved
   accuracy and is comparable with the literature.}},
ISBN = {{978-1-5386-0569-1}},
Unique-ID = {{ISI:000434278900149}},
}

@inproceedings{ ISI:000432373000248,
Author = {Varga, Robert and Costea, Arthur and Florea, Horatiu and Giosan, Ion and
   Nedevschi, Sergiu},
Book-Group-Author = {{IEEE}},
Title = {{Super-sensor for 360-degree Environment Perception: Point Cloud
   Segmentation Using Image Features}},
Booktitle = {{2017 IEEE 20TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION
   SYSTEMS (ITSC)}},
Series = {{IEEE International Conference on Intelligent Transportation Systems-ITSC}},
Year = {{2017}},
Note = {{20th IEEE International Conference on Intelligent Transportation Systems
   (ITSC), Yokohama, JAPAN, OCT 16-19, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper describes a super-sensor that enables 360-degree environment
   perception for automated vehicles in urban traffic scenarios. We use
   four fisheye cameras, four 360 degree LIDARs and a GPS/IMU sensor
   mounted on an automated vehicle to build a super-sensor that offers an
   enhanced low-level representation of the environment by harmonizing all
   the available sensor measurements. Individual sensors cannot provide a
   robust 360-degree perception due to their limitations: field of view,
   range, orientation, number of scanning rays, etc. The novelty of this
   work consists of segmenting the 3D LIDAR point cloud by associating it
   with the 2D image semantic segmentation. Another contribution is the
   sensor configuration that enables 360-degree environment perception. The
   following steps are involved in the process: calibration, timestamp
   synchronization, fisheye image unwarping, motion correction of LIDAR
   points, point cloud projection onto the images and semantic segmentation
   of images. The enhanced low-level representation will improve the
   high-level perception environment tasks such as object detection,
   classification and tracking.}},
ISSN = {{2153-0009}},
ISBN = {{978-1-5386-1526-3}},
Unique-ID = {{ISI:000432373000248}},
}

@inproceedings{ ISI:000432372100099,
Author = {Torkhani, Ghada and Ladgham, Anis and Sakly, Anis},
Book-Group-Author = {{IEEE}},
Title = {{3D Gabor-Edge Filters Applied to Face Depth Images}},
Booktitle = {{2017 18TH INTERNATIONAL CONFERENCE ON SCIENCES AND TECHNIQUES OF
   AUTOMATIC CONTROL AND COMPUTER ENGINEERING (STA)}},
Series = {{International Conference on Sciences and Techniques of Automatic Control
   and Computer Engineering}},
Year = {{2017}},
Pages = {{578-582}},
Note = {{18th International Conference on Sciences and Techniques of Automatic
   Control and Computer Engineering (STA), Monastir, TUNISIA, DEC 21-23,
   2017}},
Organization = {{IEEE Tunisia Sect; Tunisian Assoc Numer Tech \& Automat; Univ Sfax, Natl
   Engn Sch Sfax, Lab Sci \& Tech Automat Control \& Comp Engn; Tunisia
   Sect Control Syst Soc Chapter; Tunisia Sect Robot \& Automat Soc
   Chapter; Tunisia Sect Signal Proc Soc Chapter; Tunisia Sect Circuits \&
   Syst Soc Chapter; Tunisia Sect Solid State Circuits Soc Chapter}},
Abstract = {{This manuscript introduces a novel 3D face authentication system
   inspired from the advantageous capacities of Gabor-Edge filters. The
   approach studies 3D face difficulties such as expression variety,
   different rotations and exposure to illuminations. The proposed systems
   starts by preprocessing the 3D face images to resolve acquisition
   problems. Then, a filtering process is performed by implanting our 3D
   Gabor-Edge technique extended based on the classic 3D Gabor masks. The
   next step is to achieve the classification of facial features from the
   edge saliency by the artificial Neural Network Classifier (NNC). The
   evaluation of the adopted system is achieved by exporting common
   datasets from GavabDB database. Experimental results are reported to
   prove the high accuracy rates of our method compared to the recent
   researches in the same biometric field.}},
ISSN = {{2378-7163}},
ISBN = {{978-1-5386-1084-8}},
Unique-ID = {{ISI:000432372100099}},
}

@inproceedings{ ISI:000428907900036,
Author = {Zhao, Minghua and Mo, Ruiyang and Zhao, Yonggang and Shi, Zhenghao and
   Zhang, Feifei},
Editor = {{Li, G and Ge, Y and Zhang, Z and Jin, Z and Blumenstein, M}},
Title = {{An Efficient Three-Dimensional Reconstruction Approach for
   Pose-Invariant Face Recognition Based on a Single View}},
Booktitle = {{KNOWLEDGE SCIENCE, ENGINEERING AND MANAGEMENT (KSEM 2017): 10TH
   INTERNATIONAL CONFERENCE, KSEM 2017, MELBOURNE, VIC, AUSTRALIA, AUGUST
   19-20, 2017, PROCEEDINGS}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2017}},
Volume = {{10412}},
Pages = {{422-431}},
Note = {{10th International Conference on Knowledge Science, Engineering and
   Management (KSEM), Melbourne, AUSTRALIA, AUG 19-20, 2017}},
Abstract = {{A three-dimensional (3D) reconstruction approach based on a single view
   is proposed to solve the problem of lack of training samples while
   addressing multi-pose face recognition. First, a planar template is
   defined based on the geometric information of the segmented faces.
   Second, 3D faces are resampled according to the geometric relationship
   between the planar template and original 3D faces, and a normalized 3D
   face database is obtained. Third, a 3D sparse morphable model is
   established based on the normalized 3D face database, and a new 3D face
   can be reconstructed from a single face image. Lastly, virtual
   multi-pose face images can be obtained by texture mapping, rotation, and
   projection of the established 3D face, and training samples are
   enriched. Experimental results obtained using BJUT-3D and CAS-PEAL-R1
   face databases show that recognition rate of the proposed method is
   91\%, which is better than other methods for pose-invariant face
   recognition based on a single view. This is primarily because the
   training samples are enriched using the proposed 3D sparse morphable
   model based on a new dense correspondence method.}},
DOI = {{10.1007/978-3-319-63558-3\_36}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-63558-3; 978-3-319-63557-6}},
Unique-ID = {{ISI:000428907900036}},
}

@inproceedings{ ISI:000427598702135,
Author = {Mohsin, Nasreen and Payandeh, Shahram},
Book-Group-Author = {{IEEE}},
Title = {{Localization and Identification of Body Extremities Based on Data from
   Multiple Depth Sensors}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)}},
Series = {{IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings}},
Year = {{2017}},
Pages = {{2736-2741}},
Note = {{IEEE International Conference on Systems, Man, and Cybernetics (SMC),
   Banff, CANADA, OCT 05-08, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper explores the novel use of multiple depth sensors to overcome
   occlusions and improve localization and tracking of body extremities.
   The usage of data from only depth sensors not only overcomes visual
   challenges associated with RGB sensors under low illumination, but also
   protects the identity of surveyed person with high confidentiality. For
   integrating depth information from multiple sources, the paper presents
   first an overview of a novel calibration method for multiple depth
   sensors. In case of occlusion of any fiducial point in the primary
   sensor's depth image, co-ordinates of the point can be obtained from the
   frame of other sensors using the calibration parameters. To localize
   salient body parts such as hands, head and feet, a surface triangular
   mesh is applied on generated 3D point cloud from the primary sensor. The
   geodesic extrema from the mesh coincide with body extremities. The body
   extremities can be identified based on those relative geodesic distances
   between the extremities. Once the body parts are labelled, a portion of
   body can be targeted and evaluated for specific gait analysis and
   visualization. For the performance evaluation, our calibration method
   has fared well in comparison to other available techniques. Also, our
   proposed localization of salient body parts is able to successfully tag
   the specific body part i.e. the head region.}},
ISSN = {{1062-922X}},
ISBN = {{978-1-5386-1645-1}},
Unique-ID = {{ISI:000427598702135}},
}

@inproceedings{ ISI:000427635800103,
Author = {Warke, K. S. and Suralkar, Rupali and Pawar, Snehal and Sonawane, Priya
   and Wani, Shraddha},
Book-Group-Author = {{IEEE}},
Title = {{A Real Sense based multilevel security in cloud framework using Face
   recognition and Image processing}},
Booktitle = {{2017 2ND INTERNATIONAL CONFERENCE FOR CONVERGENCE IN TECHNOLOGY (I2CT)}},
Year = {{2017}},
Pages = {{531-533}},
Note = {{2nd International Conference for Convergence in Technology (I2CT),
   Siddhant Coll Engn, Pune, INDIA, APR 07-09, 2017}},
Organization = {{IEEE; Sahyadri Valley Coll Engn \& Technol; Asian Soc Sci Res; SVCET,
   Jha Sci Res Pvt Ltd}},
Abstract = {{A 3D image can be used for the authentications as it gives more
   accuracy. We can also use the gestures for the authentication purpose.
   We can add multiple authentication levels together to make system more
   secure and login process more reliable. In our system we are going to
   provide three levels of authentication i.e. 1) Text Password: -This is
   first level in which user has to enter the text password which is OTP.
   OTP will be sent to the registered email-id or at the mobile number
   given. With
   The help of AES (advance encryption standard) algorithm the data will be
   encrypt and store at database server.2) Hand Gesture Recognition: -This
   is second level in which user has to place this hand in front of camera.
   So camera can detect it,select one point out of 22 points on the hand
   whatever pattern user make in front of camera is saved as password at
   user's database.
   3) Face Recognition: -This is last level in which user has to place face
   in front of camera. So camera can detect the face and select 78 landmark
   points. And those points are saved at the user's database. After passing
   all these stages user is authenticate and can upload or download the
   documents of his/her choice.}},
ISBN = {{978-1-5090-4307-1}},
Unique-ID = {{ISI:000427635800103}},
}

@inproceedings{ ISI:000427083300058,
Author = {Jazouli, Maha and Majda, Aicha and Zarghili, Arsalane},
Editor = {{Nfaoui, E and Boumhidi, J and Sabri, A and Yahyaouy, A and Bouchaffra, D}},
Title = {{A \$P Recognizer for Automatic Facial Emotion Recognition using Kinect
   Sensor}},
Booktitle = {{2017 INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV)}},
Year = {{2017}},
Note = {{Intelligent Systems and Computer Vision (ISCV), Fez, MOROCCO, APR 17-19,
   2017}},
Organization = {{IEEE; Univ Sidi Mohamed ben Abdellah; IEEE Comp Soc; IEEE Morocco Sect}},
Abstract = {{Autism is a developmental disorder involving qualitative impairments in
   social interaction. One source of those impairments are difficulties
   with facial expressions of emotion. Autistic people often have
   difficulty to recognize or to understand other people's emotions and
   feelings, or expressing their own. This work proposes a method to
   automatically recognize seven basic emotions among autistic children in
   real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and
   Neutral. The method uses the Microsoft Kinect sensor to track and
   identify points of interest from the 3D face model and it is based on
   the \$P point-cloud recognizer to identify multi-stroke emotions as
   point-clouds. The experimental results show that our system can achieve
   above 94.28\% recognition rate. Our study provides a novel clinical tool
   to help children with autism to assisting doctors in operating rooms.}},
ISBN = {{978-1-5090-4062-9}},
Unique-ID = {{ISI:000427083300058}},
}

@inproceedings{ ISI:000427293600008,
Author = {Frikha, Tarek and Chaabane, Faten and Said, Boukhchim and Drira, Hassen
   and Abid, Mohamed and Ben Amar, Chokri},
Editor = {{ElHassouni, M and Karim, M and BenHamida, A and BenSlima, A and Solaiman, B}},
Title = {{Embedded approach for a Riemannian-based framework of analyzing 3D faces}},
Booktitle = {{2017 3RD INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)}},
Year = {{2017}},
Pages = {{43-47}},
Note = {{3rd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP), Fez, MOROCCO, MAY 22-24, 2017}},
Organization = {{Univ Sidi Mohamed Ben Abdellah; Fac Sci; Fac Med \& Pharm; CNRST; TICSM;
   IEEE Morocco Sect; IEEE Signal Proc Soc Morocco Chapter}},
Abstract = {{Developing multimedia embedded applications continues to flourish. In
   fact, a biometric facial recognition system can be used not only on PCs
   abut also in embedded systems, it is a potential enhancer to meet
   security and surveillance needs. The analysis of facial recognition
   consists offoursteps: face analysis, face expressions' recognition,
   missing data completion and full face recognition.
   This paper proposes a hardware architecture based on an adaptation
   approach foran algorithm which has proven good face detection and
   recognition in 3D space. The proposed application was tested using a co
   design technique based on a mixed Hardware Software architecture: the
   FPGA platform.}},
ISBN = {{978-1-5386-0551-6}},
Unique-ID = {{ISI:000427293600008}},
}

@inproceedings{ ISI:000426973200029,
Author = {Li, Huibin and Sun, Jian and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Location-Sensitive Sparse Representation of Deep Normal Patterns for
   Expression-robust 3D Face Recognition}},
Booktitle = {{2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)}},
Year = {{2017}},
Pages = {{234-242}},
Note = {{IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
   OCT 01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper presents a straight-forward yet efficient, and
   expression-robust 3D face recognition approach by exploring location
   sensitive sparse representation of deep normal patterns (DNP). In
   particular given raw 3D facial surfaces, we first run 3D face
   pre-processing pipeline, including nose tip detection, face region
   cropping, and pose normalization. The 3D coordinates of each normalized
   3D facial surface are then projected into 2D plane to generate geometry
   images, from which three images of facial surface normal components are
   estimated. Each normal image is then fed into a pre-trained deep face
   net to generate deep representations of facial surface normals, i.e.,
   deep normal patterns. Considering the importance of different facial
   locations, we propose a location sensitive sparse representation
   classifier (LS-SRC) for similarity measure among deep normal patterns
   associated with different 3D faces. Finally, simple score-level fusion
   of different normal components are used for the final decision. The
   proposed approach achieves significantly high performance, and reporting
   rank-one scores of 98.01\%, 97.60\%, and 96.13\% on the FRGC v2.0,
   Bosphorus, and BU-3DFE databases when only one sample per subject is
   used in the gallery. These experimental results reveals that the
   performance of 3D face recognition would be constantly improved with the
   aid of training deep models from massive 2D face images, which opens the
   door for future directions of 3D face recognition.}},
ISBN = {{978-1-5386-1124-1}},
Unique-ID = {{ISI:000426973200029}},
}

@inproceedings{ ISI:000426973200042,
Author = {Liu, Feng and Hu, Jun and Sun, Jianwei and Wang, Yang and Zhao, Qijun},
Book-Group-Author = {{IEEE}},
Title = {{Multi-Dim: A Multi-Dimensional Face Database Towards the Application of
   3D Technology in Real-World Scenarios}},
Booktitle = {{2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)}},
Year = {{2017}},
Pages = {{342-351}},
Note = {{IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
   OCT 01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{Three-dimensional (3D) faces are increasingly utilized in many
   face-related tasks. Despite the promising improvement achieved by 3D
   face technology, it is still hard to thoroughly evaluate the performance
   and effect of 3D face technology in real-world applications where
   variations frequently occur in pose, illumination, expression and many
   other factors. This is due to the lack of benchmark databases that
   contain both high precision full-view 3D faces and their 2D face
   images/videos under different conditions. In this paper, we present such
   a multi-dimensional face database (namely Multi-Dim) of high precision
   3D face scans, high definition photos, 2D stillface images with varying
   pose and expression, low quality 2D surveillance video clips, along with
   ground truth annotations for them. Based on this Multi-Dim face
   database, extensive evaluation experiments have been done with
   state-of-the-art baseline methods for constructing 3D morphable model,
   reconstructing 3D faces from single images, 3D-assisted pose
   normalization for face verification, and 3D-rendered multiview gallery
   for face identification. Our results show that 3D face technology does
   help in improving unconstrained 2D face recognition when the probe 2D
   face images are of reasonable quality, whereas it deteriorates rather
   than improves the face recognition accuracy when the probe 2D face
   images are of poor quality. We will make Multi-Dim freely available to
   the community for the purpose of advancing the 3D-based unconstrained 2D
   face recognition and related techniques towards real-world applications.}},
ISBN = {{978-1-5386-1124-1}},
Unique-ID = {{ISI:000426973200042}},
}

@inproceedings{ ISI:000426886000048,
Author = {Lopez, G. and Pallas, B. and Martinez, S. and Lauri, P. E. and Regnard,
   J. L. and Durel, C. E. and Costes, E.},
Editor = {{Marsal, J and Girona, J}},
Title = {{High-throughput phenotyping of an apple core collection: identification
   of genotypes with high water use efficiency}},
Booktitle = {{VIII INTERNATIONAL SYMPOSIUM ON IRRIGATION OF HORTICULTURAL CROPS}},
Series = {{Acta Horticulturae}},
Year = {{2017}},
Volume = {{1150}},
Pages = {{335-340}},
Note = {{8th International Symposium on Irrigation of Horticultural Crops,
   Lleida, SPAIN, JUN 08-11, 2015}},
Organization = {{Int Soc Horticultural Sci}},
Abstract = {{To detect genotypes with high water use efficiency (WUE) in apple (Malus
   x domestica), 193 genotypes from an INRA core collection were evaluated
   in 2014. Eight grafted replicates per genotype grown as one-year-old
   scions were studied in a high-throughput phenotyping platform
   (PhenoArch). Individual pot weight was recorded twice a day and
   irrigation was scheduled for 46 days according to two irrigation
   treatments: well-watered (WW), maintaining soil water content (SWC) at
   1.4 g g(-1); and water stress (WS), reducing SWC until 0.7 g g(-1) and
   maintaining this value for ten days. For each genotype, half of the
   replicates were WW while the other half were grown under WS. Plant 3D
   images were automatically acquired every two days. Analysis of images
   and pot weight differences allowed the estimation of the accumulated
   whole-plant biomass (A\_Bio) and transpiration (Plant\_T) during the
   experiment. WUE was calculated as the ratio A\_Bio/Plant\_T. A\_Bio and
   WUE had a higher genetic variation than Plant\_T under WW and WS
   conditions. The genetic variation in WUE is a promising result,
   indicating that available genetic resources such as the INRA core
   collection could be useful to improve apple plant material for the use
   of water. WS reduced A\_Bio and Plant\_T but the reduction was less
   evident in WUE. Some genotypes had similar WUE values under WW and WS
   conditions. We identified of a group of 38 genotypes with high WUE under
   WW and WS. The existence of genotypes with high WUE whatever the water
   regime in apple may encourage apple breeders to consider the use of
   these genotypes as potential parents for improving apple plant material
   for the use of water.}},
DOI = {{10.17660/ActaHortic.2017.1150.48}},
ISSN = {{0567-7572}},
EISSN = {{2406-6168}},
ISBN = {{978-94-62611-45-0}},
Unique-ID = {{ISI:000426886000048}},
}

@inproceedings{ ISI:000425931000007,
Author = {Asl, Azin Shabani and Oskoei, Mohammadreza Asghari},
Book-Group-Author = {{IEEE}},
Title = {{Depth Dependent Invariant Features Applied to Person Detection Using 3D
   Camera}},
Booktitle = {{2017 5TH IRANIAN JOINT CONGRESS ON FUZZY AND INTELLIGENT SYSTEMS (CFIS)}},
Year = {{2017}},
Pages = {{29-34}},
Note = {{5th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS),
   Qazvin Islam Azad Univ, Tehran, IRAN, MAR 07-09, 2017}},
Abstract = {{This paper is about detection and tracking a person by mobile robots in
   in-door environments, such as shopping center and hospital. It uses
   vision based approaches to recognize texture of clothes. The paper
   proposes a method to use depth (distance) reference along with scale
   invariant features (SIFT) to recognize patterns in various orientation,
   distance and illumination. SIFT is an important feature detection
   algorithm that is robust against rotation, translation, and scaling in
   2D images and to some extent against variations in lighting conditions.
   But it suffers inadequate performance for visual patterns rotated in 3D
   space. To overcome this issue, reference inputs given to the algorithm
   was extended to include images taken from different angles. The proposed
   algorithm showed considerably improved performance in detection for
   real-time applications.}},
ISBN = {{978-1-5090-4008-7}},
Unique-ID = {{ISI:000425931000007}},
}

@inproceedings{ ISI:000426676200113,
Author = {Si, Boyu and Huang, Zhaoming and Bai, Baodan},
Editor = {{ElFergany, A and Rojas, AL and Szeto, WY}},
Title = {{A Training System for Speech Disordered Children Based on the Intel
   RealSense Technology}},
Booktitle = {{PROCEEDINGS OF THE 2017 2ND INTERNATIONAL CONFERENCE ON CONTROL,
   AUTOMATION AND ARTIFICIAL INTELLIGENCE (CAAI 2017)}},
Series = {{Advances in Intelligent Systems Research}},
Year = {{2017}},
Volume = {{134}},
Pages = {{504-507}},
Note = {{2nd International Conference on Control, Automation and Artificial
   Intelligence (CAAI), Sanya, PEOPLES R CHINA, JUN 25-26, 2017}},
Organization = {{Sci \& Engn Res Ctr}},
Abstract = {{A training system for speech disordered children is presented in this
   research. The core technology includes face tracking and speech
   recognition, which are supplied by Intel RealSense SDK and its relative
   hardware, such as 3D camera F200. The system consists of the pronouncing
   learning module and the speech disorder training module. The former can
   help children patient with almost no speech ability learn the
   pronouncing motion and method in Mandarin initials and finals with
   real-time 3D image playback. The training module which is based on the
   cartoon games can intervene patients' abnormal voice onset, speech
   volume, speech tone and vowel confusion. With the help of the speech
   recognition function from Intel RealSense, it can make an immersed
   environment which is benefit for training.}},
ISSN = {{1951-6851}},
ISBN = {{978-94-6252-360-9}},
Unique-ID = {{ISI:000426676200113}},
}

@inproceedings{ ISI:000426279000124,
Author = {Wang, Haoyu and Yang, Fumeng and Zhang, Yuming and Wu, Congzhong},
Editor = {{Lv, D and Lv, Y and Bao, W}},
Title = {{3D face analysis by using Mesh-LBP feature}},
Booktitle = {{LIDAR IMAGING DETECTION AND TARGET RECOGNITION 2017}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10605}},
Number = {{1}},
Note = {{Conference on LIDAR Imaging Detection and Target Recognition, Changchun,
   PEOPLES R CHINA, JUL 23-25, 2017}},
Organization = {{Chinese Soc Opt Engn; Chinese Soc Astronaut, Photoelectron Technol Comm;
   Chinese Acad Engn; Natl Nat Sci Fdn China; Chinese Acad Sci,
   Photoelectron Technol Comm}},
Abstract = {{Face Recognition is one of the widely application of image processing.
   Corresponding two-dimensional limitations, such as the pose and
   illumination changes, to a certain extent restricted its accurate rate
   and further development. How to overcome the pose and illumination
   changes and the effects of self-occlusion is the research hotspot and
   difficulty, also attracting more and more domestic and foreign experts
   and scholars to study it. 3D face recognition fusing shape and texture
   descriptors has become a very promising research direction. Method: Our
   paper presents a 3D point cloud based on mesh local binary pattern grid
   (Mesh-LBP), then feature extraction for 3D face recognition by fusing
   shape and texture descriptors. 3D Mesh-LBP not only retains the
   integrity of the 3D geometry, is also reduces the need for recognition
   process of normalization steps, because the triangle Mesh-LBP descriptor
   is calculated on 3D grid. On the other hand, in view of multi-modal
   consistency in face recognition advantage, construction of LBP can
   fusing shape and texture information on Triangular Mesh. In this paper,
   some of the operators used to extract Mesh-LBP, Such as the normal
   vectors of the triangle each face and vertex, the gaussian curvature,
   the mean curvature, laplace operator and so on. Conclusion: First,
   Kinect devices obtain 3D point cloud face, after the pretreatment and
   normalization, then transform it into triangular grid, grid local binary
   pattern feature extraction from face key significant parts of face. For
   each local face, calculate its Mesh-LBP feature with Gaussian curvature,
   mean curvature laplace operator and so on. Experiments on the our
   research database, change the method is robust and high recognition
   accuracy.}},
DOI = {{10.1117/12.2295797}},
Article-Number = {{UNSP 106053O}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1707-0; 978-1-5106-1706-3}},
Unique-ID = {{ISI:000426279000124}},
}

@inproceedings{ ISI:000425238900023,
Author = {Kaur, Rajwant and Sharma, Dolly and Verma, Amit},
Editor = {{Sood, M and Jain, S}},
Title = {{An Advance 2D Face Recognition by Feature Extraction (ICA) and Optimize
   Multilayer Architecture}},
Booktitle = {{PROCEEDINGS OF 4TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING,
   COMPUTING AND CONTROL (ISPCC 2K17)}},
Series = {{IEEE International Conference on Signal Processing Computing and Control}},
Year = {{2017}},
Pages = {{122-129}},
Note = {{4th IEEE International Conference on Signal Processing, Computing and
   Control (ISPCC), Jaypee Univ Informat Technol, Dept Elect \& Commun
   Engn, Solan, INDIA, SEP 21-23, 2017}},
Organization = {{IEEE; IEEE Delhi Sect; IEEE Jaypee Univ Informat Technol Student Branch;
   GENTECH; AD Instruments; ARK}},
Abstract = {{Facial recognition has most significant real-life requests like
   investigation and access control. It is associated through the issue of
   appropriately verifying face pictures and transmit them person in a
   database. In a past years face study has been emerging active topic.
   Most of the face detector techniques could be classified into feature
   based methods and image based also. Feature based techniques adds
   low-level analysis, feature analysis, etc. Facial recognition is a
   system capable of verifying / identifying a human after 3D images. By
   evaluating selected facial unique features from the image and face
   dataset. Design from transformation method given vector dimensional
   illustration of individual face in a prepared set of images, Principle
   component analysis inclines to search a dimensional sub-space whose
   normal vector features correspond to the maximum variance direction in
   the real image space. The PCA algorithm evaluates the feature
   extraction, data, i.e. Eigen Values and vectors of the scatter matrix.
   In literature survey, Face recognition is a design recognition mission
   performed exactly on faces. It can be described as categorizing a facial
   either ``known{''} or ``unknown{''}, after comparing it with deposits
   known individuals. It is also necessary to need a system that has the
   capability of knowledge to recognize indefinite faces. Computational
   representations of facial recognition must statement various difficult
   issues. After existing work, we study the SIFT structures for the
   gratitude method. The novel technique is compared with well settled
   facial recognition methods, name component analysis and eigenvalues and
   vector. This algorithm is called PCA and ICA (Independent Component
   Analysis). In research work, we implement the novel approach to detect
   the face in minimum time and evaluate the better accuracy based on Back
   Propagation Neural Networks. We design the framework in face recognition
   using MATLAB 2013a simulation tool. Evaluate the performance parameters,
   i.e. the FAR (false acceptance rate), FRR (False rejection Rate) and
   Accuracy and compare the existing performance parameters i.e. accuracy.}},
ISSN = {{2376-5461}},
ISBN = {{978-1-5090-5838-9}},
Unique-ID = {{ISI:000425238900023}},
}

@inproceedings{ ISI:000425925300009,
Author = {Indumathi, T. and Pushparani, M.},
Book-Group-Author = {{IEEE}},
Title = {{Multimodel Human Authentication By Matching 3D Skull And Gait}},
Booktitle = {{2017 2ND WORLD CONGRESS ON COMPUTING AND COMMUNICATION TECHNOLOGIES
   (WCCCT)}},
Year = {{2017}},
Pages = {{38-42}},
Note = {{2nd World Congress on Computing and Communication Technologies (WCCCT),
   St Josephs Coll, Tiruchirappalli, INDIA, FEB 02-04, 2017}},
Organization = {{St Josephs Coll, Dept Comp Sci}},
Abstract = {{This research paper focuses multimodal human identifications that are
   captured in a web cam as images of face and walking style. The image can
   then be considered for further analyses of gait and skull
   characteristics as per Human Identification Systems. We propose to
   identify a skull by using a correlation measure between the 3D skull and
   3D face in terms of morphology, and measure the correlation using
   Enhance Canonical Correlation Coefficient Analysis (ECCCA). We use the
   3D skull data as the probe and 3D face geometric data as the gallery and
   match the skull with enrolled 3D faces by the correlation measure
   between the Probe and the Gallery. This paper proposes Uncorrelated
   Multilinear Discriminant Analysis (UMLDA) algorithm for the challenging
   problem of Gait Recognition. Finally, Neural Network for mat-lab tool is
   used for training and testing purpose We have created different model of
   neural network based on hidden layer, selection of training algorithm
   and setting the different parameter for training. And then, we will test
   for the combination of NN+SVM, Knearest Neighbour Classification. Here
   all these experiments are done on CASIA gait database and input video.}},
DOI = {{10.1109/WCCCT.2016.19}},
ISBN = {{978-1-5090-5573-9}},
Unique-ID = {{ISI:000425925300009}},
}

@inproceedings{ ISI:000423869700004,
Author = {Haufel, Gisela and Bulatov, Dimitri and Solbrig, Peter},
Editor = {{Michel, U and Schulz, K and Nikolakopoulos, KG and Civco, D}},
Title = {{Sensor Data Fusion for Textured Reconstruction and Virtual
   Representation of Alpine Scenes}},
Booktitle = {{EARTH RESOURCES AND ENVIRONMENTAL REMOTE SENSING/GIS APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10428}},
Note = {{17th SPIE Conference on Earth Resources and Environmental Remote
   Sensing/GIS Applications, Warsaw, POLAND, SEP 12-14, 2017}},
Organization = {{SPIE}},
Abstract = {{The concept of remote sensing is to provide information about a
   wide-range area without making physical contact with this area. If,
   additionally to satellite imagery, images and videos taken by drones
   provide a more up-to-date data at a higher resolution, or accurate
   vector data is downloadable from the Internet, one speaks of sensor data
   fusion. The concept of sensor data fusion is relevant for many
   applications, such as virtual tourism, automatic navigation, hazard
   assessment, etc. In this work, we describe sensor data fusion aiming to
   create a semantic 3D model of an extremely interesting yet challenging
   dataset: An alpine region in Southern Germany. A particular challenge of
   this work is that rock faces including overhangs are present in the
   input airborne laser point cloud. The proposed procedure for
   identification and reconstruction of overhangs from point clouds
   comprises four steps: Point cloud preparation, filtering out vegetation,
   mesh generation and texturing. Further object types are extracted in
   several interesting subsections of the dataset: Building models with
   textures from UAV (Unmanned Aerial Vehicle) videos, hills reconstructed
   as generic surfaces and textured by the orthophoto, individual trees
   detected by the watershed algorithm, as well as the vector data for
   roads retrieved from openly available shape files and GPS-device tracks.
   We pursue geo-specific reconstruction by assigning texture and width to
   roads of several pre-determined types and modeling isolated trees and
   rocks using commercial software. For visualization and simulation of the
   area, we have chosen the simulation system Virtual Battlespace 3 (VBS3).
   It becomes clear that the proposed concept of sensor data fusion allows
   a coarse reconstruction of a large scene and, at the same time, an
   accurate and up-to-date representation of its relevant subsections, in
   which simulation can take place.}},
DOI = {{10.1117/12.2278237}},
Article-Number = {{UNSP 1042805}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1321-8; 978-1-5106-1320-1}},
ORCID-Numbers = {{Bulatov, Dimitri/0000-0002-0560-2591}},
Unique-ID = {{ISI:000423869700004}},
}

@inproceedings{ ISI:000418793200017,
Author = {Hammer, Marcus and Hebel, Marcus and Arens, Michael},
Editor = {{Kamerman, G and Steinvall, O}},
Title = {{Person detection and tracking with a 360 degrees LiDAR system}},
Booktitle = {{ELECTRO-OPTICAL REMOTE SENSING XI}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10434}},
Note = {{Conference on Electro-Optical Remote Sensing XI, Warsaw, POLAND, SEP
   11-12, 2017}},
Organization = {{SPIE}},
Abstract = {{Today it is easily possible to generate dense point clouds of the sensor
   environment using 360 degrees LiDAR (Light Detection and Ranging)
   sensors which are available since a number of years. The interpretation
   of these data is much more challenging. For the automated data
   evaluation the detection and classification of objects is a fundamental
   task. Especially in urban scenarios moving objects like persons or
   vehicles are of particular interest, for instance in automatic collision
   avoidance, for mobile sensor platforms or surveillance tasks.
   In literature there are several approaches for automated person
   detection in point clouds. While most techniques show acceptable results
   in object detection, the computation time is often crucial. The runtime
   can be problematic, especially due to the amount of data in the
   panoramic 360 degrees point clouds. On the other hand, for most
   applications an object detection and classification in real time is
   needed.
   The paper presents a proposal for a fast, real-time capable algorithm
   for person detection, classification and tracking in panoramic point
   clouds.}},
DOI = {{10.1117/12.2278215}},
Article-Number = {{UNSP 104340L}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1333-1; 978-1-5106-1332-4}},
Unique-ID = {{ISI:000418793200017}},
}

@article{ ISI:000418758100001,
Author = {Dai, Yucheng and Gong, Jianhua and Li, Yi and Feng, Quanlong},
Title = {{Building segmentation and outline extraction from UAV image-derived
   point clouds by a line growing algorithm}},
Journal = {{INTERNATIONAL JOURNAL OF DIGITAL EARTH}},
Year = {{2017}},
Volume = {{10}},
Number = {{11}},
Pages = {{1077-1097}},
Abstract = {{This paper presents an approach to process raw unmanned aircraft vehicle
   (UAV) image-derived point clouds for automatically detecting, segmenting
   and regularizing buildings of complex urban landscapes. For
   regularizing, we mean the extraction of the building footprints with
   precise position and details. In the first step, vegetation points were
   extracted using a support vector machine (SVM) classifier based on
   vegetation indexes calculated from color information, then the
   traditional hierarchical stripping classification method was applied to
   classify and segment individual buildings. In the second step, we first
   determined the building boundary points with a modified convex hull
   algorithm. Then, we further segmented these points such that each point
   was assigned to a fitting line using a line growing algorithm. Then, two
   mutually perpendicular directions of each individual building were
   determined through a W-k-means clustering algorithm which used the slop
   information and principal direction constraints. Eventually, the
   building edges were regularized to form the final building footprints.
   Qualitative and quantitative measures were used to evaluate the
   performance of the proposed approach by comparing the digitized results
   from ortho images.}},
DOI = {{10.1080/17538947.2016.1269841}},
ISSN = {{1753-8947}},
EISSN = {{1753-8955}},
Unique-ID = {{ISI:000418758100001}},
}

@inproceedings{ ISI:000418371405064,
Author = {Peng, Weilong and Feng, Zhiyong and Xu, Chao and Su, Yong},
Book-Group-Author = {{IEEE}},
Title = {{Parametric T-spline Face Morphable Model for Detailed Fitting in Shape
   Subspace}},
Booktitle = {{30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR
   2017)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{5515-5523}},
Note = {{30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Honolulu, HI, JUL 21-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration
   for the synthesis of 3D faces by assuming that faces are in a linear
   class. However, the human face is in a nonlinear manifold, and a new
   test are always not in the pre-learnt subspace accurately because of the
   disparity brought by ethnicity, age, gender, etc. In the paper, we
   propose a parametric T-spline morphable model (T-splineMM) for 3D face
   representation, which has great advantages of fitting data from an
   unknown source accurately. In the model, we describe a face by C-2
   T-spline surface, and divide the face surface into several shape units
   (SUs), according to facial action coding system (FACS), on T-mesh
   instead of on the surface directly. A fitting algorithm is proposed to
   optimize coefficients of T-spline control point components along
   pre-learnt identity and expression subspaces, as well as to optimize the
   details in refinement progress. As any pre-learnt subspace is not
   complete to handle the variety and details of faces and expressions, it
   covers a limited span of morphing. SUs division and detail refinement
   make the model fitting the facial muscle deformation in a larger span of
   morphing subspace. We conduct experiments on face scan data, kinect data
   as well as the space-time data to test the performance of detail
   fitting, robustness to missing data and noise, and to demonstrate the
   effectiveness of our model. Convincing results are illustrated to
   demonstrate the effectiveness of our model compared with the popular
   methods.}},
DOI = {{10.1109/CVPR.2017.585}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-0457-1}},
Unique-ID = {{ISI:000418371405064}},
}

@inproceedings{ ISI:000418397700008,
Author = {Richter, Julia and Wiede, Christian and Dayangac, Enes and Shahenshah,
   Ahsan and Hirtz, Gangolf},
Editor = {{Fred, A and DeMarsico, M and DiBaja, GS}},
Title = {{Activity Recognition for Elderly Care by Evaluating Proximity to Objects
   and Human Skeleton Data}},
Booktitle = {{PATTERN RECOGNITION APPLICATIONS AND METHODS, ICPRAM 2016}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10163}},
Pages = {{139-155}},
Note = {{5th International conference on Pattern Recognition Applications and
   Methods (ICPRAM), Rome, ITALY, FEB 24-26, 2016}},
Abstract = {{Recently, researchers have shown an increased interest in the detection
   of activities of daily living (ADLs) for ambient assisted living (AAL)
   applications. In this study, we present an algorithm that detects
   activities related to personal hygiene. The approach is based on the
   evaluation of pose information and a person's proximity to objects
   belonging to the typical equipment of bathrooms, such as sink, toilet
   and shower. In addition to this high-level reasoning, we developed a
   skeleton-based algorithm that recognises actions using a supervised
   learning model. Therefore, we analysed several feature vectors,
   especially with regard to the representation of joint trajectories in
   the frequency domain. The results gave evidence that this high-level
   reasoning algorithm can reliably recognise hygiene-related activities.
   An evaluation of the skeleton-based algorithm shows that the defined
   actions were successfully classified with a rate of 96.66\%.}},
DOI = {{10.1007/978-3-319-53375-9\_8}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-53375-9; 978-3-319-53374-2}},
Unique-ID = {{ISI:000418397700008}},
}

@inproceedings{ ISI:000417429000016,
Author = {Li Fangmin and Chen Ke and Liu Xinhua},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Reconstruction Based on Convolutional Neural Network}},
Booktitle = {{2017 10TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTATION TECHNOLOGY
   AND AUTOMATION (ICICTA 2017)}},
Series = {{International Conference on Intelligent Computation Technology and
   Automation}},
Year = {{2017}},
Pages = {{71-74}},
Note = {{10th International Conference on Intelligent Computation Technology and
   Automation (ICICTA), Changsha, PEOPLES R CHINA, OCT 09-10, 2017}},
Organization = {{Changsha Univ Sci \& Technol, Commun Res Inst; Cent S Univ, Shenzhen Res
   Inst; Hunan City Coll, Dept Urban Management}},
Abstract = {{Fast and robust 3D reconstruction of facial geometric structure from a
   single image is a challenging task with numerous applications, but there
   exist two problems when applied ``in the wild{''}: the 3D estimates are
   unstable for different photos of the same subject; the 3D estimates are
   over-regularized and generic. In response, a robust method for
   regressing discriminative 3D morphable face models(3DMM) is described to
   support face recognition and 3D mask printing. Combining the local data
   sets with the public data sets, improving the exiting 3DMM fitting
   method and then using a convolutional neural network(CNN) to improve
   reconstruction effect. The ground truth 3D faces of the CNN are the
   pooled 3DMM parameters extracted from the photos of the same subject.
   Using CNN to regress 3DMM shape and texture parameters directly from an
   input photo and offering a method for generating huge numbers of labeled
   examples. There are two key points of the paper: one is the training
   data generation for the model training; the other is the training of 3D
   reconstruction model. Experimental results and analysis show that this
   method costs much less time than traditional methods of 3D face
   modeling, and it is improved for different races on photos with any
   angles than the existing methods based on deep learning, and the system
   has better robustness.}},
DOI = {{10.1109/ICICTA.2017.23}},
ISSN = {{1949-1263}},
ISBN = {{978-1-5386-1230-9}},
Unique-ID = {{ISI:000417429000016}},
}

@article{ ISI:000416603000002,
Author = {El Sayed, Abdul Rahman and El Chakik, Abdallah and Alabboud, Hassan and
   Yassine, Adnan},
Title = {{3D face detection based on salient features extraction and skin colour
   detection using data mining}},
Journal = {{IMAGING SCIENCE JOURNAL}},
Year = {{2017}},
Volume = {{65}},
Number = {{7}},
Pages = {{393-408}},
Abstract = {{Face detection has an essential role in many applications. In this
   paper, we propose an efficient and robust method for face detection on a
   3D point cloud represented by a weighted graph. This method classifies
   graph vertices as skin and non-skin regions based on a data mining
   predictive model. Then, the saliency degree of vertices is computed to
   identify the possible candidate face features. Finally, the matching
   between non-skin regions representing eyes, mouth and eyebrows and
   salient regions is done by detecting collisions between polytopes,
   representing these two regions. This method extracts faces from
   situations where pose variation and change of expressions can be found.
   The robustness is showed through different experimental results.
   Moreover, we study the stability of our method according to noise.
   Furthermore, we show that our method deals with 2D images.}},
DOI = {{10.1080/13682199.2017.1358528}},
ISSN = {{1368-2199}},
EISSN = {{1743-131X}},
Unique-ID = {{ISI:000416603000002}},
}

@inproceedings{ ISI:000412830800019,
Author = {Mizoguchi, Tomohiro and Ishii, Akira and Nakamura, Hiroyuki and Inoue,
   Tsuyoshi and Takamatsu, Hisashi},
Editor = {{Remondino, F and Shortis, MR}},
Title = {{Lidar-based Individual Tree Species Classification using Convolutional
   Neural Network}},
Booktitle = {{VIDEOMETRICS, RANGE IMAGING, AND APPLICATIONS XIV}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10332}},
Note = {{Conference on Videometrics, Range Imaging, and Applications XIV, Munich,
   GERMANY, JUN 26-27, 2017}},
Organization = {{SPIE}},
Abstract = {{Terrestrial lidar is commonly used for detailed documentation in the
   field of forest inventory investigation. Recent improvements of point
   cloud processing techniques enabled efficient and precise computation of
   an individual tree shape parameters, such as breast-height diameter,
   height, and volume. However, tree species are manually specified by
   skilled workers to date. Previous works for automatic tree species
   classification mainly focused on aerial or satellite images, and few
   works have been reported for classification techniques using
   ground-based sensor data. Several candidate sensors can be considered
   for classification, such as RGB or multi/hyper spectral cameras. Above
   all candidates, we use terrestrial lidar because it can obtain high
   resolution point cloud in the dark forest. We selected bark texture for
   the classification criteria, since they clearly represent unique
   characteristics of each tree and do not change their appearance under
   seasonable variation and aged deterioration. In this paper, we propose a
   new method for automatic individual tree species classification based on
   terrestrial lidar using Convolutional Neural Network (CNN). The key
   component is the creation step of a depth image which well describe the
   characteristics of each species from a point cloud. We focus on Japanese
   cedar and cypress which cover the large part of domestic forest. Our
   experimental results demonstrate the effectiveness of our proposed
   method.}},
DOI = {{10.1117/12.2270123}},
Article-Number = {{UNSP 103320O}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1110-8; 978-1-5106-1109-2}},
Unique-ID = {{ISI:000412830800019}},
}

@inproceedings{ ISI:000413068300005,
Author = {Prathusha, Sai S. and Suja, P. and Tripathi, Shikha and Louis, R.},
Editor = {{Basu, A and Das, S and Horain, P and Bhattacharya, S}},
Title = {{Emotion Recognition from Facial Expressions of 4D Videos Using Curves
   and Surface Normals}},
Booktitle = {{INTELLIGENT HUMAN COMPUTER INTERACTION, IHCI 2016}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10127}},
Pages = {{51-64}},
Note = {{8th International Conference on Intelligent Human Computer Interaction
   (IHCI), Pilani, INDIA, DEC 12-13, 2016}},
Organization = {{Council Sci \& Ind Res, Cent Elect Engn Res Inst Pilani; Birla Inst
   Technol \& Sci Pilani; Indian Inst Informat Technol}},
Abstract = {{In this paper, we propose and compare three methods for recognizing
   emotions from facial expressions using 4D videos. In the first two
   methods, the 3D faces are re-sampled by using curves to extract the
   feature information. Two different methods are presented to resample the
   faces in an intelligent way using parallel curves and radial curves. The
   movement of the face is measured through these curves using two frames:
   neutral and peak frame. The deformation matrix is formed by computing
   the distance point to point on the corresponding curves of the neutral
   frame and peak frame. This matrix is used to create the feature vector
   that will be used for classification using Support Vector Machine (SVM).
   The third method proposed is to extract the feature information from the
   face by using surface normals. At every point on the frame, surface
   normals are extracted. The deformation matrix is formed by computing the
   Euclidean distances between the corresponding normals at a point on
   neutral and peak frames. This matrix is used to create the feature
   vector that will be used for classification of emotions using SVM. The
   proposed methods are analyzed and they showed improvement over existing
   literature.}},
DOI = {{10.1007/978-3-319-52503-7\_5}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-52503-7; 978-3-319-52502-0}},
Unique-ID = {{ISI:000413068300005}},
}

@inproceedings{ ISI:000408273100011,
Author = {Rajeev, Srijith and Kamath, Shreyas K. M. and Panetta, Karen and Agaian,
   Sos},
Book-Group-Author = {{IEEE}},
Title = {{3-D Palmprint Modeling for Biometric Verification}},
Booktitle = {{2017 IEEE INTERNATIONAL SYMPOSIUM ON TECHNOLOGIES FOR HOMELAND SECURITY
   (HST)}},
Year = {{2017}},
Note = {{IEEE International Symposium on Technologies for Homeland Security
   (HST), Waltham, MA, APR 25-26, 2017}},
Organization = {{IEEE}},
Abstract = {{Palmprint is a very unique and distinctive biometric trait because of
   features such as a person's inimitable principal lines, wrinkles, delta
   points, and minutiae. These constitute the main reasons why palmprint
   verification is considered as one of the most reliable personal
   identification methods. However, a clear majority of the research on
   palm-prints are concentrated on 2-D palmprint images irrespective of the
   fact that the human palm is a 3D-surface. While 2-D palmprint
   recognition has proved to be efficient in terms of verification rate, it
   has some essential downsides. These restrictions can adversely affect
   the performance and robustness of the palmprint recognition system. One
   of the possible solutions to resolve the limitations associated with 2-D
   palm print authentication systems is (i) to use a 3-D scanning system
   and to produce high quality 3-D images with depth information; (ii) to
   map 3-D palm-print images into 2-D images which may support the usage of
   3-D images with both biometric palmprint 2-D image databases and 2-D
   palmprint recognition tools. The bloom of 3-D technologies has made it
   easier to capture and store 3-D images. The problem of a direct mapping
   approach is that a large section of the palm is hard-pressed on the
   scanner surface during 2-D based acquisition. This paper proposes a
   novel technique to unravel/map 3-D palm images to its equivalent 2-D
   palm-print image. This image can be then used to perform efficient and
   accurate 2-D identification/verification. Experimental results and
   discussions will also be presented.}},
ISBN = {{978-1-5090-6356-7}},
Unique-ID = {{ISI:000408273100011}},
}

@inproceedings{ ISI:000407106200038,
Author = {Fraser, Alex and Dallaire, Michael and Godmaire, Xavier P.},
Editor = {{Ratvik, AP}},
Title = {{Laser Marking and 3D Imaging of Aluminum Products}},
Booktitle = {{LIGHT METALS 2017}},
Series = {{Minerals Metals \& Materials Series}},
Year = {{2017}},
Pages = {{289-292}},
Note = {{146th TMS Annual Meeting and Exhibition / Conference on Light Metals,
   San Diego, CA, FEB 26-MAR 02, 2017}},
Organization = {{Minerals Metals \& Mat Soc}},
Abstract = {{Most industrial products have (challenging) 3D shapes, many of them
   require traceability and individual marking. Although some laser marking
   systems on the market have 3D capabilities, they require the 3D shape to
   be loaded in the laser controller and the part to be precisely located.
   However, many industrial processes requiring direct part identification
   cannot fulfill those precise positioning requirements. To overcome these
   limitations, a 3D laser marker with integrated 3D imaging system was
   developed. This imaging system obtains the 3D image of the piece, and
   then the laser controller starts the marking process so that the focus
   fits on the part surface. The whole 3D data acquisition and transfer
   takes less than 3 s. This solves the problem of part positioning and
   simplifies the integration, while also providing 3D data of the surface
   that can be used for quality control.}},
DOI = {{10.1007/978-3-319-51541-0\_38}},
ISSN = {{2367-1181}},
ISBN = {{978-3-319-51541-0; 978-3-319-51540-3}},
Unique-ID = {{ISI:000407106200038}},
}

@inproceedings{ ISI:000406996500085,
Author = {Carraro, Marco and Munaro, Matteo and Roitberg, Alina and Menegatti,
   Emanuele},
Editor = {{Chen, W and Hosoda, K and Menegatti, E and Shimizu, M and Wang, H}},
Title = {{Improved Skeleton Estimation by Means of Depth Data Fusion from Multiple
   Depth Cameras}},
Booktitle = {{INTELLIGENT AUTONOMOUS SYSTEMS 14}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{531}},
Pages = {{1155-1167}},
Note = {{14th International Conference on Intelligent Autonomous Systems (IAS),
   Shanghai Jiao Tong Univ, Shanghai, PEOPLES R CHINA, JUL 03-07, 2016}},
Abstract = {{In this work, we address the problem of human skeleton estimation when
   multiple depth cameras are available. We propose a system that takes
   advantage of the knowledge of the camera poses to create a collaborative
   virtual depth image of the person in the scene which consists of points
   from all the cameras and that represents the person in a frontal pose.
   This depth image is fed as input to the open-source body part detector
   in the Point Cloud Library. A further contribution of this work is the
   improvement of this detector obtained by introducing two new components:
   as a pre-processing, a people detector is applied to remove the
   background from the depth map before estimating the skeleton, while an
   alpha-beta tracking is added as a post-processing step for filtering the
   obtained joint positions over time. The overall system has been proven
   to effectively improve the skeleton estimation on two sequences of
   people in different poses acquired from two first-generation Microsoft
   Kinect.}},
DOI = {{10.1007/978-3-319-48036-7\_85}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-48036-7; 978-3-319-48035-0}},
Unique-ID = {{ISI:000406996500085}},
}

@inproceedings{ ISI:000406534300014,
Author = {McIver, Charles A. and Metcalf, Jeremy P. and Olsen, Richard C.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Spectral LiDAR Analysis for Terrain Classification}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10191}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Anaheim, CA,
   APR 11-12, 2017}},
Organization = {{SPIE}},
Abstract = {{Data from the Optech Titan airborne laser scanner were collected over
   Monterey, CA, in three wavelengths (532 nm, 1064 nm, and 1550 nm), in
   May 2016, by the National Center for Airborne LiDAR Mapping (NCALM).
   Analysis techniques have been developed using spectral technology
   largely derived from the analysis of spectral imagery. Data are analyzed
   as individual points, vs techniques that emphasize spatial binning. The
   primary tool which allows for this exploitation is the N-Dimensional
   Visualizer contained in the ENVI software package. The results allow for
   significant improvement in classification accuracy compared to results
   obtained from techniques derived from standard LiDAR analysis tools.}},
DOI = {{10.1117/12.2276658}},
Article-Number = {{UNSP 101910J}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0883-2; 978-1-5106-0884-9}},
ResearcherID-Numbers = {{Olsen, Richard C/O-2699-2015}},
ORCID-Numbers = {{Olsen, Richard C/0000-0002-8344-9297}},
Unique-ID = {{ISI:000406534300014}},
}

@inproceedings{ ISI:000405560700088,
Author = {Manit, Jirapong and Bremer, Christina and Schweikard, Achim and Ernst,
   Floris},
Editor = {{Webster, RJ and Fei, B}},
Title = {{Patient identification using a near-infrared lasers canner}},
Booktitle = {{MEDICAL IMAGING 2017: IMAGE-GUIDED PROCEDURES, ROBOTIC INTERVENTIONS,
   AND MODELING}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10135}},
Note = {{Conference on Medical Imaging - Image-Guided Procedures, Robotic
   Interventions, and Modeling, Orlando, FL, FEB 14-16, 2017}},
Organization = {{SPIE; Alpin Med Syst; Siemens Healthineers; No Digital Inc}},
Abstract = {{We propose a new biometric approach where the tissue thickness of a
   person's forehead is used as a biometric feature. Given that the spatial
   registration of two 3D laser scans of the same human face usually
   produces a low error value, the principle of point cloud registration
   and its error metric can be applied to human classification techniques.
   However, by only considering the spatial error, it is not possible to
   reliably verify a person's identity. We propose to use a novel
   near-infrared laser-based head tracking system to determine an
   additional feature, the tissue thickness, and include this in the error
   metric. Using MRI as a ground truth, data from the foreheads of 30
   subjects was collected from which a 4D reference point cloud was created
   for each subject. The measurements from the near-infrared system were
   registered with all reference point clouds using the ICP algorithm.
   Afterwards, the spatial and tissue thickness errors were extracted,
   forming a 2D feature space. For all subjects, the lowest feature
   distance resulted from the registration of a measurement and the
   reference point cloud of the same person.
   The combined registration error features yielded two clusters in the
   feature space, one from the same subject and another from the other
   subjects. When only the tissue thickness error was considered, these
   clusters were less distinct but still present. These findings could help
   to raise safety standards for head and neck cancer patients and lays the
   foundation for a future human identification technique.}},
DOI = {{10.1117/12.2254963}},
Article-Number = {{UNSP 101352L}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0715-6; 978-1-5106-0716-3}},
Unique-ID = {{ISI:000405560700088}},
}

@inproceedings{ ISI:000402657200006,
Author = {Bobulski, Janusz},
Editor = {{Choras, RS}},
Title = {{Face Recognition with 3D Face Asymmetry}},
Booktitle = {{IMAGE PROCESSING AND COMMUNICATIONS CHALLENGES 8}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{525}},
Pages = {{53-60}},
Note = {{8th International Conference on Image Processing and Communications
   (IP\&C), UTP Univ Technol \& Sci, Inst Telecommunicat \& Comp Sci,
   Bydgoszcz, POLAND, SEP 07-09, 2016}},
Organization = {{UTP Univ Technol \& Sci}},
Abstract = {{Using of 3D images for the identification was in a field of the interest
   of many researchers which developed a few methods offering good results.
   However, there are few techniques exploiting the 3D asymmetry amongst
   these methods. We propose fast algorithm for rough extraction face
   asymmetry that is used to 3D face recognition with hidden Markov models.
   This paper presents conception of fast method for determine 3D face
   asymmetry. The research results indicate that face recognition with 3D
   face asymmetry may be used in biometrics systems.}},
DOI = {{10.1007/978-3-319-47274-4\_6}},
ISSN = {{2194-5357}},
EISSN = {{2194-5365}},
ISBN = {{978-3-319-47274-4; 978-3-319-47273-7}},
ResearcherID-Numbers = {{Bobulski, Janusz/C-4998-2014}},
ORCID-Numbers = {{Bobulski, Janusz/0000-0003-3345-604X}},
Unique-ID = {{ISI:000402657200006}},
}

@inproceedings{ ISI:000399000000019,
Author = {Zhou, Changhe and Wang, Shaoqing and Li, Chao and Li, Hao and Liu, Zhao},
Editor = {{Sheng, Y and Yu, C and Zhou, C}},
Title = {{Three-dimensional identification card and applications}},
Booktitle = {{HOLOGRAPHY, DIFFRACTIVE OPTICS, AND APPLICATIONS VII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10022}},
Note = {{Conference on Holography, Diffractive Optics, and Applications VII,
   Beijing, PEOPLES R CHINA, OCT 12-14, 2016}},
Organization = {{SPIE; Chinese Opt Soc}},
Abstract = {{Three dimensional Identification Card, with its three-dimensional
   personal image displayed and stored for personal identification, is
   supposed be the advanced version of the present two-dimensional
   identification card in the future {[} 1]. Three dimensional
   Identification Card means that there are three-dimensional optical
   techniques are used, the personal image on ID card is displayed to be
   three-dimensional, so we can see three dimensional personal face. The ID
   card also stores the three-dimensional face information in its inside
   electronics chip, which might be recorded by using two-channel cameras,
   and it can be displayed in computer as three-dimensional images for
   personal identification. Three-dimensional ID card might be one
   interesting direction to update the present two-dimensional card in the
   future. Three-dimension ID card might be widely used in airport custom,
   entrance of hotel, school, university, as passport for on-line banking,
   registration of on-line game, etc...}},
DOI = {{10.1117/12.2245680}},
Article-Number = {{UNSP 100220L}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0463-6; 978-1-5106-0464-3}},
Unique-ID = {{ISI:000399000000019}},
}

@article{ ISI:000391527900002,
Author = {Wang, Yuanyuan and Zhu, Xiao Xiang and Zeisl, Bernhard and Pollefeys,
   Marc},
Title = {{Fusing Meter-Resolution 4-D InSAR Point Clouds and Optical Images for
   Semantic Urban Infrastructure Monitoring}},
Journal = {{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{55}},
Number = {{1}},
Pages = {{14-26}},
Month = {{JAN}},
Abstract = {{Using synthetic aperture radar (SAR) interferometry to monitor long-term
   millimeter-level deformation of urban infrastructures, such as
   individual buildings and bridges, is an emerging and important field in
   remote sensing. In the state-of-the-art methods, deformation parameters
   are retrieved and monitored on a pixel basis solely in the SAR image
   domain. However, the inevitable side-looking imaging geometry of SAR
   results in undesired occlusion and layover in urban area, rendering the
   current method less competent for a semantic-level monitoring of
   different urban infrastructures. This paper presents a framework of a
   semantic-level deformation monitoring by linking the precise deformation
   estimates of SAR interferometry and the semantic classification labels
   of optical images via a 3-D geometric fusion and semantic texturing. The
   proposed approach provides the first ``SARptical{''} point cloud of an
   urban area, which is the SAR tomography point cloud textured with
   attributes from optical images. This opens a new perspective of InSAR
   deformation monitoring. Interesting examples on bridge and railway
   monitoring are demonstrated.}},
DOI = {{10.1109/TGRS.2016.2554563}},
ISSN = {{0196-2892}},
EISSN = {{1558-0644}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000391527900002}},
}

@article{ ISI:000397373000001,
Author = {Jin, Hai and Wang, Xun and Zhong, Zichun and Hua, Jing},
Title = {{Robust 3D face modeling and reconstruction from frontal and side images}},
Journal = {{COMPUTER AIDED GEOMETRIC DESIGN}},
Year = {{2017}},
Volume = {{50}},
Pages = {{1-13}},
Month = {{JAN}},
Abstract = {{Robust and effective capture and reconstruction of 3D face models
   directly by smartphone users enables many applications. This paper
   presents a novel 3D face modeling and reconstruction solution that
   robustly and accurately acquire 3D face models from a couple of images
   captured by a single smartphone camera. Two selfie photos of a subject
   taken from the front and side are first used to guide our Non-Negative
   Matrix Factorization (NMF) induced part-based face model to iteratively
   reconstruct an initial 3D face of the subject. Then, an iterative detail
   updating method is applied to the initial generated 3D face to
   reconstruct facial details through optimizing lighting parameters and
   local depths. Our iterative 3D face reconstruction method permits fully
   automatic registration of a part based face representation to the
   acquired face data and the detailed 2D/3D features to build a
   high-quality 3D face model. The NMF part-based face representation
   learned from a 3D face database facilitates effective global and
   adaptive local detail data fitting alternatively. Our system is flexible
   and it allows users to conduct the capture in any uncontrolled
   environment. We demonstrate the capability of our method by allowing
   users to capture and reconstruct their 3D faces by themselves. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.cagd.2016.11.001}},
ISSN = {{0167-8396}},
EISSN = {{1879-2332}},
ResearcherID-Numbers = {{Smith, Nash/R-3104-2017}},
ORCID-Numbers = {{Smith, Nash/0000-0002-4406-0043}},
Unique-ID = {{ISI:000397373000001}},
}

@article{ ISI:000394557800008,
Author = {Raith, Stefan and Vogel, Eric Per and Anees, Naeema and Keul, Christine
   and Gueth, Jan-Frederik and Edelhoff, Daniel and Fischer, Horst},
Title = {{Artificial Neural Networks as a powerful numerical tool to classify
   specific features of a tooth based on 3D scan data}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2017}},
Volume = {{80}},
Pages = {{65-76}},
Month = {{JAN 1}},
Abstract = {{Chairside manufacturing based on digital image acquisition is
   gainingincreasing importance in dentistry. For the standardized
   application of these methods, it is paramount to have highly automated
   digital workflows that can process acquired 3D image data of dental
   surfaces. Artificial Neural Networks (ANNs) arenumerical methods
   primarily used to mimic the complex networks of neural conneetions in
   the natural brain. Our hypothesis is that an ANNcan be developed that is
   capable of classifying dental cusps with sufficient accuracy. This bears
   enormous potential for an application in chairside manufacturing
   Workflows in the dental field, as it closes the gap between digital
   acquisition of dental geometries and modern computer-aided manufacturing
   techniques.Three-dimensional surface scans of dental casts representing
   natural full dental arches were transformed to range image data. These
   data were processed using an automated algorithm to detect candidates
   for tooth cusps according to salient geometrical features. These
   candidates were classified following common dental terminology and used
   as training data for a tailored ANN. For the actual cusp feature
   description, two different approaches were developed and applied to the
   available data: The first uses the relative location of the detected
   cusps as input data and the second method directly takes the image
   information given in the range images. In addition, a combination of
   both was implemented and investigatud. Both approaches showed high
   performance with correct classifications of 93.3\% and 93.5\%,
   respectively, with improvements by the combination shown to be
   minor.This article presents for the first time a fully automated method
   for the classification of teeth that could be confirmed to work with
   sufficient precision to exhibit the potential for its use in clinical
   practice,which is a prerequisite for automated computer-aided planning
   of prosthetic treatments with subsequent automated chairside
   manufacturing.}},
DOI = {{10.1016/j.compbiomed.2016.11.013}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
ResearcherID-Numbers = {{Guth, Jan-Frederik/Q-3999-2017}},
Unique-ID = {{ISI:000394557800008}},
}

@article{ ISI:000395485900101,
Author = {Zhang, Yi and Mu, Zhichun and Yuan, Li and Zeng, Hui and Chen, Long},
Title = {{3D Ear Normalization and Recognition Based on Local Surface Variation}},
Journal = {{APPLIED SCIENCES-BASEL}},
Year = {{2017}},
Volume = {{7}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{Most existing ICP (Iterative Closet Point)-based 3D ear recognition
   approaches resort to the coarse-to-fine ICP algorithms to match 3D ear
   models. With such an approach, the gallery-probe pairs are coarsely
   aligned based on a few local feature points and then finely matched
   using the original ear point cloud. However, such an approach ignores
   the fact that not all the points in the coarsely segmented ear data make
   positive contributions to recognition. As such, the coarsely segmented
   ear data which contains a lot of redundant and noisy data could lead to
   a mismatch in the recognition scenario. Additionally, the fine ICP
   matching can easily trap in local minima without the constraint of local
   features. In this paper, an efficient and fully automatic 3D ear
   recognition system is proposed to address these issues. The system
   describes the 3D ear surface with a local featurethe Local Surface
   Variation (LSV), which is responsive to the concave and convex areas of
   the surface. Instead of being used to extract discrete key points, the
   LSV descriptor is utilized to eliminate redundancy flat non-ear data and
   get normalized and refined ear data. At the stage of recognition, only
   one-step modified iterative closest points using local surface variation
   (ICP-LSV) algorithm is proposed, which provides additional local feature
   information to the procedure of ear recognition to enhance both the
   matching accuracy and computational efficiency. On an
   Inter((R))Xeon((R))W3550, 3.07 GHz work station (DELL T3500, Beijing,
   China), the authors were able to extract features from a probe ear in
   2.32 s match the ear with a gallery ear in 0.10 s using the method
   outlined in this paper. The proposed algorithm achieves rank-one
   recognition rate of 100\% on the Chinese Academy of Sciences' Institute
   of Automation 3D Face database (CASIA-3D FaceV1, CASIA, Beijing, China,
   2004) and 98.55\% with 2.3\% equal error rate (EER) on the Collection J2
   of University of Notre Dame Biometrics Database (UND-J2, University of
   Notre Dame, South Bend, IN, USA, between 2003 and 2005).}},
DOI = {{10.3390/app7010104}},
Article-Number = {{104}},
ISSN = {{2076-3417}},
Unique-ID = {{ISI:000395485900101}},
}

@inproceedings{ ISI:000463335100014,
Author = {Mathlouthi, Soumaya and Jribi, Majdi and Ghorbel, Faouzi},
Editor = {{BlancTalon, J and Penne, R and Philips, W and Popescu, D and Scheunders, P}},
Title = {{A Novel and Accurate Local 3D Representation for Face Recognition}},
Booktitle = {{ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS (ACIVS 2017)}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10617}},
Pages = {{161-169}},
Note = {{18th International Conference on Advanced Concepts for Intelligent
   Vision Systems (ACIVS), Antwerp, BELGIUM, SEP 18-21, 2017}},
Organization = {{Antwerp Univ; Commonwealth Sci \& Ind Res Org; Ghent Univ}},
Abstract = {{In this paper, we intend to introduce a novel curved 3D face
   representation. It is constructed on some static parts of the face which
   correspond to the nose and the eyes. Each part is described by the level
   curves of the superposition of several geodesic potentials generated
   from many reference points. We propose to describe the eye region by a
   bipolar representation based on the superposition of two geodesic
   potentials generated from two reference points and the nose by a
   three-polar one (three reference points). We use the BU-3DFE database of
   3D faces to test the accuracy of the proposed approach. The obtained
   results in the sense of the Hausdorff shape distance prove the
   performance of the novel representation for 3D faces identification. The
   obtained scores are comparable to the state of the art methods in the
   most of cases.}},
DOI = {{10.1007/978-3-319-70353-4\_14}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-70353-4; 978-3-319-70352-7}},
Unique-ID = {{ISI:000463335100014}},
}

@article{ ISI:000401423700003,
Author = {Kramer, Heather A. and Collins, Brandon M. and Gallagher, Claire V. and
   Keane, John J. and Stephens, Scott L. and Kelly, Maggi},
Title = {{Accessible light detection and ranging: estimating large tree density
   for habitat identification}},
Journal = {{ECOSPHERE}},
Year = {{2016}},
Volume = {{7}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Large trees are important to a wide variety of wildlife, including many
   species of conservation concern, such as the California spotted owl
   (Strix occidentalis occidentalis). Light detection and ranging (LiDAR)
   has been successfully utilized to identify the density of large-diameter
   trees, either by segmenting the LiDAR point cloud into individual trees,
   or by building regression models between variables extracted from the
   LiDAR point cloud and field data. Neither of these methods is easily
   accessible for most land managers due to the reliance on specialized
   software, and much available LiDAR data are being underutilized due to
   the steep learning curve required for advanced processing using these
   programs. This study derived a simple, yet effective method for
   estimating the density of large-stemmed trees from the LiDAR canopy
   height model, a standard raster product derived from the LiDAR point
   cloud that is often delivered with the LiDAR and is easy to process by
   personnel trained in geographic information systems (GIS). Ground plots
   needed to be large (1 ha) to build a robust model, but the spatial
   accuracy of plot center was less crucial to model accuracy. We also
   showed that predicted large tree density is positively linked to
   California spotted owl nest sites.}},
DOI = {{10.1002/ecs2.1593}},
Article-Number = {{e01593}},
ISSN = {{2150-8925}},
Unique-ID = {{ISI:000401423700003}},
}

@article{ ISI:000396382500043,
Author = {Magnard, Christophe and Morsdorf, Felix and Small, David and Stilla, Uwe
   and Schaepman, Michael E. and Meier, Erich},
Title = {{Single tree identification using airborne multibaseline SAR
   interferometry data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2016}},
Volume = {{186}},
Pages = {{567-580}},
Month = {{DEC 1}},
Abstract = {{Remote sensing data allow large scale observation of forested
   ecosystems. Forest assessment benefits from information about individual
   trees. Multibaseline SAR interferometry (InSAR) is able to generate
   dense point clouds of forest canopies, similar to airborne laser
   scanning (ALS). This type of point cloud was generated using data from
   the Ka-band MEMPHIS system, acquired over a mainly coniferous forest
   near Vordemwald in the Swiss Midlands. This point cloud was segmented
   using an advanced clustering technique to detect individual trees and
   derive their positions, heights, and crown diameters. To evaluate the
   InSAR point cloud properties and limitations, it was compared to
   products derived from ALS and stereo-photogrammetry. All point clouds
   showed similar geolocation accuracies with 02-0.3 m relative shifts.
   Both InSAR and photogrammetry techniques yielded points predominantly
   located in the upper levels of the forest vegetation, while ALS provided
   points from the top of the canopy down to the understory and forest
   floor. The canopy height models agreed very well with each other, with
   R-2 values between 0.84 and 0.89. The detected trees and their estimated
   physical and structural parameters were validated by comparing them to
   reference forestry data. A detection rate of similar to 90\% was
   achieved for larger trees, corresponding to half of the reference trees.
   The smaller trees were detected with a success rate of similar to 50\%.
   The tree height was slightly underestimated, with a R-2 value of 0.63.
   The estimated crown diameter agreed on an average sense, however with a
   relatively low R-2 value of 0.19. Very high success rates (>90\%) were
   obtained when matching the trees detected from the InSAR-data with those
   detected from the ALS- and photogrammetry-data. There, InSAR tree
   heights were in the mean 1-1.5 m lower, with high R-2 values ranging
   between 0.8 and 0.9. Our results demonstrate the use of millimeter wave
   SAR interferometry data as an alternative to ALS- and
   photogrammetry-based data for forest monitoring. (C) 2016 Elsevier Inc.
   All rights reserved.}},
DOI = {{10.1016/j.rse.2016.09.018}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Schaepman, Michael/B-9213-2009
   }},
ORCID-Numbers = {{Schaepman, Michael/0000-0002-9627-9565
   Magnard, Christophe/0000-0002-1473-8650}},
Unique-ID = {{ISI:000396382500043}},
}

@article{ ISI:000392085100011,
Author = {Pohlmann, Stefanie T. L. and Harkness, Elaine F. and Taylor, Christopher
   J. and Astley, Susan M.},
Title = {{Evaluation of Kinect 3D Sensor for Healthcare Imaging}},
Journal = {{JOURNAL OF MEDICAL AND BIOLOGICAL ENGINEERING}},
Year = {{2016}},
Volume = {{36}},
Number = {{6, SI}},
Pages = {{857-870}},
Month = {{DEC}},
Abstract = {{Microsoft Kinect is a three-dimensional (3D) sensor originally designed
   for gaming that has received growing interest as a cost-effective and
   safe device for healthcare imaging. Recent applications of Kinect in
   health monitoring, screening, rehabilitation, assistance systems, and
   intervention support are reviewed here. The suitability of available
   technologies for healthcare imaging applications is assessed. The
   performance of Kinect I, based on structured light technology, is
   compared with that of the more recent Kinect II, which uses
   time-of-flight measurement, under conditions relevant to healthcare
   applications. The accuracy, precision, and resolution of 3D images
   generated with Kinect I and Kinect II are evaluated using flat cardboard
   models representing different skin colors (pale, medium, and dark) at
   distances ranging from 0.5 to 1.2 m and measurement angles of up to 75A
   degrees. Both sensors demonstrated high accuracy (majority of
   measurements < 2 mm) and precision (mean point to plane error < 2 mm) at
   an average resolution of at least 390 points per cm(2). Kinect I is
   capable of imaging at shorter measurement distances, but Kinect II
   enables structures angled at over 60A degrees to be evaluated. Kinect II
   showed significantly higher precision and Kinect I showed significantly
   higher resolution (both p < 0.001). The choice of object color can
   influence measurement range and precision. Although Kinect is not a
   medical imaging device, both sensor generations show performance
   adequate for a range of healthcare imaging applications. Kinect I is
   more appropriate for short-range imaging and Kinect II is more
   appropriate for imaging highly curved surfaces such as the face or
   breast.}},
DOI = {{10.1007/s40846-016-0184-2}},
ISSN = {{1609-0985}},
EISSN = {{2199-4757}},
ORCID-Numbers = {{Harkness, Elaine/0000-0001-6625-7739}},
Unique-ID = {{ISI:000392085100011}},
}

@article{ ISI:000391303000155,
Author = {Rose, Johann Christian and Kicherer, Anna and Wieland, Markus and
   Klingbeil, Lasse and Toepfer, Reinhard and Kuhlmann, Heiner},
Title = {{Towards Automated Large-Scale 3D Phenotyping of Vineyards under Field
   Conditions}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{In viticulture, phenotypic data are traditionally collected directly in
   the field via visual and manual means by an experienced person. This
   approach is time consuming, subjective and prone to human errors. In
   recent years, research therefore has focused strongly on developing
   automated and non-invasive sensor-based methods to increase data
   acquisition speed, enhance measurement accuracy and objectivity and to
   reduce labor costs. While many 2D methods based on image processing have
   been proposed for field phenotyping, only a few 3D solutions are found
   in the literature. A track-driven vehicle consisting of a camera system,
   a real-time-kinematic GPS system for positioning, as well as hardware
   for vehicle control, image storage and acquisition is used to visually
   capture a whole vine row canopy with georeferenced RGB images. In the
   first post-processing step, these images were used within a
   multi-view-stereo software to reconstruct a textured 3D point cloud of
   the whole grapevine row. A classification algorithm is then used in the
   second step to automatically classify the raw point cloud data into the
   semantic plant components, grape bunches and canopy. In the third step,
   phenotypic data for the semantic objects is gathered using the
   classification results obtaining the quantity of grape bunches, berries
   and the berry diameter.}},
DOI = {{10.3390/s16122136}},
Article-Number = {{2136}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000391303000155}},
}

@article{ ISI:000386741300011,
Author = {Li, Billy Y. L. and Xue, Mingliang and Mian, Ajmal and Liu, Wanquan and
   Krishna, Aneesh},
Title = {{Robust RGB-D face recognition using Kinect sensor}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{214}},
Pages = {{93-108}},
Month = {{NOV 19}},
Abstract = {{In this paper we propose a robust face recognition algorithm for low
   resolution RGB-D Kinect data. Many techniques are proposed for image
   preprocessing due to the noisy depth data. First, facial symmetry is
   exploited based on the 3D point cloud to obtain a canonical frontal view
   image irrespective of the initial pose and then depth data is converted
   to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are
   then used to project RGB to DCS (Discriminant Color Space) and normal
   maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust
   Sparse Coding method is proposed that codes the multiple channels (DCS
   or DNM) of a test image as a sparse combination of training samples with
   different pixel weighting. Weights are calculated dynamically in an
   iterative process to achieve robustness against variations in pose,
   illumination, facial expressions and disguise. In contrast to existing
   techniques, our multi-channel approach is more robust to variations.
   Reconstruction errors of the test image (DCS and DNM) are normalized and
   fused to decide its identity. The proposed algorithm is evaluated on
   four public databases. It achieves 98.4\% identification rate on
   CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects.
   Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2
   databases, the proposed algorithm achieves 97.6\%, 95.6\% and 95.2\%
   identification rates respectively. To the best of our knowledge, these
   are the highest identification rates reported so far for the first three
   databases. (C) 2016 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2016.06.012}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
ORCID-Numbers = {{Krishna, Aneesh/0000-0001-8637-5732
   liu, wanquan/0000-0003-4910-353X}},
Unique-ID = {{ISI:000386741300011}},
}

@article{ ISI:000386910000105,
Author = {Nakamura, Tomoya and Matsumoto, Jumpei and Nishimaru, Hiroshi and
   Bretas, Rafael Vieira and Takamura, Yusaku and Hori, Etsuro and Ono,
   Taketoshi and Nishijo, Hisao},
Title = {{A Markerless 3D Computerized Motion Capture System Incorporating a
   Skeleton Model for Monkeys}},
Journal = {{PLOS ONE}},
Year = {{2016}},
Volume = {{11}},
Number = {{11}},
Month = {{NOV 3}},
Abstract = {{In this study, we propose a novel markerless motion capture system (MCS)
   for monkeys, in which 3D surface images of monkeys were reconstructed by
   integrating data from four depth cameras, and a skeleton model of the
   monkey was fitted onto 3D images of monkeys in each frame of the video.
   To validate the MCS, first, estimated 3D positions of body parts were
   compared between the 3D MCS-assisted estimation and manual estimation
   based on visual inspection when a monkey performed a shuttling behavior
   in which it had to avoid obstacles in various positions. The mean
   estimation error of the positions of body parts (3-14 cm) and of head
   rotation (35-43 degrees) between the 3D MCS-assisted and manual
   estimation were comparable to the errors between two different
   experimenters performing manual estimation. Furthermore, the MCS could
   identify specific monkey actions, and there was no false positive nor
   false negative detection of actions compared with those in manual
   estimation. Second, to check the reproducibility of MCS-assisted
   estimation, the same analyses of the above experiments were repeated by
   a different user. The estimation errors of positions of most body parts
   between the two experimenters were significantly smaller in the
   MCS-assisted estimation than in the manual estimation. Third, effects of
   methamphetamine (MAP) administration on the spontaneous behaviors of
   four monkeys were analyzed using the MCS. MAP significantly increased
   head movements, tended to decrease locomotion speed, and had no
   significant effect on total path length. The results were comparable to
   previous human clinical data. Furthermore, estimated data following MAP
   injection (total path length, walking speed, and speed of head rotation)
   correlated significantly between the two experimenters in the
   MCS-assisted estimation (r = 0.863 to 0.999). The results suggest that
   the presented MCS in monkeys is useful in investigating neural
   mechanisms underlying various psychiatric disorders and developing
   pharmacological interventions.}},
DOI = {{10.1371/journal.pone.0166154}},
Article-Number = {{e0166154}},
ISSN = {{1932-6203}},
ORCID-Numbers = {{Matsumoto, Jumpei/0000-0003-4729-2816}},
Unique-ID = {{ISI:000386910000105}},
}

@article{ ISI:000386995100031,
Author = {Wolff, Antje and Gotz, Yvonne},
Title = {{4D phenotyping of germinating seeds and seedlings as a tool to
   objectively measure seed quality and improve field establishment and
   yield of sugar beets}},
Journal = {{INTERNATIONAL SUGAR JOURNAL}},
Year = {{2016}},
Volume = {{118}},
Number = {{1415}},
Pages = {{836-839}},
Month = {{NOV}},
Abstract = {{The plant breeding company Strube, in cooperation with the German
   Fraunhofer Institute for non-destructive testing, has developed an
   automated high-throughput germination test for sugar beet seeds. The
   phenoTest permits objective measurement and classification of
   germinating seeds and resulting seedlings. It is therefore more accurate
   and provides more information than the conventional ISTA (International
   Seed Testing Association)-germination test, which relies purely on
   visual assessment and classification into the categories ``normal{''} or
   ``abnormal{''}. This differentiation is difficult to standardise, and
   is, to a significant degree, subjective. The phenoTest is based on
   three-dimensional (3D) X-ray images. Repeated tests of the same plants
   enable an objective assessment of seedling development over the course
   of time (4D phenotyping). The individual organs of each plant (radicle,
   hypocotyl and cotyledons) are automatically identified and measured. The
   method provides detailed information on germinating capacity and vigour,
   as well as the homogeneity of a seed lot. Results are documented as
   measurement values and 3D-images of each individual plant at different
   time points. The data is used to compare seed lots concerning their
   natural germination capacity and especially vigour, the processing or
   priming technologies they experienced, the pelleting and seed treatment
   applied etc. in order to predict their field emergence potential even
   under difficult growing conditions. These analyses also helps to
   optimise all these processes in commercial seed production to obtain a
   quick, homogeneous and complete field emergence, making full use of the
   genetic yield potential of sugar beet.}},
ISSN = {{0020-8841}},
Unique-ID = {{ISI:000386995100031}},
}

@article{ ISI:000386874900020,
Author = {Guo, Yulan and Lei, Yinjie and Liu, Li and Wang, Yan and Bennamoun,
   Mohammed and Sohelf, Ferdous},
Title = {{EI3D: Expression-invariant 3D face recognition based on feature and
   shape matching}},
Journal = {{PATTERN RECOGNITION LETTERS}},
Year = {{2016}},
Volume = {{83}},
Number = {{3}},
Pages = {{403-412}},
Month = {{NOV 1}},
Abstract = {{This paper presents a local feature based shape matching algorithm for
   expression-invariant 3D face recognition. Each 3D face is first
   automatically detected from a raw 3D data and normalized to achieve pose
   invariance. The 3D face is then represented by a set of keypoints and
   their associated local feature descriptors to achieve robustness to
   expression variations. During face recognition, a probe face is compared
   against each gallery face using both local feature matching and 3D point
   cloud registration. The number of feature matches, the average distance
   of matched features, and the number of closest point pairs after
   registration are used to measure the similarity between two 3D faces.
   These similarity metrics are then fused to obtain the final results. The
   proposed algorithm has been tested on the FRGC v2 benchmark and a high
   recognition performance has been achieved. It obtained the
   state-of-the-art results by achieving an overall rank- 1 identification
   rate of 97.0\% and an average verification rate of 99.01\% at 0.001
   false acceptance rate for all faces with neutral and non-neutral
   expressions. Further, the robustness of our algorithm under different
   occlusions has been demonstrated on the Bosphorus dataset. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.patrec.2016.04.003}},
ISSN = {{0167-8655}},
EISSN = {{1872-7344}},
ResearcherID-Numbers = {{Sohel, Ferdous/C-2428-2013
   }},
ORCID-Numbers = {{Sohel, Ferdous/0000-0003-1557-4907
   Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000386874900020}},
}

@article{ ISI:000382794300032,
Author = {Deng, Lei and Chen, Zhixiang and Chen, Baohua and Duan, Yueqi and Zhou,
   Jie},
Title = {{Incremental image set querying based localization}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{208}},
Number = {{SI}},
Pages = {{315-324}},
Month = {{OCT 5}},
Abstract = {{Image based localization has been developed for many applications such
   as mobile localization, auto navigation, augmented reality and photo
   tourism. When the querying image is matched against a pre-built 3D
   feature point cloud, its pose can be estimated for future use. However,
   when the querying image is distant from the pre-built 3D point cloud,
   conventional single image-based localization method will fail. To
   address this problem, we present an incremental image set querying based
   localization framework. When single image localization fails, the system
   will incrementally ask the user to input more auxiliary images until the
   localization is successful and stable. The main idea is that image set,
   instead of single image, is matched against the pre-built 3D point cloud
   to meet the challenge. Next the image set is incrementally enlarged and
   aggregated to form a local 3D model. Compared with single image querying
   based localization method, the querying 3D model contains more
   information and geometry constraints which are essential for
   localization. Experiments have demonstrated the effectiveness and
   feasibility of the proposed framework. (C) 2016 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.neucom.2015.11.117}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000382794300032}},
}

@article{ ISI:000382794300028,
Author = {Zhang, Erhu and Chen, Wanjun and Zhang, Zhuomin and Zhang, Yan},
Title = {{Local Surface Geometric Feature for 3D human action recognition}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{208}},
Number = {{SI}},
Pages = {{281-289}},
Month = {{OCT 5}},
Abstract = {{This paper presents a novel Local Surface Geometric Feature (LSGF) for
   human action recognition from video sequences captured by a depth
   camera. The LSGF is extracted from each skeleton joint in point cloud
   space to capture the static appearance and pose cues, which includes
   joint position, normal, and local curvature. A temporal pyramid of
   covariance matrix is exploited to model both pairwise relations of
   features instead of features themselves and the temporal evolution.
   Finally, Fisher vector encoding is imported as a global representation
   for a video sequence and SVM classifier is used for classification. In
   the extensive experiments, we achieve classification results superior to
   most of previous published results on three public benchmark datasets,
   i.e., MSR-Action3D, MSR DailyActivity3D, and UTItinect Action. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2015.12.122}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000382794300028}},
}

@article{ ISI:000387670700012,
Author = {Dalponte, Michele and Coomes, David A.},
Title = {{Tree-centric mapping of forest carbon density from airborne laser
   scanning and hyperspectral data}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2016}},
Volume = {{7}},
Number = {{10}},
Pages = {{1236-1245}},
Month = {{OCT}},
Abstract = {{1. Forests are a major component of the global carbon cycle, and
   accurate estimation of forest carbon stocks and fluxes is important in
   the context of anthropogenic global change. Airborne laser scanning
   (ALS) data sets are increasingly recognized as outstanding data sources
   for high-fidelity mapping of carbon stocks at regional scales.
   2. We develop a tree-centric approach to carbon mapping, based on
   identifying individual tree crowns (ITCs) and species from airborne
   remote sensing data, from which individual tree carbon stocks are
   calculated. We identify ITCs from the laser scanning point cloud using a
   region-growing algorithm and identifying species from airborne
   hyperspectral data by machine learning. For each detected tree, we
   predict stem diameter from its height and crown-width estimate. From
   that point on, we use well-established approaches developed for
   field-based inventories: above-ground biomasses of trees are estimated
   using published allometries and summed within plots to estimate carbon
   density.
   3. We show this approach is highly reliable: tests in the Italian Alps
   demonstrated a close relationship between field-and ALS-based estimates
   of carbon stocks (r(2) = 0.98). Small trees are invisible from the air,
   and a correction factor is required to accommodate this effect.
   4. An advantage of the tree-centric approach over existing area-based
   methods is that it can produce maps at any scale and is fundamentally
   based on field-based inventory methods, making it intuitive and
   transparent. Airborne laser scanning, hyperspectral sensing and
   computational power are all advancing rapidly, making it increasingly
   feasible to use ITC approaches for effective mapping of forest carbon
   density also inside wider carbon mapping programs like REDD++.}},
DOI = {{10.1111/2041-210X.12575}},
ISSN = {{2041-210X}},
EISSN = {{2041-2096}},
ResearcherID-Numbers = {{Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000387670700012}},
}

@article{ ISI:000385597700004,
Author = {Li, Lin and Li, Dalin and Zhu, Haihong and Li, You},
Title = {{A dual growing method for the automatic extraction of individual trees
   from mobile laser scanning data}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{120}},
Pages = {{37-52}},
Month = {{OCT}},
Abstract = {{Street trees interlaced with other objects in cluttered point clouds of
   urban scenes inhibit the automatic extraction of individual trees. This
   paper proposes a method for the automatic extraction of individual trees
   from mobile laser scanning data, according to the general constitution
   of trees. Two components of each individual tree - a trunk and a crown
   can be extracted by the dual growing method. This method consists of
   coarse classification, through which most of artifacts are removed; the
   automatic selection of appropriate seeds for individual trees, by which
   the common manual initial setting is avoided; a dual growing process
   that separates one tree from others by circumscribing a trunk in an
   adaptive growing radius and segmenting a crown in constrained growing
   regions; and a refining process that draws a singular trunk from the
   interlaced other objects. The method is verified by two datasets with
   over 98\% completeness and over 96\% correctness. The low mean absolute
   percentage errors in capturing the morphological parameters of
   individual trees indicate that this method can output individual trees
   with high precision. (C) 2016 International Society for Photogrammetry
   and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.07.009}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
Unique-ID = {{ISI:000385597700004}},
}

@article{ ISI:000388855500007,
Author = {Ximena Bastidas-Rodriguez, Maria and Prieto-Ortiz, Flavio A. and
   Espejo-Mora, Edgar},
Title = {{Fractographic classification in metallic materials by using 3D
   processing and computer vision techniques}},
Journal = {{REVISTA FACULTAD DE INGENIERIA, UNIVERSIDAD PEDAGOGICA Y TECNOLOGICA DE
   COLOMBIA}},
Year = {{2016}},
Volume = {{25}},
Number = {{43}},
Pages = {{83-96}},
Month = {{SEP-DEC}},
Abstract = {{Failure analysis aims at collecting information about how and why a
   failure is produced. The first step in this process is a visual
   inspection on the flaw surface that will reveal the features, marks, and
   texture, which characterize each type of fracture. This is generally
   carried out by personnel with no experience that usually lack the
   knowledge to do it. This paper proposes a classification method for
   three kinds of fractures in crystalline materials: brittle, fatigue, and
   ductile. The method uses 3D vision, and it is expected to support
   failure analysis. The features used in this work were: i) Haralick's
   features and ii) the fractal dimension. These features were applied to
   3D images obtained from a confocal laser scanning microscopy Zeiss LSM
   700. For the classification, we evaluated two classifiers: Artificial
   Neural Networks and Support Vector Machine. The performance evaluation
   was made by extracting four marginal relations from the confusion
   matrix: accuracy, sensitivity, specificity, and precision, plus three
   evaluation methods: Receiver Operating Characteristic space, the
   Individual Classification Success Index, and the Jaccard's coefficient.
   Despite the classification percentage obtained by an expert is better
   than the one obtained with the algorithm, the algorithm achieves a
   classification percentage near or exceeding the 60 \% accuracy for the
   analyzed failure modes. The results presented here provide a good
   approach to address future research on texture analysis using 3D data.}},
DOI = {{10.19053/01211129.v25.n43.2016.5301}},
ISSN = {{0121-1129}},
EISSN = {{2357-5328}},
Unique-ID = {{ISI:000388855500007}},
}

@article{ ISI:000385488000039,
Author = {Cao, Lin and Gao, Sha and Li, Pinghao and Yun, Ting and Shen, Xin and
   Ruan, Honghua},
Title = {{Aboveground Biomass Estimation of Individual Trees in a Coastal Planted
   Forest Using Full-Waveform Airborne Laser Scanning Data}},
Journal = {{REMOTE SENSING}},
Year = {{2016}},
Volume = {{8}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{The accurate estimation of individual tree level aboveground biomass
   (AGB) is critical for understanding the carbon cycle, detecting
   potential biofuels and managing forest ecosystems. In this study, we
   assessed the capability of the metrics of point clouds, extracted from
   the full-waveform Airborne Laser Scanning (ALS) data, and of composite
   waveforms, calculated based on a voxel-based approach, for estimating
   tree level AGB individually and in combination, over a planted forest in
   the coastal region of east China. To do so, we investigated the
   importance of point cloud and waveform metrics for estimating tree-level
   AGB by all subsets models and relative weight indices. We also assessed
   the capability of the point cloud and waveform metrics based models and
   combo model (including the combination of both point cloud and waveform
   metrics) for tree-level AGB estimation and evaluated the accuracies of
   these models. The results demonstrated that most of the waveform metrics
   have relatively low correlation coefficients (<0.60) with other metrics.
   The combo models (Adjusted R-2 = 0.78-0.89), including both point cloud
   and waveform metrics, have a relatively higher performance than the
   models fitted by point cloud metrics-only (Adjusted R-2 = 0.74-0.86) and
   waveform metrics-only (Adjusted R-2 = 0.72-0.84), with the mostly
   selected metrics of the 95th percentile height (H-95), mean of height of
   median energy (HOME) and mean of the height/median ratio (HTMR). Based
   on the relative weights (i.e., the percentage of contribution for R-2)
   of the mostly selected metrics for all subsets, the metric of 95th
   percentile height (H-95) has the highest relative importance for AGB
   estimation (19.23\%), followed by 75th percentile height (H-75)
   (18.02\%) and coefficient of variation of heights (H-cv) (15.18\%) in
   the point cloud metrics based models. For the waveform metrics based
   models, the metric of mean of height of median energy (HOME) has the
   highest relative importance for AGB estimation (17.86\%), followed by
   mean of the height/median ratio (HTMR) (16.23\%) and standard deviation
   of height of median energy (HOME sigma) (14.78\%). This study
   demonstrated benefits of using full-waveform ALS data for estimating
   biomass at tree level, for sustainable forest management and mitigating
   climate change by planted forest, as China has the largest area of
   planted forest in the world, and these forests contribute to a large
   amount of carbon sequestration in terrestrial ecosystems.}},
DOI = {{10.3390/rs8090729}},
Article-Number = {{729}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000385488000039}},
}

@article{ ISI:000385150400002,
Author = {Qiu, Luwen and Zhou, Zhongwei and Guo, Jixiang and Lv, Jiancheng},
Title = {{An Automatic Registration Algorithm for 3D Maxillofacial Model}},
Journal = {{3D RESEARCH}},
Year = {{2016}},
Volume = {{7}},
Number = {{3}},
Month = {{SEP}},
Abstract = {{3D image registration aims at aligning two 3D data sets in a common
   coordinate system, which has been widely used in computer vision,
   pattern recognition and computer assisted surgery. One challenging
   problem in 3D registration is that point-wise correspondences between
   two point sets are often unknown apriori. In this work, we develop an
   automatic algorithm for 3D maxillofacial models registration including
   facial surface model and skull model. Our proposed registration
   algorithm can achieve a good alignment result between partial and whole
   maxillofacial model in spite of ambiguous matching, which has a
   potential application in the oral and maxillofacial reparative and
   reconstructive surgery. The proposed algorithm includes three steps: (1)
   3D-SIFT features extraction and FPFH descriptors construction; (2)
   feature matching using SAC-IA; (3) coarse rigid alignment and refinement
   by ICP. Experiments on facial surfaces and mandible skull models
   demonstrate the efficiency and robustness of our algorithm.}},
DOI = {{10.1007/s13319-016-0083-x}},
Article-Number = {{UNSP 20}},
ISSN = {{2092-6731}},
Unique-ID = {{ISI:000385150400002}},
}

@article{ ISI:000384777300027,
Author = {Yang, Bisheng and Huang, Ronggang and Dong, Zhen and Zang, Yufu and Li,
   Jianping},
Title = {{Two-step adaptive extraction method for ground points and breaklines
   from lidar point clouds}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{119}},
Pages = {{373-389}},
Month = {{SEP}},
Abstract = {{The extraction of ground points and breaklines is a crucial step during
   generation of high quality digital elevation models (DEMs) from airborne
   LiDAR point clouds. In this study, we propose a novel automated method
   for this task. To overcome the disadvantages of applying a single
   filtering method in areas with various types of terrain, the proposed
   method first classifies the points into a set of segments and one set of
   individual points, which are filtered by segment-based filtering and
   multi-scale morphological filtering, respectively. In the process of
   multi-scale morphological filtering, the proposed method removes
   amorphous objects from the set of individual points to decrease the
   effect of the maximum scale on the filtering result. The proposed method
   then extracts the breaklines from the ground points, which provide a
   good foundation for generation of a high quality DEM. Finally, the
   experimental results demonstrate that the proposed method extracts
   ground points in a robust manner while preserving the breaklines. (C)
   2016 International Society for Photogrammetry and Remote Sensing, Inc.
   (ISPRS). Published by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.07.002}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Yang, Bisheng/A-4642-2013}},
ORCID-Numbers = {{Yang, Bisheng/0000-0001-7736-0803}},
Unique-ID = {{ISI:000384777300027}},
}

@article{ ISI:000382679900012,
Author = {Bagchi, Parama and Bhattacharjee, Debotosh and Nasipuri, Mita},
Title = {{A robust analysis, detection and recognition of facial features in 2.5D
   images}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{75}},
Number = {{18}},
Pages = {{11059-11096}},
Month = {{SEP}},
Abstract = {{A robust technique for recognition of 3D faces which performs well with
   face images with various poses, expressions and occlusions. In this
   method, the face images represented in 3D mesh format are smoothed using
   trilinear interpolation and then converted to 2.5D image or range
   images. Nose-tip which is the most prominent feature on human face is
   detected first on the corner points selected by 3D Harris corner and
   curvedness at those corner points. K-Means clustering is applied to
   group those corner points in 2 groups. The cluster of points with larger
   curvedness values represents the possible locations of nose-tip.
   Nose-tip is finally localized using Mean-Gaussian curvature values of
   the prospective corner points in that cluster. Using the nose-tip
   location, other facial landmarks namely corners of the eyes and mouth
   are located and a facial graph is generated. The dimensionality of 2.5D
   feature space is that, depth values are stored at each (x, y) grid of
   the 2.5D image, so a 3D face image uses some function to map the depth
   value at any pixel position to the intensity with which that pixel will
   be displayed. Here finally extracted features for each subject is of
   dimensionality {[}1x21], taking into account the Euclidean distances in
   three dimensional form between each feature points detected
   automatically. Taking Euclidean distances between all pairs of landmark
   points as features, face images are classified using Multilayer
   Perceptron (MLP), as well as Support Vector Machines (SVM). Maximum
   recognition rates of 75 and 87.5 \% have been obtained in case of
   Bosphorus Databases, 62.5 and 87.5 \% in case of GavabDB databases, 75
   and 87.5 \% in case of Frav3D Databases by Multilayer Perceptron and
   Support Vector Machines respectively.}},
DOI = {{10.1007/s11042-015-2835-7}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000382679900012}},
}

@article{ ISI:000384777300010,
Author = {Mahmoudabadi, Hamid and Olsen, Michael J. and Todorovic, Sinisa},
Title = {{Efficient terrestrial laser scan segmentation exploiting data structure}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{119}},
Pages = {{135-150}},
Month = {{SEP}},
Abstract = {{New technologies such as lidar enable the rapid collection of massive
   datasets to model a 3D scene as a point cloud. However, while hardware
   technology continues to advance, processing 3D point clouds into
   informative models remains complex and time consuming. A common approach
   to increase processing efficiently is to segment the point cloud into
   smaller sections. This paper proposes a novel approach for point cloud
   segmentation using computer vision algorithms to analyze panoramic
   representations of individual laser scans. These panoramas can be
   quickly created using an inherent neighborhood structure that is
   established during the scanning process, which scans at fixed angular
   increments in a cylindrical or spherical coordinate system. In the
   proposed approach, a selected image segmentation algorithm is applied on
   several input layers exploiting this angular structure including laser
   intensity, range, normal vectors, and color information. These segments
   are then mapped back to the 3D point cloud so that modeling can be
   completed more efficiently. This approach does not depend on pre-defined
   mathematical models and consequently setting parameters for them. Unlike
   common geometrical point cloud segmentation methods, the proposed method
   employs the colorimetric and intensity data as another source of
   information. The proposed algorithm is demonstrated on several datasets
   encompassing variety of scenes and objects. Results show a very high
   perceptual (visual) level of segmentation and thereby the feasibility of
   the proposed algorithm. The proposed method is also more efficient
   compared to Random Sample Consensus (RANSAC), which is a common approach
   for point cloud segmentation. (C) 2016 International Society for
   Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.05.015}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000384777300010}},
}

@article{ ISI:000376708000002,
Author = {Alashkar, Taleb and Ben Amor, Boulbaba and Daoudi, Mohamed and Berretti,
   Stefano},
Title = {{A Grassmann framework for 4D facial shape analysis}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2016}},
Volume = {{57}},
Pages = {{21-30}},
Month = {{SEP}},
Abstract = {{In this paper, we investigate the contribution of dynamic evolution of
   3D faces to identity recognition. To this end, we adopt a subspace
   representation of the flow of curvature-maps computed on 3D facial
   frames of a sequence, after normalizing their pose. Such representation
   allows us to embody the shape as well as its temporal evolution within
   the same subspace representation. Dictionary learning and sparse coding
   over the space of fixed-dimensional subspaces, called Grassmann
   manifold, have been used to perform face recognition. We have conducted
   extensive experiments on the BU-4DFE dataset. The obtained results of
   the proposed approach provide promising results. (C) 2016 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.patcog.2016.03.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018
   Daoudi, Mohammed/H-5935-2013}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305
   Berretti, Stefano/0000-0003-1219-4386
   Daoudi, Mohammed/0000-0003-4219-7860}},
Unique-ID = {{ISI:000376708000002}},
}

@article{ ISI:000379266300013,
Author = {Quan, Wei and Matuszewski, Bogdan J. and Shark, Lik-Kwan},
Title = {{Statistical shape modelling for expression-invariant face analysis and
   recognition}},
Journal = {{PATTERN ANALYSIS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{19}},
Number = {{3}},
Pages = {{765-781}},
Month = {{AUG}},
Abstract = {{Paper introduces a 3-D shape representation scheme for automatic face
   analysis and identification, and demonstrates its invariance to facial
   expression. The core of this scheme lies on the combination of
   statistical shape modelling and non-rigid deformation matching. While
   the former matches 3-D faces with facial expression, the latter provides
   a low-dimensional feature vector that controls the deformation of model
   for matching the shape of new input, thereby enabling robust
   identification of 3-D faces. The proposed scheme is also able to handle
   the pose variation without large part of missing data. To assist the
   establishment of dense point correspondences, a modified
   free-form-deformation based on B-spline warping is applied with the help
   of extracted landmarks. The hybrid iterative closest point method is
   introduced for matching the models and new data. The feasibility and
   effectiveness of the proposed method was investigated using standard
   publicly available Gavab and BU-3DFE datasets, which contain faces with
   expression and pose changes. The performance of the system was compared
   with that of nine benchmark approaches. The experimental results
   demonstrate that the proposed scheme provides a competitive solution for
   face recognition.}},
DOI = {{10.1007/s10044-014-0439-x}},
ISSN = {{1433-7541}},
EISSN = {{1433-755X}},
Unique-ID = {{ISI:000379266300013}},
}

@article{ ISI:000378471300003,
Author = {Jiang, Hairong and Zhang, Ting and Wachs, Juan P. and Duerstock, Bradley
   S.},
Title = {{Enhanced control of a wheelchair-mounted robotic manipulator using 3-D
   vision and multimodal interaction}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2016}},
Volume = {{149}},
Pages = {{21-31}},
Month = {{AUG}},
Abstract = {{This paper presents a multiple-sensors, 3D vision-based, autonomous
   wheelchair-mounted robotic manipulator (WMRM). Two 3D sensors were
   employed: one for object recognition, and the other for recognizing body
   parts (face and hands). The goal is to recognize everyday items and
   automatically interact with them in an assistive fashion. For example,
   when a cereal box is recognized, it is grasped, poured in a bowl, and
   brought to the user. Daily objects (i.e. bowl and hat) were
   automatically detected and classified using a three-steps procedure: (1)
   remove background based on 3D information and find the point cloud of
   each object; (2) extract feature vectors for each segmented object from
   its 3D point cloud and its color image; and (3) classify feature vectors
   as objects after applying a nonlinear support vector machine (SVM). To
   retrieve specific objects, three user interface methods were adopted:
   voice-based, gesture-based, and hybrid commands. The presented system
   was tested using two common activities of daily living - feeding and
   dressing. The results revealed that an accuracy of 98.96\% is achieved
   for a dataset with twelve daily objects. The experimental results
   indicated that hybrid (gesture and speech) interaction outperforms any
   single modal interaction. (C) 2016 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.cviu.2016.03.015}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
ORCID-Numbers = {{Wachs, Juan/0000-0002-6425-5745
   Duerstock, Bradley/0000-0001-9535-2460}},
Unique-ID = {{ISI:000378471300003}},
}

@article{ ISI:000381843600009,
Author = {Ekizoglu, Oguzhan and Hocaoglu, Elif and Inci, Ercan and Can, Ismail
   Ozgur and Solmaz, Dilek and Aksoy, Sema and Buran, Cudi Ferat and Sayin,
   Ibrahim},
Title = {{Assessment of sex in a modern Turkish population using cranial
   anthropometric parameters}},
Journal = {{LEGAL MEDICINE}},
Year = {{2016}},
Volume = {{21}},
Pages = {{45-52}},
Month = {{JUL}},
Abstract = {{The utilization of radiological imaging methods in anthropometric
   studies is being expanded by the application of modern imaging methods,
   leading to a decrease in costs, a decrease in the time required for
   analysis and the ability to create three-dimensional images. This
   retrospective study investigated 400 patients within the 18-45-years age
   group (mean age: 30.7 +/- 11.2 years) using cranial computed tomography
   images. We measured 14 anthropometric parameters (basion-bregma height,
   basion-prosthion length, maximum cranial length and cranial base
   lengths, maximum cranial breadth, bizygomatic diameter, upper facial
   breadth, bimastoid diameter, orbital breadth, orbital length, biorbital
   breadth, interorbital breadth, foramen magnum breadth and foramen magnum
   length) of cranial measurements. The intra- and inter-observer
   repeatability and consistency were good. From the results of logistic
   regression analysis using morphometric measurements, the most
   conspicuous measurements in terms of dimorphism were maximum cranial
   length, bizygomatic diameter, basion-bregma height, and cranial base
   length. The most dimorphic structure was the bizygomatic diameter with
   an accuracy rate of 83\% in females and 77\% in males. In this study,
   87.5\% of females and 87.0\% of males were classified accurately by this
   model including four parameters with a sensitivity of 91.5\% and
   specificity of 85.0\%. In conclusion, CT cranial morphometric analysis
   may be reliable for the assessment of sex in the Turkish population and
   is recommended for comparison of data of modern populations with those
   of former populations. Additionally, cranial morphometric data that we
   obtained from modern Turkish population may reveal population specific
   data, which may help current criminal investigations and identification
   of disaster victims. (C) 2016 Elsevier Ireland Ltd. All rights reserved.}},
DOI = {{10.1016/j.legalmed.2016.06.001}},
ISSN = {{1344-6223}},
ORCID-Numbers = {{Ekizoglu, Oguzhan/0000-0002-0194-595X
   Buran, Ferat/0000-0002-7858-0194}},
Unique-ID = {{ISI:000381843600009}},
}

@article{ ISI:000380771500016,
Author = {Tanhuanpaa, Topi and Saarinen, Ninni and Kankare, Ville and Nurminen,
   Kimmo and Vastaranta, Mikko and Honkavaara, Eija and Karjalainen, Mika
   and Yu, Xiaowei and Holopainen, Markus and Hyyppa, Juha},
Title = {{Evaluating the Performance of High-Altitude Aerial Image-Based Digital
   Surface Models in Detecting Individual Tree Crowns in Mature Boreal
   Forests}},
Journal = {{FORESTS}},
Year = {{2016}},
Volume = {{7}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{Height models based on high-altitude aerial images provide a low-cost
   means of generating detailed 3D models of the forest canopy. In this
   study, the performance of these height models in the detection of
   individual trees was evaluated in a commercially managed boreal forest.
   Airborne digital stereo imagery (DSI) was captured from a flight
   altitude of 5 km with a ground sample distance of 50 cm and corresponds
   to regular national topographic airborne data capture programs operated
   in many countries. Tree tops were detected from smoothed canopy height
   models (CHM) using watershed segmentation. The relative amount of
   detected trees varied between 26\% and 140\%, and the RMSE of plot-level
   arithmetic mean height between 2.2 m and 3.1 m. Both the dominant tree
   species and the filter used for smoothing affected the results. Even
   though the spatial resolution of DSI-based CHM was sufficient, detecting
   individual trees from the data proved to be demanding because of the
   shading effect of the dominant trees and the limited amount of data from
   lower canopy levels and near the ground.}},
DOI = {{10.3390/f7070143}},
Article-Number = {{143}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Saarinen, Ninni/K-4296-2019
   Vastaranta, Mikko/K-9656-2018
   Karjalainen, Mika/E-3348-2017
   }},
ORCID-Numbers = {{Saarinen, Ninni/0000-0003-2730-8892
   Vastaranta, Mikko/0000-0001-6552-9122
   Karjalainen, Mika/0000-0003-4320-8007
   Tanhuanpaa, Topi/0000-0002-5509-6922
   Honkavaara, Eija/0000-0002-7236-2145
   Nurminen, Kimmo/0000-0001-8036-9446}},
Unique-ID = {{ISI:000380771500016}},
}

@article{ ISI:000379014500010,
Author = {Sutradhar, Alok and Park, Jaejong and Carrau, Diana and Nguyen, Tam H.
   and Miller, Michael J. and Paulino, Glaucio H.},
Title = {{Designing patient-specific 3D printed craniofacial implants using a
   novel topology optimization method}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{2016}},
Volume = {{54}},
Number = {{7}},
Pages = {{1123-1135}},
Month = {{JUL}},
Abstract = {{Large craniofacial defects require efficient bone replacements which
   should not only provide good aesthetics but also possess stable
   structural function. The proposed work uses a novel multiresolution
   topology optimization method to achieve the task. Using a compliance
   minimization objective, patient-specific bone replacement shapes can be
   designed for different clinical cases that ensure revival of efficient
   load transfer mechanisms in the mid-face. In this work, four clinical
   cases are introduced and their respective patient-specific designs are
   obtained using the proposed method. The optimized designs are then
   virtually inserted into the defect to visually inspect the viability of
   the design . Further, once the design is verified by the reconstructive
   surgeon, prototypes are fabricated using a 3D printer for validation.
   The robustness of the designs are mechanically tested by subjecting them
   to a physiological loading condition which mimics the masticatory
   activity. The full-field strain result through 3D image correlation and
   the finite element analysis implies that the solution can survive the
   maximum mastication of 120 lb. Also, the designs have the potential to
   restore the buttress system and provide the structural integrity. Using
   the topology optimization framework in designing the bone replacement
   shapes would deliver surgeons new alternatives for rather complicated
   mid-face reconstruction.}},
DOI = {{10.1007/s11517-015-1418-0}},
ISSN = {{0140-0118}},
EISSN = {{1741-0444}},
Unique-ID = {{ISI:000379014500010}},
}

@article{ ISI:000373271800001,
Author = {Hojris, Bo and Christensen, Sarah Christine Boesgaard and Albrechtsen,
   Hans-Jorgen and Smith, Christian and Dahlqvist, Mathis},
Title = {{A novel, optical, on-line bacteria sensor for monitoring drinking water
   quality}},
Journal = {{SCIENTIFIC REPORTS}},
Year = {{2016}},
Volume = {{6}},
Month = {{APR 4}},
Abstract = {{Today, microbial drinking water quality is monitored through either
   time-consuming laboratory methods or indirect on-line measurements.
   Results are thus either delayed or insufficient to support proactive
   action. A novel, optical, on-line bacteria sensor with a 10-minute time
   resolution has been developed. The sensor is based on 3D image
   recognition, and the obtained pictures are analyzed with algorithms
   considering 59 quantified image parameters. The sensor counts individual
   suspended particles and classifies them as either bacteria or abiotic
   particles. The technology is capable of distinguishing and quantifying
   bacteria and particles in pure and mixed suspensions, and the
   quantification correlates with total bacterial counts. Several field
   applications have demonstrated that the technology can monitor changes
   in the concentration of bacteria, and is thus well suited for rapid
   detection of critical conditions such as pollution events in drinking
   water.}},
DOI = {{10.1038/srep23935}},
Article-Number = {{23935}},
ISSN = {{2045-2322}},
ResearcherID-Numbers = {{Hojris, Bo/H-1350-2018
   Albrechtsen, Hans-Jorgen/J-1229-2014}},
ORCID-Numbers = {{Hojris, Bo/0000-0003-4129-2794
   Christensen, Sarah Christine Boesgaard/0000-0001-6183-6045
   Albrechtsen, Hans-Jorgen/0000-0003-3483-7709}},
Unique-ID = {{ISI:000373271800001}},
}

@article{ ISI:000422927300005,
Author = {Ahranjani, Behnaz Asadi and Shojaei, Bahador and Tootian, Zahra and
   Masoudifard, Madjid and Rostami, Amir},
Title = {{Anatomical, radiographical and computed tomographic study of the limbs
   skeleton of the Euphrates soft shell turtle (Rafetus euphraticus)}},
Journal = {{VETERINARY RESEARCH FORUM}},
Year = {{2016}},
Volume = {{7}},
Number = {{2}},
Pages = {{117-124}},
Month = {{SPR}},
Abstract = {{Euphrates turtle is the only soft shell turtle of Iran, and
   unfortunately is in danger of extinction due to multiple reasons.
   Imaging techniques, in addition to their importance in diagnosis of
   injuries to animals, have been used as non-invasive methods to provide
   normal anatomic views. A few studies have been conducted to understand
   body structure of the Euphrates turtle. Since there is only general
   information about the anatomy of turtle limbs, the normal skeleton of
   the Euphrates limbs was studied. For this purpose four adult Euphrates
   turtles were used. Digital radiographic examination was performed by
   computed radiographic (CR) in dorsoventral (DV) and lateral (L)
   positions. Spiral CT-scanning was done and 3D images of the bones were
   reconstructed for anatomical evaluation. For skeletal preparation, the
   skeleton was cleaned by a combination of boiling and mealworm methods
   and limbs' bones were examined anatomically. In the present study,
   simultaneous anatomic, radiographic and CT studies of bones in
   individual turtles made us possible to describe bones anatomically and
   provided comparable and complementary conditions to represent the
   abilities of the radiography and CT for better understanding of the
   anatomy. Arrangement and the number of carpal and tarsal bones are used
   in turtles' classification. Among the studied species, Euphrates turtle
   carpal and tarsal bones show the most similarities to the Apolone
   spinifera. (c) 2016 Urmia University. All rights reserved.}},
ISSN = {{2008-8140}},
EISSN = {{2322-3618}},
Unique-ID = {{ISI:000422927300005}},
}

@article{ ISI:000368511500001,
Author = {Mirshojaei, Seyedeh Fatemeh and Ahmadi, Amirhossein and Morales-Avila,
   Enrique and Ortiz-Reynoso, Mariana and Reyes-Perez, Horacio},
Title = {{Radiolabelled nanoparticles: novel classification of
   radiopharmaceuticals for molecular imaging of cancer}},
Journal = {{JOURNAL OF DRUG TARGETING}},
Year = {{2016}},
Volume = {{24}},
Number = {{2}},
Pages = {{91-101}},
Month = {{FEB 7}},
Abstract = {{Nanotechnology has been used for every single modality in the molecular
   imaging arena for imaging purposes. Synergic advantages can be explored
   when multiple molecular imaging modalities are combined with respect to
   single imaging modalities. Multifunctional nanoparticles have large
   surface areas, where multiple functional moieties can be incorporated,
   including ligands for site-specific targeting and radionuclides, which
   can be detected to create 3D images. Recently, radiolabeled
   nanoparticles with individual properties have attracted great interest
   regarding their use in multimodality tumor imaging. Multifunctional
   nanoparticles can combine diagnostic and therapeutic capabilities for
   both target-specific diagnosis and the treatment of a given disease. The
   future of nanomedicine lies in multifunctional nanoplatforms that
   combine the diagnostic ability and therapeutic effects using appropriate
   ligands, drugs, responses and technological devices, which together are
   collectively called theranostic drugs. Co-delivery of radiolabeled
   nanoparticles is useful in multifunctional molecular imaging areas
   because it comprises several advantages based on nanoparticles
   architecture, pharmacokinetics and pharmacodynamic properties.}},
DOI = {{10.3109/1061186X.2015.1048516}},
ISSN = {{1061-186X}},
EISSN = {{1029-2330}},
ResearcherID-Numbers = {{Ahmadi, Amirhossein/H-2136-2011
   }},
ORCID-Numbers = {{Ahmadi, Amirhossein/0000-0002-9737-3633
   Reyes-Perez, Horacio/0000-0001-9018-1105}},
Unique-ID = {{ISI:000368511500001}},
}

@article{ ISI:000383905800007,
Author = {de Jong, Markus A. and Wollstein, Andreas and Ruff, Clifford and
   Dunaway, David and Hysi, Pirro and Spector, Tim and Liu, Fan and
   Niessen, Wiro and Koudstaal, Maarten J. and Kayser, Manfred and Wolvius,
   Eppo B. and Bohringer, Stefan},
Title = {{An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}},
Journal = {{IEEE TRANSACTIONS ON IMAGE PROCESSING}},
Year = {{2016}},
Volume = {{25}},
Number = {{2}},
Pages = {{580-588}},
Month = {{FEB}},
Abstract = {{In this paper, we present a novel approach to automatic 3D facial
   landmarking using 2D Gabor wavelets. Our algorithm considers the face to
   be a surface and uses map projections to derive 2D features from raw
   data. Extracted features include texture, relief map, and
   transformations thereof. We extend an established 2D landmarking method
   for simultaneous evaluation of these data. The method is validated by
   performing landmarking experiments on two data sets using 21 landmarks
   and compared with an active shape model implementation. On average,
   landmarking error for our method was 1.9 mm, whereas the active shape
   model resulted in an average landmarking error of 2.3 mm. A second study
   investigating facial shape heritability in related individuals concludes
   that automatic landmarking is on par with manual landmarking for some
   landmarks. Our algorithm can be trained in 30 min to automatically
   landmark 3D facial data sets of any size, and allows for fast and robust
   landmarking of 3D faces.}},
DOI = {{10.1109/TIP.2015.2496183}},
ISSN = {{1057-7149}},
EISSN = {{1941-0042}},
ResearcherID-Numbers = {{Boehringer, Stefan/Y-2442-2018
   Liu, Fan/B-8833-2013
   }},
ORCID-Numbers = {{Boehringer, Stefan/0000-0001-9108-9212
   Liu, Fan/0000-0001-9241-8161
   Niessen, Wiro/0000-0002-5822-1995
   Dunaway, David/0000-0001-5063-9943}},
Unique-ID = {{ISI:000383905800007}},
}

@article{ ISI:000371787800087,
Author = {Alletto, Stefano and Abati, Davide and Serra, Giuseppe and Cucchiara,
   Rita},
Title = {{Exploring Architectural Details Through a Wearable Egocentric Vision
   Device}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Augmented user experiences in the cultural heritage domain are in
   increasing demand by the new digital native tourists of 21st century. In
   this paper, we propose a novel solution that aims at assisting the
   visitor during an outdoor tour of a cultural site using the unique first
   person perspective of wearable cameras. In particular, the approach
   exploits computer vision techniques to retrieve the details by proposing
   a robust descriptor based on the covariance of local features. Using a
   lightweight wearable board, the solution can localize the user with
   respect to the 3D point cloud of the historical landmark and provide him
   with information about the details at which he is currently looking.
   Experimental results validate the method both in terms of accuracy and
   computational effort. Furthermore, user evaluation based on real-world
   experiments shows that the proposal is deemed effective in enriching a
   cultural experience.}},
DOI = {{10.3390/s16020237}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000371787800087}},
}

@article{ ISI:000370350100005,
Author = {Ma, Lixia and Zheng, Guang and Eitel, Jan U. H. and Moskal, L. Monika
   and He, Wei and Huang, Huabing},
Title = {{Improved Salient Feature-Based Approach for Automatically Separating
   Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar
   Point Cloud Data of Forest Canopies}},
Journal = {{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{54}},
Number = {{2}},
Pages = {{679-696}},
Month = {{FEB}},
Abstract = {{Accurate separation of photosynthetic and nonphotosynthetic components
   in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a
   challenging but of key importance to understand the spatial distribution
   of the radiation regime, photosynthetic processes, and carbon and water
   exchanges of the forest canopy. The objective of this paper was to
   improve current methods for separating photosynthetic and
   nonphotosynthetic components in TLS data of forest canopies by adding
   two additional filters only based on its geometric information. By
   comparing the proposed approach with the eigenvalues plus color
   information-based method, we found that the proposed approach could
   effectively improve the overall producer's accuracy from 62.12\% to
   95.45\%, and the overall classification producer's accuracy would
   increase from 84.28\% to 97.80\% as the forest leaf area index (LAI)
   decreases from 4.15 to 3.13. In addition, variations in tree species had
   negligible effects on the final classification accuracy, as shown by the
   overall producer's accuracy for coniferous (93.09\%) and broadleaf
   (94.96\%) trees. To remove quantitatively the effects of the woody
   materials in a forest canopy for improving TLS-based LAI estimates, we
   also computed the ``woody-to-total area ratio{''} based on the
   classified linear class points from an individual tree. Automatic
   classification of the forest point cloud data set will facilitate the
   application of TLS on retrieving 3-D forest canopy structural
   parameters, including LAI and leaf and woody area ratios.}},
DOI = {{10.1109/TGRS.2015.2459716}},
ISSN = {{0196-2892}},
EISSN = {{1558-0644}},
ResearcherID-Numbers = {{Moskal, L. Monika/F-8715-2010
   }},
ORCID-Numbers = {{Moskal, L. Monika/0000-0003-1563-6506
   He, Wei/0000-0003-0779-2496}},
Unique-ID = {{ISI:000370350100005}},
}

@article{ ISI:000369200900006,
Author = {Shendryk, Iurii and Broich, Mark and Tulbure, Mirela G. and Alexandrov,
   Sergey V.},
Title = {{Bottom-up delineation of individual trees from full-waveform airborne
   laser scans in a structurally complex eucalypt forest}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2016}},
Volume = {{173}},
Pages = {{69-83}},
Month = {{FEB}},
Note = {{2014 ForestSAT Conference, Riva del Garda, ITALY, NOV 04-07, 2014}},
Abstract = {{Full-waveform airborne laser scanning (ALS) is a powerful tool for
   characterizing and monitoring forest structure over large areas at the
   individual tree level. Most of the existing ALS-based algorithms for
   individual tree delineation from the point cloud are top-down, which are
   accurate for delineating cone-shaped conifers, but have lower
   delineation accuracies over more structurally complex broad-leaf
   forests. Therefore, in this study we developed a new bottom-up algorithm
   for detecting trunks and delineating individual trees with complex
   shapes, such as eucalypts. Experiments were conducted in the largest
   river red gum forest in the world, located in the southeast of
   Australia, that experienced severe dieback over the past six decades.
   For detection of individual tree trunks, we used a novel approach based
   on conditional Euclidean distance clustering that takes advantage of
   spacing between laser returns. Overall, the algorithm developed in our
   study was able to detect up to 67\% of field-measured trees with
   diameter larger than or equal to 13 cm. By filtering ALS based on the
   intensity, return number and returned pulse width values, we were able
   to differentiate between woody and leaf tree components, thus improving
   the accuracy of tree trunk detections by 5\% as compared to non-filtered
   ALS. The detected trunks were used to seed random walks on graph
   algorithm for tree crown delineation. The accuracy of tree crown
   delineation for different ALS point cloud densities was assessed in
   terms of tree height and crown width and resulted in up to 68\% of
   field-measured trees being correctly delineated. The double increase in
   point density from similar to 12 points/m(2) to similar to 24
   points/m(2) resulted in tree trunk detection increase of 11\% (from 56\%
   to 67\%) and percentage of correctly delineated crowns increase of 13\%
   (from 55\% to 68\%). Our results confirm an algorithm that can be used
   to accurately delineate individual trees with complex structures (e.g.
   eucalypts and other broad leaves) and highlight the importance of
   full-waveform ALS for individual tree delineation. (C) 2015 Elsevier
   Inc. All rights reserved.}},
DOI = {{10.1016/j.rse.2015.11.008}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Tulbure, Mirela/B-3030-2012
   Tulbure, Mirela G/M-1212-2019
   }},
ORCID-Numbers = {{Tulbure, Mirela/0000-0003-1456-183X
   Tulbure, Mirela G/0000-0003-1456-183X
   Shendryk, Iurii/0000-0003-1657-1361}},
Unique-ID = {{ISI:000369200900006}},
}

@article{ ISI:000368956300004,
Author = {Sener, Emre and Mumcuoglu, Erkan U. and Hamcan, Salih},
Title = {{Bayesian segmentation of human facial tissue using 3D MR-CT information
   fusion, resolution enhancement and partial volume modelling}},
Journal = {{COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE}},
Year = {{2016}},
Volume = {{124}},
Pages = {{31-44}},
Month = {{FEB}},
Abstract = {{Background: Accurate segmentation of human head on medical images is an
   important process in a wide array of applications such as diagnosis,
   facial surgery planning, prosthesis design, and forensic identification.
   Objectives: In this study, a Bayesian method for segmentation of facial
   tissues is presented. Segmentation classes include muscle, bone, fat,
   air and skin.
   Methods: The method presented incorporates information fusion from
   multiple modalities, modelling of image resolution (measurement
   blurring), image noise, two priors helping to reduce noise and partial
   volume. Image resolution modelling employed facilitates resolution
   enhancement and superresolution capabilities during image segmentation.
   Regularization based on isotropic and directional Markov Random Field
   priors is integrated. The Bayesian model is solved iteratively yielding
   tissue class labels at every voxel of the image. Sub methods as
   variations of the main method are generated by using a combination of
   the models.
   Results: Testing of the sub-methods is performed on two patients using
   single modality three-dimensional (3D) image (magnetic resonance, MR or
   computerized tomography, CT) as well as registered MR-CT images with
   information fusion. Numerical, visual and statistical analyses of the
   methods are conducted. High segmentation accuracy values are obtained by
   the use of image resolution and partial volume models as well as
   information fusion from MR and CT images. The methods are also compared
   with our Bayesian segmentation method proposed in a previous study. The
   performance is found to be similar to our previous Bayesian approach,
   but the presented methods here eliminates ad hoc parameter tuning needed
   by the previous approach which is system and data acquisition setting
   dependent.
   Conclusions: The Bayesian approach presented provides resolution
   enhanced segmentation of very thin structures of the human head.
   Meanwhile, free parameters of the algorithm can be adjusted for
   different imaging systems and data acquisition settings in a more
   systematic way as compared with our previous study. (C) 2015 Elsevier
   Ireland Ltd. All rights reserved.}},
DOI = {{10.1016/j.cmpb.2015.10.009}},
ISSN = {{0169-2607}},
EISSN = {{1872-7565}},
ResearcherID-Numbers = {{Mumcuoglu, Erkan/B-5480-2012}},
Unique-ID = {{ISI:000368956300004}},
}

@article{ ISI:000367113200011,
Author = {Almansa, Julio and Salvat-Pujol, Francesc and Diaz-Londono, Gloria and
   Carnicer, Artur and Lallena, Antonio M. and Salvat, Francesc},
Title = {{PENGEOM-A general-purpose geometry package for Monte Carlo simulation of
   radiation transport in material systems defined by quadric surfaces}},
Journal = {{COMPUTER PHYSICS COMMUNICATIONS}},
Year = {{2016}},
Volume = {{199}},
Pages = {{102-113}},
Month = {{FEB}},
Abstract = {{The Fortran subroutine package PENGEOM provides a complete set of tools
   to handle quadric geometries in Monte Carlo simulations of radiation
   transport. The material structure where radiation propagates is assumed
   to consist of homogeneous bodies limited by quadric surfaces. The
   PENGEOM subroutines (a subset of the PENELOPE code) track particles
   through the material structure, independently of the details of the
   physics models adopted to describe the interactions. Although these
   subroutines are designed for detailed simulations of photon and electron
   transport, where all individual interactions are simulated sequentially,
   they can also be used in mixed (class II) schemes for simulating the
   transport of high-energy charged particles, where the effect of soft
   interactions is described by the random-hinge method. The definition of
   the geometry and the details of the tracking algorithm are tailored to
   optimize simulation speed. The use of fuzzy quadric surfaces minimizes
   the impact of round-off errors. The provided software includes a Java
   graphical user interface for editing and debugging the geometry
   definition file and for visualizing the material structure. Images of
   the structure are generated by using the tracking subroutines and,
   hence, they describe the geometry actually passed to the simulation
   code.
   Program summary
   Program title: Pengeom
   Catalogue identifier: AEYH\_v1\_0
   Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYH\_v1\_0.html
   Program obtainable from: CPC Program Library, Queen's University,
   Belfast, N. Ireland
   Licensing provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.html
   No. of lines in distributed program, including test data, etc.: 89390
   No. of bytes in distributed program, including test data, etc.: 5062646
   Distribution format: tar.gz
   Programming language: Fortran, Java.
   Computer: PC with Java Runtime Environment installed.
   Operating system: Windows, Linux.
   RAM: 210 MiB
   Classification: 21.1, 14.
   Nature of problem: The Fortran subroutines perform all geometry
   operations in Monte Carlo simulations of radiation transport with
   arbitrary interaction models. They track particles through material
   systems consisting of homogeneous bodies limited by quadric surfaces.
   Particles are moved in steps (free flights) of a given length, which is
   dictated by the simulation program, and are halted when they cross an
   interface between media of different compositions or when they enter
   selected bodies.
   Solution method: The pengeom subroutines are tailored to optimize
   simulation speed and accuracy. Fast tracking is accomplished by the use
   of quadric surfaces, which facilitate the calculation of ray
   intersections, and of modules (connected volumes limited by quadric
   surfaces) organized in a hierarchical structure. Optimal accuracy is
   obtained by considering fuzzy surfaces, with the aid of a simple
   algorithm that keeps control of multiple intersections of a ray and a
   surface. The Java GUI PenGeomJar provides a geometry toolbox; it allows
   building and debugging the geometry definition file, as well as
   visualizing the resulting geometry in two and three dimensions.
   Restrictions: By default pengeom can handle systems with up to 5000
   bodies and 10,000 surfaces. These numbers can be increased by editing
   the Fortran source file.
   Unusual features: All geometrical operations are performed internally.
   The connection between the steering main program and the tracking
   routines is through a Fortran module, which contains the state variables
   of the transported particle, and the input-output arguments of the
   subroutine step. Rendering of two- and three-dimensional images is
   performed by using the pengeom subroutines, so that displayed images
   correspond to the definitions passed to the simulation program.
   Additional comments: Java editor and viewer (PenGeomJar), geometry
   examples, translator to POV-Ray (TM) format, detailed manual. The
   Fortran subroutine package pengeom is part of the penelope code system
   {[}1].
   Running time: The running time much depends on the complexity of the
   material system. The most complicated example provided, phantom, an
   anthropomorphic phantom, has 264 surfaces and 169 bodies and modules,
   with different levels of grouping; the largest module contains 51
   daughters. The rendering of a 3D image of phantom with 1680x1050 pixels
   takes about 25 s (i.e., about 1.5 . 10(-5) seconds per ray) on an Intel
   Core 17-3520M CPU, with Windows 7 and subroutines compiled with
   gfortran.
   References:
   {[}1] F. Salvat, PENELOPE-2014: A Code System for Monte Carlo Simulation
   of Electron and Photon Transport, OECD/NEA Data Bank,
   Issy-les-Moulineaux, France, 2015. Available from
   http://www.nea.fr/lists/penelope.html.(C) 2015 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.cpc.2015.09.019}},
ISSN = {{0010-4655}},
EISSN = {{1879-2944}},
ResearcherID-Numbers = {{Carnicer, Artur/B-1442-2013
   Diaz-Londono, Gloria/W-2639-2018
   Salvat, Francesc/F-8255-2016
   }},
ORCID-Numbers = {{Carnicer, Artur/0000-0002-4936-5778
   Diaz-Londono, Gloria/0000-0002-3235-1193
   Salvat, Francesc/0000-0002-6162-8841
   Lallena Rojo, Antonio M./0000-0003-1962-6217}},
Unique-ID = {{ISI:000367113200011}},
}

@inproceedings{ ISI:000406771300100,
Author = {Pang, Guan and Neumann, Ulrich},
Book-Group-Author = {{IEEE}},
Title = {{3D Point Cloud Object Detection with Multi-View Convolutional Neural
   Network}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{585-590}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Efficient detection of three dimensional (3D) objects in point clouds is
   a challenging problem. Performing 3D descriptor matching or 3D
   scanning-window search with detector are both time-consuming due to the
   3-dimensional complexity. One solution is to project 3D point cloud into
   2D images and thus transform the 3D detection problem into 2D space, but
   projection at multiple viewpoints and rotations produce a large amount
   of 2D detection tasks, which limit the performance and complexity of the
   2D detection algorithm choice. We propose to use convolutional neural
   network (CNN) for the 2D detection task, because it can handle all
   viewpoints and rotations for the same class of object together, as well
   as predicting multiple classes of objects with the same network, without
   the need for individual detector for each object class. We further
   improve the detection efficiency by concatenating two extra levels of
   early rejection networks with binary outputs before the multi-class
   detection network. Experiments show that our method has competitive
   overall performance with at least one-order of magnitude speedup
   comparing with latest 3D point cloud detection methods.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771300100}},
}

@inproceedings{ ISI:000406771301004,
Author = {Tian, Guifen and Mori, Tatusya and Okuda, Yuji},
Book-Group-Author = {{IEEE}},
Title = {{Spoofing Detection for Embedded Face Recognition System using A Low Cost
   Stereo Camera}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{1017-1022}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Spoofing detection is essential for practical face recognition system.
   Based on the fact that genuine face has special geometric curvatures
   across surface, this paper brings forward an ultra-fast yet accurate
   spoofing detection approach using a low-cost stereo camera. To obtain
   curvatures, the three dimensional shapes of selected facial landmarks
   are analyzed, by fitting point cloud around each landmark to a specific
   partial face surface. Spoofing detection is then performed by evaluating
   curvatures of each landmark and integrating them together. Experiments
   verify that the approach is able to detect spoofed faces in printed
   photographs without or with various bending at FAR equal to 0.00\%.
   Meanwhile, genuine faces have a trivial opportunity to be falsely
   rejected: FRR is 0.59\% for near frontal faces and less than 5\% for
   faces with large varying poses. Detection time is 51 milliseconds when
   executed on a single processor {[}1] running at a clock frequency of
   266M Hz, this makes the detection very suitable for embedded face
   recognition system.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771301004}},
}

@inproceedings{ ISI:000406771301073,
Author = {Jhuang, Dong-Han and Lin, Daw-Tung and Tsai, Chi-Hung},
Book-Group-Author = {{IEEE}},
Title = {{Face Verification with Three-Dimensional Point Cloud by Using Deep
   Belief Networks}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{1430-1435}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Developing reliable and robust face verification systems has been a
   tough challenge in computer vision, for several decades. The variation
   in illumination and head pose may seriously inhibit the accuracy of
   two-dimensional face recognition. With the invention of a depth map
   sensor, more three-dimensional volume data can be processed to mitigate
   the problem associated with face verification. This paper presents a
   three-dimensional face verification approach that includes three phases.
   First, point cloud library is applied to estimate features such as
   normal vectors and principal curvatures of every point on a human face
   point cloud acquired from three-dimensional depth sensor. Next, we adopt
   deep belief networks to train the identification model using extracted
   features. Finally, face verification is accomplished by using the
   pre-trained deep belief networks to justify if new incoming face point
   cloud feature is the one we specified. The experimental results
   demonstrate that the proposed system performs exceptionally well with
   about 96.43\% verification accuracy.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771301073}},
}

@inproceedings{ ISI:000406771302059,
Author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni,
   Gerard},
Book-Group-Author = {{IEEE}},
Title = {{Expression Invariant 3D Face Modeling from an RGB-D Video}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{2362-2367}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{We aim to reconstruct an accurate neutral 3D face model from an RGB-D
   video in the presence of extreme expression changes. Since each depth
   frame, taken by a low-cost sensor, is noisy, point clouds from multiple
   frames can be registered and aggregated to build an accurate 3D model.
   However, direct aggregation of multiple data produces erroneous results
   in natural interaction (e.g., talking and showing expressions). We
   propose to analyze facial expression from an RGB frame and neutralize
   the corresponding 3D point cloud if needed. We first estimate the
   person's expression by fitting blend-shape coefficients using 2D facial
   landmarks for each frame and calculate an expression deformity
   (expression score). With the estimated expression score, we determine
   whether an input face is neutral or non-neutral. If the face is
   non-neutral, we proceed to neutralize the expression of the 3D point
   cloud in that frame. To neutralize the 3D point cloud of a face, we
   deform our generic 3D face model by applying the estimated blendshape
   coefficients, find displacement vectors from the deformed generic face
   to a neutral generic face, and apply the displacement vectors to the
   input 3D point cloud. After preprocessing frames in a video, we rank
   frames based on the expression scores and register the ranked frames
   into a single 3D model. Our system produces a neutral 3D face model in
   the presence of extreme expression changes even when neutral faces do
   not exist in the video.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771302059}},
}

@inproceedings{ ISI:000405706400151,
Author = {Seo, Masataka and Chen, Yen-Wei},
Editor = {{Wang, Y and An, J and Wang, L and Li, Q and Yan, G and Chang, Q}},
Title = {{Joint Subspace Learning for Reconstruction of 3D Facial Dynamic
   Expression from Single Image}},
Booktitle = {{2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016)}},
Year = {{2016}},
Pages = {{820-824}},
Note = {{9th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI), Datong, PEOPLES R CHINA, OCT
   15-17, 2016}},
Organization = {{IEEE; IEEE Engn Med \& Biol Soc; Taiyuan Univ Technol; E China Normal
   Univ}},
Abstract = {{Recently, the synthesis of 3D dynamic expressions has become an
   important concern in computer graphics, facial recognition, etc. In this
   study, we propose a regression based joint subspace learning method for
   the automatic synthesis of 3D dynamic expression images. This method
   synthesizes 3D dynamic expression images from a single 2D facial image.
   We use two subspaces (the view subspace and the frame subspace) to
   synthesize a 3D image. First, we use the view subspace to estimate
   multi-view facial images from a front image. Next, we construct a 3D
   image using the estimated multi-view facial images. Finally, we estimate
   the 3D images in different frames by using the frame subspace to
   synthesis 3D dynamic expression images. This approach is unlike the
   conventional joint subspace learning in which, the coefficients
   estimated by the input image are directly used for synthesis.
   Furthermore, we propose using textural information to improve the
   accuracy of synthesized images.}},
ISBN = {{978-1-5090-3710-0}},
Unique-ID = {{ISI:000405706400151}},
}

@inproceedings{ ISI:000405940800053,
Author = {Chellappa, Rama and Chen, Jun-Cheng and Ranjan, Rajeev and
   Sankaranarayanan, Swami and Kumar, Amit and Patel, Vishal M. and
   Castillo, Carlos D.},
Book-Group-Author = {{IEEE}},
Title = {{Towards the Design of an End-to-End Automated System for Image an
   Video-based Recognition}},
Booktitle = {{2016 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA)}},
Year = {{2016}},
Note = {{Information Theory and Applications Workshop, La Jolla, CA, JAN 31-FEB
   05, 2016}},
Abstract = {{Over many decades, researchers working in object recognition have longed
   for an end-to-end automated system that will simply accept 2D or 3D
   image or videos as inputs and output the labels of objects in the input
   data. Computer vision methods that use representations derived based on
   geometric, radiometric and neural considerations and statistical and
   structural matchers and artificial neural network-based methods where a
   multi-layer network learns the mapping from inputs to class labels have
   provided competing approaches for image recognition problems. Over the
   last four years, methods based on Deep Convolutional Neural Networks
   (DCNNs) have shown impressive performance improvements on object
   detection/recognition challenge problems. This has been made possible
   due to the availability of large annotated data, a better understanding
   of the non-linear mapping between image and class labels as well as the
   affordability of GPUs. In this paper, we present a brief history of
   developments in computer vision and artificial neural networks over the
   last forty years for the problem of image-based recognition. We then
   present the design details of a deep learning system for endto- end
   unconstrained face verification/ recognition. Some open issues regarding
   DCNNs for object recognition problems are then discussed. We caution the
   readers that the views expressed in this paper are from the authors and
   authors only!}},
ISBN = {{978-1-5090-2529-9}},
Unique-ID = {{ISI:000405940800053}},
}

@inproceedings{ ISI:000405512400082,
Author = {Wu, Zhuoran and Hou, Zhenjie and Zhang, Jian},
Editor = {{Xu, B}},
Title = {{Research on the 3D face recognition based on multi-class classifier with
   depth and point cloud data}},
Booktitle = {{PROCEEDINGS OF 2016 IEEE ADVANCED INFORMATION MANAGEMENT, COMMUNICATES,
   ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IMCEC 2016)}},
Year = {{2016}},
Pages = {{398-402}},
Note = {{IEEE Advanced Information Management, Communicates, Electronic and
   Automation Control Conference (IMCEC), Xian, PEOPLES R CHINA, OCT 03-05,
   2016}},
Organization = {{IEEE; IEEE Beijing Sect; Global Union Acad Sci \& Technol; Chongqing
   Global Union Acad Sci \& Technol; Xian Peihua Univ}},
Abstract = {{Human face recognition technology usually takes advantages of
   two-dimensional or three-dimensional data. Rising from 1980s,
   three-dimensional face recognition technology soon become one of the
   headed topic because of its admirable resistance to interference and
   more information compared with two-dimensional face recognition
   technology. The new 3D face model standardization algorithm presented in
   this article provides a solution to transfer the obtained face model to
   standardized CAND1DE-3 face model. The article also provides a new
   Bayesian classification model based on multi-class classifier, which
   could overcome the difficulty that ono-verse-one classifier has a low
   recognition rate when facing more than two people. The article conduct
   the comparison experiment based on the provided algorithm. According to
   the experiment, it could raise the face recognition rate efficiently
   when applying the standardization algorithm and training modeL}},
ISBN = {{978-1-4673-9613-4}},
Unique-ID = {{ISI:000405512400082}},
}

@inproceedings{ ISI:000401716400003,
Author = {Naveen, S. and Ahalya, R. K. and Moni, R. S.},
Book-Group-Author = {{IEEE}},
Title = {{Multimodal Face Recognition using Spectral Transformation by LBP and
   Polynomial Coefficients}},
Booktitle = {{PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION
   SYSTEMS AND NETWORKS (COMNET)}},
Series = {{International Conference on Communication Systems and Networks}},
Year = {{2016}},
Pages = {{13-17}},
Note = {{International Conference on Communication Systems and Networks (ComNet),
   Trivandrum, INDIA, JUL 21-23, 2016}},
Organization = {{IEEE}},
Abstract = {{This paper presents a multimodal face recognition using spectral
   transformation by Local Binary Pattern (LBP) and Polynomial
   Coefficients. Here 2D image and 3D image are combined to get multimodal
   face recognition. In this method a novel feature extraction is done
   using LBP and Polynomial Coefficients. Then these features are
   spectrally transformed using Discrete Fourier Transform (DFT). These
   spectrally transformed features extracted from texture image using the
   two methods are combined at the score level. Similarly this is done in
   depth image. Finally feature information from texture and depth are
   combined at the score level which gives better results than the
   individual results.}},
ISSN = {{2155-2487}},
ISBN = {{978-1-5090-3349-2}},
Unique-ID = {{ISI:000401716400003}},
}

@inproceedings{ ISI:000401510000148,
Author = {Amin, Rafiul and Shams, A. Farhan and Rahman, S. M. Mahbubur and
   Hatzinakos, Dimitrios},
Book-Group-Author = {{IEEE}},
Title = {{Evaluation of Discrimination Power of Facial Parts from 3D Point Cloud
   Data}},
Booktitle = {{2016 9TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING
   (ICECE)}},
Series = {{International Conference on Computer and Electrical Engineering ICCEE}},
Year = {{2016}},
Pages = {{602-605}},
Note = {{9th International Conference on Electrical and Computer Engineering
   (ICECE), Dhaka, BANGLADESH, DEC 20-22, 2016}},
Organization = {{Bangladesh Univ Engn \& Technol, Dept Elect \& Elect Engn; Inst Elect \&
   Elect Engineers; Energypac; PARADISE Cables ltd; BTCL; Summit Communicat
   Ltd; Dhaka Power Distribut Co Ltd}},
Abstract = {{Feature selection from facial regions is a well-known approach to
   increase the performance of 2D image-based face recognition systems. In
   case of 3D modality, the approach of region-based feature selection for
   face recognition is relatively new. In this context, this paper presents
   an approach to evaluate the discrimination power of different regions of
   a 3D facial surface for its potential use in face recognition systems.
   We propose the use of weighted average of unit normal vector on the
   facial surface as the feature for region-based face recognition from 3D
   point cloud data (PCD). The iterative closest point algorithm is
   employed for the registration of segmented regions of facial point
   clouds. A metric based on angular distance between normals is introduced
   to indicate the similarity between two surfaces of same facial region.
   Finally, the intra class correlation based discrimination score is
   formulated to find out the key facial regions such as the eyes, nose,
   and mouth that are significant while recognizing a person with facial
   surface PCD.}},
ISBN = {{978-1-5090-2963-1}},
ResearcherID-Numbers = {{Amin, Rafiul/L-8633-2019}},
Unique-ID = {{ISI:000401510000148}},
}

@inproceedings{ ISI:000400012304105,
Author = {Bolkart, Timo and Wuhrer, Stefanie},
Book-Group-Author = {{IEEE}},
Title = {{A Robust Multilinear Model Learning Framework for 3D Faces}},
Booktitle = {{2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2016}},
Pages = {{4911-4919}},
Note = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Seattle, WA, JUN 27-30, 2016}},
Organization = {{IEEE Comp Soc; Comp Vis Fdn}},
Abstract = {{Multilinear models are widely used to represent the statistical
   variations of 3D human faces as they decouple shape changes due to
   identity and expression. Existing methods to learn a multilinear face
   model degrade if not every person is captured in every expression, if
   face scans are noisy or partially occluded, if expressions are
   erroneously labeled, or if the vertex correspondence is inaccurate.
   These limitations impose requirements on the training data that
   disqualify large amounts of available 3D face data from being usable to
   learn a multilinear model. To overcome this, we introduce the first
   framework to robustly learn a multilinear model from 3D face databases
   with missing data, corrupt data, wrong semantic correspondence, and
   inaccurate vertex correspondence. To achieve this robustness to
   erroneous training data, our framework jointly learns a multilinear
   model and fixes the data. We evaluate our framework on two publicly
   available 3D face databases, and show that our framework achieves a data
   completion accuracy that is comparable to state-of-the-art tensor
   completion methods. Our method reconstructs corrupt data more accurately
   than state-of-the-art methods, and improves the quality of the learned
   model significantly for erroneously labeled expressions.}},
DOI = {{10.1109/CVPR.2016.531}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-8851-1}},
Unique-ID = {{ISI:000400012304105}},
}

@inproceedings{ ISI:000400688200019,
Author = {Starczewski, Janusz T. and Pabiasz, Sebastian and Vladymyrska, Natalia
   and Marvuglia, Antonino and Napoli, Christian and Wozniak, Marcin},
Editor = {{Rutkowski, L and Korytkowski, M and Scherer, R and Tadeusiewicz, R and Zadeh, LA and Zurada, JM}},
Title = {{Self Organizing Maps for 3D Face Understanding}},
Booktitle = {{ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING, (ICAISC 2016), PT II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9693}},
Pages = {{210-217}},
Note = {{15th International Conference on Artificial Intelligence and Soft
   Computing (ICAISC), Zakopane, POLAND, JUN 12-16, 2016}},
Organization = {{Polish Neural Network Soc; Univ Social Sci; Czestochowa Univ Technol,
   Inst Computat Intelligence}},
Abstract = {{Landmarks are unique points that can be located on every face. Facial
   landmarks typically recognized by people are correlated with
   anthropomorphic points. Our purpose is to employ in 3D face recognition
   such landmarks that are easy to interpret. Face understanding is
   construed as identification of face characteristic points with automatic
   labeling of them. In this paper, we apply methods based on Self
   Organizing Maps to understand 3D faces.}},
DOI = {{10.1007/978-3-319-39384-1\_19}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-39384-1}},
ResearcherID-Numbers = {{Marvuglia, Antonino/J-2595-2019
   Wozniak, Marcin/L-6640-2013
   }},
ORCID-Numbers = {{Wozniak, Marcin/0000-0002-9073-5347
   Starczewski, Janusz/0000-0003-4694-7868
   Napoli, Christian/0000-0002-3336-5853}},
Unique-ID = {{ISI:000400688200019}},
}

@inproceedings{ ISI:000388114601158,
Author = {Gevaert, Caroline and Persello, Claudio and Sliuzas, Richard and
   Vosselman, George},
Book-Group-Author = {{IEEE}},
Title = {{INTEGRATION OF 2D AND 3D FEATURES FROM UAV IMAGERY FOR INFORMAL
   SETTLEMENT CLASSIFICATION USING MULTIPLE KERNEL LEARNING}},
Booktitle = {{2016 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS)}},
Series = {{IEEE International Symposium on Geoscience and Remote Sensing IGARSS}},
Year = {{2016}},
Pages = {{1508-1511}},
Note = {{36th IEEE International Geoscience and Remote Sensing Symposium
   (IGARSS), Beijing, PEOPLES R CHINA, JUL 10-15, 2016}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers, Geoscience
   \& Remote Sensing Soc; NSSC}},
Abstract = {{Informal settlement upgrading projects require highresolution and
   up-to-date thematic maps in order to plan and design effective
   interventions. To this end, Unmanned Aerial Vehicles (UAVs) provide the
   opportunity to obtain very high resolution 2D orthomosaics and 3D point
   clouds where and when needed. The heterogeneous, dense structures which
   typically make up an informal settlement motivate the importance of
   integrating complex 2D and 3D features obtained from UAV data into a
   single classification problem. Multiple Kernel Learning (MKL) Support
   Vector Machines (SVMs) maintain the distinct characteristics of the
   different feature spaces by optimizing individual kernels for specific
   feature groups which are later combined into a single kernel used for
   classification. Both the kernel parameters and kernel weights can be
   optimized by considering the alignment between the kernel and an ideal
   kernel which would perfectly classify the samples. This paper
   demonstrates how extracting high-level features from both the 2D
   orthomosaic as well as the 3D point cloud (obtained by an UAV), and
   integrating them through a MKL approach, can obtain an Overall Accuracy
   of 90.29\%, a 4\% increase over the results obtained using single kernel
   methods.}},
DOI = {{10.1109/IGARSS.2016.7729385}},
ISSN = {{2153-6996}},
ISBN = {{978-1-5090-3332-4}},
ResearcherID-Numbers = {{Persello, Claudio/L-5713-2015
   Vosselman, George/D-3985-2009
   Gevaert, Caroline/H-6195-2019
   Sliuzas, Richard/K-5323-2013}},
ORCID-Numbers = {{Persello, Claudio/0000-0003-3742-5398
   Vosselman, George/0000-0001-8813-8028
   Sliuzas, Richard/0000-0001-5243-4431}},
Unique-ID = {{ISI:000388114601158}},
}

@inproceedings{ ISI:000393154600016,
Author = {Harikumar, A. and Bovolo, F. and Bruzzone, L.},
Editor = {{Bruzzone, L and Bovolo, F}},
Title = {{A novel approach to internal crown characterization for coniferous tree
   species classification}},
Booktitle = {{IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXII}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{10004}},
Note = {{Conference on Image and Signal Processing for Remote Sensing XXII,
   Edinburgh, SCOTLAND, SEP 26-28, 2016}},
Organization = {{SPIE}},
Abstract = {{The knowledge about individual trees in forest is highly beneficial in
   forest management. High density small foot- print multi-return airborne
   Light Detection and Ranging (LiDAR) data can provide a very accurate
   information about the structural properties of individual trees in
   forests. Every tree species has a unique set of crown structural
   characteristics that can be used for tree species classification. In
   this paper, we use both the internal and external crown structural
   information of a conifer tree crown, derived from a high density small
   foot-print multi-return LiDAR data acquisition for species
   classification. Considering the fact that branches are the major
   building blocks of a conifer tree crown, we obtain the internal crown
   structural information using a branch level analysis. The structure of
   each conifer branch is represented using clusters in the LiDAR point
   cloud. We propose the joint use of the k-means clustering and geometric
   shape fitting, on the LiDAR data projected onto a novel 3-dimensional
   space, to identify branch clusters. After mapping the identified
   clusters back to the original space, six internal geometric features are
   estimated using a branch-level analysis. The external crown
   characteristics are modeled by using six least correlated features based
   on cone fitting and convex hull. Species classification is performed
   using a sparse Support Vector Machines (sparse SVM) classifier.}},
DOI = {{10.1117/12.2241452}},
Article-Number = {{100040H}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0412-4; 978-1-5106-0413-1}},
Unique-ID = {{ISI:000393154600016}},
}

@inproceedings{ ISI:000390287300035,
Author = {Pan, Zhenglin and Polceanu, Mihai and Lisetti, Christine},
Editor = {{Traum, D and Swartout, W and Khooshabeh, P and Kopp, S and Scherer, S and Leuski, A}},
Title = {{On Constrained Local Model Feature Normalization for Facial Expression
   Recognition}},
Booktitle = {{INTELLIGENT VIRTUAL AGENTS, IVA 2016}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{10011}},
Pages = {{369-372}},
Note = {{16th International Conference on Intelligent Virtual Agents (IVA), Los
   Angeles, CA, SEP 20-23, 2016}},
Organization = {{Alelo; Springer; Univ So Calif, Inst Creat Technologies}},
Abstract = {{Real time user independent facial expression recognition is important
   for virtual agents but challenging. However, since in real time
   recognition users are not necessarily presenting all the emotions, some
   proposed methods are not applicable. In this paper, we present a new
   approach that instead of using the traditional base face normalization
   on whole face shapes, performs normalization on the point cloud of each
   landmark. The result shows that our method outperforms the other two
   when the user input does not contain all six universal emotions.}},
DOI = {{10.1007/978-3-319-47665-0\_35}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-47665-0; 978-3-319-47664-3}},
Unique-ID = {{ISI:000390287300035}},
}

@inproceedings{ ISI:000392743800047,
Author = {Boehm, J. and Bredif, M. and Gierlinger, T. and Kraemer, M. and
   Lindenbergh, R. and Liu, K. and Michel, F. and Sirmacek, B.},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND
   IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{301-307}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Current 3D data capturing as implemented on for example airborne or
   mobile laser scanning systems is able to efficiently sample the surface
   of a city by billions of unselective points during one working day. What
   is still difficult is to extract and visualize meaningful information
   hidden in these point clouds with the same efficiency. This is where the
   FP7 IQmulus project enters the scene. IQmulus is an interactive facility
   for processing and visualizing big spatial data. In this study the
   potential of IQmulus is demonstrated on a laser mobile mapping point
   cloud of 1 billion points sampling similar to 10 km of street
   environment in Toulouse, France. After the data is uploaded to the
   IQmulus Hadoop Distributed File System, a workflow is defined by the
   user consisting of retiling the data followed by a PCA driven local
   dimensionality analysis, which runs efficiently on the IQmulus cloud
   facility using a Spark implementation. Points scattering in 3 directions
   are clustered in the tree class, and are separated next into individual
   trees. Five hours of processing at the 12 node computing cluster results
   in the automatic identification of 4000+ urban trees. Visualization of
   the results in the IQmulus fat client helps users to appreciate the
   results, and developers to identify remaining flaws in the processing
   workflow.}},
DOI = {{10.5194/isprsarchives-XLI-B3-301-2016}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Boehm, Jan/K-2336-2012
   Bredif, Mathieu/T-3029-2018
   }},
ORCID-Numbers = {{Boehm, Jan/0000-0003-2190-0449
   Bredif, Mathieu/0000-0003-0228-1232
   Lindenbergh, Roderik/0000-0001-8655-5266}},
Unique-ID = {{ISI:000392743800047}},
}

@inproceedings{ ISI:000392743800052,
Author = {Moradi, A. and Satari, M. and Momeni, M.},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{INDIVIDUAL TREE OF URBAN FOREST EXTRACTION FROM VERY HIGH DENSITY LIDAR
   DATA}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{337-343}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Airborne LiDAR (Light Detection and Ranging) data have a high potential
   to provide 3D information from trees. Most proposed methods to extract
   individual trees detect points of tree top or bottom firstly and then
   using them as starting points in a segmentation algorithm. Hence, in
   these methods, the number and the locations of detected peak points
   heavily effect on the process of detecting individual trees. In this
   study, a new method is presented to extract individual tree segments
   using LiDAR points with 10cm point density. In this method, a two-step
   strategy is performed for the extraction of individual tree LiDAR
   points: finding deterministic segments of individual trees points and
   allocation of other LiDAR points based on these segments. This research
   is performed on two study areas in Zeebrugge, Bruges, Belgium (51.33
   degrees N, 3.20 degrees E). The accuracy assessment of this method
   showed that it could correctly classified 74.51\% of trees with 21.57\%
   and 3.92\% under- and over-segmentation errors respectively.}},
DOI = {{10.5194/isprsarchives-XLI-B3-337-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Momeni, Mehdi/0000-0003-3705-1787}},
Unique-ID = {{ISI:000392743800052}},
}

@inproceedings{ ISI:000392743800092,
Author = {Kadamen, Jayren and Sithole, George},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{AUTOMATICALLY DETERMINING SCALE WITHIN UNSTRUCTU RED POINT CLOUDS}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{617-624}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Three dimensional models obtained from imagery have an arbitrary scale
   and therefore have to be scaled. Automatically scaling these models
   requires the detection of objects in these models which can be
   computationally intensive. Real-time object detection may pose problems
   for applications such as indoor navigation. This investigation poses the
   idea that relational cues, specifically height ratios, within indoor
   environments may offer an easier means to obtain scales for models
   created using imagery. The investigation aimed to show two things, (a)
   that the size of objects, especially the height off ground is consistent
   within an environment, and (b) that based on this consistency, objects
   can be identified and their general size used to scale a model. To test
   the idea a hypothesis is first tested on a terrestrial lidar scan of an
   indoor environment. Later as a proof of concept the same test is applied
   to a model created using imagery. The most notable finding was that the
   detection of objects can be more readily done by studying the ratio
   between the dimensions of objects that have their dimensions defined by
   human physiology. For example the dimensions of desks and chairs are
   related to the height of an average person. In the test, the difference
   between generalised and actual dimensions of objects were assessed. A
   maximum difference of 3.96\% (2.93cm) was observed from automated
   scaling. By analysing the ratio between the heights (distance from the
   floor) of the tops of objects in a room, identification was also
   achieved.}},
DOI = {{10.5194/isprsarchives-XLI-B3-617-2016}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000392743800092}},
}

@inproceedings{ ISI:000392750100114,
Author = {Zhang, Zongliang and Li, Jonathan and Li, Xin and Lin, Yangbin and
   Zhang, Shanxin and Wang, Cheng},
Editor = {{Halounova, L and Safar, V and Toth, CK and Karas, J and Huadong, G and Haala, N and Habib, A and Reinartz, P and Tang, X and Li, J and Armenakis, C and Grenzdorffer, G and LeRoux, P and Stylianidis, S and Blasi, R and Menard, M and Dufourmount, H and Li, Z}},
Title = {{A FAST METHOD FOR MEASURING THE SIMILARITY BETWEEN 3D MODEL AND 3D POINT
   CLOUD}},
Booktitle = {{XXIII ISPRS Congress, Commission I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B1}},
Pages = {{725-728}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{This paper proposes a fast method for measuring the partial Similarity
   between 3D Model and 3D point Cloud (SimMC). It is crucial to measure
   SimMC for many point cloud-related applications such as 3D object
   retrieval and inverse procedural modelling. In our proposed method, the
   surface area of model and the Distance from Model to point Cloud
   (DistMC) are exploited as measurements to calculate SimMC. Here, DistMC
   is defined as the weighted distance of the distances between points
   sampled from model and point cloud Similarly, Distance from point Cloud
   to Model (DistCM) is defined as the average distance of the distances
   between points in point cloud and model. In order to reduce huge
   computational burdens brought by calculation of DistCM in some
   traditional methods, we define SimMC as the ratio of weighted surface
   area of model to DistMC. Compared to those traditional SimMC measuring
   methods that are only able to measure global similarity, our method is
   capable of measuring partial similarity by employing distance-weighted
   strategy. Moreover, our method is able to be faster than other partial
   similarity assessment methods. We demonstrate the superiority of our
   method both on synthetic data and laser scanning data.}},
DOI = {{10.5194/isprsarchives-XLI-B1-725-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhang, Zongliang/0000-0002-0175-4299}},
Unique-ID = {{ISI:000392750100114}},
}

@inproceedings{ ISI:000392739800107,
Author = {Zhou, K. and Gorte, B. and Zlatanova, S.},
Editor = {{Halounova, L and Safar, V and Remondino, F and Hodac, J and Pavelka, K and Shortis, M and Rinaudo, F and Scaioni, M and Boehm, J and RiekeZapp, D}},
Title = {{EXPLORING REGULARITIES FOR IMPROVING FACADE RECONSTRUCTION FROM POINT
   CLOUDS}},
Booktitle = {{XXIII ISPRS Congress, Commission V}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B5}},
Pages = {{749-755}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{(Semi)-automatic facade reconstruction from terrestrial LiDAR point
   clouds is often affected by both quality of point cloud itself and
   imperfectness of object recognition algorithms. In this paper, we employ
   regularities, which exist on facades, to mitigate these problems. For
   example, doors, windows and balconies often have orthogonal and parallel
   boundaries. Many windows are constructed with the same shape. They may
   be arranged at the same lines and distance intervals, so do different
   windows. By identifying regularities among objects with relatively poor
   quality, these can be applied to calibrate the objects and improve their
   quality. The paper focuses on the regularities among the windows, which
   is the majority of objects on the wall. Regularities are classified into
   three categories: within an individual window, among similar windows and
   among different windows. Nine cases are specified as a reference for
   exploration. A hierarchical clustering method is employed to identify
   and apply regularities in a feature space, where regularities can be
   identified from clusters. To find the corresponding features in the nine
   cases of regularities, two phases are distinguished for similar and
   different windows. In the first phase, ICP (iterative closest points) is
   used to identify groups of similar windows. The registered points and a
   number of transformation matrices are used to identify and apply
   regularities among similar windows. In the second phase, features are
   extracted from the boundaries of the different windows. When applying
   regularities by relocating windows, the connections, called chains,
   established among the similar windows in the first phase are preserved.
   To test the performance of the algorithms, two datasets from terrestrial
   LiDAR point clouds are used. Both show good effects on the reconstructed
   model, while still matching with original point cloud, preventing over
   or under-regularization.}},
DOI = {{10.5194/isprsarchives-XLI-B5-749-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Gorte, Bernardus/0000-0001-8953-6394}},
Unique-ID = {{ISI:000392739800107}},
}

@inproceedings{ ISI:000392266900056,
Author = {Oh, Jaesung and Bae, Hyoin and Lim, Jeongsoo and Oh, Jun-Ho},
Book-Group-Author = {{IEEE}},
Title = {{Development of Autonomous Laser Toning System based on Vision
   Recognition and Robot Manipulator}},
Booktitle = {{2016 6TH IEEE INTERNATIONAL CONFERENCE ON BIOMEDICAL ROBOTICS AND
   BIOMECHATRONICS (BIOROB)}},
Series = {{Proceedings of the IEEE RAS-EMBS International Conference on Biomedical
   Robotics and Biomechatronics}},
Year = {{2016}},
Pages = {{317-322}},
Note = {{6th IEEE International Conference on Biomedical Robotics and
   Biomechatronics (BioRob), SINGAPORE, JUN 26-29, 2016}},
Organization = {{IEEE}},
Abstract = {{In this paper, the design, implementation, and operation method of the
   autonomous laser toning system are proposed, which is called as MELON (
   Manipulator for Effective Laser tONing). The system can recognize the
   accurate treatment points from the 3D point cloud data obtained with the
   camera, and it is possible to emit the laser at the desired position and
   orientation repeatedly, precisely, and accurately using intuitive
   differential inverse kinematics of the robot manipulator. The
   feasibility test of the MELON is conducted by using a plaster cast of a
   woman's head, and then, we find that the manipulator has a workspace to
   cover the entire face of the human inductively and distribution of the
   laser emission is homogeneous on the face. Therefore, we find the
   possibility of the autonomous laser toning using MELON.}},
ISSN = {{2155-1782}},
ISBN = {{978-1-5090-3287-7}},
Unique-ID = {{ISI:000392266900056}},
}

@inproceedings{ ISI:000391534900098,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal},
Editor = {{Liew, AWC and Lovell, B and Fookes, C and Zhou, J and Gao, Y and Blumenstein, M and Wang, Z}},
Title = {{Towards Large-scale 3D Face Recognition}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND
   APPLICATIONS (DICTA)}},
Year = {{2016}},
Pages = {{682-689}},
Note = {{International Conference on Digital Image Computing - Techniques and
   Applications (DICTA), Gold Coast, AUSTRALIA, NOV 30-DEC 02, 2016}},
Organization = {{Australian Govt, Dept Defence, Defence Sci \& Technol Grp; IAPR; Canon
   Informat Syst Res Australia; IEEE; Griffith Univ; APRS}},
Abstract = {{3D face recognition holds great promise in achieving robustness to pose,
   expressions and occlusions. However, 3D face recognition algorithms are
   still far behind their 2D counterparts due to the lack of large-scale
   datasets. We present a model based algorithm for 3D face recognition and
   test its performance by combining two large public datasets of 3D faces.
   We propose a Fully Convolutional Deep Network (FCDN) to initialize our
   algorithm. Reliable seed points are then extracted from each 3D face by
   evolving level set curves with a single curvature dependent adaptive
   speed function. We then establish dense correspondence between the faces
   in the training set by matching the surface around the seed points on a
   template face to the ones on the target faces. A morphable model is then
   fitted to probe faces and face recognition is performed by matching the
   parameters of the probe and gallery faces. Our algorithm achieves state
   of the art landmark localization results. Face recognition results on
   the combined FRGCv2 and Bosphorus datasets show that our method is
   affective in recognizing query faces with real world variations in pose
   and expression, and with occlusion and missing data despite a huge
   gallery. Comparing results of individual and combined datasets show that
   the recognition accuracy drops when the size of the gallery increases.}},
ISBN = {{978-1-5090-2896-2}},
Unique-ID = {{ISI:000391534900098}},
}

@article{ ISI:000391852100014,
Author = {Meng, Ting Wei and Choi, Gary Pui-Tung and Lui, Lok Ming},
Title = {{TEMPO: Feature-Endowed Teichmiiller Extremal Mappings of Point Clouds}},
Journal = {{SIAM JOURNAL ON IMAGING SCIENCES}},
Year = {{2016}},
Volume = {{9}},
Number = {{4}},
Pages = {{1922-1962}},
Abstract = {{In recent decades, the use of three-dimensional point clouds has been
   widespread in the computer industry. The development of techniques for
   analyzing point clouds is increasingly important. In particular, mapping
   of point clouds has been a challenging problem. In this paper, we
   develop a discrete analogue of the Teichmfiller extremal mappings, which
   guarantees uniform conformality distortions on point cloud surfaces.
   Based on the discrete analogue, we propose a novel method called TEMPO
   for computing Teichmfiller extremal mappings between feature-endowed
   point clouds. Using our proposed method, the Teichmfiller metric is
   introduced for evaluating the dissimilarity of point clouds.
   Consequently, our algorithm enables accurate recognition and
   classification of point clouds. Experimental results demonstrate the
   effectiveness of our proposed method.}},
DOI = {{10.1137/15M1049117}},
ISSN = {{1936-4954}},
Unique-ID = {{ISI:000391852100014}},
}

@inproceedings{ ISI:000391015300024,
Author = {Parmehr, Ebadat G. and Amati, Marco and Fraser, Clive S.},
Editor = {{Halounova, L and Sunar, F and Potuckova, M and Patkova, L and Yoshimura, M and Soergel, U}},
Title = {{MAPPING URBAN TREE CANOPY COVER USING FUSED AIRBORNE LIDAR AND SATELLITE
   IMAGERY DATA}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION VII}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{3}},
Number = {{7}},
Pages = {{181-186}},
Note = {{23rd ISPRS Congress, Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Urban green spaces, particularly urban trees, play a key role in
   enhancing the liveability of cities. The availability of accurate and
   up-to-date maps of tree canopy cover is important for sustainable
   development of urban green spaces. LiDAR point clouds are widely used
   for the mapping of buildings and trees, and several LiDAR point cloud
   classification techniques have been proposed for automatic mapping.
   However, the effectiveness of point cloud classification techniques for
   automated tree extraction from LiDAR data can be impacted to the point
   of failure by the complexity of tree canopy shapes in urban areas.
   Multispectral imagery, which provides complementary information to LiDAR
   data, can improve point cloud classification quality. This paper
   proposes a reliable method for the extraction of tree canopy cover from
   fused LiDAR point cloud and multispectral satellite imagery data. The
   proposed method initially associates each LiDAR point with spectral
   information from the co-registered satellite imagery data. It calculates
   the normalised difference vegetation index (NDVI) value for each LiDAR
   point and corrects tree points which have been misclassified as
   buildings. Then, region growing of tree points, taking the NDVI value
   into account, is applied. Finally, the LiDAR points classified as tree
   points are utilised to generate a canopy cover map. The performance of
   the proposed tree canopy cover mapping method is experimentally
   evaluated on a data set of airborne LiDAR and WorldView 2 imagery
   covering a suburb in Melbourne, Australia.}},
DOI = {{10.5194/isprsannals-III-7-181-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Amati, Marco/0000-0002-9600-5572}},
Unique-ID = {{ISI:000391015300024}},
}

@inproceedings{ ISI:000390841700083,
Author = {Torkhani, Ghada and Ladgham, Anis and Mansouri, Mohamed Nejib and Sakly,
   Anis},
Book-Group-Author = {{IEEE}},
Title = {{Gabor-SVM Applied to 3D-2D Deformed Mesh Model}},
Booktitle = {{2016 2ND INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)}},
Year = {{2016}},
Pages = {{447-452}},
Note = {{2nd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP), Monastir, TUNISIA, MAR 21-23, 2016}},
Organization = {{IEEE; IEEE Tunisia Sect; ATMS Lab; ATSI; IEEE Explore; EMB; ENIS Sch;
   Telecom Paris; Supelec; CESBIO; Telecom SudParis; ENIT; Univ Paris Sud;
   IEEE Signal Proc Soc Tunisia Chapter; ISAAM Inst; Minist Higher Educ
   Res; IEEE EMP Tunisia Chapter; Novartis Company}},
Abstract = {{We propose a robust method for 3D face recognition using 3D to 2D
   modeling and facial curvatures detection. The 3D2D algorithm permits to
   transform 3D images into 3D triangular mesh, then the mesh model is
   deformed and fitted to the 2D space in order to obtain a 2D smoother
   mesh. Then, we apply Gabor wavelets to the deformed model in order to
   exploit surface curves in the detection of salient face features. The
   classification of the final Gabor facial model is performed using the
   support vector machines (SVM). To demonstrate the quality of our
   technique, we give some experiments using the 3D AJMAL faces database.
   The experimental results prove that the proposed method is able to give
   a good recognition quality and a high accuracy rate.}},
ISBN = {{978-1-4673-8526-8}},
Unique-ID = {{ISI:000390841700083}},
}

@inproceedings{ ISI:000390841200037,
Author = {Koppen, W. P. and Christmas, W. J. and Crouch, D. J. M. and Bodmer, W.
   F. and Kittler, J. V.},
Editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun, J}},
Title = {{Extending Non-negative Matrix Factorisation to 3D Registered Data}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2016}},
Note = {{9th International Conference on Biometrics (ICB), Halmstad Univ,
   Halmstad, SWEDEN, JUN 13-16, 2016}},
Organization = {{IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB;
   Safran Ident \& Secur; EU Horizon 2020 Project IDENT; Speed Ident AB;
   Cognitec}},
Abstract = {{The use of non-negative matrix factorisation (NMF) on 2D face images has
   been shown to result in sparse feature vectors that encode for local
   patches on the face, and thus provides a statistically justified
   approach to learning parts from wholes. However successful on 2D images,
   the method has so far not been extended to 3D images. The main reason
   for this is that 3D space is a continuum and so it is not apparent how
   to represent 3D coordinates in a non-negative fashion. This work
   compares different non-negative representations for spatial coordinates,
   and demonstrates that not all non-negative representations are suitable.
   We analyse the representational properties that make NMF a successful
   method to learn sparse 3D facial features. Using our proposed
   representation, the factorisation results in sparse and interpretable
   facial features.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-5090-1869-7}},
Unique-ID = {{ISI:000390841200037}},
}

@inproceedings{ ISI:000390841200018,
Author = {Zhang, Jinjin and Huang, Di and Wang, Yunhong and Sun, Jia},
Editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun, J}},
Title = {{Lock3DFace: A Large-Scale Database of Low-Cost Kinect 3D Faces}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2016}},
Note = {{9th International Conference on Biometrics (ICB), Halmstad Univ,
   Halmstad, SWEDEN, JUN 13-16, 2016}},
Organization = {{IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB;
   Safran Ident \& Secur; EU Horizon 2020 Project IDENT; Speed Ident AB;
   Cognitec}},
Abstract = {{In this paper, we present a large-scale database consisting of low cost
   Kinect 3D face videos, namely Lock3DFace, for 3D face analysis,
   particularly for 3D Face Recognition (FR). To the best of our knowledge,
   Lock3DFace is currently the largest low cost 3D face database for public
   academic use. The 3D samples are highly noisy and contain a diversity of
   variations in expression, pose, occlusion, time lapse, and their
   corresponding texture and near infrared channels have changes in
   lighting condition and radiation intensity, allowing for evaluating FR
   methods in complex situations. Furthermore, based on Lock3DFace, we
   design the standard experimental protocol for low-cost 3D FR, and give
   the baseline performance of individual subsets belonging to different
   scenarios for fair comparison in the future.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-5090-1869-7}},
Unique-ID = {{ISI:000390841200018}},
}

@inproceedings{ ISI:000390023100094,
Author = {Adriana Echeagaray-Patron, B. and Kober, Vitaly},
Editor = {{Tescher, AG}},
Title = {{Face recognition based on matching of local features on 3D dynamic range
   sequences}},
Booktitle = {{APPLICATIONS OF DIGITAL IMAGE PROCESSING XXXIX}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9971}},
Note = {{Conference on Applications of Digital Image Processing XXXIX, San Diego,
   CA, AUG 29-SEP 01, 2016}},
Organization = {{SPIE}},
Abstract = {{3D face recognition has attracted attention in the last decade due to
   improvement of technology of 3D image acquisition and its wide range of
   applications such as access control, surveillance, human-computer
   interaction and biometric identification systems. Most research on 3D
   face recognition has focused on analysis of 3D still data. In this work,
   a new method for face recognition using dynamic 3D range sequences is
   proposed. Experimental results are presented and discussed using 3D
   sequences in the presence of pose variation. The performance of the
   proposed method is compared with that of conventional face recognition
   algorithms based on descriptors.}},
DOI = {{10.1117/12.2236355}},
Article-Number = {{997131}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0333-2; 978-1-5106-0334-9}},
Unique-ID = {{ISI:000390023100094}},
}

@inproceedings{ ISI:000390311500021,
Author = {Hakobyan, Hayk and Hakobyan, Robert and Aslanyan, Koryun},
Editor = {{Shahbazian, E and Rogova, G}},
Title = {{Human Identification Using Virtual 3D Imaging to Control Border Crossing}},
Booktitle = {{MEETING SECURITY CHALLENGES THROUGH DATA ANALYTICS AND DECISION SUPPORT}},
Series = {{Nato Science for Peace and Security Series D-Information and
   Communication Security}},
Year = {{2016}},
Volume = {{47}},
Pages = {{226-230}},
Note = {{Advanced Research Workshop (ARW) on Meeting Security Challenges through
   Data Analytics and Decision Support, Aghveran, ARMENIA, JUN 01-05, 2015}},
Organization = {{NATO Secur Through Sci Programme}},
Abstract = {{Human identification is an important aspect of border crossing.
   Identification makes it possible to recognize criminals or unauthorized
   entities and to prevent illegal border crossing. However, the
   identification process should remain comfortable and convenient for
   authorized entities. In this paper, we present an efficient face
   recognition system based on 2 cameras, which obtains a 3D image by
   processing data from camera video streams. Existing identification
   systems are also discussed.}},
DOI = {{10.3233/978-1-61499-716-0-226}},
ISSN = {{1874-6268}},
ISBN = {{978-1-61499-716-0; 978-1-61499-715-3}},
Unique-ID = {{ISI:000390311500021}},
}

@inproceedings{ ISI:000387187800290,
Author = {Liu, Shuming and Chen, Xiaopeng and Fan, Di and Chen, Xu and Meng, Fei
   and Huang, Qiang},
Book-Group-Author = {{IEEE}},
Title = {{3D Smiling Facial Expression Recognition Based on SVM}},
Booktitle = {{2016 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION}},
Year = {{2016}},
Pages = {{1661-1666}},
Note = {{IEEE International Conference on Mechatronics and Automation, Harbin,
   PEOPLES R CHINA, AUG 07-10, 2016}},
Organization = {{IEEE}},
Abstract = {{Using Kinect acquired RGB-D image to obtain a face feature parameters
   and three-dimensional coordinates of the characteristic parameters, and
   to select the characteristic parameter Facial by Candide-3 model, and
   feature extraction and normalization. Smile face expression data
   collection through Kinect, SVM collected to smiley face data classify
   and output the result of recognition, and the results compared with
   two-dimensional image of smiling face expression recognition results.
   Experimental results show that three-dimensional image of smiling face
   expression recognition accuracy than the two-dimensional image of
   smiling face. This research has important significance for the research
   and application of facial expression recognition technology.}},
ISBN = {{978-1-5090-2396-7}},
Unique-ID = {{ISI:000387187800290}},
}

@inproceedings{ ISI:000389381200037,
Author = {Trung Truong and Ngoc Ly},
Editor = {{Nguyen, NT and Trawinski, B and Fujita, H and Hong, TP}},
Title = {{Building the Facial Expressions Recognition System Based on RGB-D Images
   in High Performance}},
Booktitle = {{Intelligent Information and Database Systems, ACIIDS 2016, Pt II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9622}},
Pages = {{377-387}},
Note = {{8th Asian Conference on Intelligent Information and Database Systems
   (ACIIDS), Da Nang, VIETNAM, MAR 14-16, 2016}},
Organization = {{Vietnam Korea Friendship Informat Technol Coll; Wroclaw Univ Technol;
   IEEE SMC Tech Comm Computat Collect Intelligence; Bina Nusantara Univ;
   Ton Duc Thang Univ; Quang Binh Univ}},
Abstract = {{In this paper, we propose a novel idea for automatic facial expression
   analysis with the aim of resolving the existing challenges in 2D images.
   The subtle combination of the geometry-based method with the
   appearance-based features in depth and color images contributes to
   increasing in distinguishable features among various facial expressions.
   Particular functions are utilised to calculate the correlation between
   expressions in order to determine the exact facial expression. Our
   approach consists of a sequence of steps including estimating the normal
   vector of facial surface, then extracting the geometric features such as
   the orientation of normal vector in the point cloud. The useful color
   information is known as LBP. According to the result of the experiment,
   we demonstrate that the effective fusion scheme of texture and shape
   feature on color and depth images. In comparison with the non fusion
   scheme, our fusion scheme has resulted in the increase of recognition
   under low and high illuminated light, about 19.84\% and 1.59\%,
   respectively.}},
DOI = {{10.1007/978-3-662-49390-8\_37}},
ISSN = {{0302-9743}},
ISBN = {{978-3-662-49390-8; 978-3-662-49389-2}},
ORCID-Numbers = {{Truong, Quang Trung/0000-0002-6242-2191}},
Unique-ID = {{ISI:000389381200037}},
}

@article{ ISI:000385343000017,
Author = {Abd Rahman, Siti Zaharah and Abdullah, Siti Norul Huda Sheikh and Hao,
   Lim Eng and Abdulameer, Mohammed Hasan and Zamani, Nazri Ahmad and
   Darus, Mohammad Zaharudin A.},
Title = {{MAPPING 2D TO 3D FORENSIC FACIAL RECOGNITION VIA BIO-INSPIRED ACTIVE
   APPEARANCE MODEL}},
Journal = {{JURNAL TEKNOLOGI}},
Year = {{2016}},
Volume = {{78}},
Number = {{2-2}},
Pages = {{121-129}},
Abstract = {{This research done is to solve the problems faced by digital forensic
   analysts in identifying a suspect captured on their CCTV. Identifying
   the suspect through the CCTV video footage is a very challenging task
   for them as it involves tedious rounds of processes to match the facial
   information in the video footage to a set of suspect's images. The
   biggest problem faced by digital forensic analysis is modeling 2D model
   extracted from CCTV video as the model does not provide enough
   information to carry out the identification process. Problems occur when
   a suspect in the video is not facing the camera, the image extracted is
   the side image of the suspect and it is difficult to make a matching
   with portrait image in the database. There are also many factors that
   contribute to the process of extracting facial information from a video
   to be difficult, such as low-quality video. Through 2D to 3D image model
   mapping, any partial face information that is incomplete can be matched
   more efficiently with 3D data by rotating it to matched position. The
   first methodology in this research is data collection; any data obtained
   through video recorder. Then, the video will be converted into an image.
   Images are used to develop the Active Appearance Model (the 2D face
   model is AAM) 2D and AAM 3D. AAM is used as an input for learning and
   testing process involving three classifiers, which are Random Forest,
   Support Vector Machine (SVM), and Neural Networks classifier. The
   experimental results show that the 3D model is more suitable for use in
   face recognition as the percentage of the recognition is higher compared
   with the 2D model.}},
ISSN = {{0127-9696}},
EISSN = {{2180-3722}},
ORCID-Numbers = {{Rahman, Syed Ziaur/0000-0002-3460-1993}},
Unique-ID = {{ISI:000385343000017}},
}

@inproceedings{ ISI:000385794300020,
Author = {Khatiwada, Bikalpa and Budge, Scott E.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Three-dimensional image reconstruction using bundle adjustment applied
   to multiple texel images}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXI}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9832}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Baltimore,
   MD, APR 19-20, 2016}},
Organization = {{SPIE}},
Abstract = {{The importance of creating 3D imagery is increasing and has many
   applications in the field of disaster response, digital elevation
   models, object recognition, and cultural heritage. Several methods have
   been proposed to register texel images, which consist of fused lidar and
   digital imagery. The previous methods were limited to registering up to
   two texel images or multiple texel swaths having only one strip of lidar
   data per swath. One area of focus still remains to register multiple
   texel images to create a 3D model.
   The process of creating true 3D images using multiple texel images is
   described. The texel camera fuses the 2D digital image and calibrated 3D
   lidar data to form a texel image. The images are then taken from several
   perspectives and registered. The advantage of using multiple full frame
   texel images over 3D- or 2D-only methods is that there will be better
   registration between images because of the overlapping 3D points as well
   as 2D texture used in the joint registration process. The individual
   position and rotation mapping to a common world coordinate frame is
   calculated for each image and optimized. The proposed methods
   incorporate bundle adjustment for jointly optimizing the registration of
   multiple images. Sparsity is exploited as there is a lack of interaction
   between parameters of different cameras. Examples of the 3D model are
   shown and analyzed for numerical accuracy.}},
DOI = {{10.1117/12.2223259}},
Article-Number = {{UNSP 98320S}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0073-7}},
ORCID-Numbers = {{Budge, Scott/0000-0002-6138-3602}},
Unique-ID = {{ISI:000385794300020}},
}

@inproceedings{ ISI:000385794300011,
Author = {Magruder, Lori A. and Leigh, Holly W. and Soderlund, Alexander and
   Clymer, Bradley and Baer, Jessica and Neuenschwander, Amy L.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Automated feature extraction for 3-dimensional point clouds}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXI}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9832}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Baltimore,
   MD, APR 19-20, 2016}},
Organization = {{SPIE}},
Abstract = {{Light detection and ranging (LIDAR) technology offers the capability to
   rapidly capture high-resolution, 3-dimensional surface data with
   centimeter-level accuracy for a large variety of applications. Due to
   the foliage-penetrating properties of LIDAR systems, these geospatial
   data sets can detect ground surfaces beneath trees, enabling the
   production of high-fidelity bare earth elevation models. Precise
   characterization of the ground surface allows for identification of
   terrain and non-terrain points within the point cloud, and facilitates
   further discernment between natural and man-made objects based solely on
   structural aspects and relative neighboring parameterizations. A
   framework is presented here for automated extraction of natural and
   man-made features that does not rely on coincident ortho-imagery or
   point RGB attributes. The TEXAS (Terrain EXtraction And Segmentation)
   algorithm is used first to generate a bare earth surface from a lidar
   survey, which is then used to classify points as terrain or non-terrain.
   Further classifications are assigned at the point level by leveraging
   local spatial information. Similarly classed points are then clustered
   together into regions to identify individual features. Descriptions of
   the spatial attributes of each region are generated, resulting in the
   identification of individual tree locations, forest extents, building
   footprints, and 3-dimensional building shapes, among others. Results of
   the fully-automated feature extraction algorithm are then compared to
   ground truth to assess completeness and accuracy of the methodology.}},
DOI = {{10.1117/12.2223845}},
Article-Number = {{UNSP 98320F}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0073-7}},
Unique-ID = {{ISI:000385794300011}},
}

@inproceedings{ ISI:000384248300039,
Author = {Ding, Yifu and Tavolara, Thomas and Cheng, Keith},
Editor = {{Gurcan, MN and Madabhushi, A}},
Title = {{Automated Detection of Retinal Cell Nuclei in 3D Micro-CT Images of
   Zebrafish using Support Vector Machine Classification}},
Booktitle = {{MEDICAL IMAGING 2016: DIGITAL PATHOLOGY}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9791}},
Note = {{Conference on Medical Imaging - Digital Pathology, San Diego, CA, MAR
   02-03, 2016}},
Organization = {{SPIE; Modus Med Devices Inc; Bruker; Poco Graphite; ImXPAD}},
Abstract = {{Our group is developing a method to examine biological specimens in
   cellular detail using synchrotron microCT. The method can acquire 3D
   images of tissue at micrometer-scale resolutions, allowing for
   individual cell types to be visualized in the context of the entire
   specimen. For model organism research, this tool will enable the rapid
   characterization of tissue architecture and cellular morphology from
   every organ system. This characterization is critical for proposed and
   ongoing ``phenome{''} projects that aim to phenotype whole-organism
   mutants and diseased tissues from different organisms including humans.
   With the envisioned collection of hundreds to thousands of images for a
   phenome project, it is important to develop quantitative image analysis
   tools for the automated scoring of organism phenotypes across organ
   systems. Here we present a first step towards that goal, demonstrating
   the use of support vector machines (SVM) in detecting retinal cell
   nuclei in 3D images of wild-type zebrafish. In addition, we apply the
   SVM classifier on a mutant zebrafish to examine whether SVMs can be used
   to capture phenotypic differences in these images. The long-term goal of
   this work is to allow cellular and tissue morphology to be characterized
   quantitatively for many organ systems, at the level of the
   whole-organism.}},
DOI = {{10.1117/12.2216940}},
Article-Number = {{97911A}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0026-3}},
ORCID-Numbers = {{Ding, Yifu/0000-0002-4629-5858}},
Unique-ID = {{ISI:000384248300039}},
}

@inproceedings{ ISI:000381427400036,
Author = {Naveen, S. and Moni, R. S.},
Editor = {{Berretti, S and Thampi, SM and Srivastava, PR}},
Title = {{Contourlet and Fourier Transform Features Based 3D Face Recognition
   System}},
Booktitle = {{INTELLIGENT SYSTEMS TECHNOLOGIES AND APPLICATIONS, VOL 1}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2016}},
Volume = {{384}},
Pages = {{411-425}},
Note = {{International Symposium on Intelligent Systems Technologies and
   Applications (ISTA), SCMS Grp Inst, SCMS Sch Engn \& Technol, Kochi,
   INDIA, AUG 10-13, 2015}},
Abstract = {{Human face recognition based on geometrical structure has been an area
   of interest among researchers for the past few decades especially in
   pattern recognition. 3D Face recognition systems are of interest in this
   context. The main advantage of 3D Face recognition is the availability
   of geometrical information of the face structure which is more or less
   unique for a subject. This paper focuses on the problems of person
   identification using 3D Face data. Use of unregistered 3D Face data for
   feature extraction significantly increases the operational speed of the
   system with huge database enrollment. In this work, unregistered Face
   data, i.e. both texture and depth is fed to a classifier in spectral
   representations of the same data. 2-D Discrete Contourlet Transform and
   2-D Discrete Fourier Transform is used here for the spectral
   representation which forms the feature matrix. Fusion of texture and
   depth statistical information of face is proposed in this paper since
   the individual schemes are of lower performance. Application of
   statistical method seems to degrade the performance of the system when
   applied to texture data and was effective in the case of depth data.
   Fusion of the matching scores proves that the recognition accuracy can
   be improved significantly by fusion of scores of multiple
   representations. FRAV3D database is used for testing the algorithm.}},
DOI = {{10.1007/978-3-319-23036-8\_36}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-23036-8; 978-3-319-23035-1}},
Unique-ID = {{ISI:000381427400036}},
}

@inproceedings{ ISI:000378122900082,
Author = {Lenz, Marcel and Krug, Robin and Welp, Hubert and Schmieder, Kirsten and
   Hofmann, Martin R.},
Editor = {{Izatt, JA and Fujimoto, JG and Tuchin, VV}},
Title = {{Ex vivo brain tumor analysis using Spectroscopic Optical Coherence
   Tomography}},
Booktitle = {{OPTICAL COHERENCE TOMOGRAPHY AND COHERENCE DOMAIN OPTICAL METHODS IN
   BIOMEDICINE XX}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9697}},
Note = {{Conference on Optical Coherence Tomography and Coherence Domain Optical
   Methods in Biomedicine XX, San Francisco, CA, FEB 15-17, 2016}},
Organization = {{SPIE}},
Abstract = {{A big challenge during neurosurgeries is to distinguish between healthy
   tissue and cancerous tissue, but currently a suitable non-invasive real
   time imaging modality is not available. Optical Coherence Tomography
   (OCT) is a potential technique for such a modality. OCT has a
   penetration depth of 1-2 mm and a resolution of 1-15 mu m which is
   sufficient to illustrate structural differences between healthy tissue
   and brain tumor. Therefore, we investigated gray and white matter of
   healthy central nervous system and meningioma samples with a Spectral
   Domain OCT System (Thorlabs Callisto). Additional OCT images were
   generated after paraffin embedding and after the samples were cut into
   10 mu m thin slices for histological investigation with a bright field
   microscope. All samples were stained with Hematoxylin and Eosin. In all
   cases B-scans and 3D images were made. Furthermore, a camera image of
   the investigated area was made by the built-in video camera of our OCT
   system. For orientation, the backsides of all samples were marked with
   blue ink. The structural differences between healthy tissue and
   meningioma samples were most pronounced directly after removal. After
   paraffin embedding these differences diminished. A correlation between
   OCT en face images and microscopy images can be seen. In order to
   increase contrast, post processing algorithms were applied. Hence we
   employed Spectroscopic OCT, pattern recognition algorithms and machine
   learning algorithms such as k-means Clustering and Principal Component
   Analysis.}},
DOI = {{10.1117/12.2214704}},
Article-Number = {{96973D}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-931-3}},
Unique-ID = {{ISI:000378122900082}},
}

@article{ ISI:000370679800010,
Author = {Kristoffersen, Miklas S. and Dueholm, Jacob V. and Gade, Rikke and
   Moeslund, Thomas B.},
Title = {{Pedestrian Counting with Occlusion Handling Using Stereo Thermal Cameras}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{The number of pedestrians walking the streets or gathered in public
   spaces is a valuable piece of information for shop owners, city
   governments, event organizers and many others. However, automatic
   counting that takes place day and night is challenging due to changing
   lighting conditions and the complexity of scenes with many people
   occluding one another. To address these challenges, this paper
   introduces the use of a stereo thermal camera setup for pedestrian
   counting. We investigate the reconstruction of 3D points in a pedestrian
   street with two thermal cameras and propose an algorithm for pedestrian
   counting based on clustering and tracking of the 3D point clouds. The
   method is tested on two five-minute video sequences captured at a public
   event with a moderate density of pedestrians and heavy occlusions. The
   counting performance is compared to the manually annotated ground truth
   and shows success rates of 95.4\% and 99.1\% for the two sequences.}},
DOI = {{10.3390/s16010062}},
Article-Number = {{62}},
ISSN = {{1424-8220}},
ORCID-Numbers = {{Gade, Rikke/0000-0002-8016-2426
   Kristoffersen, Miklas Strom/0000-0002-1409-2618}},
Unique-ID = {{ISI:000370679800010}},
}

@article{ ISI:000369518500015,
Author = {Ouamane, A. and Belahcene, M. and Benakcha, A. and Bourennane, S. and
   Taleb-Ahmed, A.},
Title = {{Robust multimodal 2D and 3D face authentication using local feature
   fusion}},
Journal = {{SIGNAL IMAGE AND VIDEO PROCESSING}},
Year = {{2016}},
Volume = {{10}},
Number = {{1}},
Pages = {{129-137}},
Month = {{JAN}},
Abstract = {{In this work, we present a robust face authentication approach merging
   multiple descriptors and exploiting both 3D and 2D information. First,
   we correct the heads rotation in 3D by iterative closest point
   algorithm, followed by an efficient preprocessing phase. Then, we
   extract different features namely: multi-scale local binary patterns
   (MSLBP), novel statistical local features (SLF), Gabor wavelets, and
   scale invariant feature transform (SIFT). The principal component
   analysis followed by enhanced fisher linear discriminant model is used
   for dimensionality reduction and classification. Finally, fusion at the
   score level is carried out using two-class support vector machines.
   Extensive experiments are conducted on the CASIA 3D faces database. The
   evaluation of individual descriptors clearly showed the superiority of
   the proposed SLF features. In addition, applying the (3D + 2D)
   multimodal score level fusion, the best result is obtained by combining
   the SLF with the MSLBP + SIFT descriptor yielding in an equal error rate
   of 0.98\% and a recognition rate of RR = 97.22 \%.}},
DOI = {{10.1007/s11760-014-0712-x}},
ISSN = {{1863-1703}},
EISSN = {{1863-1711}},
Unique-ID = {{ISI:000369518500015}},
}

@article{ ISI:000368000100006,
Author = {Tang, Pingbo and Chen, Gaoyun and Shen, Zhenglai},
Title = {{A Spatial-Context-Based Approach for Automated Spatial Change Analysis
   of Piece-Wise Linear Building Elements}},
Journal = {{COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING}},
Year = {{2016}},
Volume = {{31}},
Number = {{1}},
Pages = {{65-80}},
Month = {{JAN}},
Abstract = {{Changes of designs and construction plans often cause propagative design
   modifications, tedious construction coordination, cascading effects of
   errors, reworks, and delays in project management. Among various
   building elements, those having piece-wise linear geometries (i.e.,
   connected straight line segments), such as connected straight sections
   of ducts in mechanical, electrical, and plumbing systems, frequently
   undergo spatial changes in response to the changes of their
   surroundings. On the other hand, the piece-wise linear geometries pose
   challenges to analyzing and controlling changes in construction and
   facility management. State-of-the-art 3D change detection algorithms
   often face ambiguities about which points belong to which objects when
   piece-wise linear object are spacked in small spaces. This article
   examines a spatial-context-based framework that uses spatial
   relationships between piece-wise linear building elements (ducts in this
   article) to enable fast and reliable association of 3D data with ducts
   in as-designed models for supporting reliable change analysis. Three
   case studies showed that this framework outperformed a conventional
   change detection method, and could handle large dislocations of
   piece-wise linear elements and occlusions.}},
DOI = {{10.1111/mice.12174}},
ISSN = {{1093-9687}},
EISSN = {{1467-8667}},
Unique-ID = {{ISI:000368000100006}},
}

@article{ ISI:000367856500018,
Author = {Bellil, Wajdi and Brahim, Hajer and Ben Amar, Chokri},
Title = {{Gappy wavelet neural network for 3D occluded faces: detection and
   recognition}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{75}},
Number = {{1}},
Pages = {{365-380}},
Month = {{JAN}},
Abstract = {{The first handicap in 3D faces recognizing under unconstrained problem
   is the largest variability of the visual aspect when we use various
   sources. This great variability complicates the task of identifying
   persons from their 3D facial scans and it is the most reason that bring
   to face detection and recognition of the major problems in pattern
   recognition fields, biometrics and computer vision. We propose a new 3D
   face identification and recognition method based on Gappy Wavelet Neural
   Network (GWNN) that is able to provide better accuracy in the presence
   of facial occlusions. The proposed approach consists of three steps: the
   first step is face detection. The second step is to identify and remove
   occlusions. Occluded regions detection is done by considering that
   occlusions can be defined as local face deformations. These deformations
   are detected by a comparison between the input facial test wavelet
   coefficients and wavelet coefficients of generic face model formed by
   the mean data base faces. They are beneficial for neighborhood
   relationships between pixels rotation, dilation and translation
   invariant. Then, occluded regions are refined by removing wavelet
   coefficient above a certain threshold. Finally, the last stage of
   processing and retrieving is made based on wavelet neural network to
   recognize and to restore 3D occluded regions that gathers the most. The
   experimental results on this challenging database demonstrate that the
   proposed approach improves recognition rate performance from 93.57 to
   99.45 \% which represents a competitive result compared to the state of
   the art.}},
DOI = {{10.1007/s11042-014-2294-6}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000367856500018}},
}

@article{ ISI:000367181400024,
Author = {Westphalen, Antonio C. and Noworolski, Susan M. and Harisinghani, Mukesh
   and Jhaveri, Kartik S. and Raman, Steve S. and Rosenkrantz, Andrew B.
   and Wang, Zhen J. and Zagoria, Ronald J. and Kurhanewicz, John},
Title = {{High-Resolution 3-T Endorectal Prostate MRI: A Multireader Study of
   Radiologist Preference and Perceived Interpretive Quality of 2D and 3D
   T2-Weighted Fast Spin-Echo MR Images}},
Journal = {{AMERICAN JOURNAL OF ROENTGENOLOGY}},
Year = {{2016}},
Volume = {{206}},
Number = {{1}},
Pages = {{86-91}},
Month = {{JAN}},
Abstract = {{OBJECTIVE. The goal of this study was to compare the perceived quality
   of 3-T axial T2-weighted high-resolution 2D and high-resolution 3D fast
   spin-echo (FSE) endorectal MR images of the prostate.
   MATERIALS AND METHODS. Six radiologists independently reviewed paired
   3-T axial T2-weighted high-resolution 2D and 3D FSE endorectal MR images
   of the prostates of 85 men in two sessions. In the first session (n =
   85), each reader selected his or her preferred images; in the second
   session (n = 28), they determined their confidence in tumor
   identification and compared the depiction of the prostatic anatomy,
   tumor conspicuity, and subjective intrinsic image quality of images. A
   meta-analysis using a random-effects model, logistic regression, and the
   paired Wilcoxon rank-sum test were used for statistical analyses.
   RESULTS. Three readers preferred the 2D acquisition (67-89\%), and the
   other three preferred the 3D images (70-80\%). The option for one of the
   techniques was not associated with any of the predictor variables. The
   2D FSE images were significantly sharper than 3D FSE (p < 0.001) and
   significantly more likely to exhibit other (nonmotion) artifacts (p =
   0.002). No other statistically significant differences were found.
   CONCLUSION. Our results suggest that there are strong individual
   preferences for the 2D or 3D FSE MR images, but there was a wide
   variability among radiologists. There were differences in image quality
   (image sharpness and presence of artifacts not related to motion) but
   not in the sequences' ability to delineate the glandular anatomy and
   depict a cancerous tumor.}},
DOI = {{10.2214/AJR.14.14065}},
ISSN = {{0361-803X}},
EISSN = {{1546-3141}},
ORCID-Numbers = {{Zagoria, Ronald/0000-0001-6926-4627}},
Unique-ID = {{ISI:000367181400024}},
}

@inproceedings{ ISI:000400012301070,
Author = {Hackel, Timo and Wegner, Jan D. and Schindler, Konrad},
Book-Group-Author = {{IEEE}},
Title = {{Contour detection in unstructured 3D point clouds}},
Booktitle = {{2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2016}},
Pages = {{1610-1618}},
Note = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Seattle, WA, JUN 27-30, 2016}},
Organization = {{IEEE Comp Soc; Comp Vis Fdn}},
Abstract = {{We describe a method to automatically detect contours, i.e. lines along
   which the surface orientation sharply changes, in large-scale outdoor
   point clouds. Contours are important intermediate features for
   structuring point clouds and converting them into high-quality surface
   or solid models, and are extensively used in graphics and mapping
   applications. Yet, detecting them in unstructured, inhomogeneous point
   clouds turns out to be surprisingly difficult, and existing line
   detection algorithms largely fail. We approach contour extraction as a
   two-stage discriminative learning problem. In the first stage, a contour
   score for each individual point is predicted with a binary classifier,
   using a set of features extracted from the point's neighborhood. The
   contour scores serve as a basis to construct an overcomplete graph of
   candidate contours. The second stage selects an optimal set of contours
   from the candidates. This amounts to a further binary classification in
   a higher-order MRF, whose cliques encode a preference for connected
   contours and penalize loose ends. The method can handle point clouds >
   10(7) points in a couple of minutes, and vastly outperforms a baseline
   that performs Canny-style edge detection on a range image representation
   of the point cloud.}},
DOI = {{10.1109/CVPR.2016.178}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-8851-1}},
Unique-ID = {{ISI:000400012301070}},
}

@article{ ISI:000367827600010,
Author = {Omelina, L. and Jansen, B. and Bonnechere, B. and Oravec, M. and
   Pavlovicova, Jarmila and Jan, S. Van Sint},
Title = {{Interaction Detection with Depth Sensing and Body Tracking Cameras in
   Physical Rehabilitation}},
Journal = {{METHODS OF INFORMATION IN MEDICINE}},
Year = {{2016}},
Volume = {{55}},
Number = {{1}},
Pages = {{70-78}},
Abstract = {{Introduction: This article is part of the Focus Theme of Methods of
   Information in Medicine on ``Methodologies, Models and Algorithms for
   Patients Rehabilitation{''}.
   Objectives: This paper presents a camera based method for identifying
   the patient and detecting interactions between the patient and the
   therapist during therapy. Detecting interactions helps to discriminate
   between active and passive motion of the patient as well as to estimate
   the accuracy of the skeletal data.
   Methods: Continuous face recognition is used to detect, recognize and
   track the patient with other people in the scene (e.g. the therapist, or
   a clinician). We use a method based on local binary patterns (LBP).
   After identifying users in the scene we identify interactions between
   the patient and other people. We use a depth map/point cloud for
   estimating the distance between two people. Our method uses the
   association of depth regions to user identities and computes the minimal
   distance between the regions.
   Results: Our results show state-of-the-art performance of real-time face
   recognition using low-resolution images that is sufficient to use in
   adaptive systems. Our proposed approach for detecting interactions shows
   91.9\% overall recognition accuracy what is sufficient for applications
   in the context of serious games. We also discuss limitations of the
   proposed method as well as general limitations of using depth cameras
   for serious games.
   Conclusions: We introduced a new method for frame-by-frame automated
   identification of the patient and labeling reliable sequences of the
   patient's data recorded during rehabilitation (games). Our method
   improves automated rehabilitation systems by detecting the identity of
   the patient as well as of the therapist and by detecting the distance
   between both over time.}},
DOI = {{10.3414/ME14-01-0120}},
ISSN = {{0026-1270}},
ORCID-Numbers = {{Van Sint Jan, Serge/0000-0002-3478-171X}},
Unique-ID = {{ISI:000367827600010}},
}

@article{ ISI:000365838200007,
Author = {Fehr, Duc and Beksi, William J. and Zermas, Dimitris and
   Papanikolopoulos, Nikolaos},
Title = {{Covariance based point cloud descriptors for object detection and
   recognition}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2016}},
Volume = {{142}},
Pages = {{80-93}},
Month = {{JAN}},
Abstract = {{Processing 3D point cloud data is of primary interest in many areas of
   computer vision, including object grasping, robot navigation, and object
   recognition. The introduction of affordable RGB-D sensors has created a
   great interest in the computer vision community towards developing
   efficient algorithms for point cloud processing. Previously, capturing a
   point cloud required expensive specialized sensors such as lasers or
   dedicated range imaging devices; now, range data is readily available
   from low-cost sensors that provide easily extractable point clouds from
   a depth map. From here, an interesting challenge is to find different
   objects in the point cloud. Various descriptors have been introduced to
   match features in a point cloud. Cheap sensors are not necessarily
   designed to produce precise measurements, which means that the data is
   not as accurate as a point cloud provided from a laser or a dedicated
   range finder. Although some feature descriptors have been shown to be
   successful in recognizing objects from point clouds, there still exists
   opportunities for improvement. The aim of this paper is to introduce
   techniques from other fields, such as image processing, into 3D point
   cloud processing in order to improve rendering, classification, and
   recognition. Covariances have proven to be a success not only in image
   processing, but in other domains as well. This work develops the
   application of covariances in conjunction with 3D point cloud data. (C)
   2015 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.cviu.2015.06.008}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
ORCID-Numbers = {{Fehr, Duc/0000-0001-8541-914X}},
Unique-ID = {{ISI:000365838200007}},
}

@article{ ISI:000368942000004,
Author = {Bargoti, Suchet and Underwood, James P. and Nieto, Juan I. and
   Sukkarieh, Salah},
Title = {{A Pipeline for Trunk Detection in Trellis Structured Apple Orchards}},
Journal = {{JOURNAL OF FIELD ROBOTICS}},
Year = {{2015}},
Volume = {{32}},
Number = {{8, SI}},
Pages = {{1075-1094}},
Month = {{DEC}},
Note = {{9th International Conference on Field and Service Robotics (FSR),
   Brisbane, AUSTRALIA, NOV 09-11, 2013}},
Abstract = {{The ability of robots to meticulously cover large areas while gathering
   sensor data has widespread applications in precision agriculture. For
   autonomous operations in orchards, a suitable information management
   system is required, within which we can gather and process data relating
   to the state and performance of the crop over time, such as distinct
   yield count, canopy volume, and crop health. An efficient way to
   structure an information system is to discretize it to the individual
   tree, for which tree segmentation/detection is a key component. This
   paper presents a tree trunk detection pipeline for identifying
   individual trees in a trellis structured apple orchard, using
   ground-based lidar and image data. A coarse observation of trunk
   candidates is initially made using a Hough transformation on point cloud
   lidar data. These candidates are projected into the camera images, where
   pixelwise classification is used to update their likelihood of being a
   tree trunk. Detection is achieved by using a hidden semi-Markov model to
   leverage from contextual information provided by the repetitive
   structure of an orchard. By repeating this over individual orchard rows,
   we are able to build a tree map over the farm, which can be either GPS
   localized or represented topologically by the row and tree number. The
   pipeline was evaluated at a commercial apple orchard near Melbourne,
   Australia. Data were collected at different times of year, covering an
   area of 1.6 ha containing different apple varieties planted on two types
   of trellis systems: a vertical I-trellis structure and a Guttingen
   V-trellis structure. The results show good trunk detection performance
   for both apple varieties and trellis structures during the preharvest
   season (87-96\% accuracy) and near perfect trunk detection performance
   (99\% accuracy) during the flowering season. (C) 2015 Wiley Periodicals,
   Inc.}},
DOI = {{10.1002/rob.21583}},
ISSN = {{1556-4959}},
EISSN = {{1556-4967}},
ORCID-Numbers = {{Underwood, James/0000-0003-0189-0706}},
Unique-ID = {{ISI:000368942000004}},
}

@article{ ISI:000365704000006,
Author = {St-Onge, Benoit and Audet, Felix-Antoine and Begin, Jean},
Title = {{Characterizing the Height Structure and Composition of a Boreal Forest
   Using an Individual Tree Crown Approach Applied to Photogrammetric Point
   Clouds}},
Journal = {{FORESTS}},
Year = {{2015}},
Volume = {{6}},
Number = {{11}},
Pages = {{3899-3922}},
Month = {{NOV}},
Abstract = {{Photogrammetric point clouds (PPC) obtained by stereomatching of aerial
   photographs now have a resolution sufficient to discern individual
   trees. We have produced such PPCs of a boreal forest and delineated
   individual tree crowns using a segmentation algorithm applied to the
   canopy height model derived from the PPC and a lidar terrain model. The
   crowns were characterized in terms of height and species (spruce, fir,
   and deciduous). Species classification used the 3D shape of the single
   crowns and their reflectance properties. The same was performed on a
   lidar dataset. Results show that the quality of PPC data generally
   approaches that of airborne lidar. For pixel-based canopy height models,
   viewing geometry in aerial images, forest structure (dense vs. open
   canopies), and composition (deciduous vs. conifers) influenced the
   quality of the 3D reconstruction of PPCs relative to lidar.
   Nevertheless, when individual tree height distributions were analyzed,
   PPC-based results were very similar to those extracted from lidar. The
   random forest classification (RF) of individual trees performed better
   in the lidar case when only 3D metrics were used (83\% accuracy for
   lidar, 79\% for PPC). However, when 3D and intensity or multispectral
   data were used together, the accuracy of PPCs (89\%) surpassed that of
   lidar (86\%).}},
DOI = {{10.3390/f6113899}},
ISSN = {{1999-4907}},
Unique-ID = {{ISI:000365704000006}},
}

@article{ ISI:000359029900025,
Author = {Liang, Ronghua and Shen, Wenjia and Li, Xiao-Xin and Wang, Haixia},
Title = {{Bayesian multi-distribution-based discriminative feature extraction for
   3D face recognition}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2015}},
Volume = {{320}},
Pages = {{406-417}},
Month = {{NOV 1}},
Abstract = {{Due to the difficulties associated with the collection of 3D samples, 3D
   face recognition technologies often have to work with smaller than
   desirable sample sizes. With the aim of enlarging the training number
   for each subject, we divide each training image into several patches.
   However, this immediately introduces two further problems for 3D models:
   high computational cost and dispersive features caused by the divided 3D
   image patches. We therefore first map 3D face images into 2D depth
   images, which greatly reduces the dimension of the samples. Though the
   depth images retain most of the robust features of 3D images, such as
   pose and illumination invariance, they lose many discriminative features
   of the original 3D samples. In this study, we propose a Bayesian
   learning framework to extract the discriminative features from the depth
   images. Specifically, we concentrate the features of the intra-class
   patches to a mean feature by maximizing the multivariate Gaussian
   likelihood function, and, simultaneously, enlarge the distances between
   the inter-class mean features by maximizing the exponential priori
   distribution of the mean features. For classification, we use the
   nearest neighbor classifier combined with the Mahalanobis distance to
   calculate the distance between the features of the test image and items
   in the training set. Experiments on two widely-used 3D face databases
   demonstrate the efficiency and accuracy of our proposed method compared
   to relevant state-of-the-art methods. Published by Elsevier Inc.}},
DOI = {{10.1016/j.ins.2015.03.063}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
Unique-ID = {{ISI:000359029900025}},
}

@article{ ISI:000363075300013,
Author = {Kankare, Ville and Liang, Xinlian and Vastaranta, Mikko and Yu, Xiaowei
   and Holopainen, Markus and Hyyppa, Juha},
Title = {{Diameter distribution estimation with laser scanning based multisource
   single tree inventory}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2015}},
Volume = {{108}},
Pages = {{161-171}},
Month = {{OCT}},
Abstract = {{Tree detection and tree species recognition are bottlenecks of the
   airborne remote sensing-based single tree inventories. The effect of
   these factors in forest attribute estimation can be reduced if airborne
   measurements are aided with tree mapping information that is collected
   from the ground. The main objective here was to demonstrate the use of
   terrestrial laser scanning-derived (TLS) tree maps in aiding airborne
   laser scanning-based (ALS) single tree inventory (multisource single
   tree inventory, MS-STI) and its capability in predicting diameter
   distribution in various forest conditions. Automatic measurement of TLS
   point clouds provided the tree maps and the required reference
   information from the tree attributes. The study area was located in Evo,
   Finland, and the reference data was acquired from 27 different sample
   plots with varying forest conditions. The workflow of MS-STI included:
   (1) creation of automatic tree map from TLS point clouds, (2) automatic
   diameter at breast height (DBH) measurement from TLS point clouds, (3)
   individual tree detection (ITD) based on ALS, (4) matching the ITD
   segments to the field-measured reference, (5) ALS point cloud metric
   extraction from the single tree segments and (6) DBH estimation based on
   the derived metrics. MS-STI proved to be accurate and efficient method
   for DBH estimation and predicting diameter distribution. The overall
   accuracy (root mean squared error, RMSE) of the DBH was 36.9 mm. Results
   showed that the DBH accuracy decreased if the tree density (trees/ha)
   increased. The highest accuracies were found in old-growth forests (tree
   densities less than 500 stems/ha). MS-STI resulted in the best
   accuracies regarding Norway spruce (Picea abies (L.) H.
   Karst.)-dominated forests (RMSE of 29.9 mm). Diameter distributions were
   predicted with low error indices, thereby resulting in a good fit
   compared to the reference. Based on the results, diameter distribution
   estimation with MS-STI is highly dependent on the forest structure and
   the accuracy of the tree maps that are used. The most important
   development step in the future for the MS-STI and automatic measurements
   of the TLS point cloud is to develop tree species recognition methods
   and further develop tree detection techniques. The possibility of using
   MLS or harvester data as a basis for the required tree maps should also
   be assessed in the future. (C) 2015 International Society for
   Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2015.07.007}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Vastaranta, Mikko/K-9656-2018}},
ORCID-Numbers = {{Vastaranta, Mikko/0000-0001-6552-9122}},
Unique-ID = {{ISI:000363075300013}},
}

@article{ ISI:000360999400005,
Author = {Patil, Hemprasad and Kothari, Ashwin and Bhurchandi, Kishor},
Title = {{3-D face recognition: features, databases, algorithms and challenges}},
Journal = {{ARTIFICIAL INTELLIGENCE REVIEW}},
Year = {{2015}},
Volume = {{44}},
Number = {{3}},
Pages = {{393-441}},
Month = {{OCT}},
Abstract = {{Face recognition is being widely accepted as a biometric technique
   because of its non-intrusive nature. Despite extensive research on 2-D
   face recognition, it suffers from poor recognition rate due to pose,
   illumination, expression, ageing, makeup variations and occlusions. In
   recent years, the research focus has shifted toward face recognition
   using 3-D facial surface and shape which represent more discriminating
   features by the virtue of increased dimensionality. This paper presents
   an extensive survey of recent 3-D face recognition techniques in terms
   of feature detection, classifiers as well as published algorithms that
   address expression and occlusion variation challenges followed by our
   critical comments on the published work. It also summarizes remarkable
   3-D face databases and their features used for performance evaluation.
   Finally we suggest vital steps of a robust 3-D face recognition system
   based on the surveyed work and identify a few possible directions for
   research in this area.}},
DOI = {{10.1007/s10462-015-9431-0}},
ISSN = {{0269-2821}},
EISSN = {{1573-7462}},
Unique-ID = {{ISI:000360999400005}},
}

@article{ ISI:000361074500008,
Author = {Kurtek, Sebastian and Drira, Hassen},
Title = {{A comprehensive statistical framework for elastic shape analysis of 3D
   faces}},
Journal = {{COMPUTERS \& GRAPHICS-UK}},
Year = {{2015}},
Volume = {{51}},
Number = {{SI}},
Pages = {{52-59}},
Month = {{OCT}},
Note = {{Shape Modeling International Conference (SMI 2015), Lille, FRANCE, JUN
   24-26, 2015}},
Abstract = {{We develop a comprehensive statistical framework for analyzing shapes of
   3D faces. In particular, we adapt a recent elastic shape analysis
   framework to the case of hemispherical surfaces, and explore its use in
   a number of processing applications. This framework provides a
   parameterization-invariant, elastic Riemannian metric, which allows the
   development of mathematically rigorous tools for statistical analysis.
   Specifically, this paper describes methods for registration, comparison
   and deformation, averaging, computation of covariance and summarization
   of variability using principal component analysis, random sampling from
   generative shape models, symmetry analysis, and expression and identity
   classification. An important aspect of this work is that all tasks are
   preformed under a unified metric, which has a natural interpretation in
   terms of bending and stretching of one 3D face to align it with another.
   We use a subset of the BU-3DFE face dataset, which contains varying
   magnitudes of expression. (C) 2015 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.cag.2015.05.027}},
ISSN = {{0097-8493}},
EISSN = {{1873-7684}},
ORCID-Numbers = {{Drira, Hassen/0000-0003-1052-4353}},
Unique-ID = {{ISI:000361074500008}},
}

@article{ ISI:000360490300004,
Author = {Spreeuwers, Luuk},
Title = {{Breaking the 99\% barrier: optimisation of three-dimensional face
   recognition}},
Journal = {{IET BIOMETRICS}},
Year = {{2015}},
Volume = {{4}},
Number = {{3}},
Pages = {{169-178}},
Month = {{SEP}},
Abstract = {{This study presents optimisations to a three-dimensional (3D) face
   recognition method the authors published in 2011. The optimisations
   concern handling and estimation of motion from a single 3D image using
   the symmetry of the face, fine registration by selection of the maximum
   score for small variations of the registration parameters and efficient
   training using automatic outlier removal where only part of the
   classifier is retrained. The optimisations lead to a staggering
   performance improvement: the verification rate on Face Recognition Grand
   Challenge (FRGC) v2 data at false accept rate = 0.1\% increases from
   94.6 to 99.3\% and the identification rate increases from 99 to 99.4\%.
   Both are, to the authors' knowledge, the best scores ever published on
   the FRGC data. In addition, the registration time was reduced from about
   2.5 to 0.2-0.6 s and the number of comparisons has increased from about
   11 000 to more than 50 000 per second. For slightly decreased
   performance, even millions of comparisons can be realised. The fast
   registration means near real-time recognition with 2-5 images is
   possible. The optimisations are not specific for this method, but can be
   beneficial for other 3D face recognition methods as well.}},
DOI = {{10.1049/iet-bmt.2014.0017}},
ISSN = {{2047-4938}},
EISSN = {{2047-4946}},
Unique-ID = {{ISI:000360490300004}},
}

@article{ ISI:000358893300004,
Author = {Korez, Robert and Ibragimov, Bulat and Likar, Bostjan and Pernus, Franjo
   and Vrtovec, Tomaz},
Title = {{A Framework for Automated Spine and Vertebrae Interpolation-Based
   Detection and Model-Based Segmentation}},
Journal = {{IEEE TRANSACTIONS ON MEDICAL IMAGING}},
Year = {{2015}},
Volume = {{34}},
Number = {{8, SI}},
Pages = {{1649-1662}},
Month = {{AUG}},
Abstract = {{Automated and semi-automated detection and segmentation of spinal and
   vertebral structures from computed tomography (CT) images is a
   challenging task due to a relatively high degree of anatomical
   complexity, presence of unclear boundaries and articulation of vertebrae
   with each other, as well as due to insufficient image spatial
   resolution, partial volume effects, presence of image artifacts,
   intensity variations and low signal-to-noise ratio. In this paper, we
   describe a novel framework for automated spine and vertebrae detection
   and segmentation from 3-D CT images. A novel optimization technique
   based on interpolation theory is applied to detect the location of the
   whole spine in the 3-D image and, using the obtained location of the
   whole spine, to further detect the location of individual vertebrae
   within the spinal column. The obtained vertebra detection results
   represent a robust and accurate initialization for the subsequent
   segmentation of individual vertebrae, which is performed by an improved
   shape-constrained deformable model approach. The framework was evaluated
   on two publicly available CT spine image databases of 50 lumbar and 170
   thoracolumbar vertebrae. Quantitative comparison against corresponding
   reference vertebra segmentations yielded an overall mean
   centroid-to-centroid distance of 1.1 mm and Dice coefficient of 83.6\%
   for vertebra detection, and an overall mean symmetric surface distance
   of 0.3 mm and Dice coefficient of 94.6\% for vertebra segmentation. The
   results indicate that by applying the proposed automated detection and
   segmentation framework, vertebrae can be successfully detected and
   accurately segmented in 3-D from CT spine images.}},
DOI = {{10.1109/TMI.2015.2389334}},
ISSN = {{0278-0062}},
EISSN = {{1558-254X}},
Unique-ID = {{ISI:000358893300004}},
}

@article{ ISI:000357545400014,
Author = {Schmitt, Michael and Shahzad, Muhammad and Zhu, Xiao Xiang},
Title = {{Reconstruction of individual trees from multi-aspect TomoSAR data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2015}},
Volume = {{165}},
Pages = {{175-185}},
Month = {{AUG}},
Abstract = {{The localization and reconstruction of individual trees as well as the
   extraction of their geometrical parameters is an important field of
   research in both forestry and remote sensing. While the current
   state-of-the-art mostly focuses on the exploitation of optical imagery
   and airborne LiDAR data, modern SAR sensors have not yet met the
   interest of the research community in that regard. This paper presents a
   prototypical processing chain for the reconstruction of individual
   deciduous trees: First, single-pass multi-baseline InSAR data acquired
   from multiple aspect angles are used for the generation of a layover-
   and shadow-free 3D point cloud by tomographic SAR processing. The
   resulting point cloud is then segmented by unsupervised mean shift
   clustering, before ellipsoid models are fitted to the points of each
   cluster. From these 3D ellipsoids the relevant geometrical tree
   parameters are extracted. Evaluation with respect to a manually derived
   reference dataset prove that almost 74\% of all trees are successfully
   segmented and reconstructed, thus providing a promising perspective for
   further research toward individual tree recognition from SAR data. (C)
   2015 The Authors. Published by Elsevier Inc This is an open access
   article under the CC BY-NC-ND license.}},
DOI = {{10.1016/j.rse.2015.05.012}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000357545400014}},
}

@article{ ISI:000356107200001,
Author = {Wang, Chao and Cho, Yong K. and Kim, Changwan},
Title = {{Automatic BIM component extraction from point clouds of existing
   buildings for sustainability applications}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2015}},
Volume = {{56}},
Pages = {{1-13}},
Month = {{AUG}},
Abstract = {{Building information models (BIMs) are increasingly being applied
   throughout a building's lifecycle for various applications, such as
   progressive construction monitoring and defect detection, building
   renovation, energy simulation, and building system analysis in the
   Architectural, Engineering, Construction, and Facility Management
   (AEC/FM) domains. In conventional approaches, as-is BIM is primarily
   manually created from point clouds, which is labor-intensive, costly,
   and time consuming. This paper proposes a method for automatically
   extracting building geometries from unorganized point clouds. The
   collected raw data undergo data downsizing, boundary detection, and
   building component categorization, resulting in the building components
   being recognized as individual objects and their visualization as
   polygons. The results of tests conducted on three collected as-is
   building data to validate the technical feasibility and evaluate the
   performance of the proposed method indicate that it can simplify and
   accelerate the as-is building model from the point cloud creation
   process. (C) 2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.autcon.2015.04.001}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
ResearcherID-Numbers = {{Jeong, Yongwook/N-7413-2016}},
Unique-ID = {{ISI:000356107200001}},
}

@article{ ISI:000355894900023,
Author = {Weinmann, Martin and Jutzi, Boris and Hinz, Stefan and Mallet, Clement},
Title = {{Semantic point cloud interpretation based on optimal neighborhoods,
   relevant features and efficient classifiers}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2015}},
Volume = {{105}},
Pages = {{286-304}},
Month = {{JUL}},
Abstract = {{3D scene analysis in terms of automatically assigning 3D points a
   respective semantic label has become a topic of great importance in
   photogrammetiy, remote sensing, computer vision and robotics. In this
   paper, we address the issue of how to increase the distinctiveness of
   geometric features and select the most relevant ones among these for 3D
   scene analysis. We present a new, fully automated and versatile
   framework composed of four components: (i) neighborhood selection, (ii)
   feature extraction, (iii) feature selection and (iv) classification. For
   each component, we consider a variety of approaches which allow
   applicability in terms of simplicity, efficiency and reproducibility, so
   that end-users can easily apply the different components and do not
   require expert knowledge in the respective domains. In a detailed
   evaluation involving 7 neighborhood definitions, 21 geometric features,
   7 approaches for feature selection, 10 classifiers and 2 benchmark
   datasets, we demonstrate that the selection of optimal neighborhoods for
   individual 3D points significantly improves the results of 3D scene
   analysis. Additionally, we show that the selection of adequate feature
   subsets may even further increase the quality of the derived results
   while significantly reducing both processing time and memory
   consumption. (C) 2015 International Society for Photogrammetry and
   Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.isprsjprs.2015.01.016}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Weinmann, Martin/0000-0002-8654-7546
   Mallet, Clement/0000-0002-2675-165X}},
Unique-ID = {{ISI:000355894900023}},
}

@article{ ISI:000448059300007,
Author = {Rodriguez, Julian S. and Prieto, Flavio},
Title = {{Analysis and comparison of the cone curvature descriptor in facial
   gesture recognition tasks}},
Journal = {{INGENIERIA}},
Year = {{2015}},
Volume = {{20}},
Number = {{2}},
Pages = {{261-275}},
Month = {{JUL-DEC}},
Abstract = {{This article presents the results of analyzing the behavior of the Cone
   Curvature shape descriptor (CC) in the task of recognition of facial
   expressions in 3D images. The CC descriptor is a representation of the
   3D model computed from a set of waves modeling for each vertex of a
   polygon mesh. The 3D Facial Expression Database (BU-3DFE) was used,
   which contains images with six facial expressions. With the use of the
   CC descriptor, the expressions were recognized in an average percentage
   of 76.67\% with a neural network, and of 78.88\% with a Bayesian
   classifier. By combining the CC descriptor with other descriptors such
   as DESIRE and Spherical Spin Image, it was achieved an average
   percentage of gesture recognition of 90.27\% and 97.2\%, using the
   mentioned classifiers.}},
DOI = {{10.14483/udistrital.jour.reving.2015.2.a06}},
ISSN = {{0121-750X}},
EISSN = {{2344-8393}},
Unique-ID = {{ISI:000448059300007}},
}

@article{ ISI:000357869200002,
Author = {Duan, Zhugeng and Zhao, Dan and Zeng, Yuan and Zhao, Yujin and Wu,
   Bingfang and Zhu, Jianjun},
Title = {{Assessing and Correcting Topographic Effects on Forest Canopy Height
   Retrieval Using Airborne LiDAR Data}},
Journal = {{SENSORS}},
Year = {{2015}},
Volume = {{15}},
Number = {{6}},
Pages = {{12133-12155}},
Month = {{JUN}},
Abstract = {{Topography affects forest canopy height retrieval based on airborne
   Light Detection and Ranging (LiDAR) data a lot. This paper proposes a
   method for correcting deviations caused by topography based on
   individual tree crown segmentation. The point cloud of an individual
   tree was extracted according to crown boundaries of isolated individual
   trees from digital orthophoto maps (DOMs). Normalized canopy height was
   calculated by subtracting the elevation of centres of gravity from the
   elevation of point cloud. First, individual tree crown boundaries are
   obtained by carrying out segmentation on the DOM. Second, point clouds
   of the individual trees are extracted based on the boundaries. Third,
   precise DEM is derived from the point cloud which is classified by a
   multi-scale curvature classification algorithm. Finally, a height
   weighted correction method is applied to correct the topological
   effects. The method is applied to LiDAR data acquired in South China,
   and its effectiveness is tested using 41 field survey plots. The results
   show that the terrain impacts the canopy height of individual trees in
   that the downslope side of the tree trunk is elevated and the upslope
   side is depressed. This further affects the extraction of the location
   and crown of individual trees. A strong correlation was detected between
   the slope gradient and the proportions of returns with height
   differences more than 0.3, 0.5 and 0.8 m in the total returns, with
   coefficient of determination R-2 of 0.83, 0.76, and 0.60 (n = 41),
   respectively.}},
DOI = {{10.3390/s150612133}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000357869200002}},
}

@article{ ISI:000356741800007,
Author = {Weinmann, M. and Urban, S. and Hinz, S. and Jutzi, B. and Mallet, C.},
Title = {{Distinctive 2D and 3D features for automated large-scale scene analysis
   in urban areas}},
Journal = {{COMPUTERS \& GRAPHICS-UK}},
Year = {{2015}},
Volume = {{49}},
Pages = {{47-57}},
Month = {{JUN}},
Abstract = {{We propose a new methodology for large-scale urban 3D scene analysis in
   terms of automatically assigning 3D points the respective semantic
   labels. The methodology focuses on simplicity and reproducibility of the
   involved components as well as performance in terms of accuracy and
   computational efficiency. Exploiting a variety of low-level 2D and 3D
   geometric features, we further improve their distinctiveness by
   involving individual neighborhoods of optimal size. Due to the use of
   individual neighborhoods, the methodology is not tailored to a specific
   dataset, but in principle designed to process point clouds with a few
   millions of 3D points. Consequently, an extension has to be introduced
   for analyzing huge 3D point clouds with possibly billions of points for
   a whole city. For this purpose, we propose an extension which is based
   on an appropriate partitioning of the scene and thus allows a successive
   processing in a reasonable time without affecting the quality of the
   classification results. We demonstrate the performance of our
   methodology on two labeled benchmark datasets with respect to
   robustness, efficiency, and scalability. (C) 2015 Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.cag.2015.01.006}},
ISSN = {{0097-8493}},
EISSN = {{1873-7684}},
ORCID-Numbers = {{Mallet, Clement/0000-0002-2675-165X
   Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000356741800007}},
}

@article{ ISI:000355003600024,
Author = {Lindsay, Kaitlin E. and Ruehli, Frank J. and Deleon, Valerie Burke},
Title = {{Revealing the Face of an Ancient Egyptian: Synthesis of Current and
   Traditional Approaches to Evidence-Based Facial Approximation}},
Journal = {{ANATOMICAL RECORD-ADVANCES IN INTEGRATIVE ANATOMY AND EVOLUTIONARY
   BIOLOGY}},
Year = {{2015}},
Volume = {{298}},
Number = {{6}},
Pages = {{1144-1161}},
Month = {{JUN}},
Abstract = {{The technique of forensic facial approximation, or reconstruction, is
   one of many facets of the field of mummy studies. Although far from a
   rigorous scientific technique, evidence-based visualization of
   antemortem appearance may supplement radiological, chemical,
   histological, and epidemiological studies of ancient remains. Published
   guidelines exist for creating facial approximations, but few
   approximations are published with documentation of the specific process
   and references used. Additionally, significant new research has taken
   place in recent years which helps define best practices in the field.
   This case study records the facial approximation of a 3,000-year-old
   ancient Egyptian woman using medical imaging data and the digital
   sculpting program, ZBrush. It represents a synthesis of current
   published techniques based on the most solid anatomical and/or
   statistical evidence. Through this study, it was found that although
   certain improvements have been made in developing repeatable,
   evidence-based guidelines for facial approximation, there are many
   proposed methods still awaiting confirmation from comprehensive studies.
   This study attempts to assist artists, anthropologists, and forensic
   investigators working in facial approximation by presenting the
   recommended methods in a chronological and usable format. Anat Rec,
   298:1144-1161, 2015. (c) 2015 Wiley Periodicals, Inc.}},
DOI = {{10.1002/ar.23146}},
ISSN = {{1932-8486}},
EISSN = {{1932-8494}},
Unique-ID = {{ISI:000355003600024}},
}

@article{ ISI:000354623800005,
Author = {Mehanna, Ahmed Mohamed and Baki, Fatthi Abdel and Eid, Mohamed and Negm,
   Magdy},
Title = {{Comparison of different computed tomography post-processing modalities
   in assessment of various middle ear disorders}},
Journal = {{EUROPEAN ARCHIVES OF OTO-RHINO-LARYNGOLOGY}},
Year = {{2015}},
Volume = {{272}},
Number = {{6}},
Pages = {{1357-1370}},
Month = {{JUN}},
Abstract = {{Several anatomic structures of the middle ear are not optimally depicted
   in the standard axial and coronal planes. Several 2D and 3D
   image-processing modalities are currently available for CT examinations
   in clinical radiology departments. Till now 3D reconstructions of the
   temporal bone have not been widely used yet, and attracted only academic
   interest. The aim of this study was to compare axial (source images), 2D
   and 3DCT post-processing modalities, and to evaluate the value of 3D
   reconstructed images/virtual endoscopy (VE) in assessment of various
   middle ear disorders for identification of the best modality/view for
   assessment of a particular middle ear structure or pathology. 40
   patients with various middle ear disorders, planned for surgical
   intervention were included in prospective study. Multi-slice CT was
   performed for all patients. Scans were acquired in the axial plane. The
   axial source datasets were utilized for generation of 2D reformations
   and 3D reconstructed images. All studied images were divided into three
   categories: axial (source images), 2D reformations (MPR and
   sliding-thin-slab MIP) and 3D reconstruction (virtual endoscopy). The
   visibility of middle ear structures and pathologies with each modality
   were scored qualitatively using three-point scoring system in reference
   to operative findings. Stapes superstructure and footplate,
   incudostapedial joint, oval and round windows, tympanic segment of the
   facial nerve and tegmen were not optimally depicted in the axial plane.
   Sinus tympani and facial recess were best visualized with axial images
   or VE. 3D reconstruction/VE allowed good visualization of all parts of
   ossicular chain except stapes superstructure. Regarding pathologic
   changes, 2D reformations and 3D reconstructed images allowed better
   visualization of erosion of ossicles and tegmen. 3D reconstruction/VE
   did not allow detection of foci of otospongiosis. 2D reformations can be
   considered the mainstay in assessment of most middle ear structures and
   pathologies. 3D reconstruction/VE seems to provide a useful method for a
   preoperative general overview of the middle ear anatomy, particularly
   for the ossicular chain, round window and retrotympanum.}},
DOI = {{10.1007/s00405-014-2920-y}},
ISSN = {{0937-4477}},
EISSN = {{1434-4726}},
ORCID-Numbers = {{Eid, Mohamed/0000-0002-4830-9010}},
Unique-ID = {{ISI:000354623800005}},
}

@article{ ISI:000355288200010,
Author = {Vuollo, Ville and Sidlauskas, Mantas and Sidlauskas, Antanas and Harila,
   Virpi and Salomskiene, Loreta and Zhurov, Alexei and Holmstrom, Lasse
   and Pirttiniemi, Pertti and Heikkinen, Tuomo},
Title = {{Comparing Facial 3D Analysis With DNA Testing to Determine Zygosities of
   Twins}},
Journal = {{TWIN RESEARCH AND HUMAN GENETICS}},
Year = {{2015}},
Volume = {{18}},
Number = {{3}},
Pages = {{306-313}},
Month = {{JUN}},
Abstract = {{The aim of this study was to compare facial 3D analysis to DNA testing
   in twin zygosity determinations. Facial 3D images of 106 pairs of young
   adult Lithuanian twins were taken with a stereophotogrammetric device
   (3dMD, Atlanta, Georgia) and zygosity was determined according to
   similarity of facial form. Statistical pattern recognition methodology
   was used for classification. The results showed that in 75\% to 90\% of
   the cases, zygosity determinations were similar to DNA-based results.
   There were 81 different classification scenarios, including 3 groups, 3
   features, 3 different scaling methods, and 3 threshold levels. It
   appeared that coincidence with 0.5 mm tolerance is the most suitable
   feature for classification. Also, leaving out scaling improves results
   in most cases. Scaling was expected to equalize the magnitude of
   differences and therefore lead to better recognition performance. Still,
   better classification features and a more effective scaling method or
   classification in different facial areas could further improve the
   results. In most of the cases, male pair zygosity recognition was at a
   higher level compared with females. Erroneously classified twin pairs
   appear to be obvious outliers in the sample. In particular, faces of
   young dizygotic (DZ) twins may be so similar that it is very hard to
   define a feature that would help classify the pair as DZ.
   Correspondingly, monozygotic (MZ) twins may have faces with quite
   different shapes. Such anomalous twin pairs are interesting exceptions,
   but they form a considerable portion in both zygosity groups.}},
DOI = {{10.1017/thg.2015.16}},
ISSN = {{1832-4274}},
EISSN = {{1839-2628}},
ResearcherID-Numbers = {{Zhurov, Alexei/P-4410-2014
   }},
ORCID-Numbers = {{Zhurov, Alexei/0000-0002-5594-0740
   Pirttiniemi, Pertti/0000-0003-4514-836X}},
Unique-ID = {{ISI:000355288200010}},
}

@article{ ISI:000354402000018,
Author = {Betta, Giovanni and Capriglione, Domenico and Gasparetto, Michele and
   Zappa, Emanuele and Liguori, Consolatina and Paolillo, Alfredo},
Title = {{Face recognition based on 3D features: Management of the measurement
   uncertainty for improving the classification}},
Journal = {{MEASUREMENT}},
Year = {{2015}},
Volume = {{70}},
Pages = {{169-178}},
Month = {{JUN}},
Abstract = {{In this paper a suitable methodology for the improvement of the
   reliability of results in classification systems based on 3D images is
   proposed. More in detail, it is based on the knowledge of the
   uncertainty of the features constituting the 3D image (obtained
   processing a pair of two 2D stereoscopic images) and on a suitable
   statistical approach providing a confidence level to the classification
   result. These pieces of information are then managed in order to improve
   the classification performance in terms of correct classification and
   false reject percentages. The experimental results, obtained applying
   the methodology on an Active Appearance Models algorithm for feature
   detection and triangulating the 3D features, show that, compared with a
   basic approach (which generally does not take into account the
   uncertainty on 3D features), the proposed methodology allows to
   significantly improve the classification performance even in scenarios
   characterized by a high uncertainty. (C) 2015 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.measurement.2015.03.043}},
ISSN = {{0263-2241}},
EISSN = {{1873-412X}},
Unique-ID = {{ISI:000354402000018}},
}

@article{ ISI:000352571800033,
Author = {Mills, Graham and Fotopoulos, Georgia},
Title = {{Rock Surface Classification in a Mine Drift Using Multiscale Geometric
   Features}},
Journal = {{IEEE GEOSCIENCE AND REMOTE SENSING LETTERS}},
Year = {{2015}},
Volume = {{12}},
Number = {{6}},
Pages = {{1322-1326}},
Month = {{JUN}},
Abstract = {{Scale-dependent statistical depictions of surface morphology offer the
   potential to parameterize complex geometrical scaling relationships with
   greater detail than traditional fractal measures. Using multiscale
   operators, it is possible to identify points belonging to rough
   discontinuous surfaces in noisy point clouds solely on the basis of
   their local geometry. Many strategies for point cloud feature
   classification have been developed since the proliferation of laser
   scanning systems. Most of the techniques which are applicable to natural
   scenes employ external data sources such as hyperspectral imagery,
   return pulse intensity, and waveform data. In this letter, multiscale
   geometric parameters are used to identify individual point observations
   corresponding to rock surfaces in point clouds acquired by terrestrial
   laser scanning in scenes with man-made clutter and scanning artifacts.
   Three multiscale operators, namely, the approximate shape and density of
   a defined neighborhood and the distance of its mean point from its
   geometric center, are fused into a single feature vector. The procedure
   is demonstrated using real point cloud data acquired in a mine drift,
   with the goal of identifying points belonging to the rock face obscured
   by an overlying wire support mesh. Using the extra-trees classifier,
   extraneous returns caused by the mesh were excluded from the point cloud
   with a 97\% success rate, while 87\% of the desired surface points were
   retained.}},
DOI = {{10.1109/LGRS.2015.2398814}},
ISSN = {{1545-598X}},
EISSN = {{1558-0571}},
Unique-ID = {{ISI:000352571800033}},
}

@article{ ISI:000353891500010,
Author = {Kashani, Alireza G. and Crawford, Patrick S. and Biswas, Sufal K. and
   Graettinger, Andrew J. and Grau, David},
Title = {{Automated Tornado Damage Assessment and Wind Speed Estimation Based on
   Terrestrial Laser Scanning}},
Journal = {{JOURNAL OF COMPUTING IN CIVIL ENGINEERING}},
Year = {{2015}},
Volume = {{29}},
Number = {{3}},
Month = {{MAY}},
Abstract = {{There are more than 1,000 tornadoes in the United States each year, yet
   engineers do not typically design for tornadoes because of insufficient
   information about wind loads. Collecting building-level damage data in
   the aftermath of tornadoes can improve the understanding of tornado
   winds, but these data are difficult to collect because of safety, time,
   and access constraints. This study presents and tests an automated
   geographic information system (GIS) method using postevent point cloud
   data collected by terrestrial scanners and preevent aerial images to
   calculate the percentage of roof and wall damage and estimate wind
   speeds at an individual building scale. Simulations determined that for
   typical point cloud density (>25points/m2), a GIS raster cell size of
   40-50cm resulted in less than 10\% error in damaged roof and wall
   detection. Data collected after recent tornadoes were used to correlate
   wind speed estimates and the percent of detected damage. The developed
   method estimated wind speeds from damage data collected after the 2011
   Tuscaloosa, AL tornado at finer scales than the typical large-scale
   assessments done by reconnaissance engineers.}},
DOI = {{10.1061/(ASCE)CP.1943-5487.0000389}},
Article-Number = {{04014051}},
ISSN = {{0887-3801}},
EISSN = {{1943-5487}},
Unique-ID = {{ISI:000353891500010}},
}

@article{ ISI:000353807700021,
Author = {Hoevenaren, Inge A. and Maal, Thomas J. J. and Krikken, E. and de Haan,
   A. F. J. and Berge, S. J. and Ulrich, D. J. O.},
Title = {{Development of a three-dimensional hand model using 3D
   stereophotogrammetry: Evaluation of landmark reproducibility}},
Journal = {{JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY}},
Year = {{2015}},
Volume = {{68}},
Number = {{5}},
Pages = {{709-716}},
Month = {{MAY}},
Abstract = {{BACKGROUND: Using three-dimensional (3D) photography, exact images of
   the human body can be produced. Over the last few years, this technique
   is mainly being developed in the field of maxillofacial reconstructive
   surgery, creating fusion images with computed tomography (CT) data for
   accurate planning and prediction of treatment outcome. However, in hand
   surgery, 3D photography is not yet being used in clinical settings.
   METHODS: The aim of this study was to develop a valid method for imaging
   the hand using 3D stereophotogrammetry. The reproducibility of 30 soft
   tissue landmarks was determined using 3D stereophotogrammetric images.
   Analysis was performed by two observers on 20 3D photographs.
   Reproducibility and reliability of the landmark identification were
   determined using statistical analysis.
   RESULTS: The intra-and interobserver reproducibility of the landmarks
   were high. This study showed a high reliability coefficient for
   intraobserver (1.00) and interobserver reliability (0.99).
   Identification of the landmarks on the palmar aspect of individual
   fingers was more precise than the identification of landmarks of the
   thumb.
   CONCLUSIONS: This study shows that 3D photography can safely produce
   accurate and reproducible images of the hand, which makes the technique
   a reliable method for soft tissue analysis. 3D images can be a helpful
   tool in pre- and postoperative evaluation of reconstructive trauma
   surgery, in aesthetic surgery of the hand, and for educational purposes.
   The use in everyday practice of hand surgery and the concept of fusing
   3D photography images with radiologic images of the interior hand
   structures needs to be further explored. (C) 2014 British Association of
   Plastic, Reconstructive and Aesthetic Surgeons. Published by Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.bjps.2014.12.025}},
ISSN = {{1748-6815}},
EISSN = {{1878-0539}},
ResearcherID-Numbers = {{Ulrich, D.J.O./H-8099-2014
   Maal, Thomas/L-4497-2015
   de Haan, Antonius/L-4344-2015
   Berge, S.J./H-8011-2014}},
ORCID-Numbers = {{Maal, Thomas/0000-0002-1702-4733
   }},
Unique-ID = {{ISI:000353807700021}},
}

@article{ ISI:000352441700012,
Author = {Urbanova, Petra and Hejna, Petr and Jurda, Mikolas},
Title = {{Testing photogrammetry-based techniques for three-dimensional surface
   documentation in forensic pathology}},
Journal = {{FORENSIC SCIENCE INTERNATIONAL}},
Year = {{2015}},
Volume = {{250}},
Pages = {{77-86}},
Month = {{MAY}},
Abstract = {{Three-dimensional surface technologies particularly close range
   photogrammetry and optical surface scanning have recently advanced into
   affordable, flexible and accurate techniques. Forensic postmortem
   investigation as performed on a daily basis, however, has not yet fully
   benefited from their potentials. In the present paper, we tested two
   approaches to 3D external body documentation - digital camera-based
   photogrammetry combined with commercial Agisoft PhotoScan (R) software
   and stereophotogrammetry-based Vectra H1 (R), a portable handheld
   surface scanner. In order to conduct the study three human subjects were
   selected, a living person, a 25-year-old female, and two forensic cases
   admitted for postmortem examination at the Department of Forensic
   Medicine, Hradec Kralove, Czech Republic (both 63-year-old males), one
   dead to traumatic, self-inflicted, injuries (suicide by hanging), the
   other diagnosed with the heart failure.
   All three cases were photographed in 3608 manner with a Nikon 7000
   digital camera and simultaneously documented with the handheld scanner.
   In addition to having recorded the pre-autopsy phase of the forensic
   cases, both techniques were employed in various stages of autopsy. The
   sets of collected digital images (approximately 100 per case) were
   further processed to generate point clouds and 3D meshes. Final 3D
   models (a pair per individual) were counted for numbers of points and
   polygons, then assessed visually and compared quantitatively using ICP
   alignment algorithm and a cloud point comparison technique based on
   closest point to point distances.
   Both techniques were proven to be easy to handle and equally laborious.
   While collecting the images at autopsy took around 20 min, the
   post-processing was much more time-demanding and required up to 10 h of
   computation time. Moreover, for the full-body scanning the
   post-processing of the handheld scanner required rather time-consuming
   manual image alignment. In all instances the applied approaches produced
   high-resolution photorealistic, real sized or easy to calibrate 3D
   surface models. Both methods equally failed when the scanned body
   surface was covered with body hair or reflective moist areas. Still, it
   can be concluded that single camera close range photogrammetry and
   optical surface scanning using Vectra H1 scanner represent relatively
   low-cost solutions which were shown to be beneficial for postmortem body
   documentation in forensic pathology. (C) 2015 Elsevier Ireland Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.forsciint.2015.03.005}},
ISSN = {{0379-0738}},
EISSN = {{1872-6283}},
ORCID-Numbers = {{Urbanova, Petra/0000-0001-9321-3360}},
Unique-ID = {{ISI:000352441700012}},
}

@article{ ISI:000350839700012,
Author = {Tamrin, K. F. and Rahmatullah, B. and Samuri, S. M.},
Title = {{An experimental investigation of three-dimensional particle aggregation
   using digital holographic microscopy}},
Journal = {{OPTICS AND LASERS IN ENGINEERING}},
Year = {{2015}},
Volume = {{68}},
Pages = {{93-103}},
Month = {{MAY}},
Abstract = {{The tendency of particles to aggregate depends on particle-particle and
   particle-fluid interactions. These interactions can be characterized but
   it requires accurate 3D measurements of particle distributions. We
   introduce the application of an off-axis digital holographic microscopy
   for measuring distributions of dense micrometer (2 mu m) particles in a
   liquid solution. We demonstrate that digital holographic microscopy is
   capable of recording the instantaneous 3D position of particles in a
   flow volume. A new reconstruction method that aids identification of
   particle images was used in this work. About 62\% of the expected number
   of particles within the interrogated flow volume was detected. Based on
   the 3D position of individual particles, the tendency of particle to
   aggregate is investigated. Results show that relatively few particles
   (around 5-10 of a cohort of 1500) were aggregates. This number did not
   change significantly with time. (C) 2014 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.optlaseng.2014.12.011}},
ISSN = {{0143-8166}},
EISSN = {{1873-0302}},
Unique-ID = {{ISI:000350839700012}},
}

@article{ ISI:000349271500019,
Author = {Azazi, Amal and Lotfi, Syaheerah Lebai and Venkat, Ibrahim and
   Fernandez-Martinez, Fernando},
Title = {{Towards a robust affect recognition: Automatic facial expression
   recognition in 3D faces}},
Journal = {{EXPERT SYSTEMS WITH APPLICATIONS}},
Year = {{2015}},
Volume = {{42}},
Number = {{6}},
Pages = {{3056-3066}},
Month = {{APR 15}},
Abstract = {{Facial expressions are a powerful tool that communicates a person's
   emotional state and subsequently his/her intentions. Compared to 2D face
   images, 3D face images offer more granular cues that are not available
   in the 2D images. However, one major setback of 3D faces is that they
   impose a higher dimensionality than 2D faces. In this paper, we attempt
   to address this problem by proposing a fully automatic 3D facial
   expression recognition model that tackles the high dimensionality
   problem in a twofold solution. First, we transform the 3D faces into the
   2D plane using conformal mapping. Second, we propose a Differential
   Evolution (DE) based optimization algorithm to select the optimal facial
   feature set and the classifier parameters simultaneously. The optimal
   features are selected from a pool of Speed Up Robust Features (SURF)
   descriptors of all the prospective facial points. The proposed model
   yielded an average recognition accuracy of 79\% using the Bosphorus
   database and 79.36\% using the BU-3DFE database. In addition, we exploit
   the facial muscular movements to enhance the probability estimation (PE)
   of Support Vector Machine (SVM). Joint application of feature selection
   with the proposed enhanced PE (EPE) yielded an average recognition
   accuracy of 84\% using the Bosphorus database and 85.81\% using the
   BU-3DFE database, which is statistically significantly better (at p <
   0.01 and p < 0.001, respectively) if compared to the individual exploit
   of the optimal features only. (C) 2014 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.eswa.2014.10.042}},
ISSN = {{0957-4174}},
EISSN = {{1873-6793}},
ResearcherID-Numbers = {{Fernandez-Martinez, Fernando/M-2935-2014
   }},
ORCID-Numbers = {{Fernandez-Martinez, Fernando/0000-0003-3877-0089
   Lebai Lutfi, Syaheerah/0000-0001-7349-0061}},
Unique-ID = {{ISI:000349271500019}},
}

@article{ ISI:000351880000193,
Author = {Amaral, Carlos P. and Simoes, Marco A. and Castelo-Branco, Miguel S.},
Title = {{Neural Signals Evoked by Stimuli of Increasing Social Scene Complexity
   Are Detectable at the Single-Trial Level and Right Lateralized}},
Journal = {{PLOS ONE}},
Year = {{2015}},
Volume = {{10}},
Number = {{3}},
Month = {{MAR 25}},
Abstract = {{Classification of neural signals at the single-trial level and the study
   of their relevance in affective and cognitive neuroscience are still in
   their infancy. Here we investigated the neurophysiological correlates of
   conditions of increasing social scene complexity using 3D human models
   as targets of attention, which may also be important in autism research.
   Challenging single-trial statistical classification of EEG neural
   signals was attempted for detection of oddball stimuli with increasing
   social scene complexity. Stimuli had an oddball structure and were as
   follows: 1) flashed schematic eyes, 2) simple 3D faces flashed between
   averted and non-averted gaze (only eye position changing), 3) simple 3D
   faces flashed between averted and non-averted gaze (head and eye
   position changing), 4) animated avatar alternated its gaze direction to
   the left and to the right (head and eye position), 5) environment with 4
   animated avatars all of which change gaze and one of which is the target
   of attention. We found a late (> 300 ms) neurophysiological oddball
   correlate for all conditions irrespective of their complexity as
   assessed by repeated measures ANOVA. We attempted single-trial detection
   of this signal with automatic classifiers and obtained a significant
   balanced accuracy classification of around 79\%, which is noteworthy
   given the amount of scene complexity. Lateralization analysis showed a
   specific right lateralization only for more complex realistic social
   scenes. In sum, complex ecological animations with social content elicit
   neurophysiological events which can be characterized even at the
   single-trial level. These signals are right lateralized. These finding
   paves the way for neuroscientific studies in affective neuroscience
   based on complex social scenes, and given the detectability at the
   single trial level this suggests the feasibility of brain computer
   interfaces that can be applied to social cognition disorders such as
   autism.}},
DOI = {{10.1371/journal.pone.0121970}},
Article-Number = {{e0121970}},
ISSN = {{1932-6203}},
ResearcherID-Numbers = {{Amaral, Carlos/J-4282-2019
   Castelo-Branco, Miguel/F-3866-2019
   }},
ORCID-Numbers = {{Amaral, Carlos/0000-0002-0493-9192
   Castelo-Branco, Miguel/0000-0003-4364-6373
   Simoes, Marco/0000-0003-3713-2464}},
Unique-ID = {{ISI:000351880000193}},
}

@article{ ISI:000354036100022,
Author = {Werschler, W. Philip and Fagien, Steven and Thomas, Jane and
   Paradkar-Mitragotri, Deepali and Rotunda, Adam and Beddingfield, III,
   Frederick C.},
Title = {{Development and Validation of a Photographic Scale for Assessment of Lip
   Fullness}},
Journal = {{AESTHETIC SURGERY JOURNAL}},
Year = {{2015}},
Volume = {{35}},
Number = {{3}},
Pages = {{294-307}},
Month = {{MAR}},
Abstract = {{Background: As lip augmentation becomes more popular, validated measures
   of lip fullness for quantification of outcomes are needed.
   Objective: Develop a scale for rating lip fullness and establish its
   reliability and sensitivity for assessing clinically meaningful
   differences.
   Methods: The initial Allergan Lip Fullness Scale (iLFS; a four-point
   photographic scale with verbal descriptions) was validated by eight
   physicians rating 55 live subjects during two rounds, conducted on one
   day. In addition, subjects performed self-evaluations. The revised
   Allergan Lip Fullness Scale (LFS), a five-point scale with a broader
   range of lip presentations, was validated by 21 clinicians in two online
   image rating sessions, >= 14 days apart, in which they used the LFS to
   rate overall, upper, and lower lip fullness of 144 3-dimensional (3D)
   images. Physician inter-and intra-rater agreement, subject intra-rater
   agreement (iLFS), and subject-physician agreement (iLFS) were evaluated.
   Additionally, during online rating session 1, raters ranked 38 pairs of
   3D images, taken before and after lip augmentation, as ``clinically
   different{''} or ``not clinically different.{''} The median LFS score
   difference for clinically different pairs was calculated to determine
   the clinically meaningful difference.
   Results: Clinician inter-and intra-rater agreement for the iLFS and LFS
   was substantial to almost perfect. Subject self-assessments (iLFS) had
   substantial intra-rater reliability and a high level of agreement with
   physician assessments. Median LFS score differences for overall, upper,
   and lower lip fullness were 1 (mean: 0.63-0.69) for ``clinically
   different{''} and 0 (mean: 0.28-0.36) for ``not clinically different{''}
   image pairs; thus, clinical significance of a 1-point difference in LFS
   score was established.
   Conclusions: The LFS is a reliable instrument for physician
   classification of lip fullness. A 1-point score difference can detect
   clinically meaningful differences in lip fullness.}},
DOI = {{10.1093/asj/sju025}},
ISSN = {{1090-820X}},
EISSN = {{1527-330X}},
Unique-ID = {{ISI:000354036100022}},
}

@article{ ISI:000351796000002,
Author = {Ming, Yue},
Title = {{Robust regional bounding spherical descriptor for 3D face recognition
   and emotion analysis}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2015}},
Volume = {{35}},
Pages = {{14-22}},
Month = {{MAR}},
Abstract = {{3D face recognition and emotion analysis play important roles in many
   fields of communication and edutainment An effective facial descriptor,
   with higher discriminating capability for face recognition and higher
   descriptiveness for facial emotion analysis, is a challenging issue.
   However, in the practical applications, the descriptiveness and
   discrimination are independent and contradictory to each other. 3D
   facial data provide a promising way to balance these two aspects. In
   this paper, a robust regional bounding spherical descriptor (RBSR) is
   proposed to facilitate 3D face recognition and emotion analysis. In our
   framework, we first segment a group of regions on each 3D facial point
   cloud by shape index and spherical bands on the human face. Then the
   corresponding facial areas are projected to regional bounding spheres to
   obtain our regional descriptor. Finally, a regional and global
   regression mapping (RGRM) technique is employed to the weighted regional
   descriptor for boosting the classification accuracy. Three largest
   available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the
   performance comparison and the experimental results show a consistently
   better performance for 3D face recognition and emotion analysis. (C)
   2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2014.12.003}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000351796000002}},
}

@article{ ISI:000351134600001,
Author = {Mengesha, Taye and Hawkins, Michael and Nieuwenhuis, Maarten},
Title = {{Validation of terrestrial laser scanning data using conventional forest
   inventory methods}},
Journal = {{EUROPEAN JOURNAL OF FOREST RESEARCH}},
Year = {{2015}},
Volume = {{134}},
Number = {{2}},
Pages = {{211-222}},
Month = {{MAR}},
Abstract = {{The application of terrestrial laser scanning (TLS) in capturing forest
   inventory parameters such as diameter at breast height, height and
   diameters along stem profiles, and in monitoring forest growth, was
   investigated and validated by comparison with conventionally measured
   individual tree parameters and plot-level forest growth in a stand of
   Sitka spruce (Picea sitchensis (Bong.) Carr.) in Ireland. The data
   acquisition for all the plots with different tree sizes and different
   slopes was carried out using a terrestrial laser scanner (FARO LS 800
   HE80) in November 2007 and November 2009, using the same plot centres
   and measurement procedures. The point cloud data were processed with
   Autostem (TM) software. The results showed that TLS enables the
   acquisition of forest stand parameters with an acceptable accuracy.
   Pruning of the lower branches did not improve tree recognition and the
   number of (partly) occluded trees stayed the same. Over the 2-year
   period, the average difference between the volume increment of the trees
   visible to the scanner derived using the conventional method and
   Autostem (TM) was 4.77 m(3) ha(-1) and resulted in scanner-derived
   estimates that were lower than the estimates obtained by conventional
   method by 6.1 \%. Using a simple correction factor to account for
   occlusion in the laser scanner data, the difference between these
   estimates for all trees in the stand became an over-estimation by 6.96
   m(3) ha(-1) (8.1 \%). At heights up along the stems > 15 m, the errors
   in stem diameter estimates started to escalate.}},
DOI = {{10.1007/s10342-014-0844-0}},
ISSN = {{1612-4669}},
EISSN = {{1612-4677}},
Unique-ID = {{ISI:000351134600001}},
}

@article{ ISI:000349608400002,
Author = {Chen, Junfen and Liao, Iman Yi and Belaton, Bahari and Zaman, Munir},
Title = {{A neural network-based point registration method for 3D rigid face image}},
Journal = {{WORLD WIDE WEB-INTERNET AND WEB INFORMATION SYSTEMS}},
Year = {{2015}},
Volume = {{18}},
Number = {{2}},
Pages = {{197-214}},
Month = {{MAR}},
Abstract = {{Intelligent detection of human face image combined with the real-time
   video monitoring has been applied to improve the secure and protective
   possibility. The registration is an indispensible step before
   distinguishing the variation among the images. Neural network (NN) has a
   strong learning ability from a mass unstructured point cloud even
   containing noisy data. Neural network has been applied to register 3D
   reconstructed ear data and 3D surface of bunny and to achieve the better
   results. Motivated by this idea, NN-based registration method for 3D
   rigid face image is proposed. This paper presented the proof process of
   obtaining rotation matrix and translation vector according to the
   training process of neural network. Then the measure index of
   registration performance was provided. The elaborate experiments were
   conducted on the 3D USF face database (provided by the University of
   South Florida) to verify the effectiveness of neural network as a
   registration method. Next, two comparisons were performed, one group was
   NN-based and ICP-based registration methods and the other group was our
   proposed NN-based and other NN-based registration methods. The
   experimental results showed that neural network is a robust and
   potential registration method for rigid face image registration.
   Furthermore, our proposed NN-based registration method is extended
   easily to do 2D-to-3D registration and non-rigid face registration.}},
DOI = {{10.1007/s11280-013-0213-9}},
ISSN = {{1386-145X}},
EISSN = {{1573-1413}},
ResearcherID-Numbers = {{belaton, bahari/G-4858-2010}},
ORCID-Numbers = {{belaton, bahari/0000-0002-9099-1498}},
Unique-ID = {{ISI:000349608400002}},
}

@article{ ISI:000349588900008,
Author = {Bolkart, Timo and Wuhrer, Stefanie},
Title = {{3D faces in motion: Fully automatic registration and statistical
   analysis}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2015}},
Volume = {{131}},
Pages = {{100-115}},
Month = {{FEB}},
Abstract = {{This paper presents a representation of 3D facial motion sequences that
   allows performing statistical analysis of 3D face shapes in motion. The
   resulting statistical analysis is applied to automatically generate
   realistic facial animations and to recognize dynamic facial expressions.
   To perform statistical analysis of 3D facial shapes in motion over
   different subjects and different motion sequences, a large database of
   motion sequences needs to be brought in full correspondence. Existing
   algorithms that compute correspondences between 3D facial motion
   sequences either require manual input or suffer from instabilities
   caused by drift. For large databases, algorithms that require manual
   interaction are not practical. We propose an approach to robustly
   compute correspondences between a large set of facial motion sequences
   in a fully automatic way using a multilinear model as statistical prior.
   In order to register the motion sequences, a good initialization is
   needed. We obtain this initialization by introducing a landmark
   prediction method for 3D motion sequences based on Markov Random Fields.
   Using this motion sequence registration, we find a compact
   representation of each motion sequence consisting of one vector of
   coefficients for identity and a high dimensional curve for expression.
   Based on this representation, we synthesize new motion sequences and
   perform expression recognition. We show experimentally that the obtained
   registration is of high quality, where 56\% of all vertices are at
   distance at most 1 mm from the input data, and that our synthesized
   motion sequences look realistic. (C) 2014 Elsevier Inc. All rights
   reserved.}},
DOI = {{10.1016/j.cviu.2014.06.013}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
Unique-ID = {{ISI:000349588900008}},
}

@article{ ISI:000349308600007,
Author = {Li, Ye and Wang, YingHui and Wang, BingBo and Sui, LianSheng},
Title = {{Nose tip detection on three-dimensional faces using pose-invariant
   differential surface features}},
Journal = {{IET COMPUTER VISION}},
Year = {{2015}},
Volume = {{9}},
Number = {{1}},
Pages = {{75-84}},
Month = {{FEB}},
Abstract = {{Three-dimensional (3D) facial data offer the potential to overcome the
   difficulties caused by the variation of head pose and illumination in 2D
   face recognition. In 3D face recognition, localisation of nose tip is
   essential to face normalisation, face registration and pose correction
   etc. Most of the existing methods of nose tip detection on 3D face deal
   mainly with frontal or near-frontal poses or are rotation sensitive.
   Many of them are training-based or model-based. In this study, a novel
   method of nose tip detection is proposed. Using pose-invariant
   differential surface features - high-order and low-order curvatures, it
   can detect nose tip on 3D faces under various poses automatically and
   accurately. Moreover, it does not require training and does not depend
   on any particular model. Experimental results on GavabDB verify the
   robustness and accuracy of the proposed method.}},
DOI = {{10.1049/iet-cvi.2014.0070}},
ISSN = {{1751-9632}},
EISSN = {{1751-9640}},
Unique-ID = {{ISI:000349308600007}},
}

@article{ ISI:000349229600034,
Author = {Qiu, Wu and Yuan, Jing and Ukwatta, Eranga and Fenster, Aaron},
Title = {{Rotationally resliced 3D prostate TRUS segmentation using convex
   optimization with shape priors}},
Journal = {{MEDICAL PHYSICS}},
Year = {{2015}},
Volume = {{42}},
Number = {{2}},
Pages = {{877-891}},
Month = {{FEB}},
Abstract = {{Purpose: Efficient and accurate segmentations of 3D end-firing
   transrectal ultrasound (TRUS) images play an important role in planning
   of 3D TRUS guided prostate biopsy. However, poor image quality of the
   input 3D TRUS images, such as strong imaging artifacts and speckles,
   often makes it a challenging task to extract the prostate boundaries
   accurately and efficiently.
   Methods: In this paper, the authors propose a novel convex
   optimization-based approach to delineate the prostate surface from a
   given 3D TRUS image, which reduces the original 3D segmentation problem
   to a sequence of simple 2D segmentation subproblems over the rotational
   reslices of the 3D TRUS volume. Essentially, the authors introduce a
   novel convex relaxation-based contour evolution approach to each 2D
   slicewise image segmentation with the joint optimization of shape
   information, where the learned 2D nonlinear statistical shape prior is
   incorporated to segment the initial slice, its result is propagated as a
   shape constraint to the segmentation of the following slices. In
   practice, the proposed segmentation algorithm is implemented on a GPU to
   achieve the high computational performance.
   Results: Experimental results using 30 patient 3D TRUS images show that
   the proposed method can achieve a mean Dice similarity coefficient of
   93.4\% +/- 2.2\% in 20 s for one 3D image, outperforming the existing
   local-optimization-based methods, e.g., level-set and active-contour, in
   terms of accuracy and efficiency. In addition, inter-and intraobserver
   variability experiments show its good reproducibility.
   Conclusions: A semiautomatic segmentation approach is proposed and
   evaluated to extract the prostate boundary from 3D TRUS images acquired
   by a 3D end-firing TRUS guided prostate biopsy system. Experimental
   results suggest that it may be suitable for the clinical use involving
   the image guided prostate biopsy procedures. (C) 2015 American
   Association of Physicists in Medicine.}},
DOI = {{10.1118/1.4906129}},
ISSN = {{0094-2405}},
ResearcherID-Numbers = {{FENSTER, Aaron/K-4337-2013
   Yuan, Jing/E-8080-2015
   de Ribaupierre, Sandrine/B-7707-2015
   }},
ORCID-Numbers = {{Yuan, Jing/0000-0002-4312-7023
   de Ribaupierre, Sandrine/0000-0001-7096-2289
   Ukwatta, Eranga/0000-0003-0180-4716}},
Unique-ID = {{ISI:000349229600034}},
}

@article{ ISI:000345480900012,
Author = {Guo, Yulan and Sohel, Ferdous and Bennamoun, Mohammed and Wan, Jianwei
   and Lu, Min},
Title = {{A novel local surface feature for 3D object recognition under clutter
   and occlusion}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2015}},
Volume = {{293}},
Pages = {{196-213}},
Month = {{FEB 1}},
Abstract = {{This paper presents a highly distinctive local surface feature called
   the TriSI feature for recognizing 3D objects in the presence of clutter
   and occlusion. For a feature point, we first construct a unique and
   repeatable Local Reference Frame (LRF) using the implicit geometrical
   information of neighboring triangular faces. We then generate three
   signatures from the three orthogonal coordinate axes of the LRF. These
   signatures are concatenated and then compressed into a TriSI feature.
   Finally, we propose an effective 3D object recognition algorithm based
   on hierarchical feature matching. We tested our TriSI feature on two
   popular datasets. Rigorous experimental results show that the TriSI
   feature was highly descriptive and outperformed existing algorithms
   under all levels of Gaussian noise, Laplacian noise, shot noise, varying
   mesh resolutions, occlusion, and clutter. Moreover, we tested our
   TriSI-based 3D object recognition algorithm on four standard datasets.
   The experimental results show that our algorithm achieved the best
   overall recognition results on these datasets. (C) 2014 Elsevier Inc.
   All rights reserved.}},
DOI = {{10.1016/j.ins.2014.09.015}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
ResearcherID-Numbers = {{Sohel, Ferdous/C-2428-2013
   }},
ORCID-Numbers = {{Sohel, Ferdous/0000-0003-1557-4907
   Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000345480900012}},
}

@inproceedings{ ISI:000381569900051,
Author = {Betta, G. and Capriglione, D. and Corvino, M. and Gasparetto, M. and
   Zappa, E. and Liguori, C. and Paolillo, A.},
Book-Group-Author = {{IEEE}},
Title = {{A proposal for improving the performance of face recognition systems
   based on 3d features}},
Booktitle = {{2015 18TH AISEM ANNUAL CONFERENCE}},
Year = {{2015}},
Note = {{18th AISEM Annual Conference on Sensors and Microsystems, Trento, ITALY,
   FEB 03-05, 2015}},
Organization = {{IEEE; Fondazione Bruno Kessler; Univ Trento; Italian Assoc Sensors \&
   Microsystems}},
Abstract = {{In this paper a suitable methodology for the improvement of the
   reliability of results in classification systems based on 3D images is
   proposed. More in detail, it is based on the knowledge of the
   uncertainty of the features constituting the 3D image (obtained
   processing a pair of two 2D stereoscopic images) and on a suitable
   statistical approach providing a confidence level to the classification
   result. These pieces of information are then managed in order to improve
   the classification performance in terms of correct classification and
   missed classification percentages. The experimental results, obtained
   applying the methodology on an Active Appearance Models algorithm, a
   popular method for face recognition based on 3D features, show that,
   compared with a traditional approach (which generally does not take into
   account the uncertainty on 3D features), the proposed methodology allows
   to significantly improve the classification performance even in
   scenarios characterized by a high uncertainty.}},
ISBN = {{978-1-4799-8591-3}},
Unique-ID = {{ISI:000381569900051}},
}

@inproceedings{ ISI:000382327100024,
Author = {Wang, Yuanyuan and Zhu, Xiao Xiang},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane, G}},
Title = {{SEMANTIC INTERPRETATION OF INSAR ESTIMATES USING OPTICAL IMAGES WITH
   APPLICATION TO URBAN INFRASTRUCTURE MONITORING}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W3}},
Pages = {{153-160}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{Synthetic aperture radar interferometry (InSAR) has been an established
   method for long term large area monitoring. Since the launch of
   meter-resolution spaceborne SAR sensors, the InSAR community has shown
   that even individual buildings can be monitored in high level of detail.
   However, the current deformation analysis still remains at a primitive
   stage of pixel-wise motion parameter inversion and manual identification
   of the regions of interest. We are aiming at developing an automatic
   urban infrastructure monitoring approach by combining InSAR and the
   semantics derived from optical images, so that the deformation analysis
   can be done systematically in the semantic/object level. This paper
   explains how we transfer the semantic meaning derived from optical image
   to the InSAR point clouds, and hence different semantic classes in the
   InSAR point cloud can be automatically extracted and monitored. Examples
   on bridges and railway monitoring are demonstrated.}},
DOI = {{10.5194/isprsarchives-XL-3-W3-153-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000382327100024}},
}

@inproceedings{ ISI:000382327100086,
Author = {Gorte, Ben and Elberink, Sander Oude and Sirmacek, Beril and Wang, Jinhu},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane, G}},
Title = {{IQPC 2015 TRACK: TREE SEPARATION AND CLASSIFICATION IN MOBILE MAPPING
   LIDAR DATA}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W3}},
Pages = {{607-612}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{The European FP7 project IQmulus yearly organizes several processing
   contests, where submissions are requested for novel algorithms for point
   cloud and other big geodata processing. This paper describes the set-up
   and execution of a contest having the purpose to evaluate
   state-of-the-art algorithms for Mobile Mapping System point clouds, in
   order to detect and identify (individual) trees. By the nature of MMS
   these are trees in the vicinity of the road network (rather than in
   forests). Therefore, part of the challenge is distinguishing between
   trees and other objects, such as buildings, street furniture, cars etc.
   Three submitted segmentation and classification algorithms are thus
   evaluated.}},
DOI = {{10.5194/isprsarchives-XL-3-W3-607-2015}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Elberink, Sander Oude/D-3829-2009
   }},
ORCID-Numbers = {{Wang, Jinhu/0000-0001-7473-4152
   Gorte, Bernardus/0000-0001-8953-6394}},
Unique-ID = {{ISI:000382327100086}},
}

@inproceedings{ ISI:000380533900053,
Author = {Song, Soohwan and Jo, Sungho},
Editor = {{Kim, JH and Yang, W and Jo, J and Sincak, P and Myung, H}},
Title = {{Traversability Classification Using Super-voxel Method in Unstructured
   Terrain}},
Booktitle = {{ROBOT INTELLIGENCE TECHNOLOGY ANDAPPLICATIONS 3}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{345}},
Pages = {{595-604}},
Note = {{3rd International Conference on Robot Intelligence Technology and
   Applications, Beijing, PEOPLES R CHINA, NOV 06-08, 2014}},
Abstract = {{Estimating the traversability of terrain in an unstructured outdoor
   environment is one of the challenging issues in autonomous vehicles.
   When dealing with a large 3D point cloud, the computational cost of
   processing all of the individual points is very high. Thus voxelization
   methods are used extensively. In this paper, we propose a more
   fine-grained voxelization algorithm in the context of unstructured
   terrain classification. While the current shape of a voxel is a
   fixed-length cubic, we construct a flexible shape voxel which has
   spatial and geometrical properties. Furthermore, we propose a new shape
   histogram feature that represents the statistical characteristics of 3D
   points. The proposed method was tested using data obtained from
   unstructured outdoor environments for performance evaluation.}},
DOI = {{10.1007/978-3-319-16841-8\_53}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-16841-8; 978-3-319-16840-1}},
Unique-ID = {{ISI:000380533900053}},
}

@inproceedings{ ISI:000382326300052,
Author = {Shahzad, M. and Zhu, X. X.},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Rottensteiner, F and Yang, M and Christophe, S and Coltekin, A and Bredif, M}},
Title = {{RECONSTRUCTION OF BUILDING FOOTPRINTS USING SPACEBORNE TOMOSAR POINT
   CLOUDS}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{II-3}},
Number = {{W5}},
Pages = {{385-392}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{This paper presents an approach that automatically (but parametrically)
   reconstructs 2-D/3-D building footprints using 3-D synthetic aperture
   radar (SAR) tomography (TomoSAR) point clouds. These point clouds are
   generated by processing SAR image stacks via SAR tomographic inversion.
   The proposed approach reconstructs the building outline by exploiting
   both the roof and facade points. Initial building footprints are derived
   by applying the alpha shapes method on pre-segmented point clusters of
   individual buildings. A recursive angular deviation based refinement is
   then carried out to obtain refined/smoothed 2-D polygonal boundaries. A
   robust fusion framework then fuses the information pertaining to
   building facades to the smoothed polygons. Afterwards, a rectilinear
   building identification procedure is adopted and constraints are added
   to yield geometrically correct and visually aesthetic building shapes.
   The proposed approach is illustrated and validated using TomoSAR point
   clouds generated from a stack of TerraSAR-X high-resolution spotlight
   images from ascending orbit covering approximately 1.5 km(2) area in the
   city of Berlin, Germany.}},
DOI = {{10.5194/isprsannals-II-3-W5-385-2015}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000382326300052}},
}

@inproceedings{ ISI:000380558900020,
Author = {Pang, Guan and Qiu, Rongqi and Huang, Jing and You, Suya and Neumann,
   Ulrich},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Industrial Point Cloud Modeling and Recognition}},
Booktitle = {{2015 14th IAPR International Conference on Machine Vision Applications
   (MVA)}},
Year = {{2015}},
Pages = {{22-25}},
Note = {{14th IAPR International Conference on Machine Vision Applications (MVA),
   Tokyo, JAPAN, MAY 18-22, 2015}},
Organization = {{MVA Organisation; IAPR TC-8; National Institute of Advanced Industrial
   Science and Technology (AIST); The Telecommunications Advancement
   Foundation; KDDI Foundation}},
Abstract = {{3D modeling of point clouds is an important but time-consuming process,
   inspiring extensive research in automatic methods. Prior efforts focus
   on primitive geometry, street structures or indoor objects, but
   industrial data has rarely been pursued. Our work presents a method for
   automatic modeling and recognition of 3D industrial site point clouds,
   dividing the task into 3 separate sub-problems: pipe modeling, plane
   classification, and object recognition. The results are integrated to
   obtain the complete model, revealing some issues during the integration,
   solved by utilizing information gained from each individual process.
   Experiments show that the presented method automatically models large
   and complex industrial scenes with a quality that outperforms leading
   commercial modeling software and is comparable to professional hand-made
   models.}},
ISBN = {{978-4-9011-2214-6}},
Unique-ID = {{ISI:000380558900020}},
}

@inproceedings{ ISI:000380475300697,
Author = {Arora, Sourabh and Chawla, Shikha},
Book-Group-Author = {{IEEE}},
Title = {{An Intensified Approach to Face Recognition through Average Half Face}},
Booktitle = {{2015 ANNUAL IEEE INDIA CONFERENCE (INDICON)}},
Series = {{Annual IEEE India Conference}},
Year = {{2015}},
Note = {{12 IEEE Int C Elect Energy Env Communications Computer Control, New
   Delhi, INDIA, DEC 17-20, 2015}},
Abstract = {{Face recognition has broad excitement in the latest trend in image
   processing. Face recognition refers to identify a specific individual in
   digital image by analyzing and comparing patterns. It has numerous
   benefits which attract every sector but there are some issues such as
   more time consumption and lesser accuracy which degrade the user
   services. To solve this problem we proposed a highly accurate and fast
   method to reduce the execution time. The proposed method uses average
   half face approach because overall system's accuracy is better in it
   rather than using the original full face image. The proposed method can
   be used to recognize both 2D and 3D images. It mainly includes the
   average half face creation, feature detection, full face recognition
   through average half face using distance metrics and finally checking
   system's accuracy along with time consumption. The proposed method is
   based on eye, nose and mouth detection.}},
ISSN = {{2325-940X}},
ISBN = {{978-1-4673-6540-6}},
Unique-ID = {{ISI:000380475300697}},
}

@inproceedings{ ISI:000380475300183,
Author = {Suja, P. and Krishnasri, D. and Tripathi, Shikha},
Book-Group-Author = {{IEEE}},
Title = {{Pose Invariant Method for Emotion Recognition from 3D Images}},
Booktitle = {{2015 ANNUAL IEEE INDIA CONFERENCE (INDICON)}},
Series = {{Annual IEEE India Conference}},
Year = {{2015}},
Note = {{12 IEEE Int C Elect Energy Env Communications Computer Control, New
   Delhi, INDIA, DEC 17-20, 2015}},
Abstract = {{Information about the emotional state of a person can be inferred from
   facial expressions. Emotion recognition has become an active research
   area in recent years in various fields such as Human Robot Interaction (
   HRI), medicine, intelligent vehicle, etc., The challenges in emotion
   recognition from images with pose variations, motivates researchers to
   explore further. In this paper, we have proposed a method based on
   geometric features, considering images of 7 yaw angles (-45 degrees,-30
   degrees,-15 degrees, 0 degrees,+15 degrees,+30 degrees,+45 degrees) from
   BU3DFE database. Most of the work that has been reported considered only
   positive yaw angles. In this work, we have included both positive and
   negative yaw angles. In the proposed method, feature extraction is
   carried out by concatenating distance and angle vectors between the
   feature points, and classification is performed using neural network.
   The results obtained for images with pose variations are encouraging and
   comparable with literature where work has been performed on pitch and
   yaw angles. Using our proposed method non-frontal views achieve similar
   accuracy when compared to frontal view thus making it pose invariant.
   The proposed method may be implemented for pitch and yaw angles in
   future.}},
ISSN = {{2325-940X}},
ISBN = {{978-1-4673-6540-6}},
Unique-ID = {{ISI:000380475300183}},
}

@inproceedings{ ISI:000380611200033,
Author = {Sano, Masayuki and Matsumoto, Kazuki and Thomas, Bruce H. and Saito,
   Hideo},
Editor = {{Sandor, C and Lindeman, R and Mayol-Cuevas, W and Sakata, N and Newcombe, R and Teichrieb, V}},
Title = {{Rubix: Dynamic Spatial Augmented Reality by Extraction of Plane Regions
   with a RGB-D Camera}},
Booktitle = {{2015 IEEE International Symposium on Mixed and Augmented Reality}},
Year = {{2015}},
Pages = {{148-151}},
Note = {{Proceedings of the 2015 IEEE International Symposium on Mixed and
   Augmented Reality, Los Alamitos, CA, SEP 29-OCT 03, 2015}},
Abstract = {{Dynamic spatial augmented reality requires accurate real-time 3D pose
   information of the physical objects that are to be projected onto.
   Previous depth-based methods for tracking objects required strong
   features to enable recognition; making it difficult to estimate an
   accurate 6DOF pose for physical objects with a small set of recognizable
   features (such as a non-textured cube). We propose a more accurate
   method with fewer limitations for the pose estimation of a tangible
   object that has known planar faces and using depth data from an RGB-D
   camera only. In this paper, the physical object's shape is limited to
   cubes of different sizes. We apply this new tracking method to achieve
   dynamic projections onto these cubes. In our method, 3D points from an
   RGB-D camera are divided into a cluster of planar regions, and the point
   cloud inside each face of the object is fitted to an already-known
   geometric model of a cube. With the 6DOF pose of the physical object,
   SAR generated imagery is then projected correctly onto the physical
   object. The 6DOF tracking is designed to support tangible interactions
   with the physical object. We implemented example interactive
   applications with one or multiple cubes to show the capability of our
   method.}},
DOI = {{10.1109/ISMAR.2015.43}},
ISBN = {{978-1-4673-7660-0}},
Unique-ID = {{ISI:000380611200033}},
}

@inproceedings{ ISI:000380605400006,
Author = {Rai, Marwa Chendeb E. L. and Werghi, Naoufel and Al Muhairi, Hassan and
   Alsafar, Habiba},
Book-Group-Author = {{IEEE}},
Title = {{Using facial images for the diagnosis of genetic syndromes: A survey}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, SIGNAL PROCESSING, AND
   THEIR APPLICATIONS (ICCSPA'15)}},
Year = {{2015}},
Note = {{2015 International Conference on Communications, Signal Processing, and
   their Applications (ICCSPA'15), Sharjah, U ARAB EMIRATES, FEB 17-19,
   2015}},
Organization = {{Amer Univ Sharjah; IEEE}},
Abstract = {{The analysis of facial appearance is significant to an early diagnosis
   of medical genetic diseases. The fast development of image processing
   and machine learning techniques facilitates the detection of facial
   dysmorphic features. This paper is a survey of the recent studies
   developed for the screening of genetic abnormalities across the facial
   features obtained from two dimensional and three dimensional images.}},
ISBN = {{978-1-4799-6532-8}},
ResearcherID-Numbers = {{Werghi, Naoufel/D-2398-2018}},
ORCID-Numbers = {{Werghi, Naoufel/0000-0002-5542-448X}},
Unique-ID = {{ISI:000380605400006}},
}

@inproceedings{ ISI:000380407300092,
Author = {Pawar, Asmita A. and Patil, Nitin N.},
Book-Group-Author = {{IEEE}},
Title = {{Recognition of 3-D Faces with Missing Parts and Line Scratch Removal
   using New Technique}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)}},
Year = {{2015}},
Note = {{International Conference on Pervasive Computing (ICPC), Pune, INDIA, JAN
   08-10, 2015}},
Organization = {{IEEE Pune Sect; IEEE Comp Soc; Savitribai Phule Pune Univ; IEEE Commun
   Soc Pune Chapter; Sinhgad Inst; Sakal Times}},
Abstract = {{This paper presents an implementation of face recognition, which is a
   very important task of human face identification. Line scratch detection
   in images is a highly challenging situation because of various
   characteristics of this defect. Few characteristics are considered with
   the different texture and geometry of images. We propose a useful
   algorithm for frame-by-frame line scratch detection in face image which
   deals with 3D approach and a filtering of detection. The temporary
   filtering algorithm can be used to remove false detection due to thin
   vertical structures by detecting the scratches on an image. Experimental
   evaluation can be detecting the lines and scratches on a face image and
   they used to solve this difficult approach. Our method is used with
   missing parts in an image. Three-dimensional face recognition is an
   extended method of facial recognition is considered according with the
   geometry and texture of a face. It has been elaborated that 3D face
   recognition methods can provide high accuracy as well as high detection
   with a comparison of 2D recognition. 3D avoids such mismatch effect of
   2D face recognition algorithms. Additionally, most 3D scanners achieve
   both a 3D mesh and the texture of a face image. This allows combining
   the output of pure 3D matches with the more traditional algorithms of 2D
   face recognition, thus producing better performance.}},
ISBN = {{978-1-4799-6272-3}},
Unique-ID = {{ISI:000380407300092}},
}

@inproceedings{ ISI:000380584900016,
Author = {Lin, Xiangguo and Zhang, Jixian},
Editor = {{Zhang, J and Lu, Z and Zeng, Y}},
Title = {{SEGMENTATION-BASED GROUND POINTS DETECTION FROM MOBILE LASER SCANNING
   POINT CLOUD}},
Booktitle = {{IWIDF 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{47}},
Number = {{W4}},
Pages = {{99-102}},
Note = {{International Workshop on Image and Data Fusion (IWIDF), Kona, HI, JUL
   21-23, 2015}},
Abstract = {{In most Mobile Laser Scanning (MLS) applications, filtering is a
   necessary step. In this paper, a segmentation-based filtering method is
   proposed for MLS point cloud, where a segment rather than an individual
   point is the basic processing unit. Particularly, the MLS point cloud in
   some blocks are clustered into segments by a surface growing algorithm,
   then the object segments are detected and removed. A segment-based
   filtering method is employed to detect the ground segments. Two MLS
   point cloud datasets are used to evaluate the proposed method.
   Experiments indicate that, compared with the classic progressive TIN
   (Triangulated Irregular Network) densification algorithm, the proposed
   method is capable of reducing the omission error, the commission error
   and total error by 3.62\%, 7.87\% and 5.54\% on average, respectively.}},
DOI = {{10.5194/isprsarchives-XL-7-W4-99-2015}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000380584900016}},
}

@inproceedings{ ISI:000380388000083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 4}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380388000083}},
}

@inproceedings{ ISI:000380388000012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 4}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380388000012}},
}

@inproceedings{ ISI:000380379900083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 1}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380379900083}},
}

@inproceedings{ ISI:000380379900012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 1}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380379900012}},
}

@inproceedings{ ISI:000380393900083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 2}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380393900083}},
}

@inproceedings{ ISI:000380393900012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 2}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380393900012}},
}

@inproceedings{ ISI:000380377400083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 3}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380377400083}},
}

@inproceedings{ ISI:000380377400012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 3}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380377400012}},
}

@inproceedings{ ISI:000380390600083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 5}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380390600083}},
}

@inproceedings{ ISI:000380390600012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 5}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380390600012}},
}

@inproceedings{ ISI:000380483800005,
Author = {Liu, Jian and Zhang, Quan and Zhang, Chen and Tang, Chaojing},
Book-Group-Author = {{IEEE}},
Title = {{ROBUST NOSE TIP DETECTION FOR FACE RANGE IMAGES BASED ON LOCAL FEATURES
   IN SCALE- SPACE}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON 3D IMAGING (IC3D)}},
Series = {{International Conference on 3D Imaging}},
Year = {{2015}},
Note = {{International Conference on 3D Imaging (IC3D), Liege, BELGIUM, DEC
   14-15, 2015}},
Organization = {{The Inst of Elect and Elect Engn; Signal Proc Soc; 3D Stereo Media}},
Abstract = {{Being the most distinct feature point in 3D facial landmarks, nose tip
   plays a significant role in 3D facial studies such as face detection,
   face recognition, facial features extraction, face alignment, etc.
   Successful detection of nose tip can facilitate many tasks of 3D facial
   studies. In this paper, we propose a novel method to detect nose tip
   robustly. The method is robust to noise, needs not training, can handle
   large rotations and occlusions. To reduce computational cost, we first
   remove small isolated regions from the input range image, then establish
   scale-space by robust smoothing the preprocessed range image. In each
   scale of the scale-space, the Multi-angle Energy (ME) of each point is
   computed and sorted in descending order. Then the first. points in the
   descending order list are obtained and hierarchical clustering method is
   used to cluster these points. In the first h largest clusters, we can
   find one point with the largest ME. For all scales of the scale-space,
   we get a series of such points which are treated as nose tip candidates.
   For these candidates, we apply hierarchical clustering again. In the
   obtained largest cluster, we compute the mean value of ME. The ME of
   nose tip will be closest to the mean value. We evaluate our method in
   two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The
   experimental results verify the robustness of our method with a high
   nose tip detection rate.}},
ISSN = {{2379-1772}},
ISBN = {{978-1-5090-1265-7}},
Unique-ID = {{ISI:000380483800005}},
}

@inproceedings{ ISI:000378887900113,
Author = {Polewski, Przemyslaw and Yao, Wei and Heurich, Marco and Krzystek, Peter
   and Stilla, Uwe},
Book-Group-Author = {{IEEE}},
Title = {{Active learning approach to detecting standing dead trees from ALS point
   clouds combined with aerial infrared imagery}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   WORKSHOPS (CVPRW)}},
Series = {{IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops}},
Year = {{2015}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{Due to their role in certain essential forest processes, dead trees are
   an interesting object of study within the environmental and forest
   sciences. This paper describes an active learning-based approach to
   detecting individual standing dead trees, known as snags, from ALS point
   clouds and aerial color infrared imagery. We first segment individual
   trees within the 3D point cloud and subsequently find an approximate
   bounding polygon for each tree within the image. We utilize these
   polygons to extract features based on the pixel intensity values in the
   visible and infrared bands, which forms the basis for classifying the
   associated trees as either dead or living. We define a two-step scheme
   of selecting a small subset of training examples from a large initially
   unlabeled set of objects. In the first step, a greedy approximation of
   the kernelized feature matrix is conducted, yielding a smaller pool of
   the most representative objects. We then perform active learning on this
   moderate-sized pool, using expected error reduction as the basic method.
   We explore how the use of semi-supervised classifiers with minimum
   entropy regularizers can benefit the learning process. Based on
   validation with reference data manually labeled on images from the
   Bavarian Forest National Park, our method attains an overall accuracy of
   up to 89\% with less than 100 training examples, which corresponds to
   10\% of the pre-selected data pool.}},
ISSN = {{2160-7508}},
ISBN = {{978-1-4673-6759-2}},
ResearcherID-Numbers = {{Heurich, Marco/O-4653-2014
   Yao, Wei/J-7423-2019}},
ORCID-Numbers = {{Heurich, Marco/0000-0003-0051-2930
   Yao, Wei/0000-0001-7704-0615}},
Unique-ID = {{ISI:000378887900113}},
}

@inproceedings{ ISI:000376674000399,
Author = {Sleiman, J. Bou and Perraud, J. B. and Bousquet, B. and Palka, N. and
   Guillet, J. P. and Mounaix, P.},
Book-Group-Author = {{IEEE}},
Title = {{Chemical imaging and quantification of RDX/PETN mixtures by PLS applied
   on terahertz time-domain spectroscopy}},
Booktitle = {{2015 40TH INTERNATIONAL CONFERENCE ON INFRARED, MILLIMETER AND TERAHERTZ
   WAVES (IRMMW-THZ)}},
Series = {{International Conference on Infrared Millimeter and Terahertz Waves}},
Year = {{2015}},
Note = {{40th International Conference on Infrared, Millimeter, and Terahertz
   Waves (IRMMW-THz), Chinese Univ Hong Kong, Hong Kong, PEOPLES R CHINA,
   AUG 23-28, 2015}},
Organization = {{IEEE; IEEE Microwave Theory \& Tech Soc; Virginal Diodes Inc; TeraView;
   Microtech Instruments Inc; Hong Kong Univ Sci \& Technol, Dept Elect \&
   Comp Engn; Croucher Fdn; Capital Normal Univ; K C Wong Educ Fdn;
   Meetings \& Exhibit Hong Kong; Army Res Off; NSF}},
Abstract = {{Chemometric analysis was applied on terahertz absorbance 3D images, in
   transmission. The goal is to automatically discriminate some explosives
   on images and quantify mixtures of RDX/PETN in the frequency range of
   0.2 - 3 THz. Partial Least Square (PLS) was applied on THz absorbance
   multispectral images to quantify individual product inside pure samples
   and mixtures at each pixel on the image. Then the best score obtained is
   used to display the samples' images and provide the optimal frequencies
   combination for recognition purpose.}},
ISSN = {{2162-2027}},
ISBN = {{978-1-4799-8272-1}},
ResearcherID-Numbers = {{Palka, Norbert/G-9652-2018
   Mounaix, Patrick/E-1653-2012}},
ORCID-Numbers = {{Palka, Norbert/0000-0002-1931-876X
   }},
Unique-ID = {{ISI:000376674000399}},
}

@article{ ISI:000376676700001,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Title = {{Wavelet and decision fusion-based 3D face recognition from range image}},
Journal = {{INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{2}},
Number = {{4}},
Pages = {{306-324}},
Abstract = {{The pivotal purpose of this literature is to describe a new approach for
   3D faces recognition in the presence of pose, expression, as well as
   illumination based on fusion of wavelet coefficients. In addition,
   authors have investigated the recognition rates with series of
   experiments by ANN and K-NN. To demonstrate the robustness of the
   recognition system, Frav3D face dataset has been considered for this
   investigation. The series of variations in classifiers and their
   performance accuracies have also ranked using Wilcoxon signed-rank
   method based on their recognition rates. Range face images from
   synthesised database are processed by the Haar wavelet transform, and
   corresponding subimages are created for final fused face dataset. The
   synthesised database is created by collecting frontal face images along
   with images obtained after registration of rotated images using ERFI
   model. Moreover, to discover the features for face recognition, PCA is
   applied on fused face images. Finally, two supervised classifiers
   namely, ANN and K-NN are tested for recognition purpose. The obtained
   maximum recognition rate from our proposed methodology is 96.25\% using
   ANN classifier and 90\% of recognition rate from K-NN.}},
DOI = {{10.1504/IJAPR.2015.075942}},
ISSN = {{2049-887X}},
EISSN = {{2049-8888}},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000376676700001}},
}

@inproceedings{ ISI:000371977803080,
Author = {Batabyal, Tamal and Vaccari, Andrea and Acton, Scott T.},
Book-Group-Author = {{IEEE}},
Title = {{UGraSP: A UNIFIED FRAMEWORK FOR ACTIVITY RECOGNITION AND PERSON
   IDENTIFICATION USING GRAPH SIGNAL PROCESSING}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{3270-3274}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{With the growing availability and wide distribution of low-cost,
   high-performance 3D imaging sensors, the image analysis community has
   witnessed an increased demand for solutions to the challenges of
   activity recognition and person identification. We propose an integrated
   framework, based on graph signal processing, that simultaneously
   performs both tasks using a single set of features. The novelty of our
   approach is based on the fact that the set of features used for activity
   recognition accommodates person identification without additional
   computation. The analysis is based on the extracted structure-invariant
   graph (skeleton). The Laplacian of the skeleton is used both to identify
   the person and recognize the performed activity. While person
   identification is achieved directly from the analysis of the Laplacian,
   activity recognition is obtained after transformation, into the graph
   spectral domain, of the vectorized form of the skeletal joints 3D
   coordinates. Feature vectors for activity recognition are then derived,
   in this domain, from the covariance matrices evaluated over fixed-length
   sequential video segments. Both classification tasks are implemented
   using linear support vector machines (SVM). When applied to real
   activity datasets, our approach shows an improved performance over the
   existing state-of-the-art.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977803080}},
}

@inproceedings{ ISI:000371977804130,
Author = {Arteaga, Reynaldo J. and Ruuth, Steven J.},
Book-Group-Author = {{IEEE}},
Title = {{LAPLACE-BELTRAMI SPECTRA FOR SHAPE COMPARISON OF SURFACES IN 3D USING
   THE CLOSEST POINT METHOD}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{4511-4515}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{The need to compare separate objects arises in a wide range of
   applications. In one approach for comparing objects, `ShapeDNA' is
   constructed to give a numerical fingerprint representing an individual
   object. Shape-DNA is a cropped set of eigenvalues of the
   Laplace-Beltrami operator for the surface of the object. In this paper,
   we compute the Shape-DNA of surfaces using the closest point method. Our
   approach may be applied to a variety of surface representations
   including triangulations, point clouds and certain analytical shapes. A
   2D multidimensional scaling plot illustrates that similar objects form
   groups based on the Shape-DNAs. Our method has the benefit that it may
   be applied to surfaces defined by dense point clouds without requiring
   the construction of point connectivity.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977804130}},
}

@inproceedings{ ISI:000373207200086,
Author = {Gutfeter, Weronika and Pacut, Andrzej},
Editor = {{Jedrzejowicz, P and Nguyen, NT and Hong, TP and Czarnowski, I}},
Title = {{Face 3D biometrics goes mobile: searching for applications of portable
   depth sensor in face recognition}},
Booktitle = {{2015 IEEE 2ND INTERNATIONAL CONFERENCE ON CYBERNETICS (CYBCONF)}},
Year = {{2015}},
Pages = {{489-494}},
Note = {{IEEE 2nd International Conference on Cybernetics (CYBCONF), Gdynia,
   POLAND, JUN 24-26, 2015}},
Organization = {{IEEE; IEEE Poland Sect; Gdynia Maritime Univ; IEEE Syst, Man, \&
   Cybernet Soc; Gdynia Maritime Univ Students \& Alumni Fdn; IEEE SMC Tech
   Comm Computat Collect Intelligence; Polish Acad Sci, Comm Informat;
   Polish Soc Artificial Intelligence}},
Abstract = {{This paper presents an acquisition procedure and method of processing
   spatial images for face recognition with the use of a novel type of
   scanning device, namely mobile depth sensor Structure. Depth sensors,
   often called RGBD cameras, are able to deliver 3D images with a frame
   rate 30-60 frames per second, however they have relatively low
   resolution and a high level of noise. This kind of data is compared here
   with a high quality scans enrolled by the structural light scanner, for
   which the acquisition time is approximately 1.5 s for a single image,
   and which - because of its size - cannot be classified as a portable
   device. The purpose of this work was to find the method that will allow
   us to extract spatial features from mobile data sources analyzed here
   only in a static context. We transform the 3D data into local surface
   features and then into vectors of unified length by use of the Moving
   Least Squares method applied to a predefined grid of points on a
   reference cylinder. The feature matrices were calculated for various
   image features, and used in PCA analysis. Finally, the verification
   errors were calculated and compared to those obtained for stationary
   devices. The results show that single-image mobile sensor images lead to
   the results inferior to those of stationary sensors. However, we suggest
   a dynamic depth stream processing as the next step in the evolution of
   the described method. The presented results show that by including
   multi-frame processing into our method, it is likely to gain the
   accuracy similar to those obtained for a stationary device under
   controlled laboratory conditions.}},
ISBN = {{978-1-4799-8322-3}},
Unique-ID = {{ISI:000373207200086}},
}

@inproceedings{ ISI:000352725200030,
Author = {Shahzad, M. and Schmitt, M. and Zhu, X. X.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{SEGMENTATION AND CROWN PARAMETER EXTRACTION OF INDIVIDUAL TREES IN AN
   AIRBORNE TOMOSAR POINT CLOUD}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W2}},
Pages = {{205-209}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{The analysis of individual trees is an important field of research in
   the forest remote sensing community. While the current state-of-the-art
   mostly focuses on the exploitation of optical imagery and airborne LiDAR
   data, modern SAR sensors have not yet met the interest of the research
   community in that regard. This paper describes how several critical
   parameters of individual deciduous trees can be extraced from airborne
   multi-aspect TomoSAR point clouds: First, the point cloud is segmented
   by unsupervised mean shift clustering. Then ellipsoid models are fitted
   to the points of each cluster. Finally, from these 3D ellipsoids the
   geometrical tree parameters location, height and crown radius are
   extracted. Evaluation with respect to a manually derived reference
   dataset prove that almost 86\% of all trees are localized, thus
   providing a promising perspective for further research towards
   individual tree recognition from SAR data.}},
DOI = {{10.5194/isprsarchives-XL-3-W2-205-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000352725200030}},
}

@inproceedings{ ISI:000352725200038,
Author = {Vetrivel, A. and Gerke, M. and Kerle, N. and Vosselman, G.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{Segmentation of UAV-based images incorporating 3D point cloud
   information}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W2}},
Pages = {{261-268}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{Numerous applications related to urban scene analysis demand automatic
   recognition of buildings and distinct sub-elements. For example, if
   LiDAR data is available, only 3D information could be leveraged for the
   segmentation. However, this poses several risks, for instance, the
   in-plane objects cannot be distinguished from their surroundings. On the
   other hand, if only image based segmentation is performed, the geometric
   features (e.g., normal orientation, planarity) are not readily
   available. This renders the task of detecting the distinct sub-elements
   of the building with similar radiometric characteristic infeasible. In
   this paper the individual sub-elements of buildings are recognized
   through sub-segmentation of the building using geometric and radiometric
   characteristics jointly. 3D points generated from Unmanned Aerial
   Vehicle (UAV) images are used for inferring the geometric
   characteristics of roofs and facades of the building. However, the
   image-based 3D points are noisy, error prone and often contain gaps.
   Hence the segmentation in 3D space is not appropriate. Therefore, we
   propose to perform segmentation in image space using geometric features
   from the 3D point cloud along with the radiometric features. The initial
   detection of buildings in 3D point cloud is followed by the segmentation
   in image space using the region growing approach by utilizing various
   radiometric and 3D point cloud features. The developed method was tested
   using two data sets obtained with UAV images with a ground resolution of
   around 1-2 cm. The developed method accurately segmented most of the
   building elements when compared to the plane-based segmentation using 3D
   point cloud alone.}},
DOI = {{10.5194/isprsarchives-XL-3-W2-261-2015}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Vosselman, George/D-3985-2009
   Gerke, Markus/A-8791-2012
   Kerle, Norman/A-5508-2010}},
ORCID-Numbers = {{Vosselman, George/0000-0001-8813-8028
   Gerke, Markus/0000-0002-2221-6182
   }},
Unique-ID = {{ISI:000352725200038}},
}

@inproceedings{ ISI:000370974903006,
Author = {Linder, Timm and Wehner, Sven and Arras, Kai O.},
Book-Group-Author = {{IEEE}},
Title = {{Real-Time Full-Body Human Gender Recognition in (RGB)-D Data}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)}},
Series = {{IEEE International Conference on Robotics and Automation ICRA}},
Year = {{2015}},
Pages = {{3039-3045}},
Note = {{IEEE International Conference on Robotics and Automation (ICRA),
   Seattle, WA, MAY 26-30, 2015}},
Organization = {{IEEE}},
Abstract = {{Understanding social context is an important skill for robots that share
   a space with humans. In this paper, we address the problem of
   recognizing gender, a key piece of information when interacting with
   people and understanding human social relations and rules. Unlike
   previous work which typically considered faces or frontal body views in
   image data, we address the problem of recognizing gender in RGB-D data
   from side and back views as well. We present a large, gender-balanced,
   annotated, multi-perspective RGB-D dataset with full-body views of over
   a hundred different persons captured with both the Kinect v1 and Kinect
   v2 sensor. We then learn and compare several classifiers on the Kinect
   v2 data using a HOG baseline, two state-of-the-art deep-learning
   methods, and a recent tessellation-based learning approach. Originally
   developed for person detection in 3D data, the latter is able to learn
   the best selection, location and scale of a set of simple point cloud
   features. We show that for gender recognition, it outperforms the other
   approaches for both standing and walking people while being very
   efficient to compute with classification rates up to 150 Hz.}},
ISSN = {{1050-4729}},
ISBN = {{978-1-4799-6923-4}},
Unique-ID = {{ISI:000370974903006}},
}

@inproceedings{ ISI:000370814200017,
Author = {Bull, Geoff and Gao, Junbin and Antolovich, Michael},
Editor = {{Battiato, S and Coquillart, S and Pettre, J and Laramee, RS and Kerren, A and Braz, J}},
Title = {{Rock Fragment Boundary Detection Using Compressed Random Features}},
Booktitle = {{COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS - THEORY AND
   APPLICATIONS, VISIGRAPP 2014}},
Series = {{Communications in Computer and Information Science}},
Year = {{2015}},
Volume = {{550}},
Pages = {{273-286}},
Note = {{International Joint Conference on Computer Vision, Imaging and Computer
   Graphics Theory and Applications (VISIGRAPP), Lisbon, PORTUGAL, JAN
   05-08, 2014}},
Organization = {{Inst Syst \& Technologies Informat, Control \& Commun; Eurographics;
   IEEE Comp Soc; IEEE VGMT; IEEE TCMC}},
Abstract = {{Sections of the mining industry depend on regular analysis of rock
   fragmentation to detect trends that may affect safety or production. The
   limitations inherent in 2D imaging analysis mean that human input is
   typically needed for delineating individual rock fragments. Although
   recent advances in 3D image processing have diminished the need for
   human input, it is often infeasible for many mines to upgrade their
   existing 2D imaging systems to 3D. Hence there is still a need to
   improve delineation in 2D images. This paper proposes a method for
   delineating rock fragments by classifying compressed Haar-like features
   extracted from small image patches. The optimum size of the image
   patches and the number of compressed features are determined
   empirically. Experimental results show the proposed method gives
   superior results to the commonly used watershed algorithm, and
   compressing features improves computational efficiency such that a
   machine learning approach is practical.}},
DOI = {{10.1007/978-3-319-25117-2\_17}},
ISSN = {{1865-0929}},
ISBN = {{978-3-319-25117-2; 978-3-319-25116-5}},
ResearcherID-Numbers = {{Bull, Geoff/L-2805-2018
   Antolovich, Michael/C-1656-2012
   Gao, Junbin/A-1766-2009}},
ORCID-Numbers = {{Bull, Geoff/0000-0002-9818-5132
   Antolovich, Michael/0000-0003-2601-8332
   Gao, Junbin/0000-0001-9803-0256}},
Unique-ID = {{ISI:000370814200017}},
}

@inproceedings{ ISI:000369099700018,
Author = {Aneja, D. and Vora, S. R. and Camci, E. D. and Shapiro, L. G. and Cox,
   T. C.},
Editor = {{Traina, C and Rodrigues, PP and Kane, B and Marques, PMD and Traina, AJM}},
Title = {{Automated Detection of 3D Landmarks for the Elimination of
   Non-Biological Variation in Geometric Morphometric Analyses}},
Booktitle = {{2015 IEEE 28TH INTERNATIONAL SYMPOSIUM ON COMPUTER-BASED MEDICAL SYSTEMS
   (CBMS)}},
Series = {{IEEE International Symposium on Computer-Based Medical Systems}},
Year = {{2015}},
Pages = {{78-83}},
Note = {{28th IEEE International Symposium on Computer-Based Medical Systems
   (CBMS), Univ Sao Paulo, Sao Paulo, BRAZIL, JUN 22-25, 2015}},
Organization = {{IEEE; IEEE Comp Soc; ICMC; SUS; Ministerio Saude; Governo Fed Patria
   Educadora Brasil; CNPq; Google Brasil; FAPESP}},
Abstract = {{Landmark-based morphometric analyses are used by anthropologists,
   developmental and evolutionary biologists to understand shape and size
   differences (eg. in the cranioskeleton) between groups of specimens. The
   standard, labor intensive approach is for researchers to manually place
   landmarks on 3D image datasets. As landmark recognition is subject to
   inaccuracies of human perception, digitization of landmark coordinates
   is typically repeated (often by more than one person) and the mean
   coordinates are used. In an attempt to improve efficiency and
   reproducibility between researchers, we have developed an algorithm to
   locate landmarks on CT mouse hemi-mandible data. The method is evaluated
   on 3D meshes of 28-day old mice, and results compared to landmarks
   manually identified by experts. Quantitative shape comparison between
   two inbred mouse strains demonstrate that data obtained using our
   algorithm also has enhanced statistical power when compared to data
   obtained by manual landmarking.}},
DOI = {{10.1109/CBMS.2015.86}},
ISSN = {{1063-7125}},
ISBN = {{978-1-4673-6775-2}},
Unique-ID = {{ISI:000369099700018}},
}

@inproceedings{ ISI:000364991200038,
Author = {De Giorgis, Nikolas and Rocca, Luigi and Puppo, Enrico},
Editor = {{Murino, V and Puppo, E}},
Title = {{Scale-Space Techniques for Fiducial Points Extraction from 3D Faces}},
Booktitle = {{IMAGE ANALYSIS AND PROCESSING - ICIAP 2015, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9279}},
Pages = {{421-431}},
Note = {{18th International Conference on Image Analysis and Processing (ICIAP),
   Genoa, ITALY, SEP 07-11, 2015}},
Organization = {{Datalogic; Google Inc; Centro Studi Gruppo Orizzonti Holding; Ansaldo
   Energia; EBIT Esaote; Softeco; eVS embedded Vis Syst S r l; 3DFlow S r
   l; Camelot Biomed Syst S r l; Ist Italiano Tecnologia, Pattern Anal \&
   Comp Vis Dept; Univ Genova; Univ Verona; Camera Commercio Genova; Comune
   Genova}},
Abstract = {{We propose a method for extracting fiducial points from human faces that
   uses 3D information only and is based on two key steps: multi-scale
   curvature analysis, and the reliable tracking of features in a
   scale-space based on curvature. Our scale-space analysis, coupled to
   careful use of prior information based on variability boundaries of
   anthropometric facial proportions, does not require a training step,
   because it makes direct use of morphological characteristics of the
   analyzed surface. The proposed method precisely identifies important
   fiducial points and is able to extract new fiducial points that were
   previously unrecognized, thus paving the way to more effective
   recognition algorithms.}},
DOI = {{10.1007/978-3-319-23231-7\_38}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-23231-7; 978-3-319-23230-0}},
ORCID-Numbers = {{Puppo, Enrico/0000-0001-9780-5283}},
Unique-ID = {{ISI:000364991200038}},
}

@inproceedings{ ISI:000365181700036,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal, JK}},
Title = {{Range Face Image Registration Using ERFI from 3D Images}},
Booktitle = {{PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF
   INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{328}},
Pages = {{323-333}},
Note = {{3rd International Conference on Frontiers in Intelligent Computing -
   Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014}},
Organization = {{Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol \& Sci, CSI Student
   Branch}},
Abstract = {{In this paper, we present a novel and robust approach for 3D faces
   registration based on Energy Range Face Image (ERFI). ERFI is the
   frontal face model for the individual people from the database. It can
   be considered as a mean frontal range face image for each person. Thus,
   the total energy of the frontal range face images has been preserved by
   ERFI. For registration purpose, an interesting point or a land mark,
   which is the nose tip (or `pronasal') from face surface is extracted.
   Then, this landmark is exploited to correct the oriented faces by
   applying the 3D geometrical rotation technique with respect to the ERFI
   model for registration purpose. During the error calculation phase,
   Manhattan distance metric between the localized `pronasal' landmark on
   face image and that of ERFI model is determined on Euclidian space. The
   accuracy is quantified with selection of cut-points `T' on measured
   Manhattan distances along yaw, pitch and roll. The proposed method has
   been tested on Frav3D database and achieved 82.5\% accurate pose
   registration.}},
DOI = {{10.1007/978-3-319-12012-6\_36}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-12012-6; 978-3-319-12011-9}},
ResearcherID-Numbers = {{Bhattacharjee, Debotosh/L-8521-2015
   }},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-4483-706X
   Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000365181700036}},
}

@inproceedings{ ISI:000365181700047,
Author = {Sivasankar, C. and Srinivasan, A.},
Editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal, JK}},
Title = {{A Framework for Human Recognition Based on Locomotive Object Extraction}},
Booktitle = {{PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF
   INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{328}},
Pages = {{431-439}},
Note = {{3rd International Conference on Frontiers in Intelligent Computing -
   Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014}},
Organization = {{Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol \& Sci, CSI Student
   Branch}},
Abstract = {{Moving Object detection based on video, of late has gained momentum in
   the field of research. Moving object detection has extensive application
   areas and is used for monitoring intelligence interaction between human
   and computer, transportation of intelligence, and navigating visual
   robotics, clarity in steering systems. It is also used in various other
   fields for diagnosing, compressing images, reconstructing 3D images,
   retrieving video images and so on. Since surveillance of human movement
   detection is subjective, the human objects are precisely detected to the
   framework proposed for human detection based on the Locomotive Object
   Extraction. The issue of illumination changes and crowded human image is
   discriminated. The image is detected through the detection feature that
   identifies head and shoulder and is the loci for the proposed framework.
   The detection of individual objects has been revamped appreciably over
   the recent years but even now environmental factors and crowd-scene
   detection remains significantly difficult for detection of moving
   object. The proposed framework subtracts the background through Gaussian
   mixture model and the area of significance is extracted. The area of
   significance is transformed to white and black picture by picture
   binarization. Then, Wiener filter is employed to scale the background
   level for optimizing the results of the object in motion. The object is
   finally identified. The performance in every stage is measured and is
   evaluated. The result in each stage is compared and the performance of
   the proposed framework is that of the existing system proves
   satisfactory.}},
DOI = {{10.1007/978-3-319-12012-6\_47}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-12012-6; 978-3-319-12011-9}},
Unique-ID = {{ISI:000365181700047}},
}

@inproceedings{ ISI:000363756900031,
Author = {Ming, Yue and Jin, Yi},
Editor = {{Liu, H and Kubota, N and Zhu, X and Dillmann, R and Zhou, D}},
Title = {{Robust 3D Local SIFT Features for 3D Face Recognition}},
Booktitle = {{INTELLIGENT ROBOTICS AND APPLICATIONS (ICIRA 2015), PT III}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2015}},
Volume = {{9246}},
Pages = {{352-359}},
Note = {{8th International Conference on Intelligent Robotics and Applications
   (ICIRA), Portsmouth, ENGLAND, AUG 24-27, 2015}},
Abstract = {{In this paper, a robust 3D local SIFT feature is proposed for 3D face
   recognition. For preprocessing the original 3D face data, facial
   regional segmentation is first employed by fusing curvature
   characteristics and shape band mechanism. Then, we design a new local
   descriptor for the extracted regions, called 3D local Scale-Invariant
   Feature Transform (3D LSIFT). The key point detection based on 3D LSIFT
   can effectively reflect the geometric characteristic of 3D facial
   surface by encoding the gray and depth information captured by 3D face
   data. Then, 3D LSIFT descriptor extends to describe the discrimination
   on 3D faces. Experimental results based on the common international 3D
   face databases demonstrate the higher-qualified performance of our
   proposed algorithm with effectiveness, robustness, and universality.}},
DOI = {{10.1007/978-3-319-22873-0\_31}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-22873-0; 978-3-319-22872-3}},
Unique-ID = {{ISI:000363756900031}},
}

@inproceedings{ ISI:000362452500020,
Author = {Fu, Junsheng and Kamarainen, Joni-Kristian and Buch, Anders Glent and
   Kruger, Norbert},
Editor = {{Jawahar, CV and Shan, S}},
Title = {{Indoor Objects and Outdoor Urban Scenes Recognition by 3D Visual
   Primitives}},
Booktitle = {{COMPUTER VISION - ACCV 2014 WORKSHOPS, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9008}},
Pages = {{270-285}},
Note = {{12th Asian Conference on Computer Vision (ACCV), Singapore, SINGAPORE,
   NOV 01-05, 2014}},
Organization = {{Singapore Tourism Board; Omron; Nvidia; Garena; Samsung; Adobe; ViSenze;
   Lee Fdn; Morpx; Microsoft Res; NICTA}},
Abstract = {{Object detection, recognition and pose estimation in 3D images have
   gained momentum due to availability of 3D sensors (RGB-D) and increase
   of large scale 3D data, such as city maps. The most popular approach is
   to extract and match 3D shape descriptors that encode local scene
   structure, but omits visual appearance. Visual appearance can be
   problematic due to imaging distortions, but the assumption that local
   shape structures are sufficient to recognise objects and scenes is
   largely invalid in practise since objects may have similar shape, but
   different texture (e.g., grocery packages). In this work, we propose an
   alternative appearance-driven approach which first extracts 2D
   primitives justified by Marr's primal sketch, which are
   ``accumulated{''} over multiple views and the most stable ones are
   ``promoted{''} to 3D visual primitives. The 3D promoted primitives
   represent both structure and appearance. For recognition, we propose a
   fast and effective correspondence matching using random sampling. For
   quantitative evaluation we construct a semisynthetic benchmark dataset
   using a public 3D model dataset of 119 kitchen objects and another
   benchmark of challenging street-view images from 4 different cities. In
   the experiments, our method utilises only a stereo view for training. As
   the result, with the kitchen objects dataset our method achieved almost
   perfect recognition rate for +/- 10 degrees camera view point change and
   nearly 80\% for +/- 20 degrees, and for the street-view benchmarks it
   achieved 75\% accuracy for 160 street-view images pairs, 80\% for 96
   street-view images pairs, and 92\% for 48 street-view image pairs.}},
DOI = {{10.1007/978-3-319-16628-5\_20}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-16628-5; 978-3-319-16627-8}},
ResearcherID-Numbers = {{Kruger, Norbert/P-6315-2015
   Buch, Anders/P-4849-2015
   Kamarainen, Joni-Kristian/G-4296-2014}},
ORCID-Numbers = {{Kruger, Norbert/0000-0002-3931-116X
   Buch, Anders/0000-0002-5904-6981
   }},
Unique-ID = {{ISI:000362452500020}},
}

@inproceedings{ ISI:000361841100052,
Author = {Lin, Yizhou and Hua, Gang and Mordohai, Philippos},
Editor = {{Agapito, L and Bronstein, MM and Rother, C}},
Title = {{Egocentric Object Recognition Leveraging the 3D Shape of the Grasping
   Hand}},
Booktitle = {{COMPUTER VISION - ECCV 2014 WORKSHOPS, PT III}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{8927}},
Pages = {{746-762}},
Note = {{13th European Conference on Computer Vision (ECCV), Zurich, SWITZERLAND,
   SEP 06-12, 2014}},
Abstract = {{We present a systematic study on the relationship between the 3D shape
   of a hand that is about to grasp an object and recognition of the object
   to be grasped. In this paper, we investigate the direction from the
   shape of the hand to object recognition for unimpaired users. Our work
   shows that the 3D shape of a grasping hand from an egocentric point of
   view can help improve recognition of the objects being grasped. Previous
   work has attempted to exploit hand interactions or gaze information in
   the egocentric setting to guide object segmentation. However, all such
   analyses are conducted in 2D. We hypothesize that the 3D shape of a
   grasping hand is highly correlated to the physical attributes of the
   object being grasped. Hence, it can provide very beneficial visual
   information for object recognition. We validate this hypothesis by first
   building a 3D, egocentric vision pipeline to segment and reconstruct
   dense 3D point clouds of the grasping hands. Then, visual descriptors
   are extracted from the point cloud and subsequently fed into an object
   recognition system to recognize the object being grasped. Our
   experiments demonstrate that the 3D hand shape can indeed greatly help
   improve the visual recognition accuracy, when compared with the baseline
   where only 2D image features are utilized.}},
DOI = {{10.1007/978-3-319-16199-0\_52}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-16199-0; 978-3-319-16198-3}},
Unique-ID = {{ISI:000361841100052}},
}

@inproceedings{ ISI:000360175900188,
Author = {Naveen, S. and Moni, R. S.},
Editor = {{Samuel, P}},
Title = {{Multimodal Face Recognition System using Spectral Transformation of 2D
   Texture feature and Statistical processing of Face Range Images}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION AND
   COMMUNICATION TECHNOLOGIES, ICICT 2014}},
Series = {{Procedia Computer Science}},
Year = {{2015}},
Volume = {{46}},
Pages = {{1537-1545}},
Note = {{International Conference on Information and Communication Technologies
   (ICICT), Kochi, INDIA, DEC 03-05, 2014}},
Organization = {{Cochin Uni Sci \& Technol, Sch Engn; TEQIP Phase II}},
Abstract = {{3D Face recognition has been an area of interest for the past few
   decades in pattern recognition. This paper focuses on problems of person
   identification using 3D Face data. Here unregistered Face data, i.e.
   both texture and depth is fed to classifier in spectral representations
   of data. 2D Discrete Fourier Transform (DFT) is used for spectral
   representation. Fusion of scores improves the recognition accuracy
   significantly since use of depth information alone in spectral
   representation was not sufficient to increase accuracy. Statistical
   method seems to degrade performance of system when applied to texture
   data and was effective for depth data. (C) 2015 The Authors. Published
   by Elsevier B.V.}},
DOI = {{10.1016/j.procs.2015.02.078}},
ISSN = {{1877-0509}},
Unique-ID = {{ISI:000360175900188}},
}

@inproceedings{ ISI:000359292400017,
Author = {Karagoz, Burcu and Altan, Hakan and Kamburoglu, Kivanc},
Editor = {{Lilge, LD and Sroka, R}},
Title = {{Terahertz pulsed imaging study of dental caries}},
Booktitle = {{MEDICAL LASER APPLICATIONS AND LASER-TISSUE INTERACTIONS VII}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9542}},
Note = {{Conference on Medical Laser Applications and Laser-Tissue Interactions
   VII, Munich, GERMANY, JUN 21-23, 2015}},
Organization = {{SPIE; Opt Soc America}},
Abstract = {{Current diagnostic techniques in dentistry rely predominantly on X-rays
   to monitor dental caries. Terahertz Pulsed Imaging (TPI) has great
   potential for medical applications since it is a nondestructive imaging
   method. It does not cause any ionization hazard on biological samples
   due to low energy of THz radiation. Even though it is strongly absorbed
   by water which exhibits very unique chemical and physical properties
   that contribute to strong interaction with THz radiation, teeth can
   still be investigated in three dimensions. Recent investigations suggest
   that this method can be used in the early identification of dental
   diseases and imperfections in the tooth structure without the hazards of
   using techniques which rely on x-rays. We constructed a continuous wave
   (CW) and time-domain reflection mode raster scan THz imaging system that
   enables us to investigate various teeth samples in two or three
   dimensions. The samples comprised of either slices of individual tooth
   samples or rows of teeth embedded in wax, and the imaging was done by
   scanning the sample across the focus of the THz beam. 2D images were
   generated by acquiring the intensity of the THz radiation at each pixel,
   while 3D images were generated by collecting the amplitude of the
   reflected signal at each pixel. After analyzing the measurements in both
   the spatial and frequency domains, the results suggest that the THz
   pulse is sensitive to variations in the structure of the samples that
   suggest that this method can be useful in detecting the presence of
   caries.}},
DOI = {{10.1117/12.2183673}},
Article-Number = {{95420N}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-707-4}},
ResearcherID-Numbers = {{Altan, Hakan/A-4036-2017}},
ORCID-Numbers = {{Altan, Hakan/0000-0002-2456-8827}},
Unique-ID = {{ISI:000359292400017}},
}

@article{ ISI:000358942000004,
Author = {Dalponte, Michele and Reyes, Francesco and Kandare, Kaja and Gianelle,
   Damiano},
Title = {{Delineation of Individual Tree Crowns from ALS and Hyperspectral data: a
   comparison among four methods}},
Journal = {{EUROPEAN JOURNAL OF REMOTE SENSING}},
Year = {{2015}},
Volume = {{48}},
Pages = {{365-382}},
Abstract = {{In this paper four different delineation methods based on airborne laser
   scanning (ALS) and hyperspectral data are compared over a forest area in
   the Italian Alps. The comparison was carried out in terms of detected
   trees, while the ALS based methods are compared also in terms of
   attributes estimated (e.g. height). From the experimental results
   emerged that ALS methods outperformed hyperspectral one in terms of tree
   detection rate in two of three cases. The best results were achieved
   with a method based on region growing on an ALS image, and by one based
   on clustering of raw ALS point cloud. Regarding the estimates of the
   tree attributes all the ALS methods provided good results with very high
   accuracies when considering only big trees.}},
DOI = {{10.5721/EuJRS20154821}},
ISSN = {{2279-7254}},
ResearcherID-Numbers = {{Gianelle, Damiano/G-9437-2011
   Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Gianelle, Damiano/0000-0001-7697-5793
   Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000358942000004}},
}

@inproceedings{ ISI:000359129800018,
Author = {Lenz, Marcel and Krug, Robin and Jaedicke, Volker and Stroop, Ralf and
   Schmieder, Kirsten and Hofmann, Martin R.},
Editor = {{Bouma, BE and Wojtkowski, M}},
Title = {{Spectral Domain Optical Coherence Tomography for ex vivo brain tumor
   analysis}},
Booktitle = {{OPTICAL COHERENCE IMAGING TECHNIQUES AND IMAGING IN SCATTERING MEDIA}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9541}},
Note = {{Conference on Optical Coherence Imaging Techniques and Imaging in
   Scattering Media, Munich, GERMANY, JUN 21-23, 2015}},
Organization = {{SPIE; Opt Soc}},
Abstract = {{Non-contact imaging methods to distinguish between healthy tissue and
   brain tumor tissue during surgery would be highly desirable but are not
   yet available. Optical Coherence Tomography (OCT) is a non-invasive
   imaging technology with a resolution around 1-15 mu m and a penetration
   depth of 1-2 mm that may satisfy the demands. To analyze its potential,
   we measured ex vivo human brain tumor tissue samples from 10 patients
   with a Spectral Domain OCT system (Thorlabs Callisto: center wavelength
   of 930 nm) and compared the results with standard histology. In detail,
   three different measurements were made for each sample. First the sample
   was measured directly after surgery. Then it was embedded in paraffin
   (also H\&E staining) and examined for the second time. At last, the
   slices of each paraffin block cut by the pathology were measured. Each
   time a B-scan was created and for a better comparison with the histology
   a 3D image was generated, in order to get the corresponding en face
   images. In both, histopathological diagnosis and the analysis of the OCT
   images, different types of brain tumor showed difference in structure.
   This has been affirmed by two blinded investigators. Nevertheless the
   difference between two images of samples taken directly after surgery is
   less distinct. To enhance the contrast in the images further, we employ
   Spectroscopic OCT and pattern recognition algorithms and compare these
   results to the histopathological standard.}},
DOI = {{10.1117/12.2183614}},
Article-Number = {{95411D}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-706-7}},
Unique-ID = {{ISI:000359129800018}},
}

@article{ ISI:000355789400004,
Author = {Zou, Hongyan and Da, Feipeng and Wang, Zhaoyang},
Title = {{A novel 3D face feature based on geometry image vertical shape
   information}},
Journal = {{OPTIK}},
Year = {{2015}},
Volume = {{126}},
Number = {{9-10}},
Pages = {{898-902}},
Abstract = {{A novel and efficient face feature is proposed in this paper. 3D faces
   from the database are preprocessed and mapped to 2D geometry images.
   Then the geometry images are decomposed into wavelet responses by
   multi-scale Gabor transforms. According to analyses and experiments,
   responses that represent vertical shape information are figured out to
   be face feature for recognition. Moreover, the feature extracted by
   multi-scale Haar transforms also obtains satisfying experiment results,
   which prove that the feature is free from the extraction methods.
   Extensive experiments conducted on FRGC(Face Recognition Grand
   Challenge) v2.0 show a satisfactory performance compared with existing
   popular methods. It is also approved that the vertical shape information
   is promising for dealing with face expressions in 3D face recognition.
   (C) 2015 Elsevier GmbH. All rights reserved.}},
DOI = {{10.1016/j.ijleo.2015.02.083}},
ISSN = {{0030-4026}},
Unique-ID = {{ISI:000355789400004}},
}

@inproceedings{ ISI:000355583800011,
Author = {Varney, Nina M. and Asari, Vijayan K.},
Editor = {{Casasent, D and Alam, MS}},
Title = {{Volume component analysis for classification of LiDAR data}},
Booktitle = {{OPTICAL PATTERN RECOGNITION XXVI}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9477}},
Note = {{Conference on Optical Pattern Recognition XXVI, Baltimore, MA, APR
   22-23, 2015}},
Organization = {{SPIE}},
Abstract = {{One of the most difficult challenges of working with LiDAR data is the
   large amount of data points that are produced. Analysing these large
   data sets is an extremely time consuming process. For this reason,
   automatic perception of LiDAR scenes is a growing area of research.
   Currently, most LiDAR feature extraction relies on geometrical features
   specific to the point cloud of interest. These geometrical features are
   scene-specific, and often rely on the scale and orientation of the
   object for classification. This paper proposes a robust method for
   reduced dimensionality feature extraction of 3D objects using a volume
   component analysis (VCA) approach.1
   This VCA approach is based on principal component analysis (PCA). PCA is
   a method of reduced feature extraction that computes a covariance matrix
   from the original input vector. The eigenvectors corresponding to the
   largest eigenvalues of the covariance matrix are used to describe an
   image. Block-based PCA is an adapted method for feature extraction in
   facial images because PCA, when performed in local areas of the image,
   can extract more significant features than can be extracted when the
   entire image is considered. The image space is split into several of
   these blocks, and PCA is computed individually for each block.
   This VCA proposes that a LiDAR point cloud can be represented as a
   series of voxels whose values correspond to the point density within
   that relative location. From this voxelized space, block-based PCA is
   used to analyze sections of the space where the sections, when combined,
   will represent features of the entire 3-D object. These features are
   then used as the input to a support vector machine which is trained to
   identify four classes of objects, vegetation, vehicles, buildings and
   barriers with an overall accuracy of 93.8\%}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-593-3}},
Unique-ID = {{ISI:000355583800011}},
}

@inproceedings{ ISI:000353328200021,
Author = {Qu, Chengchao and Monari, Eduardo and Schuchert, Tobias and Beyerer,
   Juergen},
Editor = {{Lam, EY and Niel, KS}},
Title = {{Realistic Texture Extraction for 3D Face Models Robust to Self-Occlusion}},
Booktitle = {{IMAGE PROCESSING: MACHINE VISION APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9405}},
Note = {{Conference on Image Processing - Machine Vision Applications VIII, San
   Francisco, CA, FEB 10-11, 2015}},
Organization = {{Soc Imaging Sci \& Technol; SPIE}},
Abstract = {{In the context of face modeling, probably the most well-known approach
   to represent 3D faces is the 3D Morphable Model (3DMM). When 3DMM is
   fitted to a 2D image, the shape as well as the texture and illumination
   parameters are simultaneously estimated. However, if real facial texture
   is needed, texture extraction from the 2D image is necessary. This paper
   addresses the possible problems in texture extraction of a single image
   caused by self-occlusion. Unlike common approaches that leverage the
   symmetric property of the face by mirroring the visible facial part,
   which is sensitive to inhomogeneous illumination, this work first
   generates a virtual texture map for the skin area iteratively by
   averaging the color of neighbored vertices. Although this step creates
   unrealistic, overly smoothed texture, illumination stays constant
   between the real and virtual texture. In the second pass, the mirrored
   texture is gradually blended with the real or generated texture
   according to the visibility. This scheme ensures a gentle handling of
   illumination and yet yields realistic texture. Because the blending area
   only relates to non-informative area, main facial features still have
   unique appearance in different face halves. Evaluation results reveal
   realistic rendering in novel poses robust to challenging illumination
   conditions and small registration errors.}},
Article-Number = {{94050P}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-495-0}},
Unique-ID = {{ISI:000353328200021}},
}

@inproceedings{ ISI:000353122200062,
Author = {Xue, Junpeng and Su, Xianyu and Zhang, Qican},
Editor = {{Quan, C and Qian, K and Asundi, A and Chau, FS}},
Title = {{High-speed 3D face measurement based on color speckle projection}},
Booktitle = {{INTERNATIONAL CONFERENCE ON EXPERIMENTAL MECHANICS 2014}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9302}},
Note = {{International Conference on Experimental Mechanics, Singapore,
   SINGAPORE, NOV 15-17, 2014}},
Organization = {{Opt \& Photon Soc Singapore; Theoret \& Appl Mech Soc}},
Abstract = {{Nowadays, 3D face recognition has become a subject of considerable
   interest in the security field due to its unique advantages in domestic
   and international. However, acquiring color-textured 3D faces data in a
   fast and accurate manner is still highly challenging. In this paper, a
   new approach based on color speckle projection for 3D face data dynamic
   acquisition is proposed. Firstly, the projector-camera color crosstalk
   matrix that indicates how much each projector channel influences each
   camera channel is measured. Secondly, the reference-speckle-sets images
   are acquired with CCD, and then three gray sets are separated from the
   color sets using the crosstalk matrix and are saved Finally, the color
   speckle image which is modulated by face is captured, and it is split
   three gray channels. We measure the 3D face using multi-sets of speckle
   correlation methods with color speckle image in high-speed similar as
   one-shot, which greatly improves the measurement accuracy and stability.
   The suggested approach has been implemented and the results are
   supported by experiments.}},
DOI = {{10.1117/12.2076458}},
Article-Number = {{93022Y}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-388-5}},
Unique-ID = {{ISI:000353122200062}},
}

@inproceedings{ ISI:000352727000035,
Author = {Weinmann, M. and Schmidt, A. and Mallet, C. and Hinz, S. and
   Rottensteiner, F. and Jutzi, B.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{CONTEXTUAL CLASSIFICATION OF POINT CLOUD DATA BY EXPLOITING INDIVIDUAL
   3D NEIGBOURHOODS}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. II}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{2-3}},
Number = {{W4}},
Pages = {{271-278}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{The fully automated analysis of 3D point clouds is of great importance
   in photogrammetry, remote sensing and computer vision. For reliably
   extracting objects such as buildings, road inventory or vegetation, many
   approaches rely on the results of a point cloud classification, where
   each 3D point is assigned a respective semantic class label. Such an
   assignment, in turn, typically involves statistical methods for feature
   extraction and machine learning. Whereas the different components in the
   processing workflow have extensively, but separately been investigated
   in recent years, the respective connection by sharing the results of
   crucial tasks across all components has not yet been addressed. This
   connection not only encapsulates the interrelated issues of neighborhood
   selection and feature extraction, but also the issue of how to involve
   spatial context in the classification step. In this paper, we present a
   novel and generic approach for 3D scene analysis which relies on (i)
   individually optimized 3D neighborhoods for (ii) the extraction of
   distinctive geometric features and (iii) the contextual classification
   of point cloud data. For a labeled benchmark dataset, we demonstrate the
   beneficial impact of involving contextual information in the
   classification process and that using individual 3D neighborhoods of
   optimal size significantly increases the quality of the results for both
   pointwise and contextual classification.}},
DOI = {{10.5194/isprsannals-II-3-W4-271-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000352727000035}},
}

@article{ ISI:000347672900002,
Author = {da Neiva, Marcelo Baiao and Soares, Alvaro Cavalheiro and Lisboa,
   Cinthia de Oliveira and Vilella, Oswaldo de Vasconcellos and Motta,
   Alexandre Trindade},
Title = {{Evaluation of cephalometric landmark identification on CBCT multiplanar
   and 3D reconstructions}},
Journal = {{ANGLE ORTHODONTIST}},
Year = {{2015}},
Volume = {{85}},
Number = {{1}},
Pages = {{11-17}},
Month = {{JAN}},
Abstract = {{Objective: To evaluate the reliability of three-dimensional (3D)
   landmark identification in cone-beam computed tomography (CBCT) using
   two different visualization techniques.
   Materials and Methods: Twelve CBCT images were randomly selected. Three
   observers independently repeated three times the identification of 30
   landmarks using 3D reconstructions and 28 landmarks using multiplanar
   views. The values of the coordinates X, Y, and Z of each point were
   obtained and the intraclass correlation coefficient (ICC) was
   calculated.
   Results: The ICC of the 3D visualization was rated >0.90 in 67.76\% and
   45.56\%, and <= 0.45 in 13.33\% and 14.46\% of the intraobserver and
   interobserver assessments, respectively. The ICC of the multiplanar
   visualization was rated >0.90 in 82.16\% and 78.56\% and <= 0.45 in only
   16.7\% and 8.33\% of the intraobserver and interobserver assessments,
   respectively. An individual landmark classification was done according
   to ICC values.
   Conclusions: The frequency of highly reliable values was greater for
   multiplanar than 3D reconstructions. Overall, lower reliability was
   found for points on the condyle and higher reliability for those on the
   midsagittal plane. Depending on the anatomic region, the observer must
   choose the most reliable type of image visualization.}},
DOI = {{10.2319/120413-891.1}},
ISSN = {{0003-3219}},
EISSN = {{1945-7103}},
Unique-ID = {{ISI:000347672900002}},
}

@inproceedings{ ISI:000377348700061,
Author = {Liu, Jian and Zhang, Quan and Tang, Chaojing},
Book-Author = {{Xu, B}},
Title = {{CoMES: A Novel Method for Robust Nose Tip Detection in Face Range Images}},
Booktitle = {{2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION
   CONTROL CONFERENCE (IAEAC)}},
Year = {{2015}},
Pages = {{309-315}},
Note = {{IEEE Advanced Information Technology, Electronic and Automation Control
   Conference (IAEAC), Chongqing, PEOPLES R CHINA, DEC 19-20, 2015}},
Organization = {{IEEE; IEEE Beijing Sect; Global Union Acad Sci \& Technol; Chongqing
   Global Union Acad Sci \& Technol}},
Abstract = {{As the most distinct feature point in facial landmarks, nose tip plays a
   significant role in 3D facial studies. Successful detection of nose tip
   can facilitate many 3D facial studies tasks. In this paper, we propose a
   novel method to detect nose tip robustly. The method is robust to noise,
   need not training, can handle large rotations and occlusions. We first
   remove small isolated connected regions and noise from the input range
   image, then establish scale-space by robust smoothing the preprocessed
   range image. In each scale of the scale-space, we compute multi-angle
   energy of each point, then we use hierarchical clustering method to
   cluster the points whose multi-angle energies are larger than a
   threshold value. In the largest cluster, we can find one point with the
   largest multi-angle energy. For all scales of the scale-space, we get a
   series of such points and apply hierarchical clustering again for these
   points, nose tip will have the largest multi-angle energy in the largest
   cluster. We evaluate our method in FRGC v2.0 3D face database and
   BOSPHORUS 3D face database. The experimental results verify the
   robustness of our method with a high nose tip detection rate.}},
ISBN = {{978-1-4799-1980-2}},
Unique-ID = {{ISI:000377348700061}},
}

@inproceedings{ ISI:000387959204074,
Author = {Gilani, Syed Zulqarnain and Shafait, Faisal and Mian, Ajmal},
Book-Group-Author = {{IEEE}},
Title = {{Shape-based Automatic Detection of a Large Number of 3D Facial Landmarks}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2015}},
Pages = {{4639-4648}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{We present an algorithm for automatic detection of a large number of
   anthropometric landmarks on 3D faces. Our approach does not use texture
   and is completely shape based in order to detect landmarks that are
   morphologically significant. The proposed algorithm evolves level set
   curves with adaptive geometric speed functions to automatically extract
   effective seed points for dense correspondence. Correspondences are
   established by minimizing the bending energy between patches around seed
   points of given faces to those of a reference face. Given its
   hierarchical structure, our algorithm is capable of establishing
   thousands of correspondences between a large number of faces. Finally, a
   morphable model based on the dense corresponding points is fitted to an
   unseen query face for transfer of correspondences and hence automatic
   detection of landmarks. The proposed algorithm can detect any number of
   pre-defined landmarks including subtle landmarks that are even difficult
   to detect manually. Extensive experimental comparison on two benchmark
   databases containing 6, 507 scans shows that our algorithm outperforms
   six state of the art algorithms.}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-6964-0}},
ORCID-Numbers = {{Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000387959204074}},
}

@inproceedings{ ISI:000399132000096,
Author = {Fan, Hang and Zhou, Yangui and Liang, Haowen and Wang, Jiahui and Krebs,
   Peter and Lin, Daikun and Su, Jianbang and Li, Kunyang and Chen, Haiyu
   and Wang, Xiaolu and Zhou, Jianying},
Book-Group-Author = {{IEEE}},
Title = {{Glasses-free 3D display with glasses-assisted quality: key innovations
   for smart directional backlight autostereoscopy}},
Booktitle = {{2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)}},
Year = {{2015}},
Note = {{IEEE Visual Communications and Image Processing (VCIP) Conference,
   SINGAPORE, DEC 13-16, 2015}},
Organization = {{IEEE}},
Abstract = {{A glasses-free 3D display with glasses-assisted quality is presented.
   Self-adaptive algorithm is employed to optimize system parameters, which
   is applied to design the micro structure of lens array and free form
   surface backlight units. Moire contour map based on contrast sensitivity
   function is simulated and is manipulated by ameliorating the period
   ratio and the tilt angle of the superimposed periodical optical
   components. Directional transmissions of multi-users 3D images are
   realized with a finer dynamic synchronized backlight control and a face
   recognition technology. Comfortable viewings are demonstrated for two
   viewers, with full high definition for a single viewing channel. Minimum
   crosstalk as low as 2.3\% is demonstrated over a large viewing volume.}},
ISBN = {{978-1-4673-7314-2}},
Unique-ID = {{ISI:000399132000096}},
}

@article{ ISI:000215156100010,
Author = {Fernandez-Cervantes, Victor and Garcia, Arturo and Antonio Ramos, Marco
   and Mendez, Andres},
Title = {{Facial Geometry Identification through Fuzzy Patterns with RGBD Sensor}},
Journal = {{COMPUTACION Y SISTEMAS}},
Year = {{2015}},
Volume = {{19}},
Number = {{3}},
Pages = {{529-546}},
Abstract = {{Automatic human facial recognition is an important and complicated task;
   it is necessary to design algorithms capable of recognizing the constant
   patterns in the face and to use computing resources efficiently. In this
   paper we present a novel algorithm to recognize the human face in real
   time; the system's input is the depth and color data from the Microsoft
   KinectTM device. The algorithm recognizes patterns/shapes on the point
   cloud topography. The template of the face is based in facial geometry;
   the forensic theory classifies the human face with respect to constant
   patterns: cephalometric points, lines, and areas of the face. The
   topography, relative position, and symmetry are directly related to the
   craniometric points. The similarity between a point cloud cluster and a
   pattern description is measured by a fuzzy pattern theory algorithm. The
   face identification is composed by two phases: the first phase
   calculates the face pattern hypothesis of the facial points, configures
   each point shape, the related location in the areas, and lines of the
   face. Then, in the second phase, the algorithm performs a search on
   these face point configurations.}},
DOI = {{10.13053/CyS-19-3-2015}},
ISSN = {{1405-5546}},
EISSN = {{2007-9737}},
ORCID-Numbers = {{Ramos Corchado, Marco Antonio/0000-0003-3982-6988}},
Unique-ID = {{ISI:000215156100010}},
}

@article{ ISI:000214649800004,
Author = {Krotewicz, Pawel and Sankowski, Wojciech and Nowak, Piotr Stefan},
Title = {{Face recognition based on 2D and 3D data fusion}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMETRICS}},
Year = {{2015}},
Volume = {{7}},
Number = {{1}},
Pages = {{69-81}},
Abstract = {{The aim of the work presented in this paper is to present current state
   of the art of face recognition methods and describe proposal algorithms
   for face biometric identification that analyse 2D face images and 3D
   face geometry scans. Data for analysis gathered via 3D scanner are
   processed through different phases. These are: segmentation phase,
   feature extraction phase and comparison phase. Segmentation relies on
   localising characteristic landmark points of the face and projecting the
   face point cloud onto a plane constructed on the basis of these
   characteristic points. Feature extraction phase calculates separate
   feature vectors for 2D and 3D input data. Comparison phase applies
   fusion of 2D and 3D methods and calculates similarity value between two
   samples. All samples are compared against one another and results
   presented as DET curves are generated. By analysis of DET curves,
   conclusions are formulated.}},
DOI = {{10.1504/IJBM.2015.069505}},
ISSN = {{1755-8301}},
EISSN = {{1755-831X}},
Unique-ID = {{ISI:000214649800004}},
}

@inproceedings{ ISI:000348430000011,
Author = {Nguyen Hong Quy and Nguyen Hoang Quoc and Nguyen Tran Lan Anh and Yang,
   Hyung-Jeong and Pham The Bao},
Editor = {{Camacho, D and Kim, SW and Trawinski, B}},
Title = {{3D Human Face Recognition Using Sift Descriptors of Face's Feature
   Regions}},
Booktitle = {{NEW TRENDS IN COMPUTATIONAL COLLECTIVE INTELLIGENCE}},
Series = {{Studies in Computational Intelligence}},
Year = {{2015}},
Volume = {{572}},
Pages = {{117-126}},
Note = {{6th International Conference on Computational Collective Intelligence
   (ICCCI), Seoul, SOUTH KOREA, SEP 23-26, 2014}},
Abstract = {{Many researches in 3D face recognition problem have been studied because
   of adverse effects of human's age, emotions, and environmental
   conditions on 2D models. In this paper, we propose a novel method for
   recognizing 3D faces. First, a 3D human face is normalized and
   determined regions of interest (ROI). Second, SIFT algorithm is applied
   to these ROIs for detecting invariant feature points. Finally, this
   descriptor, extracted from a training image, will be stored and later
   used to identify the face in a test image. For performing reliable
   recognition, we also adjust parameters of SIFT algorithm to fit own
   characteristics of the template database. In our experiments, the
   proposed method produces promising performance up to 84.6\% of accuracy
   when using 3D Notre Dame biometric data-TEC.}},
DOI = {{10.1007/978-3-319-10774-5\_11}},
ISSN = {{1860-949X}},
ISBN = {{978-3-319-10774-5; 978-3-319-10773-8}},
Unique-ID = {{ISI:000348430000011}},
}

@inproceedings{ ISI:000380453200062,
Author = {Sindhuja, C. and Mala, K.},
Book-Group-Author = {{IEEE}},
Title = {{LANDMARK IDENTIFICATION IN 3D IMAGE FOR FACIAL EXPRESSION RECOGNITION}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTING AND
   COMMUNICATIONS TECHNOLOGIES (ICCCT 15)}},
Year = {{2015}},
Pages = {{338-343}},
Note = {{International Conference on Computing and Communications Technologies
   ((ICCCT), Chennai, INDIA, FEB 26-27, 2015}},
Organization = {{Dept Informat Technol Sri Sai Ram Engn Coll Chennai}},
Abstract = {{Facial expression recognition plays a major role in non verbal
   communication. Recognition by machine is still a challenging problem. To
   automate the recognition for human machine interaction, a system is
   proposed in this paper. The proposed system uses shape descriptors to
   identify twelve land marks which mainly contribute to the facial
   expression recognition. From the location and the size or boundary of
   the land marks by matching with Facial Landmark Model (FLM), basic
   expressions are identified. The experimental results show that the shape
   descriptors and post processing correctly identifies landmarks
   automatically. The architectural distortion of action units is used to
   identify the basic facial expressions and tested on Bosphorous data set.}},
ISBN = {{978-1-4799-7623-2}},
Unique-ID = {{ISI:000380453200062}},
}

@article{ ISI:000344204000007,
Author = {Jo, Jaeik and Choi, Heeseung and Kim, Ig-Jae and Kim, Jaihie},
Title = {{Single-view-based 3D facial reconstruction method robust against pose
   variations}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{48}},
Number = {{1}},
Pages = {{73-85}},
Month = {{JAN}},
Abstract = {{The 3D Morphable Model (3DMM) and the Structure from Motion (SfM)
   methods are widely used for 3D facial reconstruction from 2D single-view
   or multiple-view images. However, model-based methods suffer from
   disadvantages such as high computational costs and vulnerability to
   local minima and head pose variations. The SfM-based methods require
   multiple facial images in various poses. To overcome these
   disadvantages, we propose a single-view-based 3D facial reconstruction
   method that is person-specific and robust to pose variations. Our
   proposed method combines the simplified 3DMM and the SfM methods. First,
   2D initial frontal Facial Feature Points (FFPs) are estimated from a
   preliminary 3D facial image that is reconstructed by the simplified
   3DMM. Second, a bilateral symmetric facial image and its corresponding
   FFPs are obtained from the original side-view image and corresponding
   FFPs by using the mirroring technique. Finally, a more accurate the 3D
   facial shape is reconstructed by the SfM using the frontal, original,
   and bilateral symmetric FFPs. We evaluated the proposed method using
   facial images in 35 different poses. The reconstructed facial images and
   the ground-truth 3D facial shapes obtained from the scanner were
   compared. The proposed method proved more robust to pose variations than
   3DMM. The average 3D Root Mean Square Error (RMSE) between the
   reconstructed and ground-truth 3D faces was less than 2.6 mm when 2D
   FFPs were manually annotated, and less than 3.5 mm when automatically
   annotated. (C) 2014 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2014.07.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000344204000007}},
}
