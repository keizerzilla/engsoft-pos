
@article{ ISI:000463151400027,
Author = {Rasouli, Maryam S. D. and Payandeh, Shahram},
Title = {{A novel depth image analysis for sleep posture estimation}},
Journal = {{JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING}},
Year = {{2019}},
Volume = {{10}},
Number = {{5, SI}},
Pages = {{1999-2014}},
Month = {{MAY}},
Abstract = {{Recognition of sleep posture and its changes are related to information
   monitoring in a number of health-related applications such as apnea
   prevention and elderly care. This paper uses a less privacy-invading
   approach to classify sleep postures of a person in various
   configurations including side and supine postures. In order to
   accomplish this, a single depth sensor has been utilized to collect
   selective depth signals and populated a dataset associated with the
   depth data. The data is then analyzed by a novel frequency-based feature
   selection approach. These extracted features were then correlated in
   order to rank their information content in various 2D scans from the 3D
   point cloud in order to train a support vector machine (SVM). The data
   of subjects are collected under two conditions. First when they were
   covered with a thin blanket and second without any blanket. In order to
   reduce the dimensionality of the feature space, a T-test approach is
   employed to determine the most dominant set of features in the frequency
   domain. The proposed recognition approach based on the frequency domain
   is also compared with an approach using feature vector defined based on
   skeleton joints. The comparative studies are performed given various
   scenarios and by a variety of datasets. Through our study, it is shown
   that our proposed method offers better performance to that of the
   joint-based method.}},
DOI = {{10.1007/s12652-018-0796-1}},
ISSN = {{1868-5137}},
EISSN = {{1868-5145}},
Unique-ID = {{ISI:000463151400027}},
}

@article{ ISI:000458711300008,
Author = {Patruno, Cosimo and Marani, Roberto and Cicirelli, Grazia and Stella,
   Ettore and D'Orazio, Tiziana},
Title = {{People re-identification using skeleton standard posture and color
   descriptors from RGB-D data}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{89}},
Pages = {{77-90}},
Month = {{MAY}},
Abstract = {{This paper tackles the problem of people re-identification by using soft
   biometrics features. The method works on RGB-D data (color point clouds)
   to determine the best matching among a database of possible users. For
   each subject under testing, skeletal information in three-dimensions is
   used to regularize the pose and to create a skeleton standard posture
   (SSP). A partition grid, whose sizes depend on the SSP, groups the
   samples of the point cloud accordingly to their position. Every group is
   then studied to build the person signature. The same grid is then used
   for the other subjects of the database to preserve information about
   possible shape differences among users. The effectiveness of this novel
   method has been tested on three public datasets. Numerical experiments
   demonstrate an improvement of results with reference to the current
   state-of-the-art, with recognition rates of 97.84\% (on a partition of
   BIWI RGBD-ID), 61.97\% (KinectREID) and 89.71\% (RGBD-ID), respectively.
   (C) 2019 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2019.01.003}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{D'Orazio, Tiziana/H-5032-2019}},
ORCID-Numbers = {{D'Orazio, Tiziana/0000-0003-1473-7110}},
Unique-ID = {{ISI:000458711300008}},
}

@article{ ISI:000457666900036,
Author = {Lv, Chenlei and Wu, Zhongke and Wang, Xingce and Zhou, Mingquan and Toh,
   Kar-Ann},
Title = {{Nasal similarity measure of 3D faces based on curve shape space}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{88}},
Pages = {{458-469}},
Month = {{APR}},
Abstract = {{We propose a novel method for measuring the nasal similarity among 3D
   faces. Firstly, we construct a representation for the nose shape, which
   is composed of a set of geodesic curves, each crosses the bridge of the
   nose. Next, using these geodesic curves, we formulate a similarity
   measure to compare among noses in the curve shape space. Under the
   Riemannian framework, the shape space is a quotient space for which the
   scaling, translation and rotation are removed. Since the nose similarity
   measure is based on the shape comparison, the proposed method has the
   following advantages: (1) the similarity measure is robust to facial
   expressions since the nose is not affected by facial expressions; (2)
   the geometric features of the nose shape match well with the human
   perception; (3) the similarity measure is independent of the mesh grid
   because the chosen nose curves are not sensitive to the triangular mesh
   model. We construct a nasal hierarchical structure for noses
   organization which is based on nose similarity measure results. In our
   experiments, we evaluate the performance of the proposed method and
   compare it with competing methods on three public face databases namely,
   FRGC2.0, Texas3D and BosphorusDB. The results show superiority of the
   proposed method in terms of both the speed and the accuracy when the
   nasal measurements are processed in the nasal hierarchical structure and
   the nasal samples with low sampling rate (5\%-25\% of original point
   cloud). (C) 2018 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.12.006}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000457666900036}},
}

@article{ ISI:000457666900042,
Author = {Pribanic, Tomislav and Petkovic, Tomislav and Donlic, Matea},
Title = {{3D registration based on the direction sensor measurements}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{88}},
Pages = {{532-546}},
Month = {{APR}},
Abstract = {{3D registration is a very active topic, spanning research areas such as
   computational geometry, computer graphics and pattern recognition. It
   aims to solve spatial transformation that aligns two point clouds. In
   this work we propose the use of a single direction sensor, such as an
   accelerometer or a magnetometer, commonly available on contemporary
   mobile platforms, such as tablets and smartphones. Both sensors have
   been heavily investigated earlier, but only for joint use with other
   sensors, such as gyroscopes and GPS. We show a time-efficient and
   accurate 3D registration method that takes advantage of only either an
   accelerometer or a magnetometer. We demonstrate a 3D reconstruction of
   individual point clouds and the proposed 3D registration method on a
   tablet equipped with an accelerometer or a magnetometer. However, we
   point out that the proposed method is not restricted to mobile
   platforms. Indeed, it can easily be applied in any 3D measurement system
   that is upgradable with some ubiquitous direction sensor, for example by
   adding a smartphone equipped with either an accelerometer or a
   magnetometer. We compare the proposed method against several
   state-of-the-art methods implemented in the open source Point Cloud
   Library (PCL). The proposed method outperforms the PCL methods tested,
   both in terms of processing time and accuracy. (C) 2018 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.12.008}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Petkovic, Tomislav/0000-0002-3054-002X
   Donlic, Matea/0000-0001-5165-6438}},
Unique-ID = {{ISI:000457666900042}},
}

@article{ ISI:000461603400031,
Author = {Yi, Tang-Tang},
Title = {{Face Recognition Algorithm Based on 3D Point Cloud Acquired by Mixed
   Image Sensor}},
Journal = {{JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT
   INFORMATICS}},
Year = {{2019}},
Volume = {{23}},
Number = {{2}},
Pages = {{366-369}},
Month = {{MAR}},
Abstract = {{In order to solve the problem of low recognition accuracy in recognition
   of 3D face images collected by traditional sensors, a face recognition
   algorithm for 3D point cloud collected by mixed image sensors is
   proposed. The algorithm first uses the 3D wheelbase to expand the face
   image edge. According to the 3D wheelbase, the noise of extended image
   is detected, and median filtering is used to eliminate the detected
   noise. Secondly, the priority of the boundary pixels to recognize the
   face image in the denoising image recognition process is determined, and
   the key parts such as the illuminance line are analyzed, so that the
   recognition of the 3D point cloud face image is completed. Experiments
   show that the proposed algorithm improves the recognition accuracy of 3D
   face images, which recognition time is lower than that of the
   traditional algorithm by about 4 times, and the recognition efficiency
   is high.}},
DOI = {{10.20965/jaciii.2019.p0366}},
ISSN = {{1343-0130}},
EISSN = {{1883-8014}},
Unique-ID = {{ISI:000461603400031}},
}

@article{ ISI:000460046500034,
Author = {Alameda-Hernandez, Pedro and El Hamdouni, Rachid and Irigaray, Clemente
   and Chacon, Jose},
Title = {{Weak foliated rock slope stability analysis with ultra-close-range
   terrestrial digital photogrammetry}},
Journal = {{BULLETIN OF ENGINEERING GEOLOGY AND THE ENVIRONMENT}},
Year = {{2019}},
Volume = {{78}},
Number = {{2}},
Pages = {{1157-1171}},
Month = {{MAR}},
Abstract = {{This paper presents a review of the data acquisition procedures of
   geotechnical parameters for rock slope stability assessment and the
   proposal of some new improvements. For this purpose, a piece of research
   based on the slope mass rating classification system using close-range
   terrestrial digital photogrammetry (CR-TDP) has led to improvements in
   quality and timing of discontinuity data acquisition, and analyzes the
   suitability of each one of the parameters when applied to weak foliated
   rocks. TDP allows rapid 3D image acquisition of a rock slope, which can
   be analyzed using software to determine the geometrical parameters that
   affect stability. A fast procedure to perform the photogrammetric,
   non-contact survey in order to obtain the 3D images is shown in this
   paper. Being a rapid and single-person task, this procedure provides
   enough precision to be applied to weak foliated rock slopes with
   non-well-defined geometry. Furthermore, the study has focused on highly
   foliated rock outcrops, in which high resolution in the 3D images is
   very desirable. This research was applied to mountain road cuts, in
   which the use of TDP with a very close range was necessary. Through an
   application on weak rocks in the Alpujarras (Andalusia, Spain), this
   work analyzes the bias when applying TDP to materials such as these,
   under progressive weathering processes.}},
DOI = {{10.1007/s10064-017-1119-z}},
ISSN = {{1435-9529}},
EISSN = {{1435-9537}},
ResearcherID-Numbers = {{Hamdouni, Rachid El/L-1672-2017
   }},
ORCID-Numbers = {{Hamdouni, Rachid El/0000-0002-1271-3839
   Alameda Hernandez, Pedro Manuel/0000-0003-1830-1912}},
Unique-ID = {{ISI:000460046500034}},
}

@article{ ISI:000457692800013,
Author = {Abu, Arpah and Ngo, Chee Guan and Abu-Hassan, Nur Idayu Adira and
   Othman, Siti Adibah},
Title = {{Automated craniofacial landmarks detection on 3D image using geometry
   characteristics information}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2019}},
Volume = {{19}},
Number = {{13}},
Month = {{FEB 4}},
Note = {{17th International Conference on Bioinformatics (InCoB), Jawaharlal
   Nehru Univ, New Delhi, INDIA, SEP 26-28, 2018}},
Abstract = {{BackgroundIndirect anthropometry (IA) is one of the craniofacial
   anthropometry methods to perform the measurements on the digital facial
   images. In order to get the linear measurements, a few definable points
   on the structures of individual facial images have to be plotted as
   landmark points. Currently, most anthropometric studies use landmark
   points that are manually plotted on a 3D facial image by the examiner.
   This method is time-consuming and leads to human biases, which will vary
   from intra-examiners to inter-examiners when involving large data sets.
   Biased judgment also leads to a wider gap in measurement error. Thus,
   this work aims to automate the process of landmarks detection to help in
   enhancing the accuracy of measurement. In this work, automated
   craniofacial landmarks (ACL) on a 3D facial image system was developed
   using geometry characteristics information to identify the nasion (n),
   pronasale (prn), subnasale (sn), alare (al), labiale superius (ls),
   stomion (sto), labiale inferius (li), and chelion (ch). These landmarks
   were detected on the 3D facial image in .obj file format. The IA was
   also performed by manually plotting the craniofacial landmarks using
   Mirror software. In both methods, once all landmarks were detected, the
   eight linear measurements were then extracted. Paired t-test was
   performed to check the validity of ACL (i) between the subjects and (ii)
   between the two methods, by comparing the linear measurements extracted
   from both ACL and AI. The tests were performed on 60 subjects (30 males
   and 30 females).ResultsThe results on the validity of the ACL against IA
   between the subjects show accurate detection of n, sn, prn, sto, ls and
   li landmarks. The paired t-test showed that the seven linear
   measurements were statistically significant when p<0.05. As for the
   results on the validity of the ACL against IA between the methods, ACL
   is more accurate when p approximate to 0.03.ConclusionsIn conclusion,
   ACL has been validated with the eight landmarks and is suitable for
   automated facial recognition. ACL has proved its validity and
   demonstrated the practicability to be used as an alternative for IA, as
   it is time-saving and free from human biases.}},
DOI = {{10.1186/s12859-018-2548-9}},
Article-Number = {{548}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{OTHMAN, SITI ADIBAH BINTI/B-9784-2010
   Management Center, Dental Research/C-2478-2013}},
ORCID-Numbers = {{OTHMAN, SITI ADIBAH BINTI/0000-0002-3846-7701
   }},
Unique-ID = {{ISI:000457692800013}},
}

@article{ ISI:000460829200061,
Author = {Che, Erzhuo and Jung, Jaehoon and Olsen, Michael J.},
Title = {{Object Recognition, Segmentation, and Classification of Mobile Laser
   Scanning Point Clouds: A State of the Art Review}},
Journal = {{SENSORS}},
Year = {{2019}},
Volume = {{19}},
Number = {{4}},
Month = {{FEB 2}},
Abstract = {{Mobile Laser Scanning (MLS) is a versatile remote sensing technology
   based on Light Detection and Ranging (lidar) technology that has been
   utilized for a wide range of applications. Several previous reviews
   focused on applications or characteristics of these systems exist in the
   literature, however, reviews of the many innovative data processing
   strategies described in the literature have not been conducted in
   sufficient depth. To this end, we review and summarize the state of the
   art for MLS data processing approaches, including feature extraction,
   segmentation, object recognition, and classification. In this review, we
   first discuss the impact of the scene type to the development of an MLS
   data processing method. Then, where appropriate, we describe relevant
   generalized algorithms for feature extraction and segmentation that are
   applicable to and implemented in many processing approaches. The methods
   for object recognition and point cloud classification are further
   reviewed including both the general concepts as well as technical
   details. In addition, available benchmark datasets for object
   recognition and classification are summarized. Further, the current
   limitations and challenges that a significant portion of point cloud
   processing techniques face are discussed. This review concludes with our
   future outlook of the trends and opportunities of MLS data processing
   algorithms and applications.}},
DOI = {{10.3390/s19040810}},
Article-Number = {{810}},
ISSN = {{1424-8220}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000460829200061}},
}

@article{ ISI:000459798800005,
Author = {Sun, Jia and Huang, Di and Wang, Yunhong and Chen, Liming},
Title = {{Expression Robust 3D Facial Landmarking via Progressive Coarse-to-Fine
   Tuning}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2019}},
Volume = {{15}},
Number = {{1}},
Month = {{FEB}},
Abstract = {{Facial landmarking is a fundamental task in automatic machine-based face
   analysis. The majority of existing techniques for such a problem are
   based on 2D images; however, they suffer from illumination and pose
   variations that may largely degrade landmarking performance. The
   emergence of 3D data theoretically provides an alternative to overcome
   these weaknesses in the 2D domain. This article proposes a novel
   approach to 3D facial landmarking, which combines both the advantages of
   feature-based methods as well as model-based ones in a progressive
   three-stage coarse-to-fine manner (initial, intermediate, and fine
   stages). For the initial stage, a few fiducial landmarks (i.e., the nose
   tip and two inner eye corners) are robustly detected through curvature
   analysis, and these points are further exploited to initialize the
   subsequent stage. For the intermediate stage, a statistical model is
   learned in the feature space of three normal components of the facial
   point-cloud rather than the smooth original coordinates, namely Active
   Normal Model (ANM). For the fine stage, cascaded regression is employed
   to locally refine the landmarks according to their geometry attributes.
   The proposed approach can accurately localize dozens of fiducial points
   on each 3D face scan, greatly surpassing the feature-based ones, and it
   also improves the state of the art of the model-based ones in two
   aspects: sensitivity to initialization and deficiency in discrimination.
   The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE
   databases, and competitive results are achieved in comparison with
   counterparts in the literature, clearly demonstrating its effectiveness.}},
DOI = {{10.1145/3282833}},
Article-Number = {{21}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
Unique-ID = {{ISI:000459798800005}},
}

@article{ ISI:000456899900003,
Author = {Zhou, Zixiang and Gong, Jie and Hu, Xuan},
Title = {{Community-scale multi-level post-hurricane damage assessment of
   residential buildings using multi-temporal airborne LiDAR data}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2019}},
Volume = {{98}},
Pages = {{30-45}},
Month = {{FEB}},
Abstract = {{Building damage assessment is a critical task following major hurricane
   events. Use of remotely sensed data to support building damage
   assessment is a logical choice considering the difficulty of gaining
   ground access to the impacted areas immediately after hurricane events.
   However, a remote sensing based damage assessment approach is often only
   capable of detecting severely damaged buildings. In this study, an
   airborne LiDAR based approach is proposed to assess multi-level
   hurricane damage at the community scale. In the proposed approach,
   building clusters are first extracted using a density-based algorithm. A
   novel cluster matching algorithm is proposed to robustly match
   post-event and pre-event building clusters. Multiple features including
   roof area and volume, roof orientation, and roof shape are computed as
   building damage indicators. A hierarchical determination process is then
   employed to identify the extent of damage to each building object. The
   results of this study suggest that our proposed approach is capable of
   1) recognizing building objects, 2) extracting damage features, and 3)
   characterizing the extent of damage to individual building properties.}},
DOI = {{10.1016/j.autcon.2018.10.018}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
Unique-ID = {{ISI:000456899900003}},
}

@article{ ISI:000457939400106,
Author = {Zhang, Wuming and Wan, Peng and Wang, Tiejun and Cai, Shangshu and Chen,
   Yiming and Jin, Xiuliang and Yan, Guangjian},
Title = {{A Novel Approach for the Detection of Standing Tree Stems from
   Plot-Level Terrestrial Laser Scanning Data}},
Journal = {{REMOTE SENSING}},
Year = {{2019}},
Volume = {{11}},
Number = {{2}},
Month = {{JAN 2}},
Abstract = {{Tree stem detection is a key step toward retrieving detailed stem
   attributes from terrestrial laser scanning (TLS) data. Various
   point-based methods have been proposed for the stem point extraction at
   both individual tree and plot levels. The main limitation of the
   point-based methods is their high computing demand when dealing with
   plot-level TLS data. Although segment-based methods can reduce the
   computational burden and uncertainties of point cloud classification,
   its application is largely limited to urban scenes due to the complexity
   of the algorithm, as well as the conditions of natural forests. Here we
   propose a novel and simple segment-based method for efficient stem
   detection at the plot level, which is based on the curvature feature of
   the points and connected component segmentation. We tested our method
   using a public TLS dataset with six forest plots that were collected for
   the international TLS benchmarking project in Evo, Finland. Results
   showed that the mean accuracies of the stem point extraction were
   comparable to the state-of-art methods (>95\%). The accuracies of the
   stem mappings were also comparable to the methods tested in the
   international TLS benchmarking project. Additionally, our method was
   applicable to a wide range of stem forms. In short, the proposed method
   is accurate and simple; it is a sensible solution for the stem detection
   of standing trees using TLS data.}},
DOI = {{10.3390/rs11020211}},
Article-Number = {{211}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{Yan, Guangjian/0000-0001-5030-748X
   Wang, Tiejun/0000-0002-1138-8464}},
Unique-ID = {{ISI:000457939400106}},
}

@article{ ISI:000457037300020,
Author = {Shao, Gang and Shao, Guofan and Fei, Songlin},
Title = {{Delineation of individual deciduous trees in plantations with
   low-density LiDAR data}},
Journal = {{INTERNATIONAL JOURNAL OF REMOTE SENSING}},
Year = {{2019}},
Volume = {{40}},
Number = {{1}},
Pages = {{346-363}},
Month = {{JAN 2}},
Abstract = {{Delineation of individual deciduous trees with Light Detection and
   Ranging (LiDAR) data has long been sought for accurate forest inventory
   in temperate forests. Previous attempts mainly focused on high-density
   LiDAR data to obtain reliable delineation results, which may have
   limited applications due to the high cost and low availability of such
   data. Here, the feasibility of individual deciduous tree delineation
   with low-density LiDAR data was examined using a point-density-based
   algorithm. First a high-resolution point density model (PDM) was
   developed from low-density LiDAR point cloud to locate individual trees
   through the horizontal spatial distribution of LiDAR points. Then,
   individual tree crowns and associated attributes were delineated with a
   2D marker-controlled watershed segmentation. Additionally, the PDM-based
   approach was compared with a conventional canopy height model (CHM)
   based delineation. The results demonstrated that the PDM-based approach
   produced an 89\% detection accuracy to identify deciduous trees in our
   study area. The tree attributes derived from the PDM-based algorithm
   explained 81\% and 83\% of tree height and crown width variations of
   forest stands, respectively. The conventional CHM-based tree attributes,
   on the other hand, could explain only 71\% and 66\% of tree height and
   crown width, respectively. Our results suggest that the application of
   the PDM-based individual tree identification in deciduous forests with
   low-density LiDAR data is feasible and has relatively high accuracy to
   predict tree height and crown width, which are highly desired in
   large-scale forest inventory and analysis.}},
DOI = {{10.1080/01431161.2018.1513664}},
ISSN = {{0143-1161}},
EISSN = {{1366-5901}},
ORCID-Numbers = {{Shao, Gang/0000-0003-3198-966X}},
Unique-ID = {{ISI:000457037300020}},
}

@article{ ISI:000457664300030,
Author = {Teza, Giordano and Pesci, Arianna},
Title = {{Evaluation of the temperature pattern of a complex body from thermal
   imaging and 3D information: A method and its MATLAB implementation}},
Journal = {{INFRARED PHYSICS \& TECHNOLOGY}},
Year = {{2019}},
Volume = {{96}},
Pages = {{228-237}},
Month = {{JAN}},
Abstract = {{The standard setting of a camera used in Infrared thermography (IRT) is
   based on the choice of the same values of emissivity and distance for
   all pixels of a thermal image even if the emissivity depends on the
   relative position of camera and observed surface. Often this is not a
   problem. However, the resulting temperature pattern could be inadequate
   if a body having a complex shape is observed from strongly constrained
   positions. In order to face this issue, a procedure aimed at providing a
   correct temperature pattern by using 3D information related to a point
   cloud is proposed together with its MATLAB implementation (COMAP3
   toolbox). For each pixel of a thermal image, the relative position of
   camera and observed surface is estimated, leading to pixel-specific
   values of emissivity and distance. The temperature obtained in this way
   is also mapped onto the point cloud. The effectiveness of the procedure
   in recognizing areas characterized by peculiar thermal behavior is shown
   in the case of a historic cylindrical masonry bell tower (Caorle's bell
   tower, Venice, Italy).}},
DOI = {{10.1016/j.infrared.2018.11.029}},
ISSN = {{1350-4495}},
EISSN = {{1879-0275}},
ORCID-Numbers = {{PESCI, Arianna/0000-0003-1863-3132
   Teza, Giordano/0000-0002-6902-5033}},
Unique-ID = {{ISI:000457664300030}},
}

@article{ ISI:000457692900001,
Author = {Jazouli, Maha and Majda, Aicha and Merad, Djamal and Aalouane, Rachid
   and Zarghili, Arsalane},
Title = {{Automatic detection of stereotyped movements in autistic children using
   the Kinect sensor}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMEDICAL ENGINEERING AND TECHNOLOGY}},
Year = {{2019}},
Volume = {{29}},
Number = {{3}},
Pages = {{201-220}},
Abstract = {{Autism Spectrum Disorders (ASD) is a developmental disorder that affects
   communications, social skills or behaviours that can occur in some
   people. Children or adults with ASD often have repetitive motor
   movements or unusual behaviours. The objective of this work is to
   automatically detect stereotypical motor movements in real time using
   Kinect sensor. The approach is based on the \$P Point-Cloud Recogniser
   to identify multi-stroke gestures as point clouds. This paper presents
   new methodology to automatically detect five stereotypical motor
   movements: body rocking, hand flapping, fingers flapping, hand on the
   face and hands behind back. With many ASD-children, our proposed system
   gives us satisfactory results. This can help to implement a smart video
   surveillance system and then helps clinicians in the diagnosing ASD.}},
DOI = {{10.1504/IJBET.2019.097621}},
ISSN = {{1752-6418}},
EISSN = {{1752-6426}},
Unique-ID = {{ISI:000457692900001}},
}

@article{ ISI:000454963800023,
Author = {Garcia-Luna, Ramiro and Senent, Salvador and Jurado-Pina, Rafael and
   Jimenez, Rafael},
Title = {{Structure from Motion photogrammetry to characterize underground rock
   masses: Experiences from two real tunnels}},
Journal = {{TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY}},
Year = {{2019}},
Volume = {{83}},
Pages = {{262-273}},
Month = {{JAN}},
Abstract = {{A new methodology to identify discontinuity sets at the tunnel face
   based on the Structure from Motion (SfM) photogrammetric technique is
   proposed. The work focuses on the performance of this technique when
   employed to characterize the ground mass under real tunneling
   conditions, illustrating its possibilities and analyzing several aspects
   that affect the quality of the obtained results. By means of a set of
   overlapping photographs from the tunnel face, SfM constructs a 3D point
   cloud model, from which discontinuities are identified using a
   discontinuity set extractor software. To orientate and scale the digital
   model, an easy-to-use ``portable orientation template{''}, specifically
   developed for this work, is employed. The proposed methodology is
   applied to two real tunnels under construction in Northern Spain. Its
   results are compared with those obtained with a traditional analysis
   based on manual compass measurements. Results show that the SfM
   methodology provides an adequate characterization of the structure of
   the rock mass, identifying the same number of discontinuity sets as the
   compass measurements approach and with differences in orientation that
   are within the uncertainty range associated to manual measurements. Only
   one sub-horizontal set presented higher orientation differences, but
   this is mainly due to the presence of shotcrete at the face. In addition
   to the advantages of a ``distant{''} measurement technique-e.g., health
   and safety advantages, capability to characterize unreachable areas,
   etc.-, as well as to the advantage of its reduced cost, the proposed SfM
   methodology and its associated tools allow one to represent planes
   associated to each discontinuity set back into the original 3D digital
   point model, and to perform detailed analyses that clarify and improve
   the obtained results. Finally, an analysis about the minimum number of
   photographs needed to adequately characterize the tunnel face is
   conducted, with results showing that around 15 good quality photographs
   are enough for tunnel faces with excavated areas of about 50 m(2).}},
DOI = {{10.1016/j.tust.2018.09.026}},
ISSN = {{0886-7798}},
Unique-ID = {{ISI:000454963800023}},
}

@article{ ISI:000450379200003,
Author = {Kaminska, Agnieszka and Lisiewicz, Maciej and Sterenczak, Krzysztof and
   Kraszewski, Bartlomiej and Sadkowski, Rafal},
Title = {{Species-related single dead tree detection using multi-temporal ALS data
   and CIR imagery}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{219}},
Pages = {{31-43}},
Month = {{DEC 15}},
Abstract = {{The assessment of the health conditions of trees in forests is extremely
   important for biodiversity, forest management, global environment
   monitoring, and carbon dynamics. There is a vast amount of research
   using remote sensing (RS) techniques for the assessment of the current
   condition of a forest, but only a small number of these are concerned
   with detection and classification of dead trees. Among the available RS
   techniques, only the airborne laser scanner (ALS) enables dead tree
   detection at the single tree level with high accuracy.
   The main objective of the study was to identify spruce, pine and
   deciduous trees by alive or dead classifications. Three RS data sets
   including ALS (leaf-on and leaf-off) and color-infrared (CIR) imagery
   (leaf-on) were used for the study. We used intensity and structural
   variables from the ALS data and spectral information derived from aerial
   imagery for the classification procedure. Additionally, we tested the
   differences in the classification accuracy of all variants contained in
   the data integration. In the study, the random forest (RF) classifier
   was used. The study was carried out in the Polish part of the Bialowieia
   Forest (BF).
   In general, we can state that all classifications, with different
   combinations of ALS features and CIR, resulted in high overall accuracy
   (OA >= 90\%) and Kappa (kappa > 0.86). For the best variant
   (CIR\_ALS(WSn-FH)), the mean values of overall accuracy and Kappa were
   equal to 94.3\% and 0.93, respectively. The leaf -on point cloud
   features alone produced the lowest accuracies (OA = 75-81\% and x =
   0.68-0.76). Improvements of 0-0.04 in the Kappa coefficient and 0-3.1\%
   in the overall classification accuracy were found after the point cloud
   normalization for all variants. Full -height point cloud features (F)
   produced lower accuracies than the results based on features calculated
   for half of the tree height point clouds (H) and combined FH.
   The importance of each of the predictors for different data sets for
   tree species classification provided by the RF algorithm was
   investigated. The lists of top features were the same, independent of
   intensity normalization. For the classification based on both of the
   point clouds (leaf on and leaf-off), three structural features (a
   proportion of first returns for both half -height and full -height
   variants and the canopy relief ratio of points) and two intensity
   features from first returns and half -height variant (the coefficient of
   variation and skewness) were rated as the most important. In the
   classification based on the point cloud with CIR features, two image
   features were among the most important (the NDVI and mean value of
   reflectance in the green band).}},
DOI = {{10.1016/j.rse.2018.10.005}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Lisiewicz, Maciej/Y-3668-2018
   Kraszewski, Bartlomiej/U-9482-2017}},
ORCID-Numbers = {{Lisiewicz, Maciej/0000-0003-0676-8291
   Sterenczak, Krzysztof/0000-0002-9556-0144
   Kraszewski, Bartlomiej/0000-0001-6161-7619}},
Unique-ID = {{ISI:000450379200003}},
}

@article{ ISI:000460570100008,
Author = {Nguyen Duy Thanh and Khachumov, V. M.},
Title = {{Models and Methods for Matching Images in the Problem of Face
   Recognition}},
Journal = {{SCIENTIFIC AND TECHNICAL INFORMATION PROCESSING}},
Year = {{2018}},
Volume = {{45}},
Number = {{5}},
Pages = {{360-367}},
Month = {{DEC}},
Abstract = {{A subject area is analyzed and the topicality of the problem of face
   recognition is shown. Methods for matching images by applying position
   lines, convolutions and invariants to the group of affine transformation
   for 2D and 3D images are examined. Proper comparison is a necessary
   condition for solving the recognition problem. Proper use of the
   position-line method for reducing face images to the normalized form is
   shown.}},
DOI = {{10.3103/S0147688218050088}},
ISSN = {{0147-6882}},
EISSN = {{1934-8118}},
Unique-ID = {{ISI:000460570100008}},
}

@article{ ISI:000455637600091,
Author = {Zhou, Tan and Popescu, Sorin and Malambo, Lonesome and Zhao, Kaiguang
   and Krause, Keith},
Title = {{From LiDAR Waveforms to Hyper Point Clouds: A Novel Data Product to
   Characterize Vegetation Structure}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Full waveform (FW) LiDAR holds great potential for retrieving vegetation
   structure parameters at a high level of detail, but this prospect is
   constrained by practical factors such as the lack of available handy
   processing tools and the technical intricacy of waveform processing.
   This study introduces a new product named the Hyper Point Cloud (HPC),
   derived from FW LiDAR data, and explores its potential applications,
   such as tree crown delineation using the HPC-based intensity and
   percentile height (PH) surfaces, which shows promise as a solution to
   the constraints of using FW LiDAR data. The results of the HPC present a
   new direction for handling FW LiDAR data and offer prospects for
   studying the mid-story and understory of vegetation with high point
   density (similar to 182 points/m(2)). The intensity-derived digital
   surface model (DSM) generated from the HPC shows that the ground region
   has higher maximum intensity (MAXI) and mean intensity (MI) than the
   vegetation region, while having lower total intensity (TI) and number of
   intensities (NI) at a given grid cell. Our analysis of intensity
   distribution contours at the individual tree level exhibit similar
   patterns, indicating that the MAXI and MI decrease from the tree crown
   center to the tree boundary, while a rising trend is observed for TI and
   NI. These intensity variable contours provide a theoretical
   justification for using HPC-based intensity surfaces to segment tree
   crowns and exploit their potential for extracting tree attributes. The
   HPC-based intensity surfaces and the HPC-based PH Canopy Height Models
   (CHM) demonstrate promising tree segmentation results comparable to the
   LiDAR-derived CHM for estimating tree attributes such as tree locations,
   crown widths and tree heights. We envision that products such as the HPC
   and the HPC-based intensity and height surfaces introduced in this study
   can open new perspectives for the use of FW LiDAR data and alleviate the
   technical barrier of exploring FW LiDAR data for detailed vegetation
   structure characterization.}},
DOI = {{10.3390/rs10121949}},
Article-Number = {{1949}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Popescu, Sorin C/D-5981-2015
   Malambo, Lonesome/A-5693-2015}},
ORCID-Numbers = {{Popescu, Sorin C/0000-0002-8155-8801
   Zhou, Tan/0000-0002-9193-5113
   Malambo, Lonesome/0000-0002-8102-3700}},
Unique-ID = {{ISI:000455637600091}},
}

@article{ ISI:000455069600031,
Author = {Jaafar, Wan Shafrina Wan Mohd and Woodhouse, Iain Hector and Silva,
   Carlos Alberto and Omar, Hamdan and Maulud, Khairul Nizam Abdul and
   Hudak, Andrew Thomas and Klauberg, Carine and Cardil, Adrian and Mohan,
   Midhun},
Title = {{Improving Individual Tree Crown Delineation and Attributes Estimation of
   Tropical Forests Using Airborne LiDAR Data}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Individual tree crown (ITC) segmentation is an approach to isolate
   individual tree from the background vegetation and delineate precisely
   the crown boundaries for forest management and inventory purposes. ITC
   detection and delineation have been commonly generated from canopy
   height model (CHM) derived from light detection and ranging (LiDAR)
   data. Existing ITC segmentation methods, however, are limited in their
   efficiency for characterizing closed canopies, especially in tropical
   forests, due to the overlapping structure and irregular shape of tree
   crowns. Furthermore, the potential of 3-dimensional (3D) LiDAR data is
   not fully realized by existing CHM-based methods. Thus, the aim of this
   study was to develop an efficient framework for ITC segmentation in
   tropical forests using LiDAR-derived CHM and 3D point cloud data in
   order to accurately estimate tree attributes such as the tree height,
   mean crown width and aboveground biomass (AGB). The proposed framework
   entails five major steps: (1) automatically identifying dominant tree
   crowns by implementing semi-variogram statistics and morphological
   analysis; (2) generating initial tree segments using a watershed
   algorithm based on mathematical morphology; (3) identifying problematic
   segments based on predetermined set of rules; (4) tuning the problematic
   segments using a modified distance-based algorithm (DBA); and (5)
   segmenting and counting the number of individual trees based on the 3D
   LiDAR point clouds within each of the identified segment. This approach
   was developed in a way such that the 3D LiDAR points were only examined
   on problematic segments identified for further evaluations. 209
   reference trees with diameter at breast height (DBH) 10 cm were selected
   in the field in two study areas in order to validate ITC detection and
   delineation results of the proposed framework. We computed tree crown
   metrics (e.g., maximum crown height and mean crown width) to estimate
   aboveground biomass (AGB) at tree level using previously published
   allometric equations. Accuracy assessment was performed to calculate
   percentage of correctly detected trees, omission and commission errors.
   Our method correctly identified individual tree crowns with detection
   accuracy exceeding 80 percent at both forest sites. Also, our results
   showed high agreement (R-2 > 0.64) in terms of AGB estimates using 3D
   LiDAR metrics and variables measured in the field, for both sites. The
   findings from our study demonstrate the efficacy of the proposed
   framework in delineating tree crowns, even in high canopy density areas
   such as tropical rainforests, where, usually the traditional algorithms
   are limited in their performances. Moreover, the high tree delineation
   accuracy in the two study areas emphasizes the potential robustness and
   transferability of our approach to other densely forested areas across
   the globe.}},
DOI = {{10.3390/f9120759}},
Article-Number = {{759}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{wan mohd jaafar, wan shafrina/P-3916-2018
   abdul maulud, khairul nizam/J-4136-2015
   }},
ORCID-Numbers = {{Silva, Carlos Alberto/0000-0002-7844-3560
   wan mohd jaafar, wan shafrina/0000-0002-7813-088X
   abdul maulud, khairul nizam/0000-0002-9215-2778
   Hudak, Andrew/0000-0001-7480-1458}},
Unique-ID = {{ISI:000455069600031}},
}

@article{ ISI:000451733800040,
Author = {Shen, Yueqian and Lindenbergh, Roderik and Wang, Jinguo and Ferreira,
   Vagner G.},
Title = {{Extracting Individual Bricks from a Laser Scan Point Cloud of an
   Unorganized Pile of Bricks}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Bricks are the vital component of most masonry structures. Their
   maintenance is critical to the protection of masonry buildings.
   Terrestrial Light Detection and Ranging (TLidar) systems provide massive
   point cloud data in an accurate and fast way. TLidar enables us to
   sample and store the state of a brick surface in a practical way. This
   article aims to extract individual bricks from an unorganized pile of
   bricks sampled by a dense point cloud. The method automatically segments
   and models the individual bricks. The methodology is divided into five
   main steps: Filter needless points, brick boundary points removal,
   coarse segmentation using 3D component analysis, planar segmentation and
   grouping, and brick reconstruction. A novel voting scheme is used to
   segment the planar patches in an effective way. Brick reconstruction is
   based on the geometry of single brick and its corresponding nominal size
   (length, width and height). The number of bricks reconstructed is around
   75\%. An accuracy assessment is performed by comparing 3D coordinates of
   the reconstructed vertices to the manually picked vertices. The standard
   deviations of differences along x, y and z axes are 4.55 mm, 4.53 mm and
   4.60 mm, respectively. The comparison results indicate that the accuracy
   of reconstruction based on the introduced methodology is high and
   reliable. The work presented in this paper provides a theoretical basis
   and reference for large scene applications in brick-like structures.
   Meanwhile, the high-accuracy brick reconstruction lays the foundation
   for further brick displacement estimation.}},
DOI = {{10.3390/rs10111709}},
Article-Number = {{1709}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{Lindenbergh, Roderik/0000-0001-8655-5266
   Shen, Yueqian/0000-0003-2455-9012}},
Unique-ID = {{ISI:000451733800040}},
}

@article{ ISI:000433909100002,
Author = {Song, Xiaoning and Feng, Zhen-Hua and Hu, Guosheng and Kittler, Josef
   and Wu, Xiao-Jun},
Title = {{Dictionary Integration Using 3D Morphable Face Models for Pose-Invariant
   Collaborative-Representation-Based Classification}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2018}},
Volume = {{13}},
Number = {{11}},
Pages = {{2734-2745}},
Month = {{NOV}},
Abstract = {{The paper presents a dictionary integration algorithm using 3D morphable
   face models (3DMM) for pose-invariant collaborative-representation-based
   face classification. To this end, we first fit a 3DMM to the 2D face
   images of a dictionary to reconstruct the 3D shape and texture of each
   image. The 3D faces are used to render a number of virtual 2D face
   images with arbitrary pose variations to augment the training data, by
   merging the original and rendered virtual samples to create an extended
   dictionary. Second, to reduce the information redundancy of the extended
   dictionary and improve the sparsity of reconstruction coefficient
   vectors using collaborative-representation-based classification (CRC),
   we exploit an on-line class elimination scheme to optimise the extended
   dictionary by identifying the training samples of the most
   representative classes for a given query. The final goal is to perform
   pose-invariant face classification using the proposed dictionary
   integration method and the on-line pruning strategy under the CRC
   framework. Experimental results obtained for a set of well-known face
   data sets demonstrate the merits of the proposed method, especially its
   robustness to pose variations.}},
DOI = {{10.1109/TIFS.2018.2833052}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ORCID-Numbers = {{Kittler, Josef/0000-0002-8110-9205
   Feng, Zhenhua/0000-0002-4485-4249}},
Unique-ID = {{ISI:000433909100002}},
}

@article{ ISI:000447286200001,
Author = {Song, Wei and Zou, Shuanghui and Tian, Yifei and Fong, Simon and Cho,
   Kyungeun},
Title = {{Classifying 3D objects in LiDAR point clouds with a back-propagation
   neural network}},
Journal = {{HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES}},
Year = {{2018}},
Volume = {{8}},
Month = {{OCT 12}},
Abstract = {{Due to object recognition accuracy limitations, unmanned ground vehicles
   (UGVs) must perceive their environments for local path planning and
   object avoidance. To gather high-precision information about the UGV's
   surroundings, Light Detection and Ranging (LiDAR) is frequently used to
   collect large-scale point clouds. However, the complex spatial features
   of these clouds, such as being unstructured, diffuse, and disordered,
   make it difficult to segment and recognize individual objects. This
   paper therefore develops an object feature extraction and classification
   system that uses LiDAR point clouds to classify 3D objects in urban
   environments. After eliminating the ground points via a height threshold
   method, this describes the 3D objects in terms of their geometrical
   features, namely their volume, density, and eigenvalues. A
   back-propagation neural network (BPNN) model is trained (over the course
   of many iterations) to use these extracted features to classify objects
   into five types. During the training period, the parameters in each
   layer of the BPNN model are continually changed and modified via
   back-propagation using a non-linear sigmoid function. In the system, the
   object segmentation process supports obstacle detection for autonomous
   driving, and the object recognition method provides an environment
   perception function for terrain modeling. Our experimental results
   indicate that the object recognition accuracy achieve 91.5\% in outdoor
   environment.}},
DOI = {{10.1186/s13673-018-0152-7}},
Article-Number = {{29}},
ISSN = {{2192-1962}},
ORCID-Numbers = {{Wei, Song/0000-0002-5909-9661}},
Unique-ID = {{ISI:000447286200001}},
}

@article{ ISI:000445398200037,
Author = {Thierens, Laurent A. M. and De Roo, Noemi M. C. and De Pauw, Guy A. M.
   and Brusselaers, Nele},
Title = {{Quantifying Soft Tissue Changes in Cleft Lip and Palate Using
   Nonionizing Three-Dimensional Imaging: A Systematic Review}},
Journal = {{JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{76}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{Purpose: The use of nonionizing 3-dimensional (3D) imaging in cleft lip
   and palate (CLP) research is well-established; however, general
   guidelines concerning the assessment of these images are lacking. The
   aim of the present study was to review the methods for quantification of
   soft tissue changes on 3D surface images acquired before and after an
   orthopedic or surgical intervention in CLP patients.
   Materials and Methods: A systematic literature search was performed
   using the databases MEDLINE (through PubMed), CENTRAL, Web of Science,
   and EMBASE. The literature search and eligibility assessment were
   performed by 2 independent reviewers in a nonblinded standardized
   manner. Only longitudinal studies reporting the assessment of pre- and
   postoperative 3D surface images and at least 10 CLP patients were
   considered eligible.
   Results: Fifteen unique studies (reported from 1996 to 2017) were
   identified after an eligibility assessment. The assessment of the 3D
   images was performed with landmark-dependent analyses, mostly supported
   by superimposition of the pre- and postoperative images. A wide spectrum
   of superimposition techniques has been reported. The reliability of
   these assessment methods was often not reported or was insufficiently
   reported.
   Conclusions: Soft tissue changes subsequent to a surgical or an
   orthopedic intervention can be quantified on 3D surface images using
   assessment methods that are primarily based on landmark identification,
   whether or not followed by superimposition. Operator bias is inherently
   enclosed in landmark-dependent analyses. The reliability of these
   methods has been insufficiently reported. (C) 2018 American Association
   of Oral and Maxillofacial Surgeons}},
DOI = {{10.1016/j.joms.2018.05.020}},
ISSN = {{0278-2391}},
EISSN = {{1531-5053}},
Unique-ID = {{ISI:000445398200037}},
}

@article{ ISI:000442764500004,
Author = {Cardoso, Maria Joao and Vrieling, Conny and Cardoso, Jaime S. and
   Oliveira, Helder P. and Williams, Norman R. and Dixon, J. M. and PICTURE
   Project Clinical Trial Tea and PICTURE Project Delphi Panel},
Title = {{The value of 3D images in the aesthetic evaluation of breast cancer
   conservative treatment. Results from a prospective multicentric clinical
   trial}},
Journal = {{BREAST}},
Year = {{2018}},
Volume = {{41}},
Pages = {{19-24}},
Month = {{OCT}},
Abstract = {{Purpose: BCCT.core (Breast Cancer Conservative Treatment. cosmetic
   results) is a software created for the objective evaluation of aesthetic
   result of breast cancer conservative treatment using a single patient
   frontal photography. The lack of volume information has been one
   criticism, as the use of 3D information might improve accuracy in
   aesthetic evaluation. In this study, we have evaluated the added value
   of 3D information to two methods of aesthetic evaluation: a panel of
   experts; and an augmented version of the computational model -
   BCCT.core3d.
   Material and methods: Within the scope of EU Seventh Framework Programme
   Project PICTURE, 2D and 3D images from 106 patients from three clinical
   centres were evaluated by a panel of 17 experts and the BCCT.core.
   Agreement between all methods was calculated using the kappa (K) and
   weighted kappa (wK) statistics.
   Results: Subjective agreement between 2D and 3D individual evaluation
   was fair to moderate. The agreement between the expert classification
   and the BCCT.core software with both 2D and 3D features was also fair to
   moderate.
   Conclusions: The inclusion of 3D images did not add significant
   information to the aesthetic evaluation either by the panel or the
   software. Evaluation of aesthetic outcome can be performed using of the
   BCCT.core software, with a single frontal image. (C) 2018 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.breast.2018.06.008}},
ISSN = {{0960-9776}},
EISSN = {{1532-3080}},
ResearcherID-Numbers = {{WILLIAMS, Norman/C-2002-2008
   Cardoso, Jaime/I-3286-2013
   Oliveira, Helder/M-9956-2017}},
ORCID-Numbers = {{WILLIAMS, Norman/0000-0001-6496-312X
   Cardoso, Jaime/0000-0002-3760-2473
   Cardoso, Maria Joao/0000-0002-8137-3700
   Oliveira, Helder/0000-0002-6193-8540}},
Unique-ID = {{ISI:000442764500004}},
}

@article{ ISI:000440851800017,
Author = {Siqueira, Robson S. and Alexandre, Gilderlane R. and Soares, Jose M. and
   The, George A. P.},
Title = {{Triaxial Slicing for 3-D Face Recognition From Adapted Rotational
   Invariants Spatial Moments and Minimal Keypoints Dependence}},
Journal = {{IEEE ROBOTICS AND AUTOMATION LETTERS}},
Year = {{2018}},
Volume = {{3}},
Number = {{4}},
Pages = {{3513-3520}},
Month = {{OCT}},
Abstract = {{This letter presents a multiple slicing model for three-dimensional
   (3-D) images of human face, using the frontal, sagittal, and transverse
   orthogonal planes. The definition of the segments depends on just one
   key point, the nose tip, which makes it simple and independent of the
   detection of several key points. For facial recognition, attributes
   based on adapted 2-D spatial moments of Hu and 3-D spatial invariant
   rotation moments are extracted from each segment. Tests with the
   proposed model using the Bosphorus Database for neutral vs nonneutral
   ROC I experiment, applying linear discriminant analysis as classifier
   and more than one sample for training, achieved 98.7\% of verification
   rate at 0.1\% of false acceptance rate. By using the support vector
   machine as classifier the rank1 experiment recognition rates of 99\% and
   95.4\% have been achieved for a neutral vs neutral and for a neutral vs
   non neutral, respectively. These results approach the state-of-the-art
   using Bosphorus Database and even surpasses it when anger and disgust
   expressions are evaluated. In addition, we also evaluate the
   generalization of our method using the FRGC v2.0 database and achieve
   competitive results, making the technique promising, especially for its
   simplicity.}},
DOI = {{10.1109/LRA.2018.2854295}},
ISSN = {{2377-3766}},
ORCID-Numbers = {{Marques Soares, Jose/0000-0002-5111-5794
   Alexandre, Gilderlane/0000-0002-8778-5351
   The, George/0000-0002-8064-8901}},
Unique-ID = {{ISI:000440851800017}},
}

@article{ ISI:000449993800083,
Author = {Wu, Jianwei and Yao, Wei and Polewski, Przemyslaw},
Title = {{Mapping Individual Tree Species and Vitality along Urban Road Corridors
   with LiDAR and Imaging Sensors: Point Density versus View Perspective}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{To meet a growing demand for accurate high-fidelity vegetation cover
   mapping in urban areas toward biodiversity conservation and assessing
   the impact of climate change, this paper proposes a complete approach to
   species and vitality classification at single tree level by synergistic
   use of multimodality 3D remote sensing data. So far, airborne laser
   scanning system (ALS or airborne LiDAR) has shown promising results in
   tree cover mapping for urban areas. This paper analyzes the potential of
   mobile laser scanning system/mobile mapping system (MLS/MMS)-based
   methods for recognition of urban plant species and characterization of
   growth conditions using ultra-dense LiDAR point clouds and provides an
   objective comparison with the ALS-based methods. Firstly, to solve the
   extremely intensive computational burden caused by the classification of
   ultra-dense MLS data, a new method for the semantic labeling of LiDAR
   data in the urban road environment is developed based on combining a
   conditional random field (CRF) for the context-based classification of
   3D point clouds with shape priors. These priors encode geometric
   primitives found in the scene through sample consensus segmentation.
   Then, single trees are segmented from the labelled tree points using the
   3D graph cuts algorithm. Multinomial logistic regression classifiers are
   used to determine the fine deciduous urban tree species of conversation
   concern and their growth vitality. Finally, the weight-of-evidence
   (WofE) based decision fusion method is applied to combine the
   probability outputs of classification results from the MLS and ALS data.
   The experiment results obtained in city road corridors demonstrated that
   point cloud data acquired from the airborne platform achieved even
   slightly better results in terms of tree detection rate, tree species
   and vitality classification accuracy, although the tree vitality
   distribution in the test site is less balanced compared to the species
   distribution. When combined with MLS data, overall accuracies of 78\%
   and 74\% for tree species and vitality classification can be achieved,
   which has improved by 5.7\% and 4.64\% respectively compared to the
   usage of airborne data only.}},
DOI = {{10.3390/rs10091403}},
Article-Number = {{1403}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Yao, Wei/E-8520-2017}},
ORCID-Numbers = {{Yao, Wei/0000-0001-7704-0615}},
Unique-ID = {{ISI:000449993800083}},
}

@article{ ISI:000448227000069,
Author = {Ahmadi, Bahar and Javidi, Bahram and Shahbazmohamadi, Sina},
Title = {{Automated detection of counterfeit ICs using machine learning}},
Journal = {{MICROELECTRONICS RELIABILITY}},
Year = {{2018}},
Volume = {{88-90}},
Number = {{SI}},
Pages = {{371-377}},
Month = {{SEP}},
Note = {{29th European Symposium on the Reliability of Electron Devices, Failure
   Physics and Analysis (ESREF), Aalborg, DENMARK, OCT 01-05, 2018}},
Abstract = {{The electronic industry has been experiencing a growing counterfeit
   market, resulting in electronic supply chains in other industries to be
   prone to counterfeit parts as well. Over the past few years, several
   methods have been developed for evaluating the reliability of an IC and
   distinguishing them as counterfeit or authentic. Trained experts offer
   services for evaluating an IC based on destructive or non-destructive
   methods. However, defect detection and recognition are mostly dependent
   on human decision, and therefore are vulnerable to error. In this paper,
   we propose a method to automatically detect and identify die-face
   delamination on an IC die. Die-face delamination is a predominant
   internal defect in recycled ICs but can be easily missed during defect
   detection. Here, we have acquired the 3D image of an IC
   non-destructively using X-ray computed tomography and applied image
   processing techniques and machine learning algorithms on the 3D image to
   detect die-face delamination in the forms of thermally induced cracks
   and damaged surfaces.}},
DOI = {{10.1016/j.microrel.2018.06.083}},
ISSN = {{0026-2714}},
Unique-ID = {{ISI:000448227000069}},
}

@article{ ISI:000445436100006,
Author = {Halik, Lukasz and Smaczynski, Maciej},
Title = {{Geovisualisation of Relief in a Virtual Reality System on the Basis of
   Low-Level Aerial Imagery}},
Journal = {{PURE AND APPLIED GEOPHYSICS}},
Year = {{2018}},
Volume = {{175}},
Number = {{9}},
Pages = {{3209-3221}},
Month = {{SEP}},
Abstract = {{The aim of the following paper was to present the geomatic process of
   transforming low-level aerial imagery obtained with unmanned aerial
   vehicles (UAV) into a digital terrain model (DTM) and implementing the
   model into a virtual reality system (VR). The object of the study was a
   natural aggretage heap of an irregular shape and denivelations up to 11
   m. Based on the obtained photos, three point clouds (varying in the
   level of detail) were generated for the 20,000-m(2)-area. For further
   analyses, the researchers selected the point cloud with the best ratio
   of accuracy to output file size. This choice was made based on seven
   control points of the heap surveyed in the field and the corresponding
   points in the generated 3D model. The obtained several-centimetre
   differences between the control points in the field and the ones from
   the model might testify to the usefulness of the described algorithm for
   creating large-scale DTMs for engineering purposes. Finally, the chosen
   model was implemented into the VR system, which enables the most
   lifelike exploration of 3D terrain plasticity in real time, thanks to
   the first person view mode (FPV). In this mode, the user observes an
   object with the aid of a Head- mounted display (HMD), experiencing the
   geovisualisation from the inside, and virtually analysing the terrain as
   a direct animator of the observations.}},
DOI = {{10.1007/s00024-017-1755-z}},
ISSN = {{0033-4553}},
EISSN = {{1420-9136}},
Unique-ID = {{ISI:000445436100006}},
}

@article{ ISI:000445204800002,
Author = {Wang, Jinhu and Lindenbergh, Roderik and Menenti, Massimo},
Title = {{Scalable individual tree delineation in 3D point clouds}},
Journal = {{PHOTOGRAMMETRIC RECORD}},
Year = {{2018}},
Volume = {{33}},
Number = {{163}},
Pages = {{315-340}},
Month = {{SEP}},
Abstract = {{Manually monitoring and documenting trees is labour intensive. Lidar
   provides a possible solution for automatic tree-inventory generation.
   Existing approaches for segmenting trees from original point cloud data
   lack scalable and efficient methods that separate individual trees
   sampled by different laser-scanning systems with sufficient quality
   under all circumstances. In this study a new algorithm for efficient
   individual tree delineation from lidar point clouds is presented and
   validated. The proposed algorithm first resamples the points using
   cuboid (modified voxel) cells. Consecutively connected cells are
   accumulated by vertically traversing cell layers. Trees in close
   proximity are identified, based on a novel cell-adjacency analysis. The
   scalable performance of this algorithm is validated on airborne, mobile
   and terrestrial laser-scanning point clouds. Validation against ground
   truth demonstrates an improvement from 89\% to 94\% relative to a
   state-of-the-art method while computation time is similar.
   Resume La detection et la documentation manuelle des arbres est une
   tache fastidieuse. Le lidar offre une solution possible pour
   l'inventaire automatique des arbres. Les approches existantes pour la
   segmentation des arbres dans des nuages bruts de points ne proposent pas
   de methodes efficaces et adaptees a toutes les echelles pour separer des
   arbres individuels echantillonnes par differents systemes lidar avec une
   qualite acceptable en toute circonstance. Cette etude propose et valide
   un nouvel algorithme pour la delimitation efficace d'arbres individuels
   a partir de nuages de points lidar. L'algorithme propose commence par
   reechantillonner les points dans des cellules cubiques (voxels), puis
   regroupe les cellules connexes en traversant verticalement les couches
   de cellules. Les arbres proches sont identifies grace a une nouvelle
   analyse d'adjacence de cellules. La performance de cetalgorithme en
   termes d'adaptabilite au changement d'echelle est validee a partir de
   nuages de points issus de systemes laser a balayage aerien, mobile et
   terrestre. Une validation basee sur des donnees de terrain de reference
   fait etat d'une amelioration de 89\% a 94\% par rapport a des methodes
   connues pour un temps de calcul comparable.
   Zusammenfassung Eine uberwachung und Dokumentation von Baumen ist sehr
   arbeitsaufwandig. Lidar bietet das Potential fur automatische
   Bauminventur. Es gibt Ansatze zur Segmentierung von Baumen aus
   Punktwolken, die allerdings noch nicht in der Lage sind, einzelne Baume
   in Punktwolken verschiedener Lidar-Systeme zuverlassig und mit
   ausreichender Qualitat unter vielfaltigen realen Bedingungen zu
   separieren. Diese Studie stellt einen neuen Algorithmus zur effizienten
   Erfassung von Baumen in Lidar-Punktwolken dar. Der Algorithmus bildet
   Punkte mit Hilfe von quaderformigen (Voxel) Zellen um. Nacheinander
   verbundene Zellen werden durch vertikale Traverse der Zellschichten
   akkumuliert. Baume in nachster Nachbarschaft werden durch eine neuartige
   Zellanalyse identifiziert. Der Vorteil der Skalierbarkeit des
   Algorithmus wird an flugzeuggestutzten, mobilen und terrestrischen
   Laserscanpunktwolken validiert. Anhand von Solldaten ist festzustellen,
   dass bei gleicher Rechenzeit, eine Verbesserung von 89\% bis 94\% im
   Vergleich zu aktuellen Verfahren erzielt werden kann.
   Resumen Monitorizar y documentar manualmente arboles es un trabajo
   intensivo. El lidar proporciona una posible solucion para la generacion
   automatica del inventario de arboles. Los enfoques existentes para
   segmentar arboles a partir originalmente de nubes de puntos lidar
   carecen de metodos escalables y eficientes que separen arboles
   individuales muestreados por diferentes sistemas lidar con calidad
   suficiente bajo todas las circunstancias. En este estudio, se presenta y
   valida un algoritmo nuevo para la delimitacion eficiente de arboles
   individuales a partir de nubes de puntos lidar. El algoritmo propuesto
   primero remuestrea los puntos usando celulas cuboides (voxels). Los
   voxels adyacentes se acumulan atravesando verticalmente las capas de
   voxels. Basados en un nuevo analisis de adyacencia de voxels se
   identifican arboles que estan proximos. El rendimiento escalable de este
   algoritmo se valida con nubes de puntos lidar aerotransportados, moviles
   y terrestres. La validacion con verdad terreno demuestra una mejora del
   89\% al 94\% en comparacion con un metodo de vanguardia, mientras que el
   tiempo de calculo es similar.}},
DOI = {{10.1111/phor.12247}},
ISSN = {{0031-868X}},
EISSN = {{1477-9730}},
ORCID-Numbers = {{Lindenbergh, Roderik/0000-0001-8655-5266}},
Unique-ID = {{ISI:000445204800002}},
}

@article{ ISI:000442238900004,
Author = {Switonski, Adam and Krzeszowski, Tomasz and Josinski, Henryk and Kwolek,
   Bogdan and Wojciechowski, Konrad},
Title = {{Gait recognition on the basis of markerless motion tracking and DTW
   transform}},
Journal = {{IET BIOMETRICS}},
Year = {{2018}},
Volume = {{7}},
Number = {{5}},
Pages = {{415-422}},
Month = {{SEP}},
Abstract = {{In this study, a framework for view-invariant gait recognition on the
   basis of markerless motion tracking and dynamic time warping (DTW)
   transform is presented. The system consists of a proposed markerless
   motion capture system as well as introduced classification method of
   mocap data. The markerless system estimates the three-dimensional
   locations of skeleton driven joints. Such skeleton-driven point clouds
   represent poses over time. The authors align point clouds in every pair
   of frames by calculating the minimal sum of squared distances between
   the corresponding joints. A point cloud distance measure with temporal
   context has been utilised in k-nearest neighbours algorithm to compare
   time instants of motion sequences. To enhance the generalisation of the
   recognition and to shorten the processing time, for every individual a
   single multidimensional time series among several multidimensional time
   series describing the individual's gait is established. The correct
   classification rate has been determined on the basis of a real dataset
   of human gait. It contains 230 gait cycles of 22 subjects. The tracking
   results on the basis of markerless motion capture are referenced to
   Vicon system, whereas the achieved accuracies of recognition are
   compared with the ones obtained by DTW that is based on rotational data.}},
DOI = {{10.1049/iet-bmt.2017.0134}},
ISSN = {{2047-4938}},
EISSN = {{2047-4946}},
ResearcherID-Numbers = {{Krzeszowski, Tomasz/H-7717-2019}},
ORCID-Numbers = {{Krzeszowski, Tomasz/0000-0001-7359-4637}},
Unique-ID = {{ISI:000442238900004}},
}

@article{ ISI:000440350800018,
Author = {van Veen, Martinus M. and Korteweg, Steven F. S. and Dijkstra, Pieter U.
   and Werker, Paul M. N.},
Title = {{Keeping the fat on the right spot prevents contour deformity in
   temporalis muscle transposition}},
Journal = {{JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY}},
Year = {{2018}},
Volume = {{71}},
Number = {{8}},
Pages = {{1181-1187}},
Month = {{AUG}},
Abstract = {{The temporalis muscle transposition is a reliable, one-stage reanimation
   technique for longstanding facial paralysis. In the variation described
   by Rubin, the muscle is released from the temporal bone and folded over
   the zygomatic arch towards the modiolus. This results in unsightly
   temporal hollowing and zygomatic bulging. We present a modification of
   this technique, which preserves the temporal fat pad in its anatomical
   location as well as conceals temporal hollowing and prevents zygomatic
   bulging.
   The data of 23 patients treated with this modification were analysed.
   May classification was used for evaluation of mouth reanimation. Experts
   and patients scored visibility of the contour deformity on a 100-mm
   visual analogue scale (VAS) (score 0 = poor/100 = best). 3D images of
   the face were used to measure temporal hollowing and zygomatic bulging.
   3D images were compared to those of controls with a similar gender and
   age distribution.
   After a median follow-up of 5.7 years, all patients achieved symmetry at
   rest. Eleven patients achieved symmetry while smiling with closed lips
   (May classification ``Good{''}). A median (interquartile range {[}IQR])
   VAS score of 19 (6; 41) was given by experts and 25 (5; 59) by patients
   themselves. 3D volumes of zygomatic bulging differed from those of
   control subjects, although all volume differences were small (median
   <3.3 ml) and temporal hollowing did not differ significantly.
   On the basis of our results, we conclude that our modified Rubin
   temporalis transposition technique provides an elegant way to conceal
   bulging over the zygomatic arch and prevents temporal hollowing, without
   the need for fascial extensions to reach the modiolus. (C) 2018 British
   Association of Plastic, Reconstructive and Aesthetic Surgeons. Published
   by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.bjps.2018.04.007}},
ISSN = {{1748-6815}},
EISSN = {{1878-0539}},
ResearcherID-Numbers = {{Werker, Paul/L-6293-2019}},
Unique-ID = {{ISI:000440350800018}},
}

@article{ ISI:000439703500027,
Author = {Gibelli, Daniele and Pucciarelli, Valentina and Cappella, Annalisa and
   Dolci, Claudia and Sforza, Chiarella},
Title = {{Are Portable Stereophotogrammetric Devices Reliable in Facial Imaging? A
   Validation Study of VECTRA H1 Device}},
Journal = {{JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{76}},
Number = {{8}},
Pages = {{1772-1784}},
Month = {{AUG}},
Abstract = {{Purpose: Modern 3-dimensional (3D) image acquisition systems represent a
   crucial technologic development in facial anatomy because of their
   accuracy and precision. The recently introduced portable devices can
   improve facial databases by increasing the number of applications. In
   the present study, the VECTRA H1 portable stereophotogrammetric device
   was validated to verify its applicability to 3D facial analysis.
   Materials and Methods: Fifty volunteers underwent 4 facial scans using
   portable VECTRA H1 and static VECTRA M3 devices (2 for each instrument).
   Repeatability of linear, angular, surface area, and volume measurements
   was verified within the device and between devices using the
   Bland-Altman test and the calculation of absolute and relative technical
   errors of measurement (TEM and rTEM, respectively). In addition, the 2
   scans obtained by the same device and the 2 scans obtained by different
   devices were registered and superimposed to calculate the root mean
   square (RMS; point-to-point) distance between the 2 surfaces.
   Results: Most linear, angular, and surface area measurements had high
   repeatability in M3 versus M3, H1 versus H1, and M3 versus H1
   comparisons (range, 82.2 to 98.7\%; TEM range, 0.3 to 2.0 mm, 0.4
   degrees to 1.8 degrees; rTEM range, 0.2 to 3.1\%). In contrast, volumes
   and RMS distances showed evident differences in M3 versus M3 and H1
   versus H1 comparisons and reached the maximum when scans from the 2
   different devices were compared.
   Conclusion: The portable VECTRA H1 device proved reliable for assessing
   linear measurements, angles, and surface areas; conversely, the
   influence of involuntary facial movements on volumes and RMS distances
   was more important compared with the static device. (C) 2018 American
   Association of Oral and Maxillofacial Surgeons}},
DOI = {{10.1016/j.joms.2018.01.021}},
ISSN = {{0278-2391}},
EISSN = {{1531-5053}},
ResearcherID-Numbers = {{Cappella, Annalisa/V-5586-2017
   Sforza, Chiarella/C-3008-2015}},
ORCID-Numbers = {{Sforza, Chiarella/0000-0001-6532-6464}},
Unique-ID = {{ISI:000439703500027}},
}

@article{ ISI:000435048200005,
Author = {Reiser, David and Vazquez-Arellano, Manuel and Paraforos, Dimitris S.
   and Garrido-Izard, Miguel and Griepentrog, Hans W.},
Title = {{Iterative individual plant clustering in maize with assembled 2D LiDAR
   data}},
Journal = {{COMPUTERS IN INDUSTRY}},
Year = {{2018}},
Volume = {{99}},
Pages = {{42-52}},
Month = {{AUG}},
Abstract = {{A two dimensional (2D) laser scanner was mounted at the front part of a
   small 4-wheel autonomous robot with differential steering, at an angle
   of 30 degrees pointing downwards. The machine was able to drive between
   maize rows and collect concurrent time-stamped data. A robotic total
   station tracked the position of a prism mounted on the vehicle. The
   total station and laser scanner data were fused to generate a three
   dimensional (3D) point cloud. This 3D representation was used to detect
   individual plant positions, which are of particular interest for
   applications such as phenotyping, individual plant treatment and
   precision weeding. Two different methodologies were applied to the 3D
   point cloud to estimate the position of the individual plants. The first
   methodology used the Euclidian Clustering on the entire point cloud. The
   second methodology utilised the position of an initial plant and the
   fixed plant spacing to search iteratively for the best clusters. The two
   algorithms were applied at three different plant growth stages. For the
   first method, results indicated a detection rate up to 73.7\% with a
   root mean square error of 3.6 cm. The second method was able to detect
   all plants (100\% detection rate) with an accuracy of 2.7-3.0 cm, taking
   the plant spacing of 13 cm into account.}},
DOI = {{10.1016/j.compind.2018.03.023}},
ISSN = {{0166-3615}},
EISSN = {{1872-6194}},
ORCID-Numbers = {{Paraforos, Dimitrios S./0000-0001-8275-8840
   Reiser, David/0000-0003-0158-6456}},
Unique-ID = {{ISI:000435048200005}},
}

@article{ ISI:000437024600042,
Author = {Wang, Yuehang and Li, Zhengxiong and Vu, Tri and Nyayapathi, Nikhila and
   Oh, Kwang W. and Xu, Wenyao and Xia, Jun},
Title = {{A Robust and Secure Palm Vessel Biometric Sensing System Based on
   Photoacoustics}},
Journal = {{IEEE SENSORS JOURNAL}},
Year = {{2018}},
Volume = {{18}},
Number = {{14}},
Pages = {{5993-6000}},
Month = {{JUL 15}},
Abstract = {{In this paper, we propose a new palm vessel bio-metric sensing system
   based on photoacoustic imaging, which is an emerging technique that
   allows high-resolution visualization of optical absorption in deep
   tissue. Our system consists of an ultrasound (US) linear transducer
   array, an US data acquisition system, and an Nd:YAG laser emitting
   1064-nm wavelength. By scanning the array, we could get a 3-D image of
   palm vasculature. The 3-D image is further combined with our newly
   developed algorithm, Earth Mover's Distance-Radiographic Testing, to
   provide precise matching and robust recognition rate. Compared to
   conventional vein sensing techniques, our system demonstrates deeper
   imaging depth and better spatial resolution, offering securer biometric
   features to fight against counterfeits. In this paper, we imaged 20
   different hands at various poses and quantified our system performance.
   We found that the usability and accuracy of our system are comparable to
   conventional biometric techniques, such as fingerprint imaging and face
   identification. Our technique can open up avenues for better liveness
   detection and biometric measurements.}},
DOI = {{10.1109/JSEN.2018.2843119}},
ISSN = {{1530-437X}},
EISSN = {{1558-1748}},
ORCID-Numbers = {{Nyayapathi, Nikhila/0000-0002-3711-797X}},
Unique-ID = {{ISI:000437024600042}},
}

@article{ ISI:000451673800001,
Author = {Chen, Ying and Haerdie, Wolfgang K. and He, Qiang and Majer, Piotr},
Title = {{Risk related brain regions detection and individual risk classification
   with 3D image FPCA}},
Journal = {{STATISTICS \& RISK MODELING}},
Year = {{2018}},
Volume = {{35}},
Number = {{3-4}},
Pages = {{89-110}},
Month = {{JUL}},
Abstract = {{Understanding how people make decisions from risky choices has attracted
   increasing attention of researchers in economics, psychology and
   neuroscience. While economists try to evaluate individual's risk
   preference through mathematical modeling, neuroscientists answer the
   question by exploring the neural activities of the brain. We propose a
   model-free method, 3-dimensional image functional principal component
   analysis (3DIF), to provide a connection between active risk related
   brain region detection and individual's risk preference. The 3DIF
   methodology is directly applicable to 3-dimensional image data without
   artificial vectorization or mapping and simultaneously guarantees the
   contiguity of risk related brain regions rather than discrete voxels.
   Simulation study evidences an accurate and reasonable region detection
   using the 3DIF method. In real data analysis, five important risk
   related brain regions are detected, including parietal cortex (PC),
   ventrolateral prefrontal cortex (VLPFC), lateral orbifrontal cortex
   (IOFC), anterior insula (aINS) and dorsolateral prefrontal cortex
   (DLPFC), while the alternative methods only identify limited risk
   related regions. Moreover, the 3DIF method is useful for extraction of
   subjective specific signature scores that carry explanatory power for
   individual's risk attitude. In particular, the 3DIF method perfectly
   classifies both strongly and weakly risk averse subjects for in-sample
   analysis. In out-of-sample experiment, it achieves 73\%-88\% overall
   accuracy, among which 90\%-100\% strongly risk averse subjects and
   49\%-71\% weakly risk averse subjects are correctly classified with
   leave-k-out cross validations.}},
DOI = {{10.1515/strm-2017-0011}},
ISSN = {{2193-1402}},
EISSN = {{2196-7040}},
ORCID-Numbers = {{Chen, Ying/0000-0002-2577-7348}},
Unique-ID = {{ISI:000451673800001}},
}

@article{ ISI:000441334300307,
Author = {Rymarczyk, Tomasz and Klosowski, Grzegorz and Kozlowski, Edward},
Title = {{A Non-Destructive System Based on Electrical Tomography and Machine
   Learning to Analyze the Moisture of Buildings}},
Journal = {{SENSORS}},
Year = {{2018}},
Volume = {{18}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{This article presents the results of research on a new method of spatial
   analysis of walls and buildings moisture. Due to the fact that
   destructive methods are not suitable for historical buildings of great
   architectural significance, a non-destructive method based on electrical
   tomography has been adopted. A hybrid tomograph with special sensors was
   developed for the measurements. This device enables the acquisition of
   data, which are then reconstructed by appropriately developed methods
   enabling spatial analysis of wet buildings. Special electrodes that
   ensure good contact with the surface of porous building materials such
   as bricks and cement were introduced. During the research, a group of
   algorithms enabling supervised machine learning was analyzed. They have
   been used in the process of converting input electrical values into
   conductance depicted by the output image pixels. The conductance values
   of individual pixels of the output vector made it possible to obtain
   images of the interior of building walls as both flat intersections (2D)
   and spatial (3D) images. The presented group of algorithms has a high
   application value. The main advantages of the new methods are: high
   accuracy of imaging, low costs, high processing speed, ease of
   application to walls of various thickness and irregular surface. By
   comparing the results of tomographic reconstructions, the most efficient
   algorithms were identified.}},
DOI = {{10.3390/s18072285}},
Article-Number = {{2285}},
ISSN = {{1424-8220}},
ResearcherID-Numbers = {{Klosowski, Grzegorz/B-8899-2017
   Kozlowski, Edward/A-6882-2013
   Rymarczyk, Tomasz/D-6177-2015}},
ORCID-Numbers = {{Klosowski, Grzegorz/0000-0001-7927-3674
   Kozlowski, Edward/0000-0002-7147-4903
   Rymarczyk, Tomasz/0000-0002-3524-9151}},
Unique-ID = {{ISI:000441334300307}},
}

@article{ ISI:000440122900009,
Author = {Huang, Delin and Du, Shichang and Li, Guilong and Zhao, Chen and Deng,
   Yafei},
Title = {{Detection and monitoring of defects on three-dimensional curved surfaces
   based on high-density point cloud data}},
Journal = {{PRECISION ENGINEERING-JOURNAL OF THE INTERNATIONAL SOCIETIES FOR
   PRECISION ENGINEERING AND NANOTECHNOLOGY}},
Year = {{2018}},
Volume = {{53}},
Pages = {{79-95}},
Month = {{JUL}},
Abstract = {{The surface quality of three-dimensional (3-D) curved surfaces is one of
   the most important factors that can directly influence the performance
   of the final product. This paper presents a systematic approach for
   detection and monitoring of defects on 3-D curved surfaces based on
   high-density point cloud data. Firstly, an algorithm to remove outliers
   and a boundary recognition algorithm are proposed to divide the entire
   3-D curved surface including millions of measured points into multiple
   sub-regions. Secondly, two new evaluation indexes based on wavelet
   packet entropy and normal vector are explored to represent the features
   of the multiple sub-regions to determine whether the sub-regions are
   out-of-limit (OOL) of specifications. Thirdly, three quality parameters
   representing quality characteristics of a curved surface are presented
   and their values are calculated based on the clusters of OOL
   sub-regions. Finally, three individual control charts are presented to
   monitor the three quality parameters. As long as any quality parameter
   is out of the control range, the manufacturing process of the curved
   surface is determined to be out-of-control (OOC). The results of a case
   study show that the proposed approach can effectively identify the OOC
   manufacturing process and detect defects on 3-D curved surfaces.}},
DOI = {{10.1016/j.precisioneng.2018.03.001}},
ISSN = {{0141-6359}},
EISSN = {{1873-2372}},
Unique-ID = {{ISI:000440122900009}},
}

@article{ ISI:000434294800004,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal and Shafait, Faisal and Reid,
   Ian},
Title = {{Dense 3D Face Correspondence}},
Journal = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}},
Year = {{2018}},
Volume = {{40}},
Number = {{7}},
Pages = {{1584-1598}},
Month = {{JUL}},
Abstract = {{We present an algorithm that automatically establishes dense
   correspondences between a large number of 3D faces. Starting from
   automatically detected sparse correspondences on the outer boundary of
   3D faces, the algorithm triangulates existing correspondences and
   expands them iteratively by matching points of distinctive surface
   curvature along the triangle edges. After exhausting keypoint matches,
   further correspondences are established by generating evenly distributed
   points within triangles by evolving level set geodesic curves from the
   centroids of large triangles. A deformable model (K3DM) is constructed
   from the dense corresponded faces and an algorithm is proposed for
   morphing the K3DM to fit unseen faces. This algorithm iterates between
   rigid alignment of an unseen face followed by regularized morphing of
   the deformable model. We have extensively evaluated the proposed
   algorithms on synthetic data and real 3D faces from the FRGCv2,
   Bosphorus, BU3DFE and UND Ear databases using quantitative and
   qualitative benchmarks. Our algorithm achieved dense correspondences
   with a mean localisation error of 1.28 mm on synthetic faces and
   detected 14 anthropometric landmarks on unseen real faces from the
   FRGCv2 database with 3 mm precision. Furthermore, our deformable model
   fitting algorithm achieved 98.5 percent face recognition accuracy on the
   FRGCv2 and 98.6 percent on Bosphorus database. Our dense model is also
   able to generalize to unseen datasets.}},
DOI = {{10.1109/TPAMI.2017.2725279}},
ISSN = {{0162-8828}},
EISSN = {{1939-3539}},
ORCID-Numbers = {{Reid, Ian/0000-0001-7790-6423
   Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000434294800004}},
}

@article{ ISI:000435372100001,
Author = {Burkus, Mate and Schlegl, Adam Tibor and O'Sullivan, Ian and Markus,
   Istvan and Vermes, Csaba and Tunyogi-Csapo, Miklos},
Title = {{Sagittal plane assessment of spino-pelvic complex in a Central European
   population with adolescent idiopathic scoliosis: a case control study}},
Journal = {{SCOLIOSIS AND SPINAL DISORDERS}},
Year = {{2018}},
Volume = {{13}},
Month = {{JUN 14}},
Abstract = {{Background: Scoliosis is a complex three-dimensional deformity. While
   the frontal profile is well understood, increasing attention has turned
   to balance in the sagittal plane. The present study evaluated changes in
   sagittal spino-pelvic parameters in a large Hungarian population with
   adolescent idiopathic scoliosis.
   Methods: EOS 2D/3D images of 458 scoliotic and 69 control cases were
   analyzed. After performing 3D reconstructions, the sagittal parameters
   were assessed as a whole and by curve type using independent sample t
   test and linear regression analysis.
   Results: Patients with scoliosis had significantly decreased thoracic
   kyphosis (p < 0.001) with values T1-T12, 34.1 +/- 17.1 degrees vs. 43.4
   +/- 12.7 degrees in control; T4-T12, 27.1 +/- 18.8 degrees vs. 37.7 +/-
   15.1 degrees in control; and T5-T12, 24.9 +/- 15.8 degrees vs. 32.9 +/-
   15. 0 degrees in control. Changes in thoracic kyphosis correlated with
   magnitude of the Cobb angle (p < 0.001). No significant change was found
   in lumbar lordosis and the pelvic parameters. After substratification
   according to the Lenke classification and individually evaluating
   subgroups, results were similar with a significant decrease in only the
   thoracic kyphosis. A strong correlation was seen between sacral slope,
   pelvic incidence, and lumbar lordosis, and between pelvic version and
   thoracic kyphosis in control and scoliotic groups, whereas pelvic
   incidence was also seen to be correlated with thoracic kyphosis in
   scoliosis patients.
   Conclusion: Adolescent idiopathic scoliosis patients showed a
   significant decrease in thoracic kyphosis, and the magnitude of the
   decrease was directly related to the Cobb angle. Changes in pelvic
   incidence were minimal but were also significantly correlated with
   thoracic changes. Changes were similar though not identical to those
   seen in other Caucasian studies and differed from those in other
   ethnicities. Scoliotic curves and their effect on pelvic balance must
   still be regarded as individual to each patient, necessitating
   individual assessment, although changes perhaps can be predicted by
   patient ethnicity.}},
DOI = {{10.1186/s13013-018-0156-0}},
Article-Number = {{10}},
ISSN = {{2397-1789}},
ORCID-Numbers = {{Schlegl, Adam Tibor/0000-0003-0349-2525}},
Unique-ID = {{ISI:000435372100001}},
}

@article{ ISI:000440272800007,
Author = {Kwiek, Bartlomiej and Ambroziak, Marcin and Osipowicz, Katarzyna and
   Kowalewski, Cezary and Rozalski, Michal},
Title = {{Treatment of Previously Treated Facial Capillary Malformations: Results
   of Single-Center Retrospective Objective 3-Dimensional Analysis of the
   Efficacy of Large Spot 532 nm Lasers}},
Journal = {{DERMATOLOGIC SURGERY}},
Year = {{2018}},
Volume = {{44}},
Number = {{6}},
Pages = {{803-813}},
Month = {{JUN}},
Abstract = {{BACKGROUND Current treatment of facial capillary malformations (CM) has
   limited efficacy.
   OBJECTIVE To assess the efficacy of large spot 532 nm lasers for the
   treatment of previously treated facial CM with the use of 3-dimensional
   (3D) image analysis.
   PATIENTS AND METHODS Forty-three white patients aged 6 to 59 were
   included in this study. Patients had 3D photography performed before and
   after treatment with a 532 nm Nd:YAG laser with large spot and contact
   cooling. Objective analysis of percentage improvement based on 3D
   digital assessment of combined color and area improvement (global
   clearance effect {[}GCE]) were performed.
   RESULTS The median maximal improvement achieved during the treatment
   (GCE(max)) was 59.1\%. The mean number of laser procedures required to
   achieve this improvement was 6.2 (range 1-16). Improvement of minimum
   25\% (GCE25) was achieved by 88.4\% of patients, a minimum of 50\%
   (GCE50) by 61.1\%, a minimum of 75\% (GCE75) by 25.6\%, and a minimum of
   90\% (GCE90) by 4.6\%. Patients previously treated with pulsed dye
   lasers had a significantly less response than those treated with other
   modalities (GCE (max) 37.3\% vs 61.8\%, respectively).
   CONCLUSION A large spot 532 nm laser is effective in previously treated
   patients with facial CM.}},
DOI = {{10.1097/DSS.0000000000001447}},
ISSN = {{1076-0512}},
EISSN = {{1524-4725}},
ORCID-Numbers = {{Kowalewski, Cezary/0000-0002-6608-9066}},
Unique-ID = {{ISI:000440272800007}},
}

@article{ ISI:000435193700027,
Author = {Wang, Di and Brunner, Jasmin and Ma, Zhenyu and Lu, Hao and Hollaus,
   Markus and Pang, Yong and Pfeifer, Norbert},
Title = {{Separating Tree Photosynthetic and Non-Photosynthetic Components from
   Point Cloud Data Using Dynamic Segment Merging}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{5}},
Month = {{MAY}},
Abstract = {{Many biophysical forest properties such as wood volume and leaf area
   index (LAI) require prior knowledge on either photosynthetic or
   non-photosynthetic components. Laser scanning appears to be a helpful
   technique in nondestructively quantifying forest structures, as it can
   acquire an accurate three-dimensional point cloud of objects. In this
   study, we propose an unsupervised geometry-based method named Dynamic
   Segment Merging (DSM) to identify non-photosynthetic components of trees
   by semantically segmenting tree point clouds, and examining the linear
   shape prior of each resulting segment. We tested our method using one
   single tree dataset and four plot-level datasets, and compared our
   results to a supervised machine learning method. We further demonstrated
   that by using an optimal neighborhood selection method that involves
   multi-scale analysis, the results were improved. Our results showed that
   the overall accuracy ranged from 81.8\% to 92.0\% with an average value
   of 87.7\%. The supervised machine learning method had an average overall
   accuracy of 86.4\% for all datasets, on account of a collection of
   manually delineated representative training data. Our study indicates
   that separating tree photosynthetic and non-photosynthetic components
   from laser scanning data can be achieved in a fully unsupervised manner
   without the need of training data and user intervention.}},
DOI = {{10.3390/f9050252}},
Article-Number = {{252}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Wang, Di/T-2571-2018
   }},
ORCID-Numbers = {{Wang, Di/0000-0003-0232-8862
   Pang, Yong/0000-0002-9760-6580
   Pfeifer, Norbert/0000-0002-2348-7929}},
Unique-ID = {{ISI:000435193700027}},
}

@article{ ISI:000430653400001,
Author = {Lopez, R. and Gantet, P. and Julian, A. and Hitzel, A. and
   Herbault-Barres, B. and Alshehri, S. and Payoux, P.},
Title = {{Value of PET/CT 3D visualization of head and neck squamous cell
   carcinoma extended to mandible}},
Journal = {{JOURNAL OF CRANIO-MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{46}},
Number = {{5}},
Pages = {{743-748}},
Month = {{MAY}},
Abstract = {{Purpose: To study an original 3D visualization of head and neck squamous
   cell carcinoma extending to the mandible by using {[}18F]-NaF PET/CT and
   {[}18F]-FDG PET/CT imaging along with a new innovative FDG and NaF image
   analysis using dedicated software. The main interest of the 3D
   evaluation is to have a better visualization of bone extension in such
   cancers and that could also avoid unsatisfying surgical treatment later
   on. Patients and methods: A prospective study was carried out from
   November 2016 to September 2017. Twenty patients with head and neck
   squamous cell carcinoma extending to the mandible (stage 4 in the UICC
   classification) underwent {[}18F]-NaF and {[}18F]-FDG PET/CT. We
   compared the delineation of 3D quantification obtained with {[}18F]-NaF
   and {[}18F]-FDG PET/CT. In order to carry out this comparison, a method
   of visualisation and quantification of PET images was developed. This
   new approach was based on a process of quantification of radioactive
   activity within the mandibular bone that objectively defined the
   significant limits of this activity on PET images and on a 3D
   visualization. Furthermore, the spatial limits obtained by analysis of
   the PET/CT 3D images were compared to those obtained by
   histopathological examination of mandibular resection which confirmed
   intraosseous extension to the mandible. Results: The {[}18F]-NaF PET/CT
   imaging confirmed the mandibular extension in 85\% of cases and was not
   shown in {[}18F]-FDG PET/CT imaging. The {[}18F]-NaF PET/CT was
   significantly more accurate than {[}18F]-FDG PET/CT in 3D assessment of
   intraosseous extension of head and neck squamous cell carcinoma. This
   new 3D information shows the importance in the imaging approach of
   cancers. All cases of mandibular extension suspected on {[}18F]-NaF
   PET/CT imaging were confirmed based on histopathological results as a
   reference. Conclusions: The {[}18F]-NaF PET/CT 3D visualization should
   be included in the pre-treatment workups of head and neck cancers. With
   the use of a dedicated software which enables objective delineation of
   radioactive activity within the bone, it gives a very encouraging
   results. The {[}18F]-FDG PET/CT appears insufficient to confirm
   mandibular extension. This new 3D simulation management is expected to
   avoid under treatment of patients with intraosseous mandibular extension
   of head and neck cancers. However, there is also a need for a further
   study that will compare the interest of PET/CT and PET/MRI in this
   indication. (C) 2018 European Association for Cranio-Maxillo-Facial
   Surgery. Published by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jcms.2018.02.007}},
ISSN = {{1010-5182}},
EISSN = {{1878-4119}},
Unique-ID = {{ISI:000430653400001}},
}

@article{ ISI:000429200400012,
Author = {Majka, Piotr and Chlodzinska, Natalia and Turlejski, Krzysztof and
   Banasik, Tomasz and Djavadian, Ruzanna L. and Weglarz, Wladyslaw P. and
   Wojcik, Daniel K.},
Title = {{A three-dimensional stereotaxic atlas of the gray short-tailed opossum
   (Monodelphis domestica) brain}},
Journal = {{BRAIN STRUCTURE \& FUNCTION}},
Year = {{2018}},
Volume = {{223}},
Number = {{4}},
Pages = {{1779-1795}},
Month = {{MAY}},
Abstract = {{The gray short-tailed opossum (Monodelphis domestica) is a small
   marsupial gaining recognition as a laboratory animal in biomedical
   research. Despite numerous studies on opossum neuroanatomy, a consistent
   and comprehensive neuroanatomical reference for this species is still
   missing. Here we present the first three-dimensional, multimodal atlas
   of the Monodelphis opossum brain. It is based on four complementary
   imaging modalities: high resolution ex vivo magnetic resonance images,
   micro-computed tomography scans of the cranium, images of the face of
   the cutting block, and series of sections stained with the Nissl method
   and for myelinated fibers. Individual imaging modalities were
   reconstructed into a three-dimensional form and then registered to the
   MR image by means of affine and deformable registration routines. Based
   on a superimposition of the 3D images, 113 anatomical structures were
   demarcated and the volumes of individual regions were measured. The
   stereotaxic coordinate system was defined using a set of cranial
   landmarks: interaural line, bregma, and lambda, which allows for easy
   expression of any location within the brain with respect to the skull.
   The atlas is released under the Creative Commons license and available
   through various digital atlasing web services.}},
DOI = {{10.1007/s00429-017-1540-x}},
ISSN = {{1863-2653}},
EISSN = {{1863-2661}},
ResearcherID-Numbers = {{Weglarz, Wladyslaw/W-5770-2018
   Wojcik, Daniel K/C-6334-2008
   }},
ORCID-Numbers = {{Weglarz, Wladyslaw/0000-0002-3390-3615
   Majka, Piotr/0000-0002-9055-8686
   Wojcik, Daniel K/0000-0003-0812-9872
   Krzysztof, Turlejski/0000-0001-6708-7815
   Djavadian, Ruzanna/0000-0002-0416-0234}},
Unique-ID = {{ISI:000429200400012}},
}

@article{ ISI:000425652100003,
Author = {Li, Zhan and Schaefer, Michael and Strahler, Alan and Schaaf, Crystal
   and Jupp, David},
Title = {{On the utilization of novel spectral laser scanning for
   three-dimensional classification of vegetation elements}},
Journal = {{INTERFACE FOCUS}},
Year = {{2018}},
Volume = {{8}},
Number = {{2}},
Month = {{APR 6}},
Abstract = {{The Dual-Wavelength Echidna Lidar (DWEL), a full waveform terrestrial
   laser scanner (TLS), has been used to scan a variety of forested and
   agricultural environments. From these scanning campaigns, we summarize
   the benefits and challenges given by DWEL's novel coaxial
   dual-wavelength scanning technology, particularly for the
   three-dimensional (3D) classification of vegetation elements.
   Simultaneous scanning at both 1064 nm and 1548 nm by DWEL instruments
   provides a new spectral dimension to TLS data that joins the 3D spatial
   dimension of lidar as an information source. Our point cloud
   classification algorithm explores the utilization of both spectral and
   spatial attributes of individual points from DWEL scans and highlights
   the strengths and weaknesses of each attribute domain. The spectral and
   spatial attributes for vegetation element classification each perform
   better in different parts of vegetation (canopy interior, fine branches,
   coarse trunks, etc.) and under different vegetation conditions (dead or
   live, leaf-on or leaf-off, water content, etc.). These environmental
   characteristics of vegetation, convolved with the lidar instrument
   specifications and lidar data quality, result in the actual capabilities
   of spectral and spatial attributes to classify vegetation elements in 3D
   space. The spectral and spatial information domains thus complement each
   other in the classification process. The joint use of both not only
   enhances the classification accuracy but also reduces its variance
   across the multiple vegetation types we have examined, highlighting the
   value of the DWEL as a new source of 3D spectral information. Wider
   deployment of the DWEL instruments is in practice currently held back by
   challenges in instrument development and the demands of data processing
   required by coaxial dual-or multi-wavelength scanning. But the
   simultaneous 3D acquisition of both spectral and spatial features,
   offered by new multispectral scanning instruments such as the DWEL,
   opens doors to study biophysical and biochemical properties of forested
   and agricultural ecosystems at more detailed scales.}},
DOI = {{10.1098/rsfs.2017.0039}},
Article-Number = {{20170039}},
ISSN = {{2042-8898}},
EISSN = {{2042-8901}},
ORCID-Numbers = {{Schaefer, Michael/0000-0001-6584-9521
   Li, Zhan/0000-0001-6307-5200}},
Unique-ID = {{ISI:000425652100003}},
}

@article{ ISI:000433517100003,
Author = {Demisse, Girum G. and Aouada, Djamila and Ottersten, Bjorn},
Title = {{Deformation-Based 3D Facial Expression Representation}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2018}},
Volume = {{14}},
Number = {{1, S}},
Month = {{APR}},
Abstract = {{We propose a deformation-based representation for analyzing expressions
   fromthree-dimensional (3D) faces. A point cloud of a 3D face is
   decomposed into an ordered deformable set of curves that start from a
   fixed point. Subsequently, a mapping function is defined to identify the
   set of curves with an element of a high-dimensional matrix Lie group,
   specifically the direct product of SE(3). Representing 3D faces as an
   element of a high-dimensional Lie group has two main advantages. First,
   using the group structure, facial expressions can be decoupled from a
   neutral face. Second, an underlying non-linear facial expression
   manifold can be captured with the Lie group and mapped to a linear
   space, Lie algebra of the group. This opens up the possibility of
   classifying facial expressions with linear models without compromising
   the underlying manifold. Alternatively, linear combinations of
   linearised facial expressions can be mapped back from the Lie algebra to
   the Lie group. The approach is tested on the Binghamton University 3D
   Facial Expression (BU-3DFE) and the Bosphorus datasets. The results show
   that the proposed approach performed comparably, on the BU-3DFE dataset,
   without using features or extensive landmark points.}},
DOI = {{10.1145/3176649}},
Article-Number = {{17}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
ResearcherID-Numbers = {{Ottersten, Bjorn/G-1005-2011}},
ORCID-Numbers = {{Ottersten, Bjorn/0000-0003-2298-6774}},
Unique-ID = {{ISI:000433517100003}},
}

@article{ ISI:000424962000005,
Author = {Czerniawski, T. and Sankaran, B. and Nahangi, M. and Haas, C. and Leite,
   F.},
Title = {{6D DBSCAN-based segmentation of building point clouds for planar object
   classification}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2018}},
Volume = {{88}},
Pages = {{44-58}},
Month = {{APR}},
Abstract = {{Due to constraints in manufacturing and construction, buildings and many
   of the manmade objects within them are often rectangular and composed of
   planar parts. Detection and analysis of planes is, therefore, central to
   processing point clouds captured in these spaces. This paper presents a
   study of the semantic information stored in the planar objects of noisy
   building point clouds. The dataset considered is the Scene Meshes
   Dataset with aNNotations (SceneNN), a collection of over 100 indoor
   scenes captured by consumer-grade depth cameras. All planar objects
   within the dataset are detected using a new point cloud segmentation
   method that applies Density Based Spatial Clustering of Applications
   with Noise (DBSCAN) in a six dimensional clustering space. With all
   planes isolated, an extensive list of features describing the planes is
   extracted and studied using feature selection. Then dimensionality
   reduction and unsupervised learning are used to explore the
   discriminative ability of the final feature set as well as emergent
   class groupings. Finally, we train a bagged decision tree classifier
   that achieves 71.2\% accuracy in predicting the object class from which
   individual planes originate.}},
DOI = {{10.1016/j.autcon.2017.12.029}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
ORCID-Numbers = {{Czerniawski, Thomas/0000-0002-7310-6522}},
Unique-ID = {{ISI:000424962000005}},
}

@article{ ISI:000430067700001,
Author = {Landschoff, Jannes and Du Plessis, Anton and Griffiths, Charles L.},
Title = {{A micro X-ray computed tomography dataset of South African hermit crabs
   (Crustacea: Decapoda: Anomura: Paguroidea) containing scans of two rare
   specimens and three recently described species}},
Journal = {{GIGASCIENCE}},
Year = {{2018}},
Volume = {{7}},
Number = {{4}},
Month = {{MAR 14}},
Abstract = {{Background: Along with the conventional deposition of physical types at
   natural history museums, the deposition of 3-dimensional (3D) image data
   has been proposed for rare and valuable museum specimens, such as
   irrepla ceable type material. Findings: Micro computed tomography (mu
   CT) scan data of 5 hermit crab species from South Africa, including rare
   specimens and type material, depicted main identification
   characteristics of calcified body parts. However, low-image contrasts,
   especially in larger (> 50 mm total length) specimens, did not allow
   sufficient 3D reconstructions of weakly calcified and fine
   characteristics, such as soft tissue of the pleon, mouthparts, gills,
   and setation. Reconstructions of soft tissue were sometimes possible,
   depending on individual sample and scanning characteristics. The raw
   data of seven scans are publicly available for download from the GigaDB
   repository. Conclusions: Calcified body parts visualized from mu CT data
   can aid taxonomic validation and provide additional, virtual deposition
   of rare specimens. The use of a nondestructive, nonstaining mu CT
   approach for taxonomy, reconstructions of soft tissue structures,
   microscopic spines, and setae depend on species characteristics.
   Constrained to these limitations, the presented dataset can be used for
   future morphological studies. However, our virtual specimens will be
   most valuable to taxonomists who can download a digital avatar for 3D
   examination. Simultaneously, in the event of physical damage to or loss
   of the original physical specimen, this dataset serves as a vital
   insurance policy.}},
DOI = {{10.1093/gigascience/giy022}},
ISSN = {{2047-217X}},
ORCID-Numbers = {{du Plessis, Anton/0000-0002-4370-8661}},
Unique-ID = {{ISI:000430067700001}},
}

@article{ ISI:000428508200022,
Author = {Alonzo, Michael and Andersen, Hans-Erik and Morton, Douglas C. and Cook,
   Bruce D.},
Title = {{Quantifying Boreal Forest Structure and Composition Using UAV Structure
   from Motion}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{The vast extent and inaccessibility of boreal forest ecosystems are
   barriers to routine monitoring of forest structure and composition. In
   this research, we bridge the scale gap between intensive but sparse plot
   measurements and extensive remote sensing studies by collecting forest
   inventory variables at the plot scale using an unmanned aerial vehicle
   (UAV) and a structure from motion (SfM) approach. At 20 Forest Inventory
   and Analysis (FIA) subplots in interior Alaska, we acquired overlapping
   imagery and generated dense, 3D, RGB (red, green, blue) point clouds. We
   used these data to model forest type at the individual crown scale as
   well as subplot-scale tree density (TD), basal area (BA), and
   aboveground biomass (AGB). We achieved 85\% cross-validation accuracy
   for five species at the crown level. Classification accuracy was
   maximized using three variables representing crown height, form, and
   color. Consistent with previous UAV-based studies, SfM point cloud data
   generated robust models of TD (r(2) = 0.91), BA (r(2) = 0.79), and AGB
   (r(2) = 0.92), using a mix of plot-and crown-scale information. Precise
   estimation of TD required either segment counts or species information
   to differentiate black spruce from mixed white spruce plots. The
   accuracy of species-specific estimates of TD, BA, and AGB at the plot
   scale was somewhat variable, ranging from accurate estimates of black
   spruce TD (+/1\%) and aspen BA (-2\%) to misallocation of aspen AGB
   (+118\%) and white spruce AGB (-50\%). These results convey the
   potential utility of SfM data for forest type discrimination in FIA
   plots and the remaining challenges to develop classification approaches
   for species-specific estimates at the plot scale that are more robust to
   segmentation error.}},
DOI = {{10.3390/f9030119}},
Article-Number = {{119}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Morton, Douglas/D-5044-2012}},
Unique-ID = {{ISI:000428508200022}},
}

@article{ ISI:000428936900023,
Author = {Yuan, Lin and Chen, Fanglin and Zeng, Ling-Li and Wang, Lubin and Hu,
   Dewen},
Title = {{Gender Identification of Human Brain Image with A Novel 3D Descriptor}},
Journal = {{IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS}},
Year = {{2018}},
Volume = {{15}},
Number = {{2}},
Pages = {{551-561}},
Month = {{MAR-APR}},
Abstract = {{Determining gender by examining the human brain is not a simple task
   because the spatial structure of the human brain is complex, and no
   obvious differences can be seen by the naked eyes. In this paper, we
   propose a novel three-dimensional feature descriptor, the
   three-dimensional weighted histogram of gradient orientation (3D WHGO)
   to describe this complex spatial structure. The descriptor combines
   local information for signal intensity and global three-dimensional
   spatial information for the whole brain. We also improve a framework to
   address the classification of three-dimensional images based on MRI.
   This framework, three-dimensional spatial pyramid, uses additional
   information regarding the spatial relationship between features. The
   proposed method can be used to distinguish gender at the individual
   level. We examine our method by using the gender identification of
   individual magnetic resonance imaging (MRI) scans of a large sample of
   healthy adults across four research sites, resulting in up to
   individual-level accuracies under the optimized parameters for
   distinguishing between females and males. Compared with previous
   methods, the proposed method obtains higher accuracy, which suggests
   that this technology has higher discriminative power. With its improved
   performance in gender identification, the proposed method may have the
   potential to inform clinical practice and aid in research on
   neurological and psychiatric disorders.}},
DOI = {{10.1109/TCBB.2015.2448081}},
ISSN = {{1545-5963}},
EISSN = {{1557-9964}},
ResearcherID-Numbers = {{Hu, Dewen/D-1978-2015}},
ORCID-Numbers = {{Hu, Dewen/0000-0001-7357-0053}},
Unique-ID = {{ISI:000428936900023}},
}

@article{ ISI:000427548400003,
Author = {Hu, Guiqing and Taylor, Dianne W. and Liu, Jun and Taylor, Kenneth A.},
Title = {{Identification of interfaces involved in weak interactions with
   application to F-actin-aldolase rafts}},
Journal = {{JOURNAL OF STRUCTURAL BIOLOGY}},
Year = {{2018}},
Volume = {{201}},
Number = {{3}},
Pages = {{199-209}},
Month = {{MAR}},
Abstract = {{Macromolecular interactions occur with widely varying affinities. Strong
   interactions form well defined interfaces but weak interactions are more
   dynamic and variable. Weak interactions can collectively lead to large
   structures such as microvilli via cooperativity and are often the
   precursors of much stronger interactions, e.g. the initial actin-myosin
   interaction during muscle contraction. Electron tomography combined with
   subvolume alignment and classification is an ideal method for the study
   of weak interactions because a 3-D image is obtained for the individual
   interactions, which subsequently are characterized collectively. Here we
   describe a method to characterize heterogeneous F-actin-aldolase
   interactions in 2-D rafts using electron tomography. By forming separate
   averages of the two constituents and fitting an atomic structure to each
   average, together with the alignment information which relates the raw
   motif to the average, an atomic model of each crosslink is determined
   and a frequency map of contact residues is computed. The approach should
   be applicable to any large structure composed of constituents that
   interact weakly and heterogeneously.}},
DOI = {{10.1016/j.jsb.2017.11.005}},
ISSN = {{1047-8477}},
EISSN = {{1095-8657}},
Unique-ID = {{ISI:000427548400003}},
}

@article{ ISI:000427313700006,
Author = {Herfort, Benjamin and Hoefle, Bernhard and Klonner, Carolin},
Title = {{3D micro-mapping: Towards assessing the quality of crowdsourcing to
   support 3D point cloud analysis}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2018}},
Volume = {{137}},
Pages = {{73-83}},
Month = {{MAR}},
Abstract = {{In this paper, we propose a method to crowdsource the task of complex
   three-dimensional information extraction from 3D point clouds. We design
   web-based 3D micro tasks tailored to assess segmented LiDAR point clouds
   of urban trees and investigate the quality of the approach in an
   empirical user study. Our results for three different experiments with
   increasing complexity indicate that a single crowd sourcing task can be
   solved in a very short time of less than five seconds on average.
   Furthermore, the results of our empirical case study reveal that the
   accuracy, sensitivity and precision of 3D crowdsourcing are high for
   most information extraction problems. For our first experiment (binary
   classification with single answer) we obtain an accuracy of 91\%, a
   sensitivity of 95\% and a precision of 92\%. For the more complex tasks
   of the second Experiment 2 (multiple answer classification) the accuracy
   ranges from 65\% to 99\% depending on the label class. Regarding the
   third experiment - the determination of the crown base height of
   individual trees - our study highlights that crowdsourcing can be a tool
   to obtain values with even higher accuracy in comparison to an automated
   computer-based approach. Finally, we found out that the accuracy of the
   crowdsourced results for all experiments is hardly influenced by
   characteristics of the input point cloud data and of the users.
   Importantly, the results' accuracy can be estimated using agreement
   among volunteers as an intrinsic indicator, which makes a broad
   application of 3D micro-mapping very promising. (C) 2018 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2018.01.009}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Hofle, Bernhard/A-4702-2010
   }},
ORCID-Numbers = {{Hofle, Bernhard/0000-0001-5849-1461
   Klonner, Carolin/0000-0003-1981-2204}},
Unique-ID = {{ISI:000427313700006}},
}

@article{ ISI:000418312200002,
Author = {Kukunda, Collins B. and Duque-Lazo, Joaquin and Gonzalez-Ferreiro,
   Eduardo and Thaden, Hauke and Kleinn, Christoph},
Title = {{Ensemble classification of individual Pinus crowns from multispectral
   satellite imagery and airborne LiDAR}},
Journal = {{INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION}},
Year = {{2018}},
Volume = {{65}},
Pages = {{12-23}},
Month = {{MAR}},
Abstract = {{Distinguishing tree species is relevant in many contexts of remote
   sensing assisted forest inventory. Accurate tree species maps support
   management and conservation planning, pest and disease control and
   biomass estimation. This study evaluated the performance of applying
   ensemble techniques with the goal of automatically distinguishing Pinus
   sylvestris L. and Pinus uncinata Mill. Ex Mirb within a 1.3 km(2)
   mountainous area in Barcelonnette (France). Three modelling schemes were
   examined, based on: (1) high-density LiDAR data (160 returns m(-2)), (2)
   Worldview-2 multispectral imagery, and (3) Worldview-2 and LiDAR in
   combination. Variables related to the crown structure and height of
   individual trees were extracted from the normalized LiDAR point cloud at
   individual-tree level, after performing individual tree crown (ITC)
   delineation. Vegetation indices and the Haralick texture indices were
   derived from Worldview-2 images and served as independent spectral
   variables. Selection of the best predictor subset was done after a
   comparison of three variable selection procedures: (1) Random Forests
   with cross validation (AUCREcv), (2) Akaike Information Criterion (AIC)
   and (3) Bayesian Information Criterion (BIC). To classify the species, 9
   regression techniques were combined using ensemble models. Predictions
   were evaluated using cross validation and an independent dataset.
   Integration of datasets and models improved individual tree species
   classification (True Skills Statistic, TSS; from 0.67 to 0.81) over
   individual techniques and maintained strong predictive power (Relative
   Operating Characteristic, ROC = 0.91). Assemblage of regression models
   and integration of the datasets provided more reliable species
   distribution maps and associated tree-scale mapping uncertainties. Our
   study highlights the potential of model and data assemblage at improving
   species classifications needed in present-day forest planning and
   management.}},
DOI = {{10.1016/j.jag.2017.09.016}},
ISSN = {{0303-2434}},
ResearcherID-Numbers = {{Duque-Lazo, Joaquin/L-3722-2019
   Gonzalez-Ferreiro, Eduardo/Q-9709-2016}},
ORCID-Numbers = {{Duque-Lazo, Joaquin/0000-0003-4223-5070
   Gonzalez-Ferreiro, Eduardo/0000-0002-4565-2155}},
Unique-ID = {{ISI:000418312200002}},
}

@article{ ISI:000427009100019,
Author = {Ye, Cang and Qian, Xiangfei},
Title = {{3-D Object Recognition of a Robotic Navigation Aid for the Visually
   Impaired}},
Journal = {{IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING}},
Year = {{2018}},
Volume = {{26}},
Number = {{2}},
Pages = {{441-450}},
Month = {{FEB}},
Abstract = {{This paper presents a 3-D object recognition method and its
   implementation on a robotic navigation aid to allow real-time detection
   of indoor structural objects for the navigation of a blind person. The
   method segments a point cloud into numerous planar patches and extracts
   their inter-plane relationships (IPRs). Based on the existing IPRs of
   the object models, the method defines six high level features (HLFs) and
   determines the HLFs for each patch. A Gaussian-mixture-model-based plane
   classifier is then devised to classify each planar patch into one
   belonging to a particular object model. Finally, a recursive plane
   clustering procedure is used to cluster the classified planes into the
   model objects. As the proposed method uses geometric context to detect
   an object, it is robust to the object's visual appearance change. As a
   result, it is ideal for detecting structural objects (e.g., stairways,
   doorways, and so on). In addition, it has high scalability and
   parallelism. The method is also capable of detecting some indoor
   nonstructural objects. Experimental results demonstrate that the
   proposed method has a high success rate in object recognition.}},
DOI = {{10.1109/TNSRE.2017.2748419}},
ISSN = {{1534-4320}},
EISSN = {{1558-0210}},
Unique-ID = {{ISI:000427009100019}},
}

@article{ ISI:000418370200123,
Author = {Li, Ye and Wang, YingHui and Liu, Jing and Hao, Wen},
Title = {{Expression-insensitive 3D face recognition by the fusion of multiple
   subject-specific curves}},
Journal = {{NEUROCOMPUTING}},
Year = {{2018}},
Volume = {{275}},
Pages = {{1295-1307}},
Month = {{JAN 31}},
Abstract = {{This study proposes a 3D face recognition method using multiple
   subject-specific curves insensitive to intra-subject distortions caused
   by expression variations. Considering that most sharp variances in
   facial convex regions are closely related to the bone structure, the
   convex crest curves are first extracted as the most vital
   subject-specific facial curves based on the principal curvature extrema
   in convex local surfaces. Then, the central profile curve and the
   horizontal contour curve passing through the nose tip are detected by
   using the precise localization of the nose tip and symmetry plane. Based
   on their discriminative power and robustness to expression changes, the
   three types of curves are fused with appropriate weights at the
   feature-level and used for matching 3D faces with the iterative closest
   point algorithm. The combination of multiple expression-insensitive
   curves is complementary and provides sufficient and stable facial
   surface features for face recognition. In addition, for each convex
   crest curve, an expression-irrelevant factor is assigned as the adaptive
   weight to improve the face matching performance. The results of
   experiments using two public 3D databases, GavabDB and BU-3DFE,
   demonstrate the effectiveness of the proposed method, and its
   recognition rates on both databases reflect an encouraging performance.
   (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2017.09.070}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000418370200123}},
}

@article{ ISI:000423097800017,
Author = {Crouch, Daniel J. M. and Winney, Bruce and Koppen, Willem P. and
   Christmas, William J. and Hutnik, Katarzyna and Day, Tammy and Meena,
   Devendra and Boumertit, Abdelhamid and Hysi, Pirro and Nessa, Ayrun and
   Spector, Tim D. and Kittler, Josef and Bodmer, Walter F.},
Title = {{Genetics of the human face: Identification of large-effect single gene
   variants}},
Journal = {{PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA}},
Year = {{2018}},
Volume = {{115}},
Number = {{4}},
Pages = {{E676-E685}},
Month = {{JAN 23}},
Abstract = {{To discover specific variants with relatively large effects on the human
   face, we have devised an approach to identifying facial features with
   high heritability. This is based on using twin data to estimate the
   additive genetic value of each point on a face, as provided by a 3D
   camera system. In addition, we have used the ethnic difference between
   East Asian and European faces as a further source of face genetic
   variation. We use principal components (PCs) analysis to provide a fine
   definition of the surface features of human faces around the eyes and of
   the profile, and chose upper and lower 10\% extremes of the most
   heritable PCs for looking for genetic associations. Using this strategy
   for the analysis of 3D images of 1,832 unique volunteers from the
   well-characterized People of the British Isles study and 1,567 unique
   twin images from the TwinsUK cohort, together with genetic data for
   500,000 SNPs, we have identified three specific genetic variants with
   notable effects on facial profiles and eyes.}},
DOI = {{10.1073/pnas.1708207114}},
ISSN = {{0027-8424}},
Unique-ID = {{ISI:000423097800017}},
}

@inproceedings{ ISI:000462163500103,
Author = {Ju, Xiangyang and Garcia Junior, Idelmo Rangel and Silva, Leonardo de
   Freitas and Mossey, Peter and Al-Rudainy, Dhelal and Ayoub, Ashraf and
   de Mattos, Adriana Marques},
Editor = {{Li, W and Li, Q and Wang, L}},
Title = {{3D Head Shape Analysis of Suspected Zika Infected Infants}},
Booktitle = {{2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018)}},
Year = {{2018}},
Note = {{11th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI), Beijing, PEOPLES R CHINA, OCT
   13-15, 2018}},
Organization = {{IEEE; IEEE Engn Med \& Biol Soc; Beijing Univ Chem Technol; Beijing Inst
   Technol; E China Normal Univ}},
Abstract = {{The babies infected from Zika before they are born are at risk for
   problems with brain development and microcephaly. 3D head images of 43
   Zika cases and 43 controls were collected aiming to extract shape
   characteristics for diagnosis purposes. Principal component analysis
   (PCA) has been applied on the vaults and faces of the collected 3D
   images and the scores on the second principal components of the vaults
   and faces showed significant differences between the control and Zika
   groups. The shape variations from -2s to 2s illustrated the typical
   characteristics of microcephaly of the Zika babies. Canonical
   correlation analysis (CCA) showed a significant correlation in the first
   CCA variates of face and vault which indicated the potential of 3D
   facial imaging for Zika surveillance. Further head circumferences and
   distances from ear to ear were measured from the 3D images and
   preliminary results showed the adding ear to ear distances for
   classifying control and Zika children strengthened the abilities of
   tested classification models.}},
ISBN = {{978-1-5386-7604-2}},
Unique-ID = {{ISI:000462163500103}},
}

@inproceedings{ ISI:000460950900069,
Author = {Khan, Jihas and Raj, Jayakrishna and Pradeep, R.},
Editor = {{Zelinka, I and Senkerik, R and Panda, G and Kanthan, PSL}},
Title = {{Modeling of an Automotive Grade LIDAR Sensor}},
Booktitle = {{SOFT COMPUTING SYSTEMS, ICSCS 2018}},
Series = {{Communications in Computer and Information Science}},
Year = {{2018}},
Volume = {{837}},
Pages = {{676-686}},
Note = {{2nd International Conference on Soft Computing Systems (ICSCS), Baselios
   Mathews II Coll Engn, Sasthamcotta, INDIA, APR 19-20, 2018}},
Abstract = {{Automobiles use LIDAR sensor to detect different objects around the
   vehicle. For system analysis and study, this paper is proposing a LIDAR
   sensor model, which takes into consideration the impact of the real
   world information. Impact of the environment on the LIDAR sensor is
   modeled and all the possible parameters of the LIDAR are modeled as
   configurable parameters. Option to import 3D objects of any shape, type
   or dimension via FBX file format is also incorporated. 3D objects in FBX
   format shall be converted to a set of triangles first, which approximate
   the surface mesh of the 3D object. Ray cast modeling is then used to
   detect whether in a vertical distribution of LIDAR beams, an
   intersection occurs between LIDAR beam and any of the triangular face.
   If there is a collision, the collision point shall be saved as the point
   cloud data. This will be repeated around the sensor, and all such point
   cloud data points shall be appended to the final point cloud data. These
   point cloud data is then subjected to segmentation and object detection
   using belief theory. Either the processed point cloud data in object
   information format or the unprocessed raw point cloud data can be
   produced as the output by the proposed LIDAR sensor model.}},
DOI = {{10.1007/978-981-13-1936-5\_69}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-981-13-1936-5; 978-981-13-1935-8}},
Unique-ID = {{ISI:000460950900069}},
}

@inproceedings{ ISI:000457881301017,
Author = {Akita, Tokihiko and Yamauchi, Yuji and Fujiyoshi, Hironobu},
Book-Group-Author = {{IEEE}},
Title = {{Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye
   Cameras}},
Booktitle = {{2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)}},
Series = {{IEEE International Conference on Intelligent Transportation Systems-ITSC}},
Year = {{2018}},
Pages = {{1103-1108}},
Note = {{21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC), Maui, HI, NOV 04-07, 2018}},
Organization = {{IEEE; IEEE Intelligent Transportat Syst Soc}},
Abstract = {{Recently, automated emergency brake systems for pedestrian have been
   commercialized. However, they cannot detect crossing pedestrians when
   turning at intersections because the field of view is not wide enough.
   Thus, we propose to utilize a surround view camera system becoming
   popular by making it into stereo vision which is robust for the
   pedestrian recognition. However, conventional stereo camera technologies
   cannot be applied due to fisheye cameras and uncalibrated camera poses.
   Thus we have created the new method to absorb difference of the
   pedestrian appearance between cameras by machine learning for the stereo
   vision. The method of stereo matching between image patches in each
   camera image was designed by combining D-Brief and NCC with SVM. Good
   generalization performance was achieved by it compared with individual
   conventional algorithms. Furthermore, feature amounts of the point cloud
   reconstructed by the stereo pairs are utilized with Random Forest to
   discriminate pedestrians. The algorithm was evaluated for the actual
   camera images of crossing pedestrians at various intersections, and
   96.0\% of pedestrian tracking rate with high position detection accuracy
   was achieved. They were compared with Faster R-CNN as the best pattern
   recognition technique, and our proposed method indicated better
   detection performance.}},
ISSN = {{2153-0009}},
ISBN = {{978-1-7281-0323-5}},
Unique-ID = {{ISI:000457881301017}},
}

@inproceedings{ ISI:000457843602090,
Author = {Zhu, Wei and Qiu, Qiang and Huang, Jiaji and Calderbank, Robert and
   Sapiro, Guillermo and Daubechies, Ingrid},
Book-Group-Author = {{IEEE}},
Title = {{LDMNet: Low Dimensional Manifold Regularized Neural Networks}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{2743-2751}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{Deep neural networks have proved very successful on archetypal tasks for
   which large training sets are available, but when the training data are
   scarce, their performance suffers from overfitting. Many existing
   methods of reducing overfitting are data-independent. Data-dependent
   regularizations are mostly motivated by the observation that data of
   interest lie close to a manifold, which is typically hard to parametrize
   explicitly. These methods usually only focus on the geometry of the
   input data, and do not necessarily encourage the networks to produce
   geometrically meaningful features. To resolve this, we propose the
   Low-Dimensional-Manifold-regularized neural Network (LDMNet), which
   incorporates a feature regularization method that focuses on the
   geometry of both the input data and the output features. In LDMNet, we
   regularize the network by encouraging the combination of the input data
   and the output features to sample a collection of low dimensional
   manifolds, which are searched efficiently without explicit
   parametrization. To achieve this, we directly use the manifold dimension
   as a regularization term in a variational functional. The resulting
   Euler-Lagrange equation is a Laplace-Beltrami equation over a point
   cloud, which is solved by the point integral method without increasing
   the computational complexity. In the experiments, we show that LDMNet
   significantly outperforms widely-used regularizers. Moreover, LDMNet can
   extract common features of an object imaged via different modalities,
   which is very useful in real-world applications such as cross-spectral
   face recognition.}},
DOI = {{10.1109/CVPR.2018.00290}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843602090}},
}

@inproceedings{ ISI:000457843605028,
Author = {Cheng, Shiyang and Kotsia, Irene and Pantic, Maja and Zafeiriou,
   Stefanos},
Book-Group-Author = {{IEEE}},
Title = {{4DFAB: A Large Scale 4D Database for Facial Expression Analysis and
   Biometric Applications}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{5117-5126}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{The progress we are currently witnessing in many computer vision
   applications, including automatic face analysis, would not be made
   possible without tremendous efforts in collecting and annotating large
   scale visual databases. To this end, we propose 4DFAB, a new large scale
   database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes).
   4DFAB contains recordings of 180 subjects captured in four different
   sessions spanning over a five-year period. It contains 4D videos of
   subjects displaying both spontaneous and posed facial behaviours. The
   database can be used for both face and facial expression recognition, as
   well as behavioural biometrics. It can also be used to learn very
   powerful blendshapes for parametrising facial behaviour. In this paper,
   we conduct several experiments and demonstrate the usefulness of the
   database for various applications. The database will be made publicly
   available for research purposes.}},
DOI = {{10.1109/CVPR.2018.00537}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843605028}},
}

@inproceedings{ ISI:000457843609059,
Author = {Li, Jiaxin and Chen, Ben M. and Lee, Gim Hee},
Book-Group-Author = {{IEEE}},
Title = {{SO-Net: Self-Organizing Network for Point Cloud Analysis}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{9397-9406}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{This paper presents SO-Net, a permutation invariant architecture for
   deep learning with orderless point clouds. The SO-Net models the spatial
   distribution of point cloud by building a Self-Organizing Map (SOM).
   Based on the SOM, SO-Net performs hierarchical feature extraction on
   individual points and SOM nodes, and ultimately represents the input
   point cloud by a single feature vector. The receptive field of the
   network can be systematically adjusted by conducting point-to-node k
   nearest neighbor search. In recognition tasks such as point cloud
   reconstruction, classification, object part segmentation and shape
   retrieval, our proposed network demonstrates performance that is similar
   with or better than state-of-the-art approaches. In addition, the
   training speed is significantly faster than existing point cloud
   recognition networks because of the parallelizability and simplicity of
   the proposed architecture. Our code is available at the project
   website.(1)}},
DOI = {{10.1109/CVPR.2018.00979}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843609059}},
}

@inproceedings{ ISI:000455784500063,
Author = {Ye, Jing and Zhou, Changhe and Li, Chao and Miao, Chaofeng},
Editor = {{Sheng, Y and Yu, C and Zhou, C}},
Title = {{High-precision 3D Shape Measurement Based on Time-resolved VCSEL}},
Booktitle = {{HOLOGRAPHY, DIFFRACTIVE OPTICS, AND APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2018}},
Volume = {{10818}},
Note = {{Conference on Holography, Diffractive Optics, and Applications VIII,
   Beijing, PEOPLES R CHINA, OCT 11-13, 2018}},
Organization = {{SPIE; Chinese Opt Soc}},
Abstract = {{High-speed and high-precision human face 3D shape measurement plays a
   very important role in diverse applications such as human-computer
   interaction, 3D face recognition, Virtual Reality. This paper introduces
   a structured light system based on VCSEL(Vertical Cavity Surface
   Emitting Laser) with one simulated projectors and two camera for human
   face 3D shape measurement. Large-scale production cost of VCSEL is low,
   because of the manufacturing process compatible with LED. VCSEL has the
   advantages of projecting a large area of diffractive structure light and
   easy to integrate into lens array internally. The process of VCSEL
   projecting the structural light that changes over time to human face is
   simulated by computer. The ICP algorithm is used to match the image of
   single frame structure light from the right camera to the left camera. A
   single frame image of three-dimensional face point cloud is obtained by
   using binocular stereo vision principle. The multi-frame images of point
   cloud that change along time series are superposed to obtain higher
   density point cloud data and improve the measurement accuracy. This 3D
   measurement based on VCSEL has advantages of low cost, high precision,
   and small size and should be useful for practical applications.}},
DOI = {{10.1117/12.2502417}},
Article-Number = {{UNSP 1081829}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-2235-7}},
Unique-ID = {{ISI:000455784500063}},
}

@inproceedings{ ISI:000455146803022,
Author = {Liang, Jie and Liu, Feng and Tu, Huan and Zhao, Qijun and Jain, Anil K.},
Book-Group-Author = {{IEEE}},
Title = {{On Mugshot-based Arbitrary View Face Recognition}},
Booktitle = {{2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2018}},
Pages = {{3126-3131}},
Note = {{24th International Conference on Pattern Recognition (ICPR), Chinese
   Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA, AUG 20-24, 2018}},
Organization = {{Int Assoc Pattern Recognit; Chinese Assoc Automat}},
Abstract = {{Despite the wide usage of mugshot images in forensic applications, they
   are underutilized in existing automated face recognition systems. In
   this paper, we propose a novel mugshot-based arbitrary view face
   recognition method. Our approach reconstructs full 3D faces via cascaded
   regression in shape space with efficient seamless texture recovery.
   Unlike existing methods, it makes full use of the frontal and profile
   views available in mugshot images, and thus generates accurate and
   realistic 3D faces. Multi-view face images are synthesized from the
   reconstructed 3D faces to enlarge the gallery so that arbitrary view
   faces can be better recognized. Evaluation experiments were conducted on
   BFM and Multi-PIE databases by using state-of-the-art deep learning (DL)
   based face matchers. The results demonstrate the effectiveness of our
   proposed method and show that DL-based face matchers can benefit from
   mugshot images and the reconstructed 3D faces, especially for
   recognizing large off-angle faces.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5386-3788-3}},
Unique-ID = {{ISI:000455146803022}},
}

@inproceedings{ ISI:000455305000011,
Author = {Harikumar, A. and Paris, C. and Bovolo, F. and Bruzzone, L.},
Editor = {{Bruzzone, L and Bovolo, F}},
Title = {{A novel data-driven approach to tree species classification using high
   density multireturn airborne lidar data}},
Booktitle = {{IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXIV}},
Series = {{Proceedings of SPIE}},
Year = {{2018}},
Volume = {{10789}},
Note = {{Conference on Image and Signal Processing for Remote Sensing XXIV,
   Berlin, GERMANY, SEP 10-12, 2018}},
Organization = {{SPIE}},
Abstract = {{Tree species information is crucial for accurate forest parameter
   estimation. Small footprint high density multi-return Light Detection
   and Ranging (LiDAR) data contain a large amount of structural details
   for modelling and thus distinguishing individual tree species. To fully
   exploit the potential of these data, we propose a data-driven tree
   species classification approach based on a volumetric analysis of
   single-tree-point-cloud that extracts features that are able to
   characterize both the internal and the external crown structure. The
   method captures the spatial distribution of the LiDAR points within the
   crown by generating a feature vector representing the three-dimensional
   (3D) crown information. Each element in the feature vector uniquely
   corresponds to an Elementary Quantization Volume (EQV) of the crown.
   Three strategies have been defined to generate unique EQVs that model
   different representations of the crown components. The classification is
   performed by using a Support Vector Machines (C-SVM) classifier using
   the histogram intersection kernel that has the enhanced ability to give
   maximum preference to the key features in high dimensional feature
   space. All the experiments were performed on a set of 200 trees
   belonging to Norway Spruce, European Larch, Swiss Pine, and Silver Fir
   (i.e., 50 trees per species). The classifier is trained using 120 trees
   and tested on an independent set of 80 trees. The proposed method
   outperforms the classification performance of the state-of-the-art
   method used for comparison.}},
DOI = {{10.1117/12.2325634}},
Article-Number = {{UNSP 107890E}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-2162-6}},
ORCID-Numbers = {{Paris, Claudia/0000-0002-7189-6268}},
Unique-ID = {{ISI:000455305000011}},
}

@inproceedings{ ISI:000455343100004,
Author = {Desai, Kevin and Prabhakaran, Balakrishnan and Raghuraman, Suraj},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{Combining Skeletal Poses for 3D Human Model Generation using Multiple
   Kinects}},
Booktitle = {{PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18)}},
Year = {{2018}},
Pages = {{40-51}},
Note = {{9th ACM Multimedia Systems Conference (MMSys), Centrum Wiskunde \&
   Informatica Amsterdam, Amsterdam, NETHERLANDS, JUN 12-15, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGMM; ACM SIGOPS; ACM SIGMOBILE; ACM SIGCOMM}},
Abstract = {{RGB-D cameras, such as the Microsoft Kinect, provide us with the 3D
   information, color and depth, associated with the scene. Interactive 3D
   Tele-Immersion (i3DTI) systems use such RGB-D cameras to capture the
   person present in the scene in order to collaborate with other remote
   users and interact with the virtual objects present in the environment.
   Using a single camera, it becomes difficult to estimate an accurate
   skeletal pose and complete 3D model of the person, especially when the
   person is not in the complete view of the camera. With multiple cameras,
   even with partial views, it is possible to get a more accurate estimate
   of the skeleton of the person leading to a better and complete 3D model.
   In this paper, we present a real-time skeletal pose identification
   approach that leverages on the inaccurate skeletons of the individual
   Kinects, and provides a combined optimized skeleton. We estimate the
   Probability of an Accurate Joint (PAJ) for each joint from all of the
   Kinect skeletons. We determine the correct direction of the person and
   assign the correct joint sides for each skeleton. We then use a greedy
   consensus approach to combine the highly probable and accurate joints to
   estimate the combined skeleton. Using the individual skeletons, we
   segment the point clouds from all the cameras. We use the already
   computed PAJ values to obtain the Probability of an Accurate Bone (PAB).
   The individual point clouds are then combined one segment after another
   using the calculated PAB values. The generated combined point cloud is a
   complete and accurate 3D representation of the person present in the
   scene. We validate our estimated skeleton against two well-known methods
   by computing the error distance between the best view Kinect skeleton
   and the estimated skeleton. An exhaustive analysis is performed by using
   around 500000 skeletal frames in total, captured using 7 users and 7
   cameras. Visual analysis is performed by checking whether the estimated
   skeleton is completely present within the human model. We also develop a
   3D Holo-Bubble game to showcase the real-time performance of the
   combined skeleton and point cloud. Our results show that our method
   performs better than the state-of-the-art approaches that use multiple
   Kinects, in terms of objective error, visual quality and real-time user
   performance.}},
DOI = {{10.1145/3204949.3204958}},
ISBN = {{978-1-4503-5192-8}},
ORCID-Numbers = {{Desai, Kevin/0000-0002-2964-8981}},
Unique-ID = {{ISI:000455343100004}},
}

@article{ ISI:000455069200012,
Author = {Wang, Kaishi and Jiang, Yi and Zhang, Zhifei and Lu, Yongtian and Ni,
   Yusu},
Title = {{Extension of the Clinical Significance of the ``Cog{''}}},
Journal = {{ORL-JOURNAL FOR OTO-RHINO-LARYNGOLOGY HEAD AND NECK SURGERY}},
Year = {{2018}},
Volume = {{80}},
Number = {{5-6}},
Pages = {{317-325}},
Abstract = {{Objective: To study the clinical anatomy of the epitympanum, the attic,
   and its medial wall, to try to discover a new clinical operation-related
   anatomical landmark, and to investigate the adjacent anatomical
   relationship with this landmark. Materials and Methods: Eight donor
   temporal bone specimens were dissected endoscopically. For 29 healthy
   persons (17 males and 12 females), CT images of the temporal bone (57
   ears) were taken, 3-dimensional (3-D) reconstruction and
   multidimensional plane reconstruction were performed, and identification
   and assessment of 3-D spatial relationships between any 2 of these
   complex structures were done. Results: 3-D images of the temporal bone
   structures including the facial nerve, the cochlea, the semicircular
   canal, and the brain plate were reconstructed and shown in detail. We
   discovered a new clinical surgery-related anatomical landmark (the
   ``cog{''} tangent and the trailing edge of the cog). Based on the
   tangent and the trailing edge of the cog, we quantified the anatomical
   relationship between it and its neighboring important structures.
   Conclusion: Based on endoscopic anatomy and the temporal bone spiral CT
   3-D structure reconstruction of the epitympanum, the attic, and the
   adjacent structures, we found an extension of the clinical significance
   the cog. Quantification of the adjacent anatomical relationship of this
   landmark is very important for otology microsurgical operation. (c) 2018
   S. Karger AG, Basel}},
DOI = {{10.1159/000493012}},
ISSN = {{0301-1569}},
EISSN = {{1423-0275}},
Unique-ID = {{ISI:000455069200012}},
}

@inproceedings{ ISI:000451039807048,
Author = {Li, Shihua and Su, Lian and Liu, Yuhan and He, Ze},
Book-Group-Author = {{IEEE}},
Title = {{SEGMENTATION OF INDIVIDUAL TREES BASED ON A POINT CLOUD CLUSTERING
   METHOD USING AIRBORNE LIDAR DATA}},
Booktitle = {{IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING
   SYMPOSIUM}},
Series = {{IEEE International Symposium on Geoscience and Remote Sensing IGARSS}},
Year = {{2018}},
Pages = {{7520-7523}},
Note = {{38th IEEE International Geoscience and Remote Sensing Symposium
   (IGARSS), Valencia, SPAIN, JUL 22-27, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Geoscience
   \& Remote Sensing Soc; European Space Agcy}},
Abstract = {{The objective of this paper was to develop a new algorithm to segment
   individual trees directly by using the three-dimensional space
   characteristic of airborne light detection and ranging point cloud data.
   The local maximum method was used in the initial segmentation and the
   error identification tree exclusion. On the basis of the point cloud
   spatial distribution of individual trees and the adjacent relationship
   with the other trees, a point cloud clustering method was developed to
   decide the points belonging to the individual trees. This algorithm was
   tested by 6 forest plots in the Genhe forestry reserve. The results
   showed that this algorithm could segment individual trees quickly and
   accurately, and the overall accuracy of this algorithm was 96.3\%.}},
ISSN = {{2153-6996}},
ISBN = {{978-1-5386-7150-4}},
Unique-ID = {{ISI:000451039807048}},
}

@inproceedings{ ISI:000449774200039,
Author = {Abrevaya, Victoria Fernandez and Wuhrer, Stefanie and Boyer, Edmond},
Book-Group-Author = {{IEEE}},
Title = {{Spatiotemporal Modeling for Efficient Registration of Dynamic 3D Faces}},
Booktitle = {{2018 INTERNATIONAL CONFERENCE ON 3D VISION (3DV)}},
Series = {{International Conference on 3D Vision}},
Year = {{2018}},
Pages = {{371-380}},
Note = {{6th International Conference on 3D Vision (3DV), Verona, ITALY, SEP
   05-08, 2018}},
Organization = {{Agisoft; Aquifi; Google; Microsoft; Microtec; Nvidia; 3DFlow;
   Digitalviews}},
Abstract = {{We consider the registration of temporal sequences of 3D face scans.
   Face registration plays a central role in face analysis applications,
   for instance recognition or transfer tasks, among others. We propose an
   automatic approach that can register large sets of dynamic face scans
   without the need for landmarks or highly specialized acquisition setups.
   This allows for extended versatility among registered face shapes and
   deformations by enabling to leverage multiple datasets, a fundamental
   property when e.g. building statistical face models. Our approach is
   built upon a regression-based static registration method, which is
   improved by spatiotemporal modeling to exploit redundancies over both
   space and time. We experimentally demonstrate that accurate
   registrations can be obtained for varying data robustly and efficiently
   by applying our method to three standard dynamic face datasets.}},
DOI = {{10.1109/3DV.2018.00050}},
ISBN = {{978-1-5386-8425-2}},
Unique-ID = {{ISI:000449774200039}},
}

@inproceedings{ ISI:000446394502083,
Author = {Mitash, Chaitanya and Boularias, Abdeslam and Bekris, Kostas E.},
Book-Group-Author = {{IEEE}},
Title = {{Improving 6D Pose Estimation of Objects in Clutter via Physics-aware
   Monte Carlo Tree Search}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)}},
Series = {{IEEE International Conference on Robotics and Automation ICRA}},
Year = {{2018}},
Pages = {{3331-3338}},
Note = {{IEEE International Conference on Robotics and Automation (ICRA),
   Brisbane, AUSTRALIA, MAY 21-25, 2018}},
Organization = {{IEEE; CSIRO; Australian Govt, Dept Def Sci \& Technol; DJI; Queensland
   Univ Technol; Woodside; Baidu; Bosch; Houston Mechatron; Kinova Robot;
   KUKA; Hit Robot Grp; Honda Res Inst; iRobot; Mathworks; NuTonomy;
   Ouster; Uber}},
Abstract = {{This work proposes a process for efficiently searching over combinations
   of individual object 6D pose hypotheses in cluttered scenes, especially
   in cases involving occlusions and objects resting on each other. The
   initial set of candidate object poses is generated from state-of-the-art
   object detection and global point cloud registration techniques. The
   best scored pose per object by using these techniques may not be
   accurate due to overlaps and occlusions. Nevertheless, experimental
   indications provided in this work show that object poses with lower
   ranks may be closer to the real poses than ones with high ranks
   according to registration techniques. This motivates a global
   optimization process for improving these poses by taking into account
   scene-level physical interactions between objects. It also implies that
   the Cartesian product of candidate poses for interacting objects must be
   searched so as to identify the best scene-level hypothesis. To perform
   the search efficiently, the candidate poses for each object are
   clustered so as to reduce their number but still keep a sufficient
   diversity. Then, searching over the combinations of candidate object
   poses is performed through a Monte Carlo Tree Search (MCTS) process that
   uses the similarity between the observed depth image of the scene and a
   rendering of the scene given the hypothesized pose as a score that
   guides the search procedure. MCTS handles in a principled way the
   tradeoff between fine-tuning the most promising poses and exploring new
   ones, by using the Upper Confidence Bound (UCB) technique. Experimental
   results indicate that this process is able to quickly identify in
   cluttered scenes physically-consistent object poses that are
   significantly closer to ground truth compared to poses found by point
   cloud registration methods.}},
ISSN = {{1050-4729}},
ISBN = {{978-1-5386-3081-5}},
Unique-ID = {{ISI:000446394502083}},
}

@article{ ISI:000445394200001,
Author = {Hentz, Angela M. K. and Silva, Carlos A. and Dalla Corte, Ana P. and
   Netto, Sylvio P. and Stager, Michael P. and Klauberg, Carine},
Title = {{Estimating forest uniformity in Eucalyptus spp. and Pinus taeda L.
   stands using field measurements and structure from motion point clouds
   generated from unmanned aerial vehicle (UAV) data collection}},
Journal = {{FOREST SYSTEMS}},
Year = {{2018}},
Volume = {{27}},
Number = {{2}},
Abstract = {{Aim of study: In this study we applied 3D point clouds generated by
   images obtained from an Unmanned Aerial Vehicle (UAV) to evaluate the
   uniformity of young forest stands.
   Area of study: Two commercial forest stands were selected, with two
   plots each. The forest species studied were Eucalyptus spp. and Pinus
   taeda L. and the trees had an age of 1.5 years.
   Material and methods: The individual trees were detected based on
   watershed segmentation and local maxima, using the spectral values
   stored in the point cloud. After the tree detection, the heights were
   calculated using two approaches, in the first one using the Digital
   Surface Model (DSM) and a Digital Terrain Model, and in the second using
   only the DSM. We used the UAV-derived heights to estimate an uniformity
   index.
   Main results: The trees were detected with a maximum 6\% of error.
   However, the height was underestimated in all cases, in an average of 1
   and 0.7 m for Pinus and Eucalyptus stands. We proposed to use the models
   built herein to estimate tree height, but the regression models did not
   explain the variably within the data satisfactorily. Therefore, the
   uniformity index calculated using the direct UAV-height values presented
   results close to the field inventory, reaching better results when using
   the second height approach (error ranging 2.8-7.8\%).
   Research highlights: The uniformity index using the UAV-derived height
   from the proposed methods was close to the values obtained in field. We
   noted the potential for using UAV imagery in forest monitoring.}},
DOI = {{10.5424/fs/2018272-11713}},
Article-Number = {{UNSP e005}},
ISSN = {{2171-5068}},
EISSN = {{2171-9845}},
Unique-ID = {{ISI:000445394200001}},
}

@article{ ISI:000432842000001,
Author = {Petkovic, Uros and Korez, Robert and Parent, Stefan and Kadoury, Samuel
   and Vrtovec, Tomaz},
Title = {{Semi-automated 3D Cobb Angle Measurements from Scoliotic Mesh Models}},
Journal = {{ELEKTROTEHNISKI VESTNIK-ELECTROCHEMICAL REVIEW}},
Year = {{2018}},
Volume = {{85}},
Number = {{1-2}},
Pages = {{1-6}},
Abstract = {{The Cobb angle, which is the main diagnostic parameter for the
   evaluation of spinal deformities, is usually measured on two-dimensional
   coronal radiographic (X-ray) images. To provide more accurate
   measurements, there is a tendency for measuring the Cobb angle from
   three-dimensional (3D) images. In this paper we propose a semi-automated
   method for the evaluation of the 3D Cobb angle from 3D spine mesh
   models. From the manually selected upper-end and lower-end vertebra mesh
   models, we first identify the vertebral body centers, and then label
   faces of the superior and inferior endplates of the mesh model, which
   define the planes used for the measurement of the 3D Cobb angle. The
   results obtained on 60 mesh models of scoliotic spines at 17 different
   face-vertex densities indicate that the method is robust and accurate at
   the face-edge lengths below 6 mm with the corresponding mean absolute
   error of 3.0 degrees and standard deviation of 2.2 degrees when compared
   to reference measurements.}},
ISSN = {{0013-5852}},
EISSN = {{2232-3228}},
Unique-ID = {{ISI:000432842000001}},
}

@article{ ISI:000425828200076,
Author = {Anderson, Kyle E. and Glenn, Nancy F. and Spaete, Lucas P. and
   Shinneman, Douglas J. and Pilliod, David S. and Arkle, Robert S. and
   McIlroy, Susan K. and Derryberry, DeWayne R.},
Title = {{Estimating vegetation biomass and cover across large plots in shrub and
   grass dominated drylands using terrestrial lidar and machine learning}},
Journal = {{ECOLOGICAL INDICATORS}},
Year = {{2018}},
Volume = {{84}},
Pages = {{793-802}},
Month = {{JAN}},
Abstract = {{Terrestrial laser scanning (TLS) has been shown to enable an efficient,
   precise, and non-destructive inventory of vegetation structure at ranges
   up to hundreds of meters. We developed a method that leverages TLS
   collections with machine learning techniques to model and map canopy
   cover and biomass of several classes of short-stature vegetation across
   large plots. We collected high-definition TLS scans of 26 1-ha plots in
   desert grasslands and big sagebrush shrublands in southwest Idaho, USA.
   We used the Random Forests machine learning algorithm to develop
   decision tree models predicting the biomass and canopy cover of several
   vegetation classes from statistical descriptors of the aboveground
   heights of TLS points. Manual measurements of vegetation characteristics
   collected within each plot served as training and validation data.
   Models based on five or fewer TLS descriptors of vegetation heights were
   developed to predict the canopy cover fraction of shrubs (R-2 = 0.77,
   RMSE = 7\%), annual grasses (R-2 = 0.70, RMSE = 21\%), perennial grasses
   (R-2 = 0.36, RMSE = 12\%), forbs (R-2 = 0.52, RMSE = 6\%), bare earth or
   litter (R-2 = 0.49, RMSE = 19\%), and the biomass of shrubs (R-2 = 0.71,
   RMSE = 175 g) and herbaceous vegetation (R-2 = 0.61, RMSE = 99 g) (all
   values reported are out-of-bag). Our models explained much of the
   variability between predictions and manual measurements, and yet we
   expect that future applications could produce even better results by
   reducing some of the methodological sources of error that we
   encountered. Our work demonstrates how TLS can be used efficiently to
   extend manual measurement of vegetation characteristics from small to
   large plots in grasslands and shrublands, with potential application to
   other similarly structured ecosystems. Our method shows that vegetation
   structural characteristics can be modeled without classifying and
   delineating individual plants, a challenging and time-consuming step
   common in previous methods applying TLS to vegetation inventory.
   Improving application of TLS to studies of shrub steppe ecosystems will
   serve immediate management needs by enhancing vegetation inventories,
   environmental modeling studies, and the ability to train broader
   datasets collected from air and space.}},
DOI = {{10.1016/j.ecolind.2017.09.034}},
ISSN = {{1470-160X}},
EISSN = {{1872-7034}},
Unique-ID = {{ISI:000425828200076}},
}

@article{ ISI:000424092300038,
Author = {Zhou, Tan and Popescu, Sorin C. and Lawing, A. Michelle and Eriksson,
   Marian and Strimbu, Bogdan M. and Buerkner, Paul C.},
Title = {{Bayesian and Classical Machine Learning Methods: A Comparison for Tree
   Species Classification with LiDAR Waveform Signatures}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{A plethora of information contained in full-waveform (FW) Light
   Detection and Ranging (LiDAR) data offers prospects for characterizing
   vegetation structures. This study aims to investigate the capacity of FW
   LiDAR data alone for tree species identification through the integration
   of waveform metrics with machine learning methods and Bayesian
   inference. Specifically, we first conducted automatic tree segmentation
   based on the waveform-based canopy height model (CHM) using three
   approaches including TreeVaW, watershed algorithms and the combination
   of TreeVaW and watershed (TW) algorithms. Subsequently, the Random
   forests (RF) and Conditional inference forests (CF) models were employed
   to identify important tree-level waveform metrics derived from three
   distinct sources, such as raw waveforms, composite waveforms, the
   waveform-based point cloud and the combined variables from these three
   sources. Further, we discriminated tree (gray pine, blue oak, interior
   live oak) and shrub species through the RF, CF and Bayesian multinomial
   logistic regression (BMLR) using important waveform metrics identified
   in this study. Results of the tree segmentation demonstrated that the TW
   algorithms outperformed other algorithms for delineating individual tree
   crowns. The CF model overcomes waveform metrics selection bias caused by
   the RF model which favors correlated metrics and enhances the accuracy
   of subsequent classification. We also found that composite waveforms are
   more informative than raw waveforms and waveform-based point cloud for
   characterizing tree species in our study area. Both classical machine
   learning methods (the RF and CF) and the BMLR generated satisfactory
   average overall accuracy (74\% for the RF, 77\% for the CF and 81\% for
   the BMLR) and the BMLR slightly outperformed the other two methods.
   However, these three methods suffered from low individual classification
   accuracy for the blue oak which is prone to being misclassified as the
   interior live oak due to the similar characteristics of blue oak and
   interior live oak. Uncertainty estimates from the BMLR method compensate
   for this downside by providing classification results in a probabilistic
   sense and rendering users with more confidence in interpreting and
   applying classification results to real-world tasks such as forest
   inventory. Overall, this study recommends the CF method for feature
   selection and suggests that BMLR could be a superior alternative to
   classical machining learning methods.}},
DOI = {{10.3390/rs10010039}},
Article-Number = {{39}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Lawing, Michelle/F-7453-2019
   Popescu, Sorin C/D-5981-2015}},
ORCID-Numbers = {{Lawing, Michelle/0000-0003-4041-6177
   Popescu, Sorin C/0000-0002-8155-8801}},
Unique-ID = {{ISI:000424092300038}},
}

@article{ ISI:000423587100013,
Author = {Zhao, Jun-Li and Wu, Zhong-Ke and Pan, Zhen-Kuan and Duan, Fu-Qing and
   Li, Jin-Hua and Lv, Zhi-Han and Wang, Kang and Chen, Yu-Cong},
Title = {{3D Face Similarity Measure by Fr,chet Distances of Geodesics}},
Journal = {{JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY}},
Year = {{2018}},
Volume = {{33}},
Number = {{1}},
Pages = {{207-222}},
Month = {{JAN}},
Abstract = {{3D face similarity is a critical issue in computer vision, computer
   graphics and face recognition and so on. Since Fr,chet distance is an
   effective metric for measuring curve similarity, a novel 3D face
   similarity measure method based on Fr,chet distances of geodesics is
   proposed in this paper. In our method, the surface similarity between
   two 3D faces is measured by the similarity between two sets of 3D curves
   on them. Due to the intrinsic property of geodesics, we select geodesics
   as the comparison curves. Firstly, the geodesics on each 3D facial model
   emanating from the nose tip point are extracted in the same initial
   direction with equal angular increment. Secondly, the Fr,chet distances
   between the two sets of geodesics on the two compared facial models are
   computed. At last, the similarity between the two facial models is
   computed based on the Fr,chet distances of the geodesics obtained in the
   second step. We verify our method both theoretically and practically. In
   theory, we prove that the similarity of our method satisfies three
   properties: reflexivity, symmetry, and triangle inequality. And in
   practice, experiments are conducted on the open 3D face database GavaDB,
   Texas 3D Face Recognition database, and our 3D face database. After the
   comparison with iso-geodesic and Hausdorff distance method, the results
   illustrate that our method has good discrimination ability and can not
   only identify the facial models of the same person, but also distinguish
   the facial models of any two different persons.}},
DOI = {{10.1007/s11390-018-1814-7}},
ISSN = {{1000-9000}},
EISSN = {{1860-4749}},
Unique-ID = {{ISI:000423587100013}},
}

@article{ ISI:000422943700008,
Author = {Benedek, Csaba and Galai, Bence and Nagy, Balazs and Janko, Zsolt},
Title = {{Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance
   System}},
Journal = {{IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY}},
Year = {{2018}},
Volume = {{28}},
Number = {{1}},
Pages = {{101-113}},
Month = {{JAN}},
Abstract = {{This paper presents new approaches for gait and activity analysis based
   on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed
   algorithms are embedded into an integrated 4D vision and visualization
   system, which is able to analyze and interactively display real
   scenarios in natural outdoor environments with walking pedestrians. The
   main focus of the investigations is gait-based person reidentification
   during tracking and recognition of specific activity patterns, such as
   bending, waving, making phone calls, and checking the time looking at
   wristwatches. The descriptors for training and recognition are observed
   and extracted from realistic outdoor surveillance scenarios, where
   multiple pedestrians are walking in the field of interest following
   possibly intersecting trajectories; thus, the observations might often
   be affected by occlusions or background noise. Since there is no public
   database available for such scenarios, we created and published a new
   Lidar-based outdoor gait and activity data set on our website that
   contains point cloud sequences of 28 different persons extracted and
   aggregated from 35-min-long measurements. The presented results confirm
   that both efficient gait-based identification and activity recognition
   are achievable in the sparse point clouds of a single RMB Lidar sensor.
   After extracting the people trajectories, we synthesized a
   free-viewpoint video, in which moving avatar models follow the
   trajectories of the observed pedestrians in real time, ensuring that the
   leg movements of the animated avatars are synchronized with the real
   gait cycles observed in the Lidar stream.}},
DOI = {{10.1109/TCSVT.2016.2595331}},
ISSN = {{1051-8215}},
EISSN = {{1558-2205}},
ORCID-Numbers = {{Benedek, Csaba/0000-0003-3203-0741}},
Unique-ID = {{ISI:000422943700008}},
}

@article{ ISI:000418513500004,
Author = {Sghaier, Souhir and Farhat, Wajdi and Souani, Chokri},
Title = {{Novel Technique for 3D Face Recognition Using Anthropometric Methodology}},
Journal = {{INTERNATIONAL JOURNAL OF AMBIENT COMPUTING AND INTELLIGENCE}},
Year = {{2018}},
Volume = {{9}},
Number = {{1}},
Pages = {{60-77}},
Month = {{JAN-MAR}},
Abstract = {{This manuscript presents an improved system research that can detect and
   recognize the person in 3D space automatically and without the
   interaction of the people's faces. This system is based not only on a
   quantum computation and measurements to extract the vector features in
   the phase of characterization but also on learning algorithm (using SVM)
   to classify and recognize the person. This research presents an improved
   technique for automatic 3D face recognition using anthropometric
   proportions and measurement to detect and extract the area of interest
   which is unaffected by facial expression. This approach is able to treat
   incomplete and noisy images and reject the non-facial areas
   automatically. Moreover, it can deal with the presence of holes in the
   meshed and textured 3D image. It is also stable against small
   translation and rotation of the face. All the experimental tests have
   been done with two 3D face datasets FRAV 3D and GAVAB. Therefore, the
   test's results of the proposed approach are promising because they
   showed that it is competitive comparable to similar approaches in terms
   of accuracy, robustness, and flexibility. It achieves a high recognition
   performance rate of 95.35\% for faces with neutral and non-neutral
   expressions for the identification and 98.36\% for the authentification
   with GAVAB and 100\% with some gallery of FRAV 3D datasets.}},
DOI = {{10.4018/IJACI.2018010104}},
ISSN = {{1941-6237}},
EISSN = {{1941-6245}},
ResearcherID-Numbers = {{Farhat, Wajdi/N-5341-2015
   Souani, Chokri/B-1853-2015}},
ORCID-Numbers = {{Farhat, Wajdi/0000-0003-3647-8316
   Souani, Chokri/0000-0002-8987-3582}},
Unique-ID = {{ISI:000418513500004}},
}

@article{ ISI:000413384000029,
Author = {Barnes, Chloe and Balzter, Heiko and Barrett, Kirsten and Eddy, James
   and Milrier, Sam and Suarez, Juan C.},
Title = {{Airborne laser scanning and tree crown fragmentation metrics for the
   assessment of Phytophthora ramorum infected larch forest stands}},
Journal = {{FOREST ECOLOGY AND MANAGEMENT}},
Year = {{2017}},
Volume = {{404}},
Pages = {{294-305}},
Month = {{NOV 15}},
Abstract = {{The invasive phytopathogen Phytophthora ramorum has caused extensive
   infection of larch forest across areas of the UK, particularly in
   Southwest England, South Wales and Southwest Scotland. At present,
   landscape level assessment of the disease in these areas is conducted
   manually by tree health surveyors during helicopter surveys. Airborne
   laser scanning (ALS), also known as LiDAR, has previously been applied
   to the segmentation of larch tree crowns infected by P. ramorum
   infection and the detection of insect pests in coniferous tree species.
   This study evaluates metrics from high-density discrete ALS point clouds
   (24 points/m(2)) and canopy height models (CHMs) to identify individual
   trees infected with P. ramorum and to discriminate between four disease
   severity categories (NI: not infected, 1: light, 2: moderate, 3: heavy).
   The metrics derived from ALS point clouds include canopy cover,
   skewness, and bicentiles (B60, B70, B80 and B90) calculated using both a
   static (1 m) and a variable (50\% of tree height) cut-off height.
   Significant differences are found between all disease severity
   categories, except in the case of healthy individuals (NI) and those in
   the early stages of infection (category 1). In addition, fragmentation
   metrics are shown to identify the increased patchiness and infra-crown
   height irregularities of CHMs associated with individual trees subject
   to heavy infection (category 3) of P. ramorum. Classifications using a
   k-nearest neighbour (k-NN) classifier and ALS point cloud metrics to
   classify disease presence/absence and severity yielded overall
   accuracies of 72\% and 65\% respectively. The results indicate that ALS
   can be used to identify individual tree crowns subject to moderate and
   heavy P. ramorum infection in larch forests. This information
   demonstrates the potential applications of ALS for the development of a
   targeted phytosanitary approach for the management of P. ramorum.}},
DOI = {{10.1016/j.foreco.2017.08.052}},
ISSN = {{0378-1127}},
EISSN = {{1872-7042}},
ResearcherID-Numbers = {{Balzter, Heiko/B-5976-2008}},
ORCID-Numbers = {{Balzter, Heiko/0000-0002-9053-4684}},
Unique-ID = {{ISI:000413384000029}},
}

@article{ ISI:000416554100095,
Author = {Shen, Xin and Cao, Lin},
Title = {{Tree-Species Classification in Subtropical Forests Using Airborne
   Hyperspectral and LiDAR Data}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Accurate classification of tree-species is essential for sustainably
   managing forest resources and effectively monitoring species diversity.
   In this study, we used simultaneously acquired hyperspectral and LiDAR
   data from LiCHy (Hyperspectral, LiDAR and CCD) airborne system to
   classify tree-species in subtropical forests of southeast China. First,
   each individual tree crown was extracted using the LiDAR data by a point
   cloud segmentation algorithm (PCS) and the sunlit portion of each crown
   was selected using the hyperspectral data. Second, different suites of
   hyperspectral and LiDAR metrics were extracted and selected by the
   indices of Principal Component Analysis (PCA) and the mean decrease in
   Gini index (MDG) from Random Forest (RF). Finally, both hyperspectral
   metrics (based on whole crown and sunlit crown) and LiDAR metrics were
   assessed and used as inputs to Random Forest classifier to discriminate
   five tree-species at two levels of classification. The results showed
   that the tree delineation approach (point cloud segmentation algorithm)
   was suitable for detecting individual tree in this study (overall
   accuracy = 82.9\%). The classification approach provided a relatively
   high accuracy (overall accuracy > 85.4\%) for classifying five
   tree-species in the study site. The classification using both
   hyperspectral and LiDAR metrics resulted in higher accuracies than only
   hyperspectral metrics (the improvement of overall accuracies =
   0.4-5.6\%). In addition, compared with the classification using whole
   crown metrics (overall accuracies = 85.4-89.3\%), using sunlit crown
   metrics (overall accuracies = 87.1-91.5\%) improved the overall
   accuracies of 2.3\%. The results also suggested that fewer of the most
   important metrics can be used to classify tree-species effectively
   (overall accuracies = 85.8-91.0\%).}},
DOI = {{10.3390/rs9111180}},
Article-Number = {{1180}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000416554100095}},
}

@article{ ISI:000409180500015,
Author = {Sui, Dan and Hou, Deheng and Duan, Xinyu},
Title = {{An interpolation algorithm fitted for dynamic 3D face reconstruction}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2017}},
Volume = {{76}},
Number = {{19}},
Pages = {{19575-19589}},
Month = {{OCT}},
Abstract = {{In order to solve the problem of low recognition accuracy in later
   period which is caused by the too few extracted parameters in the 3D
   face recognition, and the incapable formation of completed point cloud
   structure. An automatic iterative interpolation algorithm is proposed.
   The new and more accurate 3D face data points are obtained by automatic
   iteration. This algorithm can be used to restore the data point cloud
   information of 3D facial feature in 2D images by means of facial
   three-legged structure formed by 3D face and automatic interpolation.
   Thus, it can realize to shape the 3D facial dynamic model which can be
   recognized and has high saturability. Experimental results show that the
   interpolation algorithm can achieve the complete the construction of
   facial feature based on the facial feature after 3D dynamic
   reconstruction, and the validity is higher.}},
DOI = {{10.1007/s11042-015-3233-x}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000409180500015}},
}

@article{ ISI:000404319800008,
Author = {Zhang, Jian and Li, Ke and Liang, Yun and Li, Na},
Title = {{Learning 3D faces from 2D images via Stacked Contractive Autoencoder}},
Journal = {{NEUROCOMPUTING}},
Year = {{2017}},
Volume = {{257}},
Pages = {{67-78}},
Month = {{SEP 27}},
Abstract = {{3D face reconstruction from a 2D face image has been found important to
   various applications such as face detection and recognition because a 3D
   face provides more semantic information than 2D image. This paper
   proposes a deep learning framework for 3D face reconstruction. The
   framework is designed to compute subspace feature of arbitrary face
   image, then map the feature to its counterpart in another subspace
   learned with 3D faces, and reconstruct the 3D face using the counterpart
   feature. During the course of training, we learn 2D and 3D subspaces
   through Stacked Contractive Autoencoders (SCAE), use a one-layer fully
   connected neural network to learn the mapping, and use the pre-trained
   parameters of the SCAEs and the one-layer network to initialize a deep
   feedforward neural network whose input are face images and output are 3D
   faces. The network is optimized by gradient descent algorithm with
   back-propagation. Extensive experimental results on various data sets
   indicate the effectiveness of the proposed SCAE-based 3D face
   reconstruction method. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2016.11.062}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000404319800008}},
}

@article{ ISI:000411153300086,
Author = {Liu, Ting and Lv, Jun and Qin, Yutao},
Title = {{Standardized tumor volume: an independent prognostic factor in advanced
   nasopharyngeal carcinoma}},
Journal = {{ONCOTARGET}},
Year = {{2017}},
Volume = {{8}},
Number = {{41}},
Pages = {{70299-70309}},
Month = {{SEP 19}},
Abstract = {{The study evaluated the prognostic effect of standardized tumor volume
   in patients with advanced nasopharyngeal carcinoma (NPC) treated with
   concurrent chemoradiotherapy. Between Jan 1, 2009 and December 30, 2012,
   143 patients diagnosed with NPC in UICC stage III-IVb by histopathology
   were enrolled in the study. These patients underwent intensity-modulated
   radiotherapy combined with concurrent chemotherapy. The
   three-dimensional images of tumor volume were reconstructed
   automatically by the treatment planning system. SGTVnx was calculated
   based on GTVnx/person's volume. SGTVnd was calculated based on
   GTVnd/person's volume. SGTVnx was significantly associated with the
   5-year overall survival (OS), disease-free survival (DFS), DMFS, and
   LRFS rates in univariate and multivariate analyses. Although SGTVnd was
   associated with the 5-year OS, DFS, and DMFS rates, it was not an
   independent prognostic factor for LRFS. In receiver operating
   characteristic (ROC) curve analysis, 1.091 and 0.273 were determined as
   the cut-off points for SGTVnx and SGTVnd, respectively. The 5-year OS,
   DFS, DMFS, and LRFS rates for patients with a SGTVnx > 1.091 vs. SGTVnx
   <= 1.091 was 65.4\% vs. 93.4\% (P < 0.001), 65.2\% vs. 94.8\% (P <
   0.001), 71.4\% vs. 97.4\% (P < 0.001), and 84.8\% vs. 97.3\% (P =
   0.003), respectively, for SGTVnd > 0.273 vs. SGTVnd <= 0.273 was 70.3\%
   vs. 96.5\% (P < 0.001), 70.1\% vs. 94.8\% (P < 0.001), 77.5\% vs. 98.2\%
   (P < 0.001), and 88.5\% vs. 96.6\% (P = 0.049), respectively. UICC stage
   grouping, T classification, N classification, and sex were not found to
   be independent prognostic factors for NPC. Standardized tumor volume was
   an independent prognostic factor for NPC that might improve the current
   NPC TNM classification system and provide new clinical evidence for
   personalized treatment strategies.}},
DOI = {{10.18632/oncotarget.20313}},
ISSN = {{1949-2553}},
Unique-ID = {{ISI:000411153300086}},
}

@article{ ISI:000410059200037,
Author = {Quint, S. and Christ, A. F. and Guckenberger, A. and Himbert, S. and
   Kaestner, L. and Gekle, S. and Wagner, C.},
Title = {{3D tomography of cells in micro-channels}},
Journal = {{APPLIED PHYSICS LETTERS}},
Year = {{2017}},
Volume = {{111}},
Number = {{10}},
Month = {{SEP 4}},
Abstract = {{We combine confocal imaging, microfluidics, and image analysis to record
   3D-images of cells in flow. This enables us to recover the full 3D
   representation of several hundred living cells per minute. Whereas 3D
   confocal imaging has thus far been limited to steady specimens, we
   overcome this restriction and present a method to access the 3D shape of
   moving objects. The key of our principle is a tilted arrangement of the
   micro-channel with respect to the focal plane of the microscope. This
   forces cells to traverse the focal plane in an inclined manner. As a
   consequence, individual layers of passing cells are recorded, which can
   then be assembled to obtain the volumetric representation. The full 3D
   information allows for a detailed comparison with theoretical and
   numerical predictions unfeasible with, e.g., 2D imaging. Our technique
   is exemplified by studying flowing red blood cells in a micro-channel
   reflecting the conditions prevailing in the microvasculature. We observe
   two very different types of shapes: ``croissants{''} and ``slippers.{''}
   Additionally, we perform 3D numerical simulations of our experiment to
   confirm the observations. Since 3D confocal imaging of cells in flow has
   not yet been realized, we see high potential in the field of flow
   cytometry where cell classification thus far mostly relies on 1D
   scattering and fluorescence signals. Published by AIP Publishing.}},
DOI = {{10.1063/1.4986392}},
Article-Number = {{103701}},
ISSN = {{0003-6951}},
EISSN = {{1077-3118}},
ResearcherID-Numbers = {{Wagner, Christian/A-1307-2009
   Gekle, Stephan/I-9695-2014}},
ORCID-Numbers = {{Wagner, Christian/0000-0001-7788-4594
   Gekle, Stephan/0000-0001-5597-1160}},
Unique-ID = {{ISI:000410059200037}},
}

@article{ ISI:000412378800003,
Author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Benouareth, Abdallah
   and Declercq, David},
Title = {{3D facial expression recognition using kernel methods on Riemannian
   manifold}},
Journal = {{ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE}},
Year = {{2017}},
Volume = {{64}},
Pages = {{25-32}},
Month = {{SEP}},
Abstract = {{Automatic human Facial Expressions Recognition (FER) is becoming of
   increased interest. FER finds its applications in many emerging areas
   such as affective computing and intelligent human computer interaction.
   Most of the existing work on FER has been done using 2D data which
   suffers from inherent problems of illumination changes and pose
   variations. With the development of 3D image capturing technologies, the
   acquisition of 3D data is becoming a more feasible task. The 3D data
   brings a more effective solution in addressing the issues raised by its
   2D counterpart. State-of-the-art 3D FER methods are often based on a
   single descriptor which may fail to handle the large inter-class and
   intra-class variability of the human facial expressions. In this work,
   we explore, for the first time, the usage of covariance matrices of
   descriptors, instead of the descriptors themselves, in 3D FER. Since
   covariance matrices are elements of the non-linear manifold of Symmetric
   Positive Definite (SPD) matrices, we particularly look at the
   application of manifold-based classification to the problem of 3D FER.
   We evaluate the performance of the proposed framework on the BU-3DFE and
   the Bosphorus datasets, and demonstrate its superiority compared to the
   state-of-the-art methods. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.engappai.2017.05.009}},
ISSN = {{0952-1976}},
EISSN = {{1873-6769}},
Unique-ID = {{ISI:000412378800003}},
}

@article{ ISI:000411548000002,
Author = {Jimenez-Pique, E. and Turon-Vinas, M. and Chen, H. and Trifonov, T. and
   Fair, J. and Tarres, E. and Llanes, L.},
Title = {{Focused ion beam tomography of WC-Co cemented carbides}},
Journal = {{INTERNATIONAL JOURNAL OF REFRACTORY METALS \& HARD MATERIALS}},
Year = {{2017}},
Volume = {{67}},
Pages = {{9-17}},
Month = {{SEP}},
Abstract = {{The microstructure of three different grades of WC-Co cemented carbides
   (hardmetals) has been reconstructed in three dimensions after sequential
   images obtained by focused ion beam. The three dimensional
   microstructual parameters are compared against the well-known two
   dimensional parameters of grain size, phase percentages and mean free
   path. Results show good agreement with the exception of individual grain
   recognition, which could not be univocally segmented. In the case of
   mean free path, the three-dimensional image depicts a more realistic
   description of the metal interconnections in the composite. Aiming for a
   simple example of direct application of these FIB tomography outcomes,
   reconstructed real microstructure for the coarser hardmetal grade
   studied was translated in a finite element modelling mesh, and elastic
   residual stresses were estimated from sintering to room temperature.
   Calculated thermal stresses agree with experimental results and show
   significant local variations in their value due to the complex
   microstructure of cemented carbides.}},
DOI = {{10.1016/j.ijrmhm.2017.04.007}},
ISSN = {{0263-4368}},
ResearcherID-Numbers = {{LLANES, LUIS/H-9761-2015
   }},
ORCID-Numbers = {{LLANES, LUIS/0000-0003-1054-1073
   Jimenez-Pique, Emilio/0000-0002-6950-611X}},
Unique-ID = {{ISI:000411548000002}},
}

@article{ ISI:000408398200010,
Author = {Eng, Z. H. D. and Yick, Y. Y. and Guo, Y. and Xu, H. and Reiner, M. and
   Cham, T. J. and Chen, S. H. A.},
Title = {{3D faces are recognized more accurately and faster than 2D faces, but
   with similar inversion effects}},
Journal = {{VISION RESEARCH}},
Year = {{2017}},
Volume = {{138}},
Pages = {{78-85}},
Month = {{SEP}},
Abstract = {{Recognition of faces typically occurs via holistic processing where
   individual features are combined to provide an overall facial
   representation. However, when faces are inverted, there is greater
   reliance on featural processing where faces are recognized based on
   their individual features. These findings are based on a substantial
   number of studies using 2-dimensional (2D) faces and it is unknown
   whether these results can be extended to 3-dimensional (3D) faces, which
   have more depth information that is absent in the typical 2D stimuli
   used in face recognition literature. The current study used the face
   inversion paradigm as a means to investigate how holistic and featural
   processing are differentially influenced by 2D and 3D faces. Twenty-five
   participants completed a delayed face-matching task consisting of
   upright and inverted faces that were presented as both 2D and 3D
   stereoscopic images. Recognition accuracy was significantly higher for
   3D upright faces compared to 2D upright faces, providing support that
   the enriched visual information in 3D stereoscopic images facilitates
   holistic processing that is essential for the recognition of upright
   faces. Typical face inversion effects were also obtained, regardless of
   whether the faces were presented in 2D or 3D. Moreover, recognition
   performances for 2D inverted and 3D inverted faces did not differ. Taken
   together, these results demonstrated that 3D stereoscopic effects
   influence face recognition during holistic processing but not during
   featural processing. Our findings therefore provide a novel perspective
   that furthers our understanding of face recognition mechanisms, shedding
   light on how the integration of stereoscopic information in 3D faces
   influences face recognition processes. (c) 2017 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.visres.2017.06.004}},
ISSN = {{0042-6989}},
EISSN = {{1878-5646}},
ResearcherID-Numbers = {{Chen, SH Annabel/F-3742-2011
   }},
ORCID-Numbers = {{Chen, SH Annabel/0000-0002-1540-5516
   Xu, Hong/0000-0003-1389-5408}},
Unique-ID = {{ISI:000408398200010}},
}

@article{ ISI:000407961700001,
Author = {Damon, James and Gasparovic, Ellen},
Title = {{Modeling Multi-object Configurations via Medial/Skeletal Linking
   Structures}},
Journal = {{INTERNATIONAL JOURNAL OF COMPUTER VISION}},
Year = {{2017}},
Volume = {{124}},
Number = {{3}},
Pages = {{255-272}},
Month = {{SEP}},
Abstract = {{We introduce a method for modeling a configuration of objects in 2D or
   3D images using a mathematical ``skeletal linking structure{''} which
   will simultaneously capture the individual shape features of the objects
   and their positional information relative to one another. The objects
   may either have smooth boundaries and be disjoint from the others or
   share common portions of their boundaries with other objects in a
   piecewise smooth manner. These structures include a special class of
   ``Blum medial linking structures{''}, which are intrinsically associated
   to the configuration and build upon the Blum medial axes of the
   individual objects. We give a classification of the properties of Blum
   linking structures for generic configurations. The skeletal linking
   structures add increased flexibility for modeling configurations of
   objects by relaxing the Blum conditions and they extend in a minimal way
   the individual ``skeletal structures{''} which have been previously used
   for modeling individual objects and capturing their geometric
   properties. This allows for the mathematical methods introduced for
   single objects to be significantly extended to the entire configuration
   of objects. These methods not only capture the internal shape structures
   of the individual objects but also the external structure of the
   neighboring regions of the objects. In the subsequent second paper
   (Damon and Gasparovic in Shape and positional geometry of multi-object
   configurations) we use these structures to identify specific external
   regions which capture positional information about neighboring objects,
   and we develop numerical measures for closeness of portions of objects
   and their significance for the configuration. This allows us to use the
   same mathematical structures to simultaneously analyze both the shape
   properties of the individual objects and positional properties of the
   configuration. This provides a framework for analyzing the statistical
   properties of collections of similar configurations such as for
   applications to medical imaging.}},
DOI = {{10.1007/s11263-017-1019-5}},
ISSN = {{0920-5691}},
EISSN = {{1573-1405}},
Unique-ID = {{ISI:000407961700001}},
}

@article{ ISI:000403135200018,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal and Eastwood, Peter},
Title = {{Deep, dense and accurate 3D face correspondence for generating
   population specific deformable models}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2017}},
Volume = {{69}},
Pages = {{238-250}},
Month = {{SEP}},
Abstract = {{We present a multilinear algorithm to automatically establish dense
   point-to-point correspondence over an arbitrarily large number of
   population specific 3D faces across identities, facial expressions and
   poses. The algorithm is initialized with a subset of anthropometric
   landmarks detected by our proposed Deep Landmark Identification Network
   which is trained on synthetic images. The landmarks are used to segment
   the 3D face into Voronoi regions by evolving geodesic level set curves.
   Exploiting the intrinsic features of these regions, we extract
   discriminative keypoints on the facial manifold to elastically match the
   regions across faces for establishing dense correspondence. Finally, we
   generate a Region based 3D Deformable Model which is fitted to unseen
   faces to transfer the correspondences. We evaluate our algorithm on the
   tasks of facial landmark detection and recognition using two benchmark
   datasets. Comparison with thirteen state-of-the-art techniques shows the
   efficacy of our algorithm. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2017.04.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Eastwood, Peter/0000-0002-4490-4138
   Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000403135200018}},
}

@article{ ISI:000413881700007,
Author = {Ouamane, Abdelmalik and Boutellaa, Elhocine and Bengherabi, Messaoud and
   Taleb-Ahmed, Abdelmalik and Hadid, Abdenour},
Title = {{A novel statistical and multiscale local binary feature for 2D and 3D
   face verification}},
Journal = {{COMPUTERS \& ELECTRICAL ENGINEERING}},
Year = {{2017}},
Volume = {{62}},
Pages = {{68-80}},
Month = {{AUG}},
Abstract = {{In this paper, we propose a face verification framework using 2D and 3D
   images. We first introduce a novel face descriptor based on the local
   statistics of the 2D and 3D images. In the proposed framework, the novel
   descriptor is combined with three other popular and effective local
   descriptors, namely, Local Binary Patterns (LBP), Local Phase
   Quantization (LPQ) and Binarized Statistical Image Features (BSIF). The
   multiscale variants of these four descriptors are investigated seeking
   better performance. To reduce the feature vector dimensionality and
   mitigate the class intra-variability, we use Exponential Discriminant
   Analysis (EDA) and Within Class Covariance Normalization (WCCN),
   respectively. Finally, a score level fusion scheme is adopted to combine
   different face descriptors and modalities. An extensive evaluation of
   the proposed framework is carried out on two publicly available and
   largely used 2D+3D face databases, namely FRGC v2.0 and CAISA 3D.
   Promising results that favorably compare to the state of the art are
   obtained. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.compeleceng.2017.01.001}},
ISSN = {{0045-7906}},
EISSN = {{1879-0755}},
Unique-ID = {{ISI:000413881700007}},
}

@article{ ISI:000410870200001,
Author = {Jo, Jaeik and Kim, Hyunjun and Kim, Jaihie},
Title = {{3D facial shape reconstruction using macro- and micro-level features
   from high resolution facial images}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{64}},
Pages = {{1-9}},
Month = {{AUG}},
Abstract = {{Three-dimensional (3D) facial modeling and stereo matching-based methods
   are widely used for 3D facial reconstruction from 2D single-view and
   multiple-view images. However, these methods cannot realistically
   reconstruct 3D faces because they use insufficient numbers of
   macro-level Facial Feature Points (FFPs). This paper proposes an
   accurate and person-specific 3D facial reconstruction method that uses
   ample numbers of macro and micro-level FFPs to enable coverage of all
   facial regions of high resolution facial images. Comparisons of 3D
   facial images reconstructed using the proposed method for ground-truth
   3D facial images from the Bosphorus 3D database show that the method is
   superior to a conventional Active Appearance Model-Structure from Motion
   (AAM + SfM)-based method in terms of average 3D root mean square error
   between the reconstructed and ground-truth 3D faces. Further, the
   proposed method achieved outstanding accuracy in local facial regions
   such as the cheek areas where extraction of FFPs is difficult for
   existing methods. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/jimavis.2017.05.001}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000410870200001}},
}

@article{ ISI:000410870200008,
Author = {Xia, Baiqiang and Ben Amor, Boulbaba and Daoudi, Mohamed},
Title = {{= Joint gender, ethnicity and age estimation from 3D faces An
   experimental illustration of their correlations}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{64}},
Pages = {{90-102}},
Month = {{AUG}},
Abstract = {{Humans present clear demographic traits which allow their peers to
   recognize their gender and ethnic groups as well as estimate their age.
   Abundant literature has investigated the problem of automated gender,
   ethnicity and age recognition from facial images. However, despite the
   co-existence of these traits, most of the studies have addressed them
   separately, very little attention has been given to their correlations.
   In this work, we address the problem of joint demographic estimation and
   investigate the correlation through the morphological differences in 3D
   facial shapes. To this end, a set of facial features are extracted to
   capture the 3D shape differences among the demographic groups. Then, a
   correlation-based feature selection is applied to highlight salient
   features and remove redundancy. These features are later fed to Random
   Forest for gender and ethnicity classification, and age estimation.
   Extensive experiments conducted on FRGCv2 dataset, under
   Expression-Dependent and Expression-Independent settings, demonstrate
   the effectiveness of the proposed approaches for the three traits, and
   also show the accuracy improvement when considering their correlations.
   To the best of our knowledge, this is the first study exploring the
   correlations of these facial soft-biometric traits using 3D faces. This
   is also the first work which studies the problem of age estimation from
   3D Faces.(1) (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2017.06.004}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305}},
Unique-ID = {{ISI:000410870200008}},
}

@article{ ISI:000403860600008,
Author = {Zhu, Qing and Li, Yuan and Hu, Han and Wu, Bo},
Title = {{Robust point cloud classification based on multi-level semantic
   relationships for urban scenes}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{129}},
Pages = {{86-102}},
Month = {{JUL}},
Abstract = {{The semantic classification of point clouds is a fundamental part of
   three-dimensional urban reconstruction. For datasets with high spatial
   resolution but significantly more noises, a general trend is to exploit
   more contexture information to surmount the decrease of discrimination
   of features for classification. However, previous works on adoption of
   contexture information are either too restrictive or only in a small
   region and in this paper, we propose a point cloud classification method
   based on multi-level semantic relationships, including
   point-homogeneity, supervoxel-adjacency and class-knowledge constraints,
   which is more versatile and incrementally propagate the classification
   cues from individual points to the object level and formulate them as a
   graphical model. The point-homogeneity constraint clusters points with
   similar geometric and radiometric properties into regular-shaped
   supervoxels that correspond to the vertices in the graphical model. The
   supervoxel-adjacency constraint contributes to the pairwise interactions
   by providing explicit adjacent relationships between supervoxels. The
   class knowledge constraint operates at the object level based on
   semantic rules, guaranteeing the classification correctness of
   supervoxel clusters at that level. International Society of
   Photogrammetry and Remote Sensing (ISPRS) benchmark tests have shown
   that the proposed method achieves state-of-the-art performance with an
   average per-area completeness and correctness of 93.88\% and 95.78\%,
   respectively. The evaluation of classification of photogrammetric point
   clouds and DSM generated from aerial imagery confirms the method's
   reliability in several challenging urban scenes. (C) 2017 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2017.04.022}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Wu, Bo/J-6177-2012
   Hu, Han/V-5068-2018}},
ORCID-Numbers = {{Wu, Bo/0000-0001-9530-3044
   Hu, Han/0000-0003-1137-2208}},
Unique-ID = {{ISI:000403860600008}},
}

@article{ ISI:000403031400006,
Author = {Milenkovic, Milutin and Wagner, Wolfgang and Quast, Raphael and Hollaus,
   Markus and Ressl, Camillo and Pfeifer, Norbert},
Title = {{Total canopy transmittance estimated from small-footprint, full-waveform
   airborne LiDAR}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{128}},
Pages = {{61-72}},
Month = {{JUN}},
Abstract = {{Canopy transmittance is a directional and wavelength-specific physical
   parameter that quantifies the amount of radiation attenuated when
   passing through a vegetation layer. The parameter has been estimated
   from LiDAR data in many different ways over the years. While early LiDAR
   methods treated each returned echo equally or weighted the echoes
   according to their return order, recent methods have focused more on the
   echo energy. In this study, we suggest a new method of estimating the
   total canopy transmittance considering only the energy of ground echoes.
   Therefore, this method does not require assumptions for the reflectance
   or absorption behavior of vegetation. As the oblique looking geometry of
   LiDAR is explicitly considered, canopy transmittance can be derived for
   individual laser beams and can be mapped spatially. The method was
   applied on a contemporary full-waveform LiDAR data set collected under
   leaf-off conditions and over a study site that contains two sub regions:
   one with a mixed (coniferous and deciduous) forest and another that is
   predominantly a deciduous forest in an alluvial plain. The resulting
   canopy transmittance map was analyzed for both sub regions and compared
   to aerial photos and the well-known fractional cover method. A visual
   comparison with aerial photos showed that even single trees and small
   canopy openings are visible in the canopy transmittance map. In
   comparison with the fractional cover method, the canopy transmittance
   map showed no saturation, i.e., there was better separability between
   patches with different vegetation structure. (C) 2017 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2017.03.008}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Quast, Raphael/0000-0003-0419-4546
   Milenkovic, Milutin/0000-0003-3256-6669
   Pfeifer, Norbert/0000-0002-2348-7929}},
Unique-ID = {{ISI:000403031400006}},
}

@article{ ISI:000401888600006,
Author = {Coomes, David A. and Dalponte, Michele and Jucker, Tommaso and Asner,
   Gregory P. and Banin, Lindsay F. and Burslem, David F. R. P. and Lewis,
   Simon L. and Nilus, Reuben and Phillips, Oliver L. and Phua, Mui-How and
   Qie, Lan},
Title = {{Area-based vs tree-centric approaches to mapping forest carbon in
   Southeast Asian forests from airborne laser scanning data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2017}},
Volume = {{194}},
Pages = {{77-88}},
Month = {{JUN 1}},
Abstract = {{Tropical forests are a key component of the global carbon cycle, and
   mapping their carbon density is essential for understanding human
   influences on climate and for ecosystem-service-based payments for
   forest protection. Discrete-return airborne laser scanning (ALS) is
   increasingly recognised as a high-quality technology for mapping
   tropical forest carbon, because it generates 3D point clouds of forest
   structure from which aboveground carbon density (ACD) can be estimated.
   Area-based models are state of the art when it comes to estimating ACD
   from ALS data, but discard tree-level information contained within the
   ALS point cloud. This paper compares area based and tree-centric models
   for estimating ACD in lowland old-growth forests in Sabah, Malaysia.
   These forests are challenging to map because of their immense height. We
   compare the performance of (a) an area-based model developed by Asner
   and Mascaro (2014), and used primarily in the neotropics hitherto, with
   (b) a tree-centric approach that uses a new algorithm (itcSegment) to
   locate trees within the ALS canopy height model, measures their heights
   and crown widths, and calculates biomass from these dimensions. We find
   that Asner and Mascaro's model needed regional calibration, reflecting
   the distinctive structure of Southeast Asian forests. We also discover
   that forest basal area is closely related to canopy gap fraction
   measured by ALS, and use this finding to refine Asner and Mascaro's
   model. Finally, we show that our tree-centric approach is less accurate
   at estimating ACD than the best-performing area-based model (RMSE 18\%
   vs 13\%). Tree-centric modelling is appealing because it is based on
   summing the biomass of individual trees, but until algorithms can detect
   understory trees reliably and estimate biomass from crown dimensions
   precisely, areas-based modelling will remain the method of choice. (C)
   2017 The Authors. Published by Elsevier Inc.}},
DOI = {{10.1016/j.rse.2017.03.017}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Burslem, David FRP/F-1204-2019
   Jucker, Tommaso/S-4724-2017
   Phillips, Oliver L/A-1523-2011
   Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Burslem, David FRP/0000-0001-6033-0990
   Jucker, Tommaso/0000-0002-0751-6312
   Phillips, Oliver L/0000-0002-8993-6168
   Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000401888600006}},
}

@article{ ISI:000402350200003,
Author = {Eskandari, A. H. and Arjmand, N. and Shirazi-Adl, A. and Farahmand, F.},
Title = {{Subject-specific 2D/3D image registration and kinematics-driven
   musculoskeletal model of the spine}},
Journal = {{JOURNAL OF BIOMECHANICS}},
Year = {{2017}},
Volume = {{57}},
Pages = {{18-26}},
Month = {{MAY}},
Abstract = {{An essential input to the musculoskeletal (MS) trunk models that
   estimate muscle and spine forces is kinematics of the thorax, pelvis,
   and lumbar vertebrae. While thorax and pelvis kinematics are usually
   measured via skin motion capture devices (with inherent errors on the
   proper identification of the underlying bony landmarks and the relative
   skin-sensor-bone movements), those of the intervening lumbar vertebrae
   are commonly approximated at fixed proportions based on the
   thorax-pelvis kinematics. This study proposes an image-based kinematics
   measurement approach to drive subject-specific (musculature, geometry,
   mass, and center of masses) MS models. Kinematics of the thorax, pelvis,
   and individual lumbar vertebrae as well as disc inclinations, gravity
   loading, and musculature were all measured via different imaging
   techniques. The model estimated muscle and lumbar forces in various
   upright and flexed postures in which kinematics were obtained using
   upright fluoroscopy via 2D/3D image registration. Predictions of this
   novel image-kinematics-driven model (Img-KD) were compared with those of
   the traditional kinematics-driven (T-KD) model in which individual
   lumbar vertebral rotations were assumed based on thorax-pelvis
   orientations. Results indicated that while differences between Img-KD
   and T-KD models remained small for the force in the global muscles
   (attached to the thoracic cage) (<15\%), L4-S1 compression (<15\%), and
   shear (<20\%) forces in average for all the simulated tasks, they were
   relatively larger for the force in the local muscles (attached to the
   lumbar vertebrae). Assuming that the skin-based measurements of thorax
   and pelvis kinematics are accurate enough, the T-KD model predictions of
   spinal forces remain reliable. (C) 2017 Elsevier Ltd. All rights
   reserved,}},
DOI = {{10.1016/j.jbiomech.2017.03.011}},
ISSN = {{0021-9290}},
EISSN = {{1873-2380}},
ResearcherID-Numbers = {{Farahmand, Farzam/B-3921-2011}},
ORCID-Numbers = {{Farahmand, Farzam/0000-0001-8900-7003}},
Unique-ID = {{ISI:000402350200003}},
}

@article{ ISI:000390884300006,
Author = {Jones, Scott P. and Dwyer, Dominic M. and Lewis, Michael B.},
Title = {{The utility of multiple synthesized views in the recognition of
   unfamiliar faces}},
Journal = {{QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY}},
Year = {{2017}},
Volume = {{70}},
Number = {{5}},
Pages = {{906-918}},
Month = {{MAY}},
Abstract = {{The ability to recognize an unfamiliar individual on the basis of prior
   exposure to a photograph is notoriously poor and prone to errors, but
   recognition accuracy is improved when multiple photographs are
   available. In applied situations, when only limited real images are
   available (e.g., from a mugshot or CCTV image), the generation of new
   images might provide a technological prosthesis for otherwise fallible
   human recognition. We report two experiments examining the effects of
   providing computer-generated additional views of a target face. In
   Experiment 1, provision of computer-generated views supported better
   target face recognition than exposure to the target image alone and
   equivalent performance to that for exposure of multiple photograph
   views. Experiment 2 replicated the advantage of providing generated
   views, but also indicated an advantage for multiple viewings of the
   single target photograph. These results strengthen the claim that
   identifying a target face can be improved by providing multiple
   synthesized views based on a single target image. In addition, our
   results suggest that the degree of advantage provided by synthesized
   views may be affected by the quality of synthesized material.}},
DOI = {{10.1080/17470218.2016.1158302}},
ISSN = {{1747-0218}},
EISSN = {{1747-0226}},
ResearcherID-Numbers = {{Dwyer, Dominic Michael/D-1498-2009
   }},
ORCID-Numbers = {{Dwyer, Dominic Michael/0000-0001-8069-5508
   Lewis, Michael/0000-0002-5735-5318
   Jones, Scott/0000-0001-5516-4385}},
Unique-ID = {{ISI:000390884300006}},
}

@article{ ISI:000400214700018,
Author = {Dunham, Lisa and Wartman, Joseph and Olsen, Michael J. and O'Banion,
   Matthew and Cunningham, Keith},
Title = {{Rockfall Activity Index (RAI): A lidar-derived, morphology-based method
   for hazard assessment}},
Journal = {{ENGINEERING GEOLOGY}},
Year = {{2017}},
Volume = {{221}},
Pages = {{184-192}},
Month = {{APR 20}},
Abstract = {{In this paper, we introduce the Rockfall Activity Index (RAI), a point
   cloud-derived, high-resolution, morphology based approach for assessing
   rockfall hazards. With the RAI methodology, rockfall hazards are
   evaluated in a two-step procedure. First, morphological indices (local
   slope and roughness) are used to classify mass wasting processes acting
   on a rock-slope. These classifications are then used with estimated
   instability rates to map rockfall activity across an entire slope face.
   The rockfall hazard is quantified as the estimated annual kinetic energy
   produced by rockfall along 1-m length segments of a slope face. Field
   assessment of the RAI method at multiple study sites indicates that the
   morphology-derived classification and hazard assessment routines provide
   results that closely match the observed behavior and performance of rock
   slopes. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.enggeo.2017.03.009}},
ISSN = {{0013-7952}},
EISSN = {{1872-6917}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000400214700018}},
}

@article{ ISI:000425868600001,
Author = {Wang, Lamei and Chen, Wenfeng and Li, Hong},
Title = {{Use of 3D faces facilitates facial expression recognition in children}},
Journal = {{SCIENTIFIC REPORTS}},
Year = {{2017}},
Volume = {{7}},
Month = {{APR 3}},
Abstract = {{This study assessed whether presenting 3D face stimuli could facilitate
   children's facial expression recognition. Seventy-one children aged
   between 3 and 6 participated in the study. Their task was to judge
   whether a face presented in each trial showed a happy or fearful
   expression. Half of the face stimuli were shown with 3D representations,
   whereas the other half of the images were shown as 2D pictures. We
   compared expression recognition under these conditions. The results
   showed that the use of 3D faces improved the speed of facial expression
   recognition in both boys and girls. Moreover, 3D faces improved boys'
   recognition accuracy for fearful expressions. Since fear is the most
   difficult facial expression for children to recognize, the facilitation
   effect of 3D faces has important practical implications for children
   with difficulties in facial expression recognition. The potential
   benefits of 3D representation for other expressions also have
   implications for developing more realistic assessments of children's
   expression recognition.}},
DOI = {{10.1038/srep45464}},
Article-Number = {{45464}},
ISSN = {{2045-2322}},
ResearcherID-Numbers = {{Chen, Wenfeng/H-2424-2012
   }},
ORCID-Numbers = {{Chen, Wenfeng/0000-0002-4271-8366
   wang, la mei/0000-0002-9203-5539}},
Unique-ID = {{ISI:000425868600001}},
}

@article{ ISI:000401097300027,
Author = {Santoro, Valeria and Lubelli, Sergio and De Donno, Antonio and
   Inchingolo, Alessio and Lavecchia, Fulvio and Introna, Francesco},
Title = {{Photogrammetric 3D skull/photo superimposition: A pilot study}},
Journal = {{FORENSIC SCIENCE INTERNATIONAL}},
Year = {{2017}},
Volume = {{273}},
Pages = {{168-174}},
Month = {{APR}},
Abstract = {{The identification of bodies through the examination of skeletal remains
   holds a prominent place in the field of forensic investigations.
   Technological advancements in 3D facial acquisition techniques have led
   to the proposal of a new body identification technique that involves a
   combination of craniofacial superimposition and photogrammetry. The aim
   of this study was to test the method by superimposing various
   computerized 3D images of skulls onto various photographs of missing
   people taken while they were still alive in cases when there was a
   suspicion that the skulls in question belonged to them. The technique is
   divided into four phases: preparatory phase, 3d acquisition phase,
   superimposition phase, and metric image analysis 3d.
   The actual superimposition of the images was carried out in the fourth
   step. and was done so by comparing the skull images with the selected
   photos.
   Using a specific software, the two images (i.e. the 3D avatar and the
   photo of the missing person) were superimposed. Cross-comparisons of 5
   skulls discovered in a mass grave, and of 2 skulls retrieved in the
   crawlspace of a house were performed. The morphologyc phase reveals a
   full overlap between skulls and photos of disappeared persons. Metric
   phase reveals that correlation coefficients of this values, higher than
   0.998-0,997 allow to confirm identification hypothesis. (C) 2017
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.forsciint.2017.02.006}},
ISSN = {{0379-0738}},
EISSN = {{1872-6283}},
Unique-ID = {{ISI:000401097300027}},
}

@article{ ISI:000398720100091,
Author = {Weinmann, Martin and Weinmann, Michael and Mallet, Clement and Bredif,
   Mathieu},
Title = {{A Classification-Segmentation Framework for the Detection of Individual
   Trees in Dense MMS Point Cloud Data Acquired in Urban Areas}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{In this paper, we present a novel framework for detecting individual
   trees in densely sampled 3D point cloud data acquired in urban areas.
   Given a 3D point cloud, the objective is to assign point-wise labels
   that are both class-aware and instance-aware, a task that is known as
   instance-level segmentation. To achieve this, our framework addresses
   two successive steps. The first step of our framework is given by the
   use of geometric features for a binary point-wise semantic
   classification with the objective of assigning semantic class labels to
   irregularly distributed 3D points, whereby the labels are defined as
   ``tree points{''} and ``other points{''}. The second step of our
   framework is given by a semantic segmentation with the objective of
   separating individual trees within the ``tree points{''}. This is
   achieved by applying an efficient adaptation of the mean shift algorithm
   and a subsequent segment-based shape analysis relying on semantic rules
   to only retain plausible tree segments. We demonstrate the performance
   of our framework on a publicly available benchmark dataset, which has
   been acquired with a mobile mapping system in the city of Delft in the
   Netherlands. This dataset contains 10.13 M labeled 3D points among which
   17.6\% are labeled as ``tree points{''}. The derived results clearly
   reveal a semantic classification of high accuracy (up to 90.77\%) and an
   instance-level segmentation of high plausibility, while the simplicity,
   applicability and efficiency of the involved methods even allow applying
   the complete framework on a standard laptop computer with a reasonable
   processing time (less than 2.5 h).}},
DOI = {{10.3390/rs9030277}},
Article-Number = {{277}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Bredif, Mathieu/T-3029-2018
   }},
ORCID-Numbers = {{Bredif, Mathieu/0000-0003-0228-1232
   Mallet, Clement/0000-0002-2675-165X
   Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000398720100091}},
}

@article{ ISI:000399162400002,
Author = {Lindberg, Eva and Holmgren, Johan},
Title = {{Individual Tree Crown Methods for 3D Data from Remote Sensing}},
Journal = {{CURRENT FORESTRY REPORTS}},
Year = {{2017}},
Volume = {{3}},
Number = {{1}},
Pages = {{19-31}},
Month = {{MAR}},
Abstract = {{Purpose of Review The rapid development of remote sensing technology
   hasmade dense 3D data available from airborne laser scanning and
   recently also photogrammetric point clouds. This paper reviews methods
   for extraction of individual trees from 3D data and their applications
   in forestry and ecology.
   Recent Findings Methods for analysis of 3D data at tree level have been
   developed since the turn of the century. The first algorithms were based
   on 2D surface models of the upper contours of tree crowns. These methods
   are robust and provide information about the trees in the top-most
   canopy. There are also methods that use the complete 3D data. However,
   development of these 3D methods is still needed to include use of
   geometric properties. To detect a large fraction of the tallest trees, a
   surface model method generally gives the best results, but detection of
   smaller trees below the top-most canopy requires methods utilizing the
   whole point cloud. Several new sensors are now available with capability
   to describe the upper part of the canopy, which can be used to
   frequently update vegetation maps. Highly sensitive laser photo
   detectors have become available for civilian applications, which will
   enable acquisition of high-resolution 3D laser data for large areas to
   much lower costs.
   Summary Methods for ITC delineation from 3D data provide information
   about a large fraction of the trees, but there is still a challenge to
   make optimal use of the information from whole point cloud. Newly
   developed sensors might make ITC methods cheaper and feasible for large
   areas.}},
DOI = {{10.1007/s40725-017-0051-6}},
ISSN = {{2198-6436}},
Unique-ID = {{ISI:000399162400002}},
}

@article{ ISI:000398720100002,
Author = {Nevalainen, Olli and Honkavaara, Eija and Tuominen, Sakari and Viljanen,
   Niko and Hakala, Teemu and Yu, Xiaowei and Hyyppa, Juha and Saari,
   Heikki and Polonen, Ilkka and Imai, Nilton N. and Tommaselli, Antonio M.
   G.},
Title = {{Individual Tree Detection and Classification with UAV-Based
   Photogrammetric Point Clouds and Hyperspectral Imaging}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{Small unmanned aerial vehicle (UAV) based remote sensing is a rapidly
   evolving technology. Novel sensors and methods are entering the market,
   offering completely new possibilities to carry out remote sensing tasks.
   Three-dimensional (3D) hyperspectral remote sensing is a novel and
   powerful technology that has recently become available to small UAVs.
   This study investigated the performance of UAV-based photogrammetry and
   hyperspectral imaging in individual tree detection and tree species
   classification in boreal forests. Eleven test sites with 4151 reference
   trees representing various tree species and developmental stages were
   collected in June 2014 using a UAV remote sensing system equipped with a
   frame format hyperspectral camera and an RGB camera in highly variable
   weather conditions. Dense point clouds were measured photogrammetrically
   by automatic image matching using high resolution RGB images with a 5 cm
   point interval. Spectral features were obtained from the hyperspectral
   image blocks, the large radiometric variation of which was compensated
   for by using a novel approach based on radiometric block adjustment with
   the support of in-flight irradiance observations. Spectral and 3D point
   cloud features were used in the classification experiment with various
   classifiers. The best results were obtained with Random Forest and
   Multilayer Perceptron (MLP) which both gave 95\% overall accuracies and
   an F-score of 0.93. Accuracy of individual tree identification from the
   photogrammetric point clouds varied between 40\% and 95\%, depending on
   the characteristics of the area. Challenges in reference measurements
   might also have reduced these numbers. Results were promising,
   indicating that hyperspectral 3D remote sensing was operational from a
   UAV platform even in very difficult conditions. These novel methods are
   expected to provide a powerful tool for automating various environmental
   close-range remote sensing tasks in the very near future.}},
DOI = {{10.3390/rs9030185}},
Article-Number = {{185}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Imai, Nilton/O-8909-2018
   }},
ORCID-Numbers = {{Imai, Nilton/0000-0003-0516-0567
   Nevalainen, Olli/0000-0002-4826-2929
   Honkavaara, Eija/0000-0002-7236-2145}},
Unique-ID = {{ISI:000398720100002}},
}

@article{ ISI:000398720100102,
Author = {Ni, Huan and Lin, Xiangguo and Zhang, Jixian},
Title = {{Classification of ALS Point Cloud with Improved Point Cloud Segmentation
   and Random Forests}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{This paper presents an automated and effective framework for classifying
   airborne laser scanning (ALS) point clouds. The framework is composed of
   four stages: (i) step-wise point cloud segmentation, (ii) feature
   extraction, (iii) Random Forests (RF) based feature selection and
   classification, and (iv) post-processing. First, a step-wise point cloud
   segmentation method is proposed to extract three kinds of segments,
   including planar, smooth and rough surfaces. Second, a segment, rather
   than an individual point, is taken as the basic processing unit to
   extract features. Third, RF is employed to select features and classify
   these segments. Finally, semantic rules are employed to optimize the
   classification result. Three datasets provided by Open Topography are
   utilized to test the proposed method. Experiments show that our method
   achieves a superior classification result with an overall classification
   accuracy larger than 91.17\%, and kappa coefficient larger than 83.79\%.}},
DOI = {{10.3390/rs9030288}},
Article-Number = {{288}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000398720100102}},
}

@article{ ISI:000397032500012,
Author = {Yao, Fei and Wang, Jian and Yao, Ju and Hang, Fangrong and Lei, Xu and
   Cao, Yongke},
Title = {{Three-dimensional image reconstruction with free open-source OsiriX
   software in video-assisted thoracoscopic lobectomy and segmentectomy}},
Journal = {{INTERNATIONAL JOURNAL OF SURGERY}},
Year = {{2017}},
Volume = {{39}},
Pages = {{16-22}},
Month = {{MAR}},
Abstract = {{Objective: The aim of this retrospective study was to evaluate the
   practice and the feasibility of Osirix, a free and open-source medical
   imaging software, in performing accurate video-assisted thoracoscopic
   lobectomy and segmentectomy.
   Methods: From July 2014 to April 2016, 63 patients received anatomical
   video-assisted thoracoscopic surgery (VATS), either lobectomy or
   segmentectomy, in our department. Three-dimensional (3D) reconstruction
   images of 61 (96.8\%) patients were preoperatively obtained with
   contrast-enhanced computed tomography (CT). Preoperative resection
   simulations were accomplished with patient-individual reconstructed 3D
   images. For lobectomy, pulmonary lobar veins, arteries and bronchi were
   identified meticulously by carefully reviewing the 3D images on the
   display. For segmentectomy, the intrasegmental veins in the affected
   segment for division and the intersegmental veins to be preserved were
   identified on the 3D images. Patient preoperative characteristics,
   surgical outcomes and postoperative data were reviewed from a
   prospective database.
   Results: The study cohort of 63 patients included 33 (52.4\%) men and 30
   (47.6\%) women, of whom 46 (73.0\%) underwent VATS lobectomy and 17
   (27.0\%) underwent VATS segmentectomy. There was 1 conversion from VATS
   lobectomy to open thoracotomy because of fibrocalcified lymph nodes. A
   VATS lobectomy was performed in 1 case after completing the
   segmentectomy because invasive adenocarcinoma was detected by
   intraoperative frozen-section analysis. There were no 30-day or 90-day
   operative mortalities
   Conclusions: The free, simple, and user-friendly software program Osirix
   can provide a 3D anatomic structure of pulmonary vessels and a clear
   vision into the space between the lesion and adjacent tissues, which
   allows surgeons to make preoperative simulations and improve the
   accuracy and safety of actual surgery. (C) 2017 IJS Publishing Group
   Ltd. Published by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.ijsu.2017.01.079}},
ISSN = {{1743-9191}},
EISSN = {{1743-9159}},
Unique-ID = {{ISI:000397032500012}},
}

@article{ ISI:000395034500015,
Author = {Gibelli, Daniele and De Angelis, Danilo and Poppa, Pasquale and Sforza,
   Chiarella and Cattaneo, Cristina},
Title = {{An Assessment of How Facial Mimicry Can Change Facial Morphology:
   Implications for Identification}},
Journal = {{JOURNAL OF FORENSIC SCIENCES}},
Year = {{2017}},
Volume = {{62}},
Number = {{2}},
Pages = {{405-410}},
Month = {{MAR}},
Abstract = {{The assessment of facial mimicry is important in forensic anthropology;
   in addition, the application of modern 3D image acquisition systems may
   help for the analysis of facial surfaces. This study aimed at exposing a
   novel method for comparing 3D profiles in different facial expressions.
   Ten male adults, aged between 30 and 40 years, underwent acquisitions by
   stereophotogrammetry (VECTRA-3D (R)) with different expressions
   (neutral, happy, sad, angry, surprised). The acquisition of each
   individual was then superimposed on the neutral one according to nine
   landmarks, and the root mean square (RMS) value between the two
   expressions was calculated. The highest difference in comparison with
   the neutral standard was shown by the happy expression (RMS 4.11 mm),
   followed by the surprised (RMS 2.74 mm), sad (RMS 1.3 mm), and angry
   ones (RMS 1.21 mm). This pilot study shows that the 3D-3D
   superimposition may provide reliable results concerning facial
   alteration due to mimicry.}},
DOI = {{10.1111/1556-4029.13295}},
ISSN = {{0022-1198}},
EISSN = {{1556-4029}},
ORCID-Numbers = {{De Angelis, Danilo/0000-0001-6388-9415}},
Unique-ID = {{ISI:000395034500015}},
}

@article{ ISI:000395034500021,
Author = {Gibelli, Daniele and De Angelis, Danilo and Poppa, Pasquale and Sforza,
   Chiarella and Cattaneo, Cristina},
Title = {{A View to the Future: A Novel Approach for 3D-3D Superimposition and
   Quantification of Differences for Identification from Next-Generation
   Video Surveillance Systems}},
Journal = {{JOURNAL OF FORENSIC SCIENCES}},
Year = {{2017}},
Volume = {{62}},
Number = {{2}},
Pages = {{457-461}},
Month = {{MAR}},
Abstract = {{Techniques of 2D-3D superimposition are widely used in cases of personal
   identification from video surveillance systems. However, the progressive
   improvement of 3D image acquisition technology will enable operators to
   perform also 3D-3D facial superimposition. This study aims at analyzing
   the possible applications of 3D-3D superimposition to personal
   identification, although from a theoretical point of view. Twenty
   subjects underwent a facial 3D scan by stereophotogrammetry twice at
   different time periods. Scans were superimposed two by two according to
   nine landmarks, and root-mean-square (RMS) value of point-to-point
   distances was calculated. When the two superimposed models belonged to
   the same individual, RMS value was 2.10 mm, while it was 4.47 mm in
   mismatches with a statistically significant difference (p < 0.0001).
   This experiment shows the potential of 3D-3D superimposition: Further
   studies are needed to ascertain technical limits which may occur in
   practice and to improve methods useful in the forensic practice.}},
DOI = {{10.1111/1556-4029.13290}},
ISSN = {{0022-1198}},
EISSN = {{1556-4029}},
ResearcherID-Numbers = {{Sforza, Chiarella/C-3008-2015
   }},
ORCID-Numbers = {{Sforza, Chiarella/0000-0001-6532-6464
   De Angelis, Danilo/0000-0001-6388-9415}},
Unique-ID = {{ISI:000395034500021}},
}

@article{ ISI:000395521200012,
Author = {Chen, Jingdao and Fang, Yihai and Cho, Yong K. and Kim, Changwan},
Title = {{Principal Axes Descriptor for Automated Construction-Equipment
   Classification from Point Clouds}},
Journal = {{JOURNAL OF COMPUTING IN CIVIL ENGINEERING}},
Year = {{2017}},
Volume = {{31}},
Number = {{2}},
Month = {{MAR}},
Abstract = {{Recognizing construction assets (e.g.,materials, equipment, labor) from
   point cloud data of construction environments provides essential
   information for engineering and management applications including
   progress monitoring, safety management, supply-chain management, and
   quality control. This study introduces a novel principal axes descriptor
   (PAD) for construction-equipment classification from point cloud data.
   Scattered as-is point clouds are first processed with downsampling,
   segmentation, and clustering steps to obtain individual instances of
   construction equipment. A geometric descriptor consisting of dimensional
   variation, occupancy distribution, shape profile, and plane counting
   features is then calculated to encode three-dimensional (3D)
   characteristics of each equipment category. Using the derived features,
   machine learning methods such as k-nearest neighbors and support vector
   machine are employed to determine class membership among major
   construction-equipment categories such as backhoe loader, bulldozer,
   dump truck, excavator, and front loader. Construction-equipment
   classification with the proposed PAD was validated using computer-aided
   design (CAD)-generated point clouds as training data and laser-scanned
   point clouds from an equipment yard as testing data. The recognition
   performance was further evaluated using point clouds from a construction
   site as well as a pose variation data set. PAD was shown to achieve a
   higher recall rate and lower computation time compared to competing 3D
   descriptors. The results indicate that the proposed descriptor is a
   viable solution for construction-equipment classification from point
   cloud data. (C) 2016 American Society of Civil Engineers.}},
DOI = {{10.1061/(ASCE)CP.1943-5487.0000628}},
Article-Number = {{04016058}},
ISSN = {{0887-3801}},
EISSN = {{1943-5487}},
ORCID-Numbers = {{Fang, Yihai/0000-0002-9451-4947}},
Unique-ID = {{ISI:000395521200012}},
}

@article{ ISI:000397013700050,
Author = {Hu, Xingbo and Chen, Wei and Xu, Weiyang},
Title = {{Adaptive Mean Shift-Based Identification of Individual Trees Using
   Airborne LiDAR Data}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Identifying individual trees and delineating their canopy structures
   from the forest point cloud data acquired by an airborne LiDAR (Light
   Detection And Ranging) has significant implications in forestry
   inventory. Once accurately identified, tree structural attributes such
   as tree height, crown diameter, canopy based height and diameter at
   breast height can be derived. This paper focuses on a novel
   computationally efficient method to adaptively calibrate the kernel
   bandwidth of a computational scheme based on mean shift-a non-parametric
   probability density-based clustering technique-to segment the 3D
   (three-dimensional) forest point clouds and identify individual tree
   crowns. The basic concept of this method is to partition the 3D space
   over each test plot into small vertical units (irregular columns
   containing 3D spatial features from one or more trees) first, by using a
   fixed bandwidth mean shift procedure and a small square grouping
   technique, and then rough estimation of crown sizes for distinct trees
   within a unit, based on an original 2D (two-dimensional) incremental
   grid projection technique, is applied to provide a basis for dynamical
   calibration of the kernel bandwidth for an adaptive mean shift procedure
   performed in each partition. The adaptive mean shift-based scheme, which
   incorporates our proposed bandwidth calibration method, is validated on
   10 test plots of a dense, multi-layered evergreen broad-leaved forest
   located in South China. Experimental results reveal that this approach
   can work effectively and when compared to the conventional point-based
   approaches (e.g., region growing, k-means clustering, fixed bandwidth or
   multi-scale mean shift), its accuracies are relatively high: it detects
   86 percent of the trees ({''}recall{''}) and 92 percent of the
   identified trees are correct ({''}precision{''}), showing good potential
   for use in the area of forest inventory.}},
DOI = {{10.3390/rs9020148}},
Article-Number = {{148}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{xu, weiyang/0000-0002-8980-6005}},
Unique-ID = {{ISI:000397013700050}},
}

@article{ ISI:000397013700010,
Author = {Yu, Xiaowei and Hyyppa, Juha and Litkey, Paula and Kaartinen, Harri and
   Vastaranta, Mikko and Holopainen, Markus},
Title = {{Single-Sensor Solution to Tree Species Classification Using
   Multispectral Airborne Laser Scanning}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{This paper investigated the potential of multispectral airborne laser
   scanning (ALS) data for individual tree detection and tree species
   classification. The aim was to develop a single-sensor solution for
   forest mapping that is capable of providing species-specific
   information, required for forest management and planning purposes.
   Experiments were conducted using 1903 ground measured trees from 22
   sample plots and multispectral ALS data, acquired with an Optech Titan
   scanner over a boreal forest, mainly consisting of Scots pine (Pinus
   Sylvestris), Norway spruce (Picea Abies), and birch (Betula sp.), in
   southern Finland. ALS-features used as predictors for tree species were
   extracted from segmented tree objects and used in random forest
   classification. Different combinations of features, including point
   cloud features, and intensity features of single and multiple channels,
   were tested. Among the field-measured trees, 61.3\% were correctly
   detected. The best overall accuracy (OA) of tree species classification
   achieved for correctly-detected trees was 85.9\% (Kappa = 0.75), using a
   point cloud and single-channel intensity features combination, which was
   not significantly different from the ones that were obtained either
   using all features (OA = 85.6\%, Kappa = 0.75), or single-channel
   intensity features alone (OA = 85.4\%, Kappa = 0.75). Point cloud
   features alone achieved the lowest accuracy, with an OA of 76.0\%.
   Field-measured trees were also divided into four categories. An
   examination of the classification accuracy for four categories of trees
   showed that isolated and dominant trees can be detected with a detection
   rate of 91.9\%, and classified with a high overall accuracy of 90.5\%.
   The corresponding detection rate and accuracy were 81.5\% and 89.8\% for
   a group of trees, 26.4\% and 79.1\% for trees next to a larger tree, and
   7.2\% and 53.9\% for trees situated under a larger tree, respectively.
   The results suggest that Channel 2 (1064 nm) contains more information
   for separating pine, spruce, and birch, followed by channel 1 (1550 nm)
   and channel 3 (532 nm) with an overall accuracy of 81.9\%, 78.3\%, and
   69.1\%, respectively. Our results indicate that the use of multispectral
   ALS data has great potential to lead to a single-sensor solution for
   forest mapping.}},
DOI = {{10.3390/rs9020108}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Kaartinen, Harri/B-1474-2015
   Vastaranta, Mikko/K-9656-2018}},
ORCID-Numbers = {{Vastaranta, Mikko/0000-0001-6552-9122}},
Unique-ID = {{ISI:000397013700010}},
}

@article{ ISI:000394522400015,
Author = {Bobulski, J.},
Title = {{Multimodal face recognition method with two-dimensional hidden Markov
   model}},
Journal = {{BULLETIN OF THE POLISH ACADEMY OF SCIENCES-TECHNICAL SCIENCES}},
Year = {{2017}},
Volume = {{65}},
Number = {{1}},
Pages = {{121-128}},
Month = {{FEB}},
Abstract = {{The paper presents a new solution for the face recognition based on
   two-dimensional hidden Markov models. The traditional HMM uses
   one-dimensional data vectors, which is a drawback in the case of 2D and
   3D image processing, because part of the information is lost during the
   conversion to one-dimensional features vector. The paper presents a
   concept of the full ergodic 2DHMM, which can be used in 2D and 3D face
   recognition. The experimental results demonstrate that the system based
   on two dimensional hidden Markov models is able to achieve a good
   recognition rate for 2D, 3D and multimodal (2D+3D) face images
   recognition, and is faster than ICP method.}},
DOI = {{10.1515/bpasts-2017-0015}},
ISSN = {{0239-7528}},
EISSN = {{2300-1917}},
ResearcherID-Numbers = {{Bobulski, Janusz/C-4998-2014}},
ORCID-Numbers = {{Bobulski, Janusz/0000-0003-3345-604X}},
Unique-ID = {{ISI:000394522400015}},
}

@article{ ISI:000395844700002,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Title = {{Statistical non-rigid ICP algorithm and its application to 3D face
   alignment}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{58}},
Pages = {{3-12}},
Month = {{FEB}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means Of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region. (C) 2016 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.imavis.2016.10.007}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000395844700002}},
}

@article{ ISI:000391965900001,
Author = {Dinh-Cuong Hoang and Liang-Chia Chen and Thanh-Hung Nguyen},
Title = {{Sub-OBB based object recognition and localization algorithm using range
   images}},
Journal = {{MEASUREMENT SCIENCE AND TECHNOLOGY}},
Year = {{2017}},
Volume = {{28}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{This paper presents a novel approach to recognize and estimate pose of
   the 3D objects in cluttered range images. The key technical breakthrough
   of the developed approach can enable robust object recognition and
   localization under undesirable condition such as environmental
   illumination variation as well as optical occlusion to viewing the
   object partially. First, the acquired point clouds are segmented into
   individual object point clouds based on the developed 3D object
   segmentation for randomly stacked objects. Second, an efficient
   shape-matching algorithm called Sub-OBB based object recognition by
   using the proposed oriented bounding box (OBB) regional area-based
   descriptor is performed to reliably recognize the object. Then, the 3D
   position and orientation of the object can be roughly estimated by
   aligning the OBB of segmented object point cloud with OBB of matched
   point cloud in a database generated from CAD model and 3D virtual
   camera. To detect accurate pose of the object, the iterative closest
   point (ICP) algorithm is used to match the object model with the
   segmented point clouds. From the feasibility test of several scenarios,
   the developed approach is verified to be feasible for object pose
   recognition and localization.}},
DOI = {{10.1088/1361-6501/aa513a}},
Article-Number = {{025401}},
ISSN = {{0957-0233}},
EISSN = {{1361-6501}},
Unique-ID = {{ISI:000391965900001}},
}

@inproceedings{ ISI:000453217100010,
Author = {Balding, Steven and Davis, Darryl N.},
Editor = {{Gao, Y and Fallah, S and Jin, Y and Lekakou, C}},
Title = {{Combining Depth and Intensity Images to Produce Enhanced Object
   Detection for Use in a Robotic Colony}},
Booktitle = {{TOWARDS AUTONOMOUS ROBOTIC SYSTEMS (TAROS 2017)}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10454}},
Pages = {{115-125}},
Note = {{18th Annual Conference on Towards Autonomous Robotics (TAROS), Univ
   Surrey, Guildford, ENGLAND, JUL 19-21, 2017}},
Organization = {{Airbus Defence \& Space; Chinese Acad Sci; IET; Springer; UK Robot
   Autonomous Syst Network}},
Abstract = {{Robotic colonies that can communicate with each other and interact with
   their ambient environments can be utilized for a wide range of research
   and industrial applications. However amongst the problems that these
   colonies face is that of the isolating objects within an environment.
   Robotic colonies that can isolate objects within the environment can not
   only map that environment in detail, but interact with that ambient
   space. Many object recognition techniques exist, however these are often
   complex and computationally expensive, leading to overly complex
   implementations. In this paper a simple model is proposed to isolate
   objects, these can then be recognize and tagged. The model will be using
   2D and 3D perspectives of the perceptual data to produce a probability
   map of the outline of an object, therefore addressing the defects that
   exist with 2D and 3D image techniques. Some of the defects that will be
   addressed are; low level illumination and objects at similar depths.
   These issues may not be completely solved, however, the model provided
   will provide results confident enough for use in a robotic colony.}},
DOI = {{10.1007/978-3-319-64107-2\_10}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-64107-2; 978-3-319-64106-5}},
ORCID-Numbers = {{Davis, Darryl/0000-0001-5236-6141}},
Unique-ID = {{ISI:000453217100010}},
}

@inproceedings{ ISI:000425498402048,
Author = {Liu, Liu and Li, Hongdong and Dai, Yuchao},
Book-Group-Author = {{IEEE}},
Title = {{Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale
   3D Map}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)}},
Series = {{IEEE International Conference on Computer Vision}},
Year = {{2017}},
Pages = {{2391-2400}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{Given an image of a street scene in a city, this paper develops a new
   method that can quickly and precisely pinpoint at which location (as
   well as viewing direction) the image was taken, against a pre-stored
   large-scale 3D point-cloud map of the city. We adopt the recently
   developed 2D-3D direct feature matching framework for this task
   {[}23,31,32,42-44]. This is a challenging task especially for
   large-scale problems. As the map size grows bigger, many 3D points in
   the wider geographical area can be visually very similar-or even
   identical-causing severe ambiguities in 2D-3D feature matching. The key
   is to quickly and unambiguously find the correct matches between a query
   image and the large 3D map. Existing methods solve this problem mainly
   via comparing individual features' visual similarities in a local and
   per feature manner, thus only local solutions can be found, inadequate
   for large-scale applications.
   In this paper, we introduce a global method which harnesses global
   contextual information exhibited both within the query image and among
   all the 3D points in the map. This is achieved by a novel global ranking
   algorithm, applied to a Markov network built upon the 3D map, which
   takes account of not only visual similarities between individual 2D-3D
   matches, but also their global compatibilities (as measured by
   co-visibility) among all matching pairs found in the scene. Tests on
   standard benchmark datasets show that our method achieved both higher
   precision and comparable recall, compared with the state-of-the-art.}},
DOI = {{10.1109/ICCV.2017.260}},
ISSN = {{1550-5499}},
ISBN = {{978-1-5386-1032-9}},
ORCID-Numbers = {{Liu, Liu/0000-0002-5880-5974}},
Unique-ID = {{ISI:000425498402048}},
}

@article{ ISI:000397995100002,
Author = {Ahmed, Oumer S. and Shemrock, Adam and Chabot, Dominique and Dillon,
   Chris and Williams, Griffin and Wasson, Rachel and Franklin, Steven E.},
Title = {{Hierarchical land cover and vegetation classification using
   multispectral data acquired from an unmanned aerial vehicle}},
Journal = {{INTERNATIONAL JOURNAL OF REMOTE SENSING}},
Year = {{2017}},
Volume = {{38}},
Number = {{8-10}},
Pages = {{2037-2052}},
Note = {{Conference on Small Unmanned Aerial Systems (sUAS) for Environmental
   Research, Univ Worcester, Worcester, ENGLAND, JUN, 2016}},
Abstract = {{The use of multispectral cameras deployed on unmanned aerial vehicles
   (UAVs) in land cover and vegetation mapping applications continues to
   improve and receive increasing recognition and adoption by resource
   management and forest survey practitioners. Comparisons of different
   camera data and platform performance characteristics are an important
   contribution in understanding the role and operational capability of
   this technology. In this article, object-based classification accuracies
   for different cover types and vegetation species of interest in central
   Ontario were examined using data from three UAV-based multispectral
   cameras. Five land-cover classes (forest, shrub, herbaceous, bare soil,
   and built-up) were determined to be up to 95\% correct overall with
   calibrated multispectral Parrot Sequoia digital camera data compared to
   independent field observations. The levels of classification accuracy
   decreased approximately 10-15\% when spectrally less capable
   consumer-grade RGB sensors were used. Multispectral Parrot Sequoia
   classification accuracy was approximately 89\% when more detailed
   vegetation classes, including individual deciduous tree species, shrub
   communities and agricultural crops, were analysed. Additional work is
   suggested in the use of such UAV multispectral and point cloud data in
   ash tree discrimination to support emerald ash borer infestation
   detection and management, and in analysis of functional and structural
   vegetation characteristics (e.g. leaf area index).}},
DOI = {{10.1080/01431161.2017.1294781}},
ISSN = {{0143-1161}},
EISSN = {{1366-5901}},
Unique-ID = {{ISI:000397995100002}},
}

@inproceedings{ ISI:000446968900006,
Author = {Rihani, Amal and Jribi, Majdi and Ghorbel, Faouzi},
Editor = {{BenAmor, B and Chaieb, F and Ghorbel, F}},
Title = {{Enhancing 3D Face Recognition by a Robust Version of ICP Based on the
   Three Polar Representation}},
Booktitle = {{REPRESENTATIONS, ANALYSIS AND RECOGNITION OF SHAPE AND MOTION FROM
   IMAGING DATA}},
Series = {{Communications in Computer and Information Science}},
Year = {{2017}},
Volume = {{684}},
Pages = {{65-74}},
Note = {{6th International Workshop on Representations, Analysis and Recognition
   of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
   OCT 27-29, 2016}},
Abstract = {{In this paper, we intend to propose a framework for the description and
   the matching of three dimensional faces. Our starting point is the
   representation of the 3D face by an invariant description under the M(3)
   group of translations and rotations. This representation is materialized
   by the points of the arc-length reparametrization of all the level
   curves of the three polar representation. These points are indexed by
   their level curve number and their position in each level. With this
   type of description we need a step of registration to align 3D faces
   with different expressions. Therefore, we propose to use a robust
   version of the iterative closest point algorithm (ICP) adopted to 3D
   face recognition context. We test the accuracy of our approach on a part
   of the BU-3DFE database of 3D faces. The obtained results for many
   protocols of the identification scenario show the performance of such
   framework.}},
DOI = {{10.1007/978-3-319-60654-5\_6}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-319-60654-5; 978-3-319-60653-8}},
Unique-ID = {{ISI:000446968900006}},
}

@inproceedings{ ISI:000438669700055,
Author = {Secanj, Marin and Arbanas, Snjezana Mihalic and Kordic, Branko and
   Krkac, Martin and Gazibara, Sanja Bernat},
Editor = {{Mikos, M and Vilimek, V and Yin, Y and Sassa, K}},
Title = {{Identification of Rock Fall Prone Areas on the Steep Slopes Above the
   Town of Omis, Croatia}},
Booktitle = {{ADVANCING CULTURE OF LIVING WITH LANDSLIDES, VOL 5: LANDSLIDES IN
   DIFFERENT ENVIRONMENTS}},
Year = {{2017}},
Pages = {{481-487}},
Note = {{4th World Landslide Forum, Ljubljana, SLOVENIA, MAY 29-JUN 02, 2017}},
Organization = {{Int Consortium Landslides; Int Programme Landslides, Global Promot Comm;
   Geol Survey Slovenia Ljubljana; Univ Ljubljana; Republ Slovenia, Minist
   Environm \& Spatial Planning; Republic Slovenia, Minist Infrastructure;
   Slovenian Natl Platform Disaster Risk Reduct; Int Programme Landslides;
   Slovenian Chamber Engineers; Int Assoc Hydrogeologists Slovene Comm;
   Water Management Soc Slovenia; Geomorphol Assoc Slovenia; Inst Water
   Republ Slovenia; Slovenian Geol Soc; Slovenian Geotechn Soc; IHP UNESCO,
   Slovenian Natl Comm; Slovenian Assoc Geodesy \& Geophys}},
Abstract = {{The aim of this paper was identification of rock fall prone areas above
   the historical town of Omis, located at the Adriatic coast in Croatia.
   Unstable areas were identified by kinematic analysis performed based on
   relative orientations of discontinuities and slope face. Input data was
   extracted from the surface model created from the high-resolution point
   cloud. The town of Omis is threatened by rock falls, because of its
   specific location just at the toe of Mt. Omiska Dinara. Rock fall risk
   is even higher due to rich cultural and historical heritage of the town.
   Collection of spatial data was performed by Time of Flight and
   phase-shift terrestrial laser scanners in order to derivate high
   resolution point cloud necessary for derivation of surface model.
   Split-FX software was used to extract discontinuity surfaces were
   semi-automatically from the point cloud data. Spatial kinematic analysis
   was performed for each triangle of TIN surface model of the investigated
   slopes to identify locations of possible instability mechanism. From the
   results of the spatial kinematic analysis, the most critical parts of
   the slope have identified for planar and wedge failure and flexural and
   block toppling. Verification of identified rock fall areas was performed
   by visual inspection of hazardous blocks at the surface model.
   Identified rock fall prone areas, unstable blocks and probable
   instability mechanisms on the steep slopes above the town Omis, present
   the input data for risk reduction by efficient design of
   countermeasures.}},
DOI = {{10.1007/978-3-319-53483-1\_57}},
ISBN = {{978-3-319-53483-1; 978-3-319-53482-4}},
ORCID-Numbers = {{Secanj, Marin/0000-0002-9818-9731}},
Unique-ID = {{ISI:000438669700055}},
}

@inproceedings{ ISI:000434278900149,
Author = {Swetha, K. M. and Suja, P.},
Editor = {{Niranjan, SK and Manvi, SS and Kodabagi, MM and Hulipalled, VR}},
Title = {{A Geometric Approach for Recognizing Emotions From 3D Images with Pose
   Variations}},
Booktitle = {{PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES
   FOR SMART NATION (SMARTTECHCON)}},
Year = {{2017}},
Pages = {{805-809}},
Note = {{International Conference On Smart Technologies For Smart Nation
   (SmartTechCon), REVA Univ, Bengaluru, INDIA, AUG 17-19, 2017}},
Organization = {{IEEE; IEEE Bangalore Sect; IEEE Computat Intelligence Soc, Bangalore
   Chapter; CSIR}},
Abstract = {{Emotions are an incredibly important aspect of human life. Research on
   emotion recognition for the past few decades have resulted in
   development of several fields. In the current scenario, it is necessary
   that machines/robots need to identify human emotions and respond
   accordingly. Applications in this field can be seen in security,
   entertainment and Human Machine Interface/Human Robot Interface. Recent
   works on 3D images have gained importance due to its accuracy in real
   life applications as emotions can be recognised at different head poses.
   The intention of this work has been to develop an algorithm for
   recognition of emotion from facial expressions, which recognizes 6 basic
   emotions, which are anger, fear, happy, disgust, sad and surprise from
   3D images in 7 yaw angles (+45 degrees to -45 degrees) and 3 pitch
   angles (+15 degrees, 0 degrees, -15 degrees). Most of the reported work
   considers + yaw angles. While in the current work, both positive as well
   as negative pitch and yaw angles are considered. BU3DFE database is used
   for the implementation. The proposed method resulted in improved
   accuracy and is comparable with the literature.}},
ISBN = {{978-1-5386-0569-1}},
Unique-ID = {{ISI:000434278900149}},
}

@inproceedings{ ISI:000432373000248,
Author = {Varga, Robert and Costea, Arthur and Florea, Horatiu and Giosan, Ion and
   Nedevschi, Sergiu},
Book-Group-Author = {{IEEE}},
Title = {{Super-sensor for 360-degree Environment Perception: Point Cloud
   Segmentation Using Image Features}},
Booktitle = {{2017 IEEE 20TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION
   SYSTEMS (ITSC)}},
Series = {{IEEE International Conference on Intelligent Transportation Systems-ITSC}},
Year = {{2017}},
Note = {{20th IEEE International Conference on Intelligent Transportation Systems
   (ITSC), Yokohama, JAPAN, OCT 16-19, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper describes a super-sensor that enables 360-degree environment
   perception for automated vehicles in urban traffic scenarios. We use
   four fisheye cameras, four 360 degree LIDARs and a GPS/IMU sensor
   mounted on an automated vehicle to build a super-sensor that offers an
   enhanced low-level representation of the environment by harmonizing all
   the available sensor measurements. Individual sensors cannot provide a
   robust 360-degree perception due to their limitations: field of view,
   range, orientation, number of scanning rays, etc. The novelty of this
   work consists of segmenting the 3D LIDAR point cloud by associating it
   with the 2D image semantic segmentation. Another contribution is the
   sensor configuration that enables 360-degree environment perception. The
   following steps are involved in the process: calibration, timestamp
   synchronization, fisheye image unwarping, motion correction of LIDAR
   points, point cloud projection onto the images and semantic segmentation
   of images. The enhanced low-level representation will improve the
   high-level perception environment tasks such as object detection,
   classification and tracking.}},
ISSN = {{2153-0009}},
ISBN = {{978-1-5386-1526-3}},
Unique-ID = {{ISI:000432373000248}},
}

@inproceedings{ ISI:000432372100099,
Author = {Torkhani, Ghada and Ladgham, Anis and Sakly, Anis},
Book-Group-Author = {{IEEE}},
Title = {{3D Gabor-Edge Filters Applied to Face Depth Images}},
Booktitle = {{2017 18TH INTERNATIONAL CONFERENCE ON SCIENCES AND TECHNIQUES OF
   AUTOMATIC CONTROL AND COMPUTER ENGINEERING (STA)}},
Series = {{International Conference on Sciences and Techniques of Automatic Control
   and Computer Engineering}},
Year = {{2017}},
Pages = {{578-582}},
Note = {{18th International Conference on Sciences and Techniques of Automatic
   Control and Computer Engineering (STA), Monastir, TUNISIA, DEC 21-23,
   2017}},
Organization = {{IEEE Tunisia Sect; Tunisian Assoc Numer Tech \& Automat; Univ Sfax, Natl
   Engn Sch Sfax, Lab Sci \& Tech Automat Control \& Comp Engn; Tunisia
   Sect Control Syst Soc Chapter; Tunisia Sect Robot \& Automat Soc
   Chapter; Tunisia Sect Signal Proc Soc Chapter; Tunisia Sect Circuits \&
   Syst Soc Chapter; Tunisia Sect Solid State Circuits Soc Chapter}},
Abstract = {{This manuscript introduces a novel 3D face authentication system
   inspired from the advantageous capacities of Gabor-Edge filters. The
   approach studies 3D face difficulties such as expression variety,
   different rotations and exposure to illuminations. The proposed systems
   starts by preprocessing the 3D face images to resolve acquisition
   problems. Then, a filtering process is performed by implanting our 3D
   Gabor-Edge technique extended based on the classic 3D Gabor masks. The
   next step is to achieve the classification of facial features from the
   edge saliency by the artificial Neural Network Classifier (NNC). The
   evaluation of the adopted system is achieved by exporting common
   datasets from GavabDB database. Experimental results are reported to
   prove the high accuracy rates of our method compared to the recent
   researches in the same biometric field.}},
ISSN = {{2378-7163}},
ISBN = {{978-1-5386-1084-8}},
Unique-ID = {{ISI:000432372100099}},
}

@inproceedings{ ISI:000428907900036,
Author = {Zhao, Minghua and Mo, Ruiyang and Zhao, Yonggang and Shi, Zhenghao and
   Zhang, Feifei},
Editor = {{Li, G and Ge, Y and Zhang, Z and Jin, Z and Blumenstein, M}},
Title = {{An Efficient Three-Dimensional Reconstruction Approach for
   Pose-Invariant Face Recognition Based on a Single View}},
Booktitle = {{KNOWLEDGE SCIENCE, ENGINEERING AND MANAGEMENT (KSEM 2017): 10TH
   INTERNATIONAL CONFERENCE, KSEM 2017, MELBOURNE, VIC, AUSTRALIA, AUGUST
   19-20, 2017, PROCEEDINGS}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2017}},
Volume = {{10412}},
Pages = {{422-431}},
Note = {{10th International Conference on Knowledge Science, Engineering and
   Management (KSEM), Melbourne, AUSTRALIA, AUG 19-20, 2017}},
Abstract = {{A three-dimensional (3D) reconstruction approach based on a single view
   is proposed to solve the problem of lack of training samples while
   addressing multi-pose face recognition. First, a planar template is
   defined based on the geometric information of the segmented faces.
   Second, 3D faces are resampled according to the geometric relationship
   between the planar template and original 3D faces, and a normalized 3D
   face database is obtained. Third, a 3D sparse morphable model is
   established based on the normalized 3D face database, and a new 3D face
   can be reconstructed from a single face image. Lastly, virtual
   multi-pose face images can be obtained by texture mapping, rotation, and
   projection of the established 3D face, and training samples are
   enriched. Experimental results obtained using BJUT-3D and CAS-PEAL-R1
   face databases show that recognition rate of the proposed method is
   91\%, which is better than other methods for pose-invariant face
   recognition based on a single view. This is primarily because the
   training samples are enriched using the proposed 3D sparse morphable
   model based on a new dense correspondence method.}},
DOI = {{10.1007/978-3-319-63558-3\_36}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-63558-3; 978-3-319-63557-6}},
Unique-ID = {{ISI:000428907900036}},
}

@inproceedings{ ISI:000427598702135,
Author = {Mohsin, Nasreen and Payandeh, Shahram},
Book-Group-Author = {{IEEE}},
Title = {{Localization and Identification of Body Extremities Based on Data from
   Multiple Depth Sensors}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)}},
Series = {{IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings}},
Year = {{2017}},
Pages = {{2736-2741}},
Note = {{IEEE International Conference on Systems, Man, and Cybernetics (SMC),
   Banff, CANADA, OCT 05-08, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper explores the novel use of multiple depth sensors to overcome
   occlusions and improve localization and tracking of body extremities.
   The usage of data from only depth sensors not only overcomes visual
   challenges associated with RGB sensors under low illumination, but also
   protects the identity of surveyed person with high confidentiality. For
   integrating depth information from multiple sources, the paper presents
   first an overview of a novel calibration method for multiple depth
   sensors. In case of occlusion of any fiducial point in the primary
   sensor's depth image, co-ordinates of the point can be obtained from the
   frame of other sensors using the calibration parameters. To localize
   salient body parts such as hands, head and feet, a surface triangular
   mesh is applied on generated 3D point cloud from the primary sensor. The
   geodesic extrema from the mesh coincide with body extremities. The body
   extremities can be identified based on those relative geodesic distances
   between the extremities. Once the body parts are labelled, a portion of
   body can be targeted and evaluated for specific gait analysis and
   visualization. For the performance evaluation, our calibration method
   has fared well in comparison to other available techniques. Also, our
   proposed localization of salient body parts is able to successfully tag
   the specific body part i.e. the head region.}},
ISSN = {{1062-922X}},
ISBN = {{978-1-5386-1645-1}},
Unique-ID = {{ISI:000427598702135}},
}

@inproceedings{ ISI:000427635800103,
Author = {Warke, K. S. and Suralkar, Rupali and Pawar, Snehal and Sonawane, Priya
   and Wani, Shraddha},
Book-Group-Author = {{IEEE}},
Title = {{A Real Sense based multilevel security in cloud framework using Face
   recognition and Image processing}},
Booktitle = {{2017 2ND INTERNATIONAL CONFERENCE FOR CONVERGENCE IN TECHNOLOGY (I2CT)}},
Year = {{2017}},
Pages = {{531-533}},
Note = {{2nd International Conference for Convergence in Technology (I2CT),
   Siddhant Coll Engn, Pune, INDIA, APR 07-09, 2017}},
Organization = {{IEEE; Sahyadri Valley Coll Engn \& Technol; Asian Soc Sci Res; SVCET,
   Jha Sci Res Pvt Ltd}},
Abstract = {{A 3D image can be used for the authentications as it gives more
   accuracy. We can also use the gestures for the authentication purpose.
   We can add multiple authentication levels together to make system more
   secure and login process more reliable. In our system we are going to
   provide three levels of authentication i.e. 1) Text Password: -This is
   first level in which user has to enter the text password which is OTP.
   OTP will be sent to the registered email-id or at the mobile number
   given. With
   The help of AES (advance encryption standard) algorithm the data will be
   encrypt and store at database server.2) Hand Gesture Recognition: -This
   is second level in which user has to place this hand in front of camera.
   So camera can detect it,select one point out of 22 points on the hand
   whatever pattern user make in front of camera is saved as password at
   user's database.
   3) Face Recognition: -This is last level in which user has to place face
   in front of camera. So camera can detect the face and select 78 landmark
   points. And those points are saved at the user's database. After passing
   all these stages user is authenticate and can upload or download the
   documents of his/her choice.}},
ISBN = {{978-1-5090-4307-1}},
Unique-ID = {{ISI:000427635800103}},
}

@inproceedings{ ISI:000427083300058,
Author = {Jazouli, Maha and Majda, Aicha and Zarghili, Arsalane},
Editor = {{Nfaoui, E and Boumhidi, J and Sabri, A and Yahyaouy, A and Bouchaffra, D}},
Title = {{A \$P Recognizer for Automatic Facial Emotion Recognition using Kinect
   Sensor}},
Booktitle = {{2017 INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV)}},
Year = {{2017}},
Note = {{Intelligent Systems and Computer Vision (ISCV), Fez, MOROCCO, APR 17-19,
   2017}},
Organization = {{IEEE; Univ Sidi Mohamed ben Abdellah; IEEE Comp Soc; IEEE Morocco Sect}},
Abstract = {{Autism is a developmental disorder involving qualitative impairments in
   social interaction. One source of those impairments are difficulties
   with facial expressions of emotion. Autistic people often have
   difficulty to recognize or to understand other people's emotions and
   feelings, or expressing their own. This work proposes a method to
   automatically recognize seven basic emotions among autistic children in
   real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and
   Neutral. The method uses the Microsoft Kinect sensor to track and
   identify points of interest from the 3D face model and it is based on
   the \$P point-cloud recognizer to identify multi-stroke emotions as
   point-clouds. The experimental results show that our system can achieve
   above 94.28\% recognition rate. Our study provides a novel clinical tool
   to help children with autism to assisting doctors in operating rooms.}},
ISBN = {{978-1-5090-4062-9}},
Unique-ID = {{ISI:000427083300058}},
}

@inproceedings{ ISI:000427293600008,
Author = {Frikha, Tarek and Chaabane, Faten and Said, Boukhchim and Drira, Hassen
   and Abid, Mohamed and Ben Amar, Chokri},
Editor = {{ElHassouni, M and Karim, M and BenHamida, A and BenSlima, A and Solaiman, B}},
Title = {{Embedded approach for a Riemannian-based framework of analyzing 3D faces}},
Booktitle = {{2017 3RD INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)}},
Year = {{2017}},
Pages = {{43-47}},
Note = {{3rd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP), Fez, MOROCCO, MAY 22-24, 2017}},
Organization = {{Univ Sidi Mohamed Ben Abdellah; Fac Sci; Fac Med \& Pharm; CNRST; TICSM;
   IEEE Morocco Sect; IEEE Signal Proc Soc Morocco Chapter}},
Abstract = {{Developing multimedia embedded applications continues to flourish. In
   fact, a biometric facial recognition system can be used not only on PCs
   abut also in embedded systems, it is a potential enhancer to meet
   security and surveillance needs. The analysis of facial recognition
   consists offoursteps: face analysis, face expressions' recognition,
   missing data completion and full face recognition.
   This paper proposes a hardware architecture based on an adaptation
   approach foran algorithm which has proven good face detection and
   recognition in 3D space. The proposed application was tested using a co
   design technique based on a mixed Hardware Software architecture: the
   FPGA platform.}},
ISBN = {{978-1-5386-0551-6}},
Unique-ID = {{ISI:000427293600008}},
}

@inproceedings{ ISI:000426973200029,
Author = {Li, Huibin and Sun, Jian and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Location-Sensitive Sparse Representation of Deep Normal Patterns for
   Expression-robust 3D Face Recognition}},
Booktitle = {{2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)}},
Year = {{2017}},
Pages = {{234-242}},
Note = {{IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
   OCT 01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper presents a straight-forward yet efficient, and
   expression-robust 3D face recognition approach by exploring location
   sensitive sparse representation of deep normal patterns (DNP). In
   particular given raw 3D facial surfaces, we first run 3D face
   pre-processing pipeline, including nose tip detection, face region
   cropping, and pose normalization. The 3D coordinates of each normalized
   3D facial surface are then projected into 2D plane to generate geometry
   images, from which three images of facial surface normal components are
   estimated. Each normal image is then fed into a pre-trained deep face
   net to generate deep representations of facial surface normals, i.e.,
   deep normal patterns. Considering the importance of different facial
   locations, we propose a location sensitive sparse representation
   classifier (LS-SRC) for similarity measure among deep normal patterns
   associated with different 3D faces. Finally, simple score-level fusion
   of different normal components are used for the final decision. The
   proposed approach achieves significantly high performance, and reporting
   rank-one scores of 98.01\%, 97.60\%, and 96.13\% on the FRGC v2.0,
   Bosphorus, and BU-3DFE databases when only one sample per subject is
   used in the gallery. These experimental results reveals that the
   performance of 3D face recognition would be constantly improved with the
   aid of training deep models from massive 2D face images, which opens the
   door for future directions of 3D face recognition.}},
ISBN = {{978-1-5386-1124-1}},
Unique-ID = {{ISI:000426973200029}},
}

@inproceedings{ ISI:000426973200042,
Author = {Liu, Feng and Hu, Jun and Sun, Jianwei and Wang, Yang and Zhao, Qijun},
Book-Group-Author = {{IEEE}},
Title = {{Multi-Dim: A Multi-Dimensional Face Database Towards the Application of
   3D Technology in Real-World Scenarios}},
Booktitle = {{2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)}},
Year = {{2017}},
Pages = {{342-351}},
Note = {{IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
   OCT 01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{Three-dimensional (3D) faces are increasingly utilized in many
   face-related tasks. Despite the promising improvement achieved by 3D
   face technology, it is still hard to thoroughly evaluate the performance
   and effect of 3D face technology in real-world applications where
   variations frequently occur in pose, illumination, expression and many
   other factors. This is due to the lack of benchmark databases that
   contain both high precision full-view 3D faces and their 2D face
   images/videos under different conditions. In this paper, we present such
   a multi-dimensional face database (namely Multi-Dim) of high precision
   3D face scans, high definition photos, 2D stillface images with varying
   pose and expression, low quality 2D surveillance video clips, along with
   ground truth annotations for them. Based on this Multi-Dim face
   database, extensive evaluation experiments have been done with
   state-of-the-art baseline methods for constructing 3D morphable model,
   reconstructing 3D faces from single images, 3D-assisted pose
   normalization for face verification, and 3D-rendered multiview gallery
   for face identification. Our results show that 3D face technology does
   help in improving unconstrained 2D face recognition when the probe 2D
   face images are of reasonable quality, whereas it deteriorates rather
   than improves the face recognition accuracy when the probe 2D face
   images are of poor quality. We will make Multi-Dim freely available to
   the community for the purpose of advancing the 3D-based unconstrained 2D
   face recognition and related techniques towards real-world applications.}},
ISBN = {{978-1-5386-1124-1}},
Unique-ID = {{ISI:000426973200042}},
}

@inproceedings{ ISI:000426886000048,
Author = {Lopez, G. and Pallas, B. and Martinez, S. and Lauri, P. E. and Regnard,
   J. L. and Durel, C. E. and Costes, E.},
Editor = {{Marsal, J and Girona, J}},
Title = {{High-throughput phenotyping of an apple core collection: identification
   of genotypes with high water use efficiency}},
Booktitle = {{VIII INTERNATIONAL SYMPOSIUM ON IRRIGATION OF HORTICULTURAL CROPS}},
Series = {{Acta Horticulturae}},
Year = {{2017}},
Volume = {{1150}},
Pages = {{335-340}},
Note = {{8th International Symposium on Irrigation of Horticultural Crops,
   Lleida, SPAIN, JUN 08-11, 2015}},
Organization = {{Int Soc Horticultural Sci}},
Abstract = {{To detect genotypes with high water use efficiency (WUE) in apple (Malus
   x domestica), 193 genotypes from an INRA core collection were evaluated
   in 2014. Eight grafted replicates per genotype grown as one-year-old
   scions were studied in a high-throughput phenotyping platform
   (PhenoArch). Individual pot weight was recorded twice a day and
   irrigation was scheduled for 46 days according to two irrigation
   treatments: well-watered (WW), maintaining soil water content (SWC) at
   1.4 g g(-1); and water stress (WS), reducing SWC until 0.7 g g(-1) and
   maintaining this value for ten days. For each genotype, half of the
   replicates were WW while the other half were grown under WS. Plant 3D
   images were automatically acquired every two days. Analysis of images
   and pot weight differences allowed the estimation of the accumulated
   whole-plant biomass (A\_Bio) and transpiration (Plant\_T) during the
   experiment. WUE was calculated as the ratio A\_Bio/Plant\_T. A\_Bio and
   WUE had a higher genetic variation than Plant\_T under WW and WS
   conditions. The genetic variation in WUE is a promising result,
   indicating that available genetic resources such as the INRA core
   collection could be useful to improve apple plant material for the use
   of water. WS reduced A\_Bio and Plant\_T but the reduction was less
   evident in WUE. Some genotypes had similar WUE values under WW and WS
   conditions. We identified of a group of 38 genotypes with high WUE under
   WW and WS. The existence of genotypes with high WUE whatever the water
   regime in apple may encourage apple breeders to consider the use of
   these genotypes as potential parents for improving apple plant material
   for the use of water.}},
DOI = {{10.17660/ActaHortic.2017.1150.48}},
ISSN = {{0567-7572}},
EISSN = {{2406-6168}},
ISBN = {{978-94-62611-45-0}},
Unique-ID = {{ISI:000426886000048}},
}

@inproceedings{ ISI:000425931000007,
Author = {Asl, Azin Shabani and Oskoei, Mohammadreza Asghari},
Book-Group-Author = {{IEEE}},
Title = {{Depth Dependent Invariant Features Applied to Person Detection Using 3D
   Camera}},
Booktitle = {{2017 5TH IRANIAN JOINT CONGRESS ON FUZZY AND INTELLIGENT SYSTEMS (CFIS)}},
Year = {{2017}},
Pages = {{29-34}},
Note = {{5th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS),
   Qazvin Islam Azad Univ, Tehran, IRAN, MAR 07-09, 2017}},
Abstract = {{This paper is about detection and tracking a person by mobile robots in
   in-door environments, such as shopping center and hospital. It uses
   vision based approaches to recognize texture of clothes. The paper
   proposes a method to use depth (distance) reference along with scale
   invariant features (SIFT) to recognize patterns in various orientation,
   distance and illumination. SIFT is an important feature detection
   algorithm that is robust against rotation, translation, and scaling in
   2D images and to some extent against variations in lighting conditions.
   But it suffers inadequate performance for visual patterns rotated in 3D
   space. To overcome this issue, reference inputs given to the algorithm
   was extended to include images taken from different angles. The proposed
   algorithm showed considerably improved performance in detection for
   real-time applications.}},
ISBN = {{978-1-5090-4008-7}},
Unique-ID = {{ISI:000425931000007}},
}

@inproceedings{ ISI:000426676200113,
Author = {Si, Boyu and Huang, Zhaoming and Bai, Baodan},
Editor = {{ElFergany, A and Rojas, AL and Szeto, WY}},
Title = {{A Training System for Speech Disordered Children Based on the Intel
   RealSense Technology}},
Booktitle = {{PROCEEDINGS OF THE 2017 2ND INTERNATIONAL CONFERENCE ON CONTROL,
   AUTOMATION AND ARTIFICIAL INTELLIGENCE (CAAI 2017)}},
Series = {{Advances in Intelligent Systems Research}},
Year = {{2017}},
Volume = {{134}},
Pages = {{504-507}},
Note = {{2nd International Conference on Control, Automation and Artificial
   Intelligence (CAAI), Sanya, PEOPLES R CHINA, JUN 25-26, 2017}},
Organization = {{Sci \& Engn Res Ctr}},
Abstract = {{A training system for speech disordered children is presented in this
   research. The core technology includes face tracking and speech
   recognition, which are supplied by Intel RealSense SDK and its relative
   hardware, such as 3D camera F200. The system consists of the pronouncing
   learning module and the speech disorder training module. The former can
   help children patient with almost no speech ability learn the
   pronouncing motion and method in Mandarin initials and finals with
   real-time 3D image playback. The training module which is based on the
   cartoon games can intervene patients' abnormal voice onset, speech
   volume, speech tone and vowel confusion. With the help of the speech
   recognition function from Intel RealSense, it can make an immersed
   environment which is benefit for training.}},
ISSN = {{1951-6851}},
ISBN = {{978-94-6252-360-9}},
Unique-ID = {{ISI:000426676200113}},
}

@inproceedings{ ISI:000426279000124,
Author = {Wang, Haoyu and Yang, Fumeng and Zhang, Yuming and Wu, Congzhong},
Editor = {{Lv, D and Lv, Y and Bao, W}},
Title = {{3D face analysis by using Mesh-LBP feature}},
Booktitle = {{LIDAR IMAGING DETECTION AND TARGET RECOGNITION 2017}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10605}},
Number = {{1}},
Note = {{Conference on LIDAR Imaging Detection and Target Recognition, Changchun,
   PEOPLES R CHINA, JUL 23-25, 2017}},
Organization = {{Chinese Soc Opt Engn; Chinese Soc Astronaut, Photoelectron Technol Comm;
   Chinese Acad Engn; Natl Nat Sci Fdn China; Chinese Acad Sci,
   Photoelectron Technol Comm}},
Abstract = {{Face Recognition is one of the widely application of image processing.
   Corresponding two-dimensional limitations, such as the pose and
   illumination changes, to a certain extent restricted its accurate rate
   and further development. How to overcome the pose and illumination
   changes and the effects of self-occlusion is the research hotspot and
   difficulty, also attracting more and more domestic and foreign experts
   and scholars to study it. 3D face recognition fusing shape and texture
   descriptors has become a very promising research direction. Method: Our
   paper presents a 3D point cloud based on mesh local binary pattern grid
   (Mesh-LBP), then feature extraction for 3D face recognition by fusing
   shape and texture descriptors. 3D Mesh-LBP not only retains the
   integrity of the 3D geometry, is also reduces the need for recognition
   process of normalization steps, because the triangle Mesh-LBP descriptor
   is calculated on 3D grid. On the other hand, in view of multi-modal
   consistency in face recognition advantage, construction of LBP can
   fusing shape and texture information on Triangular Mesh. In this paper,
   some of the operators used to extract Mesh-LBP, Such as the normal
   vectors of the triangle each face and vertex, the gaussian curvature,
   the mean curvature, laplace operator and so on. Conclusion: First,
   Kinect devices obtain 3D point cloud face, after the pretreatment and
   normalization, then transform it into triangular grid, grid local binary
   pattern feature extraction from face key significant parts of face. For
   each local face, calculate its Mesh-LBP feature with Gaussian curvature,
   mean curvature laplace operator and so on. Experiments on the our
   research database, change the method is robust and high recognition
   accuracy.}},
DOI = {{10.1117/12.2295797}},
Article-Number = {{UNSP 106053O}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1707-0; 978-1-5106-1706-3}},
Unique-ID = {{ISI:000426279000124}},
}

@inproceedings{ ISI:000425238900023,
Author = {Kaur, Rajwant and Sharma, Dolly and Verma, Amit},
Editor = {{Sood, M and Jain, S}},
Title = {{An Advance 2D Face Recognition by Feature Extraction (ICA) and Optimize
   Multilayer Architecture}},
Booktitle = {{PROCEEDINGS OF 4TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING,
   COMPUTING AND CONTROL (ISPCC 2K17)}},
Series = {{IEEE International Conference on Signal Processing Computing and Control}},
Year = {{2017}},
Pages = {{122-129}},
Note = {{4th IEEE International Conference on Signal Processing, Computing and
   Control (ISPCC), Jaypee Univ Informat Technol, Dept Elect \& Commun
   Engn, Solan, INDIA, SEP 21-23, 2017}},
Organization = {{IEEE; IEEE Delhi Sect; IEEE Jaypee Univ Informat Technol Student Branch;
   GENTECH; AD Instruments; ARK}},
Abstract = {{Facial recognition has most significant real-life requests like
   investigation and access control. It is associated through the issue of
   appropriately verifying face pictures and transmit them person in a
   database. In a past years face study has been emerging active topic.
   Most of the face detector techniques could be classified into feature
   based methods and image based also. Feature based techniques adds
   low-level analysis, feature analysis, etc. Facial recognition is a
   system capable of verifying / identifying a human after 3D images. By
   evaluating selected facial unique features from the image and face
   dataset. Design from transformation method given vector dimensional
   illustration of individual face in a prepared set of images, Principle
   component analysis inclines to search a dimensional sub-space whose
   normal vector features correspond to the maximum variance direction in
   the real image space. The PCA algorithm evaluates the feature
   extraction, data, i.e. Eigen Values and vectors of the scatter matrix.
   In literature survey, Face recognition is a design recognition mission
   performed exactly on faces. It can be described as categorizing a facial
   either ``known{''} or ``unknown{''}, after comparing it with deposits
   known individuals. It is also necessary to need a system that has the
   capability of knowledge to recognize indefinite faces. Computational
   representations of facial recognition must statement various difficult
   issues. After existing work, we study the SIFT structures for the
   gratitude method. The novel technique is compared with well settled
   facial recognition methods, name component analysis and eigenvalues and
   vector. This algorithm is called PCA and ICA (Independent Component
   Analysis). In research work, we implement the novel approach to detect
   the face in minimum time and evaluate the better accuracy based on Back
   Propagation Neural Networks. We design the framework in face recognition
   using MATLAB 2013a simulation tool. Evaluate the performance parameters,
   i.e. the FAR (false acceptance rate), FRR (False rejection Rate) and
   Accuracy and compare the existing performance parameters i.e. accuracy.}},
ISSN = {{2376-5461}},
ISBN = {{978-1-5090-5838-9}},
Unique-ID = {{ISI:000425238900023}},
}

@inproceedings{ ISI:000425925300009,
Author = {Indumathi, T. and Pushparani, M.},
Book-Group-Author = {{IEEE}},
Title = {{Multimodel Human Authentication By Matching 3D Skull And Gait}},
Booktitle = {{2017 2ND WORLD CONGRESS ON COMPUTING AND COMMUNICATION TECHNOLOGIES
   (WCCCT)}},
Year = {{2017}},
Pages = {{38-42}},
Note = {{2nd World Congress on Computing and Communication Technologies (WCCCT),
   St Josephs Coll, Tiruchirappalli, INDIA, FEB 02-04, 2017}},
Organization = {{St Josephs Coll, Dept Comp Sci}},
Abstract = {{This research paper focuses multimodal human identifications that are
   captured in a web cam as images of face and walking style. The image can
   then be considered for further analyses of gait and skull
   characteristics as per Human Identification Systems. We propose to
   identify a skull by using a correlation measure between the 3D skull and
   3D face in terms of morphology, and measure the correlation using
   Enhance Canonical Correlation Coefficient Analysis (ECCCA). We use the
   3D skull data as the probe and 3D face geometric data as the gallery and
   match the skull with enrolled 3D faces by the correlation measure
   between the Probe and the Gallery. This paper proposes Uncorrelated
   Multilinear Discriminant Analysis (UMLDA) algorithm for the challenging
   problem of Gait Recognition. Finally, Neural Network for mat-lab tool is
   used for training and testing purpose We have created different model of
   neural network based on hidden layer, selection of training algorithm
   and setting the different parameter for training. And then, we will test
   for the combination of NN+SVM, Knearest Neighbour Classification. Here
   all these experiments are done on CASIA gait database and input video.}},
DOI = {{10.1109/WCCCT.2016.19}},
ISBN = {{978-1-5090-5573-9}},
Unique-ID = {{ISI:000425925300009}},
}

@inproceedings{ ISI:000423869700004,
Author = {Haufel, Gisela and Bulatov, Dimitri and Solbrig, Peter},
Editor = {{Michel, U and Schulz, K and Nikolakopoulos, KG and Civco, D}},
Title = {{Sensor Data Fusion for Textured Reconstruction and Virtual
   Representation of Alpine Scenes}},
Booktitle = {{EARTH RESOURCES AND ENVIRONMENTAL REMOTE SENSING/GIS APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10428}},
Note = {{17th SPIE Conference on Earth Resources and Environmental Remote
   Sensing/GIS Applications, Warsaw, POLAND, SEP 12-14, 2017}},
Organization = {{SPIE}},
Abstract = {{The concept of remote sensing is to provide information about a
   wide-range area without making physical contact with this area. If,
   additionally to satellite imagery, images and videos taken by drones
   provide a more up-to-date data at a higher resolution, or accurate
   vector data is downloadable from the Internet, one speaks of sensor data
   fusion. The concept of sensor data fusion is relevant for many
   applications, such as virtual tourism, automatic navigation, hazard
   assessment, etc. In this work, we describe sensor data fusion aiming to
   create a semantic 3D model of an extremely interesting yet challenging
   dataset: An alpine region in Southern Germany. A particular challenge of
   this work is that rock faces including overhangs are present in the
   input airborne laser point cloud. The proposed procedure for
   identification and reconstruction of overhangs from point clouds
   comprises four steps: Point cloud preparation, filtering out vegetation,
   mesh generation and texturing. Further object types are extracted in
   several interesting subsections of the dataset: Building models with
   textures from UAV (Unmanned Aerial Vehicle) videos, hills reconstructed
   as generic surfaces and textured by the orthophoto, individual trees
   detected by the watershed algorithm, as well as the vector data for
   roads retrieved from openly available shape files and GPS-device tracks.
   We pursue geo-specific reconstruction by assigning texture and width to
   roads of several pre-determined types and modeling isolated trees and
   rocks using commercial software. For visualization and simulation of the
   area, we have chosen the simulation system Virtual Battlespace 3 (VBS3).
   It becomes clear that the proposed concept of sensor data fusion allows
   a coarse reconstruction of a large scene and, at the same time, an
   accurate and up-to-date representation of its relevant subsections, in
   which simulation can take place.}},
DOI = {{10.1117/12.2278237}},
Article-Number = {{UNSP 1042805}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1321-8; 978-1-5106-1320-1}},
ORCID-Numbers = {{Bulatov, Dimitri/0000-0002-0560-2591}},
Unique-ID = {{ISI:000423869700004}},
}

@inproceedings{ ISI:000418793200017,
Author = {Hammer, Marcus and Hebel, Marcus and Arens, Michael},
Editor = {{Kamerman, G and Steinvall, O}},
Title = {{Person detection and tracking with a 360 degrees LiDAR system}},
Booktitle = {{ELECTRO-OPTICAL REMOTE SENSING XI}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10434}},
Note = {{Conference on Electro-Optical Remote Sensing XI, Warsaw, POLAND, SEP
   11-12, 2017}},
Organization = {{SPIE}},
Abstract = {{Today it is easily possible to generate dense point clouds of the sensor
   environment using 360 degrees LiDAR (Light Detection and Ranging)
   sensors which are available since a number of years. The interpretation
   of these data is much more challenging. For the automated data
   evaluation the detection and classification of objects is a fundamental
   task. Especially in urban scenarios moving objects like persons or
   vehicles are of particular interest, for instance in automatic collision
   avoidance, for mobile sensor platforms or surveillance tasks.
   In literature there are several approaches for automated person
   detection in point clouds. While most techniques show acceptable results
   in object detection, the computation time is often crucial. The runtime
   can be problematic, especially due to the amount of data in the
   panoramic 360 degrees point clouds. On the other hand, for most
   applications an object detection and classification in real time is
   needed.
   The paper presents a proposal for a fast, real-time capable algorithm
   for person detection, classification and tracking in panoramic point
   clouds.}},
DOI = {{10.1117/12.2278215}},
Article-Number = {{UNSP 104340L}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1333-1; 978-1-5106-1332-4}},
Unique-ID = {{ISI:000418793200017}},
}

@article{ ISI:000418758100001,
Author = {Dai, Yucheng and Gong, Jianhua and Li, Yi and Feng, Quanlong},
Title = {{Building segmentation and outline extraction from UAV image-derived
   point clouds by a line growing algorithm}},
Journal = {{INTERNATIONAL JOURNAL OF DIGITAL EARTH}},
Year = {{2017}},
Volume = {{10}},
Number = {{11}},
Pages = {{1077-1097}},
Abstract = {{This paper presents an approach to process raw unmanned aircraft vehicle
   (UAV) image-derived point clouds for automatically detecting, segmenting
   and regularizing buildings of complex urban landscapes. For
   regularizing, we mean the extraction of the building footprints with
   precise position and details. In the first step, vegetation points were
   extracted using a support vector machine (SVM) classifier based on
   vegetation indexes calculated from color information, then the
   traditional hierarchical stripping classification method was applied to
   classify and segment individual buildings. In the second step, we first
   determined the building boundary points with a modified convex hull
   algorithm. Then, we further segmented these points such that each point
   was assigned to a fitting line using a line growing algorithm. Then, two
   mutually perpendicular directions of each individual building were
   determined through a W-k-means clustering algorithm which used the slop
   information and principal direction constraints. Eventually, the
   building edges were regularized to form the final building footprints.
   Qualitative and quantitative measures were used to evaluate the
   performance of the proposed approach by comparing the digitized results
   from ortho images.}},
DOI = {{10.1080/17538947.2016.1269841}},
ISSN = {{1753-8947}},
EISSN = {{1753-8955}},
Unique-ID = {{ISI:000418758100001}},
}

@inproceedings{ ISI:000418371405064,
Author = {Peng, Weilong and Feng, Zhiyong and Xu, Chao and Su, Yong},
Book-Group-Author = {{IEEE}},
Title = {{Parametric T-spline Face Morphable Model for Detailed Fitting in Shape
   Subspace}},
Booktitle = {{30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR
   2017)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{5515-5523}},
Note = {{30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Honolulu, HI, JUL 21-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration
   for the synthesis of 3D faces by assuming that faces are in a linear
   class. However, the human face is in a nonlinear manifold, and a new
   test are always not in the pre-learnt subspace accurately because of the
   disparity brought by ethnicity, age, gender, etc. In the paper, we
   propose a parametric T-spline morphable model (T-splineMM) for 3D face
   representation, which has great advantages of fitting data from an
   unknown source accurately. In the model, we describe a face by C-2
   T-spline surface, and divide the face surface into several shape units
   (SUs), according to facial action coding system (FACS), on T-mesh
   instead of on the surface directly. A fitting algorithm is proposed to
   optimize coefficients of T-spline control point components along
   pre-learnt identity and expression subspaces, as well as to optimize the
   details in refinement progress. As any pre-learnt subspace is not
   complete to handle the variety and details of faces and expressions, it
   covers a limited span of morphing. SUs division and detail refinement
   make the model fitting the facial muscle deformation in a larger span of
   morphing subspace. We conduct experiments on face scan data, kinect data
   as well as the space-time data to test the performance of detail
   fitting, robustness to missing data and noise, and to demonstrate the
   effectiveness of our model. Convincing results are illustrated to
   demonstrate the effectiveness of our model compared with the popular
   methods.}},
DOI = {{10.1109/CVPR.2017.585}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-0457-1}},
Unique-ID = {{ISI:000418371405064}},
}

@inproceedings{ ISI:000418397700008,
Author = {Richter, Julia and Wiede, Christian and Dayangac, Enes and Shahenshah,
   Ahsan and Hirtz, Gangolf},
Editor = {{Fred, A and DeMarsico, M and DiBaja, GS}},
Title = {{Activity Recognition for Elderly Care by Evaluating Proximity to Objects
   and Human Skeleton Data}},
Booktitle = {{PATTERN RECOGNITION APPLICATIONS AND METHODS, ICPRAM 2016}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10163}},
Pages = {{139-155}},
Note = {{5th International conference on Pattern Recognition Applications and
   Methods (ICPRAM), Rome, ITALY, FEB 24-26, 2016}},
Abstract = {{Recently, researchers have shown an increased interest in the detection
   of activities of daily living (ADLs) for ambient assisted living (AAL)
   applications. In this study, we present an algorithm that detects
   activities related to personal hygiene. The approach is based on the
   evaluation of pose information and a person's proximity to objects
   belonging to the typical equipment of bathrooms, such as sink, toilet
   and shower. In addition to this high-level reasoning, we developed a
   skeleton-based algorithm that recognises actions using a supervised
   learning model. Therefore, we analysed several feature vectors,
   especially with regard to the representation of joint trajectories in
   the frequency domain. The results gave evidence that this high-level
   reasoning algorithm can reliably recognise hygiene-related activities.
   An evaluation of the skeleton-based algorithm shows that the defined
   actions were successfully classified with a rate of 96.66\%.}},
DOI = {{10.1007/978-3-319-53375-9\_8}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-53375-9; 978-3-319-53374-2}},
Unique-ID = {{ISI:000418397700008}},
}

@inproceedings{ ISI:000417429000016,
Author = {Li Fangmin and Chen Ke and Liu Xinhua},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Reconstruction Based on Convolutional Neural Network}},
Booktitle = {{2017 10TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTATION TECHNOLOGY
   AND AUTOMATION (ICICTA 2017)}},
Series = {{International Conference on Intelligent Computation Technology and
   Automation}},
Year = {{2017}},
Pages = {{71-74}},
Note = {{10th International Conference on Intelligent Computation Technology and
   Automation (ICICTA), Changsha, PEOPLES R CHINA, OCT 09-10, 2017}},
Organization = {{Changsha Univ Sci \& Technol, Commun Res Inst; Cent S Univ, Shenzhen Res
   Inst; Hunan City Coll, Dept Urban Management}},
Abstract = {{Fast and robust 3D reconstruction of facial geometric structure from a
   single image is a challenging task with numerous applications, but there
   exist two problems when applied ``in the wild{''}: the 3D estimates are
   unstable for different photos of the same subject; the 3D estimates are
   over-regularized and generic. In response, a robust method for
   regressing discriminative 3D morphable face models(3DMM) is described to
   support face recognition and 3D mask printing. Combining the local data
   sets with the public data sets, improving the exiting 3DMM fitting
   method and then using a convolutional neural network(CNN) to improve
   reconstruction effect. The ground truth 3D faces of the CNN are the
   pooled 3DMM parameters extracted from the photos of the same subject.
   Using CNN to regress 3DMM shape and texture parameters directly from an
   input photo and offering a method for generating huge numbers of labeled
   examples. There are two key points of the paper: one is the training
   data generation for the model training; the other is the training of 3D
   reconstruction model. Experimental results and analysis show that this
   method costs much less time than traditional methods of 3D face
   modeling, and it is improved for different races on photos with any
   angles than the existing methods based on deep learning, and the system
   has better robustness.}},
DOI = {{10.1109/ICICTA.2017.23}},
ISSN = {{1949-1263}},
ISBN = {{978-1-5386-1230-9}},
Unique-ID = {{ISI:000417429000016}},
}

@article{ ISI:000416603000002,
Author = {El Sayed, Abdul Rahman and El Chakik, Abdallah and Alabboud, Hassan and
   Yassine, Adnan},
Title = {{3D face detection based on salient features extraction and skin colour
   detection using data mining}},
Journal = {{IMAGING SCIENCE JOURNAL}},
Year = {{2017}},
Volume = {{65}},
Number = {{7}},
Pages = {{393-408}},
Abstract = {{Face detection has an essential role in many applications. In this
   paper, we propose an efficient and robust method for face detection on a
   3D point cloud represented by a weighted graph. This method classifies
   graph vertices as skin and non-skin regions based on a data mining
   predictive model. Then, the saliency degree of vertices is computed to
   identify the possible candidate face features. Finally, the matching
   between non-skin regions representing eyes, mouth and eyebrows and
   salient regions is done by detecting collisions between polytopes,
   representing these two regions. This method extracts faces from
   situations where pose variation and change of expressions can be found.
   The robustness is showed through different experimental results.
   Moreover, we study the stability of our method according to noise.
   Furthermore, we show that our method deals with 2D images.}},
DOI = {{10.1080/13682199.2017.1358528}},
ISSN = {{1368-2199}},
EISSN = {{1743-131X}},
Unique-ID = {{ISI:000416603000002}},
}

@inproceedings{ ISI:000412830800019,
Author = {Mizoguchi, Tomohiro and Ishii, Akira and Nakamura, Hiroyuki and Inoue,
   Tsuyoshi and Takamatsu, Hisashi},
Editor = {{Remondino, F and Shortis, MR}},
Title = {{Lidar-based Individual Tree Species Classification using Convolutional
   Neural Network}},
Booktitle = {{VIDEOMETRICS, RANGE IMAGING, AND APPLICATIONS XIV}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10332}},
Note = {{Conference on Videometrics, Range Imaging, and Applications XIV, Munich,
   GERMANY, JUN 26-27, 2017}},
Organization = {{SPIE}},
Abstract = {{Terrestrial lidar is commonly used for detailed documentation in the
   field of forest inventory investigation. Recent improvements of point
   cloud processing techniques enabled efficient and precise computation of
   an individual tree shape parameters, such as breast-height diameter,
   height, and volume. However, tree species are manually specified by
   skilled workers to date. Previous works for automatic tree species
   classification mainly focused on aerial or satellite images, and few
   works have been reported for classification techniques using
   ground-based sensor data. Several candidate sensors can be considered
   for classification, such as RGB or multi/hyper spectral cameras. Above
   all candidates, we use terrestrial lidar because it can obtain high
   resolution point cloud in the dark forest. We selected bark texture for
   the classification criteria, since they clearly represent unique
   characteristics of each tree and do not change their appearance under
   seasonable variation and aged deterioration. In this paper, we propose a
   new method for automatic individual tree species classification based on
   terrestrial lidar using Convolutional Neural Network (CNN). The key
   component is the creation step of a depth image which well describe the
   characteristics of each species from a point cloud. We focus on Japanese
   cedar and cypress which cover the large part of domestic forest. Our
   experimental results demonstrate the effectiveness of our proposed
   method.}},
DOI = {{10.1117/12.2270123}},
Article-Number = {{UNSP 103320O}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1110-8; 978-1-5106-1109-2}},
Unique-ID = {{ISI:000412830800019}},
}

@inproceedings{ ISI:000413068300005,
Author = {Prathusha, Sai S. and Suja, P. and Tripathi, Shikha and Louis, R.},
Editor = {{Basu, A and Das, S and Horain, P and Bhattacharya, S}},
Title = {{Emotion Recognition from Facial Expressions of 4D Videos Using Curves
   and Surface Normals}},
Booktitle = {{INTELLIGENT HUMAN COMPUTER INTERACTION, IHCI 2016}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10127}},
Pages = {{51-64}},
Note = {{8th International Conference on Intelligent Human Computer Interaction
   (IHCI), Pilani, INDIA, DEC 12-13, 2016}},
Organization = {{Council Sci \& Ind Res, Cent Elect Engn Res Inst Pilani; Birla Inst
   Technol \& Sci Pilani; Indian Inst Informat Technol}},
Abstract = {{In this paper, we propose and compare three methods for recognizing
   emotions from facial expressions using 4D videos. In the first two
   methods, the 3D faces are re-sampled by using curves to extract the
   feature information. Two different methods are presented to resample the
   faces in an intelligent way using parallel curves and radial curves. The
   movement of the face is measured through these curves using two frames:
   neutral and peak frame. The deformation matrix is formed by computing
   the distance point to point on the corresponding curves of the neutral
   frame and peak frame. This matrix is used to create the feature vector
   that will be used for classification using Support Vector Machine (SVM).
   The third method proposed is to extract the feature information from the
   face by using surface normals. At every point on the frame, surface
   normals are extracted. The deformation matrix is formed by computing the
   Euclidean distances between the corresponding normals at a point on
   neutral and peak frames. This matrix is used to create the feature
   vector that will be used for classification of emotions using SVM. The
   proposed methods are analyzed and they showed improvement over existing
   literature.}},
DOI = {{10.1007/978-3-319-52503-7\_5}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-52503-7; 978-3-319-52502-0}},
Unique-ID = {{ISI:000413068300005}},
}

@inproceedings{ ISI:000408273100011,
Author = {Rajeev, Srijith and Kamath, Shreyas K. M. and Panetta, Karen and Agaian,
   Sos},
Book-Group-Author = {{IEEE}},
Title = {{3-D Palmprint Modeling for Biometric Verification}},
Booktitle = {{2017 IEEE INTERNATIONAL SYMPOSIUM ON TECHNOLOGIES FOR HOMELAND SECURITY
   (HST)}},
Year = {{2017}},
Note = {{IEEE International Symposium on Technologies for Homeland Security
   (HST), Waltham, MA, APR 25-26, 2017}},
Organization = {{IEEE}},
Abstract = {{Palmprint is a very unique and distinctive biometric trait because of
   features such as a person's inimitable principal lines, wrinkles, delta
   points, and minutiae. These constitute the main reasons why palmprint
   verification is considered as one of the most reliable personal
   identification methods. However, a clear majority of the research on
   palm-prints are concentrated on 2-D palmprint images irrespective of the
   fact that the human palm is a 3D-surface. While 2-D palmprint
   recognition has proved to be efficient in terms of verification rate, it
   has some essential downsides. These restrictions can adversely affect
   the performance and robustness of the palmprint recognition system. One
   of the possible solutions to resolve the limitations associated with 2-D
   palm print authentication systems is (i) to use a 3-D scanning system
   and to produce high quality 3-D images with depth information; (ii) to
   map 3-D palm-print images into 2-D images which may support the usage of
   3-D images with both biometric palmprint 2-D image databases and 2-D
   palmprint recognition tools. The bloom of 3-D technologies has made it
   easier to capture and store 3-D images. The problem of a direct mapping
   approach is that a large section of the palm is hard-pressed on the
   scanner surface during 2-D based acquisition. This paper proposes a
   novel technique to unravel/map 3-D palm images to its equivalent 2-D
   palm-print image. This image can be then used to perform efficient and
   accurate 2-D identification/verification. Experimental results and
   discussions will also be presented.}},
ISBN = {{978-1-5090-6356-7}},
Unique-ID = {{ISI:000408273100011}},
}

@inproceedings{ ISI:000407106200038,
Author = {Fraser, Alex and Dallaire, Michael and Godmaire, Xavier P.},
Editor = {{Ratvik, AP}},
Title = {{Laser Marking and 3D Imaging of Aluminum Products}},
Booktitle = {{LIGHT METALS 2017}},
Series = {{Minerals Metals \& Materials Series}},
Year = {{2017}},
Pages = {{289-292}},
Note = {{146th TMS Annual Meeting and Exhibition / Conference on Light Metals,
   San Diego, CA, FEB 26-MAR 02, 2017}},
Organization = {{Minerals Metals \& Mat Soc}},
Abstract = {{Most industrial products have (challenging) 3D shapes, many of them
   require traceability and individual marking. Although some laser marking
   systems on the market have 3D capabilities, they require the 3D shape to
   be loaded in the laser controller and the part to be precisely located.
   However, many industrial processes requiring direct part identification
   cannot fulfill those precise positioning requirements. To overcome these
   limitations, a 3D laser marker with integrated 3D imaging system was
   developed. This imaging system obtains the 3D image of the piece, and
   then the laser controller starts the marking process so that the focus
   fits on the part surface. The whole 3D data acquisition and transfer
   takes less than 3 s. This solves the problem of part positioning and
   simplifies the integration, while also providing 3D data of the surface
   that can be used for quality control.}},
DOI = {{10.1007/978-3-319-51541-0\_38}},
ISSN = {{2367-1181}},
ISBN = {{978-3-319-51541-0; 978-3-319-51540-3}},
Unique-ID = {{ISI:000407106200038}},
}

@inproceedings{ ISI:000406996500085,
Author = {Carraro, Marco and Munaro, Matteo and Roitberg, Alina and Menegatti,
   Emanuele},
Editor = {{Chen, W and Hosoda, K and Menegatti, E and Shimizu, M and Wang, H}},
Title = {{Improved Skeleton Estimation by Means of Depth Data Fusion from Multiple
   Depth Cameras}},
Booktitle = {{INTELLIGENT AUTONOMOUS SYSTEMS 14}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{531}},
Pages = {{1155-1167}},
Note = {{14th International Conference on Intelligent Autonomous Systems (IAS),
   Shanghai Jiao Tong Univ, Shanghai, PEOPLES R CHINA, JUL 03-07, 2016}},
Abstract = {{In this work, we address the problem of human skeleton estimation when
   multiple depth cameras are available. We propose a system that takes
   advantage of the knowledge of the camera poses to create a collaborative
   virtual depth image of the person in the scene which consists of points
   from all the cameras and that represents the person in a frontal pose.
   This depth image is fed as input to the open-source body part detector
   in the Point Cloud Library. A further contribution of this work is the
   improvement of this detector obtained by introducing two new components:
   as a pre-processing, a people detector is applied to remove the
   background from the depth map before estimating the skeleton, while an
   alpha-beta tracking is added as a post-processing step for filtering the
   obtained joint positions over time. The overall system has been proven
   to effectively improve the skeleton estimation on two sequences of
   people in different poses acquired from two first-generation Microsoft
   Kinect.}},
DOI = {{10.1007/978-3-319-48036-7\_85}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-48036-7; 978-3-319-48035-0}},
Unique-ID = {{ISI:000406996500085}},
}

@inproceedings{ ISI:000406534300014,
Author = {McIver, Charles A. and Metcalf, Jeremy P. and Olsen, Richard C.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Spectral LiDAR Analysis for Terrain Classification}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10191}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Anaheim, CA,
   APR 11-12, 2017}},
Organization = {{SPIE}},
Abstract = {{Data from the Optech Titan airborne laser scanner were collected over
   Monterey, CA, in three wavelengths (532 nm, 1064 nm, and 1550 nm), in
   May 2016, by the National Center for Airborne LiDAR Mapping (NCALM).
   Analysis techniques have been developed using spectral technology
   largely derived from the analysis of spectral imagery. Data are analyzed
   as individual points, vs techniques that emphasize spatial binning. The
   primary tool which allows for this exploitation is the N-Dimensional
   Visualizer contained in the ENVI software package. The results allow for
   significant improvement in classification accuracy compared to results
   obtained from techniques derived from standard LiDAR analysis tools.}},
DOI = {{10.1117/12.2276658}},
Article-Number = {{UNSP 101910J}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0883-2; 978-1-5106-0884-9}},
ResearcherID-Numbers = {{Olsen, Richard C/O-2699-2015}},
ORCID-Numbers = {{Olsen, Richard C/0000-0002-8344-9297}},
Unique-ID = {{ISI:000406534300014}},
}

@inproceedings{ ISI:000405560700088,
Author = {Manit, Jirapong and Bremer, Christina and Schweikard, Achim and Ernst,
   Floris},
Editor = {{Webster, RJ and Fei, B}},
Title = {{Patient identification using a near-infrared lasers canner}},
Booktitle = {{MEDICAL IMAGING 2017: IMAGE-GUIDED PROCEDURES, ROBOTIC INTERVENTIONS,
   AND MODELING}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10135}},
Note = {{Conference on Medical Imaging - Image-Guided Procedures, Robotic
   Interventions, and Modeling, Orlando, FL, FEB 14-16, 2017}},
Organization = {{SPIE; Alpin Med Syst; Siemens Healthineers; No Digital Inc}},
Abstract = {{We propose a new biometric approach where the tissue thickness of a
   person's forehead is used as a biometric feature. Given that the spatial
   registration of two 3D laser scans of the same human face usually
   produces a low error value, the principle of point cloud registration
   and its error metric can be applied to human classification techniques.
   However, by only considering the spatial error, it is not possible to
   reliably verify a person's identity. We propose to use a novel
   near-infrared laser-based head tracking system to determine an
   additional feature, the tissue thickness, and include this in the error
   metric. Using MRI as a ground truth, data from the foreheads of 30
   subjects was collected from which a 4D reference point cloud was created
   for each subject. The measurements from the near-infrared system were
   registered with all reference point clouds using the ICP algorithm.
   Afterwards, the spatial and tissue thickness errors were extracted,
   forming a 2D feature space. For all subjects, the lowest feature
   distance resulted from the registration of a measurement and the
   reference point cloud of the same person.
   The combined registration error features yielded two clusters in the
   feature space, one from the same subject and another from the other
   subjects. When only the tissue thickness error was considered, these
   clusters were less distinct but still present. These findings could help
   to raise safety standards for head and neck cancer patients and lays the
   foundation for a future human identification technique.}},
DOI = {{10.1117/12.2254963}},
Article-Number = {{UNSP 101352L}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0715-6; 978-1-5106-0716-3}},
Unique-ID = {{ISI:000405560700088}},
}

@inproceedings{ ISI:000402657200006,
Author = {Bobulski, Janusz},
Editor = {{Choras, RS}},
Title = {{Face Recognition with 3D Face Asymmetry}},
Booktitle = {{IMAGE PROCESSING AND COMMUNICATIONS CHALLENGES 8}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{525}},
Pages = {{53-60}},
Note = {{8th International Conference on Image Processing and Communications
   (IP\&C), UTP Univ Technol \& Sci, Inst Telecommunicat \& Comp Sci,
   Bydgoszcz, POLAND, SEP 07-09, 2016}},
Organization = {{UTP Univ Technol \& Sci}},
Abstract = {{Using of 3D images for the identification was in a field of the interest
   of many researchers which developed a few methods offering good results.
   However, there are few techniques exploiting the 3D asymmetry amongst
   these methods. We propose fast algorithm for rough extraction face
   asymmetry that is used to 3D face recognition with hidden Markov models.
   This paper presents conception of fast method for determine 3D face
   asymmetry. The research results indicate that face recognition with 3D
   face asymmetry may be used in biometrics systems.}},
DOI = {{10.1007/978-3-319-47274-4\_6}},
ISSN = {{2194-5357}},
EISSN = {{2194-5365}},
ISBN = {{978-3-319-47274-4; 978-3-319-47273-7}},
ResearcherID-Numbers = {{Bobulski, Janusz/C-4998-2014}},
ORCID-Numbers = {{Bobulski, Janusz/0000-0003-3345-604X}},
Unique-ID = {{ISI:000402657200006}},
}

@inproceedings{ ISI:000399000000019,
Author = {Zhou, Changhe and Wang, Shaoqing and Li, Chao and Li, Hao and Liu, Zhao},
Editor = {{Sheng, Y and Yu, C and Zhou, C}},
Title = {{Three-dimensional identification card and applications}},
Booktitle = {{HOLOGRAPHY, DIFFRACTIVE OPTICS, AND APPLICATIONS VII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10022}},
Note = {{Conference on Holography, Diffractive Optics, and Applications VII,
   Beijing, PEOPLES R CHINA, OCT 12-14, 2016}},
Organization = {{SPIE; Chinese Opt Soc}},
Abstract = {{Three dimensional Identification Card, with its three-dimensional
   personal image displayed and stored for personal identification, is
   supposed be the advanced version of the present two-dimensional
   identification card in the future {[} 1]. Three dimensional
   Identification Card means that there are three-dimensional optical
   techniques are used, the personal image on ID card is displayed to be
   three-dimensional, so we can see three dimensional personal face. The ID
   card also stores the three-dimensional face information in its inside
   electronics chip, which might be recorded by using two-channel cameras,
   and it can be displayed in computer as three-dimensional images for
   personal identification. Three-dimensional ID card might be one
   interesting direction to update the present two-dimensional card in the
   future. Three-dimension ID card might be widely used in airport custom,
   entrance of hotel, school, university, as passport for on-line banking,
   registration of on-line game, etc...}},
DOI = {{10.1117/12.2245680}},
Article-Number = {{UNSP 100220L}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0463-6; 978-1-5106-0464-3}},
Unique-ID = {{ISI:000399000000019}},
}

@article{ ISI:000391527900002,
Author = {Wang, Yuanyuan and Zhu, Xiao Xiang and Zeisl, Bernhard and Pollefeys,
   Marc},
Title = {{Fusing Meter-Resolution 4-D InSAR Point Clouds and Optical Images for
   Semantic Urban Infrastructure Monitoring}},
Journal = {{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{55}},
Number = {{1}},
Pages = {{14-26}},
Month = {{JAN}},
Abstract = {{Using synthetic aperture radar (SAR) interferometry to monitor long-term
   millimeter-level deformation of urban infrastructures, such as
   individual buildings and bridges, is an emerging and important field in
   remote sensing. In the state-of-the-art methods, deformation parameters
   are retrieved and monitored on a pixel basis solely in the SAR image
   domain. However, the inevitable side-looking imaging geometry of SAR
   results in undesired occlusion and layover in urban area, rendering the
   current method less competent for a semantic-level monitoring of
   different urban infrastructures. This paper presents a framework of a
   semantic-level deformation monitoring by linking the precise deformation
   estimates of SAR interferometry and the semantic classification labels
   of optical images via a 3-D geometric fusion and semantic texturing. The
   proposed approach provides the first ``SARptical{''} point cloud of an
   urban area, which is the SAR tomography point cloud textured with
   attributes from optical images. This opens a new perspective of InSAR
   deformation monitoring. Interesting examples on bridge and railway
   monitoring are demonstrated.}},
DOI = {{10.1109/TGRS.2016.2554563}},
ISSN = {{0196-2892}},
EISSN = {{1558-0644}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000391527900002}},
}

@article{ ISI:000397373000001,
Author = {Jin, Hai and Wang, Xun and Zhong, Zichun and Hua, Jing},
Title = {{Robust 3D face modeling and reconstruction from frontal and side images}},
Journal = {{COMPUTER AIDED GEOMETRIC DESIGN}},
Year = {{2017}},
Volume = {{50}},
Pages = {{1-13}},
Month = {{JAN}},
Abstract = {{Robust and effective capture and reconstruction of 3D face models
   directly by smartphone users enables many applications. This paper
   presents a novel 3D face modeling and reconstruction solution that
   robustly and accurately acquire 3D face models from a couple of images
   captured by a single smartphone camera. Two selfie photos of a subject
   taken from the front and side are first used to guide our Non-Negative
   Matrix Factorization (NMF) induced part-based face model to iteratively
   reconstruct an initial 3D face of the subject. Then, an iterative detail
   updating method is applied to the initial generated 3D face to
   reconstruct facial details through optimizing lighting parameters and
   local depths. Our iterative 3D face reconstruction method permits fully
   automatic registration of a part based face representation to the
   acquired face data and the detailed 2D/3D features to build a
   high-quality 3D face model. The NMF part-based face representation
   learned from a 3D face database facilitates effective global and
   adaptive local detail data fitting alternatively. Our system is flexible
   and it allows users to conduct the capture in any uncontrolled
   environment. We demonstrate the capability of our method by allowing
   users to capture and reconstruct their 3D faces by themselves. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.cagd.2016.11.001}},
ISSN = {{0167-8396}},
EISSN = {{1879-2332}},
ResearcherID-Numbers = {{Smith, Nash/R-3104-2017}},
ORCID-Numbers = {{Smith, Nash/0000-0002-4406-0043}},
Unique-ID = {{ISI:000397373000001}},
}

@article{ ISI:000394557800008,
Author = {Raith, Stefan and Vogel, Eric Per and Anees, Naeema and Keul, Christine
   and Gueth, Jan-Frederik and Edelhoff, Daniel and Fischer, Horst},
Title = {{Artificial Neural Networks as a powerful numerical tool to classify
   specific features of a tooth based on 3D scan data}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2017}},
Volume = {{80}},
Pages = {{65-76}},
Month = {{JAN 1}},
Abstract = {{Chairside manufacturing based on digital image acquisition is
   gainingincreasing importance in dentistry. For the standardized
   application of these methods, it is paramount to have highly automated
   digital workflows that can process acquired 3D image data of dental
   surfaces. Artificial Neural Networks (ANNs) arenumerical methods
   primarily used to mimic the complex networks of neural conneetions in
   the natural brain. Our hypothesis is that an ANNcan be developed that is
   capable of classifying dental cusps with sufficient accuracy. This bears
   enormous potential for an application in chairside manufacturing
   Workflows in the dental field, as it closes the gap between digital
   acquisition of dental geometries and modern computer-aided manufacturing
   techniques.Three-dimensional surface scans of dental casts representing
   natural full dental arches were transformed to range image data. These
   data were processed using an automated algorithm to detect candidates
   for tooth cusps according to salient geometrical features. These
   candidates were classified following common dental terminology and used
   as training data for a tailored ANN. For the actual cusp feature
   description, two different approaches were developed and applied to the
   available data: The first uses the relative location of the detected
   cusps as input data and the second method directly takes the image
   information given in the range images. In addition, a combination of
   both was implemented and investigatud. Both approaches showed high
   performance with correct classifications of 93.3\% and 93.5\%,
   respectively, with improvements by the combination shown to be
   minor.This article presents for the first time a fully automated method
   for the classification of teeth that could be confirmed to work with
   sufficient precision to exhibit the potential for its use in clinical
   practice,which is a prerequisite for automated computer-aided planning
   of prosthetic treatments with subsequent automated chairside
   manufacturing.}},
DOI = {{10.1016/j.compbiomed.2016.11.013}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
ResearcherID-Numbers = {{Guth, Jan-Frederik/Q-3999-2017}},
Unique-ID = {{ISI:000394557800008}},
}

@article{ ISI:000395485900101,
Author = {Zhang, Yi and Mu, Zhichun and Yuan, Li and Zeng, Hui and Chen, Long},
Title = {{3D Ear Normalization and Recognition Based on Local Surface Variation}},
Journal = {{APPLIED SCIENCES-BASEL}},
Year = {{2017}},
Volume = {{7}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{Most existing ICP (Iterative Closet Point)-based 3D ear recognition
   approaches resort to the coarse-to-fine ICP algorithms to match 3D ear
   models. With such an approach, the gallery-probe pairs are coarsely
   aligned based on a few local feature points and then finely matched
   using the original ear point cloud. However, such an approach ignores
   the fact that not all the points in the coarsely segmented ear data make
   positive contributions to recognition. As such, the coarsely segmented
   ear data which contains a lot of redundant and noisy data could lead to
   a mismatch in the recognition scenario. Additionally, the fine ICP
   matching can easily trap in local minima without the constraint of local
   features. In this paper, an efficient and fully automatic 3D ear
   recognition system is proposed to address these issues. The system
   describes the 3D ear surface with a local featurethe Local Surface
   Variation (LSV), which is responsive to the concave and convex areas of
   the surface. Instead of being used to extract discrete key points, the
   LSV descriptor is utilized to eliminate redundancy flat non-ear data and
   get normalized and refined ear data. At the stage of recognition, only
   one-step modified iterative closest points using local surface variation
   (ICP-LSV) algorithm is proposed, which provides additional local feature
   information to the procedure of ear recognition to enhance both the
   matching accuracy and computational efficiency. On an
   Inter((R))Xeon((R))W3550, 3.07 GHz work station (DELL T3500, Beijing,
   China), the authors were able to extract features from a probe ear in
   2.32 s match the ear with a gallery ear in 0.10 s using the method
   outlined in this paper. The proposed algorithm achieves rank-one
   recognition rate of 100\% on the Chinese Academy of Sciences' Institute
   of Automation 3D Face database (CASIA-3D FaceV1, CASIA, Beijing, China,
   2004) and 98.55\% with 2.3\% equal error rate (EER) on the Collection J2
   of University of Notre Dame Biometrics Database (UND-J2, University of
   Notre Dame, South Bend, IN, USA, between 2003 and 2005).}},
DOI = {{10.3390/app7010104}},
Article-Number = {{104}},
ISSN = {{2076-3417}},
Unique-ID = {{ISI:000395485900101}},
}

@inproceedings{ ISI:000463335100014,
Author = {Mathlouthi, Soumaya and Jribi, Majdi and Ghorbel, Faouzi},
Editor = {{BlancTalon, J and Penne, R and Philips, W and Popescu, D and Scheunders, P}},
Title = {{A Novel and Accurate Local 3D Representation for Face Recognition}},
Booktitle = {{ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS (ACIVS 2017)}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10617}},
Pages = {{161-169}},
Note = {{18th International Conference on Advanced Concepts for Intelligent
   Vision Systems (ACIVS), Antwerp, BELGIUM, SEP 18-21, 2017}},
Organization = {{Antwerp Univ; Commonwealth Sci \& Ind Res Org; Ghent Univ}},
Abstract = {{In this paper, we intend to introduce a novel curved 3D face
   representation. It is constructed on some static parts of the face which
   correspond to the nose and the eyes. Each part is described by the level
   curves of the superposition of several geodesic potentials generated
   from many reference points. We propose to describe the eye region by a
   bipolar representation based on the superposition of two geodesic
   potentials generated from two reference points and the nose by a
   three-polar one (three reference points). We use the BU-3DFE database of
   3D faces to test the accuracy of the proposed approach. The obtained
   results in the sense of the Hausdorff shape distance prove the
   performance of the novel representation for 3D faces identification. The
   obtained scores are comparable to the state of the art methods in the
   most of cases.}},
DOI = {{10.1007/978-3-319-70353-4\_14}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-70353-4; 978-3-319-70352-7}},
Unique-ID = {{ISI:000463335100014}},
}

@article{ ISI:000401423700003,
Author = {Kramer, Heather A. and Collins, Brandon M. and Gallagher, Claire V. and
   Keane, John J. and Stephens, Scott L. and Kelly, Maggi},
Title = {{Accessible light detection and ranging: estimating large tree density
   for habitat identification}},
Journal = {{ECOSPHERE}},
Year = {{2016}},
Volume = {{7}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Large trees are important to a wide variety of wildlife, including many
   species of conservation concern, such as the California spotted owl
   (Strix occidentalis occidentalis). Light detection and ranging (LiDAR)
   has been successfully utilized to identify the density of large-diameter
   trees, either by segmenting the LiDAR point cloud into individual trees,
   or by building regression models between variables extracted from the
   LiDAR point cloud and field data. Neither of these methods is easily
   accessible for most land managers due to the reliance on specialized
   software, and much available LiDAR data are being underutilized due to
   the steep learning curve required for advanced processing using these
   programs. This study derived a simple, yet effective method for
   estimating the density of large-stemmed trees from the LiDAR canopy
   height model, a standard raster product derived from the LiDAR point
   cloud that is often delivered with the LiDAR and is easy to process by
   personnel trained in geographic information systems (GIS). Ground plots
   needed to be large (1 ha) to build a robust model, but the spatial
   accuracy of plot center was less crucial to model accuracy. We also
   showed that predicted large tree density is positively linked to
   California spotted owl nest sites.}},
DOI = {{10.1002/ecs2.1593}},
Article-Number = {{e01593}},
ISSN = {{2150-8925}},
Unique-ID = {{ISI:000401423700003}},
}

@article{ ISI:000396382500043,
Author = {Magnard, Christophe and Morsdorf, Felix and Small, David and Stilla, Uwe
   and Schaepman, Michael E. and Meier, Erich},
Title = {{Single tree identification using airborne multibaseline SAR
   interferometry data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2016}},
Volume = {{186}},
Pages = {{567-580}},
Month = {{DEC 1}},
Abstract = {{Remote sensing data allow large scale observation of forested
   ecosystems. Forest assessment benefits from information about individual
   trees. Multibaseline SAR interferometry (InSAR) is able to generate
   dense point clouds of forest canopies, similar to airborne laser
   scanning (ALS). This type of point cloud was generated using data from
   the Ka-band MEMPHIS system, acquired over a mainly coniferous forest
   near Vordemwald in the Swiss Midlands. This point cloud was segmented
   using an advanced clustering technique to detect individual trees and
   derive their positions, heights, and crown diameters. To evaluate the
   InSAR point cloud properties and limitations, it was compared to
   products derived from ALS and stereo-photogrammetry. All point clouds
   showed similar geolocation accuracies with 02-0.3 m relative shifts.
   Both InSAR and photogrammetry techniques yielded points predominantly
   located in the upper levels of the forest vegetation, while ALS provided
   points from the top of the canopy down to the understory and forest
   floor. The canopy height models agreed very well with each other, with
   R-2 values between 0.84 and 0.89. The detected trees and their estimated
   physical and structural parameters were validated by comparing them to
   reference forestry data. A detection rate of similar to 90\% was
   achieved for larger trees, corresponding to half of the reference trees.
   The smaller trees were detected with a success rate of similar to 50\%.
   The tree height was slightly underestimated, with a R-2 value of 0.63.
   The estimated crown diameter agreed on an average sense, however with a
   relatively low R-2 value of 0.19. Very high success rates (>90\%) were
   obtained when matching the trees detected from the InSAR-data with those
   detected from the ALS- and photogrammetry-data. There, InSAR tree
   heights were in the mean 1-1.5 m lower, with high R-2 values ranging
   between 0.8 and 0.9. Our results demonstrate the use of millimeter wave
   SAR interferometry data as an alternative to ALS- and
   photogrammetry-based data for forest monitoring. (C) 2016 Elsevier Inc.
   All rights reserved.}},
DOI = {{10.1016/j.rse.2016.09.018}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Schaepman, Michael/B-9213-2009
   }},
ORCID-Numbers = {{Schaepman, Michael/0000-0002-9627-9565
   Magnard, Christophe/0000-0002-1473-8650}},
Unique-ID = {{ISI:000396382500043}},
}

@article{ ISI:000392085100011,
Author = {Pohlmann, Stefanie T. L. and Harkness, Elaine F. and Taylor, Christopher
   J. and Astley, Susan M.},
Title = {{Evaluation of Kinect 3D Sensor for Healthcare Imaging}},
Journal = {{JOURNAL OF MEDICAL AND BIOLOGICAL ENGINEERING}},
Year = {{2016}},
Volume = {{36}},
Number = {{6, SI}},
Pages = {{857-870}},
Month = {{DEC}},
Abstract = {{Microsoft Kinect is a three-dimensional (3D) sensor originally designed
   for gaming that has received growing interest as a cost-effective and
   safe device for healthcare imaging. Recent applications of Kinect in
   health monitoring, screening, rehabilitation, assistance systems, and
   intervention support are reviewed here. The suitability of available
   technologies for healthcare imaging applications is assessed. The
   performance of Kinect I, based on structured light technology, is
   compared with that of the more recent Kinect II, which uses
   time-of-flight measurement, under conditions relevant to healthcare
   applications. The accuracy, precision, and resolution of 3D images
   generated with Kinect I and Kinect II are evaluated using flat cardboard
   models representing different skin colors (pale, medium, and dark) at
   distances ranging from 0.5 to 1.2 m and measurement angles of up to 75A
   degrees. Both sensors demonstrated high accuracy (majority of
   measurements < 2 mm) and precision (mean point to plane error < 2 mm) at
   an average resolution of at least 390 points per cm(2). Kinect I is
   capable of imaging at shorter measurement distances, but Kinect II
   enables structures angled at over 60A degrees to be evaluated. Kinect II
   showed significantly higher precision and Kinect I showed significantly
   higher resolution (both p < 0.001). The choice of object color can
   influence measurement range and precision. Although Kinect is not a
   medical imaging device, both sensor generations show performance
   adequate for a range of healthcare imaging applications. Kinect I is
   more appropriate for short-range imaging and Kinect II is more
   appropriate for imaging highly curved surfaces such as the face or
   breast.}},
DOI = {{10.1007/s40846-016-0184-2}},
ISSN = {{1609-0985}},
EISSN = {{2199-4757}},
ORCID-Numbers = {{Harkness, Elaine/0000-0001-6625-7739}},
Unique-ID = {{ISI:000392085100011}},
}

@article{ ISI:000391303000155,
Author = {Rose, Johann Christian and Kicherer, Anna and Wieland, Markus and
   Klingbeil, Lasse and Toepfer, Reinhard and Kuhlmann, Heiner},
Title = {{Towards Automated Large-Scale 3D Phenotyping of Vineyards under Field
   Conditions}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{In viticulture, phenotypic data are traditionally collected directly in
   the field via visual and manual means by an experienced person. This
   approach is time consuming, subjective and prone to human errors. In
   recent years, research therefore has focused strongly on developing
   automated and non-invasive sensor-based methods to increase data
   acquisition speed, enhance measurement accuracy and objectivity and to
   reduce labor costs. While many 2D methods based on image processing have
   been proposed for field phenotyping, only a few 3D solutions are found
   in the literature. A track-driven vehicle consisting of a camera system,
   a real-time-kinematic GPS system for positioning, as well as hardware
   for vehicle control, image storage and acquisition is used to visually
   capture a whole vine row canopy with georeferenced RGB images. In the
   first post-processing step, these images were used within a
   multi-view-stereo software to reconstruct a textured 3D point cloud of
   the whole grapevine row. A classification algorithm is then used in the
   second step to automatically classify the raw point cloud data into the
   semantic plant components, grape bunches and canopy. In the third step,
   phenotypic data for the semantic objects is gathered using the
   classification results obtaining the quantity of grape bunches, berries
   and the berry diameter.}},
DOI = {{10.3390/s16122136}},
Article-Number = {{2136}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000391303000155}},
}

@article{ ISI:000386741300011,
Author = {Li, Billy Y. L. and Xue, Mingliang and Mian, Ajmal and Liu, Wanquan and
   Krishna, Aneesh},
Title = {{Robust RGB-D face recognition using Kinect sensor}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{214}},
Pages = {{93-108}},
Month = {{NOV 19}},
Abstract = {{In this paper we propose a robust face recognition algorithm for low
   resolution RGB-D Kinect data. Many techniques are proposed for image
   preprocessing due to the noisy depth data. First, facial symmetry is
   exploited based on the 3D point cloud to obtain a canonical frontal view
   image irrespective of the initial pose and then depth data is converted
   to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are
   then used to project RGB to DCS (Discriminant Color Space) and normal
   maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust
   Sparse Coding method is proposed that codes the multiple channels (DCS
   or DNM) of a test image as a sparse combination of training samples with
   different pixel weighting. Weights are calculated dynamically in an
   iterative process to achieve robustness against variations in pose,
   illumination, facial expressions and disguise. In contrast to existing
   techniques, our multi-channel approach is more robust to variations.
   Reconstruction errors of the test image (DCS and DNM) are normalized and
   fused to decide its identity. The proposed algorithm is evaluated on
   four public databases. It achieves 98.4\% identification rate on
   CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects.
   Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2
   databases, the proposed algorithm achieves 97.6\%, 95.6\% and 95.2\%
   identification rates respectively. To the best of our knowledge, these
   are the highest identification rates reported so far for the first three
   databases. (C) 2016 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2016.06.012}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
ORCID-Numbers = {{Krishna, Aneesh/0000-0001-8637-5732
   liu, wanquan/0000-0003-4910-353X}},
Unique-ID = {{ISI:000386741300011}},
}

@article{ ISI:000386910000105,
Author = {Nakamura, Tomoya and Matsumoto, Jumpei and Nishimaru, Hiroshi and
   Bretas, Rafael Vieira and Takamura, Yusaku and Hori, Etsuro and Ono,
   Taketoshi and Nishijo, Hisao},
Title = {{A Markerless 3D Computerized Motion Capture System Incorporating a
   Skeleton Model for Monkeys}},
Journal = {{PLOS ONE}},
Year = {{2016}},
Volume = {{11}},
Number = {{11}},
Month = {{NOV 3}},
Abstract = {{In this study, we propose a novel markerless motion capture system (MCS)
   for monkeys, in which 3D surface images of monkeys were reconstructed by
   integrating data from four depth cameras, and a skeleton model of the
   monkey was fitted onto 3D images of monkeys in each frame of the video.
   To validate the MCS, first, estimated 3D positions of body parts were
   compared between the 3D MCS-assisted estimation and manual estimation
   based on visual inspection when a monkey performed a shuttling behavior
   in which it had to avoid obstacles in various positions. The mean
   estimation error of the positions of body parts (3-14 cm) and of head
   rotation (35-43 degrees) between the 3D MCS-assisted and manual
   estimation were comparable to the errors between two different
   experimenters performing manual estimation. Furthermore, the MCS could
   identify specific monkey actions, and there was no false positive nor
   false negative detection of actions compared with those in manual
   estimation. Second, to check the reproducibility of MCS-assisted
   estimation, the same analyses of the above experiments were repeated by
   a different user. The estimation errors of positions of most body parts
   between the two experimenters were significantly smaller in the
   MCS-assisted estimation than in the manual estimation. Third, effects of
   methamphetamine (MAP) administration on the spontaneous behaviors of
   four monkeys were analyzed using the MCS. MAP significantly increased
   head movements, tended to decrease locomotion speed, and had no
   significant effect on total path length. The results were comparable to
   previous human clinical data. Furthermore, estimated data following MAP
   injection (total path length, walking speed, and speed of head rotation)
   correlated significantly between the two experimenters in the
   MCS-assisted estimation (r = 0.863 to 0.999). The results suggest that
   the presented MCS in monkeys is useful in investigating neural
   mechanisms underlying various psychiatric disorders and developing
   pharmacological interventions.}},
DOI = {{10.1371/journal.pone.0166154}},
Article-Number = {{e0166154}},
ISSN = {{1932-6203}},
ORCID-Numbers = {{Matsumoto, Jumpei/0000-0003-4729-2816}},
Unique-ID = {{ISI:000386910000105}},
}

@article{ ISI:000386995100031,
Author = {Wolff, Antje and Gotz, Yvonne},
Title = {{4D phenotyping of germinating seeds and seedlings as a tool to
   objectively measure seed quality and improve field establishment and
   yield of sugar beets}},
Journal = {{INTERNATIONAL SUGAR JOURNAL}},
Year = {{2016}},
Volume = {{118}},
Number = {{1415}},
Pages = {{836-839}},
Month = {{NOV}},
Abstract = {{The plant breeding company Strube, in cooperation with the German
   Fraunhofer Institute for non-destructive testing, has developed an
   automated high-throughput germination test for sugar beet seeds. The
   phenoTest permits objective measurement and classification of
   germinating seeds and resulting seedlings. It is therefore more accurate
   and provides more information than the conventional ISTA (International
   Seed Testing Association)-germination test, which relies purely on
   visual assessment and classification into the categories ``normal{''} or
   ``abnormal{''}. This differentiation is difficult to standardise, and
   is, to a significant degree, subjective. The phenoTest is based on
   three-dimensional (3D) X-ray images. Repeated tests of the same plants
   enable an objective assessment of seedling development over the course
   of time (4D phenotyping). The individual organs of each plant (radicle,
   hypocotyl and cotyledons) are automatically identified and measured. The
   method provides detailed information on germinating capacity and vigour,
   as well as the homogeneity of a seed lot. Results are documented as
   measurement values and 3D-images of each individual plant at different
   time points. The data is used to compare seed lots concerning their
   natural germination capacity and especially vigour, the processing or
   priming technologies they experienced, the pelleting and seed treatment
   applied etc. in order to predict their field emergence potential even
   under difficult growing conditions. These analyses also helps to
   optimise all these processes in commercial seed production to obtain a
   quick, homogeneous and complete field emergence, making full use of the
   genetic yield potential of sugar beet.}},
ISSN = {{0020-8841}},
Unique-ID = {{ISI:000386995100031}},
}

@article{ ISI:000386874900020,
Author = {Guo, Yulan and Lei, Yinjie and Liu, Li and Wang, Yan and Bennamoun,
   Mohammed and Sohelf, Ferdous},
Title = {{EI3D: Expression-invariant 3D face recognition based on feature and
   shape matching}},
Journal = {{PATTERN RECOGNITION LETTERS}},
Year = {{2016}},
Volume = {{83}},
Number = {{3}},
Pages = {{403-412}},
Month = {{NOV 1}},
Abstract = {{This paper presents a local feature based shape matching algorithm for
   expression-invariant 3D face recognition. Each 3D face is first
   automatically detected from a raw 3D data and normalized to achieve pose
   invariance. The 3D face is then represented by a set of keypoints and
   their associated local feature descriptors to achieve robustness to
   expression variations. During face recognition, a probe face is compared
   against each gallery face using both local feature matching and 3D point
   cloud registration. The number of feature matches, the average distance
   of matched features, and the number of closest point pairs after
   registration are used to measure the similarity between two 3D faces.
   These similarity metrics are then fused to obtain the final results. The
   proposed algorithm has been tested on the FRGC v2 benchmark and a high
   recognition performance has been achieved. It obtained the
   state-of-the-art results by achieving an overall rank- 1 identification
   rate of 97.0\% and an average verification rate of 99.01\% at 0.001
   false acceptance rate for all faces with neutral and non-neutral
   expressions. Further, the robustness of our algorithm under different
   occlusions has been demonstrated on the Bosphorus dataset. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.patrec.2016.04.003}},
ISSN = {{0167-8655}},
EISSN = {{1872-7344}},
ResearcherID-Numbers = {{Sohel, Ferdous/C-2428-2013
   }},
ORCID-Numbers = {{Sohel, Ferdous/0000-0003-1557-4907
   Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000386874900020}},
}

@article{ ISI:000382794300032,
Author = {Deng, Lei and Chen, Zhixiang and Chen, Baohua and Duan, Yueqi and Zhou,
   Jie},
Title = {{Incremental image set querying based localization}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{208}},
Number = {{SI}},
Pages = {{315-324}},
Month = {{OCT 5}},
Abstract = {{Image based localization has been developed for many applications such
   as mobile localization, auto navigation, augmented reality and photo
   tourism. When the querying image is matched against a pre-built 3D
   feature point cloud, its pose can be estimated for future use. However,
   when the querying image is distant from the pre-built 3D point cloud,
   conventional single image-based localization method will fail. To
   address this problem, we present an incremental image set querying based
   localization framework. When single image localization fails, the system
   will incrementally ask the user to input more auxiliary images until the
   localization is successful and stable. The main idea is that image set,
   instead of single image, is matched against the pre-built 3D point cloud
   to meet the challenge. Next the image set is incrementally enlarged and
   aggregated to form a local 3D model. Compared with single image querying
   based localization method, the querying 3D model contains more
   information and geometry constraints which are essential for
   localization. Experiments have demonstrated the effectiveness and
   feasibility of the proposed framework. (C) 2016 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.neucom.2015.11.117}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000382794300032}},
}

@article{ ISI:000382794300028,
Author = {Zhang, Erhu and Chen, Wanjun and Zhang, Zhuomin and Zhang, Yan},
Title = {{Local Surface Geometric Feature for 3D human action recognition}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{208}},
Number = {{SI}},
Pages = {{281-289}},
Month = {{OCT 5}},
Abstract = {{This paper presents a novel Local Surface Geometric Feature (LSGF) for
   human action recognition from video sequences captured by a depth
   camera. The LSGF is extracted from each skeleton joint in point cloud
   space to capture the static appearance and pose cues, which includes
   joint position, normal, and local curvature. A temporal pyramid of
   covariance matrix is exploited to model both pairwise relations of
   features instead of features themselves and the temporal evolution.
   Finally, Fisher vector encoding is imported as a global representation
   for a video sequence and SVM classifier is used for classification. In
   the extensive experiments, we achieve classification results superior to
   most of previous published results on three public benchmark datasets,
   i.e., MSR-Action3D, MSR DailyActivity3D, and UTItinect Action. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2015.12.122}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000382794300028}},
}

@article{ ISI:000387670700012,
Author = {Dalponte, Michele and Coomes, David A.},
Title = {{Tree-centric mapping of forest carbon density from airborne laser
   scanning and hyperspectral data}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2016}},
Volume = {{7}},
Number = {{10}},
Pages = {{1236-1245}},
Month = {{OCT}},
Abstract = {{1. Forests are a major component of the global carbon cycle, and
   accurate estimation of forest carbon stocks and fluxes is important in
   the context of anthropogenic global change. Airborne laser scanning
   (ALS) data sets are increasingly recognized as outstanding data sources
   for high-fidelity mapping of carbon stocks at regional scales.
   2. We develop a tree-centric approach to carbon mapping, based on
   identifying individual tree crowns (ITCs) and species from airborne
   remote sensing data, from which individual tree carbon stocks are
   calculated. We identify ITCs from the laser scanning point cloud using a
   region-growing algorithm and identifying species from airborne
   hyperspectral data by machine learning. For each detected tree, we
   predict stem diameter from its height and crown-width estimate. From
   that point on, we use well-established approaches developed for
   field-based inventories: above-ground biomasses of trees are estimated
   using published allometries and summed within plots to estimate carbon
   density.
   3. We show this approach is highly reliable: tests in the Italian Alps
   demonstrated a close relationship between field-and ALS-based estimates
   of carbon stocks (r(2) = 0.98). Small trees are invisible from the air,
   and a correction factor is required to accommodate this effect.
   4. An advantage of the tree-centric approach over existing area-based
   methods is that it can produce maps at any scale and is fundamentally
   based on field-based inventory methods, making it intuitive and
   transparent. Airborne laser scanning, hyperspectral sensing and
   computational power are all advancing rapidly, making it increasingly
   feasible to use ITC approaches for effective mapping of forest carbon
   density also inside wider carbon mapping programs like REDD++.}},
DOI = {{10.1111/2041-210X.12575}},
ISSN = {{2041-210X}},
EISSN = {{2041-2096}},
ResearcherID-Numbers = {{Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000387670700012}},
}

@article{ ISI:000385597700004,
Author = {Li, Lin and Li, Dalin and Zhu, Haihong and Li, You},
Title = {{A dual growing method for the automatic extraction of individual trees
   from mobile laser scanning data}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{120}},
Pages = {{37-52}},
Month = {{OCT}},
Abstract = {{Street trees interlaced with other objects in cluttered point clouds of
   urban scenes inhibit the automatic extraction of individual trees. This
   paper proposes a method for the automatic extraction of individual trees
   from mobile laser scanning data, according to the general constitution
   of trees. Two components of each individual tree - a trunk and a crown
   can be extracted by the dual growing method. This method consists of
   coarse classification, through which most of artifacts are removed; the
   automatic selection of appropriate seeds for individual trees, by which
   the common manual initial setting is avoided; a dual growing process
   that separates one tree from others by circumscribing a trunk in an
   adaptive growing radius and segmenting a crown in constrained growing
   regions; and a refining process that draws a singular trunk from the
   interlaced other objects. The method is verified by two datasets with
   over 98\% completeness and over 96\% correctness. The low mean absolute
   percentage errors in capturing the morphological parameters of
   individual trees indicate that this method can output individual trees
   with high precision. (C) 2016 International Society for Photogrammetry
   and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.07.009}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
Unique-ID = {{ISI:000385597700004}},
}

@article{ ISI:000388855500007,
Author = {Ximena Bastidas-Rodriguez, Maria and Prieto-Ortiz, Flavio A. and
   Espejo-Mora, Edgar},
Title = {{Fractographic classification in metallic materials by using 3D
   processing and computer vision techniques}},
Journal = {{REVISTA FACULTAD DE INGENIERIA, UNIVERSIDAD PEDAGOGICA Y TECNOLOGICA DE
   COLOMBIA}},
Year = {{2016}},
Volume = {{25}},
Number = {{43}},
Pages = {{83-96}},
Month = {{SEP-DEC}},
Abstract = {{Failure analysis aims at collecting information about how and why a
   failure is produced. The first step in this process is a visual
   inspection on the flaw surface that will reveal the features, marks, and
   texture, which characterize each type of fracture. This is generally
   carried out by personnel with no experience that usually lack the
   knowledge to do it. This paper proposes a classification method for
   three kinds of fractures in crystalline materials: brittle, fatigue, and
   ductile. The method uses 3D vision, and it is expected to support
   failure analysis. The features used in this work were: i) Haralick's
   features and ii) the fractal dimension. These features were applied to
   3D images obtained from a confocal laser scanning microscopy Zeiss LSM
   700. For the classification, we evaluated two classifiers: Artificial
   Neural Networks and Support Vector Machine. The performance evaluation
   was made by extracting four marginal relations from the confusion
   matrix: accuracy, sensitivity, specificity, and precision, plus three
   evaluation methods: Receiver Operating Characteristic space, the
   Individual Classification Success Index, and the Jaccard's coefficient.
   Despite the classification percentage obtained by an expert is better
   than the one obtained with the algorithm, the algorithm achieves a
   classification percentage near or exceeding the 60 \% accuracy for the
   analyzed failure modes. The results presented here provide a good
   approach to address future research on texture analysis using 3D data.}},
DOI = {{10.19053/01211129.v25.n43.2016.5301}},
ISSN = {{0121-1129}},
EISSN = {{2357-5328}},
Unique-ID = {{ISI:000388855500007}},
}

@article{ ISI:000385488000039,
Author = {Cao, Lin and Gao, Sha and Li, Pinghao and Yun, Ting and Shen, Xin and
   Ruan, Honghua},
Title = {{Aboveground Biomass Estimation of Individual Trees in a Coastal Planted
   Forest Using Full-Waveform Airborne Laser Scanning Data}},
Journal = {{REMOTE SENSING}},
Year = {{2016}},
Volume = {{8}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{The accurate estimation of individual tree level aboveground biomass
   (AGB) is critical for understanding the carbon cycle, detecting
   potential biofuels and managing forest ecosystems. In this study, we
   assessed the capability of the metrics of point clouds, extracted from
   the full-waveform Airborne Laser Scanning (ALS) data, and of composite
   waveforms, calculated based on a voxel-based approach, for estimating
   tree level AGB individually and in combination, over a planted forest in
   the coastal region of east China. To do so, we investigated the
   importance of point cloud and waveform metrics for estimating tree-level
   AGB by all subsets models and relative weight indices. We also assessed
   the capability of the point cloud and waveform metrics based models and
   combo model (including the combination of both point cloud and waveform
   metrics) for tree-level AGB estimation and evaluated the accuracies of
   these models. The results demonstrated that most of the waveform metrics
   have relatively low correlation coefficients (<0.60) with other metrics.
   The combo models (Adjusted R-2 = 0.78-0.89), including both point cloud
   and waveform metrics, have a relatively higher performance than the
   models fitted by point cloud metrics-only (Adjusted R-2 = 0.74-0.86) and
   waveform metrics-only (Adjusted R-2 = 0.72-0.84), with the mostly
   selected metrics of the 95th percentile height (H-95), mean of height of
   median energy (HOME) and mean of the height/median ratio (HTMR). Based
   on the relative weights (i.e., the percentage of contribution for R-2)
   of the mostly selected metrics for all subsets, the metric of 95th
   percentile height (H-95) has the highest relative importance for AGB
   estimation (19.23\%), followed by 75th percentile height (H-75)
   (18.02\%) and coefficient of variation of heights (H-cv) (15.18\%) in
   the point cloud metrics based models. For the waveform metrics based
   models, the metric of mean of height of median energy (HOME) has the
   highest relative importance for AGB estimation (17.86\%), followed by
   mean of the height/median ratio (HTMR) (16.23\%) and standard deviation
   of height of median energy (HOME sigma) (14.78\%). This study
   demonstrated benefits of using full-waveform ALS data for estimating
   biomass at tree level, for sustainable forest management and mitigating
   climate change by planted forest, as China has the largest area of
   planted forest in the world, and these forests contribute to a large
   amount of carbon sequestration in terrestrial ecosystems.}},
DOI = {{10.3390/rs8090729}},
Article-Number = {{729}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000385488000039}},
}

@article{ ISI:000385150400002,
Author = {Qiu, Luwen and Zhou, Zhongwei and Guo, Jixiang and Lv, Jiancheng},
Title = {{An Automatic Registration Algorithm for 3D Maxillofacial Model}},
Journal = {{3D RESEARCH}},
Year = {{2016}},
Volume = {{7}},
Number = {{3}},
Month = {{SEP}},
Abstract = {{3D image registration aims at aligning two 3D data sets in a common
   coordinate system, which has been widely used in computer vision,
   pattern recognition and computer assisted surgery. One challenging
   problem in 3D registration is that point-wise correspondences between
   two point sets are often unknown apriori. In this work, we develop an
   automatic algorithm for 3D maxillofacial models registration including
   facial surface model and skull model. Our proposed registration
   algorithm can achieve a good alignment result between partial and whole
   maxillofacial model in spite of ambiguous matching, which has a
   potential application in the oral and maxillofacial reparative and
   reconstructive surgery. The proposed algorithm includes three steps: (1)
   3D-SIFT features extraction and FPFH descriptors construction; (2)
   feature matching using SAC-IA; (3) coarse rigid alignment and refinement
   by ICP. Experiments on facial surfaces and mandible skull models
   demonstrate the efficiency and robustness of our algorithm.}},
DOI = {{10.1007/s13319-016-0083-x}},
Article-Number = {{UNSP 20}},
ISSN = {{2092-6731}},
Unique-ID = {{ISI:000385150400002}},
}

@article{ ISI:000384777300027,
Author = {Yang, Bisheng and Huang, Ronggang and Dong, Zhen and Zang, Yufu and Li,
   Jianping},
Title = {{Two-step adaptive extraction method for ground points and breaklines
   from lidar point clouds}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{119}},
Pages = {{373-389}},
Month = {{SEP}},
Abstract = {{The extraction of ground points and breaklines is a crucial step during
   generation of high quality digital elevation models (DEMs) from airborne
   LiDAR point clouds. In this study, we propose a novel automated method
   for this task. To overcome the disadvantages of applying a single
   filtering method in areas with various types of terrain, the proposed
   method first classifies the points into a set of segments and one set of
   individual points, which are filtered by segment-based filtering and
   multi-scale morphological filtering, respectively. In the process of
   multi-scale morphological filtering, the proposed method removes
   amorphous objects from the set of individual points to decrease the
   effect of the maximum scale on the filtering result. The proposed method
   then extracts the breaklines from the ground points, which provide a
   good foundation for generation of a high quality DEM. Finally, the
   experimental results demonstrate that the proposed method extracts
   ground points in a robust manner while preserving the breaklines. (C)
   2016 International Society for Photogrammetry and Remote Sensing, Inc.
   (ISPRS). Published by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.07.002}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Yang, Bisheng/A-4642-2013}},
ORCID-Numbers = {{Yang, Bisheng/0000-0001-7736-0803}},
Unique-ID = {{ISI:000384777300027}},
}

@article{ ISI:000382679900012,
Author = {Bagchi, Parama and Bhattacharjee, Debotosh and Nasipuri, Mita},
Title = {{A robust analysis, detection and recognition of facial features in 2.5D
   images}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{75}},
Number = {{18}},
Pages = {{11059-11096}},
Month = {{SEP}},
Abstract = {{A robust technique for recognition of 3D faces which performs well with
   face images with various poses, expressions and occlusions. In this
   method, the face images represented in 3D mesh format are smoothed using
   trilinear interpolation and then converted to 2.5D image or range
   images. Nose-tip which is the most prominent feature on human face is
   detected first on the corner points selected by 3D Harris corner and
   curvedness at those corner points. K-Means clustering is applied to
   group those corner points in 2 groups. The cluster of points with larger
   curvedness values represents the possible locations of nose-tip.
   Nose-tip is finally localized using Mean-Gaussian curvature values of
   the prospective corner points in that cluster. Using the nose-tip
   location, other facial landmarks namely corners of the eyes and mouth
   are located and a facial graph is generated. The dimensionality of 2.5D
   feature space is that, depth values are stored at each (x, y) grid of
   the 2.5D image, so a 3D face image uses some function to map the depth
   value at any pixel position to the intensity with which that pixel will
   be displayed. Here finally extracted features for each subject is of
   dimensionality {[}1x21], taking into account the Euclidean distances in
   three dimensional form between each feature points detected
   automatically. Taking Euclidean distances between all pairs of landmark
   points as features, face images are classified using Multilayer
   Perceptron (MLP), as well as Support Vector Machines (SVM). Maximum
   recognition rates of 75 and 87.5 \% have been obtained in case of
   Bosphorus Databases, 62.5 and 87.5 \% in case of GavabDB databases, 75
   and 87.5 \% in case of Frav3D Databases by Multilayer Perceptron and
   Support Vector Machines respectively.}},
DOI = {{10.1007/s11042-015-2835-7}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000382679900012}},
}

@article{ ISI:000384777300010,
Author = {Mahmoudabadi, Hamid and Olsen, Michael J. and Todorovic, Sinisa},
Title = {{Efficient terrestrial laser scan segmentation exploiting data structure}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{119}},
Pages = {{135-150}},
Month = {{SEP}},
Abstract = {{New technologies such as lidar enable the rapid collection of massive
   datasets to model a 3D scene as a point cloud. However, while hardware
   technology continues to advance, processing 3D point clouds into
   informative models remains complex and time consuming. A common approach
   to increase processing efficiently is to segment the point cloud into
   smaller sections. This paper proposes a novel approach for point cloud
   segmentation using computer vision algorithms to analyze panoramic
   representations of individual laser scans. These panoramas can be
   quickly created using an inherent neighborhood structure that is
   established during the scanning process, which scans at fixed angular
   increments in a cylindrical or spherical coordinate system. In the
   proposed approach, a selected image segmentation algorithm is applied on
   several input layers exploiting this angular structure including laser
   intensity, range, normal vectors, and color information. These segments
   are then mapped back to the 3D point cloud so that modeling can be
   completed more efficiently. This approach does not depend on pre-defined
   mathematical models and consequently setting parameters for them. Unlike
   common geometrical point cloud segmentation methods, the proposed method
   employs the colorimetric and intensity data as another source of
   information. The proposed algorithm is demonstrated on several datasets
   encompassing variety of scenes and objects. Results show a very high
   perceptual (visual) level of segmentation and thereby the feasibility of
   the proposed algorithm. The proposed method is also more efficient
   compared to Random Sample Consensus (RANSAC), which is a common approach
   for point cloud segmentation. (C) 2016 International Society for
   Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.05.015}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000384777300010}},
}

@article{ ISI:000376708000002,
Author = {Alashkar, Taleb and Ben Amor, Boulbaba and Daoudi, Mohamed and Berretti,
   Stefano},
Title = {{A Grassmann framework for 4D facial shape analysis}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2016}},
Volume = {{57}},
Pages = {{21-30}},
Month = {{SEP}},
Abstract = {{In this paper, we investigate the contribution of dynamic evolution of
   3D faces to identity recognition. To this end, we adopt a subspace
   representation of the flow of curvature-maps computed on 3D facial
   frames of a sequence, after normalizing their pose. Such representation
   allows us to embody the shape as well as its temporal evolution within
   the same subspace representation. Dictionary learning and sparse coding
   over the space of fixed-dimensional subspaces, called Grassmann
   manifold, have been used to perform face recognition. We have conducted
   extensive experiments on the BU-4DFE dataset. The obtained results of
   the proposed approach provide promising results. (C) 2016 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.patcog.2016.03.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018
   Daoudi, Mohammed/H-5935-2013}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305
   Berretti, Stefano/0000-0003-1219-4386
   Daoudi, Mohammed/0000-0003-4219-7860}},
Unique-ID = {{ISI:000376708000002}},
}

@article{ ISI:000379266300013,
Author = {Quan, Wei and Matuszewski, Bogdan J. and Shark, Lik-Kwan},
Title = {{Statistical shape modelling for expression-invariant face analysis and
   recognition}},
Journal = {{PATTERN ANALYSIS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{19}},
Number = {{3}},
Pages = {{765-781}},
Month = {{AUG}},
Abstract = {{Paper introduces a 3-D shape representation scheme for automatic face
   analysis and identification, and demonstrates its invariance to facial
   expression. The core of this scheme lies on the combination of
   statistical shape modelling and non-rigid deformation matching. While
   the former matches 3-D faces with facial expression, the latter provides
   a low-dimensional feature vector that controls the deformation of model
   for matching the shape of new input, thereby enabling robust
   identification of 3-D faces. The proposed scheme is also able to handle
   the pose variation without large part of missing data. To assist the
   establishment of dense point correspondences, a modified
   free-form-deformation based on B-spline warping is applied with the help
   of extracted landmarks. The hybrid iterative closest point method is
   introduced for matching the models and new data. The feasibility and
   effectiveness of the proposed method was investigated using standard
   publicly available Gavab and BU-3DFE datasets, which contain faces with
   expression and pose changes. The performance of the system was compared
   with that of nine benchmark approaches. The experimental results
   demonstrate that the proposed scheme provides a competitive solution for
   face recognition.}},
DOI = {{10.1007/s10044-014-0439-x}},
ISSN = {{1433-7541}},
EISSN = {{1433-755X}},
Unique-ID = {{ISI:000379266300013}},
}

@article{ ISI:000378471300003,
Author = {Jiang, Hairong and Zhang, Ting and Wachs, Juan P. and Duerstock, Bradley
   S.},
Title = {{Enhanced control of a wheelchair-mounted robotic manipulator using 3-D
   vision and multimodal interaction}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2016}},
Volume = {{149}},
Pages = {{21-31}},
Month = {{AUG}},
Abstract = {{This paper presents a multiple-sensors, 3D vision-based, autonomous
   wheelchair-mounted robotic manipulator (WMRM). Two 3D sensors were
   employed: one for object recognition, and the other for recognizing body
   parts (face and hands). The goal is to recognize everyday items and
   automatically interact with them in an assistive fashion. For example,
   when a cereal box is recognized, it is grasped, poured in a bowl, and
   brought to the user. Daily objects (i.e. bowl and hat) were
   automatically detected and classified using a three-steps procedure: (1)
   remove background based on 3D information and find the point cloud of
   each object; (2) extract feature vectors for each segmented object from
   its 3D point cloud and its color image; and (3) classify feature vectors
   as objects after applying a nonlinear support vector machine (SVM). To
   retrieve specific objects, three user interface methods were adopted:
   voice-based, gesture-based, and hybrid commands. The presented system
   was tested using two common activities of daily living - feeding and
   dressing. The results revealed that an accuracy of 98.96\% is achieved
   for a dataset with twelve daily objects. The experimental results
   indicated that hybrid (gesture and speech) interaction outperforms any
   single modal interaction. (C) 2016 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.cviu.2016.03.015}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
ORCID-Numbers = {{Wachs, Juan/0000-0002-6425-5745
   Duerstock, Bradley/0000-0001-9535-2460}},
Unique-ID = {{ISI:000378471300003}},
}

@article{ ISI:000381843600009,
Author = {Ekizoglu, Oguzhan and Hocaoglu, Elif and Inci, Ercan and Can, Ismail
   Ozgur and Solmaz, Dilek and Aksoy, Sema and Buran, Cudi Ferat and Sayin,
   Ibrahim},
Title = {{Assessment of sex in a modern Turkish population using cranial
   anthropometric parameters}},
Journal = {{LEGAL MEDICINE}},
Year = {{2016}},
Volume = {{21}},
Pages = {{45-52}},
Month = {{JUL}},
Abstract = {{The utilization of radiological imaging methods in anthropometric
   studies is being expanded by the application of modern imaging methods,
   leading to a decrease in costs, a decrease in the time required for
   analysis and the ability to create three-dimensional images. This
   retrospective study investigated 400 patients within the 18-45-years age
   group (mean age: 30.7 +/- 11.2 years) using cranial computed tomography
   images. We measured 14 anthropometric parameters (basion-bregma height,
   basion-prosthion length, maximum cranial length and cranial base
   lengths, maximum cranial breadth, bizygomatic diameter, upper facial
   breadth, bimastoid diameter, orbital breadth, orbital length, biorbital
   breadth, interorbital breadth, foramen magnum breadth and foramen magnum
   length) of cranial measurements. The intra- and inter-observer
   repeatability and consistency were good. From the results of logistic
   regression analysis using morphometric measurements, the most
   conspicuous measurements in terms of dimorphism were maximum cranial
   length, bizygomatic diameter, basion-bregma height, and cranial base
   length. The most dimorphic structure was the bizygomatic diameter with
   an accuracy rate of 83\% in females and 77\% in males. In this study,
   87.5\% of females and 87.0\% of males were classified accurately by this
   model including four parameters with a sensitivity of 91.5\% and
   specificity of 85.0\%. In conclusion, CT cranial morphometric analysis
   may be reliable for the assessment of sex in the Turkish population and
   is recommended for comparison of data of modern populations with those
   of former populations. Additionally, cranial morphometric data that we
   obtained from modern Turkish population may reveal population specific
   data, which may help current criminal investigations and identification
   of disaster victims. (C) 2016 Elsevier Ireland Ltd. All rights reserved.}},
DOI = {{10.1016/j.legalmed.2016.06.001}},
ISSN = {{1344-6223}},
ORCID-Numbers = {{Ekizoglu, Oguzhan/0000-0002-0194-595X
   Buran, Ferat/0000-0002-7858-0194}},
Unique-ID = {{ISI:000381843600009}},
}

@article{ ISI:000380771500016,
Author = {Tanhuanpaa, Topi and Saarinen, Ninni and Kankare, Ville and Nurminen,
   Kimmo and Vastaranta, Mikko and Honkavaara, Eija and Karjalainen, Mika
   and Yu, Xiaowei and Holopainen, Markus and Hyyppa, Juha},
Title = {{Evaluating the Performance of High-Altitude Aerial Image-Based Digital
   Surface Models in Detecting Individual Tree Crowns in Mature Boreal
   Forests}},
Journal = {{FORESTS}},
Year = {{2016}},
Volume = {{7}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{Height models based on high-altitude aerial images provide a low-cost
   means of generating detailed 3D models of the forest canopy. In this
   study, the performance of these height models in the detection of
   individual trees was evaluated in a commercially managed boreal forest.
   Airborne digital stereo imagery (DSI) was captured from a flight
   altitude of 5 km with a ground sample distance of 50 cm and corresponds
   to regular national topographic airborne data capture programs operated
   in many countries. Tree tops were detected from smoothed canopy height
   models (CHM) using watershed segmentation. The relative amount of
   detected trees varied between 26\% and 140\%, and the RMSE of plot-level
   arithmetic mean height between 2.2 m and 3.1 m. Both the dominant tree
   species and the filter used for smoothing affected the results. Even
   though the spatial resolution of DSI-based CHM was sufficient, detecting
   individual trees from the data proved to be demanding because of the
   shading effect of the dominant trees and the limited amount of data from
   lower canopy levels and near the ground.}},
DOI = {{10.3390/f7070143}},
Article-Number = {{143}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Saarinen, Ninni/K-4296-2019
   Vastaranta, Mikko/K-9656-2018
   Karjalainen, Mika/E-3348-2017
   }},
ORCID-Numbers = {{Saarinen, Ninni/0000-0003-2730-8892
   Vastaranta, Mikko/0000-0001-6552-9122
   Karjalainen, Mika/0000-0003-4320-8007
   Tanhuanpaa, Topi/0000-0002-5509-6922
   Honkavaara, Eija/0000-0002-7236-2145
   Nurminen, Kimmo/0000-0001-8036-9446}},
Unique-ID = {{ISI:000380771500016}},
}

@article{ ISI:000379014500010,
Author = {Sutradhar, Alok and Park, Jaejong and Carrau, Diana and Nguyen, Tam H.
   and Miller, Michael J. and Paulino, Glaucio H.},
Title = {{Designing patient-specific 3D printed craniofacial implants using a
   novel topology optimization method}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{2016}},
Volume = {{54}},
Number = {{7}},
Pages = {{1123-1135}},
Month = {{JUL}},
Abstract = {{Large craniofacial defects require efficient bone replacements which
   should not only provide good aesthetics but also possess stable
   structural function. The proposed work uses a novel multiresolution
   topology optimization method to achieve the task. Using a compliance
   minimization objective, patient-specific bone replacement shapes can be
   designed for different clinical cases that ensure revival of efficient
   load transfer mechanisms in the mid-face. In this work, four clinical
   cases are introduced and their respective patient-specific designs are
   obtained using the proposed method. The optimized designs are then
   virtually inserted into the defect to visually inspect the viability of
   the design . Further, once the design is verified by the reconstructive
   surgeon, prototypes are fabricated using a 3D printer for validation.
   The robustness of the designs are mechanically tested by subjecting them
   to a physiological loading condition which mimics the masticatory
   activity. The full-field strain result through 3D image correlation and
   the finite element analysis implies that the solution can survive the
   maximum mastication of 120 lb. Also, the designs have the potential to
   restore the buttress system and provide the structural integrity. Using
   the topology optimization framework in designing the bone replacement
   shapes would deliver surgeons new alternatives for rather complicated
   mid-face reconstruction.}},
DOI = {{10.1007/s11517-015-1418-0}},
ISSN = {{0140-0118}},
EISSN = {{1741-0444}},
Unique-ID = {{ISI:000379014500010}},
}

@article{ ISI:000373271800001,
Author = {Hojris, Bo and Christensen, Sarah Christine Boesgaard and Albrechtsen,
   Hans-Jorgen and Smith, Christian and Dahlqvist, Mathis},
Title = {{A novel, optical, on-line bacteria sensor for monitoring drinking water
   quality}},
Journal = {{SCIENTIFIC REPORTS}},
Year = {{2016}},
Volume = {{6}},
Month = {{APR 4}},
Abstract = {{Today, microbial drinking water quality is monitored through either
   time-consuming laboratory methods or indirect on-line measurements.
   Results are thus either delayed or insufficient to support proactive
   action. A novel, optical, on-line bacteria sensor with a 10-minute time
   resolution has been developed. The sensor is based on 3D image
   recognition, and the obtained pictures are analyzed with algorithms
   considering 59 quantified image parameters. The sensor counts individual
   suspended particles and classifies them as either bacteria or abiotic
   particles. The technology is capable of distinguishing and quantifying
   bacteria and particles in pure and mixed suspensions, and the
   quantification correlates with total bacterial counts. Several field
   applications have demonstrated that the technology can monitor changes
   in the concentration of bacteria, and is thus well suited for rapid
   detection of critical conditions such as pollution events in drinking
   water.}},
DOI = {{10.1038/srep23935}},
Article-Number = {{23935}},
ISSN = {{2045-2322}},
ResearcherID-Numbers = {{Hojris, Bo/H-1350-2018
   Albrechtsen, Hans-Jorgen/J-1229-2014}},
ORCID-Numbers = {{Hojris, Bo/0000-0003-4129-2794
   Christensen, Sarah Christine Boesgaard/0000-0001-6183-6045
   Albrechtsen, Hans-Jorgen/0000-0003-3483-7709}},
Unique-ID = {{ISI:000373271800001}},
}

@article{ ISI:000422927300005,
Author = {Ahranjani, Behnaz Asadi and Shojaei, Bahador and Tootian, Zahra and
   Masoudifard, Madjid and Rostami, Amir},
Title = {{Anatomical, radiographical and computed tomographic study of the limbs
   skeleton of the Euphrates soft shell turtle (Rafetus euphraticus)}},
Journal = {{VETERINARY RESEARCH FORUM}},
Year = {{2016}},
Volume = {{7}},
Number = {{2}},
Pages = {{117-124}},
Month = {{SPR}},
Abstract = {{Euphrates turtle is the only soft shell turtle of Iran, and
   unfortunately is in danger of extinction due to multiple reasons.
   Imaging techniques, in addition to their importance in diagnosis of
   injuries to animals, have been used as non-invasive methods to provide
   normal anatomic views. A few studies have been conducted to understand
   body structure of the Euphrates turtle. Since there is only general
   information about the anatomy of turtle limbs, the normal skeleton of
   the Euphrates limbs was studied. For this purpose four adult Euphrates
   turtles were used. Digital radiographic examination was performed by
   computed radiographic (CR) in dorsoventral (DV) and lateral (L)
   positions. Spiral CT-scanning was done and 3D images of the bones were
   reconstructed for anatomical evaluation. For skeletal preparation, the
   skeleton was cleaned by a combination of boiling and mealworm methods
   and limbs' bones were examined anatomically. In the present study,
   simultaneous anatomic, radiographic and CT studies of bones in
   individual turtles made us possible to describe bones anatomically and
   provided comparable and complementary conditions to represent the
   abilities of the radiography and CT for better understanding of the
   anatomy. Arrangement and the number of carpal and tarsal bones are used
   in turtles' classification. Among the studied species, Euphrates turtle
   carpal and tarsal bones show the most similarities to the Apolone
   spinifera. (c) 2016 Urmia University. All rights reserved.}},
ISSN = {{2008-8140}},
EISSN = {{2322-3618}},
Unique-ID = {{ISI:000422927300005}},
}

@article{ ISI:000368511500001,
Author = {Mirshojaei, Seyedeh Fatemeh and Ahmadi, Amirhossein and Morales-Avila,
   Enrique and Ortiz-Reynoso, Mariana and Reyes-Perez, Horacio},
Title = {{Radiolabelled nanoparticles: novel classification of
   radiopharmaceuticals for molecular imaging of cancer}},
Journal = {{JOURNAL OF DRUG TARGETING}},
Year = {{2016}},
Volume = {{24}},
Number = {{2}},
Pages = {{91-101}},
Month = {{FEB 7}},
Abstract = {{Nanotechnology has been used for every single modality in the molecular
   imaging arena for imaging purposes. Synergic advantages can be explored
   when multiple molecular imaging modalities are combined with respect to
   single imaging modalities. Multifunctional nanoparticles have large
   surface areas, where multiple functional moieties can be incorporated,
   including ligands for site-specific targeting and radionuclides, which
   can be detected to create 3D images. Recently, radiolabeled
   nanoparticles with individual properties have attracted great interest
   regarding their use in multimodality tumor imaging. Multifunctional
   nanoparticles can combine diagnostic and therapeutic capabilities for
   both target-specific diagnosis and the treatment of a given disease. The
   future of nanomedicine lies in multifunctional nanoplatforms that
   combine the diagnostic ability and therapeutic effects using appropriate
   ligands, drugs, responses and technological devices, which together are
   collectively called theranostic drugs. Co-delivery of radiolabeled
   nanoparticles is useful in multifunctional molecular imaging areas
   because it comprises several advantages based on nanoparticles
   architecture, pharmacokinetics and pharmacodynamic properties.}},
DOI = {{10.3109/1061186X.2015.1048516}},
ISSN = {{1061-186X}},
EISSN = {{1029-2330}},
ResearcherID-Numbers = {{Ahmadi, Amirhossein/H-2136-2011
   }},
ORCID-Numbers = {{Ahmadi, Amirhossein/0000-0002-9737-3633
   Reyes-Perez, Horacio/0000-0001-9018-1105}},
Unique-ID = {{ISI:000368511500001}},
}

@article{ ISI:000383905800007,
Author = {de Jong, Markus A. and Wollstein, Andreas and Ruff, Clifford and
   Dunaway, David and Hysi, Pirro and Spector, Tim and Liu, Fan and
   Niessen, Wiro and Koudstaal, Maarten J. and Kayser, Manfred and Wolvius,
   Eppo B. and Bohringer, Stefan},
Title = {{An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}},
Journal = {{IEEE TRANSACTIONS ON IMAGE PROCESSING}},
Year = {{2016}},
Volume = {{25}},
Number = {{2}},
Pages = {{580-588}},
Month = {{FEB}},
Abstract = {{In this paper, we present a novel approach to automatic 3D facial
   landmarking using 2D Gabor wavelets. Our algorithm considers the face to
   be a surface and uses map projections to derive 2D features from raw
   data. Extracted features include texture, relief map, and
   transformations thereof. We extend an established 2D landmarking method
   for simultaneous evaluation of these data. The method is validated by
   performing landmarking experiments on two data sets using 21 landmarks
   and compared with an active shape model implementation. On average,
   landmarking error for our method was 1.9 mm, whereas the active shape
   model resulted in an average landmarking error of 2.3 mm. A second study
   investigating facial shape heritability in related individuals concludes
   that automatic landmarking is on par with manual landmarking for some
   landmarks. Our algorithm can be trained in 30 min to automatically
   landmark 3D facial data sets of any size, and allows for fast and robust
   landmarking of 3D faces.}},
DOI = {{10.1109/TIP.2015.2496183}},
ISSN = {{1057-7149}},
EISSN = {{1941-0042}},
ResearcherID-Numbers = {{Boehringer, Stefan/Y-2442-2018
   Liu, Fan/B-8833-2013
   }},
ORCID-Numbers = {{Boehringer, Stefan/0000-0001-9108-9212
   Liu, Fan/0000-0001-9241-8161
   Niessen, Wiro/0000-0002-5822-1995
   Dunaway, David/0000-0001-5063-9943}},
Unique-ID = {{ISI:000383905800007}},
}

@article{ ISI:000371787800087,
Author = {Alletto, Stefano and Abati, Davide and Serra, Giuseppe and Cucchiara,
   Rita},
Title = {{Exploring Architectural Details Through a Wearable Egocentric Vision
   Device}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Augmented user experiences in the cultural heritage domain are in
   increasing demand by the new digital native tourists of 21st century. In
   this paper, we propose a novel solution that aims at assisting the
   visitor during an outdoor tour of a cultural site using the unique first
   person perspective of wearable cameras. In particular, the approach
   exploits computer vision techniques to retrieve the details by proposing
   a robust descriptor based on the covariance of local features. Using a
   lightweight wearable board, the solution can localize the user with
   respect to the 3D point cloud of the historical landmark and provide him
   with information about the details at which he is currently looking.
   Experimental results validate the method both in terms of accuracy and
   computational effort. Furthermore, user evaluation based on real-world
   experiments shows that the proposal is deemed effective in enriching a
   cultural experience.}},
DOI = {{10.3390/s16020237}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000371787800087}},
}

@article{ ISI:000370350100005,
Author = {Ma, Lixia and Zheng, Guang and Eitel, Jan U. H. and Moskal, L. Monika
   and He, Wei and Huang, Huabing},
Title = {{Improved Salient Feature-Based Approach for Automatically Separating
   Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar
   Point Cloud Data of Forest Canopies}},
Journal = {{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{54}},
Number = {{2}},
Pages = {{679-696}},
Month = {{FEB}},
Abstract = {{Accurate separation of photosynthetic and nonphotosynthetic components
   in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a
   challenging but of key importance to understand the spatial distribution
   of the radiation regime, photosynthetic processes, and carbon and water
   exchanges of the forest canopy. The objective of this paper was to
   improve current methods for separating photosynthetic and
   nonphotosynthetic components in TLS data of forest canopies by adding
   two additional filters only based on its geometric information. By
   comparing the proposed approach with the eigenvalues plus color
   information-based method, we found that the proposed approach could
   effectively improve the overall producer's accuracy from 62.12\% to
   95.45\%, and the overall classification producer's accuracy would
   increase from 84.28\% to 97.80\% as the forest leaf area index (LAI)
   decreases from 4.15 to 3.13. In addition, variations in tree species had
   negligible effects on the final classification accuracy, as shown by the
   overall producer's accuracy for coniferous (93.09\%) and broadleaf
   (94.96\%) trees. To remove quantitatively the effects of the woody
   materials in a forest canopy for improving TLS-based LAI estimates, we
   also computed the ``woody-to-total area ratio{''} based on the
   classified linear class points from an individual tree. Automatic
   classification of the forest point cloud data set will facilitate the
   application of TLS on retrieving 3-D forest canopy structural
   parameters, including LAI and leaf and woody area ratios.}},
DOI = {{10.1109/TGRS.2015.2459716}},
ISSN = {{0196-2892}},
EISSN = {{1558-0644}},
ResearcherID-Numbers = {{Moskal, L. Monika/F-8715-2010
   }},
ORCID-Numbers = {{Moskal, L. Monika/0000-0003-1563-6506
   He, Wei/0000-0003-0779-2496}},
Unique-ID = {{ISI:000370350100005}},
}

@article{ ISI:000369200900006,
Author = {Shendryk, Iurii and Broich, Mark and Tulbure, Mirela G. and Alexandrov,
   Sergey V.},
Title = {{Bottom-up delineation of individual trees from full-waveform airborne
   laser scans in a structurally complex eucalypt forest}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2016}},
Volume = {{173}},
Pages = {{69-83}},
Month = {{FEB}},
Note = {{2014 ForestSAT Conference, Riva del Garda, ITALY, NOV 04-07, 2014}},
Abstract = {{Full-waveform airborne laser scanning (ALS) is a powerful tool for
   characterizing and monitoring forest structure over large areas at the
   individual tree level. Most of the existing ALS-based algorithms for
   individual tree delineation from the point cloud are top-down, which are
   accurate for delineating cone-shaped conifers, but have lower
   delineation accuracies over more structurally complex broad-leaf
   forests. Therefore, in this study we developed a new bottom-up algorithm
   for detecting trunks and delineating individual trees with complex
   shapes, such as eucalypts. Experiments were conducted in the largest
   river red gum forest in the world, located in the southeast of
   Australia, that experienced severe dieback over the past six decades.
   For detection of individual tree trunks, we used a novel approach based
   on conditional Euclidean distance clustering that takes advantage of
   spacing between laser returns. Overall, the algorithm developed in our
   study was able to detect up to 67\% of field-measured trees with
   diameter larger than or equal to 13 cm. By filtering ALS based on the
   intensity, return number and returned pulse width values, we were able
   to differentiate between woody and leaf tree components, thus improving
   the accuracy of tree trunk detections by 5\% as compared to non-filtered
   ALS. The detected trunks were used to seed random walks on graph
   algorithm for tree crown delineation. The accuracy of tree crown
   delineation for different ALS point cloud densities was assessed in
   terms of tree height and crown width and resulted in up to 68\% of
   field-measured trees being correctly delineated. The double increase in
   point density from similar to 12 points/m(2) to similar to 24
   points/m(2) resulted in tree trunk detection increase of 11\% (from 56\%
   to 67\%) and percentage of correctly delineated crowns increase of 13\%
   (from 55\% to 68\%). Our results confirm an algorithm that can be used
   to accurately delineate individual trees with complex structures (e.g.
   eucalypts and other broad leaves) and highlight the importance of
   full-waveform ALS for individual tree delineation. (C) 2015 Elsevier
   Inc. All rights reserved.}},
DOI = {{10.1016/j.rse.2015.11.008}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Tulbure, Mirela/B-3030-2012
   Tulbure, Mirela G/M-1212-2019
   }},
ORCID-Numbers = {{Tulbure, Mirela/0000-0003-1456-183X
   Tulbure, Mirela G/0000-0003-1456-183X
   Shendryk, Iurii/0000-0003-1657-1361}},
Unique-ID = {{ISI:000369200900006}},
}

@article{ ISI:000368956300004,
Author = {Sener, Emre and Mumcuoglu, Erkan U. and Hamcan, Salih},
Title = {{Bayesian segmentation of human facial tissue using 3D MR-CT information
   fusion, resolution enhancement and partial volume modelling}},
Journal = {{COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE}},
Year = {{2016}},
Volume = {{124}},
Pages = {{31-44}},
Month = {{FEB}},
Abstract = {{Background: Accurate segmentation of human head on medical images is an
   important process in a wide array of applications such as diagnosis,
   facial surgery planning, prosthesis design, and forensic identification.
   Objectives: In this study, a Bayesian method for segmentation of facial
   tissues is presented. Segmentation classes include muscle, bone, fat,
   air and skin.
   Methods: The method presented incorporates information fusion from
   multiple modalities, modelling of image resolution (measurement
   blurring), image noise, two priors helping to reduce noise and partial
   volume. Image resolution modelling employed facilitates resolution
   enhancement and superresolution capabilities during image segmentation.
   Regularization based on isotropic and directional Markov Random Field
   priors is integrated. The Bayesian model is solved iteratively yielding
   tissue class labels at every voxel of the image. Sub methods as
   variations of the main method are generated by using a combination of
   the models.
   Results: Testing of the sub-methods is performed on two patients using
   single modality three-dimensional (3D) image (magnetic resonance, MR or
   computerized tomography, CT) as well as registered MR-CT images with
   information fusion. Numerical, visual and statistical analyses of the
   methods are conducted. High segmentation accuracy values are obtained by
   the use of image resolution and partial volume models as well as
   information fusion from MR and CT images. The methods are also compared
   with our Bayesian segmentation method proposed in a previous study. The
   performance is found to be similar to our previous Bayesian approach,
   but the presented methods here eliminates ad hoc parameter tuning needed
   by the previous approach which is system and data acquisition setting
   dependent.
   Conclusions: The Bayesian approach presented provides resolution
   enhanced segmentation of very thin structures of the human head.
   Meanwhile, free parameters of the algorithm can be adjusted for
   different imaging systems and data acquisition settings in a more
   systematic way as compared with our previous study. (C) 2015 Elsevier
   Ireland Ltd. All rights reserved.}},
DOI = {{10.1016/j.cmpb.2015.10.009}},
ISSN = {{0169-2607}},
EISSN = {{1872-7565}},
ResearcherID-Numbers = {{Mumcuoglu, Erkan/B-5480-2012}},
Unique-ID = {{ISI:000368956300004}},
}

@article{ ISI:000367113200011,
Author = {Almansa, Julio and Salvat-Pujol, Francesc and Diaz-Londono, Gloria and
   Carnicer, Artur and Lallena, Antonio M. and Salvat, Francesc},
Title = {{PENGEOM-A general-purpose geometry package for Monte Carlo simulation of
   radiation transport in material systems defined by quadric surfaces}},
Journal = {{COMPUTER PHYSICS COMMUNICATIONS}},
Year = {{2016}},
Volume = {{199}},
Pages = {{102-113}},
Month = {{FEB}},
Abstract = {{The Fortran subroutine package PENGEOM provides a complete set of tools
   to handle quadric geometries in Monte Carlo simulations of radiation
   transport. The material structure where radiation propagates is assumed
   to consist of homogeneous bodies limited by quadric surfaces. The
   PENGEOM subroutines (a subset of the PENELOPE code) track particles
   through the material structure, independently of the details of the
   physics models adopted to describe the interactions. Although these
   subroutines are designed for detailed simulations of photon and electron
   transport, where all individual interactions are simulated sequentially,
   they can also be used in mixed (class II) schemes for simulating the
   transport of high-energy charged particles, where the effect of soft
   interactions is described by the random-hinge method. The definition of
   the geometry and the details of the tracking algorithm are tailored to
   optimize simulation speed. The use of fuzzy quadric surfaces minimizes
   the impact of round-off errors. The provided software includes a Java
   graphical user interface for editing and debugging the geometry
   definition file and for visualizing the material structure. Images of
   the structure are generated by using the tracking subroutines and,
   hence, they describe the geometry actually passed to the simulation
   code.
   Program summary
   Program title: Pengeom
   Catalogue identifier: AEYH\_v1\_0
   Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYH\_v1\_0.html
   Program obtainable from: CPC Program Library, Queen's University,
   Belfast, N. Ireland
   Licensing provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.html
   No. of lines in distributed program, including test data, etc.: 89390
   No. of bytes in distributed program, including test data, etc.: 5062646
   Distribution format: tar.gz
   Programming language: Fortran, Java.
   Computer: PC with Java Runtime Environment installed.
   Operating system: Windows, Linux.
   RAM: 210 MiB
   Classification: 21.1, 14.
   Nature of problem: The Fortran subroutines perform all geometry
   operations in Monte Carlo simulations of radiation transport with
   arbitrary interaction models. They track particles through material
   systems consisting of homogeneous bodies limited by quadric surfaces.
   Particles are moved in steps (free flights) of a given length, which is
   dictated by the simulation program, and are halted when they cross an
   interface between media of different compositions or when they enter
   selected bodies.
   Solution method: The pengeom subroutines are tailored to optimize
   simulation speed and accuracy. Fast tracking is accomplished by the use
   of quadric surfaces, which facilitate the calculation of ray
   intersections, and of modules (connected volumes limited by quadric
   surfaces) organized in a hierarchical structure. Optimal accuracy is
   obtained by considering fuzzy surfaces, with the aid of a simple
   algorithm that keeps control of multiple intersections of a ray and a
   surface. The Java GUI PenGeomJar provides a geometry toolbox; it allows
   building and debugging the geometry definition file, as well as
   visualizing the resulting geometry in two and three dimensions.
   Restrictions: By default pengeom can handle systems with up to 5000
   bodies and 10,000 surfaces. These numbers can be increased by editing
   the Fortran source file.
   Unusual features: All geometrical operations are performed internally.
   The connection between the steering main program and the tracking
   routines is through a Fortran module, which contains the state variables
   of the transported particle, and the input-output arguments of the
   subroutine step. Rendering of two- and three-dimensional images is
   performed by using the pengeom subroutines, so that displayed images
   correspond to the definitions passed to the simulation program.
   Additional comments: Java editor and viewer (PenGeomJar), geometry
   examples, translator to POV-Ray (TM) format, detailed manual. The
   Fortran subroutine package pengeom is part of the penelope code system
   {[}1].
   Running time: The running time much depends on the complexity of the
   material system. The most complicated example provided, phantom, an
   anthropomorphic phantom, has 264 surfaces and 169 bodies and modules,
   with different levels of grouping; the largest module contains 51
   daughters. The rendering of a 3D image of phantom with 1680x1050 pixels
   takes about 25 s (i.e., about 1.5 . 10(-5) seconds per ray) on an Intel
   Core 17-3520M CPU, with Windows 7 and subroutines compiled with
   gfortran.
   References:
   {[}1] F. Salvat, PENELOPE-2014: A Code System for Monte Carlo Simulation
   of Electron and Photon Transport, OECD/NEA Data Bank,
   Issy-les-Moulineaux, France, 2015. Available from
   http://www.nea.fr/lists/penelope.html.(C) 2015 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.cpc.2015.09.019}},
ISSN = {{0010-4655}},
EISSN = {{1879-2944}},
ResearcherID-Numbers = {{Carnicer, Artur/B-1442-2013
   Diaz-Londono, Gloria/W-2639-2018
   Salvat, Francesc/F-8255-2016
   }},
ORCID-Numbers = {{Carnicer, Artur/0000-0002-4936-5778
   Diaz-Londono, Gloria/0000-0002-3235-1193
   Salvat, Francesc/0000-0002-6162-8841
   Lallena Rojo, Antonio M./0000-0003-1962-6217}},
Unique-ID = {{ISI:000367113200011}},
}

@inproceedings{ ISI:000406771300100,
Author = {Pang, Guan and Neumann, Ulrich},
Book-Group-Author = {{IEEE}},
Title = {{3D Point Cloud Object Detection with Multi-View Convolutional Neural
   Network}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{585-590}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Efficient detection of three dimensional (3D) objects in point clouds is
   a challenging problem. Performing 3D descriptor matching or 3D
   scanning-window search with detector are both time-consuming due to the
   3-dimensional complexity. One solution is to project 3D point cloud into
   2D images and thus transform the 3D detection problem into 2D space, but
   projection at multiple viewpoints and rotations produce a large amount
   of 2D detection tasks, which limit the performance and complexity of the
   2D detection algorithm choice. We propose to use convolutional neural
   network (CNN) for the 2D detection task, because it can handle all
   viewpoints and rotations for the same class of object together, as well
   as predicting multiple classes of objects with the same network, without
   the need for individual detector for each object class. We further
   improve the detection efficiency by concatenating two extra levels of
   early rejection networks with binary outputs before the multi-class
   detection network. Experiments show that our method has competitive
   overall performance with at least one-order of magnitude speedup
   comparing with latest 3D point cloud detection methods.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771300100}},
}

@inproceedings{ ISI:000406771301004,
Author = {Tian, Guifen and Mori, Tatusya and Okuda, Yuji},
Book-Group-Author = {{IEEE}},
Title = {{Spoofing Detection for Embedded Face Recognition System using A Low Cost
   Stereo Camera}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{1017-1022}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Spoofing detection is essential for practical face recognition system.
   Based on the fact that genuine face has special geometric curvatures
   across surface, this paper brings forward an ultra-fast yet accurate
   spoofing detection approach using a low-cost stereo camera. To obtain
   curvatures, the three dimensional shapes of selected facial landmarks
   are analyzed, by fitting point cloud around each landmark to a specific
   partial face surface. Spoofing detection is then performed by evaluating
   curvatures of each landmark and integrating them together. Experiments
   verify that the approach is able to detect spoofed faces in printed
   photographs without or with various bending at FAR equal to 0.00\%.
   Meanwhile, genuine faces have a trivial opportunity to be falsely
   rejected: FRR is 0.59\% for near frontal faces and less than 5\% for
   faces with large varying poses. Detection time is 51 milliseconds when
   executed on a single processor {[}1] running at a clock frequency of
   266M Hz, this makes the detection very suitable for embedded face
   recognition system.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771301004}},
}

@inproceedings{ ISI:000406771301073,
Author = {Jhuang, Dong-Han and Lin, Daw-Tung and Tsai, Chi-Hung},
Book-Group-Author = {{IEEE}},
Title = {{Face Verification with Three-Dimensional Point Cloud by Using Deep
   Belief Networks}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{1430-1435}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Developing reliable and robust face verification systems has been a
   tough challenge in computer vision, for several decades. The variation
   in illumination and head pose may seriously inhibit the accuracy of
   two-dimensional face recognition. With the invention of a depth map
   sensor, more three-dimensional volume data can be processed to mitigate
   the problem associated with face verification. This paper presents a
   three-dimensional face verification approach that includes three phases.
   First, point cloud library is applied to estimate features such as
   normal vectors and principal curvatures of every point on a human face
   point cloud acquired from three-dimensional depth sensor. Next, we adopt
   deep belief networks to train the identification model using extracted
   features. Finally, face verification is accomplished by using the
   pre-trained deep belief networks to justify if new incoming face point
   cloud feature is the one we specified. The experimental results
   demonstrate that the proposed system performs exceptionally well with
   about 96.43\% verification accuracy.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771301073}},
}

@inproceedings{ ISI:000406771302059,
Author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni,
   Gerard},
Book-Group-Author = {{IEEE}},
Title = {{Expression Invariant 3D Face Modeling from an RGB-D Video}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{2362-2367}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{We aim to reconstruct an accurate neutral 3D face model from an RGB-D
   video in the presence of extreme expression changes. Since each depth
   frame, taken by a low-cost sensor, is noisy, point clouds from multiple
   frames can be registered and aggregated to build an accurate 3D model.
   However, direct aggregation of multiple data produces erroneous results
   in natural interaction (e.g., talking and showing expressions). We
   propose to analyze facial expression from an RGB frame and neutralize
   the corresponding 3D point cloud if needed. We first estimate the
   person's expression by fitting blend-shape coefficients using 2D facial
   landmarks for each frame and calculate an expression deformity
   (expression score). With the estimated expression score, we determine
   whether an input face is neutral or non-neutral. If the face is
   non-neutral, we proceed to neutralize the expression of the 3D point
   cloud in that frame. To neutralize the 3D point cloud of a face, we
   deform our generic 3D face model by applying the estimated blendshape
   coefficients, find displacement vectors from the deformed generic face
   to a neutral generic face, and apply the displacement vectors to the
   input 3D point cloud. After preprocessing frames in a video, we rank
   frames based on the expression scores and register the ranked frames
   into a single 3D model. Our system produces a neutral 3D face model in
   the presence of extreme expression changes even when neutral faces do
   not exist in the video.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771302059}},
}

@inproceedings{ ISI:000405706400151,
Author = {Seo, Masataka and Chen, Yen-Wei},
Editor = {{Wang, Y and An, J and Wang, L and Li, Q and Yan, G and Chang, Q}},
Title = {{Joint Subspace Learning for Reconstruction of 3D Facial Dynamic
   Expression from Single Image}},
Booktitle = {{2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016)}},
Year = {{2016}},
Pages = {{820-824}},
Note = {{9th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI), Datong, PEOPLES R CHINA, OCT
   15-17, 2016}},
Organization = {{IEEE; IEEE Engn Med \& Biol Soc; Taiyuan Univ Technol; E China Normal
   Univ}},
Abstract = {{Recently, the synthesis of 3D dynamic expressions has become an
   important concern in computer graphics, facial recognition, etc. In this
   study, we propose a regression based joint subspace learning method for
   the automatic synthesis of 3D dynamic expression images. This method
   synthesizes 3D dynamic expression images from a single 2D facial image.
   We use two subspaces (the view subspace and the frame subspace) to
   synthesize a 3D image. First, we use the view subspace to estimate
   multi-view facial images from a front image. Next, we construct a 3D
   image using the estimated multi-view facial images. Finally, we estimate
   the 3D images in different frames by using the frame subspace to
   synthesis 3D dynamic expression images. This approach is unlike the
   conventional joint subspace learning in which, the coefficients
   estimated by the input image are directly used for synthesis.
   Furthermore, we propose using textural information to improve the
   accuracy of synthesized images.}},
ISBN = {{978-1-5090-3710-0}},
Unique-ID = {{ISI:000405706400151}},
}

@inproceedings{ ISI:000405940800053,
Author = {Chellappa, Rama and Chen, Jun-Cheng and Ranjan, Rajeev and
   Sankaranarayanan, Swami and Kumar, Amit and Patel, Vishal M. and
   Castillo, Carlos D.},
Book-Group-Author = {{IEEE}},
Title = {{Towards the Design of an End-to-End Automated System for Image an
   Video-based Recognition}},
Booktitle = {{2016 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA)}},
Year = {{2016}},
Note = {{Information Theory and Applications Workshop, La Jolla, CA, JAN 31-FEB
   05, 2016}},
Abstract = {{Over many decades, researchers working in object recognition have longed
   for an end-to-end automated system that will simply accept 2D or 3D
   image or videos as inputs and output the labels of objects in the input
   data. Computer vision methods that use representations derived based on
   geometric, radiometric and neural considerations and statistical and
   structural matchers and artificial neural network-based methods where a
   multi-layer network learns the mapping from inputs to class labels have
   provided competing approaches for image recognition problems. Over the
   last four years, methods based on Deep Convolutional Neural Networks
   (DCNNs) have shown impressive performance improvements on object
   detection/recognition challenge problems. This has been made possible
   due to the availability of large annotated data, a better understanding
   of the non-linear mapping between image and class labels as well as the
   affordability of GPUs. In this paper, we present a brief history of
   developments in computer vision and artificial neural networks over the
   last forty years for the problem of image-based recognition. We then
   present the design details of a deep learning system for endto- end
   unconstrained face verification/ recognition. Some open issues regarding
   DCNNs for object recognition problems are then discussed. We caution the
   readers that the views expressed in this paper are from the authors and
   authors only!}},
ISBN = {{978-1-5090-2529-9}},
Unique-ID = {{ISI:000405940800053}},
}

@inproceedings{ ISI:000405512400082,
Author = {Wu, Zhuoran and Hou, Zhenjie and Zhang, Jian},
Editor = {{Xu, B}},
Title = {{Research on the 3D face recognition based on multi-class classifier with
   depth and point cloud data}},
Booktitle = {{PROCEEDINGS OF 2016 IEEE ADVANCED INFORMATION MANAGEMENT, COMMUNICATES,
   ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IMCEC 2016)}},
Year = {{2016}},
Pages = {{398-402}},
Note = {{IEEE Advanced Information Management, Communicates, Electronic and
   Automation Control Conference (IMCEC), Xian, PEOPLES R CHINA, OCT 03-05,
   2016}},
Organization = {{IEEE; IEEE Beijing Sect; Global Union Acad Sci \& Technol; Chongqing
   Global Union Acad Sci \& Technol; Xian Peihua Univ}},
Abstract = {{Human face recognition technology usually takes advantages of
   two-dimensional or three-dimensional data. Rising from 1980s,
   three-dimensional face recognition technology soon become one of the
   headed topic because of its admirable resistance to interference and
   more information compared with two-dimensional face recognition
   technology. The new 3D face model standardization algorithm presented in
   this article provides a solution to transfer the obtained face model to
   standardized CAND1DE-3 face model. The article also provides a new
   Bayesian classification model based on multi-class classifier, which
   could overcome the difficulty that ono-verse-one classifier has a low
   recognition rate when facing more than two people. The article conduct
   the comparison experiment based on the provided algorithm. According to
   the experiment, it could raise the face recognition rate efficiently
   when applying the standardization algorithm and training modeL}},
ISBN = {{978-1-4673-9613-4}},
Unique-ID = {{ISI:000405512400082}},
}

@inproceedings{ ISI:000401716400003,
Author = {Naveen, S. and Ahalya, R. K. and Moni, R. S.},
Book-Group-Author = {{IEEE}},
Title = {{Multimodal Face Recognition using Spectral Transformation by LBP and
   Polynomial Coefficients}},
Booktitle = {{PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION
   SYSTEMS AND NETWORKS (COMNET)}},
Series = {{International Conference on Communication Systems and Networks}},
Year = {{2016}},
Pages = {{13-17}},
Note = {{International Conference on Communication Systems and Networks (ComNet),
   Trivandrum, INDIA, JUL 21-23, 2016}},
Organization = {{IEEE}},
Abstract = {{This paper presents a multimodal face recognition using spectral
   transformation by Local Binary Pattern (LBP) and Polynomial
   Coefficients. Here 2D image and 3D image are combined to get multimodal
   face recognition. In this method a novel feature extraction is done
   using LBP and Polynomial Coefficients. Then these features are
   spectrally transformed using Discrete Fourier Transform (DFT). These
   spectrally transformed features extracted from texture image using the
   two methods are combined at the score level. Similarly this is done in
   depth image. Finally feature information from texture and depth are
   combined at the score level which gives better results than the
   individual results.}},
ISSN = {{2155-2487}},
ISBN = {{978-1-5090-3349-2}},
Unique-ID = {{ISI:000401716400003}},
}

@inproceedings{ ISI:000401510000148,
Author = {Amin, Rafiul and Shams, A. Farhan and Rahman, S. M. Mahbubur and
   Hatzinakos, Dimitrios},
Book-Group-Author = {{IEEE}},
Title = {{Evaluation of Discrimination Power of Facial Parts from 3D Point Cloud
   Data}},
Booktitle = {{2016 9TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING
   (ICECE)}},
Series = {{International Conference on Computer and Electrical Engineering ICCEE}},
Year = {{2016}},
Pages = {{602-605}},
Note = {{9th International Conference on Electrical and Computer Engineering
   (ICECE), Dhaka, BANGLADESH, DEC 20-22, 2016}},
Organization = {{Bangladesh Univ Engn \& Technol, Dept Elect \& Elect Engn; Inst Elect \&
   Elect Engineers; Energypac; PARADISE Cables ltd; BTCL; Summit Communicat
   Ltd; Dhaka Power Distribut Co Ltd}},
Abstract = {{Feature selection from facial regions is a well-known approach to
   increase the performance of 2D image-based face recognition systems. In
   case of 3D modality, the approach of region-based feature selection for
   face recognition is relatively new. In this context, this paper presents
   an approach to evaluate the discrimination power of different regions of
   a 3D facial surface for its potential use in face recognition systems.
   We propose the use of weighted average of unit normal vector on the
   facial surface as the feature for region-based face recognition from 3D
   point cloud data (PCD). The iterative closest point algorithm is
   employed for the registration of segmented regions of facial point
   clouds. A metric based on angular distance between normals is introduced
   to indicate the similarity between two surfaces of same facial region.
   Finally, the intra class correlation based discrimination score is
   formulated to find out the key facial regions such as the eyes, nose,
   and mouth that are significant while recognizing a person with facial
   surface PCD.}},
ISBN = {{978-1-5090-2963-1}},
ResearcherID-Numbers = {{Amin, Rafiul/L-8633-2019}},
Unique-ID = {{ISI:000401510000148}},
}

@inproceedings{ ISI:000400012304105,
Author = {Bolkart, Timo and Wuhrer, Stefanie},
Book-Group-Author = {{IEEE}},
Title = {{A Robust Multilinear Model Learning Framework for 3D Faces}},
Booktitle = {{2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2016}},
Pages = {{4911-4919}},
Note = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Seattle, WA, JUN 27-30, 2016}},
Organization = {{IEEE Comp Soc; Comp Vis Fdn}},
Abstract = {{Multilinear models are widely used to represent the statistical
   variations of 3D human faces as they decouple shape changes due to
   identity and expression. Existing methods to learn a multilinear face
   model degrade if not every person is captured in every expression, if
   face scans are noisy or partially occluded, if expressions are
   erroneously labeled, or if the vertex correspondence is inaccurate.
   These limitations impose requirements on the training data that
   disqualify large amounts of available 3D face data from being usable to
   learn a multilinear model. To overcome this, we introduce the first
   framework to robustly learn a multilinear model from 3D face databases
   with missing data, corrupt data, wrong semantic correspondence, and
   inaccurate vertex correspondence. To achieve this robustness to
   erroneous training data, our framework jointly learns a multilinear
   model and fixes the data. We evaluate our framework on two publicly
   available 3D face databases, and show that our framework achieves a data
   completion accuracy that is comparable to state-of-the-art tensor
   completion methods. Our method reconstructs corrupt data more accurately
   than state-of-the-art methods, and improves the quality of the learned
   model significantly for erroneously labeled expressions.}},
DOI = {{10.1109/CVPR.2016.531}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-8851-1}},
Unique-ID = {{ISI:000400012304105}},
}

@inproceedings{ ISI:000400688200019,
Author = {Starczewski, Janusz T. and Pabiasz, Sebastian and Vladymyrska, Natalia
   and Marvuglia, Antonino and Napoli, Christian and Wozniak, Marcin},
Editor = {{Rutkowski, L and Korytkowski, M and Scherer, R and Tadeusiewicz, R and Zadeh, LA and Zurada, JM}},
Title = {{Self Organizing Maps for 3D Face Understanding}},
Booktitle = {{ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING, (ICAISC 2016), PT II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9693}},
Pages = {{210-217}},
Note = {{15th International Conference on Artificial Intelligence and Soft
   Computing (ICAISC), Zakopane, POLAND, JUN 12-16, 2016}},
Organization = {{Polish Neural Network Soc; Univ Social Sci; Czestochowa Univ Technol,
   Inst Computat Intelligence}},
Abstract = {{Landmarks are unique points that can be located on every face. Facial
   landmarks typically recognized by people are correlated with
   anthropomorphic points. Our purpose is to employ in 3D face recognition
   such landmarks that are easy to interpret. Face understanding is
   construed as identification of face characteristic points with automatic
   labeling of them. In this paper, we apply methods based on Self
   Organizing Maps to understand 3D faces.}},
DOI = {{10.1007/978-3-319-39384-1\_19}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-39384-1}},
ResearcherID-Numbers = {{Marvuglia, Antonino/J-2595-2019
   Wozniak, Marcin/L-6640-2013
   }},
ORCID-Numbers = {{Wozniak, Marcin/0000-0002-9073-5347
   Starczewski, Janusz/0000-0003-4694-7868
   Napoli, Christian/0000-0002-3336-5853}},
Unique-ID = {{ISI:000400688200019}},
}

@inproceedings{ ISI:000388114601158,
Author = {Gevaert, Caroline and Persello, Claudio and Sliuzas, Richard and
   Vosselman, George},
Book-Group-Author = {{IEEE}},
Title = {{INTEGRATION OF 2D AND 3D FEATURES FROM UAV IMAGERY FOR INFORMAL
   SETTLEMENT CLASSIFICATION USING MULTIPLE KERNEL LEARNING}},
Booktitle = {{2016 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS)}},
Series = {{IEEE International Symposium on Geoscience and Remote Sensing IGARSS}},
Year = {{2016}},
Pages = {{1508-1511}},
Note = {{36th IEEE International Geoscience and Remote Sensing Symposium
   (IGARSS), Beijing, PEOPLES R CHINA, JUL 10-15, 2016}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers, Geoscience
   \& Remote Sensing Soc; NSSC}},
Abstract = {{Informal settlement upgrading projects require highresolution and
   up-to-date thematic maps in order to plan and design effective
   interventions. To this end, Unmanned Aerial Vehicles (UAVs) provide the
   opportunity to obtain very high resolution 2D orthomosaics and 3D point
   clouds where and when needed. The heterogeneous, dense structures which
   typically make up an informal settlement motivate the importance of
   integrating complex 2D and 3D features obtained from UAV data into a
   single classification problem. Multiple Kernel Learning (MKL) Support
   Vector Machines (SVMs) maintain the distinct characteristics of the
   different feature spaces by optimizing individual kernels for specific
   feature groups which are later combined into a single kernel used for
   classification. Both the kernel parameters and kernel weights can be
   optimized by considering the alignment between the kernel and an ideal
   kernel which would perfectly classify the samples. This paper
   demonstrates how extracting high-level features from both the 2D
   orthomosaic as well as the 3D point cloud (obtained by an UAV), and
   integrating them through a MKL approach, can obtain an Overall Accuracy
   of 90.29\%, a 4\% increase over the results obtained using single kernel
   methods.}},
DOI = {{10.1109/IGARSS.2016.7729385}},
ISSN = {{2153-6996}},
ISBN = {{978-1-5090-3332-4}},
ResearcherID-Numbers = {{Persello, Claudio/L-5713-2015
   Vosselman, George/D-3985-2009
   Gevaert, Caroline/H-6195-2019
   Sliuzas, Richard/K-5323-2013}},
ORCID-Numbers = {{Persello, Claudio/0000-0003-3742-5398
   Vosselman, George/0000-0001-8813-8028
   Sliuzas, Richard/0000-0001-5243-4431}},
Unique-ID = {{ISI:000388114601158}},
}

@inproceedings{ ISI:000393154600016,
Author = {Harikumar, A. and Bovolo, F. and Bruzzone, L.},
Editor = {{Bruzzone, L and Bovolo, F}},
Title = {{A novel approach to internal crown characterization for coniferous tree
   species classification}},
Booktitle = {{IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXII}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{10004}},
Note = {{Conference on Image and Signal Processing for Remote Sensing XXII,
   Edinburgh, SCOTLAND, SEP 26-28, 2016}},
Organization = {{SPIE}},
Abstract = {{The knowledge about individual trees in forest is highly beneficial in
   forest management. High density small foot- print multi-return airborne
   Light Detection and Ranging (LiDAR) data can provide a very accurate
   information about the structural properties of individual trees in
   forests. Every tree species has a unique set of crown structural
   characteristics that can be used for tree species classification. In
   this paper, we use both the internal and external crown structural
   information of a conifer tree crown, derived from a high density small
   foot-print multi-return LiDAR data acquisition for species
   classification. Considering the fact that branches are the major
   building blocks of a conifer tree crown, we obtain the internal crown
   structural information using a branch level analysis. The structure of
   each conifer branch is represented using clusters in the LiDAR point
   cloud. We propose the joint use of the k-means clustering and geometric
   shape fitting, on the LiDAR data projected onto a novel 3-dimensional
   space, to identify branch clusters. After mapping the identified
   clusters back to the original space, six internal geometric features are
   estimated using a branch-level analysis. The external crown
   characteristics are modeled by using six least correlated features based
   on cone fitting and convex hull. Species classification is performed
   using a sparse Support Vector Machines (sparse SVM) classifier.}},
DOI = {{10.1117/12.2241452}},
Article-Number = {{100040H}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0412-4; 978-1-5106-0413-1}},
Unique-ID = {{ISI:000393154600016}},
}

@inproceedings{ ISI:000390287300035,
Author = {Pan, Zhenglin and Polceanu, Mihai and Lisetti, Christine},
Editor = {{Traum, D and Swartout, W and Khooshabeh, P and Kopp, S and Scherer, S and Leuski, A}},
Title = {{On Constrained Local Model Feature Normalization for Facial Expression
   Recognition}},
Booktitle = {{INTELLIGENT VIRTUAL AGENTS, IVA 2016}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{10011}},
Pages = {{369-372}},
Note = {{16th International Conference on Intelligent Virtual Agents (IVA), Los
   Angeles, CA, SEP 20-23, 2016}},
Organization = {{Alelo; Springer; Univ So Calif, Inst Creat Technologies}},
Abstract = {{Real time user independent facial expression recognition is important
   for virtual agents but challenging. However, since in real time
   recognition users are not necessarily presenting all the emotions, some
   proposed methods are not applicable. In this paper, we present a new
   approach that instead of using the traditional base face normalization
   on whole face shapes, performs normalization on the point cloud of each
   landmark. The result shows that our method outperforms the other two
   when the user input does not contain all six universal emotions.}},
DOI = {{10.1007/978-3-319-47665-0\_35}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-47665-0; 978-3-319-47664-3}},
Unique-ID = {{ISI:000390287300035}},
}

@inproceedings{ ISI:000392743800047,
Author = {Boehm, J. and Bredif, M. and Gierlinger, T. and Kraemer, M. and
   Lindenbergh, R. and Liu, K. and Michel, F. and Sirmacek, B.},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND
   IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{301-307}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Current 3D data capturing as implemented on for example airborne or
   mobile laser scanning systems is able to efficiently sample the surface
   of a city by billions of unselective points during one working day. What
   is still difficult is to extract and visualize meaningful information
   hidden in these point clouds with the same efficiency. This is where the
   FP7 IQmulus project enters the scene. IQmulus is an interactive facility
   for processing and visualizing big spatial data. In this study the
   potential of IQmulus is demonstrated on a laser mobile mapping point
   cloud of 1 billion points sampling similar to 10 km of street
   environment in Toulouse, France. After the data is uploaded to the
   IQmulus Hadoop Distributed File System, a workflow is defined by the
   user consisting of retiling the data followed by a PCA driven local
   dimensionality analysis, which runs efficiently on the IQmulus cloud
   facility using a Spark implementation. Points scattering in 3 directions
   are clustered in the tree class, and are separated next into individual
   trees. Five hours of processing at the 12 node computing cluster results
   in the automatic identification of 4000+ urban trees. Visualization of
   the results in the IQmulus fat client helps users to appreciate the
   results, and developers to identify remaining flaws in the processing
   workflow.}},
DOI = {{10.5194/isprsarchives-XLI-B3-301-2016}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Boehm, Jan/K-2336-2012
   Bredif, Mathieu/T-3029-2018
   }},
ORCID-Numbers = {{Boehm, Jan/0000-0003-2190-0449
   Bredif, Mathieu/0000-0003-0228-1232
   Lindenbergh, Roderik/0000-0001-8655-5266}},
Unique-ID = {{ISI:000392743800047}},
}

@inproceedings{ ISI:000392743800052,
Author = {Moradi, A. and Satari, M. and Momeni, M.},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{INDIVIDUAL TREE OF URBAN FOREST EXTRACTION FROM VERY HIGH DENSITY LIDAR
   DATA}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{337-343}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Airborne LiDAR (Light Detection and Ranging) data have a high potential
   to provide 3D information from trees. Most proposed methods to extract
   individual trees detect points of tree top or bottom firstly and then
   using them as starting points in a segmentation algorithm. Hence, in
   these methods, the number and the locations of detected peak points
   heavily effect on the process of detecting individual trees. In this
   study, a new method is presented to extract individual tree segments
   using LiDAR points with 10cm point density. In this method, a two-step
   strategy is performed for the extraction of individual tree LiDAR
   points: finding deterministic segments of individual trees points and
   allocation of other LiDAR points based on these segments. This research
   is performed on two study areas in Zeebrugge, Bruges, Belgium (51.33
   degrees N, 3.20 degrees E). The accuracy assessment of this method
   showed that it could correctly classified 74.51\% of trees with 21.57\%
   and 3.92\% under- and over-segmentation errors respectively.}},
DOI = {{10.5194/isprsarchives-XLI-B3-337-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Momeni, Mehdi/0000-0003-3705-1787}},
Unique-ID = {{ISI:000392743800052}},
}

@inproceedings{ ISI:000392743800092,
Author = {Kadamen, Jayren and Sithole, George},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{AUTOMATICALLY DETERMINING SCALE WITHIN UNSTRUCTU RED POINT CLOUDS}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{617-624}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Three dimensional models obtained from imagery have an arbitrary scale
   and therefore have to be scaled. Automatically scaling these models
   requires the detection of objects in these models which can be
   computationally intensive. Real-time object detection may pose problems
   for applications such as indoor navigation. This investigation poses the
   idea that relational cues, specifically height ratios, within indoor
   environments may offer an easier means to obtain scales for models
   created using imagery. The investigation aimed to show two things, (a)
   that the size of objects, especially the height off ground is consistent
   within an environment, and (b) that based on this consistency, objects
   can be identified and their general size used to scale a model. To test
   the idea a hypothesis is first tested on a terrestrial lidar scan of an
   indoor environment. Later as a proof of concept the same test is applied
   to a model created using imagery. The most notable finding was that the
   detection of objects can be more readily done by studying the ratio
   between the dimensions of objects that have their dimensions defined by
   human physiology. For example the dimensions of desks and chairs are
   related to the height of an average person. In the test, the difference
   between generalised and actual dimensions of objects were assessed. A
   maximum difference of 3.96\% (2.93cm) was observed from automated
   scaling. By analysing the ratio between the heights (distance from the
   floor) of the tops of objects in a room, identification was also
   achieved.}},
DOI = {{10.5194/isprsarchives-XLI-B3-617-2016}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000392743800092}},
}

@inproceedings{ ISI:000392750100114,
Author = {Zhang, Zongliang and Li, Jonathan and Li, Xin and Lin, Yangbin and
   Zhang, Shanxin and Wang, Cheng},
Editor = {{Halounova, L and Safar, V and Toth, CK and Karas, J and Huadong, G and Haala, N and Habib, A and Reinartz, P and Tang, X and Li, J and Armenakis, C and Grenzdorffer, G and LeRoux, P and Stylianidis, S and Blasi, R and Menard, M and Dufourmount, H and Li, Z}},
Title = {{A FAST METHOD FOR MEASURING THE SIMILARITY BETWEEN 3D MODEL AND 3D POINT
   CLOUD}},
Booktitle = {{XXIII ISPRS Congress, Commission I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B1}},
Pages = {{725-728}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{This paper proposes a fast method for measuring the partial Similarity
   between 3D Model and 3D point Cloud (SimMC). It is crucial to measure
   SimMC for many point cloud-related applications such as 3D object
   retrieval and inverse procedural modelling. In our proposed method, the
   surface area of model and the Distance from Model to point Cloud
   (DistMC) are exploited as measurements to calculate SimMC. Here, DistMC
   is defined as the weighted distance of the distances between points
   sampled from model and point cloud Similarly, Distance from point Cloud
   to Model (DistCM) is defined as the average distance of the distances
   between points in point cloud and model. In order to reduce huge
   computational burdens brought by calculation of DistCM in some
   traditional methods, we define SimMC as the ratio of weighted surface
   area of model to DistMC. Compared to those traditional SimMC measuring
   methods that are only able to measure global similarity, our method is
   capable of measuring partial similarity by employing distance-weighted
   strategy. Moreover, our method is able to be faster than other partial
   similarity assessment methods. We demonstrate the superiority of our
   method both on synthetic data and laser scanning data.}},
DOI = {{10.5194/isprsarchives-XLI-B1-725-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhang, Zongliang/0000-0002-0175-4299}},
Unique-ID = {{ISI:000392750100114}},
}

@inproceedings{ ISI:000392739800107,
Author = {Zhou, K. and Gorte, B. and Zlatanova, S.},
Editor = {{Halounova, L and Safar, V and Remondino, F and Hodac, J and Pavelka, K and Shortis, M and Rinaudo, F and Scaioni, M and Boehm, J and RiekeZapp, D}},
Title = {{EXPLORING REGULARITIES FOR IMPROVING FACADE RECONSTRUCTION FROM POINT
   CLOUDS}},
Booktitle = {{XXIII ISPRS Congress, Commission V}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B5}},
Pages = {{749-755}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{(Semi)-automatic facade reconstruction from terrestrial LiDAR point
   clouds is often affected by both quality of point cloud itself and
   imperfectness of object recognition algorithms. In this paper, we employ
   regularities, which exist on facades, to mitigate these problems. For
   example, doors, windows and balconies often have orthogonal and parallel
   boundaries. Many windows are constructed with the same shape. They may
   be arranged at the same lines and distance intervals, so do different
   windows. By identifying regularities among objects with relatively poor
   quality, these can be applied to calibrate the objects and improve their
   quality. The paper focuses on the regularities among the windows, which
   is the majority of objects on the wall. Regularities are classified into
   three categories: within an individual window, among similar windows and
   among different windows. Nine cases are specified as a reference for
   exploration. A hierarchical clustering method is employed to identify
   and apply regularities in a feature space, where regularities can be
   identified from clusters. To find the corresponding features in the nine
   cases of regularities, two phases are distinguished for similar and
   different windows. In the first phase, ICP (iterative closest points) is
   used to identify groups of similar windows. The registered points and a
   number of transformation matrices are used to identify and apply
   regularities among similar windows. In the second phase, features are
   extracted from the boundaries of the different windows. When applying
   regularities by relocating windows, the connections, called chains,
   established among the similar windows in the first phase are preserved.
   To test the performance of the algorithms, two datasets from terrestrial
   LiDAR point clouds are used. Both show good effects on the reconstructed
   model, while still matching with original point cloud, preventing over
   or under-regularization.}},
DOI = {{10.5194/isprsarchives-XLI-B5-749-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Gorte, Bernardus/0000-0001-8953-6394}},
Unique-ID = {{ISI:000392739800107}},
}

@inproceedings{ ISI:000392266900056,
Author = {Oh, Jaesung and Bae, Hyoin and Lim, Jeongsoo and Oh, Jun-Ho},
Book-Group-Author = {{IEEE}},
Title = {{Development of Autonomous Laser Toning System based on Vision
   Recognition and Robot Manipulator}},
Booktitle = {{2016 6TH IEEE INTERNATIONAL CONFERENCE ON BIOMEDICAL ROBOTICS AND
   BIOMECHATRONICS (BIOROB)}},
Series = {{Proceedings of the IEEE RAS-EMBS International Conference on Biomedical
   Robotics and Biomechatronics}},
Year = {{2016}},
Pages = {{317-322}},
Note = {{6th IEEE International Conference on Biomedical Robotics and
   Biomechatronics (BioRob), SINGAPORE, JUN 26-29, 2016}},
Organization = {{IEEE}},
Abstract = {{In this paper, the design, implementation, and operation method of the
   autonomous laser toning system are proposed, which is called as MELON (
   Manipulator for Effective Laser tONing). The system can recognize the
   accurate treatment points from the 3D point cloud data obtained with the
   camera, and it is possible to emit the laser at the desired position and
   orientation repeatedly, precisely, and accurately using intuitive
   differential inverse kinematics of the robot manipulator. The
   feasibility test of the MELON is conducted by using a plaster cast of a
   woman's head, and then, we find that the manipulator has a workspace to
   cover the entire face of the human inductively and distribution of the
   laser emission is homogeneous on the face. Therefore, we find the
   possibility of the autonomous laser toning using MELON.}},
ISSN = {{2155-1782}},
ISBN = {{978-1-5090-3287-7}},
Unique-ID = {{ISI:000392266900056}},
}

@inproceedings{ ISI:000391534900098,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal},
Editor = {{Liew, AWC and Lovell, B and Fookes, C and Zhou, J and Gao, Y and Blumenstein, M and Wang, Z}},
Title = {{Towards Large-scale 3D Face Recognition}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND
   APPLICATIONS (DICTA)}},
Year = {{2016}},
Pages = {{682-689}},
Note = {{International Conference on Digital Image Computing - Techniques and
   Applications (DICTA), Gold Coast, AUSTRALIA, NOV 30-DEC 02, 2016}},
Organization = {{Australian Govt, Dept Defence, Defence Sci \& Technol Grp; IAPR; Canon
   Informat Syst Res Australia; IEEE; Griffith Univ; APRS}},
Abstract = {{3D face recognition holds great promise in achieving robustness to pose,
   expressions and occlusions. However, 3D face recognition algorithms are
   still far behind their 2D counterparts due to the lack of large-scale
   datasets. We present a model based algorithm for 3D face recognition and
   test its performance by combining two large public datasets of 3D faces.
   We propose a Fully Convolutional Deep Network (FCDN) to initialize our
   algorithm. Reliable seed points are then extracted from each 3D face by
   evolving level set curves with a single curvature dependent adaptive
   speed function. We then establish dense correspondence between the faces
   in the training set by matching the surface around the seed points on a
   template face to the ones on the target faces. A morphable model is then
   fitted to probe faces and face recognition is performed by matching the
   parameters of the probe and gallery faces. Our algorithm achieves state
   of the art landmark localization results. Face recognition results on
   the combined FRGCv2 and Bosphorus datasets show that our method is
   affective in recognizing query faces with real world variations in pose
   and expression, and with occlusion and missing data despite a huge
   gallery. Comparing results of individual and combined datasets show that
   the recognition accuracy drops when the size of the gallery increases.}},
ISBN = {{978-1-5090-2896-2}},
Unique-ID = {{ISI:000391534900098}},
}

@article{ ISI:000391852100014,
Author = {Meng, Ting Wei and Choi, Gary Pui-Tung and Lui, Lok Ming},
Title = {{TEMPO: Feature-Endowed Teichmiiller Extremal Mappings of Point Clouds}},
Journal = {{SIAM JOURNAL ON IMAGING SCIENCES}},
Year = {{2016}},
Volume = {{9}},
Number = {{4}},
Pages = {{1922-1962}},
Abstract = {{In recent decades, the use of three-dimensional point clouds has been
   widespread in the computer industry. The development of techniques for
   analyzing point clouds is increasingly important. In particular, mapping
   of point clouds has been a challenging problem. In this paper, we
   develop a discrete analogue of the Teichmfiller extremal mappings, which
   guarantees uniform conformality distortions on point cloud surfaces.
   Based on the discrete analogue, we propose a novel method called TEMPO
   for computing Teichmfiller extremal mappings between feature-endowed
   point clouds. Using our proposed method, the Teichmfiller metric is
   introduced for evaluating the dissimilarity of point clouds.
   Consequently, our algorithm enables accurate recognition and
   classification of point clouds. Experimental results demonstrate the
   effectiveness of our proposed method.}},
DOI = {{10.1137/15M1049117}},
ISSN = {{1936-4954}},
Unique-ID = {{ISI:000391852100014}},
}

@inproceedings{ ISI:000391015300024,
Author = {Parmehr, Ebadat G. and Amati, Marco and Fraser, Clive S.},
Editor = {{Halounova, L and Sunar, F and Potuckova, M and Patkova, L and Yoshimura, M and Soergel, U}},
Title = {{MAPPING URBAN TREE CANOPY COVER USING FUSED AIRBORNE LIDAR AND SATELLITE
   IMAGERY DATA}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION VII}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{3}},
Number = {{7}},
Pages = {{181-186}},
Note = {{23rd ISPRS Congress, Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Urban green spaces, particularly urban trees, play a key role in
   enhancing the liveability of cities. The availability of accurate and
   up-to-date maps of tree canopy cover is important for sustainable
   development of urban green spaces. LiDAR point clouds are widely used
   for the mapping of buildings and trees, and several LiDAR point cloud
   classification techniques have been proposed for automatic mapping.
   However, the effectiveness of point cloud classification techniques for
   automated tree extraction from LiDAR data can be impacted to the point
   of failure by the complexity of tree canopy shapes in urban areas.
   Multispectral imagery, which provides complementary information to LiDAR
   data, can improve point cloud classification quality. This paper
   proposes a reliable method for the extraction of tree canopy cover from
   fused LiDAR point cloud and multispectral satellite imagery data. The
   proposed method initially associates each LiDAR point with spectral
   information from the co-registered satellite imagery data. It calculates
   the normalised difference vegetation index (NDVI) value for each LiDAR
   point and corrects tree points which have been misclassified as
   buildings. Then, region growing of tree points, taking the NDVI value
   into account, is applied. Finally, the LiDAR points classified as tree
   points are utilised to generate a canopy cover map. The performance of
   the proposed tree canopy cover mapping method is experimentally
   evaluated on a data set of airborne LiDAR and WorldView 2 imagery
   covering a suburb in Melbourne, Australia.}},
DOI = {{10.5194/isprsannals-III-7-181-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Amati, Marco/0000-0002-9600-5572}},
Unique-ID = {{ISI:000391015300024}},
}

@inproceedings{ ISI:000390841700083,
Author = {Torkhani, Ghada and Ladgham, Anis and Mansouri, Mohamed Nejib and Sakly,
   Anis},
Book-Group-Author = {{IEEE}},
Title = {{Gabor-SVM Applied to 3D-2D Deformed Mesh Model}},
Booktitle = {{2016 2ND INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)}},
Year = {{2016}},
Pages = {{447-452}},
Note = {{2nd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP), Monastir, TUNISIA, MAR 21-23, 2016}},
Organization = {{IEEE; IEEE Tunisia Sect; ATMS Lab; ATSI; IEEE Explore; EMB; ENIS Sch;
   Telecom Paris; Supelec; CESBIO; Telecom SudParis; ENIT; Univ Paris Sud;
   IEEE Signal Proc Soc Tunisia Chapter; ISAAM Inst; Minist Higher Educ
   Res; IEEE EMP Tunisia Chapter; Novartis Company}},
Abstract = {{We propose a robust method for 3D face recognition using 3D to 2D
   modeling and facial curvatures detection. The 3D2D algorithm permits to
   transform 3D images into 3D triangular mesh, then the mesh model is
   deformed and fitted to the 2D space in order to obtain a 2D smoother
   mesh. Then, we apply Gabor wavelets to the deformed model in order to
   exploit surface curves in the detection of salient face features. The
   classification of the final Gabor facial model is performed using the
   support vector machines (SVM). To demonstrate the quality of our
   technique, we give some experiments using the 3D AJMAL faces database.
   The experimental results prove that the proposed method is able to give
   a good recognition quality and a high accuracy rate.}},
ISBN = {{978-1-4673-8526-8}},
Unique-ID = {{ISI:000390841700083}},
}

@inproceedings{ ISI:000390841200037,
Author = {Koppen, W. P. and Christmas, W. J. and Crouch, D. J. M. and Bodmer, W.
   F. and Kittler, J. V.},
Editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun, J}},
Title = {{Extending Non-negative Matrix Factorisation to 3D Registered Data}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2016}},
Note = {{9th International Conference on Biometrics (ICB), Halmstad Univ,
   Halmstad, SWEDEN, JUN 13-16, 2016}},
Organization = {{IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB;
   Safran Ident \& Secur; EU Horizon 2020 Project IDENT; Speed Ident AB;
   Cognitec}},
Abstract = {{The use of non-negative matrix factorisation (NMF) on 2D face images has
   been shown to result in sparse feature vectors that encode for local
   patches on the face, and thus provides a statistically justified
   approach to learning parts from wholes. However successful on 2D images,
   the method has so far not been extended to 3D images. The main reason
   for this is that 3D space is a continuum and so it is not apparent how
   to represent 3D coordinates in a non-negative fashion. This work
   compares different non-negative representations for spatial coordinates,
   and demonstrates that not all non-negative representations are suitable.
   We analyse the representational properties that make NMF a successful
   method to learn sparse 3D facial features. Using our proposed
   representation, the factorisation results in sparse and interpretable
   facial features.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-5090-1869-7}},
Unique-ID = {{ISI:000390841200037}},
}

@inproceedings{ ISI:000390841200018,
Author = {Zhang, Jinjin and Huang, Di and Wang, Yunhong and Sun, Jia},
Editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun, J}},
Title = {{Lock3DFace: A Large-Scale Database of Low-Cost Kinect 3D Faces}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2016}},
Note = {{9th International Conference on Biometrics (ICB), Halmstad Univ,
   Halmstad, SWEDEN, JUN 13-16, 2016}},
Organization = {{IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB;
   Safran Ident \& Secur; EU Horizon 2020 Project IDENT; Speed Ident AB;
   Cognitec}},
Abstract = {{In this paper, we present a large-scale database consisting of low cost
   Kinect 3D face videos, namely Lock3DFace, for 3D face analysis,
   particularly for 3D Face Recognition (FR). To the best of our knowledge,
   Lock3DFace is currently the largest low cost 3D face database for public
   academic use. The 3D samples are highly noisy and contain a diversity of
   variations in expression, pose, occlusion, time lapse, and their
   corresponding texture and near infrared channels have changes in
   lighting condition and radiation intensity, allowing for evaluating FR
   methods in complex situations. Furthermore, based on Lock3DFace, we
   design the standard experimental protocol for low-cost 3D FR, and give
   the baseline performance of individual subsets belonging to different
   scenarios for fair comparison in the future.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-5090-1869-7}},
Unique-ID = {{ISI:000390841200018}},
}

@inproceedings{ ISI:000390023100094,
Author = {Adriana Echeagaray-Patron, B. and Kober, Vitaly},
Editor = {{Tescher, AG}},
Title = {{Face recognition based on matching of local features on 3D dynamic range
   sequences}},
Booktitle = {{APPLICATIONS OF DIGITAL IMAGE PROCESSING XXXIX}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9971}},
Note = {{Conference on Applications of Digital Image Processing XXXIX, San Diego,
   CA, AUG 29-SEP 01, 2016}},
Organization = {{SPIE}},
Abstract = {{3D face recognition has attracted attention in the last decade due to
   improvement of technology of 3D image acquisition and its wide range of
   applications such as access control, surveillance, human-computer
   interaction and biometric identification systems. Most research on 3D
   face recognition has focused on analysis of 3D still data. In this work,
   a new method for face recognition using dynamic 3D range sequences is
   proposed. Experimental results are presented and discussed using 3D
   sequences in the presence of pose variation. The performance of the
   proposed method is compared with that of conventional face recognition
   algorithms based on descriptors.}},
DOI = {{10.1117/12.2236355}},
Article-Number = {{997131}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0333-2; 978-1-5106-0334-9}},
Unique-ID = {{ISI:000390023100094}},
}

@inproceedings{ ISI:000390311500021,
Author = {Hakobyan, Hayk and Hakobyan, Robert and Aslanyan, Koryun},
Editor = {{Shahbazian, E and Rogova, G}},
Title = {{Human Identification Using Virtual 3D Imaging to Control Border Crossing}},
Booktitle = {{MEETING SECURITY CHALLENGES THROUGH DATA ANALYTICS AND DECISION SUPPORT}},
Series = {{Nato Science for Peace and Security Series D-Information and
   Communication Security}},
Year = {{2016}},
Volume = {{47}},
Pages = {{226-230}},
Note = {{Advanced Research Workshop (ARW) on Meeting Security Challenges through
   Data Analytics and Decision Support, Aghveran, ARMENIA, JUN 01-05, 2015}},
Organization = {{NATO Secur Through Sci Programme}},
Abstract = {{Human identification is an important aspect of border crossing.
   Identification makes it possible to recognize criminals or unauthorized
   entities and to prevent illegal border crossing. However, the
   identification process should remain comfortable and convenient for
   authorized entities. In this paper, we present an efficient face
   recognition system based on 2 cameras, which obtains a 3D image by
   processing data from camera video streams. Existing identification
   systems are also discussed.}},
DOI = {{10.3233/978-1-61499-716-0-226}},
ISSN = {{1874-6268}},
ISBN = {{978-1-61499-716-0; 978-1-61499-715-3}},
Unique-ID = {{ISI:000390311500021}},
}

@inproceedings{ ISI:000387187800290,
Author = {Liu, Shuming and Chen, Xiaopeng and Fan, Di and Chen, Xu and Meng, Fei
   and Huang, Qiang},
Book-Group-Author = {{IEEE}},
Title = {{3D Smiling Facial Expression Recognition Based on SVM}},
Booktitle = {{2016 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION}},
Year = {{2016}},
Pages = {{1661-1666}},
Note = {{IEEE International Conference on Mechatronics and Automation, Harbin,
   PEOPLES R CHINA, AUG 07-10, 2016}},
Organization = {{IEEE}},
Abstract = {{Using Kinect acquired RGB-D image to obtain a face feature parameters
   and three-dimensional coordinates of the characteristic parameters, and
   to select the characteristic parameter Facial by Candide-3 model, and
   feature extraction and normalization. Smile face expression data
   collection through Kinect, SVM collected to smiley face data classify
   and output the result of recognition, and the results compared with
   two-dimensional image of smiling face expression recognition results.
   Experimental results show that three-dimensional image of smiling face
   expression recognition accuracy than the two-dimensional image of
   smiling face. This research has important significance for the research
   and application of facial expression recognition technology.}},
ISBN = {{978-1-5090-2396-7}},
Unique-ID = {{ISI:000387187800290}},
}

@inproceedings{ ISI:000389381200037,
Author = {Trung Truong and Ngoc Ly},
Editor = {{Nguyen, NT and Trawinski, B and Fujita, H and Hong, TP}},
Title = {{Building the Facial Expressions Recognition System Based on RGB-D Images
   in High Performance}},
Booktitle = {{Intelligent Information and Database Systems, ACIIDS 2016, Pt II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9622}},
Pages = {{377-387}},
Note = {{8th Asian Conference on Intelligent Information and Database Systems
   (ACIIDS), Da Nang, VIETNAM, MAR 14-16, 2016}},
Organization = {{Vietnam Korea Friendship Informat Technol Coll; Wroclaw Univ Technol;
   IEEE SMC Tech Comm Computat Collect Intelligence; Bina Nusantara Univ;
   Ton Duc Thang Univ; Quang Binh Univ}},
Abstract = {{In this paper, we propose a novel idea for automatic facial expression
   analysis with the aim of resolving the existing challenges in 2D images.
   The subtle combination of the geometry-based method with the
   appearance-based features in depth and color images contributes to
   increasing in distinguishable features among various facial expressions.
   Particular functions are utilised to calculate the correlation between
   expressions in order to determine the exact facial expression. Our
   approach consists of a sequence of steps including estimating the normal
   vector of facial surface, then extracting the geometric features such as
   the orientation of normal vector in the point cloud. The useful color
   information is known as LBP. According to the result of the experiment,
   we demonstrate that the effective fusion scheme of texture and shape
   feature on color and depth images. In comparison with the non fusion
   scheme, our fusion scheme has resulted in the increase of recognition
   under low and high illuminated light, about 19.84\% and 1.59\%,
   respectively.}},
DOI = {{10.1007/978-3-662-49390-8\_37}},
ISSN = {{0302-9743}},
ISBN = {{978-3-662-49390-8; 978-3-662-49389-2}},
ORCID-Numbers = {{Truong, Quang Trung/0000-0002-6242-2191}},
Unique-ID = {{ISI:000389381200037}},
}

@article{ ISI:000385343000017,
Author = {Abd Rahman, Siti Zaharah and Abdullah, Siti Norul Huda Sheikh and Hao,
   Lim Eng and Abdulameer, Mohammed Hasan and Zamani, Nazri Ahmad and
   Darus, Mohammad Zaharudin A.},
Title = {{MAPPING 2D TO 3D FORENSIC FACIAL RECOGNITION VIA BIO-INSPIRED ACTIVE
   APPEARANCE MODEL}},
Journal = {{JURNAL TEKNOLOGI}},
Year = {{2016}},
Volume = {{78}},
Number = {{2-2}},
Pages = {{121-129}},
Abstract = {{This research done is to solve the problems faced by digital forensic
   analysts in identifying a suspect captured on their CCTV. Identifying
   the suspect through the CCTV video footage is a very challenging task
   for them as it involves tedious rounds of processes to match the facial
   information in the video footage to a set of suspect's images. The
   biggest problem faced by digital forensic analysis is modeling 2D model
   extracted from CCTV video as the model does not provide enough
   information to carry out the identification process. Problems occur when
   a suspect in the video is not facing the camera, the image extracted is
   the side image of the suspect and it is difficult to make a matching
   with portrait image in the database. There are also many factors that
   contribute to the process of extracting facial information from a video
   to be difficult, such as low-quality video. Through 2D to 3D image model
   mapping, any partial face information that is incomplete can be matched
   more efficiently with 3D data by rotating it to matched position. The
   first methodology in this research is data collection; any data obtained
   through video recorder. Then, the video will be converted into an image.
   Images are used to develop the Active Appearance Model (the 2D face
   model is AAM) 2D and AAM 3D. AAM is used as an input for learning and
   testing process involving three classifiers, which are Random Forest,
   Support Vector Machine (SVM), and Neural Networks classifier. The
   experimental results show that the 3D model is more suitable for use in
   face recognition as the percentage of the recognition is higher compared
   with the 2D model.}},
ISSN = {{0127-9696}},
EISSN = {{2180-3722}},
ORCID-Numbers = {{Rahman, Syed Ziaur/0000-0002-3460-1993}},
Unique-ID = {{ISI:000385343000017}},
}

@inproceedings{ ISI:000385794300020,
Author = {Khatiwada, Bikalpa and Budge, Scott E.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Three-dimensional image reconstruction using bundle adjustment applied
   to multiple texel images}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXI}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9832}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Baltimore,
   MD, APR 19-20, 2016}},
Organization = {{SPIE}},
Abstract = {{The importance of creating 3D imagery is increasing and has many
   applications in the field of disaster response, digital elevation
   models, object recognition, and cultural heritage. Several methods have
   been proposed to register texel images, which consist of fused lidar and
   digital imagery. The previous methods were limited to registering up to
   two texel images or multiple texel swaths having only one strip of lidar
   data per swath. One area of focus still remains to register multiple
   texel images to create a 3D model.
   The process of creating true 3D images using multiple texel images is
   described. The texel camera fuses the 2D digital image and calibrated 3D
   lidar data to form a texel image. The images are then taken from several
   perspectives and registered. The advantage of using multiple full frame
   texel images over 3D- or 2D-only methods is that there will be better
   registration between images because of the overlapping 3D points as well
   as 2D texture used in the joint registration process. The individual
   position and rotation mapping to a common world coordinate frame is
   calculated for each image and optimized. The proposed methods
   incorporate bundle adjustment for jointly optimizing the registration of
   multiple images. Sparsity is exploited as there is a lack of interaction
   between parameters of different cameras. Examples of the 3D model are
   shown and analyzed for numerical accuracy.}},
DOI = {{10.1117/12.2223259}},
Article-Number = {{UNSP 98320S}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0073-7}},
ORCID-Numbers = {{Budge, Scott/0000-0002-6138-3602}},
Unique-ID = {{ISI:000385794300020}},
}

@inproceedings{ ISI:000385794300011,
Author = {Magruder, Lori A. and Leigh, Holly W. and Soderlund, Alexander and
   Clymer, Bradley and Baer, Jessica and Neuenschwander, Amy L.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Automated feature extraction for 3-dimensional point clouds}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXI}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9832}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Baltimore,
   MD, APR 19-20, 2016}},
Organization = {{SPIE}},
Abstract = {{Light detection and ranging (LIDAR) technology offers the capability to
   rapidly capture high-resolution, 3-dimensional surface data with
   centimeter-level accuracy for a large variety of applications. Due to
   the foliage-penetrating properties of LIDAR systems, these geospatial
   data sets can detect ground surfaces beneath trees, enabling the
   production of high-fidelity bare earth elevation models. Precise
   characterization of the ground surface allows for identification of
   terrain and non-terrain points within the point cloud, and facilitates
   further discernment between natural and man-made objects based solely on
   structural aspects and relative neighboring parameterizations. A
   framework is presented here for automated extraction of natural and
   man-made features that does not rely on coincident ortho-imagery or
   point RGB attributes. The TEXAS (Terrain EXtraction And Segmentation)
   algorithm is used first to generate a bare earth surface from a lidar
   survey, which is then used to classify points as terrain or non-terrain.
   Further classifications are assigned at the point level by leveraging
   local spatial information. Similarly classed points are then clustered
   together into regions to identify individual features. Descriptions of
   the spatial attributes of each region are generated, resulting in the
   identification of individual tree locations, forest extents, building
   footprints, and 3-dimensional building shapes, among others. Results of
   the fully-automated feature extraction algorithm are then compared to
   ground truth to assess completeness and accuracy of the methodology.}},
DOI = {{10.1117/12.2223845}},
Article-Number = {{UNSP 98320F}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0073-7}},
Unique-ID = {{ISI:000385794300011}},
}

@inproceedings{ ISI:000384248300039,
Author = {Ding, Yifu and Tavolara, Thomas and Cheng, Keith},
Editor = {{Gurcan, MN and Madabhushi, A}},
Title = {{Automated Detection of Retinal Cell Nuclei in 3D Micro-CT Images of
   Zebrafish using Support Vector Machine Classification}},
Booktitle = {{MEDICAL IMAGING 2016: DIGITAL PATHOLOGY}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9791}},
Note = {{Conference on Medical Imaging - Digital Pathology, San Diego, CA, MAR
   02-03, 2016}},
Organization = {{SPIE; Modus Med Devices Inc; Bruker; Poco Graphite; ImXPAD}},
Abstract = {{Our group is developing a method to examine biological specimens in
   cellular detail using synchrotron microCT. The method can acquire 3D
   images of tissue at micrometer-scale resolutions, allowing for
   individual cell types to be visualized in the context of the entire
   specimen. For model organism research, this tool will enable the rapid
   characterization of tissue architecture and cellular morphology from
   every organ system. This characterization is critical for proposed and
   ongoing ``phenome{''} projects that aim to phenotype whole-organism
   mutants and diseased tissues from different organisms including humans.
   With the envisioned collection of hundreds to thousands of images for a
   phenome project, it is important to develop quantitative image analysis
   tools for the automated scoring of organism phenotypes across organ
   systems. Here we present a first step towards that goal, demonstrating
   the use of support vector machines (SVM) in detecting retinal cell
   nuclei in 3D images of wild-type zebrafish. In addition, we apply the
   SVM classifier on a mutant zebrafish to examine whether SVMs can be used
   to capture phenotypic differences in these images. The long-term goal of
   this work is to allow cellular and tissue morphology to be characterized
   quantitatively for many organ systems, at the level of the
   whole-organism.}},
DOI = {{10.1117/12.2216940}},
Article-Number = {{97911A}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0026-3}},
ORCID-Numbers = {{Ding, Yifu/0000-0002-4629-5858}},
Unique-ID = {{ISI:000384248300039}},
}

@inproceedings{ ISI:000381427400036,
Author = {Naveen, S. and Moni, R. S.},
Editor = {{Berretti, S and Thampi, SM and Srivastava, PR}},
Title = {{Contourlet and Fourier Transform Features Based 3D Face Recognition
   System}},
Booktitle = {{INTELLIGENT SYSTEMS TECHNOLOGIES AND APPLICATIONS, VOL 1}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2016}},
Volume = {{384}},
Pages = {{411-425}},
Note = {{International Symposium on Intelligent Systems Technologies and
   Applications (ISTA), SCMS Grp Inst, SCMS Sch Engn \& Technol, Kochi,
   INDIA, AUG 10-13, 2015}},
Abstract = {{Human face recognition based on geometrical structure has been an area
   of interest among researchers for the past few decades especially in
   pattern recognition. 3D Face recognition systems are of interest in this
   context. The main advantage of 3D Face recognition is the availability
   of geometrical information of the face structure which is more or less
   unique for a subject. This paper focuses on the problems of person
   identification using 3D Face data. Use of unregistered 3D Face data for
   feature extraction significantly increases the operational speed of the
   system with huge database enrollment. In this work, unregistered Face
   data, i.e. both texture and depth is fed to a classifier in spectral
   representations of the same data. 2-D Discrete Contourlet Transform and
   2-D Discrete Fourier Transform is used here for the spectral
   representation which forms the feature matrix. Fusion of texture and
   depth statistical information of face is proposed in this paper since
   the individual schemes are of lower performance. Application of
   statistical method seems to degrade the performance of the system when
   applied to texture data and was effective in the case of depth data.
   Fusion of the matching scores proves that the recognition accuracy can
   be improved significantly by fusion of scores of multiple
   representations. FRAV3D database is used for testing the algorithm.}},
DOI = {{10.1007/978-3-319-23036-8\_36}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-23036-8; 978-3-319-23035-1}},
Unique-ID = {{ISI:000381427400036}},
}

@inproceedings{ ISI:000378122900082,
Author = {Lenz, Marcel and Krug, Robin and Welp, Hubert and Schmieder, Kirsten and
   Hofmann, Martin R.},
Editor = {{Izatt, JA and Fujimoto, JG and Tuchin, VV}},
Title = {{Ex vivo brain tumor analysis using Spectroscopic Optical Coherence
   Tomography}},
Booktitle = {{OPTICAL COHERENCE TOMOGRAPHY AND COHERENCE DOMAIN OPTICAL METHODS IN
   BIOMEDICINE XX}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9697}},
Note = {{Conference on Optical Coherence Tomography and Coherence Domain Optical
   Methods in Biomedicine XX, San Francisco, CA, FEB 15-17, 2016}},
Organization = {{SPIE}},
Abstract = {{A big challenge during neurosurgeries is to distinguish between healthy
   tissue and cancerous tissue, but currently a suitable non-invasive real
   time imaging modality is not available. Optical Coherence Tomography
   (OCT) is a potential technique for such a modality. OCT has a
   penetration depth of 1-2 mm and a resolution of 1-15 mu m which is
   sufficient to illustrate structural differences between healthy tissue
   and brain tumor. Therefore, we investigated gray and white matter of
   healthy central nervous system and meningioma samples with a Spectral
   Domain OCT System (Thorlabs Callisto). Additional OCT images were
   generated after paraffin embedding and after the samples were cut into
   10 mu m thin slices for histological investigation with a bright field
   microscope. All samples were stained with Hematoxylin and Eosin. In all
   cases B-scans and 3D images were made. Furthermore, a camera image of
   the investigated area was made by the built-in video camera of our OCT
   system. For orientation, the backsides of all samples were marked with
   blue ink. The structural differences between healthy tissue and
   meningioma samples were most pronounced directly after removal. After
   paraffin embedding these differences diminished. A correlation between
   OCT en face images and microscopy images can be seen. In order to
   increase contrast, post processing algorithms were applied. Hence we
   employed Spectroscopic OCT, pattern recognition algorithms and machine
   learning algorithms such as k-means Clustering and Principal Component
   Analysis.}},
DOI = {{10.1117/12.2214704}},
Article-Number = {{96973D}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-931-3}},
Unique-ID = {{ISI:000378122900082}},
}

@article{ ISI:000370679800010,
Author = {Kristoffersen, Miklas S. and Dueholm, Jacob V. and Gade, Rikke and
   Moeslund, Thomas B.},
Title = {{Pedestrian Counting with Occlusion Handling Using Stereo Thermal Cameras}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{The number of pedestrians walking the streets or gathered in public
   spaces is a valuable piece of information for shop owners, city
   governments, event organizers and many others. However, automatic
   counting that takes place day and night is challenging due to changing
   lighting conditions and the complexity of scenes with many people
   occluding one another. To address these challenges, this paper
   introduces the use of a stereo thermal camera setup for pedestrian
   counting. We investigate the reconstruction of 3D points in a pedestrian
   street with two thermal cameras and propose an algorithm for pedestrian
   counting based on clustering and tracking of the 3D point clouds. The
   method is tested on two five-minute video sequences captured at a public
   event with a moderate density of pedestrians and heavy occlusions. The
   counting performance is compared to the manually annotated ground truth
   and shows success rates of 95.4\% and 99.1\% for the two sequences.}},
DOI = {{10.3390/s16010062}},
Article-Number = {{62}},
ISSN = {{1424-8220}},
ORCID-Numbers = {{Gade, Rikke/0000-0002-8016-2426
   Kristoffersen, Miklas Strom/0000-0002-1409-2618}},
Unique-ID = {{ISI:000370679800010}},
}

@article{ ISI:000369518500015,
Author = {Ouamane, A. and Belahcene, M. and Benakcha, A. and Bourennane, S. and
   Taleb-Ahmed, A.},
Title = {{Robust multimodal 2D and 3D face authentication using local feature
   fusion}},
Journal = {{SIGNAL IMAGE AND VIDEO PROCESSING}},
Year = {{2016}},
Volume = {{10}},
Number = {{1}},
Pages = {{129-137}},
Month = {{JAN}},
Abstract = {{In this work, we present a robust face authentication approach merging
   multiple descriptors and exploiting both 3D and 2D information. First,
   we correct the heads rotation in 3D by iterative closest point
   algorithm, followed by an efficient preprocessing phase. Then, we
   extract different features namely: multi-scale local binary patterns
   (MSLBP), novel statistical local features (SLF), Gabor wavelets, and
   scale invariant feature transform (SIFT). The principal component
   analysis followed by enhanced fisher linear discriminant model is used
   for dimensionality reduction and classification. Finally, fusion at the
   score level is carried out using two-class support vector machines.
   Extensive experiments are conducted on the CASIA 3D faces database. The
   evaluation of individual descriptors clearly showed the superiority of
   the proposed SLF features. In addition, applying the (3D + 2D)
   multimodal score level fusion, the best result is obtained by combining
   the SLF with the MSLBP + SIFT descriptor yielding in an equal error rate
   of 0.98\% and a recognition rate of RR = 97.22 \%.}},
DOI = {{10.1007/s11760-014-0712-x}},
ISSN = {{1863-1703}},
EISSN = {{1863-1711}},
Unique-ID = {{ISI:000369518500015}},
}

@article{ ISI:000368000100006,
Author = {Tang, Pingbo and Chen, Gaoyun and Shen, Zhenglai},
Title = {{A Spatial-Context-Based Approach for Automated Spatial Change Analysis
   of Piece-Wise Linear Building Elements}},
Journal = {{COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING}},
Year = {{2016}},
Volume = {{31}},
Number = {{1}},
Pages = {{65-80}},
Month = {{JAN}},
Abstract = {{Changes of designs and construction plans often cause propagative design
   modifications, tedious construction coordination, cascading effects of
   errors, reworks, and delays in project management. Among various
   building elements, those having piece-wise linear geometries (i.e.,
   connected straight line segments), such as connected straight sections
   of ducts in mechanical, electrical, and plumbing systems, frequently
   undergo spatial changes in response to the changes of their
   surroundings. On the other hand, the piece-wise linear geometries pose
   challenges to analyzing and controlling changes in construction and
   facility management. State-of-the-art 3D change detection algorithms
   often face ambiguities about which points belong to which objects when
   piece-wise linear object are spacked in small spaces. This article
   examines a spatial-context-based framework that uses spatial
   relationships between piece-wise linear building elements (ducts in this
   article) to enable fast and reliable association of 3D data with ducts
   in as-designed models for supporting reliable change analysis. Three
   case studies showed that this framework outperformed a conventional
   change detection method, and could handle large dislocations of
   piece-wise linear elements and occlusions.}},
DOI = {{10.1111/mice.12174}},
ISSN = {{1093-9687}},
EISSN = {{1467-8667}},
Unique-ID = {{ISI:000368000100006}},
}

@article{ ISI:000367856500018,
Author = {Bellil, Wajdi and Brahim, Hajer and Ben Amar, Chokri},
Title = {{Gappy wavelet neural network for 3D occluded faces: detection and
   recognition}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{75}},
Number = {{1}},
Pages = {{365-380}},
Month = {{JAN}},
Abstract = {{The first handicap in 3D faces recognizing under unconstrained problem
   is the largest variability of the visual aspect when we use various
   sources. This great variability complicates the task of identifying
   persons from their 3D facial scans and it is the most reason that bring
   to face detection and recognition of the major problems in pattern
   recognition fields, biometrics and computer vision. We propose a new 3D
   face identification and recognition method based on Gappy Wavelet Neural
   Network (GWNN) that is able to provide better accuracy in the presence
   of facial occlusions. The proposed approach consists of three steps: the
   first step is face detection. The second step is to identify and remove
   occlusions. Occluded regions detection is done by considering that
   occlusions can be defined as local face deformations. These deformations
   are detected by a comparison between the input facial test wavelet
   coefficients and wavelet coefficients of generic face model formed by
   the mean data base faces. They are beneficial for neighborhood
   relationships between pixels rotation, dilation and translation
   invariant. Then, occluded regions are refined by removing wavelet
   coefficient above a certain threshold. Finally, the last stage of
   processing and retrieving is made based on wavelet neural network to
   recognize and to restore 3D occluded regions that gathers the most. The
   experimental results on this challenging database demonstrate that the
   proposed approach improves recognition rate performance from 93.57 to
   99.45 \% which represents a competitive result compared to the state of
   the art.}},
DOI = {{10.1007/s11042-014-2294-6}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000367856500018}},
}

@article{ ISI:000367181400024,
Author = {Westphalen, Antonio C. and Noworolski, Susan M. and Harisinghani, Mukesh
   and Jhaveri, Kartik S. and Raman, Steve S. and Rosenkrantz, Andrew B.
   and Wang, Zhen J. and Zagoria, Ronald J. and Kurhanewicz, John},
Title = {{High-Resolution 3-T Endorectal Prostate MRI: A Multireader Study of
   Radiologist Preference and Perceived Interpretive Quality of 2D and 3D
   T2-Weighted Fast Spin-Echo MR Images}},
Journal = {{AMERICAN JOURNAL OF ROENTGENOLOGY}},
Year = {{2016}},
Volume = {{206}},
Number = {{1}},
Pages = {{86-91}},
Month = {{JAN}},
Abstract = {{OBJECTIVE. The goal of this study was to compare the perceived quality
   of 3-T axial T2-weighted high-resolution 2D and high-resolution 3D fast
   spin-echo (FSE) endorectal MR images of the prostate.
   MATERIALS AND METHODS. Six radiologists independently reviewed paired
   3-T axial T2-weighted high-resolution 2D and 3D FSE endorectal MR images
   of the prostates of 85 men in two sessions. In the first session (n =
   85), each reader selected his or her preferred images; in the second
   session (n = 28), they determined their confidence in tumor
   identification and compared the depiction of the prostatic anatomy,
   tumor conspicuity, and subjective intrinsic image quality of images. A
   meta-analysis using a random-effects model, logistic regression, and the
   paired Wilcoxon rank-sum test were used for statistical analyses.
   RESULTS. Three readers preferred the 2D acquisition (67-89\%), and the
   other three preferred the 3D images (70-80\%). The option for one of the
   techniques was not associated with any of the predictor variables. The
   2D FSE images were significantly sharper than 3D FSE (p < 0.001) and
   significantly more likely to exhibit other (nonmotion) artifacts (p =
   0.002). No other statistically significant differences were found.
   CONCLUSION. Our results suggest that there are strong individual
   preferences for the 2D or 3D FSE MR images, but there was a wide
   variability among radiologists. There were differences in image quality
   (image sharpness and presence of artifacts not related to motion) but
   not in the sequences' ability to delineate the glandular anatomy and
   depict a cancerous tumor.}},
DOI = {{10.2214/AJR.14.14065}},
ISSN = {{0361-803X}},
EISSN = {{1546-3141}},
ORCID-Numbers = {{Zagoria, Ronald/0000-0001-6926-4627}},
Unique-ID = {{ISI:000367181400024}},
}

@inproceedings{ ISI:000400012301070,
Author = {Hackel, Timo and Wegner, Jan D. and Schindler, Konrad},
Book-Group-Author = {{IEEE}},
Title = {{Contour detection in unstructured 3D point clouds}},
Booktitle = {{2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2016}},
Pages = {{1610-1618}},
Note = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Seattle, WA, JUN 27-30, 2016}},
Organization = {{IEEE Comp Soc; Comp Vis Fdn}},
Abstract = {{We describe a method to automatically detect contours, i.e. lines along
   which the surface orientation sharply changes, in large-scale outdoor
   point clouds. Contours are important intermediate features for
   structuring point clouds and converting them into high-quality surface
   or solid models, and are extensively used in graphics and mapping
   applications. Yet, detecting them in unstructured, inhomogeneous point
   clouds turns out to be surprisingly difficult, and existing line
   detection algorithms largely fail. We approach contour extraction as a
   two-stage discriminative learning problem. In the first stage, a contour
   score for each individual point is predicted with a binary classifier,
   using a set of features extracted from the point's neighborhood. The
   contour scores serve as a basis to construct an overcomplete graph of
   candidate contours. The second stage selects an optimal set of contours
   from the candidates. This amounts to a further binary classification in
   a higher-order MRF, whose cliques encode a preference for connected
   contours and penalize loose ends. The method can handle point clouds >
   10(7) points in a couple of minutes, and vastly outperforms a baseline
   that performs Canny-style edge detection on a range image representation
   of the point cloud.}},
DOI = {{10.1109/CVPR.2016.178}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-8851-1}},
Unique-ID = {{ISI:000400012301070}},
}

@article{ ISI:000367827600010,
Author = {Omelina, L. and Jansen, B. and Bonnechere, B. and Oravec, M. and
   Pavlovicova, Jarmila and Jan, S. Van Sint},
Title = {{Interaction Detection with Depth Sensing and Body Tracking Cameras in
   Physical Rehabilitation}},
Journal = {{METHODS OF INFORMATION IN MEDICINE}},
Year = {{2016}},
Volume = {{55}},
Number = {{1}},
Pages = {{70-78}},
Abstract = {{Introduction: This article is part of the Focus Theme of Methods of
   Information in Medicine on ``Methodologies, Models and Algorithms for
   Patients Rehabilitation{''}.
   Objectives: This paper presents a camera based method for identifying
   the patient and detecting interactions between the patient and the
   therapist during therapy. Detecting interactions helps to discriminate
   between active and passive motion of the patient as well as to estimate
   the accuracy of the skeletal data.
   Methods: Continuous face recognition is used to detect, recognize and
   track the patient with other people in the scene (e.g. the therapist, or
   a clinician). We use a method based on local binary patterns (LBP).
   After identifying users in the scene we identify interactions between
   the patient and other people. We use a depth map/point cloud for
   estimating the distance between two people. Our method uses the
   association of depth regions to user identities and computes the minimal
   distance between the regions.
   Results: Our results show state-of-the-art performance of real-time face
   recognition using low-resolution images that is sufficient to use in
   adaptive systems. Our proposed approach for detecting interactions shows
   91.9\% overall recognition accuracy what is sufficient for applications
   in the context of serious games. We also discuss limitations of the
   proposed method as well as general limitations of using depth cameras
   for serious games.
   Conclusions: We introduced a new method for frame-by-frame automated
   identification of the patient and labeling reliable sequences of the
   patient's data recorded during rehabilitation (games). Our method
   improves automated rehabilitation systems by detecting the identity of
   the patient as well as of the therapist and by detecting the distance
   between both over time.}},
DOI = {{10.3414/ME14-01-0120}},
ISSN = {{0026-1270}},
ORCID-Numbers = {{Van Sint Jan, Serge/0000-0002-3478-171X}},
Unique-ID = {{ISI:000367827600010}},
}

@article{ ISI:000365838200007,
Author = {Fehr, Duc and Beksi, William J. and Zermas, Dimitris and
   Papanikolopoulos, Nikolaos},
Title = {{Covariance based point cloud descriptors for object detection and
   recognition}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2016}},
Volume = {{142}},
Pages = {{80-93}},
Month = {{JAN}},
Abstract = {{Processing 3D point cloud data is of primary interest in many areas of
   computer vision, including object grasping, robot navigation, and object
   recognition. The introduction of affordable RGB-D sensors has created a
   great interest in the computer vision community towards developing
   efficient algorithms for point cloud processing. Previously, capturing a
   point cloud required expensive specialized sensors such as lasers or
   dedicated range imaging devices; now, range data is readily available
   from low-cost sensors that provide easily extractable point clouds from
   a depth map. From here, an interesting challenge is to find different
   objects in the point cloud. Various descriptors have been introduced to
   match features in a point cloud. Cheap sensors are not necessarily
   designed to produce precise measurements, which means that the data is
   not as accurate as a point cloud provided from a laser or a dedicated
   range finder. Although some feature descriptors have been shown to be
   successful in recognizing objects from point clouds, there still exists
   opportunities for improvement. The aim of this paper is to introduce
   techniques from other fields, such as image processing, into 3D point
   cloud processing in order to improve rendering, classification, and
   recognition. Covariances have proven to be a success not only in image
   processing, but in other domains as well. This work develops the
   application of covariances in conjunction with 3D point cloud data. (C)
   2015 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.cviu.2015.06.008}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
ORCID-Numbers = {{Fehr, Duc/0000-0001-8541-914X}},
Unique-ID = {{ISI:000365838200007}},
}

@article{ ISI:000368942000004,
Author = {Bargoti, Suchet and Underwood, James P. and Nieto, Juan I. and
   Sukkarieh, Salah},
Title = {{A Pipeline for Trunk Detection in Trellis Structured Apple Orchards}},
Journal = {{JOURNAL OF FIELD ROBOTICS}},
Year = {{2015}},
Volume = {{32}},
Number = {{8, SI}},
Pages = {{1075-1094}},
Month = {{DEC}},
Note = {{9th International Conference on Field and Service Robotics (FSR),
   Brisbane, AUSTRALIA, NOV 09-11, 2013}},
Abstract = {{The ability of robots to meticulously cover large areas while gathering
   sensor data has widespread applications in precision agriculture. For
   autonomous operations in orchards, a suitable information management
   system is required, within which we can gather and process data relating
   to the state and performance of the crop over time, such as distinct
   yield count, canopy volume, and crop health. An efficient way to
   structure an information system is to discretize it to the individual
   tree, for which tree segmentation/detection is a key component. This
   paper presents a tree trunk detection pipeline for identifying
   individual trees in a trellis structured apple orchard, using
   ground-based lidar and image data. A coarse observation of trunk
   candidates is initially made using a Hough transformation on point cloud
   lidar data. These candidates are projected into the camera images, where
   pixelwise classification is used to update their likelihood of being a
   tree trunk. Detection is achieved by using a hidden semi-Markov model to
   leverage from contextual information provided by the repetitive
   structure of an orchard. By repeating this over individual orchard rows,
   we are able to build a tree map over the farm, which can be either GPS
   localized or represented topologically by the row and tree number. The
   pipeline was evaluated at a commercial apple orchard near Melbourne,
   Australia. Data were collected at different times of year, covering an
   area of 1.6 ha containing different apple varieties planted on two types
   of trellis systems: a vertical I-trellis structure and a Guttingen
   V-trellis structure. The results show good trunk detection performance
   for both apple varieties and trellis structures during the preharvest
   season (87-96\% accuracy) and near perfect trunk detection performance
   (99\% accuracy) during the flowering season. (C) 2015 Wiley Periodicals,
   Inc.}},
DOI = {{10.1002/rob.21583}},
ISSN = {{1556-4959}},
EISSN = {{1556-4967}},
ORCID-Numbers = {{Underwood, James/0000-0003-0189-0706}},
Unique-ID = {{ISI:000368942000004}},
}

@article{ ISI:000365704000006,
Author = {St-Onge, Benoit and Audet, Felix-Antoine and Begin, Jean},
Title = {{Characterizing the Height Structure and Composition of a Boreal Forest
   Using an Individual Tree Crown Approach Applied to Photogrammetric Point
   Clouds}},
Journal = {{FORESTS}},
Year = {{2015}},
Volume = {{6}},
Number = {{11}},
Pages = {{3899-3922}},
Month = {{NOV}},
Abstract = {{Photogrammetric point clouds (PPC) obtained by stereomatching of aerial
   photographs now have a resolution sufficient to discern individual
   trees. We have produced such PPCs of a boreal forest and delineated
   individual tree crowns using a segmentation algorithm applied to the
   canopy height model derived from the PPC and a lidar terrain model. The
   crowns were characterized in terms of height and species (spruce, fir,
   and deciduous). Species classification used the 3D shape of the single
   crowns and their reflectance properties. The same was performed on a
   lidar dataset. Results show that the quality of PPC data generally
   approaches that of airborne lidar. For pixel-based canopy height models,
   viewing geometry in aerial images, forest structure (dense vs. open
   canopies), and composition (deciduous vs. conifers) influenced the
   quality of the 3D reconstruction of PPCs relative to lidar.
   Nevertheless, when individual tree height distributions were analyzed,
   PPC-based results were very similar to those extracted from lidar. The
   random forest classification (RF) of individual trees performed better
   in the lidar case when only 3D metrics were used (83\% accuracy for
   lidar, 79\% for PPC). However, when 3D and intensity or multispectral
   data were used together, the accuracy of PPCs (89\%) surpassed that of
   lidar (86\%).}},
DOI = {{10.3390/f6113899}},
ISSN = {{1999-4907}},
Unique-ID = {{ISI:000365704000006}},
}

@article{ ISI:000359029900025,
Author = {Liang, Ronghua and Shen, Wenjia and Li, Xiao-Xin and Wang, Haixia},
Title = {{Bayesian multi-distribution-based discriminative feature extraction for
   3D face recognition}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2015}},
Volume = {{320}},
Pages = {{406-417}},
Month = {{NOV 1}},
Abstract = {{Due to the difficulties associated with the collection of 3D samples, 3D
   face recognition technologies often have to work with smaller than
   desirable sample sizes. With the aim of enlarging the training number
   for each subject, we divide each training image into several patches.
   However, this immediately introduces two further problems for 3D models:
   high computational cost and dispersive features caused by the divided 3D
   image patches. We therefore first map 3D face images into 2D depth
   images, which greatly reduces the dimension of the samples. Though the
   depth images retain most of the robust features of 3D images, such as
   pose and illumination invariance, they lose many discriminative features
   of the original 3D samples. In this study, we propose a Bayesian
   learning framework to extract the discriminative features from the depth
   images. Specifically, we concentrate the features of the intra-class
   patches to a mean feature by maximizing the multivariate Gaussian
   likelihood function, and, simultaneously, enlarge the distances between
   the inter-class mean features by maximizing the exponential priori
   distribution of the mean features. For classification, we use the
   nearest neighbor classifier combined with the Mahalanobis distance to
   calculate the distance between the features of the test image and items
   in the training set. Experiments on two widely-used 3D face databases
   demonstrate the efficiency and accuracy of our proposed method compared
   to relevant state-of-the-art methods. Published by Elsevier Inc.}},
DOI = {{10.1016/j.ins.2015.03.063}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
Unique-ID = {{ISI:000359029900025}},
}

@article{ ISI:000363075300013,
Author = {Kankare, Ville and Liang, Xinlian and Vastaranta, Mikko and Yu, Xiaowei
   and Holopainen, Markus and Hyyppa, Juha},
Title = {{Diameter distribution estimation with laser scanning based multisource
   single tree inventory}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2015}},
Volume = {{108}},
Pages = {{161-171}},
Month = {{OCT}},
Abstract = {{Tree detection and tree species recognition are bottlenecks of the
   airborne remote sensing-based single tree inventories. The effect of
   these factors in forest attribute estimation can be reduced if airborne
   measurements are aided with tree mapping information that is collected
   from the ground. The main objective here was to demonstrate the use of
   terrestrial laser scanning-derived (TLS) tree maps in aiding airborne
   laser scanning-based (ALS) single tree inventory (multisource single
   tree inventory, MS-STI) and its capability in predicting diameter
   distribution in various forest conditions. Automatic measurement of TLS
   point clouds provided the tree maps and the required reference
   information from the tree attributes. The study area was located in Evo,
   Finland, and the reference data was acquired from 27 different sample
   plots with varying forest conditions. The workflow of MS-STI included:
   (1) creation of automatic tree map from TLS point clouds, (2) automatic
   diameter at breast height (DBH) measurement from TLS point clouds, (3)
   individual tree detection (ITD) based on ALS, (4) matching the ITD
   segments to the field-measured reference, (5) ALS point cloud metric
   extraction from the single tree segments and (6) DBH estimation based on
   the derived metrics. MS-STI proved to be accurate and efficient method
   for DBH estimation and predicting diameter distribution. The overall
   accuracy (root mean squared error, RMSE) of the DBH was 36.9 mm. Results
   showed that the DBH accuracy decreased if the tree density (trees/ha)
   increased. The highest accuracies were found in old-growth forests (tree
   densities less than 500 stems/ha). MS-STI resulted in the best
   accuracies regarding Norway spruce (Picea abies (L.) H.
   Karst.)-dominated forests (RMSE of 29.9 mm). Diameter distributions were
   predicted with low error indices, thereby resulting in a good fit
   compared to the reference. Based on the results, diameter distribution
   estimation with MS-STI is highly dependent on the forest structure and
   the accuracy of the tree maps that are used. The most important
   development step in the future for the MS-STI and automatic measurements
   of the TLS point cloud is to develop tree species recognition methods
   and further develop tree detection techniques. The possibility of using
   MLS or harvester data as a basis for the required tree maps should also
   be assessed in the future. (C) 2015 International Society for
   Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2015.07.007}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Vastaranta, Mikko/K-9656-2018}},
ORCID-Numbers = {{Vastaranta, Mikko/0000-0001-6552-9122}},
Unique-ID = {{ISI:000363075300013}},
}

@article{ ISI:000360999400005,
Author = {Patil, Hemprasad and Kothari, Ashwin and Bhurchandi, Kishor},
Title = {{3-D face recognition: features, databases, algorithms and challenges}},
Journal = {{ARTIFICIAL INTELLIGENCE REVIEW}},
Year = {{2015}},
Volume = {{44}},
Number = {{3}},
Pages = {{393-441}},
Month = {{OCT}},
Abstract = {{Face recognition is being widely accepted as a biometric technique
   because of its non-intrusive nature. Despite extensive research on 2-D
   face recognition, it suffers from poor recognition rate due to pose,
   illumination, expression, ageing, makeup variations and occlusions. In
   recent years, the research focus has shifted toward face recognition
   using 3-D facial surface and shape which represent more discriminating
   features by the virtue of increased dimensionality. This paper presents
   an extensive survey of recent 3-D face recognition techniques in terms
   of feature detection, classifiers as well as published algorithms that
   address expression and occlusion variation challenges followed by our
   critical comments on the published work. It also summarizes remarkable
   3-D face databases and their features used for performance evaluation.
   Finally we suggest vital steps of a robust 3-D face recognition system
   based on the surveyed work and identify a few possible directions for
   research in this area.}},
DOI = {{10.1007/s10462-015-9431-0}},
ISSN = {{0269-2821}},
EISSN = {{1573-7462}},
Unique-ID = {{ISI:000360999400005}},
}

@article{ ISI:000361074500008,
Author = {Kurtek, Sebastian and Drira, Hassen},
Title = {{A comprehensive statistical framework for elastic shape analysis of 3D
   faces}},
Journal = {{COMPUTERS \& GRAPHICS-UK}},
Year = {{2015}},
Volume = {{51}},
Number = {{SI}},
Pages = {{52-59}},
Month = {{OCT}},
Note = {{Shape Modeling International Conference (SMI 2015), Lille, FRANCE, JUN
   24-26, 2015}},
Abstract = {{We develop a comprehensive statistical framework for analyzing shapes of
   3D faces. In particular, we adapt a recent elastic shape analysis
   framework to the case of hemispherical surfaces, and explore its use in
   a number of processing applications. This framework provides a
   parameterization-invariant, elastic Riemannian metric, which allows the
   development of mathematically rigorous tools for statistical analysis.
   Specifically, this paper describes methods for registration, comparison
   and deformation, averaging, computation of covariance and summarization
   of variability using principal component analysis, random sampling from
   generative shape models, symmetry analysis, and expression and identity
   classification. An important aspect of this work is that all tasks are
   preformed under a unified metric, which has a natural interpretation in
   terms of bending and stretching of one 3D face to align it with another.
   We use a subset of the BU-3DFE face dataset, which contains varying
   magnitudes of expression. (C) 2015 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.cag.2015.05.027}},
ISSN = {{0097-8493}},
EISSN = {{1873-7684}},
ORCID-Numbers = {{Drira, Hassen/0000-0003-1052-4353}},
Unique-ID = {{ISI:000361074500008}},
}

@article{ ISI:000360490300004,
Author = {Spreeuwers, Luuk},
Title = {{Breaking the 99\% barrier: optimisation of three-dimensional face
   recognition}},
Journal = {{IET BIOMETRICS}},
Year = {{2015}},
Volume = {{4}},
Number = {{3}},
Pages = {{169-178}},
Month = {{SEP}},
Abstract = {{This study presents optimisations to a three-dimensional (3D) face
   recognition method the authors published in 2011. The optimisations
   concern handling and estimation of motion from a single 3D image using
   the symmetry of the face, fine registration by selection of the maximum
   score for small variations of the registration parameters and efficient
   training using automatic outlier removal where only part of the
   classifier is retrained. The optimisations lead to a staggering
   performance improvement: the verification rate on Face Recognition Grand
   Challenge (FRGC) v2 data at false accept rate = 0.1\% increases from
   94.6 to 99.3\% and the identification rate increases from 99 to 99.4\%.
   Both are, to the authors' knowledge, the best scores ever published on
   the FRGC data. In addition, the registration time was reduced from about
   2.5 to 0.2-0.6 s and the number of comparisons has increased from about
   11 000 to more than 50 000 per second. For slightly decreased
   performance, even millions of comparisons can be realised. The fast
   registration means near real-time recognition with 2-5 images is
   possible. The optimisations are not specific for this method, but can be
   beneficial for other 3D face recognition methods as well.}},
DOI = {{10.1049/iet-bmt.2014.0017}},
ISSN = {{2047-4938}},
EISSN = {{2047-4946}},
Unique-ID = {{ISI:000360490300004}},
}

@article{ ISI:000358893300004,
Author = {Korez, Robert and Ibragimov, Bulat and Likar, Bostjan and Pernus, Franjo
   and Vrtovec, Tomaz},
Title = {{A Framework for Automated Spine and Vertebrae Interpolation-Based
   Detection and Model-Based Segmentation}},
Journal = {{IEEE TRANSACTIONS ON MEDICAL IMAGING}},
Year = {{2015}},
Volume = {{34}},
Number = {{8, SI}},
Pages = {{1649-1662}},
Month = {{AUG}},
Abstract = {{Automated and semi-automated detection and segmentation of spinal and
   vertebral structures from computed tomography (CT) images is a
   challenging task due to a relatively high degree of anatomical
   complexity, presence of unclear boundaries and articulation of vertebrae
   with each other, as well as due to insufficient image spatial
   resolution, partial volume effects, presence of image artifacts,
   intensity variations and low signal-to-noise ratio. In this paper, we
   describe a novel framework for automated spine and vertebrae detection
   and segmentation from 3-D CT images. A novel optimization technique
   based on interpolation theory is applied to detect the location of the
   whole spine in the 3-D image and, using the obtained location of the
   whole spine, to further detect the location of individual vertebrae
   within the spinal column. The obtained vertebra detection results
   represent a robust and accurate initialization for the subsequent
   segmentation of individual vertebrae, which is performed by an improved
   shape-constrained deformable model approach. The framework was evaluated
   on two publicly available CT spine image databases of 50 lumbar and 170
   thoracolumbar vertebrae. Quantitative comparison against corresponding
   reference vertebra segmentations yielded an overall mean
   centroid-to-centroid distance of 1.1 mm and Dice coefficient of 83.6\%
   for vertebra detection, and an overall mean symmetric surface distance
   of 0.3 mm and Dice coefficient of 94.6\% for vertebra segmentation. The
   results indicate that by applying the proposed automated detection and
   segmentation framework, vertebrae can be successfully detected and
   accurately segmented in 3-D from CT spine images.}},
DOI = {{10.1109/TMI.2015.2389334}},
ISSN = {{0278-0062}},
EISSN = {{1558-254X}},
Unique-ID = {{ISI:000358893300004}},
}

@article{ ISI:000357545400014,
Author = {Schmitt, Michael and Shahzad, Muhammad and Zhu, Xiao Xiang},
Title = {{Reconstruction of individual trees from multi-aspect TomoSAR data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2015}},
Volume = {{165}},
Pages = {{175-185}},
Month = {{AUG}},
Abstract = {{The localization and reconstruction of individual trees as well as the
   extraction of their geometrical parameters is an important field of
   research in both forestry and remote sensing. While the current
   state-of-the-art mostly focuses on the exploitation of optical imagery
   and airborne LiDAR data, modern SAR sensors have not yet met the
   interest of the research community in that regard. This paper presents a
   prototypical processing chain for the reconstruction of individual
   deciduous trees: First, single-pass multi-baseline InSAR data acquired
   from multiple aspect angles are used for the generation of a layover-
   and shadow-free 3D point cloud by tomographic SAR processing. The
   resulting point cloud is then segmented by unsupervised mean shift
   clustering, before ellipsoid models are fitted to the points of each
   cluster. From these 3D ellipsoids the relevant geometrical tree
   parameters are extracted. Evaluation with respect to a manually derived
   reference dataset prove that almost 74\% of all trees are successfully
   segmented and reconstructed, thus providing a promising perspective for
   further research toward individual tree recognition from SAR data. (C)
   2015 The Authors. Published by Elsevier Inc This is an open access
   article under the CC BY-NC-ND license.}},
DOI = {{10.1016/j.rse.2015.05.012}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000357545400014}},
}

@article{ ISI:000356107200001,
Author = {Wang, Chao and Cho, Yong K. and Kim, Changwan},
Title = {{Automatic BIM component extraction from point clouds of existing
   buildings for sustainability applications}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2015}},
Volume = {{56}},
Pages = {{1-13}},
Month = {{AUG}},
Abstract = {{Building information models (BIMs) are increasingly being applied
   throughout a building's lifecycle for various applications, such as
   progressive construction monitoring and defect detection, building
   renovation, energy simulation, and building system analysis in the
   Architectural, Engineering, Construction, and Facility Management
   (AEC/FM) domains. In conventional approaches, as-is BIM is primarily
   manually created from point clouds, which is labor-intensive, costly,
   and time consuming. This paper proposes a method for automatically
   extracting building geometries from unorganized point clouds. The
   collected raw data undergo data downsizing, boundary detection, and
   building component categorization, resulting in the building components
   being recognized as individual objects and their visualization as
   polygons. The results of tests conducted on three collected as-is
   building data to validate the technical feasibility and evaluate the
   performance of the proposed method indicate that it can simplify and
   accelerate the as-is building model from the point cloud creation
   process. (C) 2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.autcon.2015.04.001}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
ResearcherID-Numbers = {{Jeong, Yongwook/N-7413-2016}},
Unique-ID = {{ISI:000356107200001}},
}

@article{ ISI:000355894900023,
Author = {Weinmann, Martin and Jutzi, Boris and Hinz, Stefan and Mallet, Clement},
Title = {{Semantic point cloud interpretation based on optimal neighborhoods,
   relevant features and efficient classifiers}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2015}},
Volume = {{105}},
Pages = {{286-304}},
Month = {{JUL}},
Abstract = {{3D scene analysis in terms of automatically assigning 3D points a
   respective semantic label has become a topic of great importance in
   photogrammetiy, remote sensing, computer vision and robotics. In this
   paper, we address the issue of how to increase the distinctiveness of
   geometric features and select the most relevant ones among these for 3D
   scene analysis. We present a new, fully automated and versatile
   framework composed of four components: (i) neighborhood selection, (ii)
   feature extraction, (iii) feature selection and (iv) classification. For
   each component, we consider a variety of approaches which allow
   applicability in terms of simplicity, efficiency and reproducibility, so
   that end-users can easily apply the different components and do not
   require expert knowledge in the respective domains. In a detailed
   evaluation involving 7 neighborhood definitions, 21 geometric features,
   7 approaches for feature selection, 10 classifiers and 2 benchmark
   datasets, we demonstrate that the selection of optimal neighborhoods for
   individual 3D points significantly improves the results of 3D scene
   analysis. Additionally, we show that the selection of adequate feature
   subsets may even further increase the quality of the derived results
   while significantly reducing both processing time and memory
   consumption. (C) 2015 International Society for Photogrammetry and
   Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.isprsjprs.2015.01.016}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Weinmann, Martin/0000-0002-8654-7546
   Mallet, Clement/0000-0002-2675-165X}},
Unique-ID = {{ISI:000355894900023}},
}

@article{ ISI:000448059300007,
Author = {Rodriguez, Julian S. and Prieto, Flavio},
Title = {{Analysis and comparison of the cone curvature descriptor in facial
   gesture recognition tasks}},
Journal = {{INGENIERIA}},
Year = {{2015}},
Volume = {{20}},
Number = {{2}},
Pages = {{261-275}},
Month = {{JUL-DEC}},
Abstract = {{This article presents the results of analyzing the behavior of the Cone
   Curvature shape descriptor (CC) in the task of recognition of facial
   expressions in 3D images. The CC descriptor is a representation of the
   3D model computed from a set of waves modeling for each vertex of a
   polygon mesh. The 3D Facial Expression Database (BU-3DFE) was used,
   which contains images with six facial expressions. With the use of the
   CC descriptor, the expressions were recognized in an average percentage
   of 76.67\% with a neural network, and of 78.88\% with a Bayesian
   classifier. By combining the CC descriptor with other descriptors such
   as DESIRE and Spherical Spin Image, it was achieved an average
   percentage of gesture recognition of 90.27\% and 97.2\%, using the
   mentioned classifiers.}},
DOI = {{10.14483/udistrital.jour.reving.2015.2.a06}},
ISSN = {{0121-750X}},
EISSN = {{2344-8393}},
Unique-ID = {{ISI:000448059300007}},
}

@article{ ISI:000357869200002,
Author = {Duan, Zhugeng and Zhao, Dan and Zeng, Yuan and Zhao, Yujin and Wu,
   Bingfang and Zhu, Jianjun},
Title = {{Assessing and Correcting Topographic Effects on Forest Canopy Height
   Retrieval Using Airborne LiDAR Data}},
Journal = {{SENSORS}},
Year = {{2015}},
Volume = {{15}},
Number = {{6}},
Pages = {{12133-12155}},
Month = {{JUN}},
Abstract = {{Topography affects forest canopy height retrieval based on airborne
   Light Detection and Ranging (LiDAR) data a lot. This paper proposes a
   method for correcting deviations caused by topography based on
   individual tree crown segmentation. The point cloud of an individual
   tree was extracted according to crown boundaries of isolated individual
   trees from digital orthophoto maps (DOMs). Normalized canopy height was
   calculated by subtracting the elevation of centres of gravity from the
   elevation of point cloud. First, individual tree crown boundaries are
   obtained by carrying out segmentation on the DOM. Second, point clouds
   of the individual trees are extracted based on the boundaries. Third,
   precise DEM is derived from the point cloud which is classified by a
   multi-scale curvature classification algorithm. Finally, a height
   weighted correction method is applied to correct the topological
   effects. The method is applied to LiDAR data acquired in South China,
   and its effectiveness is tested using 41 field survey plots. The results
   show that the terrain impacts the canopy height of individual trees in
   that the downslope side of the tree trunk is elevated and the upslope
   side is depressed. This further affects the extraction of the location
   and crown of individual trees. A strong correlation was detected between
   the slope gradient and the proportions of returns with height
   differences more than 0.3, 0.5 and 0.8 m in the total returns, with
   coefficient of determination R-2 of 0.83, 0.76, and 0.60 (n = 41),
   respectively.}},
DOI = {{10.3390/s150612133}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000357869200002}},
}

@article{ ISI:000356741800007,
Author = {Weinmann, M. and Urban, S. and Hinz, S. and Jutzi, B. and Mallet, C.},
Title = {{Distinctive 2D and 3D features for automated large-scale scene analysis
   in urban areas}},
Journal = {{COMPUTERS \& GRAPHICS-UK}},
Year = {{2015}},
Volume = {{49}},
Pages = {{47-57}},
Month = {{JUN}},
Abstract = {{We propose a new methodology for large-scale urban 3D scene analysis in
   terms of automatically assigning 3D points the respective semantic
   labels. The methodology focuses on simplicity and reproducibility of the
   involved components as well as performance in terms of accuracy and
   computational efficiency. Exploiting a variety of low-level 2D and 3D
   geometric features, we further improve their distinctiveness by
   involving individual neighborhoods of optimal size. Due to the use of
   individual neighborhoods, the methodology is not tailored to a specific
   dataset, but in principle designed to process point clouds with a few
   millions of 3D points. Consequently, an extension has to be introduced
   for analyzing huge 3D point clouds with possibly billions of points for
   a whole city. For this purpose, we propose an extension which is based
   on an appropriate partitioning of the scene and thus allows a successive
   processing in a reasonable time without affecting the quality of the
   classification results. We demonstrate the performance of our
   methodology on two labeled benchmark datasets with respect to
   robustness, efficiency, and scalability. (C) 2015 Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.cag.2015.01.006}},
ISSN = {{0097-8493}},
EISSN = {{1873-7684}},
ORCID-Numbers = {{Mallet, Clement/0000-0002-2675-165X
   Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000356741800007}},
}

@article{ ISI:000355003600024,
Author = {Lindsay, Kaitlin E. and Ruehli, Frank J. and Deleon, Valerie Burke},
Title = {{Revealing the Face of an Ancient Egyptian: Synthesis of Current and
   Traditional Approaches to Evidence-Based Facial Approximation}},
Journal = {{ANATOMICAL RECORD-ADVANCES IN INTEGRATIVE ANATOMY AND EVOLUTIONARY
   BIOLOGY}},
Year = {{2015}},
Volume = {{298}},
Number = {{6}},
Pages = {{1144-1161}},
Month = {{JUN}},
Abstract = {{The technique of forensic facial approximation, or reconstruction, is
   one of many facets of the field of mummy studies. Although far from a
   rigorous scientific technique, evidence-based visualization of
   antemortem appearance may supplement radiological, chemical,
   histological, and epidemiological studies of ancient remains. Published
   guidelines exist for creating facial approximations, but few
   approximations are published with documentation of the specific process
   and references used. Additionally, significant new research has taken
   place in recent years which helps define best practices in the field.
   This case study records the facial approximation of a 3,000-year-old
   ancient Egyptian woman using medical imaging data and the digital
   sculpting program, ZBrush. It represents a synthesis of current
   published techniques based on the most solid anatomical and/or
   statistical evidence. Through this study, it was found that although
   certain improvements have been made in developing repeatable,
   evidence-based guidelines for facial approximation, there are many
   proposed methods still awaiting confirmation from comprehensive studies.
   This study attempts to assist artists, anthropologists, and forensic
   investigators working in facial approximation by presenting the
   recommended methods in a chronological and usable format. Anat Rec,
   298:1144-1161, 2015. (c) 2015 Wiley Periodicals, Inc.}},
DOI = {{10.1002/ar.23146}},
ISSN = {{1932-8486}},
EISSN = {{1932-8494}},
Unique-ID = {{ISI:000355003600024}},
}

@article{ ISI:000354623800005,
Author = {Mehanna, Ahmed Mohamed and Baki, Fatthi Abdel and Eid, Mohamed and Negm,
   Magdy},
Title = {{Comparison of different computed tomography post-processing modalities
   in assessment of various middle ear disorders}},
Journal = {{EUROPEAN ARCHIVES OF OTO-RHINO-LARYNGOLOGY}},
Year = {{2015}},
Volume = {{272}},
Number = {{6}},
Pages = {{1357-1370}},
Month = {{JUN}},
Abstract = {{Several anatomic structures of the middle ear are not optimally depicted
   in the standard axial and coronal planes. Several 2D and 3D
   image-processing modalities are currently available for CT examinations
   in clinical radiology departments. Till now 3D reconstructions of the
   temporal bone have not been widely used yet, and attracted only academic
   interest. The aim of this study was to compare axial (source images), 2D
   and 3DCT post-processing modalities, and to evaluate the value of 3D
   reconstructed images/virtual endoscopy (VE) in assessment of various
   middle ear disorders for identification of the best modality/view for
   assessment of a particular middle ear structure or pathology. 40
   patients with various middle ear disorders, planned for surgical
   intervention were included in prospective study. Multi-slice CT was
   performed for all patients. Scans were acquired in the axial plane. The
   axial source datasets were utilized for generation of 2D reformations
   and 3D reconstructed images. All studied images were divided into three
   categories: axial (source images), 2D reformations (MPR and
   sliding-thin-slab MIP) and 3D reconstruction (virtual endoscopy). The
   visibility of middle ear structures and pathologies with each modality
   were scored qualitatively using three-point scoring system in reference
   to operative findings. Stapes superstructure and footplate,
   incudostapedial joint, oval and round windows, tympanic segment of the
   facial nerve and tegmen were not optimally depicted in the axial plane.
   Sinus tympani and facial recess were best visualized with axial images
   or VE. 3D reconstruction/VE allowed good visualization of all parts of
   ossicular chain except stapes superstructure. Regarding pathologic
   changes, 2D reformations and 3D reconstructed images allowed better
   visualization of erosion of ossicles and tegmen. 3D reconstruction/VE
   did not allow detection of foci of otospongiosis. 2D reformations can be
   considered the mainstay in assessment of most middle ear structures and
   pathologies. 3D reconstruction/VE seems to provide a useful method for a
   preoperative general overview of the middle ear anatomy, particularly
   for the ossicular chain, round window and retrotympanum.}},
DOI = {{10.1007/s00405-014-2920-y}},
ISSN = {{0937-4477}},
EISSN = {{1434-4726}},
ORCID-Numbers = {{Eid, Mohamed/0000-0002-4830-9010}},
Unique-ID = {{ISI:000354623800005}},
}

@article{ ISI:000355288200010,
Author = {Vuollo, Ville and Sidlauskas, Mantas and Sidlauskas, Antanas and Harila,
   Virpi and Salomskiene, Loreta and Zhurov, Alexei and Holmstrom, Lasse
   and Pirttiniemi, Pertti and Heikkinen, Tuomo},
Title = {{Comparing Facial 3D Analysis With DNA Testing to Determine Zygosities of
   Twins}},
Journal = {{TWIN RESEARCH AND HUMAN GENETICS}},
Year = {{2015}},
Volume = {{18}},
Number = {{3}},
Pages = {{306-313}},
Month = {{JUN}},
Abstract = {{The aim of this study was to compare facial 3D analysis to DNA testing
   in twin zygosity determinations. Facial 3D images of 106 pairs of young
   adult Lithuanian twins were taken with a stereophotogrammetric device
   (3dMD, Atlanta, Georgia) and zygosity was determined according to
   similarity of facial form. Statistical pattern recognition methodology
   was used for classification. The results showed that in 75\% to 90\% of
   the cases, zygosity determinations were similar to DNA-based results.
   There were 81 different classification scenarios, including 3 groups, 3
   features, 3 different scaling methods, and 3 threshold levels. It
   appeared that coincidence with 0.5 mm tolerance is the most suitable
   feature for classification. Also, leaving out scaling improves results
   in most cases. Scaling was expected to equalize the magnitude of
   differences and therefore lead to better recognition performance. Still,
   better classification features and a more effective scaling method or
   classification in different facial areas could further improve the
   results. In most of the cases, male pair zygosity recognition was at a
   higher level compared with females. Erroneously classified twin pairs
   appear to be obvious outliers in the sample. In particular, faces of
   young dizygotic (DZ) twins may be so similar that it is very hard to
   define a feature that would help classify the pair as DZ.
   Correspondingly, monozygotic (MZ) twins may have faces with quite
   different shapes. Such anomalous twin pairs are interesting exceptions,
   but they form a considerable portion in both zygosity groups.}},
DOI = {{10.1017/thg.2015.16}},
ISSN = {{1832-4274}},
EISSN = {{1839-2628}},
ResearcherID-Numbers = {{Zhurov, Alexei/P-4410-2014
   }},
ORCID-Numbers = {{Zhurov, Alexei/0000-0002-5594-0740
   Pirttiniemi, Pertti/0000-0003-4514-836X}},
Unique-ID = {{ISI:000355288200010}},
}

@article{ ISI:000354402000018,
Author = {Betta, Giovanni and Capriglione, Domenico and Gasparetto, Michele and
   Zappa, Emanuele and Liguori, Consolatina and Paolillo, Alfredo},
Title = {{Face recognition based on 3D features: Management of the measurement
   uncertainty for improving the classification}},
Journal = {{MEASUREMENT}},
Year = {{2015}},
Volume = {{70}},
Pages = {{169-178}},
Month = {{JUN}},
Abstract = {{In this paper a suitable methodology for the improvement of the
   reliability of results in classification systems based on 3D images is
   proposed. More in detail, it is based on the knowledge of the
   uncertainty of the features constituting the 3D image (obtained
   processing a pair of two 2D stereoscopic images) and on a suitable
   statistical approach providing a confidence level to the classification
   result. These pieces of information are then managed in order to improve
   the classification performance in terms of correct classification and
   false reject percentages. The experimental results, obtained applying
   the methodology on an Active Appearance Models algorithm for feature
   detection and triangulating the 3D features, show that, compared with a
   basic approach (which generally does not take into account the
   uncertainty on 3D features), the proposed methodology allows to
   significantly improve the classification performance even in scenarios
   characterized by a high uncertainty. (C) 2015 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.measurement.2015.03.043}},
ISSN = {{0263-2241}},
EISSN = {{1873-412X}},
Unique-ID = {{ISI:000354402000018}},
}

@article{ ISI:000352571800033,
Author = {Mills, Graham and Fotopoulos, Georgia},
Title = {{Rock Surface Classification in a Mine Drift Using Multiscale Geometric
   Features}},
Journal = {{IEEE GEOSCIENCE AND REMOTE SENSING LETTERS}},
Year = {{2015}},
Volume = {{12}},
Number = {{6}},
Pages = {{1322-1326}},
Month = {{JUN}},
Abstract = {{Scale-dependent statistical depictions of surface morphology offer the
   potential to parameterize complex geometrical scaling relationships with
   greater detail than traditional fractal measures. Using multiscale
   operators, it is possible to identify points belonging to rough
   discontinuous surfaces in noisy point clouds solely on the basis of
   their local geometry. Many strategies for point cloud feature
   classification have been developed since the proliferation of laser
   scanning systems. Most of the techniques which are applicable to natural
   scenes employ external data sources such as hyperspectral imagery,
   return pulse intensity, and waveform data. In this letter, multiscale
   geometric parameters are used to identify individual point observations
   corresponding to rock surfaces in point clouds acquired by terrestrial
   laser scanning in scenes with man-made clutter and scanning artifacts.
   Three multiscale operators, namely, the approximate shape and density of
   a defined neighborhood and the distance of its mean point from its
   geometric center, are fused into a single feature vector. The procedure
   is demonstrated using real point cloud data acquired in a mine drift,
   with the goal of identifying points belonging to the rock face obscured
   by an overlying wire support mesh. Using the extra-trees classifier,
   extraneous returns caused by the mesh were excluded from the point cloud
   with a 97\% success rate, while 87\% of the desired surface points were
   retained.}},
DOI = {{10.1109/LGRS.2015.2398814}},
ISSN = {{1545-598X}},
EISSN = {{1558-0571}},
Unique-ID = {{ISI:000352571800033}},
}

@article{ ISI:000353891500010,
Author = {Kashani, Alireza G. and Crawford, Patrick S. and Biswas, Sufal K. and
   Graettinger, Andrew J. and Grau, David},
Title = {{Automated Tornado Damage Assessment and Wind Speed Estimation Based on
   Terrestrial Laser Scanning}},
Journal = {{JOURNAL OF COMPUTING IN CIVIL ENGINEERING}},
Year = {{2015}},
Volume = {{29}},
Number = {{3}},
Month = {{MAY}},
Abstract = {{There are more than 1,000 tornadoes in the United States each year, yet
   engineers do not typically design for tornadoes because of insufficient
   information about wind loads. Collecting building-level damage data in
   the aftermath of tornadoes can improve the understanding of tornado
   winds, but these data are difficult to collect because of safety, time,
   and access constraints. This study presents and tests an automated
   geographic information system (GIS) method using postevent point cloud
   data collected by terrestrial scanners and preevent aerial images to
   calculate the percentage of roof and wall damage and estimate wind
   speeds at an individual building scale. Simulations determined that for
   typical point cloud density (>25points/m2), a GIS raster cell size of
   40-50cm resulted in less than 10\% error in damaged roof and wall
   detection. Data collected after recent tornadoes were used to correlate
   wind speed estimates and the percent of detected damage. The developed
   method estimated wind speeds from damage data collected after the 2011
   Tuscaloosa, AL tornado at finer scales than the typical large-scale
   assessments done by reconnaissance engineers.}},
DOI = {{10.1061/(ASCE)CP.1943-5487.0000389}},
Article-Number = {{04014051}},
ISSN = {{0887-3801}},
EISSN = {{1943-5487}},
Unique-ID = {{ISI:000353891500010}},
}

@article{ ISI:000353807700021,
Author = {Hoevenaren, Inge A. and Maal, Thomas J. J. and Krikken, E. and de Haan,
   A. F. J. and Berge, S. J. and Ulrich, D. J. O.},
Title = {{Development of a three-dimensional hand model using 3D
   stereophotogrammetry: Evaluation of landmark reproducibility}},
Journal = {{JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY}},
Year = {{2015}},
Volume = {{68}},
Number = {{5}},
Pages = {{709-716}},
Month = {{MAY}},
Abstract = {{BACKGROUND: Using three-dimensional (3D) photography, exact images of
   the human body can be produced. Over the last few years, this technique
   is mainly being developed in the field of maxillofacial reconstructive
   surgery, creating fusion images with computed tomography (CT) data for
   accurate planning and prediction of treatment outcome. However, in hand
   surgery, 3D photography is not yet being used in clinical settings.
   METHODS: The aim of this study was to develop a valid method for imaging
   the hand using 3D stereophotogrammetry. The reproducibility of 30 soft
   tissue landmarks was determined using 3D stereophotogrammetric images.
   Analysis was performed by two observers on 20 3D photographs.
   Reproducibility and reliability of the landmark identification were
   determined using statistical analysis.
   RESULTS: The intra-and interobserver reproducibility of the landmarks
   were high. This study showed a high reliability coefficient for
   intraobserver (1.00) and interobserver reliability (0.99).
   Identification of the landmarks on the palmar aspect of individual
   fingers was more precise than the identification of landmarks of the
   thumb.
   CONCLUSIONS: This study shows that 3D photography can safely produce
   accurate and reproducible images of the hand, which makes the technique
   a reliable method for soft tissue analysis. 3D images can be a helpful
   tool in pre- and postoperative evaluation of reconstructive trauma
   surgery, in aesthetic surgery of the hand, and for educational purposes.
   The use in everyday practice of hand surgery and the concept of fusing
   3D photography images with radiologic images of the interior hand
   structures needs to be further explored. (C) 2014 British Association of
   Plastic, Reconstructive and Aesthetic Surgeons. Published by Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.bjps.2014.12.025}},
ISSN = {{1748-6815}},
EISSN = {{1878-0539}},
ResearcherID-Numbers = {{Ulrich, D.J.O./H-8099-2014
   Maal, Thomas/L-4497-2015
   de Haan, Antonius/L-4344-2015
   Berge, S.J./H-8011-2014}},
ORCID-Numbers = {{Maal, Thomas/0000-0002-1702-4733
   }},
Unique-ID = {{ISI:000353807700021}},
}

@article{ ISI:000352441700012,
Author = {Urbanova, Petra and Hejna, Petr and Jurda, Mikolas},
Title = {{Testing photogrammetry-based techniques for three-dimensional surface
   documentation in forensic pathology}},
Journal = {{FORENSIC SCIENCE INTERNATIONAL}},
Year = {{2015}},
Volume = {{250}},
Pages = {{77-86}},
Month = {{MAY}},
Abstract = {{Three-dimensional surface technologies particularly close range
   photogrammetry and optical surface scanning have recently advanced into
   affordable, flexible and accurate techniques. Forensic postmortem
   investigation as performed on a daily basis, however, has not yet fully
   benefited from their potentials. In the present paper, we tested two
   approaches to 3D external body documentation - digital camera-based
   photogrammetry combined with commercial Agisoft PhotoScan (R) software
   and stereophotogrammetry-based Vectra H1 (R), a portable handheld
   surface scanner. In order to conduct the study three human subjects were
   selected, a living person, a 25-year-old female, and two forensic cases
   admitted for postmortem examination at the Department of Forensic
   Medicine, Hradec Kralove, Czech Republic (both 63-year-old males), one
   dead to traumatic, self-inflicted, injuries (suicide by hanging), the
   other diagnosed with the heart failure.
   All three cases were photographed in 3608 manner with a Nikon 7000
   digital camera and simultaneously documented with the handheld scanner.
   In addition to having recorded the pre-autopsy phase of the forensic
   cases, both techniques were employed in various stages of autopsy. The
   sets of collected digital images (approximately 100 per case) were
   further processed to generate point clouds and 3D meshes. Final 3D
   models (a pair per individual) were counted for numbers of points and
   polygons, then assessed visually and compared quantitatively using ICP
   alignment algorithm and a cloud point comparison technique based on
   closest point to point distances.
   Both techniques were proven to be easy to handle and equally laborious.
   While collecting the images at autopsy took around 20 min, the
   post-processing was much more time-demanding and required up to 10 h of
   computation time. Moreover, for the full-body scanning the
   post-processing of the handheld scanner required rather time-consuming
   manual image alignment. In all instances the applied approaches produced
   high-resolution photorealistic, real sized or easy to calibrate 3D
   surface models. Both methods equally failed when the scanned body
   surface was covered with body hair or reflective moist areas. Still, it
   can be concluded that single camera close range photogrammetry and
   optical surface scanning using Vectra H1 scanner represent relatively
   low-cost solutions which were shown to be beneficial for postmortem body
   documentation in forensic pathology. (C) 2015 Elsevier Ireland Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.forsciint.2015.03.005}},
ISSN = {{0379-0738}},
EISSN = {{1872-6283}},
ORCID-Numbers = {{Urbanova, Petra/0000-0001-9321-3360}},
Unique-ID = {{ISI:000352441700012}},
}

@article{ ISI:000350839700012,
Author = {Tamrin, K. F. and Rahmatullah, B. and Samuri, S. M.},
Title = {{An experimental investigation of three-dimensional particle aggregation
   using digital holographic microscopy}},
Journal = {{OPTICS AND LASERS IN ENGINEERING}},
Year = {{2015}},
Volume = {{68}},
Pages = {{93-103}},
Month = {{MAY}},
Abstract = {{The tendency of particles to aggregate depends on particle-particle and
   particle-fluid interactions. These interactions can be characterized but
   it requires accurate 3D measurements of particle distributions. We
   introduce the application of an off-axis digital holographic microscopy
   for measuring distributions of dense micrometer (2 mu m) particles in a
   liquid solution. We demonstrate that digital holographic microscopy is
   capable of recording the instantaneous 3D position of particles in a
   flow volume. A new reconstruction method that aids identification of
   particle images was used in this work. About 62\% of the expected number
   of particles within the interrogated flow volume was detected. Based on
   the 3D position of individual particles, the tendency of particle to
   aggregate is investigated. Results show that relatively few particles
   (around 5-10 of a cohort of 1500) were aggregates. This number did not
   change significantly with time. (C) 2014 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.optlaseng.2014.12.011}},
ISSN = {{0143-8166}},
EISSN = {{1873-0302}},
Unique-ID = {{ISI:000350839700012}},
}

@article{ ISI:000349271500019,
Author = {Azazi, Amal and Lotfi, Syaheerah Lebai and Venkat, Ibrahim and
   Fernandez-Martinez, Fernando},
Title = {{Towards a robust affect recognition: Automatic facial expression
   recognition in 3D faces}},
Journal = {{EXPERT SYSTEMS WITH APPLICATIONS}},
Year = {{2015}},
Volume = {{42}},
Number = {{6}},
Pages = {{3056-3066}},
Month = {{APR 15}},
Abstract = {{Facial expressions are a powerful tool that communicates a person's
   emotional state and subsequently his/her intentions. Compared to 2D face
   images, 3D face images offer more granular cues that are not available
   in the 2D images. However, one major setback of 3D faces is that they
   impose a higher dimensionality than 2D faces. In this paper, we attempt
   to address this problem by proposing a fully automatic 3D facial
   expression recognition model that tackles the high dimensionality
   problem in a twofold solution. First, we transform the 3D faces into the
   2D plane using conformal mapping. Second, we propose a Differential
   Evolution (DE) based optimization algorithm to select the optimal facial
   feature set and the classifier parameters simultaneously. The optimal
   features are selected from a pool of Speed Up Robust Features (SURF)
   descriptors of all the prospective facial points. The proposed model
   yielded an average recognition accuracy of 79\% using the Bosphorus
   database and 79.36\% using the BU-3DFE database. In addition, we exploit
   the facial muscular movements to enhance the probability estimation (PE)
   of Support Vector Machine (SVM). Joint application of feature selection
   with the proposed enhanced PE (EPE) yielded an average recognition
   accuracy of 84\% using the Bosphorus database and 85.81\% using the
   BU-3DFE database, which is statistically significantly better (at p <
   0.01 and p < 0.001, respectively) if compared to the individual exploit
   of the optimal features only. (C) 2014 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.eswa.2014.10.042}},
ISSN = {{0957-4174}},
EISSN = {{1873-6793}},
ResearcherID-Numbers = {{Fernandez-Martinez, Fernando/M-2935-2014
   }},
ORCID-Numbers = {{Fernandez-Martinez, Fernando/0000-0003-3877-0089
   Lebai Lutfi, Syaheerah/0000-0001-7349-0061}},
Unique-ID = {{ISI:000349271500019}},
}

@article{ ISI:000351880000193,
Author = {Amaral, Carlos P. and Simoes, Marco A. and Castelo-Branco, Miguel S.},
Title = {{Neural Signals Evoked by Stimuli of Increasing Social Scene Complexity
   Are Detectable at the Single-Trial Level and Right Lateralized}},
Journal = {{PLOS ONE}},
Year = {{2015}},
Volume = {{10}},
Number = {{3}},
Month = {{MAR 25}},
Abstract = {{Classification of neural signals at the single-trial level and the study
   of their relevance in affective and cognitive neuroscience are still in
   their infancy. Here we investigated the neurophysiological correlates of
   conditions of increasing social scene complexity using 3D human models
   as targets of attention, which may also be important in autism research.
   Challenging single-trial statistical classification of EEG neural
   signals was attempted for detection of oddball stimuli with increasing
   social scene complexity. Stimuli had an oddball structure and were as
   follows: 1) flashed schematic eyes, 2) simple 3D faces flashed between
   averted and non-averted gaze (only eye position changing), 3) simple 3D
   faces flashed between averted and non-averted gaze (head and eye
   position changing), 4) animated avatar alternated its gaze direction to
   the left and to the right (head and eye position), 5) environment with 4
   animated avatars all of which change gaze and one of which is the target
   of attention. We found a late (> 300 ms) neurophysiological oddball
   correlate for all conditions irrespective of their complexity as
   assessed by repeated measures ANOVA. We attempted single-trial detection
   of this signal with automatic classifiers and obtained a significant
   balanced accuracy classification of around 79\%, which is noteworthy
   given the amount of scene complexity. Lateralization analysis showed a
   specific right lateralization only for more complex realistic social
   scenes. In sum, complex ecological animations with social content elicit
   neurophysiological events which can be characterized even at the
   single-trial level. These signals are right lateralized. These finding
   paves the way for neuroscientific studies in affective neuroscience
   based on complex social scenes, and given the detectability at the
   single trial level this suggests the feasibility of brain computer
   interfaces that can be applied to social cognition disorders such as
   autism.}},
DOI = {{10.1371/journal.pone.0121970}},
Article-Number = {{e0121970}},
ISSN = {{1932-6203}},
ResearcherID-Numbers = {{Amaral, Carlos/J-4282-2019
   Castelo-Branco, Miguel/F-3866-2019
   }},
ORCID-Numbers = {{Amaral, Carlos/0000-0002-0493-9192
   Castelo-Branco, Miguel/0000-0003-4364-6373
   Simoes, Marco/0000-0003-3713-2464}},
Unique-ID = {{ISI:000351880000193}},
}

@article{ ISI:000354036100022,
Author = {Werschler, W. Philip and Fagien, Steven and Thomas, Jane and
   Paradkar-Mitragotri, Deepali and Rotunda, Adam and Beddingfield, III,
   Frederick C.},
Title = {{Development and Validation of a Photographic Scale for Assessment of Lip
   Fullness}},
Journal = {{AESTHETIC SURGERY JOURNAL}},
Year = {{2015}},
Volume = {{35}},
Number = {{3}},
Pages = {{294-307}},
Month = {{MAR}},
Abstract = {{Background: As lip augmentation becomes more popular, validated measures
   of lip fullness for quantification of outcomes are needed.
   Objective: Develop a scale for rating lip fullness and establish its
   reliability and sensitivity for assessing clinically meaningful
   differences.
   Methods: The initial Allergan Lip Fullness Scale (iLFS; a four-point
   photographic scale with verbal descriptions) was validated by eight
   physicians rating 55 live subjects during two rounds, conducted on one
   day. In addition, subjects performed self-evaluations. The revised
   Allergan Lip Fullness Scale (LFS), a five-point scale with a broader
   range of lip presentations, was validated by 21 clinicians in two online
   image rating sessions, >= 14 days apart, in which they used the LFS to
   rate overall, upper, and lower lip fullness of 144 3-dimensional (3D)
   images. Physician inter-and intra-rater agreement, subject intra-rater
   agreement (iLFS), and subject-physician agreement (iLFS) were evaluated.
   Additionally, during online rating session 1, raters ranked 38 pairs of
   3D images, taken before and after lip augmentation, as ``clinically
   different{''} or ``not clinically different.{''} The median LFS score
   difference for clinically different pairs was calculated to determine
   the clinically meaningful difference.
   Results: Clinician inter-and intra-rater agreement for the iLFS and LFS
   was substantial to almost perfect. Subject self-assessments (iLFS) had
   substantial intra-rater reliability and a high level of agreement with
   physician assessments. Median LFS score differences for overall, upper,
   and lower lip fullness were 1 (mean: 0.63-0.69) for ``clinically
   different{''} and 0 (mean: 0.28-0.36) for ``not clinically different{''}
   image pairs; thus, clinical significance of a 1-point difference in LFS
   score was established.
   Conclusions: The LFS is a reliable instrument for physician
   classification of lip fullness. A 1-point score difference can detect
   clinically meaningful differences in lip fullness.}},
DOI = {{10.1093/asj/sju025}},
ISSN = {{1090-820X}},
EISSN = {{1527-330X}},
Unique-ID = {{ISI:000354036100022}},
}

@article{ ISI:000351796000002,
Author = {Ming, Yue},
Title = {{Robust regional bounding spherical descriptor for 3D face recognition
   and emotion analysis}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2015}},
Volume = {{35}},
Pages = {{14-22}},
Month = {{MAR}},
Abstract = {{3D face recognition and emotion analysis play important roles in many
   fields of communication and edutainment An effective facial descriptor,
   with higher discriminating capability for face recognition and higher
   descriptiveness for facial emotion analysis, is a challenging issue.
   However, in the practical applications, the descriptiveness and
   discrimination are independent and contradictory to each other. 3D
   facial data provide a promising way to balance these two aspects. In
   this paper, a robust regional bounding spherical descriptor (RBSR) is
   proposed to facilitate 3D face recognition and emotion analysis. In our
   framework, we first segment a group of regions on each 3D facial point
   cloud by shape index and spherical bands on the human face. Then the
   corresponding facial areas are projected to regional bounding spheres to
   obtain our regional descriptor. Finally, a regional and global
   regression mapping (RGRM) technique is employed to the weighted regional
   descriptor for boosting the classification accuracy. Three largest
   available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the
   performance comparison and the experimental results show a consistently
   better performance for 3D face recognition and emotion analysis. (C)
   2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2014.12.003}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000351796000002}},
}

@article{ ISI:000351134600001,
Author = {Mengesha, Taye and Hawkins, Michael and Nieuwenhuis, Maarten},
Title = {{Validation of terrestrial laser scanning data using conventional forest
   inventory methods}},
Journal = {{EUROPEAN JOURNAL OF FOREST RESEARCH}},
Year = {{2015}},
Volume = {{134}},
Number = {{2}},
Pages = {{211-222}},
Month = {{MAR}},
Abstract = {{The application of terrestrial laser scanning (TLS) in capturing forest
   inventory parameters such as diameter at breast height, height and
   diameters along stem profiles, and in monitoring forest growth, was
   investigated and validated by comparison with conventionally measured
   individual tree parameters and plot-level forest growth in a stand of
   Sitka spruce (Picea sitchensis (Bong.) Carr.) in Ireland. The data
   acquisition for all the plots with different tree sizes and different
   slopes was carried out using a terrestrial laser scanner (FARO LS 800
   HE80) in November 2007 and November 2009, using the same plot centres
   and measurement procedures. The point cloud data were processed with
   Autostem (TM) software. The results showed that TLS enables the
   acquisition of forest stand parameters with an acceptable accuracy.
   Pruning of the lower branches did not improve tree recognition and the
   number of (partly) occluded trees stayed the same. Over the 2-year
   period, the average difference between the volume increment of the trees
   visible to the scanner derived using the conventional method and
   Autostem (TM) was 4.77 m(3) ha(-1) and resulted in scanner-derived
   estimates that were lower than the estimates obtained by conventional
   method by 6.1 \%. Using a simple correction factor to account for
   occlusion in the laser scanner data, the difference between these
   estimates for all trees in the stand became an over-estimation by 6.96
   m(3) ha(-1) (8.1 \%). At heights up along the stems > 15 m, the errors
   in stem diameter estimates started to escalate.}},
DOI = {{10.1007/s10342-014-0844-0}},
ISSN = {{1612-4669}},
EISSN = {{1612-4677}},
Unique-ID = {{ISI:000351134600001}},
}

@article{ ISI:000349608400002,
Author = {Chen, Junfen and Liao, Iman Yi and Belaton, Bahari and Zaman, Munir},
Title = {{A neural network-based point registration method for 3D rigid face image}},
Journal = {{WORLD WIDE WEB-INTERNET AND WEB INFORMATION SYSTEMS}},
Year = {{2015}},
Volume = {{18}},
Number = {{2}},
Pages = {{197-214}},
Month = {{MAR}},
Abstract = {{Intelligent detection of human face image combined with the real-time
   video monitoring has been applied to improve the secure and protective
   possibility. The registration is an indispensible step before
   distinguishing the variation among the images. Neural network (NN) has a
   strong learning ability from a mass unstructured point cloud even
   containing noisy data. Neural network has been applied to register 3D
   reconstructed ear data and 3D surface of bunny and to achieve the better
   results. Motivated by this idea, NN-based registration method for 3D
   rigid face image is proposed. This paper presented the proof process of
   obtaining rotation matrix and translation vector according to the
   training process of neural network. Then the measure index of
   registration performance was provided. The elaborate experiments were
   conducted on the 3D USF face database (provided by the University of
   South Florida) to verify the effectiveness of neural network as a
   registration method. Next, two comparisons were performed, one group was
   NN-based and ICP-based registration methods and the other group was our
   proposed NN-based and other NN-based registration methods. The
   experimental results showed that neural network is a robust and
   potential registration method for rigid face image registration.
   Furthermore, our proposed NN-based registration method is extended
   easily to do 2D-to-3D registration and non-rigid face registration.}},
DOI = {{10.1007/s11280-013-0213-9}},
ISSN = {{1386-145X}},
EISSN = {{1573-1413}},
ResearcherID-Numbers = {{belaton, bahari/G-4858-2010}},
ORCID-Numbers = {{belaton, bahari/0000-0002-9099-1498}},
Unique-ID = {{ISI:000349608400002}},
}

@article{ ISI:000349588900008,
Author = {Bolkart, Timo and Wuhrer, Stefanie},
Title = {{3D faces in motion: Fully automatic registration and statistical
   analysis}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2015}},
Volume = {{131}},
Pages = {{100-115}},
Month = {{FEB}},
Abstract = {{This paper presents a representation of 3D facial motion sequences that
   allows performing statistical analysis of 3D face shapes in motion. The
   resulting statistical analysis is applied to automatically generate
   realistic facial animations and to recognize dynamic facial expressions.
   To perform statistical analysis of 3D facial shapes in motion over
   different subjects and different motion sequences, a large database of
   motion sequences needs to be brought in full correspondence. Existing
   algorithms that compute correspondences between 3D facial motion
   sequences either require manual input or suffer from instabilities
   caused by drift. For large databases, algorithms that require manual
   interaction are not practical. We propose an approach to robustly
   compute correspondences between a large set of facial motion sequences
   in a fully automatic way using a multilinear model as statistical prior.
   In order to register the motion sequences, a good initialization is
   needed. We obtain this initialization by introducing a landmark
   prediction method for 3D motion sequences based on Markov Random Fields.
   Using this motion sequence registration, we find a compact
   representation of each motion sequence consisting of one vector of
   coefficients for identity and a high dimensional curve for expression.
   Based on this representation, we synthesize new motion sequences and
   perform expression recognition. We show experimentally that the obtained
   registration is of high quality, where 56\% of all vertices are at
   distance at most 1 mm from the input data, and that our synthesized
   motion sequences look realistic. (C) 2014 Elsevier Inc. All rights
   reserved.}},
DOI = {{10.1016/j.cviu.2014.06.013}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
Unique-ID = {{ISI:000349588900008}},
}

@article{ ISI:000349308600007,
Author = {Li, Ye and Wang, YingHui and Wang, BingBo and Sui, LianSheng},
Title = {{Nose tip detection on three-dimensional faces using pose-invariant
   differential surface features}},
Journal = {{IET COMPUTER VISION}},
Year = {{2015}},
Volume = {{9}},
Number = {{1}},
Pages = {{75-84}},
Month = {{FEB}},
Abstract = {{Three-dimensional (3D) facial data offer the potential to overcome the
   difficulties caused by the variation of head pose and illumination in 2D
   face recognition. In 3D face recognition, localisation of nose tip is
   essential to face normalisation, face registration and pose correction
   etc. Most of the existing methods of nose tip detection on 3D face deal
   mainly with frontal or near-frontal poses or are rotation sensitive.
   Many of them are training-based or model-based. In this study, a novel
   method of nose tip detection is proposed. Using pose-invariant
   differential surface features - high-order and low-order curvatures, it
   can detect nose tip on 3D faces under various poses automatically and
   accurately. Moreover, it does not require training and does not depend
   on any particular model. Experimental results on GavabDB verify the
   robustness and accuracy of the proposed method.}},
DOI = {{10.1049/iet-cvi.2014.0070}},
ISSN = {{1751-9632}},
EISSN = {{1751-9640}},
Unique-ID = {{ISI:000349308600007}},
}

@article{ ISI:000349229600034,
Author = {Qiu, Wu and Yuan, Jing and Ukwatta, Eranga and Fenster, Aaron},
Title = {{Rotationally resliced 3D prostate TRUS segmentation using convex
   optimization with shape priors}},
Journal = {{MEDICAL PHYSICS}},
Year = {{2015}},
Volume = {{42}},
Number = {{2}},
Pages = {{877-891}},
Month = {{FEB}},
Abstract = {{Purpose: Efficient and accurate segmentations of 3D end-firing
   transrectal ultrasound (TRUS) images play an important role in planning
   of 3D TRUS guided prostate biopsy. However, poor image quality of the
   input 3D TRUS images, such as strong imaging artifacts and speckles,
   often makes it a challenging task to extract the prostate boundaries
   accurately and efficiently.
   Methods: In this paper, the authors propose a novel convex
   optimization-based approach to delineate the prostate surface from a
   given 3D TRUS image, which reduces the original 3D segmentation problem
   to a sequence of simple 2D segmentation subproblems over the rotational
   reslices of the 3D TRUS volume. Essentially, the authors introduce a
   novel convex relaxation-based contour evolution approach to each 2D
   slicewise image segmentation with the joint optimization of shape
   information, where the learned 2D nonlinear statistical shape prior is
   incorporated to segment the initial slice, its result is propagated as a
   shape constraint to the segmentation of the following slices. In
   practice, the proposed segmentation algorithm is implemented on a GPU to
   achieve the high computational performance.
   Results: Experimental results using 30 patient 3D TRUS images show that
   the proposed method can achieve a mean Dice similarity coefficient of
   93.4\% +/- 2.2\% in 20 s for one 3D image, outperforming the existing
   local-optimization-based methods, e.g., level-set and active-contour, in
   terms of accuracy and efficiency. In addition, inter-and intraobserver
   variability experiments show its good reproducibility.
   Conclusions: A semiautomatic segmentation approach is proposed and
   evaluated to extract the prostate boundary from 3D TRUS images acquired
   by a 3D end-firing TRUS guided prostate biopsy system. Experimental
   results suggest that it may be suitable for the clinical use involving
   the image guided prostate biopsy procedures. (C) 2015 American
   Association of Physicists in Medicine.}},
DOI = {{10.1118/1.4906129}},
ISSN = {{0094-2405}},
ResearcherID-Numbers = {{FENSTER, Aaron/K-4337-2013
   Yuan, Jing/E-8080-2015
   de Ribaupierre, Sandrine/B-7707-2015
   }},
ORCID-Numbers = {{Yuan, Jing/0000-0002-4312-7023
   de Ribaupierre, Sandrine/0000-0001-7096-2289
   Ukwatta, Eranga/0000-0003-0180-4716}},
Unique-ID = {{ISI:000349229600034}},
}

@article{ ISI:000345480900012,
Author = {Guo, Yulan and Sohel, Ferdous and Bennamoun, Mohammed and Wan, Jianwei
   and Lu, Min},
Title = {{A novel local surface feature for 3D object recognition under clutter
   and occlusion}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2015}},
Volume = {{293}},
Pages = {{196-213}},
Month = {{FEB 1}},
Abstract = {{This paper presents a highly distinctive local surface feature called
   the TriSI feature for recognizing 3D objects in the presence of clutter
   and occlusion. For a feature point, we first construct a unique and
   repeatable Local Reference Frame (LRF) using the implicit geometrical
   information of neighboring triangular faces. We then generate three
   signatures from the three orthogonal coordinate axes of the LRF. These
   signatures are concatenated and then compressed into a TriSI feature.
   Finally, we propose an effective 3D object recognition algorithm based
   on hierarchical feature matching. We tested our TriSI feature on two
   popular datasets. Rigorous experimental results show that the TriSI
   feature was highly descriptive and outperformed existing algorithms
   under all levels of Gaussian noise, Laplacian noise, shot noise, varying
   mesh resolutions, occlusion, and clutter. Moreover, we tested our
   TriSI-based 3D object recognition algorithm on four standard datasets.
   The experimental results show that our algorithm achieved the best
   overall recognition results on these datasets. (C) 2014 Elsevier Inc.
   All rights reserved.}},
DOI = {{10.1016/j.ins.2014.09.015}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
ResearcherID-Numbers = {{Sohel, Ferdous/C-2428-2013
   }},
ORCID-Numbers = {{Sohel, Ferdous/0000-0003-1557-4907
   Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000345480900012}},
}

@inproceedings{ ISI:000381569900051,
Author = {Betta, G. and Capriglione, D. and Corvino, M. and Gasparetto, M. and
   Zappa, E. and Liguori, C. and Paolillo, A.},
Book-Group-Author = {{IEEE}},
Title = {{A proposal for improving the performance of face recognition systems
   based on 3d features}},
Booktitle = {{2015 18TH AISEM ANNUAL CONFERENCE}},
Year = {{2015}},
Note = {{18th AISEM Annual Conference on Sensors and Microsystems, Trento, ITALY,
   FEB 03-05, 2015}},
Organization = {{IEEE; Fondazione Bruno Kessler; Univ Trento; Italian Assoc Sensors \&
   Microsystems}},
Abstract = {{In this paper a suitable methodology for the improvement of the
   reliability of results in classification systems based on 3D images is
   proposed. More in detail, it is based on the knowledge of the
   uncertainty of the features constituting the 3D image (obtained
   processing a pair of two 2D stereoscopic images) and on a suitable
   statistical approach providing a confidence level to the classification
   result. These pieces of information are then managed in order to improve
   the classification performance in terms of correct classification and
   missed classification percentages. The experimental results, obtained
   applying the methodology on an Active Appearance Models algorithm, a
   popular method for face recognition based on 3D features, show that,
   compared with a traditional approach (which generally does not take into
   account the uncertainty on 3D features), the proposed methodology allows
   to significantly improve the classification performance even in
   scenarios characterized by a high uncertainty.}},
ISBN = {{978-1-4799-8591-3}},
Unique-ID = {{ISI:000381569900051}},
}

@inproceedings{ ISI:000382327100024,
Author = {Wang, Yuanyuan and Zhu, Xiao Xiang},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane, G}},
Title = {{SEMANTIC INTERPRETATION OF INSAR ESTIMATES USING OPTICAL IMAGES WITH
   APPLICATION TO URBAN INFRASTRUCTURE MONITORING}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W3}},
Pages = {{153-160}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{Synthetic aperture radar interferometry (InSAR) has been an established
   method for long term large area monitoring. Since the launch of
   meter-resolution spaceborne SAR sensors, the InSAR community has shown
   that even individual buildings can be monitored in high level of detail.
   However, the current deformation analysis still remains at a primitive
   stage of pixel-wise motion parameter inversion and manual identification
   of the regions of interest. We are aiming at developing an automatic
   urban infrastructure monitoring approach by combining InSAR and the
   semantics derived from optical images, so that the deformation analysis
   can be done systematically in the semantic/object level. This paper
   explains how we transfer the semantic meaning derived from optical image
   to the InSAR point clouds, and hence different semantic classes in the
   InSAR point cloud can be automatically extracted and monitored. Examples
   on bridges and railway monitoring are demonstrated.}},
DOI = {{10.5194/isprsarchives-XL-3-W3-153-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000382327100024}},
}

@inproceedings{ ISI:000382327100086,
Author = {Gorte, Ben and Elberink, Sander Oude and Sirmacek, Beril and Wang, Jinhu},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane, G}},
Title = {{IQPC 2015 TRACK: TREE SEPARATION AND CLASSIFICATION IN MOBILE MAPPING
   LIDAR DATA}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W3}},
Pages = {{607-612}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{The European FP7 project IQmulus yearly organizes several processing
   contests, where submissions are requested for novel algorithms for point
   cloud and other big geodata processing. This paper describes the set-up
   and execution of a contest having the purpose to evaluate
   state-of-the-art algorithms for Mobile Mapping System point clouds, in
   order to detect and identify (individual) trees. By the nature of MMS
   these are trees in the vicinity of the road network (rather than in
   forests). Therefore, part of the challenge is distinguishing between
   trees and other objects, such as buildings, street furniture, cars etc.
   Three submitted segmentation and classification algorithms are thus
   evaluated.}},
DOI = {{10.5194/isprsarchives-XL-3-W3-607-2015}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Elberink, Sander Oude/D-3829-2009
   }},
ORCID-Numbers = {{Wang, Jinhu/0000-0001-7473-4152
   Gorte, Bernardus/0000-0001-8953-6394}},
Unique-ID = {{ISI:000382327100086}},
}

@inproceedings{ ISI:000380533900053,
Author = {Song, Soohwan and Jo, Sungho},
Editor = {{Kim, JH and Yang, W and Jo, J and Sincak, P and Myung, H}},
Title = {{Traversability Classification Using Super-voxel Method in Unstructured
   Terrain}},
Booktitle = {{ROBOT INTELLIGENCE TECHNOLOGY ANDAPPLICATIONS 3}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{345}},
Pages = {{595-604}},
Note = {{3rd International Conference on Robot Intelligence Technology and
   Applications, Beijing, PEOPLES R CHINA, NOV 06-08, 2014}},
Abstract = {{Estimating the traversability of terrain in an unstructured outdoor
   environment is one of the challenging issues in autonomous vehicles.
   When dealing with a large 3D point cloud, the computational cost of
   processing all of the individual points is very high. Thus voxelization
   methods are used extensively. In this paper, we propose a more
   fine-grained voxelization algorithm in the context of unstructured
   terrain classification. While the current shape of a voxel is a
   fixed-length cubic, we construct a flexible shape voxel which has
   spatial and geometrical properties. Furthermore, we propose a new shape
   histogram feature that represents the statistical characteristics of 3D
   points. The proposed method was tested using data obtained from
   unstructured outdoor environments for performance evaluation.}},
DOI = {{10.1007/978-3-319-16841-8\_53}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-16841-8; 978-3-319-16840-1}},
Unique-ID = {{ISI:000380533900053}},
}

@inproceedings{ ISI:000382326300052,
Author = {Shahzad, M. and Zhu, X. X.},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Rottensteiner, F and Yang, M and Christophe, S and Coltekin, A and Bredif, M}},
Title = {{RECONSTRUCTION OF BUILDING FOOTPRINTS USING SPACEBORNE TOMOSAR POINT
   CLOUDS}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{II-3}},
Number = {{W5}},
Pages = {{385-392}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{This paper presents an approach that automatically (but parametrically)
   reconstructs 2-D/3-D building footprints using 3-D synthetic aperture
   radar (SAR) tomography (TomoSAR) point clouds. These point clouds are
   generated by processing SAR image stacks via SAR tomographic inversion.
   The proposed approach reconstructs the building outline by exploiting
   both the roof and facade points. Initial building footprints are derived
   by applying the alpha shapes method on pre-segmented point clusters of
   individual buildings. A recursive angular deviation based refinement is
   then carried out to obtain refined/smoothed 2-D polygonal boundaries. A
   robust fusion framework then fuses the information pertaining to
   building facades to the smoothed polygons. Afterwards, a rectilinear
   building identification procedure is adopted and constraints are added
   to yield geometrically correct and visually aesthetic building shapes.
   The proposed approach is illustrated and validated using TomoSAR point
   clouds generated from a stack of TerraSAR-X high-resolution spotlight
   images from ascending orbit covering approximately 1.5 km(2) area in the
   city of Berlin, Germany.}},
DOI = {{10.5194/isprsannals-II-3-W5-385-2015}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000382326300052}},
}

@inproceedings{ ISI:000380558900020,
Author = {Pang, Guan and Qiu, Rongqi and Huang, Jing and You, Suya and Neumann,
   Ulrich},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Industrial Point Cloud Modeling and Recognition}},
Booktitle = {{2015 14th IAPR International Conference on Machine Vision Applications
   (MVA)}},
Year = {{2015}},
Pages = {{22-25}},
Note = {{14th IAPR International Conference on Machine Vision Applications (MVA),
   Tokyo, JAPAN, MAY 18-22, 2015}},
Organization = {{MVA Organisation; IAPR TC-8; National Institute of Advanced Industrial
   Science and Technology (AIST); The Telecommunications Advancement
   Foundation; KDDI Foundation}},
Abstract = {{3D modeling of point clouds is an important but time-consuming process,
   inspiring extensive research in automatic methods. Prior efforts focus
   on primitive geometry, street structures or indoor objects, but
   industrial data has rarely been pursued. Our work presents a method for
   automatic modeling and recognition of 3D industrial site point clouds,
   dividing the task into 3 separate sub-problems: pipe modeling, plane
   classification, and object recognition. The results are integrated to
   obtain the complete model, revealing some issues during the integration,
   solved by utilizing information gained from each individual process.
   Experiments show that the presented method automatically models large
   and complex industrial scenes with a quality that outperforms leading
   commercial modeling software and is comparable to professional hand-made
   models.}},
ISBN = {{978-4-9011-2214-6}},
Unique-ID = {{ISI:000380558900020}},
}

@inproceedings{ ISI:000380475300697,
Author = {Arora, Sourabh and Chawla, Shikha},
Book-Group-Author = {{IEEE}},
Title = {{An Intensified Approach to Face Recognition through Average Half Face}},
Booktitle = {{2015 ANNUAL IEEE INDIA CONFERENCE (INDICON)}},
Series = {{Annual IEEE India Conference}},
Year = {{2015}},
Note = {{12 IEEE Int C Elect Energy Env Communications Computer Control, New
   Delhi, INDIA, DEC 17-20, 2015}},
Abstract = {{Face recognition has broad excitement in the latest trend in image
   processing. Face recognition refers to identify a specific individual in
   digital image by analyzing and comparing patterns. It has numerous
   benefits which attract every sector but there are some issues such as
   more time consumption and lesser accuracy which degrade the user
   services. To solve this problem we proposed a highly accurate and fast
   method to reduce the execution time. The proposed method uses average
   half face approach because overall system's accuracy is better in it
   rather than using the original full face image. The proposed method can
   be used to recognize both 2D and 3D images. It mainly includes the
   average half face creation, feature detection, full face recognition
   through average half face using distance metrics and finally checking
   system's accuracy along with time consumption. The proposed method is
   based on eye, nose and mouth detection.}},
ISSN = {{2325-940X}},
ISBN = {{978-1-4673-6540-6}},
Unique-ID = {{ISI:000380475300697}},
}

@inproceedings{ ISI:000380475300183,
Author = {Suja, P. and Krishnasri, D. and Tripathi, Shikha},
Book-Group-Author = {{IEEE}},
Title = {{Pose Invariant Method for Emotion Recognition from 3D Images}},
Booktitle = {{2015 ANNUAL IEEE INDIA CONFERENCE (INDICON)}},
Series = {{Annual IEEE India Conference}},
Year = {{2015}},
Note = {{12 IEEE Int C Elect Energy Env Communications Computer Control, New
   Delhi, INDIA, DEC 17-20, 2015}},
Abstract = {{Information about the emotional state of a person can be inferred from
   facial expressions. Emotion recognition has become an active research
   area in recent years in various fields such as Human Robot Interaction (
   HRI), medicine, intelligent vehicle, etc., The challenges in emotion
   recognition from images with pose variations, motivates researchers to
   explore further. In this paper, we have proposed a method based on
   geometric features, considering images of 7 yaw angles (-45 degrees,-30
   degrees,-15 degrees, 0 degrees,+15 degrees,+30 degrees,+45 degrees) from
   BU3DFE database. Most of the work that has been reported considered only
   positive yaw angles. In this work, we have included both positive and
   negative yaw angles. In the proposed method, feature extraction is
   carried out by concatenating distance and angle vectors between the
   feature points, and classification is performed using neural network.
   The results obtained for images with pose variations are encouraging and
   comparable with literature where work has been performed on pitch and
   yaw angles. Using our proposed method non-frontal views achieve similar
   accuracy when compared to frontal view thus making it pose invariant.
   The proposed method may be implemented for pitch and yaw angles in
   future.}},
ISSN = {{2325-940X}},
ISBN = {{978-1-4673-6540-6}},
Unique-ID = {{ISI:000380475300183}},
}

@inproceedings{ ISI:000380611200033,
Author = {Sano, Masayuki and Matsumoto, Kazuki and Thomas, Bruce H. and Saito,
   Hideo},
Editor = {{Sandor, C and Lindeman, R and Mayol-Cuevas, W and Sakata, N and Newcombe, R and Teichrieb, V}},
Title = {{Rubix: Dynamic Spatial Augmented Reality by Extraction of Plane Regions
   with a RGB-D Camera}},
Booktitle = {{2015 IEEE International Symposium on Mixed and Augmented Reality}},
Year = {{2015}},
Pages = {{148-151}},
Note = {{Proceedings of the 2015 IEEE International Symposium on Mixed and
   Augmented Reality, Los Alamitos, CA, SEP 29-OCT 03, 2015}},
Abstract = {{Dynamic spatial augmented reality requires accurate real-time 3D pose
   information of the physical objects that are to be projected onto.
   Previous depth-based methods for tracking objects required strong
   features to enable recognition; making it difficult to estimate an
   accurate 6DOF pose for physical objects with a small set of recognizable
   features (such as a non-textured cube). We propose a more accurate
   method with fewer limitations for the pose estimation of a tangible
   object that has known planar faces and using depth data from an RGB-D
   camera only. In this paper, the physical object's shape is limited to
   cubes of different sizes. We apply this new tracking method to achieve
   dynamic projections onto these cubes. In our method, 3D points from an
   RGB-D camera are divided into a cluster of planar regions, and the point
   cloud inside each face of the object is fitted to an already-known
   geometric model of a cube. With the 6DOF pose of the physical object,
   SAR generated imagery is then projected correctly onto the physical
   object. The 6DOF tracking is designed to support tangible interactions
   with the physical object. We implemented example interactive
   applications with one or multiple cubes to show the capability of our
   method.}},
DOI = {{10.1109/ISMAR.2015.43}},
ISBN = {{978-1-4673-7660-0}},
Unique-ID = {{ISI:000380611200033}},
}

@inproceedings{ ISI:000380605400006,
Author = {Rai, Marwa Chendeb E. L. and Werghi, Naoufel and Al Muhairi, Hassan and
   Alsafar, Habiba},
Book-Group-Author = {{IEEE}},
Title = {{Using facial images for the diagnosis of genetic syndromes: A survey}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, SIGNAL PROCESSING, AND
   THEIR APPLICATIONS (ICCSPA'15)}},
Year = {{2015}},
Note = {{2015 International Conference on Communications, Signal Processing, and
   their Applications (ICCSPA'15), Sharjah, U ARAB EMIRATES, FEB 17-19,
   2015}},
Organization = {{Amer Univ Sharjah; IEEE}},
Abstract = {{The analysis of facial appearance is significant to an early diagnosis
   of medical genetic diseases. The fast development of image processing
   and machine learning techniques facilitates the detection of facial
   dysmorphic features. This paper is a survey of the recent studies
   developed for the screening of genetic abnormalities across the facial
   features obtained from two dimensional and three dimensional images.}},
ISBN = {{978-1-4799-6532-8}},
ResearcherID-Numbers = {{Werghi, Naoufel/D-2398-2018}},
ORCID-Numbers = {{Werghi, Naoufel/0000-0002-5542-448X}},
Unique-ID = {{ISI:000380605400006}},
}

@inproceedings{ ISI:000380407300092,
Author = {Pawar, Asmita A. and Patil, Nitin N.},
Book-Group-Author = {{IEEE}},
Title = {{Recognition of 3-D Faces with Missing Parts and Line Scratch Removal
   using New Technique}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)}},
Year = {{2015}},
Note = {{International Conference on Pervasive Computing (ICPC), Pune, INDIA, JAN
   08-10, 2015}},
Organization = {{IEEE Pune Sect; IEEE Comp Soc; Savitribai Phule Pune Univ; IEEE Commun
   Soc Pune Chapter; Sinhgad Inst; Sakal Times}},
Abstract = {{This paper presents an implementation of face recognition, which is a
   very important task of human face identification. Line scratch detection
   in images is a highly challenging situation because of various
   characteristics of this defect. Few characteristics are considered with
   the different texture and geometry of images. We propose a useful
   algorithm for frame-by-frame line scratch detection in face image which
   deals with 3D approach and a filtering of detection. The temporary
   filtering algorithm can be used to remove false detection due to thin
   vertical structures by detecting the scratches on an image. Experimental
   evaluation can be detecting the lines and scratches on a face image and
   they used to solve this difficult approach. Our method is used with
   missing parts in an image. Three-dimensional face recognition is an
   extended method of facial recognition is considered according with the
   geometry and texture of a face. It has been elaborated that 3D face
   recognition methods can provide high accuracy as well as high detection
   with a comparison of 2D recognition. 3D avoids such mismatch effect of
   2D face recognition algorithms. Additionally, most 3D scanners achieve
   both a 3D mesh and the texture of a face image. This allows combining
   the output of pure 3D matches with the more traditional algorithms of 2D
   face recognition, thus producing better performance.}},
ISBN = {{978-1-4799-6272-3}},
Unique-ID = {{ISI:000380407300092}},
}

@inproceedings{ ISI:000380584900016,
Author = {Lin, Xiangguo and Zhang, Jixian},
Editor = {{Zhang, J and Lu, Z and Zeng, Y}},
Title = {{SEGMENTATION-BASED GROUND POINTS DETECTION FROM MOBILE LASER SCANNING
   POINT CLOUD}},
Booktitle = {{IWIDF 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{47}},
Number = {{W4}},
Pages = {{99-102}},
Note = {{International Workshop on Image and Data Fusion (IWIDF), Kona, HI, JUL
   21-23, 2015}},
Abstract = {{In most Mobile Laser Scanning (MLS) applications, filtering is a
   necessary step. In this paper, a segmentation-based filtering method is
   proposed for MLS point cloud, where a segment rather than an individual
   point is the basic processing unit. Particularly, the MLS point cloud in
   some blocks are clustered into segments by a surface growing algorithm,
   then the object segments are detected and removed. A segment-based
   filtering method is employed to detect the ground segments. Two MLS
   point cloud datasets are used to evaluate the proposed method.
   Experiments indicate that, compared with the classic progressive TIN
   (Triangulated Irregular Network) densification algorithm, the proposed
   method is capable of reducing the omission error, the commission error
   and total error by 3.62\%, 7.87\% and 5.54\% on average, respectively.}},
DOI = {{10.5194/isprsarchives-XL-7-W4-99-2015}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000380584900016}},
}

@inproceedings{ ISI:000380388000083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 4}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380388000083}},
}

@inproceedings{ ISI:000380388000012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 4}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380388000012}},
}

@inproceedings{ ISI:000380379900083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 1}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380379900083}},
}

@inproceedings{ ISI:000380379900012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 1}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380379900012}},
}

@inproceedings{ ISI:000380393900083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 2}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380393900083}},
}

@inproceedings{ ISI:000380393900012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 2}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380393900012}},
}

@inproceedings{ ISI:000380377400083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 3}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380377400083}},
}

@inproceedings{ ISI:000380377400012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 3}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380377400012}},
}

@inproceedings{ ISI:000380390600083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 5}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380390600083}},
}

@inproceedings{ ISI:000380390600012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 5}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380390600012}},
}

@inproceedings{ ISI:000380483800005,
Author = {Liu, Jian and Zhang, Quan and Zhang, Chen and Tang, Chaojing},
Book-Group-Author = {{IEEE}},
Title = {{ROBUST NOSE TIP DETECTION FOR FACE RANGE IMAGES BASED ON LOCAL FEATURES
   IN SCALE- SPACE}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON 3D IMAGING (IC3D)}},
Series = {{International Conference on 3D Imaging}},
Year = {{2015}},
Note = {{International Conference on 3D Imaging (IC3D), Liege, BELGIUM, DEC
   14-15, 2015}},
Organization = {{The Inst of Elect and Elect Engn; Signal Proc Soc; 3D Stereo Media}},
Abstract = {{Being the most distinct feature point in 3D facial landmarks, nose tip
   plays a significant role in 3D facial studies such as face detection,
   face recognition, facial features extraction, face alignment, etc.
   Successful detection of nose tip can facilitate many tasks of 3D facial
   studies. In this paper, we propose a novel method to detect nose tip
   robustly. The method is robust to noise, needs not training, can handle
   large rotations and occlusions. To reduce computational cost, we first
   remove small isolated regions from the input range image, then establish
   scale-space by robust smoothing the preprocessed range image. In each
   scale of the scale-space, the Multi-angle Energy (ME) of each point is
   computed and sorted in descending order. Then the first. points in the
   descending order list are obtained and hierarchical clustering method is
   used to cluster these points. In the first h largest clusters, we can
   find one point with the largest ME. For all scales of the scale-space,
   we get a series of such points which are treated as nose tip candidates.
   For these candidates, we apply hierarchical clustering again. In the
   obtained largest cluster, we compute the mean value of ME. The ME of
   nose tip will be closest to the mean value. We evaluate our method in
   two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The
   experimental results verify the robustness of our method with a high
   nose tip detection rate.}},
ISSN = {{2379-1772}},
ISBN = {{978-1-5090-1265-7}},
Unique-ID = {{ISI:000380483800005}},
}

@inproceedings{ ISI:000378887900113,
Author = {Polewski, Przemyslaw and Yao, Wei and Heurich, Marco and Krzystek, Peter
   and Stilla, Uwe},
Book-Group-Author = {{IEEE}},
Title = {{Active learning approach to detecting standing dead trees from ALS point
   clouds combined with aerial infrared imagery}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   WORKSHOPS (CVPRW)}},
Series = {{IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops}},
Year = {{2015}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{Due to their role in certain essential forest processes, dead trees are
   an interesting object of study within the environmental and forest
   sciences. This paper describes an active learning-based approach to
   detecting individual standing dead trees, known as snags, from ALS point
   clouds and aerial color infrared imagery. We first segment individual
   trees within the 3D point cloud and subsequently find an approximate
   bounding polygon for each tree within the image. We utilize these
   polygons to extract features based on the pixel intensity values in the
   visible and infrared bands, which forms the basis for classifying the
   associated trees as either dead or living. We define a two-step scheme
   of selecting a small subset of training examples from a large initially
   unlabeled set of objects. In the first step, a greedy approximation of
   the kernelized feature matrix is conducted, yielding a smaller pool of
   the most representative objects. We then perform active learning on this
   moderate-sized pool, using expected error reduction as the basic method.
   We explore how the use of semi-supervised classifiers with minimum
   entropy regularizers can benefit the learning process. Based on
   validation with reference data manually labeled on images from the
   Bavarian Forest National Park, our method attains an overall accuracy of
   up to 89\% with less than 100 training examples, which corresponds to
   10\% of the pre-selected data pool.}},
ISSN = {{2160-7508}},
ISBN = {{978-1-4673-6759-2}},
ResearcherID-Numbers = {{Heurich, Marco/O-4653-2014
   Yao, Wei/J-7423-2019}},
ORCID-Numbers = {{Heurich, Marco/0000-0003-0051-2930
   Yao, Wei/0000-0001-7704-0615}},
Unique-ID = {{ISI:000378887900113}},
}

@inproceedings{ ISI:000376674000399,
Author = {Sleiman, J. Bou and Perraud, J. B. and Bousquet, B. and Palka, N. and
   Guillet, J. P. and Mounaix, P.},
Book-Group-Author = {{IEEE}},
Title = {{Chemical imaging and quantification of RDX/PETN mixtures by PLS applied
   on terahertz time-domain spectroscopy}},
Booktitle = {{2015 40TH INTERNATIONAL CONFERENCE ON INFRARED, MILLIMETER AND TERAHERTZ
   WAVES (IRMMW-THZ)}},
Series = {{International Conference on Infrared Millimeter and Terahertz Waves}},
Year = {{2015}},
Note = {{40th International Conference on Infrared, Millimeter, and Terahertz
   Waves (IRMMW-THz), Chinese Univ Hong Kong, Hong Kong, PEOPLES R CHINA,
   AUG 23-28, 2015}},
Organization = {{IEEE; IEEE Microwave Theory \& Tech Soc; Virginal Diodes Inc; TeraView;
   Microtech Instruments Inc; Hong Kong Univ Sci \& Technol, Dept Elect \&
   Comp Engn; Croucher Fdn; Capital Normal Univ; K C Wong Educ Fdn;
   Meetings \& Exhibit Hong Kong; Army Res Off; NSF}},
Abstract = {{Chemometric analysis was applied on terahertz absorbance 3D images, in
   transmission. The goal is to automatically discriminate some explosives
   on images and quantify mixtures of RDX/PETN in the frequency range of
   0.2 - 3 THz. Partial Least Square (PLS) was applied on THz absorbance
   multispectral images to quantify individual product inside pure samples
   and mixtures at each pixel on the image. Then the best score obtained is
   used to display the samples' images and provide the optimal frequencies
   combination for recognition purpose.}},
ISSN = {{2162-2027}},
ISBN = {{978-1-4799-8272-1}},
ResearcherID-Numbers = {{Palka, Norbert/G-9652-2018
   Mounaix, Patrick/E-1653-2012}},
ORCID-Numbers = {{Palka, Norbert/0000-0002-1931-876X
   }},
Unique-ID = {{ISI:000376674000399}},
}

@article{ ISI:000376676700001,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Title = {{Wavelet and decision fusion-based 3D face recognition from range image}},
Journal = {{INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{2}},
Number = {{4}},
Pages = {{306-324}},
Abstract = {{The pivotal purpose of this literature is to describe a new approach for
   3D faces recognition in the presence of pose, expression, as well as
   illumination based on fusion of wavelet coefficients. In addition,
   authors have investigated the recognition rates with series of
   experiments by ANN and K-NN. To demonstrate the robustness of the
   recognition system, Frav3D face dataset has been considered for this
   investigation. The series of variations in classifiers and their
   performance accuracies have also ranked using Wilcoxon signed-rank
   method based on their recognition rates. Range face images from
   synthesised database are processed by the Haar wavelet transform, and
   corresponding subimages are created for final fused face dataset. The
   synthesised database is created by collecting frontal face images along
   with images obtained after registration of rotated images using ERFI
   model. Moreover, to discover the features for face recognition, PCA is
   applied on fused face images. Finally, two supervised classifiers
   namely, ANN and K-NN are tested for recognition purpose. The obtained
   maximum recognition rate from our proposed methodology is 96.25\% using
   ANN classifier and 90\% of recognition rate from K-NN.}},
DOI = {{10.1504/IJAPR.2015.075942}},
ISSN = {{2049-887X}},
EISSN = {{2049-8888}},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000376676700001}},
}

@inproceedings{ ISI:000371977803080,
Author = {Batabyal, Tamal and Vaccari, Andrea and Acton, Scott T.},
Book-Group-Author = {{IEEE}},
Title = {{UGraSP: A UNIFIED FRAMEWORK FOR ACTIVITY RECOGNITION AND PERSON
   IDENTIFICATION USING GRAPH SIGNAL PROCESSING}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{3270-3274}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{With the growing availability and wide distribution of low-cost,
   high-performance 3D imaging sensors, the image analysis community has
   witnessed an increased demand for solutions to the challenges of
   activity recognition and person identification. We propose an integrated
   framework, based on graph signal processing, that simultaneously
   performs both tasks using a single set of features. The novelty of our
   approach is based on the fact that the set of features used for activity
   recognition accommodates person identification without additional
   computation. The analysis is based on the extracted structure-invariant
   graph (skeleton). The Laplacian of the skeleton is used both to identify
   the person and recognize the performed activity. While person
   identification is achieved directly from the analysis of the Laplacian,
   activity recognition is obtained after transformation, into the graph
   spectral domain, of the vectorized form of the skeletal joints 3D
   coordinates. Feature vectors for activity recognition are then derived,
   in this domain, from the covariance matrices evaluated over fixed-length
   sequential video segments. Both classification tasks are implemented
   using linear support vector machines (SVM). When applied to real
   activity datasets, our approach shows an improved performance over the
   existing state-of-the-art.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977803080}},
}

@inproceedings{ ISI:000371977804130,
Author = {Arteaga, Reynaldo J. and Ruuth, Steven J.},
Book-Group-Author = {{IEEE}},
Title = {{LAPLACE-BELTRAMI SPECTRA FOR SHAPE COMPARISON OF SURFACES IN 3D USING
   THE CLOSEST POINT METHOD}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{4511-4515}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{The need to compare separate objects arises in a wide range of
   applications. In one approach for comparing objects, `ShapeDNA' is
   constructed to give a numerical fingerprint representing an individual
   object. Shape-DNA is a cropped set of eigenvalues of the
   Laplace-Beltrami operator for the surface of the object. In this paper,
   we compute the Shape-DNA of surfaces using the closest point method. Our
   approach may be applied to a variety of surface representations
   including triangulations, point clouds and certain analytical shapes. A
   2D multidimensional scaling plot illustrates that similar objects form
   groups based on the Shape-DNAs. Our method has the benefit that it may
   be applied to surfaces defined by dense point clouds without requiring
   the construction of point connectivity.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977804130}},
}

@inproceedings{ ISI:000373207200086,
Author = {Gutfeter, Weronika and Pacut, Andrzej},
Editor = {{Jedrzejowicz, P and Nguyen, NT and Hong, TP and Czarnowski, I}},
Title = {{Face 3D biometrics goes mobile: searching for applications of portable
   depth sensor in face recognition}},
Booktitle = {{2015 IEEE 2ND INTERNATIONAL CONFERENCE ON CYBERNETICS (CYBCONF)}},
Year = {{2015}},
Pages = {{489-494}},
Note = {{IEEE 2nd International Conference on Cybernetics (CYBCONF), Gdynia,
   POLAND, JUN 24-26, 2015}},
Organization = {{IEEE; IEEE Poland Sect; Gdynia Maritime Univ; IEEE Syst, Man, \&
   Cybernet Soc; Gdynia Maritime Univ Students \& Alumni Fdn; IEEE SMC Tech
   Comm Computat Collect Intelligence; Polish Acad Sci, Comm Informat;
   Polish Soc Artificial Intelligence}},
Abstract = {{This paper presents an acquisition procedure and method of processing
   spatial images for face recognition with the use of a novel type of
   scanning device, namely mobile depth sensor Structure. Depth sensors,
   often called RGBD cameras, are able to deliver 3D images with a frame
   rate 30-60 frames per second, however they have relatively low
   resolution and a high level of noise. This kind of data is compared here
   with a high quality scans enrolled by the structural light scanner, for
   which the acquisition time is approximately 1.5 s for a single image,
   and which - because of its size - cannot be classified as a portable
   device. The purpose of this work was to find the method that will allow
   us to extract spatial features from mobile data sources analyzed here
   only in a static context. We transform the 3D data into local surface
   features and then into vectors of unified length by use of the Moving
   Least Squares method applied to a predefined grid of points on a
   reference cylinder. The feature matrices were calculated for various
   image features, and used in PCA analysis. Finally, the verification
   errors were calculated and compared to those obtained for stationary
   devices. The results show that single-image mobile sensor images lead to
   the results inferior to those of stationary sensors. However, we suggest
   a dynamic depth stream processing as the next step in the evolution of
   the described method. The presented results show that by including
   multi-frame processing into our method, it is likely to gain the
   accuracy similar to those obtained for a stationary device under
   controlled laboratory conditions.}},
ISBN = {{978-1-4799-8322-3}},
Unique-ID = {{ISI:000373207200086}},
}

@inproceedings{ ISI:000352725200030,
Author = {Shahzad, M. and Schmitt, M. and Zhu, X. X.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{SEGMENTATION AND CROWN PARAMETER EXTRACTION OF INDIVIDUAL TREES IN AN
   AIRBORNE TOMOSAR POINT CLOUD}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W2}},
Pages = {{205-209}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{The analysis of individual trees is an important field of research in
   the forest remote sensing community. While the current state-of-the-art
   mostly focuses on the exploitation of optical imagery and airborne LiDAR
   data, modern SAR sensors have not yet met the interest of the research
   community in that regard. This paper describes how several critical
   parameters of individual deciduous trees can be extraced from airborne
   multi-aspect TomoSAR point clouds: First, the point cloud is segmented
   by unsupervised mean shift clustering. Then ellipsoid models are fitted
   to the points of each cluster. Finally, from these 3D ellipsoids the
   geometrical tree parameters location, height and crown radius are
   extracted. Evaluation with respect to a manually derived reference
   dataset prove that almost 86\% of all trees are localized, thus
   providing a promising perspective for further research towards
   individual tree recognition from SAR data.}},
DOI = {{10.5194/isprsarchives-XL-3-W2-205-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000352725200030}},
}

@inproceedings{ ISI:000352725200038,
Author = {Vetrivel, A. and Gerke, M. and Kerle, N. and Vosselman, G.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{Segmentation of UAV-based images incorporating 3D point cloud
   information}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W2}},
Pages = {{261-268}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{Numerous applications related to urban scene analysis demand automatic
   recognition of buildings and distinct sub-elements. For example, if
   LiDAR data is available, only 3D information could be leveraged for the
   segmentation. However, this poses several risks, for instance, the
   in-plane objects cannot be distinguished from their surroundings. On the
   other hand, if only image based segmentation is performed, the geometric
   features (e.g., normal orientation, planarity) are not readily
   available. This renders the task of detecting the distinct sub-elements
   of the building with similar radiometric characteristic infeasible. In
   this paper the individual sub-elements of buildings are recognized
   through sub-segmentation of the building using geometric and radiometric
   characteristics jointly. 3D points generated from Unmanned Aerial
   Vehicle (UAV) images are used for inferring the geometric
   characteristics of roofs and facades of the building. However, the
   image-based 3D points are noisy, error prone and often contain gaps.
   Hence the segmentation in 3D space is not appropriate. Therefore, we
   propose to perform segmentation in image space using geometric features
   from the 3D point cloud along with the radiometric features. The initial
   detection of buildings in 3D point cloud is followed by the segmentation
   in image space using the region growing approach by utilizing various
   radiometric and 3D point cloud features. The developed method was tested
   using two data sets obtained with UAV images with a ground resolution of
   around 1-2 cm. The developed method accurately segmented most of the
   building elements when compared to the plane-based segmentation using 3D
   point cloud alone.}},
DOI = {{10.5194/isprsarchives-XL-3-W2-261-2015}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Vosselman, George/D-3985-2009
   Gerke, Markus/A-8791-2012
   Kerle, Norman/A-5508-2010}},
ORCID-Numbers = {{Vosselman, George/0000-0001-8813-8028
   Gerke, Markus/0000-0002-2221-6182
   }},
Unique-ID = {{ISI:000352725200038}},
}

@inproceedings{ ISI:000370974903006,
Author = {Linder, Timm and Wehner, Sven and Arras, Kai O.},
Book-Group-Author = {{IEEE}},
Title = {{Real-Time Full-Body Human Gender Recognition in (RGB)-D Data}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)}},
Series = {{IEEE International Conference on Robotics and Automation ICRA}},
Year = {{2015}},
Pages = {{3039-3045}},
Note = {{IEEE International Conference on Robotics and Automation (ICRA),
   Seattle, WA, MAY 26-30, 2015}},
Organization = {{IEEE}},
Abstract = {{Understanding social context is an important skill for robots that share
   a space with humans. In this paper, we address the problem of
   recognizing gender, a key piece of information when interacting with
   people and understanding human social relations and rules. Unlike
   previous work which typically considered faces or frontal body views in
   image data, we address the problem of recognizing gender in RGB-D data
   from side and back views as well. We present a large, gender-balanced,
   annotated, multi-perspective RGB-D dataset with full-body views of over
   a hundred different persons captured with both the Kinect v1 and Kinect
   v2 sensor. We then learn and compare several classifiers on the Kinect
   v2 data using a HOG baseline, two state-of-the-art deep-learning
   methods, and a recent tessellation-based learning approach. Originally
   developed for person detection in 3D data, the latter is able to learn
   the best selection, location and scale of a set of simple point cloud
   features. We show that for gender recognition, it outperforms the other
   approaches for both standing and walking people while being very
   efficient to compute with classification rates up to 150 Hz.}},
ISSN = {{1050-4729}},
ISBN = {{978-1-4799-6923-4}},
Unique-ID = {{ISI:000370974903006}},
}

@inproceedings{ ISI:000370814200017,
Author = {Bull, Geoff and Gao, Junbin and Antolovich, Michael},
Editor = {{Battiato, S and Coquillart, S and Pettre, J and Laramee, RS and Kerren, A and Braz, J}},
Title = {{Rock Fragment Boundary Detection Using Compressed Random Features}},
Booktitle = {{COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS - THEORY AND
   APPLICATIONS, VISIGRAPP 2014}},
Series = {{Communications in Computer and Information Science}},
Year = {{2015}},
Volume = {{550}},
Pages = {{273-286}},
Note = {{International Joint Conference on Computer Vision, Imaging and Computer
   Graphics Theory and Applications (VISIGRAPP), Lisbon, PORTUGAL, JAN
   05-08, 2014}},
Organization = {{Inst Syst \& Technologies Informat, Control \& Commun; Eurographics;
   IEEE Comp Soc; IEEE VGMT; IEEE TCMC}},
Abstract = {{Sections of the mining industry depend on regular analysis of rock
   fragmentation to detect trends that may affect safety or production. The
   limitations inherent in 2D imaging analysis mean that human input is
   typically needed for delineating individual rock fragments. Although
   recent advances in 3D image processing have diminished the need for
   human input, it is often infeasible for many mines to upgrade their
   existing 2D imaging systems to 3D. Hence there is still a need to
   improve delineation in 2D images. This paper proposes a method for
   delineating rock fragments by classifying compressed Haar-like features
   extracted from small image patches. The optimum size of the image
   patches and the number of compressed features are determined
   empirically. Experimental results show the proposed method gives
   superior results to the commonly used watershed algorithm, and
   compressing features improves computational efficiency such that a
   machine learning approach is practical.}},
DOI = {{10.1007/978-3-319-25117-2\_17}},
ISSN = {{1865-0929}},
ISBN = {{978-3-319-25117-2; 978-3-319-25116-5}},
ResearcherID-Numbers = {{Bull, Geoff/L-2805-2018
   Antolovich, Michael/C-1656-2012
   Gao, Junbin/A-1766-2009}},
ORCID-Numbers = {{Bull, Geoff/0000-0002-9818-5132
   Antolovich, Michael/0000-0003-2601-8332
   Gao, Junbin/0000-0001-9803-0256}},
Unique-ID = {{ISI:000370814200017}},
}

@inproceedings{ ISI:000369099700018,
Author = {Aneja, D. and Vora, S. R. and Camci, E. D. and Shapiro, L. G. and Cox,
   T. C.},
Editor = {{Traina, C and Rodrigues, PP and Kane, B and Marques, PMD and Traina, AJM}},
Title = {{Automated Detection of 3D Landmarks for the Elimination of
   Non-Biological Variation in Geometric Morphometric Analyses}},
Booktitle = {{2015 IEEE 28TH INTERNATIONAL SYMPOSIUM ON COMPUTER-BASED MEDICAL SYSTEMS
   (CBMS)}},
Series = {{IEEE International Symposium on Computer-Based Medical Systems}},
Year = {{2015}},
Pages = {{78-83}},
Note = {{28th IEEE International Symposium on Computer-Based Medical Systems
   (CBMS), Univ Sao Paulo, Sao Paulo, BRAZIL, JUN 22-25, 2015}},
Organization = {{IEEE; IEEE Comp Soc; ICMC; SUS; Ministerio Saude; Governo Fed Patria
   Educadora Brasil; CNPq; Google Brasil; FAPESP}},
Abstract = {{Landmark-based morphometric analyses are used by anthropologists,
   developmental and evolutionary biologists to understand shape and size
   differences (eg. in the cranioskeleton) between groups of specimens. The
   standard, labor intensive approach is for researchers to manually place
   landmarks on 3D image datasets. As landmark recognition is subject to
   inaccuracies of human perception, digitization of landmark coordinates
   is typically repeated (often by more than one person) and the mean
   coordinates are used. In an attempt to improve efficiency and
   reproducibility between researchers, we have developed an algorithm to
   locate landmarks on CT mouse hemi-mandible data. The method is evaluated
   on 3D meshes of 28-day old mice, and results compared to landmarks
   manually identified by experts. Quantitative shape comparison between
   two inbred mouse strains demonstrate that data obtained using our
   algorithm also has enhanced statistical power when compared to data
   obtained by manual landmarking.}},
DOI = {{10.1109/CBMS.2015.86}},
ISSN = {{1063-7125}},
ISBN = {{978-1-4673-6775-2}},
Unique-ID = {{ISI:000369099700018}},
}

@inproceedings{ ISI:000364991200038,
Author = {De Giorgis, Nikolas and Rocca, Luigi and Puppo, Enrico},
Editor = {{Murino, V and Puppo, E}},
Title = {{Scale-Space Techniques for Fiducial Points Extraction from 3D Faces}},
Booktitle = {{IMAGE ANALYSIS AND PROCESSING - ICIAP 2015, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9279}},
Pages = {{421-431}},
Note = {{18th International Conference on Image Analysis and Processing (ICIAP),
   Genoa, ITALY, SEP 07-11, 2015}},
Organization = {{Datalogic; Google Inc; Centro Studi Gruppo Orizzonti Holding; Ansaldo
   Energia; EBIT Esaote; Softeco; eVS embedded Vis Syst S r l; 3DFlow S r
   l; Camelot Biomed Syst S r l; Ist Italiano Tecnologia, Pattern Anal \&
   Comp Vis Dept; Univ Genova; Univ Verona; Camera Commercio Genova; Comune
   Genova}},
Abstract = {{We propose a method for extracting fiducial points from human faces that
   uses 3D information only and is based on two key steps: multi-scale
   curvature analysis, and the reliable tracking of features in a
   scale-space based on curvature. Our scale-space analysis, coupled to
   careful use of prior information based on variability boundaries of
   anthropometric facial proportions, does not require a training step,
   because it makes direct use of morphological characteristics of the
   analyzed surface. The proposed method precisely identifies important
   fiducial points and is able to extract new fiducial points that were
   previously unrecognized, thus paving the way to more effective
   recognition algorithms.}},
DOI = {{10.1007/978-3-319-23231-7\_38}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-23231-7; 978-3-319-23230-0}},
ORCID-Numbers = {{Puppo, Enrico/0000-0001-9780-5283}},
Unique-ID = {{ISI:000364991200038}},
}

@inproceedings{ ISI:000365181700036,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal, JK}},
Title = {{Range Face Image Registration Using ERFI from 3D Images}},
Booktitle = {{PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF
   INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{328}},
Pages = {{323-333}},
Note = {{3rd International Conference on Frontiers in Intelligent Computing -
   Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014}},
Organization = {{Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol \& Sci, CSI Student
   Branch}},
Abstract = {{In this paper, we present a novel and robust approach for 3D faces
   registration based on Energy Range Face Image (ERFI). ERFI is the
   frontal face model for the individual people from the database. It can
   be considered as a mean frontal range face image for each person. Thus,
   the total energy of the frontal range face images has been preserved by
   ERFI. For registration purpose, an interesting point or a land mark,
   which is the nose tip (or `pronasal') from face surface is extracted.
   Then, this landmark is exploited to correct the oriented faces by
   applying the 3D geometrical rotation technique with respect to the ERFI
   model for registration purpose. During the error calculation phase,
   Manhattan distance metric between the localized `pronasal' landmark on
   face image and that of ERFI model is determined on Euclidian space. The
   accuracy is quantified with selection of cut-points `T' on measured
   Manhattan distances along yaw, pitch and roll. The proposed method has
   been tested on Frav3D database and achieved 82.5\% accurate pose
   registration.}},
DOI = {{10.1007/978-3-319-12012-6\_36}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-12012-6; 978-3-319-12011-9}},
ResearcherID-Numbers = {{Bhattacharjee, Debotosh/L-8521-2015
   }},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-4483-706X
   Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000365181700036}},
}

@inproceedings{ ISI:000365181700047,
Author = {Sivasankar, C. and Srinivasan, A.},
Editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal, JK}},
Title = {{A Framework for Human Recognition Based on Locomotive Object Extraction}},
Booktitle = {{PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF
   INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{328}},
Pages = {{431-439}},
Note = {{3rd International Conference on Frontiers in Intelligent Computing -
   Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014}},
Organization = {{Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol \& Sci, CSI Student
   Branch}},
Abstract = {{Moving Object detection based on video, of late has gained momentum in
   the field of research. Moving object detection has extensive application
   areas and is used for monitoring intelligence interaction between human
   and computer, transportation of intelligence, and navigating visual
   robotics, clarity in steering systems. It is also used in various other
   fields for diagnosing, compressing images, reconstructing 3D images,
   retrieving video images and so on. Since surveillance of human movement
   detection is subjective, the human objects are precisely detected to the
   framework proposed for human detection based on the Locomotive Object
   Extraction. The issue of illumination changes and crowded human image is
   discriminated. The image is detected through the detection feature that
   identifies head and shoulder and is the loci for the proposed framework.
   The detection of individual objects has been revamped appreciably over
   the recent years but even now environmental factors and crowd-scene
   detection remains significantly difficult for detection of moving
   object. The proposed framework subtracts the background through Gaussian
   mixture model and the area of significance is extracted. The area of
   significance is transformed to white and black picture by picture
   binarization. Then, Wiener filter is employed to scale the background
   level for optimizing the results of the object in motion. The object is
   finally identified. The performance in every stage is measured and is
   evaluated. The result in each stage is compared and the performance of
   the proposed framework is that of the existing system proves
   satisfactory.}},
DOI = {{10.1007/978-3-319-12012-6\_47}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-12012-6; 978-3-319-12011-9}},
Unique-ID = {{ISI:000365181700047}},
}

@inproceedings{ ISI:000363756900031,
Author = {Ming, Yue and Jin, Yi},
Editor = {{Liu, H and Kubota, N and Zhu, X and Dillmann, R and Zhou, D}},
Title = {{Robust 3D Local SIFT Features for 3D Face Recognition}},
Booktitle = {{INTELLIGENT ROBOTICS AND APPLICATIONS (ICIRA 2015), PT III}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2015}},
Volume = {{9246}},
Pages = {{352-359}},
Note = {{8th International Conference on Intelligent Robotics and Applications
   (ICIRA), Portsmouth, ENGLAND, AUG 24-27, 2015}},
Abstract = {{In this paper, a robust 3D local SIFT feature is proposed for 3D face
   recognition. For preprocessing the original 3D face data, facial
   regional segmentation is first employed by fusing curvature
   characteristics and shape band mechanism. Then, we design a new local
   descriptor for the extracted regions, called 3D local Scale-Invariant
   Feature Transform (3D LSIFT). The key point detection based on 3D LSIFT
   can effectively reflect the geometric characteristic of 3D facial
   surface by encoding the gray and depth information captured by 3D face
   data. Then, 3D LSIFT descriptor extends to describe the discrimination
   on 3D faces. Experimental results based on the common international 3D
   face databases demonstrate the higher-qualified performance of our
   proposed algorithm with effectiveness, robustness, and universality.}},
DOI = {{10.1007/978-3-319-22873-0\_31}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-22873-0; 978-3-319-22872-3}},
Unique-ID = {{ISI:000363756900031}},
}

@inproceedings{ ISI:000362452500020,
Author = {Fu, Junsheng and Kamarainen, Joni-Kristian and Buch, Anders Glent and
   Kruger, Norbert},
Editor = {{Jawahar, CV and Shan, S}},
Title = {{Indoor Objects and Outdoor Urban Scenes Recognition by 3D Visual
   Primitives}},
Booktitle = {{COMPUTER VISION - ACCV 2014 WORKSHOPS, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9008}},
Pages = {{270-285}},
Note = {{12th Asian Conference on Computer Vision (ACCV), Singapore, SINGAPORE,
   NOV 01-05, 2014}},
Organization = {{Singapore Tourism Board; Omron; Nvidia; Garena; Samsung; Adobe; ViSenze;
   Lee Fdn; Morpx; Microsoft Res; NICTA}},
Abstract = {{Object detection, recognition and pose estimation in 3D images have
   gained momentum due to availability of 3D sensors (RGB-D) and increase
   of large scale 3D data, such as city maps. The most popular approach is
   to extract and match 3D shape descriptors that encode local scene
   structure, but omits visual appearance. Visual appearance can be
   problematic due to imaging distortions, but the assumption that local
   shape structures are sufficient to recognise objects and scenes is
   largely invalid in practise since objects may have similar shape, but
   different texture (e.g., grocery packages). In this work, we propose an
   alternative appearance-driven approach which first extracts 2D
   primitives justified by Marr's primal sketch, which are
   ``accumulated{''} over multiple views and the most stable ones are
   ``promoted{''} to 3D visual primitives. The 3D promoted primitives
   represent both structure and appearance. For recognition, we propose a
   fast and effective correspondence matching using random sampling. For
   quantitative evaluation we construct a semisynthetic benchmark dataset
   using a public 3D model dataset of 119 kitchen objects and another
   benchmark of challenging street-view images from 4 different cities. In
   the experiments, our method utilises only a stereo view for training. As
   the result, with the kitchen objects dataset our method achieved almost
   perfect recognition rate for +/- 10 degrees camera view point change and
   nearly 80\% for +/- 20 degrees, and for the street-view benchmarks it
   achieved 75\% accuracy for 160 street-view images pairs, 80\% for 96
   street-view images pairs, and 92\% for 48 street-view image pairs.}},
DOI = {{10.1007/978-3-319-16628-5\_20}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-16628-5; 978-3-319-16627-8}},
ResearcherID-Numbers = {{Kruger, Norbert/P-6315-2015
   Buch, Anders/P-4849-2015
   Kamarainen, Joni-Kristian/G-4296-2014}},
ORCID-Numbers = {{Kruger, Norbert/0000-0002-3931-116X
   Buch, Anders/0000-0002-5904-6981
   }},
Unique-ID = {{ISI:000362452500020}},
}

@inproceedings{ ISI:000361841100052,
Author = {Lin, Yizhou and Hua, Gang and Mordohai, Philippos},
Editor = {{Agapito, L and Bronstein, MM and Rother, C}},
Title = {{Egocentric Object Recognition Leveraging the 3D Shape of the Grasping
   Hand}},
Booktitle = {{COMPUTER VISION - ECCV 2014 WORKSHOPS, PT III}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{8927}},
Pages = {{746-762}},
Note = {{13th European Conference on Computer Vision (ECCV), Zurich, SWITZERLAND,
   SEP 06-12, 2014}},
Abstract = {{We present a systematic study on the relationship between the 3D shape
   of a hand that is about to grasp an object and recognition of the object
   to be grasped. In this paper, we investigate the direction from the
   shape of the hand to object recognition for unimpaired users. Our work
   shows that the 3D shape of a grasping hand from an egocentric point of
   view can help improve recognition of the objects being grasped. Previous
   work has attempted to exploit hand interactions or gaze information in
   the egocentric setting to guide object segmentation. However, all such
   analyses are conducted in 2D. We hypothesize that the 3D shape of a
   grasping hand is highly correlated to the physical attributes of the
   object being grasped. Hence, it can provide very beneficial visual
   information for object recognition. We validate this hypothesis by first
   building a 3D, egocentric vision pipeline to segment and reconstruct
   dense 3D point clouds of the grasping hands. Then, visual descriptors
   are extracted from the point cloud and subsequently fed into an object
   recognition system to recognize the object being grasped. Our
   experiments demonstrate that the 3D hand shape can indeed greatly help
   improve the visual recognition accuracy, when compared with the baseline
   where only 2D image features are utilized.}},
DOI = {{10.1007/978-3-319-16199-0\_52}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-16199-0; 978-3-319-16198-3}},
Unique-ID = {{ISI:000361841100052}},
}

@inproceedings{ ISI:000360175900188,
Author = {Naveen, S. and Moni, R. S.},
Editor = {{Samuel, P}},
Title = {{Multimodal Face Recognition System using Spectral Transformation of 2D
   Texture feature and Statistical processing of Face Range Images}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION AND
   COMMUNICATION TECHNOLOGIES, ICICT 2014}},
Series = {{Procedia Computer Science}},
Year = {{2015}},
Volume = {{46}},
Pages = {{1537-1545}},
Note = {{International Conference on Information and Communication Technologies
   (ICICT), Kochi, INDIA, DEC 03-05, 2014}},
Organization = {{Cochin Uni Sci \& Technol, Sch Engn; TEQIP Phase II}},
Abstract = {{3D Face recognition has been an area of interest for the past few
   decades in pattern recognition. This paper focuses on problems of person
   identification using 3D Face data. Here unregistered Face data, i.e.
   both texture and depth is fed to classifier in spectral representations
   of data. 2D Discrete Fourier Transform (DFT) is used for spectral
   representation. Fusion of scores improves the recognition accuracy
   significantly since use of depth information alone in spectral
   representation was not sufficient to increase accuracy. Statistical
   method seems to degrade performance of system when applied to texture
   data and was effective for depth data. (C) 2015 The Authors. Published
   by Elsevier B.V.}},
DOI = {{10.1016/j.procs.2015.02.078}},
ISSN = {{1877-0509}},
Unique-ID = {{ISI:000360175900188}},
}

@inproceedings{ ISI:000359292400017,
Author = {Karagoz, Burcu and Altan, Hakan and Kamburoglu, Kivanc},
Editor = {{Lilge, LD and Sroka, R}},
Title = {{Terahertz pulsed imaging study of dental caries}},
Booktitle = {{MEDICAL LASER APPLICATIONS AND LASER-TISSUE INTERACTIONS VII}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9542}},
Note = {{Conference on Medical Laser Applications and Laser-Tissue Interactions
   VII, Munich, GERMANY, JUN 21-23, 2015}},
Organization = {{SPIE; Opt Soc America}},
Abstract = {{Current diagnostic techniques in dentistry rely predominantly on X-rays
   to monitor dental caries. Terahertz Pulsed Imaging (TPI) has great
   potential for medical applications since it is a nondestructive imaging
   method. It does not cause any ionization hazard on biological samples
   due to low energy of THz radiation. Even though it is strongly absorbed
   by water which exhibits very unique chemical and physical properties
   that contribute to strong interaction with THz radiation, teeth can
   still be investigated in three dimensions. Recent investigations suggest
   that this method can be used in the early identification of dental
   diseases and imperfections in the tooth structure without the hazards of
   using techniques which rely on x-rays. We constructed a continuous wave
   (CW) and time-domain reflection mode raster scan THz imaging system that
   enables us to investigate various teeth samples in two or three
   dimensions. The samples comprised of either slices of individual tooth
   samples or rows of teeth embedded in wax, and the imaging was done by
   scanning the sample across the focus of the THz beam. 2D images were
   generated by acquiring the intensity of the THz radiation at each pixel,
   while 3D images were generated by collecting the amplitude of the
   reflected signal at each pixel. After analyzing the measurements in both
   the spatial and frequency domains, the results suggest that the THz
   pulse is sensitive to variations in the structure of the samples that
   suggest that this method can be useful in detecting the presence of
   caries.}},
DOI = {{10.1117/12.2183673}},
Article-Number = {{95420N}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-707-4}},
ResearcherID-Numbers = {{Altan, Hakan/A-4036-2017}},
ORCID-Numbers = {{Altan, Hakan/0000-0002-2456-8827}},
Unique-ID = {{ISI:000359292400017}},
}

@article{ ISI:000358942000004,
Author = {Dalponte, Michele and Reyes, Francesco and Kandare, Kaja and Gianelle,
   Damiano},
Title = {{Delineation of Individual Tree Crowns from ALS and Hyperspectral data: a
   comparison among four methods}},
Journal = {{EUROPEAN JOURNAL OF REMOTE SENSING}},
Year = {{2015}},
Volume = {{48}},
Pages = {{365-382}},
Abstract = {{In this paper four different delineation methods based on airborne laser
   scanning (ALS) and hyperspectral data are compared over a forest area in
   the Italian Alps. The comparison was carried out in terms of detected
   trees, while the ALS based methods are compared also in terms of
   attributes estimated (e.g. height). From the experimental results
   emerged that ALS methods outperformed hyperspectral one in terms of tree
   detection rate in two of three cases. The best results were achieved
   with a method based on region growing on an ALS image, and by one based
   on clustering of raw ALS point cloud. Regarding the estimates of the
   tree attributes all the ALS methods provided good results with very high
   accuracies when considering only big trees.}},
DOI = {{10.5721/EuJRS20154821}},
ISSN = {{2279-7254}},
ResearcherID-Numbers = {{Gianelle, Damiano/G-9437-2011
   Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Gianelle, Damiano/0000-0001-7697-5793
   Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000358942000004}},
}

@inproceedings{ ISI:000359129800018,
Author = {Lenz, Marcel and Krug, Robin and Jaedicke, Volker and Stroop, Ralf and
   Schmieder, Kirsten and Hofmann, Martin R.},
Editor = {{Bouma, BE and Wojtkowski, M}},
Title = {{Spectral Domain Optical Coherence Tomography for ex vivo brain tumor
   analysis}},
Booktitle = {{OPTICAL COHERENCE IMAGING TECHNIQUES AND IMAGING IN SCATTERING MEDIA}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9541}},
Note = {{Conference on Optical Coherence Imaging Techniques and Imaging in
   Scattering Media, Munich, GERMANY, JUN 21-23, 2015}},
Organization = {{SPIE; Opt Soc}},
Abstract = {{Non-contact imaging methods to distinguish between healthy tissue and
   brain tumor tissue during surgery would be highly desirable but are not
   yet available. Optical Coherence Tomography (OCT) is a non-invasive
   imaging technology with a resolution around 1-15 mu m and a penetration
   depth of 1-2 mm that may satisfy the demands. To analyze its potential,
   we measured ex vivo human brain tumor tissue samples from 10 patients
   with a Spectral Domain OCT system (Thorlabs Callisto: center wavelength
   of 930 nm) and compared the results with standard histology. In detail,
   three different measurements were made for each sample. First the sample
   was measured directly after surgery. Then it was embedded in paraffin
   (also H\&E staining) and examined for the second time. At last, the
   slices of each paraffin block cut by the pathology were measured. Each
   time a B-scan was created and for a better comparison with the histology
   a 3D image was generated, in order to get the corresponding en face
   images. In both, histopathological diagnosis and the analysis of the OCT
   images, different types of brain tumor showed difference in structure.
   This has been affirmed by two blinded investigators. Nevertheless the
   difference between two images of samples taken directly after surgery is
   less distinct. To enhance the contrast in the images further, we employ
   Spectroscopic OCT and pattern recognition algorithms and compare these
   results to the histopathological standard.}},
DOI = {{10.1117/12.2183614}},
Article-Number = {{95411D}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-706-7}},
Unique-ID = {{ISI:000359129800018}},
}

@article{ ISI:000355789400004,
Author = {Zou, Hongyan and Da, Feipeng and Wang, Zhaoyang},
Title = {{A novel 3D face feature based on geometry image vertical shape
   information}},
Journal = {{OPTIK}},
Year = {{2015}},
Volume = {{126}},
Number = {{9-10}},
Pages = {{898-902}},
Abstract = {{A novel and efficient face feature is proposed in this paper. 3D faces
   from the database are preprocessed and mapped to 2D geometry images.
   Then the geometry images are decomposed into wavelet responses by
   multi-scale Gabor transforms. According to analyses and experiments,
   responses that represent vertical shape information are figured out to
   be face feature for recognition. Moreover, the feature extracted by
   multi-scale Haar transforms also obtains satisfying experiment results,
   which prove that the feature is free from the extraction methods.
   Extensive experiments conducted on FRGC(Face Recognition Grand
   Challenge) v2.0 show a satisfactory performance compared with existing
   popular methods. It is also approved that the vertical shape information
   is promising for dealing with face expressions in 3D face recognition.
   (C) 2015 Elsevier GmbH. All rights reserved.}},
DOI = {{10.1016/j.ijleo.2015.02.083}},
ISSN = {{0030-4026}},
Unique-ID = {{ISI:000355789400004}},
}

@inproceedings{ ISI:000355583800011,
Author = {Varney, Nina M. and Asari, Vijayan K.},
Editor = {{Casasent, D and Alam, MS}},
Title = {{Volume component analysis for classification of LiDAR data}},
Booktitle = {{OPTICAL PATTERN RECOGNITION XXVI}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9477}},
Note = {{Conference on Optical Pattern Recognition XXVI, Baltimore, MA, APR
   22-23, 2015}},
Organization = {{SPIE}},
Abstract = {{One of the most difficult challenges of working with LiDAR data is the
   large amount of data points that are produced. Analysing these large
   data sets is an extremely time consuming process. For this reason,
   automatic perception of LiDAR scenes is a growing area of research.
   Currently, most LiDAR feature extraction relies on geometrical features
   specific to the point cloud of interest. These geometrical features are
   scene-specific, and often rely on the scale and orientation of the
   object for classification. This paper proposes a robust method for
   reduced dimensionality feature extraction of 3D objects using a volume
   component analysis (VCA) approach.1
   This VCA approach is based on principal component analysis (PCA). PCA is
   a method of reduced feature extraction that computes a covariance matrix
   from the original input vector. The eigenvectors corresponding to the
   largest eigenvalues of the covariance matrix are used to describe an
   image. Block-based PCA is an adapted method for feature extraction in
   facial images because PCA, when performed in local areas of the image,
   can extract more significant features than can be extracted when the
   entire image is considered. The image space is split into several of
   these blocks, and PCA is computed individually for each block.
   This VCA proposes that a LiDAR point cloud can be represented as a
   series of voxels whose values correspond to the point density within
   that relative location. From this voxelized space, block-based PCA is
   used to analyze sections of the space where the sections, when combined,
   will represent features of the entire 3-D object. These features are
   then used as the input to a support vector machine which is trained to
   identify four classes of objects, vegetation, vehicles, buildings and
   barriers with an overall accuracy of 93.8\%}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-593-3}},
Unique-ID = {{ISI:000355583800011}},
}

@inproceedings{ ISI:000353328200021,
Author = {Qu, Chengchao and Monari, Eduardo and Schuchert, Tobias and Beyerer,
   Juergen},
Editor = {{Lam, EY and Niel, KS}},
Title = {{Realistic Texture Extraction for 3D Face Models Robust to Self-Occlusion}},
Booktitle = {{IMAGE PROCESSING: MACHINE VISION APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9405}},
Note = {{Conference on Image Processing - Machine Vision Applications VIII, San
   Francisco, CA, FEB 10-11, 2015}},
Organization = {{Soc Imaging Sci \& Technol; SPIE}},
Abstract = {{In the context of face modeling, probably the most well-known approach
   to represent 3D faces is the 3D Morphable Model (3DMM). When 3DMM is
   fitted to a 2D image, the shape as well as the texture and illumination
   parameters are simultaneously estimated. However, if real facial texture
   is needed, texture extraction from the 2D image is necessary. This paper
   addresses the possible problems in texture extraction of a single image
   caused by self-occlusion. Unlike common approaches that leverage the
   symmetric property of the face by mirroring the visible facial part,
   which is sensitive to inhomogeneous illumination, this work first
   generates a virtual texture map for the skin area iteratively by
   averaging the color of neighbored vertices. Although this step creates
   unrealistic, overly smoothed texture, illumination stays constant
   between the real and virtual texture. In the second pass, the mirrored
   texture is gradually blended with the real or generated texture
   according to the visibility. This scheme ensures a gentle handling of
   illumination and yet yields realistic texture. Because the blending area
   only relates to non-informative area, main facial features still have
   unique appearance in different face halves. Evaluation results reveal
   realistic rendering in novel poses robust to challenging illumination
   conditions and small registration errors.}},
Article-Number = {{94050P}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-495-0}},
Unique-ID = {{ISI:000353328200021}},
}

@inproceedings{ ISI:000353122200062,
Author = {Xue, Junpeng and Su, Xianyu and Zhang, Qican},
Editor = {{Quan, C and Qian, K and Asundi, A and Chau, FS}},
Title = {{High-speed 3D face measurement based on color speckle projection}},
Booktitle = {{INTERNATIONAL CONFERENCE ON EXPERIMENTAL MECHANICS 2014}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9302}},
Note = {{International Conference on Experimental Mechanics, Singapore,
   SINGAPORE, NOV 15-17, 2014}},
Organization = {{Opt \& Photon Soc Singapore; Theoret \& Appl Mech Soc}},
Abstract = {{Nowadays, 3D face recognition has become a subject of considerable
   interest in the security field due to its unique advantages in domestic
   and international. However, acquiring color-textured 3D faces data in a
   fast and accurate manner is still highly challenging. In this paper, a
   new approach based on color speckle projection for 3D face data dynamic
   acquisition is proposed. Firstly, the projector-camera color crosstalk
   matrix that indicates how much each projector channel influences each
   camera channel is measured. Secondly, the reference-speckle-sets images
   are acquired with CCD, and then three gray sets are separated from the
   color sets using the crosstalk matrix and are saved Finally, the color
   speckle image which is modulated by face is captured, and it is split
   three gray channels. We measure the 3D face using multi-sets of speckle
   correlation methods with color speckle image in high-speed similar as
   one-shot, which greatly improves the measurement accuracy and stability.
   The suggested approach has been implemented and the results are
   supported by experiments.}},
DOI = {{10.1117/12.2076458}},
Article-Number = {{93022Y}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-388-5}},
Unique-ID = {{ISI:000353122200062}},
}

@inproceedings{ ISI:000352727000035,
Author = {Weinmann, M. and Schmidt, A. and Mallet, C. and Hinz, S. and
   Rottensteiner, F. and Jutzi, B.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{CONTEXTUAL CLASSIFICATION OF POINT CLOUD DATA BY EXPLOITING INDIVIDUAL
   3D NEIGBOURHOODS}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. II}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{2-3}},
Number = {{W4}},
Pages = {{271-278}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{The fully automated analysis of 3D point clouds is of great importance
   in photogrammetry, remote sensing and computer vision. For reliably
   extracting objects such as buildings, road inventory or vegetation, many
   approaches rely on the results of a point cloud classification, where
   each 3D point is assigned a respective semantic class label. Such an
   assignment, in turn, typically involves statistical methods for feature
   extraction and machine learning. Whereas the different components in the
   processing workflow have extensively, but separately been investigated
   in recent years, the respective connection by sharing the results of
   crucial tasks across all components has not yet been addressed. This
   connection not only encapsulates the interrelated issues of neighborhood
   selection and feature extraction, but also the issue of how to involve
   spatial context in the classification step. In this paper, we present a
   novel and generic approach for 3D scene analysis which relies on (i)
   individually optimized 3D neighborhoods for (ii) the extraction of
   distinctive geometric features and (iii) the contextual classification
   of point cloud data. For a labeled benchmark dataset, we demonstrate the
   beneficial impact of involving contextual information in the
   classification process and that using individual 3D neighborhoods of
   optimal size significantly increases the quality of the results for both
   pointwise and contextual classification.}},
DOI = {{10.5194/isprsannals-II-3-W4-271-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000352727000035}},
}

@article{ ISI:000347672900002,
Author = {da Neiva, Marcelo Baiao and Soares, Alvaro Cavalheiro and Lisboa,
   Cinthia de Oliveira and Vilella, Oswaldo de Vasconcellos and Motta,
   Alexandre Trindade},
Title = {{Evaluation of cephalometric landmark identification on CBCT multiplanar
   and 3D reconstructions}},
Journal = {{ANGLE ORTHODONTIST}},
Year = {{2015}},
Volume = {{85}},
Number = {{1}},
Pages = {{11-17}},
Month = {{JAN}},
Abstract = {{Objective: To evaluate the reliability of three-dimensional (3D)
   landmark identification in cone-beam computed tomography (CBCT) using
   two different visualization techniques.
   Materials and Methods: Twelve CBCT images were randomly selected. Three
   observers independently repeated three times the identification of 30
   landmarks using 3D reconstructions and 28 landmarks using multiplanar
   views. The values of the coordinates X, Y, and Z of each point were
   obtained and the intraclass correlation coefficient (ICC) was
   calculated.
   Results: The ICC of the 3D visualization was rated >0.90 in 67.76\% and
   45.56\%, and <= 0.45 in 13.33\% and 14.46\% of the intraobserver and
   interobserver assessments, respectively. The ICC of the multiplanar
   visualization was rated >0.90 in 82.16\% and 78.56\% and <= 0.45 in only
   16.7\% and 8.33\% of the intraobserver and interobserver assessments,
   respectively. An individual landmark classification was done according
   to ICC values.
   Conclusions: The frequency of highly reliable values was greater for
   multiplanar than 3D reconstructions. Overall, lower reliability was
   found for points on the condyle and higher reliability for those on the
   midsagittal plane. Depending on the anatomic region, the observer must
   choose the most reliable type of image visualization.}},
DOI = {{10.2319/120413-891.1}},
ISSN = {{0003-3219}},
EISSN = {{1945-7103}},
Unique-ID = {{ISI:000347672900002}},
}

@inproceedings{ ISI:000377348700061,
Author = {Liu, Jian and Zhang, Quan and Tang, Chaojing},
Book-Author = {{Xu, B}},
Title = {{CoMES: A Novel Method for Robust Nose Tip Detection in Face Range Images}},
Booktitle = {{2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION
   CONTROL CONFERENCE (IAEAC)}},
Year = {{2015}},
Pages = {{309-315}},
Note = {{IEEE Advanced Information Technology, Electronic and Automation Control
   Conference (IAEAC), Chongqing, PEOPLES R CHINA, DEC 19-20, 2015}},
Organization = {{IEEE; IEEE Beijing Sect; Global Union Acad Sci \& Technol; Chongqing
   Global Union Acad Sci \& Technol}},
Abstract = {{As the most distinct feature point in facial landmarks, nose tip plays a
   significant role in 3D facial studies. Successful detection of nose tip
   can facilitate many 3D facial studies tasks. In this paper, we propose a
   novel method to detect nose tip robustly. The method is robust to noise,
   need not training, can handle large rotations and occlusions. We first
   remove small isolated connected regions and noise from the input range
   image, then establish scale-space by robust smoothing the preprocessed
   range image. In each scale of the scale-space, we compute multi-angle
   energy of each point, then we use hierarchical clustering method to
   cluster the points whose multi-angle energies are larger than a
   threshold value. In the largest cluster, we can find one point with the
   largest multi-angle energy. For all scales of the scale-space, we get a
   series of such points and apply hierarchical clustering again for these
   points, nose tip will have the largest multi-angle energy in the largest
   cluster. We evaluate our method in FRGC v2.0 3D face database and
   BOSPHORUS 3D face database. The experimental results verify the
   robustness of our method with a high nose tip detection rate.}},
ISBN = {{978-1-4799-1980-2}},
Unique-ID = {{ISI:000377348700061}},
}

@inproceedings{ ISI:000387959204074,
Author = {Gilani, Syed Zulqarnain and Shafait, Faisal and Mian, Ajmal},
Book-Group-Author = {{IEEE}},
Title = {{Shape-based Automatic Detection of a Large Number of 3D Facial Landmarks}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2015}},
Pages = {{4639-4648}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{We present an algorithm for automatic detection of a large number of
   anthropometric landmarks on 3D faces. Our approach does not use texture
   and is completely shape based in order to detect landmarks that are
   morphologically significant. The proposed algorithm evolves level set
   curves with adaptive geometric speed functions to automatically extract
   effective seed points for dense correspondence. Correspondences are
   established by minimizing the bending energy between patches around seed
   points of given faces to those of a reference face. Given its
   hierarchical structure, our algorithm is capable of establishing
   thousands of correspondences between a large number of faces. Finally, a
   morphable model based on the dense corresponding points is fitted to an
   unseen query face for transfer of correspondences and hence automatic
   detection of landmarks. The proposed algorithm can detect any number of
   pre-defined landmarks including subtle landmarks that are even difficult
   to detect manually. Extensive experimental comparison on two benchmark
   databases containing 6, 507 scans shows that our algorithm outperforms
   six state of the art algorithms.}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-6964-0}},
ORCID-Numbers = {{Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000387959204074}},
}

@inproceedings{ ISI:000399132000096,
Author = {Fan, Hang and Zhou, Yangui and Liang, Haowen and Wang, Jiahui and Krebs,
   Peter and Lin, Daikun and Su, Jianbang and Li, Kunyang and Chen, Haiyu
   and Wang, Xiaolu and Zhou, Jianying},
Book-Group-Author = {{IEEE}},
Title = {{Glasses-free 3D display with glasses-assisted quality: key innovations
   for smart directional backlight autostereoscopy}},
Booktitle = {{2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)}},
Year = {{2015}},
Note = {{IEEE Visual Communications and Image Processing (VCIP) Conference,
   SINGAPORE, DEC 13-16, 2015}},
Organization = {{IEEE}},
Abstract = {{A glasses-free 3D display with glasses-assisted quality is presented.
   Self-adaptive algorithm is employed to optimize system parameters, which
   is applied to design the micro structure of lens array and free form
   surface backlight units. Moire contour map based on contrast sensitivity
   function is simulated and is manipulated by ameliorating the period
   ratio and the tilt angle of the superimposed periodical optical
   components. Directional transmissions of multi-users 3D images are
   realized with a finer dynamic synchronized backlight control and a face
   recognition technology. Comfortable viewings are demonstrated for two
   viewers, with full high definition for a single viewing channel. Minimum
   crosstalk as low as 2.3\% is demonstrated over a large viewing volume.}},
ISBN = {{978-1-4673-7314-2}},
Unique-ID = {{ISI:000399132000096}},
}

@article{ ISI:000215156100010,
Author = {Fernandez-Cervantes, Victor and Garcia, Arturo and Antonio Ramos, Marco
   and Mendez, Andres},
Title = {{Facial Geometry Identification through Fuzzy Patterns with RGBD Sensor}},
Journal = {{COMPUTACION Y SISTEMAS}},
Year = {{2015}},
Volume = {{19}},
Number = {{3}},
Pages = {{529-546}},
Abstract = {{Automatic human facial recognition is an important and complicated task;
   it is necessary to design algorithms capable of recognizing the constant
   patterns in the face and to use computing resources efficiently. In this
   paper we present a novel algorithm to recognize the human face in real
   time; the system's input is the depth and color data from the Microsoft
   KinectTM device. The algorithm recognizes patterns/shapes on the point
   cloud topography. The template of the face is based in facial geometry;
   the forensic theory classifies the human face with respect to constant
   patterns: cephalometric points, lines, and areas of the face. The
   topography, relative position, and symmetry are directly related to the
   craniometric points. The similarity between a point cloud cluster and a
   pattern description is measured by a fuzzy pattern theory algorithm. The
   face identification is composed by two phases: the first phase
   calculates the face pattern hypothesis of the facial points, configures
   each point shape, the related location in the areas, and lines of the
   face. Then, in the second phase, the algorithm performs a search on
   these face point configurations.}},
DOI = {{10.13053/CyS-19-3-2015}},
ISSN = {{1405-5546}},
EISSN = {{2007-9737}},
ORCID-Numbers = {{Ramos Corchado, Marco Antonio/0000-0003-3982-6988}},
Unique-ID = {{ISI:000215156100010}},
}

@article{ ISI:000214649800004,
Author = {Krotewicz, Pawel and Sankowski, Wojciech and Nowak, Piotr Stefan},
Title = {{Face recognition based on 2D and 3D data fusion}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMETRICS}},
Year = {{2015}},
Volume = {{7}},
Number = {{1}},
Pages = {{69-81}},
Abstract = {{The aim of the work presented in this paper is to present current state
   of the art of face recognition methods and describe proposal algorithms
   for face biometric identification that analyse 2D face images and 3D
   face geometry scans. Data for analysis gathered via 3D scanner are
   processed through different phases. These are: segmentation phase,
   feature extraction phase and comparison phase. Segmentation relies on
   localising characteristic landmark points of the face and projecting the
   face point cloud onto a plane constructed on the basis of these
   characteristic points. Feature extraction phase calculates separate
   feature vectors for 2D and 3D input data. Comparison phase applies
   fusion of 2D and 3D methods and calculates similarity value between two
   samples. All samples are compared against one another and results
   presented as DET curves are generated. By analysis of DET curves,
   conclusions are formulated.}},
DOI = {{10.1504/IJBM.2015.069505}},
ISSN = {{1755-8301}},
EISSN = {{1755-831X}},
Unique-ID = {{ISI:000214649800004}},
}

@inproceedings{ ISI:000348430000011,
Author = {Nguyen Hong Quy and Nguyen Hoang Quoc and Nguyen Tran Lan Anh and Yang,
   Hyung-Jeong and Pham The Bao},
Editor = {{Camacho, D and Kim, SW and Trawinski, B}},
Title = {{3D Human Face Recognition Using Sift Descriptors of Face's Feature
   Regions}},
Booktitle = {{NEW TRENDS IN COMPUTATIONAL COLLECTIVE INTELLIGENCE}},
Series = {{Studies in Computational Intelligence}},
Year = {{2015}},
Volume = {{572}},
Pages = {{117-126}},
Note = {{6th International Conference on Computational Collective Intelligence
   (ICCCI), Seoul, SOUTH KOREA, SEP 23-26, 2014}},
Abstract = {{Many researches in 3D face recognition problem have been studied because
   of adverse effects of human's age, emotions, and environmental
   conditions on 2D models. In this paper, we propose a novel method for
   recognizing 3D faces. First, a 3D human face is normalized and
   determined regions of interest (ROI). Second, SIFT algorithm is applied
   to these ROIs for detecting invariant feature points. Finally, this
   descriptor, extracted from a training image, will be stored and later
   used to identify the face in a test image. For performing reliable
   recognition, we also adjust parameters of SIFT algorithm to fit own
   characteristics of the template database. In our experiments, the
   proposed method produces promising performance up to 84.6\% of accuracy
   when using 3D Notre Dame biometric data-TEC.}},
DOI = {{10.1007/978-3-319-10774-5\_11}},
ISSN = {{1860-949X}},
ISBN = {{978-3-319-10774-5; 978-3-319-10773-8}},
Unique-ID = {{ISI:000348430000011}},
}

@inproceedings{ ISI:000380453200062,
Author = {Sindhuja, C. and Mala, K.},
Book-Group-Author = {{IEEE}},
Title = {{LANDMARK IDENTIFICATION IN 3D IMAGE FOR FACIAL EXPRESSION RECOGNITION}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTING AND
   COMMUNICATIONS TECHNOLOGIES (ICCCT 15)}},
Year = {{2015}},
Pages = {{338-343}},
Note = {{International Conference on Computing and Communications Technologies
   ((ICCCT), Chennai, INDIA, FEB 26-27, 2015}},
Organization = {{Dept Informat Technol Sri Sai Ram Engn Coll Chennai}},
Abstract = {{Facial expression recognition plays a major role in non verbal
   communication. Recognition by machine is still a challenging problem. To
   automate the recognition for human machine interaction, a system is
   proposed in this paper. The proposed system uses shape descriptors to
   identify twelve land marks which mainly contribute to the facial
   expression recognition. From the location and the size or boundary of
   the land marks by matching with Facial Landmark Model (FLM), basic
   expressions are identified. The experimental results show that the shape
   descriptors and post processing correctly identifies landmarks
   automatically. The architectural distortion of action units is used to
   identify the basic facial expressions and tested on Bosphorous data set.}},
ISBN = {{978-1-4799-7623-2}},
Unique-ID = {{ISI:000380453200062}},
}

@article{ ISI:000344204000007,
Author = {Jo, Jaeik and Choi, Heeseung and Kim, Ig-Jae and Kim, Jaihie},
Title = {{Single-view-based 3D facial reconstruction method robust against pose
   variations}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{48}},
Number = {{1}},
Pages = {{73-85}},
Month = {{JAN}},
Abstract = {{The 3D Morphable Model (3DMM) and the Structure from Motion (SfM)
   methods are widely used for 3D facial reconstruction from 2D single-view
   or multiple-view images. However, model-based methods suffer from
   disadvantages such as high computational costs and vulnerability to
   local minima and head pose variations. The SfM-based methods require
   multiple facial images in various poses. To overcome these
   disadvantages, we propose a single-view-based 3D facial reconstruction
   method that is person-specific and robust to pose variations. Our
   proposed method combines the simplified 3DMM and the SfM methods. First,
   2D initial frontal Facial Feature Points (FFPs) are estimated from a
   preliminary 3D facial image that is reconstructed by the simplified
   3DMM. Second, a bilateral symmetric facial image and its corresponding
   FFPs are obtained from the original side-view image and corresponding
   FFPs by using the mirroring technique. Finally, a more accurate the 3D
   facial shape is reconstructed by the SfM using the frontal, original,
   and bilateral symmetric FFPs. We evaluated the proposed method using
   facial images in 35 different poses. The reconstructed facial images and
   the ground-truth 3D facial shapes obtained from the scanner were
   compared. The proposed method proved more robust to pose variations than
   3DMM. The average 3D Root Mean Square Error (RMSE) between the
   reconstructed and ground-truth 3D faces was less than 2.6 mm when 2D
   FFPs were manually annotated, and less than 3.5 mm when automatically
   annotated. (C) 2014 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2014.07.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000344204000007}},
}
