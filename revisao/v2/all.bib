@inproceedings{Nozawa:2015:FRS:2787626.2792634,
 author = {Nozawa, Naoki and Kuwahara, Daiki and Morishima, Shigeo},
 title = {3D Face Reconstruction from a Single Non-frontal Face Image},
 booktitle = {ACM SIGGRAPH 2015 Posters},
 series = {SIGGRAPH '15},
 year = {2015},
 isbn = {978-1-4503-3632-1},
 location = {Los Angeles, California},
 pages = {57:1--57:1},
 articleno = {57},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/2787626.2792634},
 doi = {10.1145/2787626.2792634},
 acmid = {2792634},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Fischer:2015:AFI:2756601.2756619,
 author = {Fischer, Robert and Vielhauer, Claus},
 title = {Automated Firearm Identification: On Using a Novel Multiple-Slice-Shape (MSS) Approach for Comparison and Matching of Firing Pin Impression Topography},
 booktitle = {Proceedings of the 3rd ACM Workshop on Information Hiding and Multimedia Security},
 series = {IH\&\#38;MMSec '15},
 year = {2015},
 isbn = {978-1-4503-3587-4},
 location = {Portland, Oregon, USA},
 pages = {161--171},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/2756601.2756619},
 doi = {10.1145/2756601.2756619},
 acmid = {2756619},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {digital crime scene analysis, digitized forensics, firearm identification, firing pin shape matching, multiple slice shape, new forensic features, pattern classification, topography processing},
}

@article{Demisse:2018:DFE:3190503.3176649,
 author = {Demisse, Girum G. and Aouada, Djamila and Ottersten, Bj\"{o}rn},
 title = {Deformation-Based 3D Facial Expression Representation},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 issue_date = {April 2018},
 volume = {14},
 number = {1s},
 month = mar,
 year = {2018},
 issn = {1551-6857},
 pages = {17:1--17:22},
 articleno = {17},
 numpages = {22},
 url = {http://doi.acm.org/10.1145/3176649},
 doi = {10.1145/3176649},
 acmid = {3176649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face deformation, 3D facial expression representation, expression modelling},
}

@inproceedings{Desai:2018:CSP:3204949.3204958,
 author = {Desai, Kevin and Prabhakaran, Balakrishnan and Raghuraman, Suraj},
 title = {Combining Skeletal Poses for 3D Human Model Generation Using Multiple Kinects},
 booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
 series = {MMSys '18},
 year = {2018},
 isbn = {978-1-4503-5192-8},
 location = {Amsterdam, Netherlands},
 pages = {40--51},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3204949.3204958},
 doi = {10.1145/3204949.3204958},
 acmid = {3204958},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D model, combined skeleton, interactive 3D tele-immersion, point cloud combination},
}

@inproceedings{Butler:2016:CFE:2851581.2892535,
 author = {Butler, Crystal and Subramanian, Lakshmi and Michalowicz, Stephanie},
 title = {Crowdsourced Facial Expression Mapping Using a 3D Avatar},
 booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
 series = {CHI EA '16},
 year = {2016},
 isbn = {978-1-4503-4082-3},
 location = {San Jose, California, USA},
 pages = {2798--2804},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/2851581.2892535},
 doi = {10.1145/2851581.2892535},
 acmid = {2892535},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D facial modeling, avatars, crowdsourcing, expression recognition, facial expressions, facs},
}

@inproceedings{Abbad:2018:FRP:3177148.3180087,
 author = {Abbad, Abdelghafour and Abbad, Khalid and Tairi, Hamid},
 title = {3D Face Recognition in the Presence of Facial Expressions Based on Empirical Mode Decomposition},
 booktitle = {Proceedings of the 2Nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
 series = {MedPRAI '18},
 year = {2018},
 isbn = {978-1-4503-5290-1},
 location = {Rabat, Morocco},
 pages = {1--6},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3177148.3180087},
 doi = {10.1145/3177148.3180087},
 acmid = {3180087},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face recognition, EMD, expression, facial curves, geometric features, local features},
}

@article{Yuan:2018:GIH:3205271.3205279,
 author = {Yuan, Lin and Chen, Fanglin and Zeng, Ling-Li and Wang, Lubin and Hu, Dewen},
 title = {Gender Identification of Human Brain Image with A Novel 3D Descriptor},
 journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
 issue_date = {March 2018},
 volume = {15},
 number = {2},
 month = mar,
 year = {2018},
 issn = {1545-5963},
 pages = {551--561},
 numpages = {11},
 url = {https://doi.org/10.1109/TCBB.2015.2448081},
 doi = {10.1109/TCBB.2015.2448081},
 acmid = {3205279},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
}

@inproceedings{Guo:2018:SVF:3240876.3240913,
 author = {Guo, Xingyan and Jin, Yi and Li, Yidong and Xing, Junliang and Lang, Congyan},
 title = {Stabilizing Video Facial Landmark Detection and Tracking via Global and Local Filtering},
 booktitle = {Proceedings of the 10th International Conference on Internet Multimedia Computing and Service},
 series = {ICIMCS '18},
 year = {2018},
 isbn = {978-1-4503-6520-8},
 location = {Nanjing, China},
 pages = {20:1--20:7},
 articleno = {20},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3240876.3240913},
 doi = {10.1145/3240876.3240913},
 acmid = {3240913},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D face model, landmark detection, landmark smoothing},
}

@inproceedings{Amir:2016:DEV:3001773.3001804,
 author = {Amir, Mohd Hezri and Quek, Albert and Sulaiman, Nur Rasyid Bin and See, John},
 title = {DUKE: Enhancing Virtual Reality Based FPS Game with Full-body Interactions},
 booktitle = {Proceedings of the 13th International Conference on Advances in Computer Entertainment Technology},
 series = {ACE '16},
 year = {2016},
 isbn = {978-1-4503-4773-0},
 location = {Osaka, Japan},
 pages = {35:1--35:6},
 articleno = {35},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3001773.3001804},
 doi = {10.1145/3001773.3001804},
 acmid = {3001804},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {First-Person-Shooter, Gestures Recognition, Immersive Gameplay, Virtual Reality},
}

@inproceedings{Kopinski:2016:DLA:2994374.2994392,
 author = {Kopinski, Thomas and Sachara, Fabian and Handmann, Uwe},
 title = {A Deep Learning Approach to Mid-air Gesture Interaction for Mobile Devices from Time-of-Flight Data},
 booktitle = {Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
 series = {MOBIQUITOUS 2016},
 year = {2016},
 isbn = {978-1-4503-4750-1},
 location = {Hiroshima, Japan},
 pages = {1--9},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/2994374.2994392},
 doi = {10.1145/2994374.2994392},
 acmid = {2994392},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Deep Learning, Object Recognition, mid-air gestures},
}

@inproceedings{Korn:2017:DAE:3064663.3064755,
 author = {Korn, Oliver and Stamm, Lukas and Moeckl, Gerd},
 title = {Designing Authentic Emotions for Non-Human Characters: A Study Evaluating Virtual Affective Behavior},
 booktitle = {Proceedings of the 2017 Conference on Designing Interactive Systems},
 series = {DIS '17},
 year = {2017},
 isbn = {978-1-4503-4922-2},
 location = {Edinburgh, United Kingdom},
 pages = {477--487},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3064663.3064755},
 doi = {10.1145/3064663.3064755},
 acmid = {3064755},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {affective, animation, body cues, design, emotion, emotion recognition, facial expressions, games, perception},
}

@inproceedings{Balint-Benczedi:2015:KAR:2772879.2773515,
 author = {B\'{a}lint-Bencz{\'e}di, Ferenc and Wiedemeyer, Thiemo and Tenorth, Moritz and Be\ssler, Daniel and Beetz, Michael},
 title = {A Knowledge-Based Approach to Robotic Perception Using Unstructured Information Management},
 booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
 series = {AAMAS '15},
 year = {2015},
 isbn = {978-1-4503-3413-6},
 location = {Istanbul, Turkey},
 pages = {1941--1942},
 numpages = {2},
 url = {http://dl.acm.org/citation.cfm?id=2772879.2773515},
 acmid = {2773515},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 keywords = {knowledge based reasoning, personal robotics, robot perception},
}

@inproceedings{Asteriadis:2015:SHA:2769493.2769569,
 author = {Asteriadis, Stylianos and Daras, Petros},
 title = {Skeleton-based Human Action Recognition Using Basis Vectors},
 booktitle = {Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
 series = {PETRA '15},
 year = {2015},
 isbn = {978-1-4503-3452-5},
 location = {Corfu, Greece},
 pages = {49:1--49:4},
 articleno = {49},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2769493.2769569},
 doi = {10.1145/2769493.2769569},
 acmid = {2769569},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {action recognition, gesture recognition, kinect data analysis},
}

@inproceedings{Cohen:2018:BSB:3177148.3180081,
 author = {Cohen, Fernand S. and Li, Chenxi},
 title = {3D Building Synthesis Based on Images and Affine Invariant Salient Features},
 booktitle = {Proceedings of the 2Nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
 series = {MedPRAI '18},
 year = {2018},
 isbn = {978-1-4503-5290-1},
 location = {Rabat, Morocco},
 pages = {44--51},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/3177148.3180081},
 doi = {10.1145/3177148.3180081},
 acmid = {3180081},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D building reconstruction, GPS, invariants, localization, salient features},
}

@inproceedings{Liu:2018:MBE:3242969.3264989,
 author = {Liu, Chuanhe and Tang, Tianhao and Lv, Kui and Wang, Minghao},
 title = {Multi-Feature Based Emotion Recognition for Video Clips},
 booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
 series = {ICMI '18},
 year = {2018},
 isbn = {978-1-4503-5692-3},
 location = {Boulder, CO, USA},
 pages = {630--634},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3242969.3264989},
 doi = {10.1145/3242969.3264989},
 acmid = {3264989},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3d face landmark, deep learning, densenet, emotion recognition, emotiw 2018, inception net, lstm, soundnet},
}

@inproceedings{Nozawa:2016:FGR:2945078.2945102,
 author = {Nozawa, Tsukasa and Kato, Takuya and Savkin, Pavel A. and Nozawa, Naoki and Morishima, Shigeo},
 title = {3D Facial Geometry Reconstruction Using Patch Database},
 booktitle = {ACM SIGGRAPH 2016 Posters},
 series = {SIGGRAPH '16},
 year = {2016},
 isbn = {978-1-4503-4371-8},
 location = {Anaheim, California},
 pages = {24:1--24:2},
 articleno = {24},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2945078.2945102},
 doi = {10.1145/2945078.2945102},
 acmid = {2945102},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D reconstruction, shape from X, texture synthesis},
}

@inproceedings{Xie:2016:IFR:3028842.3028853,
 author = {Xie, Lanchi and Xu, Lei and Zhang, Ning and Guo, Jingjing and Yan, Yuwen and Li, Zhihui and Li, Zhigang and Xu, Xiaojing},
 title = {Improved Face Recognition Result Reranking Based on Shape Contexts},
 booktitle = {Proceedings of the 2016 International Conference on Intelligent Information Processing},
 series = {ICIIP '16},
 year = {2016},
 isbn = {978-1-4503-4799-0},
 location = {Wuhan, China},
 pages = {11:1--11:6},
 articleno = {11},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3028842.3028853},
 doi = {10.1145/3028842.3028853},
 acmid = {3028853},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {face recognition, reranking, shape contexts, shape matching, similarity calculation},
}

@article{Jin:2018:LFE:3295616.3200572,
 author = {Jin, Hai and Lian, Yuanfeng and Hua, Jing},
 title = {Learning Facial Expressions with 3D Mesh Convolutional Neural Network},
 journal = {ACM Trans. Intell. Syst. Technol.},
 issue_date = {January 2019},
 volume = {10},
 number = {1},
 month = nov,
 year = {2018},
 issn = {2157-6904},
 pages = {7:1--7:22},
 articleno = {7},
 numpages = {22},
 url = {http://doi.acm.org/10.1145/3200572},
 doi = {10.1145/3200572},
 acmid = {3200572},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D mesh convolutional neural networks, Facial expression analysis, visual analysis},
}

@article{Ramachandra:2017:PAD:3058791.3038924,
 author = {Ramachandra, Raghavendra and Busch, Christoph},
 title = {Presentation Attack Detection Methods for Face Recognition Systems: A Comprehensive Survey},
 journal = {ACM Comput. Surv.},
 issue_date = {April 2017},
 volume = {50},
 number = {1},
 month = mar,
 year = {2017},
 issn = {0360-0300},
 pages = {8:1--8:37},
 articleno = {8},
 numpages = {37},
 url = {http://doi.acm.org/10.1145/3038924},
 doi = {10.1145/3038924},
 acmid = {3038924},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Biometrics, antispoofing, attacks, countermeasure, face recognition, security},
}

@inproceedings{Baig:2018:MDL:3240508.3241394,
 author = {Baig, Mohammed Habibullah and Varghese, Jibin Rajan and Wang, Zhangyang},
 title = {MusicMapp: A Deep Learning Based Solution for Music Exploration and Visual Interaction},
 booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
 series = {MM '18},
 year = {2018},
 isbn = {978-1-4503-5665-7},
 location = {Seoul, Republic of Korea},
 pages = {1253--1255},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/3240508.3241394},
 doi = {10.1145/3240508.3241394},
 acmid = {3241394},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustering, deep learning, music classification, visualization},
}

@inproceedings{Fang:2018:RCB:3197768.3201576,
 author = {Fang, Qinyuan and Kyrarini, Maria and Ristic-Durrant, Danijela and Gr\"{a}ser, Axel},
 title = {RGB-D Camera Based 3D Human Mouth Detection and Tracking Towards Robotic Feeding Assistance},
 booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
 series = {PETRA '18},
 year = {2018},
 isbn = {978-1-4503-6390-7},
 location = {Corfu, Greece},
 pages = {391--396},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3197768.3201576},
 doi = {10.1145/3197768.3201576},
 acmid = {3201576},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D Point Cloud, Assistive Robotics, Mouth Detection, RGB-D Camera, Robot Control},
}

@inproceedings{Pollok:2018:NMD:3301506.3301542,
 author = {Pollok, Thomas},
 title = {A New Multi-Camera Dataset with Surveillance, Mobile and Stereo Cameras for Tracking, Situation Analysis and Crime Scene Investigation Applications},
 booktitle = {Proceedings of the 2018 the 2Nd International Conference on Video and Image Processing},
 series = {ICVIP 2018},
 year = {2018},
 isbn = {978-1-4503-6613-7},
 location = {Hong Kong, Hong Kong},
 pages = {171--175},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3301506.3301542},
 doi = {10.1145/3301506.3301542},
 acmid = {3301542},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Crime Scene Investigation, Dataset, Mobile Camera, Multi-Camera Calibration, Stereo, Surveillance Camera},
}

@inproceedings{Kheffache:2015:MNG:2820926.2820939,
 author = {Kheffache, Aghiles and Pantaleoni, Marco and Zhou, Bo and Durante, Paolo Berto},
 title = {Multiverse: A Next Generation Data Storage for Alembic},
 booktitle = {SIGGRAPH Asia 2015 Posters},
 series = {SA '15},
 year = {2015},
 isbn = {978-1-4503-3926-1},
 location = {Kobe, Japan},
 pages = {15:1--15:1},
 articleno = {15},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/2820926.2820939},
 doi = {10.1145/2820926.2820939},
 acmid = {2820939},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Zeng:2019:PDG:3324320.3324370,
 author = {Zeng, Yingjie and Nie, Lanshun},
 title = {Poster: Deep Gait Recognition via Millimeter Wave},
 booktitle = {Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks},
 series = {EWSN ?19},
 year = {2019},
 isbn = {978-0-9949886-3-8},
 location = {Beijing, China},
 pages = {254--255},
 numpages = {2},
 url = {http://dl.acm.org/citation.cfm?id=3324320.3324370},
 acmid = {3324370},
 publisher = {Junction Publishing},
 address = {USA},
}

@inproceedings{Kempfle:2018:RRE:3266157.3266208,
 author = {Kempfle, Jochen and Van Laerhoven, Kristof},
 title = {Respiration Rate Estimation with Depth Cameras: An Evaluation of Parameters},
 booktitle = {Proceedings of the 5th International Workshop on Sensor-based Activity Recognition and Interaction},
 series = {iWOAR '18},
 year = {2018},
 isbn = {978-1-4503-6487-4},
 location = {Berlin, Germany},
 pages = {4:1--4:10},
 articleno = {4},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3266157.3266208},
 doi = {10.1145/3266157.3266208},
 acmid = {3266208},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Kinect v2, ToF sensing, non-contact measurement, respiration measurement, respiratory rate},
}

@INPROCEEDINGS{7292772, 
author={C. {Sindhuja} and K. {Mala}}, 
booktitle={2015 International Conference on Computing and Communications Technologies (ICCCT)}, 
title={Landmark identification in 3D image for facial expression recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={338-343}, 
abstract={Facial expression recognition plays a major role in non verbal communication. Recognition by machine is still a challenging problem. To automate the recognition for human machine interaction, a system is proposed in this paper. The proposed system uses shape descriptors to identify twelve land marks which mainly contribute to the facial expression recognition. From the location and the size or boundary of the land marks by matching with Facial Landmark Model (FLM), basic expressions are identified. The experimental results show that the shape descriptors and post processing correctly identifies landmarks automatically. The architectural distortion of action units is used to identify the basic facial expressions and tested on Bosphorous data set.}, 
keywords={emotion recognition;face recognition;landmark identification;3D image;facial expression recognition;nonverbal communication;human machine interaction;shape descriptors;facial landmark model;FLM;post processing;architectural distortion;facial expression identification;Bosphorous data set;Shape;Indexes;Face recognition;Nose;Mouth;Three-dimensional displays;Feature extraction;Landmark detection;Facial Landmark Model;Shape index}, 
doi={10.1109/ICCCT2.2015.7292772}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7558813, 
author={S. {Liu} and X. {Chen} and D. {Fan} and X. {Chen} and F. {Meng} and Q. {Huang}}, 
booktitle={2016 IEEE International Conference on Mechatronics and Automation}, 
title={3D smiling facial expression recognition based on SVM}, 
year={2016}, 
volume={}, 
number={}, 
pages={1661-1666}, 
abstract={Using Kinect acquired RGB-D image to obtain a face feature parameters and three-dimensional coordinates of the characteristic parameters, and to select the characteristic parameter Facial by Candide-3 model, and feature extraction and normalization. Smile face expression data collection through Kinect, SVM collected to smiley face data classify and output the result of recognition, and the results compared with two-dimensional image of smiling face expression recognition results. Experimental results show that three-dimensional image of smiling face expression recognition accuracy than the two-dimensional image of smiling face. This research has important significance for the research and application of facial expression recognition technology.}, 
keywords={emotion recognition;face recognition;feature extraction;image classification;interactive devices;support vector machines;3D smiling facial expression recognition;SVM;Kinect;RGB-D image;face feature parameters;three-dimensional coordinates;characteristic parameters;Candide-3 model;feature extraction;normalization;smile face expression data collection;smiley face data classification;smiling face expression recognition two-dimensional image;smiling face expression recognition three-dimensional image;facial expression recognition technology;Feature extraction;Face;Face recognition;Support vector machines;Image recognition;Training;Data mining;Facial Expression Recognition;Feature Extraction;Support Vector Machine;Kinect}, 
doi={10.1109/ICMA.2016.7558813}, 
ISSN={2152-744X}, 
month={Aug},}
@INPROCEEDINGS{7087053, 
author={A. A. {Pawar} and N. N. {Patil}}, 
booktitle={2015 International Conference on Pervasive Computing (ICPC)}, 
title={Recognition of 3-D faces with missing parts and line scratch removal using new technique}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={This paper presents an implementation of face recognition, which is a very important task of human face identification. Line scratch detection in images is a highly challenging situation because of various characteristics of this defect. Few characteristics are considered with the different texture and geometry of images. We propose a useful algorithm for frame-by-frame line scratch detection in face image which deals with 3D approach and a filtering of detection. The temporary filtering algorithm can be used to remove false detection due to thin vertical structures by detecting the scratches on an image. Experimental evaluation can be detecting the lines and scratches on a face image and they used to solve this difficult approach. Our method is used with missing parts in an image. Three-dimensional face recognition is an extended method of facial recognition is considered according with the geometry and texture of a face. It has been elaborated that 3D face recognition methods can provide high accuracy as well as high detection with a comparison of 2D recognition. 3D avoids such mismatch effect of 2D face recognition algorithms. Additionally, most 3D scanners achieve both a 3D mesh and the texture of a face image. This allows combining the output of pure 3D matches with the more traditional algorithms of 2D face recognition, thus producing better performance.}, 
keywords={computational geometry;face recognition;filtering theory;image matching;image texture;mesh generation;object detection;3D face recognition;missing parts;line scratch removal;human face identification;frame-by-frame line scratch detection;image texture;image geometry;detection filtering;false detection removal;3D scanners;3D mesh;pure 3D matches;Face recognition;Three-dimensional displays;Transforms;Filtering;Image recognition;Noise;Films;3D Images;Adaptive detection;Face mask;Hough transforms;ICP algorithm;Line scratches;Missing parts;RANSAC;SIFT}, 
doi={10.1109/PERVASIVE.2015.7087053}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7443288, 
author={ and and S. {Tripathi}}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={Pose invariant method for emotion recognition from 3D images}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Information about the emotional state of a person can be inferred from facial expressions. Emotion recognition has become an active research area in recent years in various fields such as Human Robot Interaction (HRI), medicine, intelligent vehicle, etc., The challenges in emotion recognition from images with pose variations, motivates researchers to explore further. In this paper, we have proposed a method based on geometric features, considering images of 7 yaw angles (-45°,-30°,-15°,0°,+15°,+30°,+45°) from BU3DFE database. Most of the work that has been reported considered only positive yaw angles. In this work, we have included both positive and negative yaw angles. In the proposed method, feature extraction is carried out by concatenating distance and angle vectors between the feature points, and classification is performed using neural network. The results obtained for images with pose variations are encouraging and comparable with literature where work has been performed on pitch and yaw angles. Using our proposed method non-frontal views achieve similar accuracy when compared to frontal view thus making it pose invariant. The proposed method may be implemented for pitch and yaw angles in future.}, 
keywords={emotion recognition;feature extraction;image classification;neural nets;pose estimation;visual databases;pose invariant method;emotion recognition;3D image;person emotional state;facial expressions;pose variation;geometric features;BU3DFE database;positive yaw angles;negative yaw angles;feature extraction;concatenating distance;angle vectors;feature points;classification;neural network;nonfrontal views;Emotion recognition;Databases;Three-dimensional displays;Feature extraction;Eyebrows;Euclidean distance;Mouth;BU3DFE database;feature points;feature extraction;classification;neural network}, 
doi={10.1109/INDICON.2015.7443288}, 
ISSN={2325-9418}, 
month={Dec},}
@INPROCEEDINGS{7351408, 
author={T. {Batabyal} and A. {Vaccari} and S. T. {Acton}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={UGraSP: A unified framework for activity recognition and person identification using graph signal processing}, 
year={2015}, 
volume={}, 
number={}, 
pages={3270-3274}, 
abstract={With the growing availability and wide distribution of low-cost, high-performance 3D imaging sensors, the image analysis community has witnessed an increased demand for solutions to the challenges of activity recognition and person identification. We propose an integrated framework, based on graph signal processing, that simultaneously performs both tasks using a single set of features. The novelty of our approach is based on the fact that the set of features used for activity recognition accommodates person identification without additional computation. The analysis is based on the extracted structure-invariant graph (skeleton). The Laplacian of the skeleton is used both to identify the person and recognize the performed activity. While person identification is achieved directly from the analysis of the Laplacian, activity recognition is obtained after transformation, into the graph spectral domain, of the vectorized form of the skeletal joints 3D coordinates. Feature vectors for activity recognition are then derived, in this domain, from the covariance matrices evaluated over fixed-length sequential video segments. Both classification tasks are implemented using linear support vector machines (SVM). When applied to real activity datasets, our approach shows an improved performance over the existing state-of-the-art.}, 
keywords={covariance matrices;feature extraction;graph theory;image classification;object detection;object recognition;support vector machines;video signal processing;UGraSP;unified framework;image analysis community;activity recognition;person identification;integrated framework;graph signal processing;feature tasks;structure-invariant graph extraction;graph skeleton;skeleton Laplacian;graph spectral domain;vectorized form;skeletal joints 3D coordinates;feature vectors;covariance matrices;fixed-length sequential video segments;classification tasks;linear support vector machines;SVM;real activity datasets;performance improvement;Laplace equations;Skeleton;Three-dimensional displays;Motion segmentation;Image recognition;Sensors;Support vector machines;Laplacian;Adjacency Matrix;Graph Signal Processing;Graph Fourier Transform;activity Recognition;Person Identification;Point cloud datasets}, 
doi={10.1109/ICIP.2015.7351408}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8571963, 
author={S. {Yamada} and H. {Lu} and J. K. {Tan} and H. {Kim} and N. {Kimura} and T. {Okawachi} and E. {Nozoe} and N. {Nakamura}}, 
booktitle={2018 18th International Conference on Control, Automation and Systems (ICCAS)}, 
title={Extraction of Median Plane from Facial 3D Point Cloud Based on Symmetry Analysis Using ICP Algorithm}, 
year={2018}, 
volume={}, 
number={}, 
pages={1347-1350}, 
abstract={Cleft lip is a kind of congenital facial morphological abnormality. In the clinical field of cleft lip, it is necessary to analyze symmetric shape. However, there is no method to analyze the cleft lip technique based on symmetrical viewpoints. On the other hand, in our previous method to find a symmetric axis using a 2D image, since the middle line is extracted only from the front view of the face moire image. There was a problem that low accuracy was obtained by slight rotation of the face and it was not possible to consider 3D information. In this paper, we propose a method to extract the median plane of the face by analyzing based on bilateral symmetry by using 3D point cloud on the face of front. By extracting the median plane, we believe that not only surgical assistance of doctor be possible but also become a clue to development of simulation software which is the end goal.}, 
keywords={biomechanics;face recognition;feature extraction;image reconstruction;medical image processing;surgery;symmetric shape;clinical field;congenital facial morphological abnormality;ICP algorithm;symmetry analysis;facial 3D point cloud;bilateral symmetry;median plane;problem that low accuracy;face moire image;middle line;symmetric axis;symmetrical viewpoints;cleft lip technique;Three-dimensional displays;Lips;Face;Surgery;Iterative closest point algorithm;Two dimensional displays;Nose;Cleft lip;ICP algorithm;3D point cloud;Point Cloud Library;Facial median plane;Symmetry analysis.}, 
doi={}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7910452, 
author={S. G. {Gunanto} and M. {Hariadi} and E. M. {Yuniarno}}, 
booktitle={2016 2nd International Conference of Industrial, Mechanical, Electrical, and Chemical Engineering (ICIMECE)}, 
title={Computer facial animation with synthesize marker on 3D faces surface}, 
year={2016}, 
volume={}, 
number={}, 
pages={260-263}, 
abstract={An animated character has its own characteristics and behaviour. The animator needs to be skilled enough to make a complex animation, especially on making face expression. a Well defined facial expression can represent the emotional condition and make the animation more expressing the mood. But most facial animation is done by manually, frame-by-frame. It is very time-consuming. This research proposed a combination methods for handling the facial animation based on marker location and face surface from the 3D character. The motion data captured based on the location and movement of the marker then implemented on 3D face model to generate the motion guidance. As a guidance, this marker data role as a centroid of a vertex cluster. The cluster provided by implementing segmentation fp-NN Clustering method based on surface and can visualize the deformation using linear blend skinning methods. The result from this research shows that this system can automatically generate facial animations based on the marker data and the surface segmentation. The visualization of deformation arranges accordingly to the motion captured data and organized sequentially.}, 
keywords={computer animation;data visualisation;face recognition;image segmentation;neural nets;pattern clustering;computer facial animation;synthesize marker;3D face surface;facial expression;3D character;motion data capture;motion guidance;vertex cluster;segmentation fp-NN Clustering method;linear blend skinning methods;Facial animation;Three-dimensional displays;Solid modeling;Motion segmentation;Surface treatment;Interpolation;facial animation;feature marker;surface}, 
doi={10.1109/ICIMECE.2016.7910452}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7153124, 
author={G. {Pang} and R. {Qiu} and J. {Huang} and S. {You} and U. {Neumann}}, 
booktitle={2015 14th IAPR International Conference on Machine Vision Applications (MVA)}, 
title={Automatic 3D industrial point cloud modeling and recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={22-25}, 
abstract={3D modeling of point clouds is an important but time-consuming process, inspiring extensive research in automatic methods. Prior efforts focus on primitive geometry, street structures or indoor objects, but industrial data has rarely been pursued. Our work presents a method for automatic modeling and recognition of 3D industrial site point clouds, dividing the task into 3 separate sub-problems: pipe modeling, plane classification, and object recognition. The results are integrated to obtain the complete model, revealing some issues during the integration, solved by utilizing information gained from each individual process. Experiments show that the presented method automatically models large and complex industrial scenes with a quality that outperforms leading commercial modeling software and is comparable to professional hand-made models.}, 
keywords={image classification;mechanical engineering computing;object recognition;pipes;automatic 3D industrial point cloud modeling;automatic 3D industrial point cloud recognition;primitive geometry;street structures;indoor objects;plane classification;pipe modeling;object recognition;Three-dimensional displays;Solid modeling;Data models;Object recognition;Libraries;Software;Surface treatment}, 
doi={10.1109/MVA.2015.7153124}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7378673, 
author={ and and }, 
booktitle={2015 23rd International Conference on Geoinformatics}, 
title={A method of extracting human facial feature points based on 3D laser scanning point cloud data}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-3}, 
abstract={Three-dimensional body measurement is determined by measuring the size of each part of the body and body shape, in order to study the morphological characteristics of the human body, and provide basic information for human industrial design, ergonomics, engineering design, anthropological research, and medicine. This paper presents a three-dimensional point cloud data extraction method of facial feature points by doing nose points with examples, using this method can accurately locate the feature points, and there is no specific stations toward requirements in the process of scanning point cloud of the human body, it is convenient and quick. The results show that this method can meet the ergonomics and other features to quickly locate and measure the body size requirements.}, 
keywords={computer graphics;face recognition;feature extraction;optical scanners;human facial feature point extraction;3D laser scanning point cloud data;three-dimensional body measurement;morphological characteristics;human body;human industrial design;ergonomics;engineering design;anthropological research;medicine;three-dimensional point cloud data extraction method;nose point;Atmospheric modeling;Ergonomics;Biomedical imaging;The face feature point;Nose point;Measurement;Three-dimensional point cloud;Three-dimensional laser scanning}, 
doi={10.1109/GEOINFORMATICS.2015.7378673}, 
ISSN={2161-024X}, 
month={June},}
@INPROCEEDINGS{8054955, 
author={M. {Jazouli} and A. {Majda} and A. {Zarghili}}, 
booktitle={2017 Intelligent Systems and Computer Vision (ISCV)}, 
title={A $P recognizer for automatic facial emotion recognition using Kinect sensor}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Autism is a developmental disorder involving qualitative impairments in social interaction. One source of those impairments are difficulties with facial expressions of emotion. Autistic people often have difficulty to recognize or to understand other people's emotions and feelings, or expressing their own. This work proposes a method to automatically recognize seven basic emotions among autistic children in real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and Neutral. The method uses the Microsoft Kinect sensor to track and identify points of interest from the 3D face model and it is based on the $P point-cloud recognizer to identify multi-stroke emotions as point-clouds. The experimental results show that our system can achieve above 94.28% recognition rate. Our study provides a novel clinical tool to help children with autism to assisting doctors in operating rooms.}, 
keywords={emotion recognition;face recognition;feature extraction;multistroke emotions;point-clouds;recognition rate;autism;$P recognizer;automatic facial emotion recognition;developmental disorder;qualitative impairments;social interaction;autistic people;autistic children;Microsoft Kinect sensor;3D face model;$P point-cloud recognizer;Face recognition;Emotion recognition;Face;Three-dimensional displays;Autism;Support vector machines;Algorithm design and analysis;ASD;Autism;emotion;face expression;Kinect;$P Recognizer}, 
doi={10.1109/ISACV.2017.8054955}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8687254, 
author={T. {Terada} and Y. {Chen} and R. {Kimura}}, 
booktitle={2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
title={3D Facial Landmark Detection Using Deep Convolutional Neural Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={390-393}, 
abstract={Various applications have been developed in facial research to aid the recognition of personal attributes, analysis of race, and personal authentication for the security industry and other research fields. Thus, it is possible to identify the differences in facial shape based on place of birth or country. The present study analyzes facial shape using scanned 3D facial images and investigates ways to extract facial landmarks from the 3D facial images. The detection of the facial landmark requires the normalization of the facial scale and position among in the 3D image data to analyze the facial shape. Therefore, it is difficult to obtain accurate facial landmarks from 3D facial images. Our method decomposes the task into the following three parts: (a) conversion of data from the 3D facial image to a 2D image, (b) extraction of facial landmarks from the 3D image using Convolutional Neural Network (CNN), (c) inversion of the identified facial landmarks from 2D to 3D images. In experiments, we compared the accuracy of this model for facial landmark detection.}, 
keywords={convolutional neural nets;face recognition;facial landmark detection;3D facial image;personal authentication;convolutional neural network;security industry;CNN;component;landmarks detection;3d facial image;point cloud;facial analysis;cnn}, 
doi={10.1109/FSKD.2018.8687254}, 
ISSN={}, 
month={July},}
@ARTICLE{7130613, 
author={L. {Yuan} and F. {Chen} and L. {Zeng} and L. {Wang} and D. {Hu}}, 
journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
title={Gender Identification of Human Brain Image with A Novel 3D Descriptor}, 
year={2018}, 
volume={15}, 
number={2}, 
pages={551-561}, 
abstract={Determining gender by examining the human brain is not a simple task because the spatial structure of the human brain is complex, and no obvious differences can be seen by the naked eyes. In this paper, we propose a novel three-dimensional feature descriptor, the three-dimensional weighted histogram of gradient orientation (3D WHGO) to describe this complex spatial structure. The descriptor combines local information for signal intensity and global three-dimensional spatial information for the whole brain. We also improve a framework to address the classification of three-dimensional images based on MRI. This framework, three-dimensional spatial pyramid, uses additional information regarding the spatial relationship between features. The proposed method can be used to distinguish gender at the individual level. We examine our method by using the gender identification of individual magnetic resonance imaging (MRI) scans of a large sample of healthy adults across four research sites, resulting in up to individual-level accuracies under the optimized parameters for distinguishing between females and males. Compared with previous methods, the proposed method obtains higher accuracy, which suggests that this technology has higher discriminative power. With its improved performance in gender identification, the proposed method may have the potential to inform clinical practice and aid in research on neurological and psychiatric disorders.}, 
keywords={biomedical MRI;brain;image classification;medical image processing;neurophysiology;local information;signal intensity;three-dimensional spatial information;three-dimensional images;MRI;three-dimensional spatial pyramid;spatial relationship;individual level;gender identification;individual magnetic resonance imaging scans;individual-level accuracies;human brain image;naked eyes;three-dimensional feature descriptor;three-dimensional weighted histogram;gradient orientation;3D WHGO;complex spatial structure;Three-dimensional displays;Histograms;Kernel;Spatial resolution;Magnetic resonance imaging;Computational biology;Three-dimensional descriptor;gender identification;neuroimaging data analysis;pattern classification}, 
doi={10.1109/TCBB.2015.2448081}, 
ISSN={1545-5963}, 
month={March},}
@INPROCEEDINGS{8518089, 
author={S. {Li} and L. {Su} and Y. {Liu} and Z. {He}}, 
booktitle={IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium}, 
title={Segmentation of Individual Trees Based on a Point Cloud Clustering Method Using Airborne Lidar Data}, 
year={2018}, 
volume={}, 
number={}, 
pages={7520-7523}, 
abstract={The objective of this paper was to develop a new algorithm to segment individual trees directly by using the three-dimensional space characteristic of airborne light detection and ranging point cloud data. The local maximum method was used in the initial segmentation and the error identification tree exclusion. On the basis of the point cloud spatial distribution of individual trees and the adjacent relationship with the other trees, a point cloud clustering method was developed to decide the points belonging to the individual trees. This algorithm was tested by 6 forest plots in the Genhe forestry reserve. The results showed that this algorithm could segment individual trees quickly and accurately, and the overall accuracy of this algorithm was 96.3%.}, 
keywords={forestry;geophysical image processing;image segmentation;optical radar;pattern clustering;remote sensing by laser beam;vegetation mapping;segment individual trees;point cloud clustering method;airborne lidar data;three-dimensional space characteristic;airborne light detection;ranging point cloud data;local maximum method;initial segmentation;error identification tree exclusion;point cloud spatial distribution;Genhe forestry reserve;Vegetation;Three-dimensional displays;Forestry;Laser radar;Clustering algorithms;Remote sensing;Lasers;LiDAR;segmentation;tree;clustering}, 
doi={10.1109/IGARSS.2018.8518089}, 
ISSN={2153-7003}, 
month={July},}
@INPROCEEDINGS{7338568, 
author={F. {Ergüner} and P. O. {Durdu}}, 
booktitle={2015 9th International Conference on Application of Information and Communication Technologies (AICT)}, 
title={Multimodal natural interaction for 3D images}, 
year={2015}, 
volume={}, 
number={}, 
pages={305-309}, 
abstract={Due to the improvements in computer technology, interaction between computer and human has been evolved from command-line based interfaces to natural user interfaces that enables interaction in a more human-human way such as by speech, hand and body gestures, facial expressions and eye gaze. In this study controlling three dimensional images with gestures and speech using a three dimensional depth camera is realized in order to ensure human computer interaction in a more natural way. For this purpose realized system allows starting and closing the application and interaction with the three dimensional images using only speech and gestures but not using keyboard and mouse. System allows three different speech commands to start, close the application and reset the three dimensional image. Furthermore gesture-based commands are used to rotate, pan and zoom the three dimensional image.}, 
keywords={gesture recognition;human computer interaction;keyboards;mouse controllers (computers);user interfaces;multimodal natural interaction;human computer interaction;command-line based interfaces;natural user interfaces;facial expressions;body gestures;hand gestures;eye gaze;three dimensional images;keyboard;mouse;gesture-based commands;Three-dimensional displays;Speech;Biomedical imaging;Software;User interfaces;Cameras;Computers;Human Computer Interaction;Multimodal Natural Interaction;Kinect;Natural User Interface}, 
doi={10.1109/ICAICT.2015.7338568}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7163090, 
author={X. {Yang} and D. {Huang} and Y. {Wang} and L. {Chen}}, 
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
title={Automatic 3D facial expression recognition using geometric scattering representation}, 
year={2015}, 
volume={1}, 
number={}, 
pages={1-6}, 
abstract={Facial Expression Recognition (FER) is one of the most active topics in the domain of computer vision and pattern recognition, and it has received increasing attention for its wide application potentials as well as attractive scientific challenges. In this paper, we present a novel method to automatic 3D FER based on geometric scattering representation. A set of maps of shape features in terms of multiple order differential quantities, i.e. the Normal Maps (NOM) and the Shape Index Maps (SIM), are first jointly adopted to comprehensively describe geometry attributes of the facial surface. The scattering operator is then introduced to further highlight expression related cues on these maps, thereby constructing geometric scattering representations of 3D faces for classification. The scattering descriptor not only encodes distinct local shape changes of various expressions as by several milestone descriptors, such as SIFT, HOG, etc., but also captures subtle information hidden in high frequencies, which is quite crucial to better distinguish expressions that are easily confused. We evaluate the proposed approach on the BU-3DFE database, and the performance is up to 84.8% and 82.7% with two commonly used protocols respectively which is superior to the state of the art ones.}, 
keywords={emotion recognition;face recognition;image classification;image representation;shape recognition;automatic 3D facial expression recognition;automatic 3D FER;BU-3DFE database;local shape changes;3D face classification;scattering operator;facial surface geometry attributes;SIM;shape index maps;NOM;normal maps;multiple order differential quantities;shape feature maps;geometric scattering representation;Three-dimensional displays;Shape;Scattering;Indexes;Support vector machines;Solid modeling;Feature extraction}, 
doi={10.1109/FG.2015.7163090}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8358484, 
author={K. M. {Swetha} and P. {Suja}}, 
booktitle={2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon)}, 
title={A geometric approach for recognizing emotions from 3D images with pose variations}, 
year={2017}, 
volume={}, 
number={}, 
pages={805-809}, 
abstract={Emotions are an incredibly important aspect of human life. Research on emotion recognition for the past few decades have resulted in development of several fields. In the current scenario, it is necessary that machines/robots need to identify human emotions and respond accordingly. Applications in this field can be seen in security, entertainment and Human Machine Interface/Human Robot Interface. Recent works on 3D images have gained importance due to its accuracy in real life applications as emotions can be recognised at different head poses. The intention of this work has been to develop an algorithm for recognition of emotion from facial expressions, which recognizes 6 basic emotions, which are anger, fear, happy, disgust, sad and surprise from 3D images in 7 yaw angles (+45° to -45°) and 3 pitch angles (+15°,0°, -15°). Most of the reported work considers + yaw angles. While in the current work, both positive as well as negative pitch and yaw angles are considered. BU3DFE database is used for the implementation. The proposed method resulted in improved accuracy and is comparable with the literature.}, 
keywords={emotion recognition;face recognition;geometry;pose estimation;BU3DFE database;geometric approach;pose variations;emotion recognition;human emotions;head poses;yaw angles;pitch angles;3D image;facial expressions;Basic emotions;feature points;BU3DFE database;classification}, 
doi={10.1109/SmartTechCon.2017.8358484}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8525654, 
author={M. P. {Zapf} and A. {Gupta} and L. Y. {Morales Saiki} and M. {Kawanabe}}, 
booktitle={2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}, 
title={Data-Driven, 3-D Classification of Person-Object Relationships and Semantic Context Clustering for Robotics and AI Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={180-187}, 
abstract={We introduce a framework for detection and classification of spatio-temporal person-object interactions. Our method clusters similar semantic contexts from interactions detected from RGB-D data. 2-D object detection (YOLO) is run on RGB data from a Kinect v2 sensor on a mobile robot navigating an office and observing persons and desk spaces. Person and object detections are converted into 3-D point cloud time series via RGB-Depth co-registration and successive Euclidean and k-means spatial clustering. 3-D person and object point cloud streams are used to create time-series occupancy maps and person-object co-localization maps. From these maps, spatiotemporal correlations between persons and distinct objects are computed. Correlation patterns are clustered using k-means to obtain distinct human-object interactions, i.e. segment semantic context over time. We evaluated the performance of our approach to detect person-object correlations and cluster semantic context by recording 90 30-second RGB-D data episodes, with three persons handling representative objects (books, cups, bottles). Experimental results show that our framework is able to consistently assign semantic context to the same cluster in > 79% of cases (scene frames). Semantic contexts in visual scenes can be distinguished without the need to provide prior information, allowing mobile agents to learn and explore in new environments.}, 
keywords={feature extraction;image classification;image colour analysis;image motion analysis;image registration;image segmentation;mobile robots;object detection;path planning;pattern clustering;robot vision;time series;data-driven;3D classification;k-means spatial clustering;spatio-temporal person-object interaction detection;spatio-temporal person-object interaction classification;2D object detection;3D point cloud time series;RGB-depth coregistration;Euclidean clustering;person-object colocalization maps;correlation pattern clustering;semantic context segmentation;mobile agents;RGB-D data episodes;person-object correlations;human-object interactions;spatiotemporal correlations;time-series occupancy maps;object point cloud streams;mobile robot;Kinect v2 sensor;RGB data;semantic context clustering;person-object relationships;Three-dimensional displays;Robot sensing systems;Semantics;Object detection;Feature extraction;Correlation}, 
doi={10.1109/ROMAN.2018.8525654}, 
ISSN={1944-9437}, 
month={Aug},}
@INPROCEEDINGS{7081271, 
author={M. C. {EL Rai} and N. {Werghi} and H. {Al Muhairi} and H. {Alsafar}}, 
booktitle={2015 International Conference on Communications, Signal Processing, and their Applications (ICCSPA'15)}, 
title={Using facial images for the diagnosis of genetic syndromes: A survey}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The analysis of facial appearance is significant to an early diagnosis of medical genetic diseases. The fast development of image processing and machine learning techniques facilitates the detection of facial dysmorphic features. This paper is a survey of the recent studies developed for the screening of genetic abnormalities across the facial features obtained from two dimensional and three dimensional images.}, 
keywords={diseases;face recognition;learning (artificial intelligence);medical image processing;patient diagnosis;facial images;genetic syndromes diagnosis;facial appearance analysis;medical genetic diseases;image processing;machine learning techniques;facial dysmorphic features;genetic abnormalities;facial features;two dimensional images;three dimensional images;Three-dimensional displays;Face;Genetics;Feature extraction;Principal component analysis;Accuracy;Databases;Facial images;2D imaging;3D imaging;Landmarks;Dysmorphology;Classification}, 
doi={10.1109/ICCSPA.2015.7081271}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8075548, 
author={T. {Frikha} and F. {Chaabane} and B. {Said} and H. {Drira} and M. {Abid} and C. {Ben Amar} and L. {Lille}}, 
booktitle={2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
title={Embedded approach for a Riemannian-based framework of analyzing 3D faces}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform.}, 
keywords={biometrics (access control);embedded systems;face recognition;feature extraction;field programmable gate arrays;hardware-software codesign;software architecture;stereo image processing;tensors;biometric facial recognition system;embedded systems;data completion;full face recognition;hardware architecture;adaptation approach;face detection;Riemannian-based framework;3D face analysis;multimedia embedded applications;face expressions recognition;mixed hardware software architecture;codesign technique;FPGA platform;Computer architecture;Shape;Face recognition;Multimedia communication;Three-dimensional displays;Embedded systems;Measurement;Facial analysis;face detection;Facial expressions;3D face recognition;embedded architecture;elastic analysis algorithm;Riemann geometry;Curve analysis}, 
doi={10.1109/ATSIP.2017.8075548}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8578635, 
author={S. {Cheng} and I. {Kotsia} and M. {Pantic} and S. {Zafeiriou}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={5117-5126}, 
abstract={The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.}, 
keywords={biometrics (access control);computer vision;emotion recognition;face recognition;image capture;image resolution;stereo image processing;visual databases;high-resolution 3D faces;4DFAB;facial behaviour;facial expression recognition;behavioural biometrics;computer vision applications;automatic face analysis;scale visual databases;face recognition;Databases;Three-dimensional displays;Face;Face recognition;Cameras;Two dimensional displays;Task analysis}, 
doi={10.1109/CVPR.2018.00537}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{7853992, 
author={R. {Amin} and A. F. {Shams} and S. M. M. {Rahman} and D. {Hatzinakos}}, 
booktitle={2016 9th International Conference on Electrical and Computer Engineering (ICECE)}, 
title={Evaluation of discrimination power of facial parts from 3D point cloud data}, 
year={2016}, 
volume={}, 
number={}, 
pages={602-605}, 
abstract={Feature selection from facial regions is a well-known approach to increase the performance of 2D image-based face recognition systems. In case of 3D modality, the approach of region-based feature selection for face recognition is relatively new. In this context, this paper presents an approach to evaluate the discrimination power of different regions of a 3D facial surface for its potential use in face recognition systems. We propose the use of weighted average of unit normal vector on the facial surface as the feature for region-based face recognition from 3D point cloud data (PCD). The iterative closest point algorithm is employed for the registration of segmented regions of facial point clouds. A metric based on angular distance between normals is introduced to indicate the similarity between two surfaces of same facial region. Finally, the intra class correlation based discrimination score is formulated to find out the key facial regions such as the eyes, nose, and mouth that are significant while recognizing a person with facial surface PCD.}, 
keywords={computational geometry;correlation methods;face recognition;feature selection;image registration;image segmentation;discrimination power evaluation;facial parts;2D image-based face recognition systems;3D modality;region-based feature selection;3D facial surface;3D point cloud data;3D PCD;iterative closest point algorithm;segmented region registration;angular distance;intra class correlation;discrimination score;Three-dimensional displays;Face;Face recognition;Measurement;Two dimensional displays;Nose;Databases}, 
doi={10.1109/ICECE.2016.7853992}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7852823, 
author={ and }, 
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={Joint subspace learning for reconstruction of 3D facial dynamic expression from single image}, 
year={2016}, 
volume={}, 
number={}, 
pages={820-824}, 
abstract={Recently, the synthesis of 3D dynamic expressions has become an important concern in computer graphics, facial recognition, etc. In this study, we propose a regression based joint subspace learning method for the automatic synthesis of 3D dynamic expression images. This method synthesizes 3D dynamic expression images from a single 2D facial image. We use two subspaces (the view subspace and the frame subspace) to synthesize a 3D image. First, we use the view subspace to estimate multi-view facial images from a front image. Next, we construct a 3D image using the estimated multi-view facial images. Finally, we estimate the 3D images in different frames by using the frame subspace to synthesis 3D dynamic expression images. This approach is unlike the conventional joint subspace learning in which, the coefficients estimated by the input image are directly used for synthesis. Furthermore, we propose using textural information to improve the accuracy of synthesized images.}, 
keywords={computer graphics;estimation theory;face recognition;image reconstruction;learning (artificial intelligence);regression analysis;stereo image processing;3D facial dynamic expression reconstruction;single image;computer graphics;facial recognition;regression based joint subspace learning;multiview facial image estimation;Three-dimensional displays;Shape;Image reconstruction;Two dimensional displays;Principal component analysis;Training;Joints;3D dynamic expressions;multi-view facial shape;joint subspace learning;regression}, 
doi={10.1109/CISP-BMEI.2016.7852823}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7428562, 
author={J. {Liu} and Q. {Zhang} and C. {Tang}}, 
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
title={Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information}, 
year={2015}, 
volume={}, 
number={}, 
pages={276-284}, 
abstract={3D facial landmarks play a significant role in many 3D facial studies. Due to the complex geometry of high resolution 3D human face, landmark detection remains to be a challenge problem. This paper proposes a novel method to detect landmarks automatically for high resolution 3D faces without learning or training, solely using geometric information. For high resolution 3D faces, geodesic remeshing is used to reduce the vertices number, then the remeshed 3D faces are parameterized and interpolated on a regular grid, which enable us to compute the differential geometric features more efficiently and accurately. To detect landmarks, we extract global and local constraints for each landmark by considering relative positions of landmarks and differential geometric features, then landmarks are detected by combine these constraints. We evaluate our method and compare it with other methods which are mostly related to ours in a large scale publicly available 3D face data set, BJUT-3D face database. The experimental results show that our method is robust and effective, and achieves better performance than existing methods.}, 
keywords={face recognition;feature extraction;image resolution;interpolation;mesh generation;object detection;automatic landmark detection;high resolution nonrigid 3D faces;geometric information;3D facial landmarks;geodesic remeshing;vertices number;parameterization;interpolation;regular grid;differential geometric features;global constraints extraction;local constraints extraction;Three-dimensional displays;Indexes;Shape;Training;Feature extraction;Facial animation;Mesh generation;3D faces;landmarks;geometric information;geodesic remeshing;differential geometric features}, 
doi={10.1109/IAEAC.2015.7428562}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8272703, 
author={H. {Li} and J. {Sun} and L. {Chen}}, 
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)}, 
title={Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition}, 
year={2017}, 
volume={}, 
number={}, 
pages={234-242}, 
abstract={This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01%, 97.60%, and 96.13% on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition.}, 
keywords={face recognition;feature extraction;geometry;image classification;image representation;pose estimation;location-sensitive sparse representation;expression-robust 3D face recognition approach;given raw 3D facial surfaces;3D face pre-processing pipeline;face region cropping;normalized 3D facial surface;facial surface normal components;deep face net;facial surface normals;different facial locations;location sensitive sparse representation classifier;different 3D faces;different normal components;BU-3DFE databases;massive 2D face images;Three-dimensional displays;Face;Face recognition;Solid modeling;Two dimensional displays;Shape;Deformable models}, 
doi={10.1109/BTAS.2017.8272703}, 
ISSN={2474-9699}, 
month={Oct},}
@INPROCEEDINGS{8579077, 
author={J. {Li} and B. M. {Chen} and G. H. {Lee}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={SO-Net: Self-Organizing Network for Point Cloud Analysis}, 
year={2018}, 
volume={}, 
number={}, 
pages={9397-9406}, 
abstract={This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website1.}, 
keywords={computer vision;feature extraction;learning (artificial intelligence);nearest neighbour methods;self-organising feature maps;deep learning;orderless point clouds;SO-Net models;spatial distribution;individual points;SOM nodes;input point cloud;single feature vector;point cloud reconstruction;part segmentation;shape retrieval;point cloud recognition networks;point cloud analysis;permutation invariant architecture;self-organizing map;self-organizing network;point-to-node k nearest neighbor search;SO-Net;hierarchical feature extraction;Three-dimensional displays;Feature extraction;Training;Shape;Two dimensional displays;Graphical models}, 
doi={10.1109/CVPR.2018.00979}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{8269662, 
author={R. {Kaur} and D. {Sharma} and A. {Verma}}, 
booktitle={2017 4th International Conference on Signal Processing, Computing and Control (ISPCC)}, 
title={An advance 2D face recognition by feature extraction (ICA) and optimize multilayer architecture}, 
year={2017}, 
volume={}, 
number={}, 
pages={122-129}, 
abstract={Facial recognition has most significant real-life requests like investigation and access control. It is associated through the issue of appropriately verifying face pictures and transmit them person in a database. In a past years face study has been emerging active topic. Most of the face detector techniques could be classified into feature based methods and image based also. Feature based techniques adds low-level analysis, feature analysis, etc. Facial recognition is a system capable of verifying / identifying a human after 3D images. By evaluating selected facial unique features from the image and face dataset. Design from transformation method given vector dimensional illustration of individual face in a prepared set of images, Principle component analysis inclines to search a dimensional sub-space whose normal vector features correspond to the maximum variance direction in the real image space. The PCA algorithm evaluates the feature extraction, data, i.e. Eigen Values and vectors of the scatter matrix. In literature survey, Face recognition is a design recognition mission performed exactly on faces. It can be described as categorizing a facial either “known” or “unknown”, after comparing it with deposits known individuals. It is also necessary to need a system that has the capability of knowledge to recognize indefinite faces. Computational representations of facial recognition must statement various difficult issues. After existing work, we study the SIFT structures for the gratitude method. The novel technique is compared with well settled facial recognition methods, name component analysis and eigenvalues and vector. This algorithm is called PCA and ICA (Independent Component Analysis). In research work, we implement the novel approach to detect the face in minimum time and evaluate the better accuracy based on Back Propagation Neural Networks. We design the framework in face recognition using MATLAB 2013a simulation tool. Evaluate the performance parameters, i.e. the FAR (false acceptance rate), FRR (False rejection Rate) and Accuracy and compare the existing performance parameters i.e. accuracy.}, 
keywords={backpropagation;eigenvalues and eigenfunctions;face recognition;feature extraction;independent component analysis;principal component analysis;face recognition;feature extraction;appropriately verifying face pictures;face detector techniques;feature based techniques;low-level analysis;face dataset;individual face;image space;indefinite faces;settled facial recognition methods;independent component analysis;principle component analysis;vector dimensional illustration;normal vector features;backpropagation neural networks;Face recognition;Face;Feature extraction;Algorithm design and analysis;Signal processing algorithms;Videos;Databases;Face recognition;Features of face;Eigen values and Vectors;Neural Network}, 
doi={10.1109/ISPCC.2017.8269662}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8003592, 
author={A. S. {Asl} and M. A. {Oskoei}}, 
booktitle={2017 5th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)}, 
title={Depth dependent invariant features applied to person detection using 3D camera}, 
year={2017}, 
volume={}, 
number={}, 
pages={29-34}, 
abstract={This paper is about detection and tracking a person by mobile robots in in-door environments, such as shopping center and hospital. It uses vision based approaches to recognize texture of clothes. The paper proposes a method to use depth (distance) reference along with scale invariant features (SIFT) to recognize patterns in various orientation, distance and illumination. SIFT is an important feature detection algorithm that is robust against rotation, translation, and scaling in 2D images and to some extent against variations in lighting conditions. But it suffers inadequate performance for visual patterns rotated in 3D space. To overcome this issue, reference inputs given to the algorithm was extended to include images taken from different angles. The proposed algorithm showed considerably improved performance in detection for real-time applications.}, 
keywords={feature extraction;image sensors;image texture;mobile robots;object recognition;object tracking;robot vision;transforms;depth dependent invariant features;person detection;3D camera;person tracking;mobile robots;shopping center;hospital;vision based approaches;cloth texture recognition;depth reference;distance reference;scale invariant features;SIFT;pattern recognition;orientation;distance;illumination;feature detection algorithm;Feature extraction;Cameras;Three-dimensional displays;Robot kinematics;Detection algorithms;Robot sensing systems;Scale invariant features;SIFT;3D images;Person detection;mobile robot}, 
doi={10.1109/CFIS.2017.8003592}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8346387, 
author={H. M. R. {Afzal} and S. {Luo} and M. K. {Afzal}}, 
booktitle={2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)}, 
title={Reconstruction of 3D facial image using a single 2D image}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In many scenarios related to image processing, 3D face reconstruction is considered an essential part. A robust and fast method of 3D face reconstruction from a single 2D image is presented. This method consists of three main steps including extraction of features from single image, calculating the depth of image and adjustment of a 3D model on the direction of Z-axis. In the first step, the features of image are extracted by using supervised descent method (SDM). Using SDM, face regions like facial components (eyes, nose lips) and face contours are detected. Second step consists of depth prediction by implementation of multivariate Gaussian distribution. Finally, 3D face is constructed with the help of features and the depth information and 3D database. The proposed method has been verified by conducting several experiments depicted in evaluation section. Our method is robust in nature and gives good results even using a single image, comparing to other methods that use multiple images for reconstruction of 3D images.}, 
keywords={face recognition;feature extraction;Gaussian distribution;gradient methods;image reconstruction;stereo image processing;3D face reconstruction;supervised descent method;SDM;face regions;3D model;facial components;face contours;depth prediction;multivariate Gaussian distribution;image processing;single 2D image;3D facial image;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Shape;Two dimensional displays;Solid modeling;3D face reconstruction;features extraction;Gaussain distribution;facial modeling}, 
doi={10.1109/ICOMET.2018.8346387}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7523133, 
author={G. {Torkhani} and A. {Ladgham} and M. N. {Mansouri} and A. {Sakly}}, 
booktitle={2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
title={Gabor-SVM applied to 3D-2D deformed mesh model}, 
year={2016}, 
volume={}, 
number={}, 
pages={447-452}, 
abstract={We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate.}, 
keywords={face recognition;feature extraction;Gabor filters;mesh generation;support vector machines;visual databases;wavelet transforms;3D-2D deformed mesh model;Gabor-SVM;3D face recognition;2D modeling;3D modeling;facial curvatures detection;3D image transformation;3D triangular mesh;2D space;2D smoother mesh;Gabor wavelets;surface curves;salient face feature detection;Gabor facial model;support vector machines;3D AJMAL face database;Three-dimensional displays;Solid modeling;Face recognition;Face;Deformable models;Feature extraction;Databases;3D face recognition;salient points;deformed mesh model;facial curvatures;Gabor wavelet;SVM}, 
doi={10.1109/ATSIP.2016.7523133}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7443802, 
author={S. {Arora} and S. {Chawla}}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={An intensified approach to face recognition through average half face}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Face recognition has broad excitement in the latest trend in image processing. Face recognition refers to identify a specific individual in digital image by analyzing and comparing patterns. It has numerous benefits which attract every sector but there are some issues such as more time consumption and lesser accuracy which degrade the user services. To solve this problem we proposed a highly accurate and fast method to reduce the execution time. The proposed method uses average half face approach because overall system's accuracy is better in it rather than using the original full face image. The proposed method can be used to recognize both 2D and 3D images. It mainly includes the average half face creation, feature detection, full face recognition through average half face using distance metrics and finally checking system's accuracy along with time consumption. The proposed method is based on eye, nose and mouth detection.}, 
keywords={face recognition;feature extraction;face recognition;average half face approach;digital image processing;half face creation;feature detection;distance metrics;eye detection;nose detection;mouth detection;Face;Face recognition;Nose;Databases;Mouth;Three-dimensional displays;Image processing;Face recognition;Image processing;Accuracy;Average half face;Distance metrics}, 
doi={10.1109/INDICON.2015.7443802}, 
ISSN={2325-9418}, 
month={Dec},}
@INPROCEEDINGS{7823978, 
author={S. {Naveen} and R. K. {Ahalya} and R. S. {Moni}}, 
booktitle={2016 International Conference on Communication Systems and Networks (ComNet)}, 
title={Multimodal face recognition using spectral transformation by LBP and polynomial coefficients}, 
year={2016}, 
volume={}, 
number={}, 
pages={13-17}, 
abstract={This paper presents a multimodal face recognition using spectral transformation by Local Binary Pattern (LBP) and Polynomial Coefficients. Here 2D image and 3D image are combined to get multimodal face recognition. In this method a novel feature extraction is done using LBP and Polynomial Coefficients. Then these features are spectrally transformed using Discrete Fourier Transform (DFT). These spectrally transformed features extracted from texture image using the two methods are combined at the score level. Similarly this is done in depth image. Finally feature information from texture and depth are combined at the score level which gives better results than the individual results.}, 
keywords={discrete Fourier transforms;face recognition;feature extraction;image texture;multimodal face recognition;spectral transformation;LBP;polynomial coefficients;local binary pattern;2D image;3D image;feature extraction;discrete Fourier transform;DFT;texture image;depth image;feature information;Feature extraction;Face recognition;Discrete Fourier transforms;Discrete cosine transforms;Databases;Face;Two dimensional displays;Texture;Depth;Local Binary Pattern (LBP);Polynomial Coefficients;Multimodal;Discrete Fourier Transform (DFT)}, 
doi={10.1109/CSN.2016.7823978}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7899769, 
author={G. {Tian} and T. {Mori} and Y. {Okuda}}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Spoofing detection for embedded face recognition system using a low cost stereo camera}, 
year={2016}, 
volume={}, 
number={}, 
pages={1017-1022}, 
abstract={Spoofing detection is essential for practical face recognition system. Based on the fact that genuine face has special geometric curvatures across surface, this paper brings forward an ultra-fast yet accurate spoofing detection approach using a low-cost stereo camera. To obtain curvatures, the three dimensional shapes of selected facial landmarks are analyzed, by fitting point cloud around each landmark to a specific partial face surface. Spoofing detection is then performed by evaluating curvatures of each landmark and integrating them together. Experiments verify that the approach is able to detect spoofed faces in printed photographs without or with various bending at FAR equal to 0.00%. Meanwhile, genuine faces have a trivial opportunity to be falsely rejected: FRR is 0.59% for near frontal faces and less than 5% for faces with large varying poses. Detection time is 51 milliseconds when executed on a single processor [1] running at a clock frequency of 266M Hz, this makes the detection very suitable for embedded face recognition system.}, 
keywords={face recognition;stereo image processing;spoofing detection;embedded face recognition system;low cost stereo camera;facial landmark 3D shapes;frequency 266 MHz;Face;Three-dimensional displays;Nose;Face recognition;Cameras;Surface fitting;Fitting;spoof detection;point cloud;surface fitting;stereo vision}, 
doi={10.1109/ICPR.2016.7899769}, 
ISSN={}, 
month={Dec},}
@ARTICLE{8408720, 
author={R. S. {Siqueira} and G. R. {Alexandre} and J. M. {Soares} and G. A. P. {Thé}}, 
journal={IEEE Robotics and Automation Letters}, 
title={Triaxial Slicing for 3-D Face Recognition From Adapted Rotational Invariants Spatial Moments and Minimal Keypoints Dependence}, 
year={2018}, 
volume={3}, 
number={4}, 
pages={3513-3520}, 
abstract={This letter presents a multiple slicing model for threedimensional (3-D) images of human face, using the frontal, sagittal, and transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2-D spatial moments of Hu and 3-D spatial invariant rotation moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs nonneutral ROC I experiment, applying linear discriminant analysis as classifier and more than one sample for training, achieved 98.7% of verification rate at 0.1% of false acceptance rate. By using the support vector machine as classifier the rank1 experiment recognition rates of 99% and 95.4% have been achieved for a neutral vs neutral and for a neutral vs nonneutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when anger and disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity.}, 
keywords={face recognition;feature extraction;image classification;support vector machines;verification rate;false acceptance rate;support vector machine;rank1 experiment recognition rates;Bosphorus Database;3-D face recognition;multiple slicing model;human face;orthogonal planes;nose tip;facial recognition;3-D spatial invariant rotation moments;neutral vs nonneutral ROC;linear discriminant analysis;adapted rotational invariant spatial moments;minimal keypoint dependence;Three-dimensional displays;Face;Feature extraction;Nose;Two dimensional displays;Robustness;Iterative closest point algorithm;Computer vision for automation;recognition;surveillance systems}, 
doi={10.1109/LRA.2018.2854295}, 
ISSN={2377-3766}, 
month={Oct},}
@ARTICLE{7265032, 
author={L. {Ma} and G. {Zheng} and J. U. H. {Eitel} and L. M. {Moskal} and W. {He} and H. {Huang}}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={Improved Salient Feature-Based Approach for Automatically Separating Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar Point Cloud Data of Forest Canopies}, 
year={2016}, 
volume={54}, 
number={2}, 
pages={679-696}, 
abstract={Accurate separation of photosynthetic and nonphotosynthetic components in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a challenging but of key importance to understand the spatial distribution of the radiation regime, photosynthetic processes, and carbon and water exchanges of the forest canopy. The objective of this paper was to improve current methods for separating photosynthetic and nonphotosynthetic components in TLS data of forest canopies by adding two additional filters only based on its geometric information. By comparing the proposed approach with the eigenvalues plus color information-based method, we found that the proposed approach could effectively improve the overall producer's accuracy from 62.12% to 95.45%, and the overall classification producer's accuracy would increase from 84.28% to 97.80% as the forest leaf area index (LAI) decreases from 4.15 to 3.13. In addition, variations in tree species had negligible effects on the final classification accuracy, as shown by the overall producer's accuracy for coniferous (93.09%) and broadleaf (94.96%) trees. To remove quantitatively the effects of the woody materials in a forest canopy for improving TLS-based LAI estimates, we also computed the “woody-to-total area ratio” based on the classified linear class points from an individual tree. Automatic classification of the forest point cloud data set will facilitate the application of TLS on retrieving 3-D forest canopy structural parameters, including LAI and leaf and woody area ratios.}, 
keywords={forestry;optical radar;photosynthesis;remote sensing by radar;salient feature-based approach;photosynthetic components;nonphotosynthetic components;terrestrial lidar point cloud data;forest canopies;TLS data;geometric information;eigenvalues;information-based method;forest leaf area index;Three-dimensional displays;Vegetation;Laser radar;Eigenvalues and eigenfunctions;Accuracy;Surface emitting lasers;Probability density function;Light detection and ranging (lidar);pattern recognition;point classification;terrestrial laser scanning (TLS);woody-to-total area ratio;Light detection and ranging (lidar);pattern recognition;point classification;terrestrial laser scanning (TLS);woody-to-total area ratio}, 
doi={10.1109/TGRS.2015.2459716}, 
ISSN={0196-2892}, 
month={Feb},}
@INPROCEEDINGS{8661454, 
author={A. {Camarena-Ibarrola} and M. {Castro-Coria} and K. {Figueroa}}, 
booktitle={2018 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)}, 
title={Cloud Point Matching for Text-Independent Speaker Identification}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In Text-Independent speaker identification, the individual that produced some captured speech signal has to be identified without his collaboration, he might not even know that he is being the subject of an identification process. The system could not ask the individual to utter some specific word or phrase, which is precisely what is done in Text-Dependent speaker recognition. Text-Independent speaker identification is far more complicated since we cannot simply measure the similarity of an utterance of a word or phrase to another utterance made by the same speaker of the same word or phrase in which case we could use the dynamics of the speech signal. In this paper we search in the speech signal looking for voiced speech segments and estimate its first three formants, so we end up with a three-dimensional point cloud for each speaker of the collection of known speakers. To identify a speaker we have to measure the similarity of a point-cloud from an unknown speaker to the point-clouds that belong to known speakers, we do that by searching for local structures in the cloud in a way that is highly scalable and robust. We performed tests with both a collection of our own in Spanish and with the English Language Speech Database for Speaker Recognition (ELSDSR) from the Technical University of Denmark achieving results that improve recent published work with ELSDSR.}, 
keywords={speaker recognition;speech processing;cloud point matching;captured speech signal;three-dimensional point cloud;text-independent speaker identification;text-dependent speaker recognition;English Language Speech Database for Speaker Recognition;Technical University of Denmark;Three-dimensional displays;Speaker recognition;Feature extraction;Correlation;Mel frequency cepstral coefficient;Speech processing;Databases}, 
doi={10.1109/ROPEC.2018.8661454}, 
ISSN={2573-0770}, 
month={Nov},}
@ARTICLE{7524772, 
author={C. {Benedek} and B. {Gálai} and B. {Nagy} and Z. {Jankó}}, 
journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
title={Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance System}, 
year={2018}, 
volume={28}, 
number={1}, 
pages={101-113}, 
abstract={This paper presents new approaches for gait and activity analysis based on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed algorithms are embedded into an integrated 4D vision and visualization system, which is able to analyze and interactively display real scenarios in natural outdoor environments with walking pedestrians. The main focus of the investigations is gait-based person reidentification during tracking and recognition of specific activity patterns, such as bending, waving, making phone calls, and checking the time looking at wristwatches. The descriptors for training and recognition are observed and extracted from realistic outdoor surveillance scenarios, where multiple pedestrians are walking in the field of interest following possibly intersecting trajectories; thus, the observations might often be affected by occlusions or background noise. Since there is no public database available for such scenarios, we created and published a new Lidar-based outdoor gait and activity data set on our website that contains point cloud sequences of 28 different persons extracted and aggregated from 35-min-long measurements. The presented results confirm that both efficient gait-based identification and activity recognition are achievable in the sparse point clouds of a single RMB Lidar sensor. After extracting the people trajectories, we synthesized a free-viewpoint video, in which moving avatar models follow the trajectories of the observed pedestrians in real time, ensuring that the leg movements of the animated avatars are synchronized with the real gait cycles observed in the Lidar stream.}, 
keywords={gait analysis;image motion analysis;image recognition;optical radar;gait analysis;activity recognition;4D surveillance system;walking pedestrians;person reidentification;point cloud sequences;sparse point clouds;gait cycles;Lidar stream;multibeam Lidar sensor;RMB Lidar sensor;Laser radar;Three-dimensional displays;Trajectory;Surveillance;Legged locomotion;Databases;Visualization;4D reconstruction;activity recognition;gait recognition;multibeam Lidar}, 
doi={10.1109/TCSVT.2016.2595331}, 
ISSN={1051-8215}, 
month={Jan},}
@INPROCEEDINGS{8389776, 
author={G. {Geetha} and M. {Safa} and C. {Fancy} and K. {Chittal}}, 
booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)}, 
title={3D face recognition using Hadoop}, 
year={2017}, 
volume={}, 
number={}, 
pages={1882-1885}, 
abstract={Face Recognition is one of the biometric technique to vestige the given faces. We present a 3D face recognition method using Hadoop to recognize 3D faces under varying expressions, lighting and different poses to overcome the challenges of the 2D face recognition. In this paper, a threshold facial region of an image is detected and preprocessing is done based through the image excellence. If the selected face is frontal face with good lighting, extract the prerequisite features and do the necessary comparison steps to recognize the faces. In case, if the selected face is in bad lighting, then perform histogram equalization and normalization to increase the contrast. Different Poses and expressions are the very challenging zones which require surplus pre-processing to improve the performance of the face recognition system. Hence an enhanced normalization method called 3D Morphable Model are used as a pre- processing technique to create a frontal view from a non-frontal view and also merge images with different views in to a single frontal view. Next to diminish the number of features used for recognition process; we emulate the linear discriminant analysis method for further classification. Eventually, we used an open-source Hadoop Image Processing Interface (HIPI) to act as an interface for MapReduce technology for recognition.}, 
keywords={biometrics (access control);face recognition;feature extraction;parallel processing;frontal face;biometric technique;3D face recognition method;histogram equalization;surplus preprocessing;enhanced normalization method;3D morphable model;single frontal view;nonfrontal view;linear discriminant analysis method;open-source Hadoop image processing interface;HIPI;MapReduce technology;Face;Face recognition;Three-dimensional displays;Lighting;Feature extraction;Solid modeling;Hadoop;Image Processing;Map Reduce;Linear Discriminant analysis}, 
doi={10.1109/ICECDS.2017.8389776}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8125644, 
author={A. V. {Kumar} and V. V. R. {Prasad} and K. M. {Bhurchandi} and V. R. {Satpute} and L. {Pious} and S. {Kar}}, 
booktitle={2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
title={Dense reconstruction of 3D human face using 5 images and no reference model}, 
year={2017}, 
volume={}, 
number={}, 
pages={1185-1190}, 
abstract={3D reconstruction of human faces is of high importance in applications like forensics, facial features based human tracking and realistic reconstruction of human faces in virtual reality applications. The published algorithms so far require either a large number of views of a human face or a 3D facial reference model. This paper proposes a technique for 3D human face reconstruction using only five views without any reference model unlike most of the contemporary facial reconstruction techniques. Face localization is done followed by facial feature point extraction in the facial images. The features are tracked in the other images using the Kanade-Lucas-Tomasi (KLT) object tracking algorithm. 3D location of each feature is calculated in all the images using triangulation and a 3D point cloud is formed. The Point cloud is processed to remove outliers and holes. The 3D face is then formed by interpolating and meshing the point cloud. This computationally efficient and low cost technique outperforms commercial and online available software and published algorithms.}, 
keywords={face recognition;feature extraction;image reconstruction;object tracking;solid modelling;virtual reality;dense reconstruction;facial features;human tracking;3D facial reference model;contemporary facial reconstruction techniques;face localization;facial feature point extraction;facial images;point cloud;3D human face;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Skin;Image color analysis;Cameras;3D Reconstruction;meshing;point cloud;tracking;triangulation}, 
doi={10.1109/CoDIT.2017.8125644}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7943450, 
author={S. {Rajeev} and K. K. M. {Shreyas} and K. {Panetta} and S. {Agaian}}, 
booktitle={2017 IEEE International Symposium on Technologies for Homeland Security (HST)}, 
title={3-D palmprint modeling for biometric verification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Palmprint is a very unique and distinctive biometric trait because of features such as a person's inimitable principal lines, wrinkles, delta points, and minutiae. These constitute the main reasons why palmprint verification is considered as one of the most reliable personal identification methods. However, a clear majority of the research on palm-prints are concentrated on 2-D palmprint images irrespective of the fact that the human palm is a 3D-surface. While 2-D palmprint recognition has proved to be efficient in terms of verification rate, it has some essential downsides. These restrictions can adversely affect the performance and robustness of the palmprint recognition system. One of the possible solutions to resolve the limitations associated with 2-D palm print authentication systems is (i) to use a 3-D scanning system and to produce high quality 3-D images with depth information; (ii) to map 3-D palm-print images into 2-D images which may support the usage of 3-D images with both biometric palmprint 2-D image databases and 2-D palmprint recognition tools. The bloom of 3-D technologies has made it easier to capture and store 3-D images. The problem of a direct mapping approach is that a large section of the palm is hard-pressed on the scanner surface during 2-D based acquisition. This paper proposes a novel technique to unravel/map 3-D palm images to its equivalent 2-D palm-print image. This image can be then used to perform efficient and accurate 2-D identification/ verification. Experimental results and discussions will also be presented.}, 
keywords={authorisation;biometrics (access control);computer graphics;palmprint recognition;biometric verification;biometric trait;2-D palmprint images;2-D palmprint recognition;2-D palmprint authentication systems;3-D palmprint images;biometric palmprint 2-D image databases;2-D palmprint recognition tools;2-D identification-verification;Fingerprint recognition;Databases;Solid modeling;Mathematical model;Biomedical imaging;Feature extraction;biometric;palm-print;2-D;3-D;authentication;verification;identification;unrolling;mapping;modeling;security}, 
doi={10.1109/THS.2017.7943450}, 
ISSN={}, 
month={April},}
@ARTICLE{7050293, 
author={G. {Mills} and G. {Fotopoulos}}, 
journal={IEEE Geoscience and Remote Sensing Letters}, 
title={Rock Surface Classification in a Mine Drift Using Multiscale Geometric Features}, 
year={2015}, 
volume={12}, 
number={6}, 
pages={1322-1326}, 
abstract={Scale-dependent statistical depictions of surface morphology offer the potential to parameterize complex geometrical scaling relationships with greater detail than traditional fractal measures. Using multiscale operators, it is possible to identify points belonging to rough discontinuous surfaces in noisy point clouds solely on the basis of their local geometry. Many strategies for point cloud feature classification have been developed since the proliferation of laser scanning systems. Most of the techniques which are applicable to natural scenes employ external data sources such as hyperspectral imagery, return pulse intensity, and waveform data. In this letter, multiscale geometric parameters are used to identify individual point observations corresponding to rock surfaces in point clouds acquired by terrestrial laser scanning in scenes with man-made clutter and scanning artifacts. Three multiscale operators, namely, the approximate shape and density of a defined neighborhood and the distance of its mean point from its geometric center, are fused into a single feature vector. The procedure is demonstrated using real point cloud data acquired in a mine drift, with the goal of identifying points belonging to the rock face obscured by an overlying wire support mesh. Using the extra-trees classifier, extraneous returns caused by the mesh were excluded from the point cloud with a 97% success rate, while 87% of the desired surface points were retained.}, 
keywords={feature extraction;geometry;geophysical signal processing;mining;remote sensing by laser beam;rocks;surface morphology;rock surface classification;mine drift;multiscale geometric features;scale-dependent statistical depictions;surface morphology;complex geometrical scaling relationships;traditional fractal measures;multiscale operators;rough discontinuous surfaces;noisy point clouds;local geometry;point cloud feature classification;laser scanning systems;natural scenes;external data sources;hyperspectral imagery;return pulse intensity;waveform data;multiscale geometric parameters;terrestrial laser scanning;man-made clutter;scanning artifacts;defined neighborhood;single feature vector;real point cloud data;rock face;overlying wire support mesh;extra-trees classifier;Three-dimensional displays;Rocks;Rough surfaces;Surface roughness;Fractals;Surface morphology;Classification;geology;mining industry;point clouds;terrestrial LiDAR;Classification;geology;mining industry;point clouds;terrestrial LiDAR}, 
doi={10.1109/LGRS.2015.2398814}, 
ISSN={1545-598X}, 
month={June},}
@INPROCEEDINGS{8634657, 
author={S. A. {Ali Shah} and M. {Bennamoun} and M. {Molton}}, 
booktitle={2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)}, 
title={A Fully Automatic Framework for Prediction of 3D Facial Rejuvenation}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={How will I look afterwards? is a common question asked by the patients undergoing a cosmetic procedure. Cosmetic practitioners at present can only offer subjective and descriptive replies. This subjective prediction is a serious concern for patients undergoing cosmetic treatment and therefore necessitates the development of automatic techniques for facial quantification. This paper proposes a novel machine learning approach to quantify and predict the outcome of 3D facial rejuvenation prior to actual cosmetic procedure. The facial rejuvenation prediction results are achieved by estimating the dermal filler volume in 3D faces. This involves estimation of structural changes in 3D face images and to learn underlying structural mapping. Our preliminary experimental results show that the proposed model achieves superior prediction accuracy on real world dataset compared to baseline methods. The computational time analysis shows that the proposed technique is very efficient (at test time) which makes it suitable for real time applications.}, 
keywords={cosmetics;face recognition;learning (artificial intelligence);superior prediction accuracy;fully automatic framework;3D facial rejuvenation;common question;cosmetic practitioners;cosmetic treatment;automatic techniques;facial quantification;actual cosmetic procedure;facial rejuvenation prediction results;3D face images;Three-dimensional displays;Prediction algorithms;Solid modeling;Predictive models;Training;Machine learning;Australia;Facial rejuvenation;Predictive Modeling;Machine Learning}, 
doi={10.1109/IVCNZ.2018.8634657}, 
ISSN={2151-2205}, 
month={Nov},}
@INPROCEEDINGS{7780900, 
author={T. {Bolkart} and S. {Wuhrer}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Robust Multilinear Model Learning Framework for 3D Faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={4911-4919}, 
abstract={Multilinear models are widely used to represent the statistical variations of 3D human faces as they decouple shape changes due to identity and expression. Existing methods to learn a multilinear face model degrade if not every person is captured in every expression, if face scans are noisy or partially occluded, if expressions are erroneously labeled, or if the vertex correspondence is inaccurate. These limitations impose requirements on the training data that disqualify large amounts of available 3D face data from being usable to learn a multilinear model. To overcome this, we introduce the first framework to robustly learn a multilinear model from 3D face databases with missing data, corrupt data, wrong semantic correspondence, and inaccurate vertex correspondence. To achieve this robustness to erroneous training data, our framework jointly learns a multilinear model and fixes the data. We evaluate our framework on two publicly available 3D face databases, and show that our framework achieves a data completion accuracy that is comparable to state-of-the-art tensor completion methods. Our method reconstructs corrupt data more accurately than state-of-the-art methods, and improves the quality of the learned model significantly for erroneously labeled expressions.}, 
keywords={computer graphics;face recognition;learning (artificial intelligence);visual databases;robust multilinear model learning framework;3D faces;statistical variations;3D human faces;multilinear face model;face scans;3D face databases;missing data;corrupt data;semantic correspondence;inaccurate vertex correspondence;erroneous training data;data completion accuracy;tensor completion methods;Data models;Computational modeling;Three-dimensional displays;Solid modeling;Semantics;Robustness;Shape}, 
doi={10.1109/CVPR.2016.531}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{8099972, 
author={L. {Sheng} and J. {Cai} and T. {Cham} and V. {Pavlovic} and K. N. {Ngan}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Generative Model for Depth-Based Robust 3D Facial Pose Tracking}, 
year={2017}, 
volume={}, 
number={}, 
pages={4598-4607}, 
abstract={We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face models visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.}, 
keywords={face recognition;object tracking;pose estimation;ray tracing;statistical analysis;generative model;arbitrary facial expression variations;generative framework;face models visibility;depth-based robust 3D facial pose tracking;unconstrained scenarios;face model adaptation;statistical 3D face model;uncertainty prediction;distribution prediction;ray visibility constraint;Biwi datasets;ICT-3DHP datasets;Face;Three-dimensional displays;Solid modeling;Adaptation models;Robustness;Shape;Probabilistic logic}, 
doi={10.1109/CVPR.2017.489}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{7139616, 
author={T. {Linder} and S. {Wehner} and K. O. {Arras}}, 
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Real-time full-body human gender recognition in (RGB)-D data}, 
year={2015}, 
volume={}, 
number={}, 
pages={3039-3045}, 
abstract={Understanding social context is an important skill for robots that share a space with humans. In this paper, we address the problem of recognizing gender, a key piece of information when interacting with people and understanding human social relations and rules. Unlike previous work which typically considered faces or frontal body views in image data, we address the problem of recognizing gender in RGB-D data from side and back views as well. We present a large, gender-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the Kinect v1 and Kinect v2 sensor. We then learn and compare several classifiers on the Kinect v2 data using a HOG baseline, two state-of-the-art deep-learning methods, and a recent tessellation-based learning approach. Originally developed for person detection in 3D data, the latter is able to learn the best selection, location and scale of a set of simple point cloud features. We show that for gender recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 Hz.}, 
keywords={human-robot interaction;image classification;image sensors;learning (artificial intelligence);object detection;object recognition;real-time full-body human gender recognition;RGB-D data;human social relations;human social rules;image data;multiperspective RGB-D dataset;gender-balanced RGB-D dataset;annotated RGB-D dataset;Kinect v2 sensor;Kinect v1 sensor;HOG baseline;deep-learning methods;tessellation-based learning approach;person detection;point cloud features;Three-dimensional displays;Robot sensing systems;Training;Legged locomotion;Accuracy;Support vector machines}, 
doi={10.1109/ICRA.2015.7139616}, 
ISSN={1050-4729}, 
month={May},}
@ARTICLE{7312454, 
author={M. A. {de Jong} and A. {Wollstein} and C. {Ruff} and D. {Dunaway} and P. {Hysi} and T. {Spector} and F. {Liu} and W. {Niessen} and M. J. {Koudstaal} and M. {Kayser} and E. B. {Wolvius} and S. {Böhringer}}, 
journal={IEEE Transactions on Image Processing}, 
title={An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}, 
year={2016}, 
volume={25}, 
number={2}, 
pages={580-588}, 
abstract={In this paper, we present a novel approach to automatic 3D facial landmarking using 2D Gabor wavelets. Our algorithm considers the face to be a surface and uses map projections to derive 2D features from raw data. Extracted features include texture, relief map, and transformations thereof. We extend an established 2D landmarking method for simultaneous evaluation of these data. The method is validated by performing landmarking experiments on two data sets using 21 landmarks and compared with an active shape model implementation. On average, landmarking error for our method was 1.9 mm, whereas the active shape model resulted in an average landmarking error of 2.3 mm. A second study investigating facial shape heritability in related individuals concludes that automatic landmarking is on par with manual landmarking for some landmarks. Our algorithm can be trained in 30 min to automatically landmark 3D facial data sets of any size, and allows for fast and robust landmarking of 3D faces.}, 
keywords={face recognition;feature extraction;Gabor filters;wavelet transforms;automatic 3D facial landmarking algorithm;2D Gabor wavelets;map projections;feature extraction;data sets;active shape model;landmarking error;facial shape heritability;automatic landmarking;Three-dimensional displays;Face;Ellipsoids;Accuracy;Training;Solid modeling;Nose;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Algorithms;Anatomic Landmarks;Face;Humans;Imaging, Three-Dimensional;Pattern Recognition, Automated;Wavelet Analysis}, 
doi={10.1109/TIP.2015.2496183}, 
ISSN={1057-7149}, 
month={Feb},}
@ARTICLE{8024025, 
author={C. {Ye} and X. {Qian}}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired}, 
year={2018}, 
volume={26}, 
number={2}, 
pages={441-450}, 
abstract={This paper presents a 3-D object recognition method and its implementation on a robotic navigation aid to allow real-time detection of indoor structural objects for the navigation of a blind person. The method segments a point cloud into numerous planar patches and extracts their inter-plane relationships (IPRs).Based on the existing IPRs of the object models, the method defines six high level features (HLFs) and determines the HLFs for each patch. A Gaussian-mixture-model-based plane classifier is then devised to classify each planar patch into one belonging to a particular object model. Finally, a recursive plane clustering procedure is used to cluster the classified planes into the model objects. As the proposed method uses geometric context to detect an object, it is robust to the object's visual appearance change. As a result, it is ideal for detecting structural objects (e.g., stairways, doorways, and so on). In addition, it has high scalability and parallelism. The method is also capable of detecting some indoor non-structural objects. Experimental results demonstrate that the proposed method has a high success rate in object recognition.}, 
keywords={feature extraction;Gaussian processes;geometry;handicapped aids;image segmentation;mobile robots;object recognition;path planning;pattern clustering;robot vision;nonstructural objects;robotic navigation aid;real-time detection;indoor structural objects;numerous planar patches;object models;HLFs;Gaussian-mixture-model;plane classifier;planar patch;particular object model;recursive plane clustering procedure;point cloud;3D-object recognition;interplane relationships;Object recognition;Three-dimensional displays;Feature extraction;Visualization;Navigation;Cameras;RNA;Robotic navigation aid;visually impaired;3D object recognition;geometric context;Gaussian mixture model}, 
doi={10.1109/TNSRE.2017.2748419}, 
ISSN={1534-4320}, 
month={Feb},}
@INPROCEEDINGS{7163161, 
author={S. {Cheng} and I. {Marras} and S. {Zafeiriou} and M. {Pantic}}, 
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
title={Active nonrigid ICP algorithm}, 
year={2015}, 
volume={1}, 
number={}, 
pages={1-8}, 
abstract={The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15-20 years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region.}, 
keywords={face recognition;statistical analysis;facial surfaces;iterative closest point algorithm;nonrigid iterative closest point algorithms;3D face fitting;mouth region;statistical 3D model;3D facial deformation;single facial model;3D facial shape;3D mesh;3D facial model;active nonrigid ICP algorithm;Face;Three-dimensional displays;Iterative closest point algorithm;Solid modeling;Deformable models;Shape;Mouth}, 
doi={10.1109/FG.2015.7163161}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7299095, 
author={S. Z. {Gilani} and F. {Shafait} and A. {Mian}}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Shape-based automatic detection of a large number of 3D facial landmarks}, 
year={2015}, 
volume={}, 
number={}, 
pages={4639-4648}, 
abstract={We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morphologically significant. The proposed algorithm evolves level set curves with adaptive geometric speed functions to automatically extract effective seed points for dense correspondence. Correspondences are established by minimizing the bending energy between patches around seed points of given faces to those of a reference face. Given its hierarchical structure, our algorithm is capable of establishing thousands of correspondences between a large number of faces. Finally, a morphable model based on the dense corresponding points is fitted to an unseen query face for transfer of correspondences and hence automatic detection of landmarks. The proposed algorithm can detect any number of pre-defined landmarks including subtle landmarks that are even difficult to detect manually. Extensive experimental comparison on two benchmark databases containing 6, 507 scans shows that our algorithm outperforms six state of the art algorithms.}, 
keywords={face recognition;feature extraction;geometry;minimisation;shape recognition;shape-based automatic detection;3D facial landmark;geometric speed function;bending energy minimization;Three-dimensional displays;Shape;Databases;Level set;Algorithm design and analysis;Feature extraction;Mathematical model}, 
doi={10.1109/CVPR.2015.7299095}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{8314888, 
author={G. {Torkhani} and A. {Ladgham} and A. {Sakly}}, 
booktitle={2017 18th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering (STA)}, 
title={3D Gabor-Edge filters applied to face depth images}, 
year={2017}, 
volume={}, 
number={}, 
pages={578-582}, 
abstract={This manuscript introduces a novel 3D face authentication system inspired from the advantageous capacities of Gabor-Edge filters. The approach studies 3D face difficulties such as expression variety, different rotations and exposure to illuminations. The proposed systems starts by preprocessing the 3D face images to resolve acquisition problems. Then, a filtering process is performed by implanting our 3D Gabor-Edge technique extended based on the classic 3D Gabor masks. The next step is to achieve the classification of facial features from the edge saliency by the artificial Neural Network Classifier (NNC). The evaluation of the adopted system is achieved by exporting common datasets from GavabDB database. Experimental results are reported to prove the high accuracy rates of our method compared to the recent researches in the same biometric field.}, 
keywords={biometrics (access control);edge detection;face recognition;feature extraction;Gabor filters;image classification;neural nets;3D face difficulties;3D face images;3D Gabor-Edge technique;classic 3D Gabor masks;edge saliency;Gabor-edge filters;3D face authentication system;acquisition problems;facial feature classification;artificial neural network classifier;GavabDB database;biometric field;Three-dimensional displays;Face;Feature extraction;Authentication;Gabor filters;Face recognition;face authentication;Gabor filtering;3D images;saliency}, 
doi={10.1109/STA.2017.8314888}, 
ISSN={2573-539X}, 
month={Dec},}
@INPROCEEDINGS{7852696, 
author={F. {Li} and C. {Lai} and S. {Jin} and Y. {Peng}}, 
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={Automatic calibration of 3D human faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={135-139}, 
abstract={This paper presents a method for correcting the posture of 3D face with unknown position and posture based on the standard face pose. We define a standard posture of face which is not affected by the head gesture. Then the algorithms of principal component analysis and iterative closest point are used to achieve the preliminary adjustment of the target model. For the precise calibration, we use the method of depth values iterative searching to find the point of nose precisely, and propose an algorithm of searching along a straight line to find the ridge line of nose. Ultimately we determine the difference between the posture of the target model and standard posture to obtain accurate calculation of the target model posture.}, 
keywords={calibration;computer graphics;face recognition;principal component analysis;automatic calibration;3D human faces;standard face pose;standard posture;head gesture;principal component analysis;iterative closest point;precise calibration;iterative searching;target model posture;Three-dimensional displays;Calibration;Nose;Standards;Iterative closest point algorithm;Solid modeling;Principal component analysis;3D Faces;Posture Correction;Face Calibration;Facial Feature Points}, 
doi={10.1109/CISP-BMEI.2016.7852696}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8317846, 
author={R. {Varga} and A. {Costea} and H. {Florea} and I. {Giosan} and S. {Nedevschi}}, 
booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)}, 
title={Super-sensor for 360-degree environment perception: Point cloud segmentation using image features}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={This paper describes a super-sensor that enables 360-degree environment perception for automated vehicles in urban traffic scenarios. We use four fisheye cameras, four 360-degree LIDARs and a GPS/IMU sensor mounted on an automated vehicle to build a super-sensor that offers an enhanced low-level representation of the environment by harmonizing all the available sensor measurements. Individual sensors cannot provide a robust 360-degree perception due to their limitations: field of view, range, orientation, number of scanning rays, etc. The novelty of this work consists of segmenting the 3D LIDAR point cloud by associating it with the 2D image semantic segmentation. Another contribution is the sensor configuration that enables 360-degree environment perception. The following steps are involved in the process: calibration, timestamp synchronization, fisheye image unwarping, motion correction of LIDAR points, point cloud projection onto the images and semantic segmentation of images. The enhanced low-level representation will improve the high-level perception environment tasks such as object detection, classification and tracking.}, 
keywords={image classification;image motion analysis;image representation;image resolution;image segmentation;image sensors;object detection;object tracking;optical radar;360-degree environment perception;point cloud segmentation;automated vehicle;360-degree LIDARs;GPS/IMU sensor;3D LIDAR point cloud;sensor measurements;image semantic segmentation;Conferences;Intelligent transportation systems;automated driving;environment perception;fisheye images;3D LIDAR points;360-degree perception;super-sensor}, 
doi={10.1109/ITSC.2017.8317846}, 
ISSN={2153-0017}, 
month={Oct},}
@INPROCEEDINGS{7550062, 
author={ and D. {Huang} and Y. {Wang} and J. {Sun}}, 
booktitle={2016 International Conference on Biometrics (ICB)}, 
title={Lock3DFace: A large-scale database of low-cost Kinect 3D faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future.}, 
keywords={emotion recognition;face recognition;image texture;pose estimation;video databases;video signal processing;large-scale database;low-cost Kinect 3D face videos;Lock3DFace database;3D face analysis;3D face recognition;3D FR;expression analysis;pose analysis;occlusion analysis;time lapse analysis;texture analysis;near infrared channels;lighting condition;radiation intensity;Three-dimensional displays;Databases;Videos;Solid modeling;Two dimensional displays;Lighting;Sensors}, 
doi={10.1109/ICB.2016.7550062}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7729385, 
author={C. {Gevaert} and C. {Persello} and R. {Sliuzas} and G. {Vosselman}}, 
booktitle={2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)}, 
title={Integration of 2D and 3D features from UAV imagery for informal settlement classification using Multiple Kernel Learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={1508-1511}, 
abstract={Informal settlement upgrading projects require high-resolution and up-to-date thematic maps in order to plan and design effective interventions. To this end, Unmanned Aerial Vehicles (UAVs) provide the opportunity to obtain very high resolution 2D orthomosaics and 3D point clouds where and when needed. The heterogeneous, dense structures which typically make up an informal settlement motivate the importance of integrating complex 2D and 3D features obtained from UAV data into a single classification problem. Multiple Kernel Learning (MKL) Support Vector Machines (SVMs) maintain the distinct characteristics of the different feature spaces by optimizing individual kernels for specific feature groups which are later combined into a single kernel used for classification. Both the kernel parameters and kernel weights can be optimized by considering the alignment between the kernel and an ideal kernel which would perfectly classify the samples. This paper demonstrates how extracting high-level features from both the 2D orthomosaic as well as the 3D point cloud (obtained by an UAV), and integrating them through a MKL approach, can obtain an Overall Accuracy of 90.29%, a 4% increase over the results obtained using single kernel methods.}, 
keywords={autonomous aerial vehicles;geophysical image processing;image classification;land use;remote sensing;support vector machines;informal settlement classification;multiple kernel learning;unmanned aerial vehicle;UAV imagery;2D orthomosaics;3D point cloud;support vector machine;Kernel;Three-dimensional displays;Feature extraction;Support vector machines;Two dimensional displays;Unmanned aerial vehicles;Vegetation mapping;informal settlements;image classification;unmanned aerial vehicles;support vector machines;multiple kernel learning}, 
doi={10.1109/IGARSS.2016.7729385}, 
ISSN={2153-7003}, 
month={July},}
@ARTICLE{8440881, 
author={A. {Switonski} and T. {Krzeszowski} and H. {Josinski} and B. {Kwolek} and K. {Wojciechowski}}, 
journal={IET Biometrics}, 
title={Gait recognition on the basis of markerless motion tracking and DTW transform}, 
year={2018}, 
volume={7}, 
number={5}, 
pages={415-422}, 
abstract={In this study, a framework for view-invariant gait recognition on the basis of markerless motion tracking and dynamic time warping (DTW) transform is presented. The system consists of a proposed markerless motion capture system as well as introduced classification method of mocap data. The markerless system estimates the three-dimensional locations of skeleton driven joints. Such skeleton-driven point clouds represent poses over time. The authors align point clouds in every pair of frames by calculating the minimal sum of squared distances between the corresponding joints. A point cloud distance measure with temporal context has been utilised ink-nearest neighbours algorithm to compare time instants of motion sequences. To enhance the generalisation of the recognition and to shorten the processing time, for every individual a single multidimensional time series among several multidimensional time series describing the individual's gait is established. The correct classification rate has been determined on the basis of a real dataset of human gait. It contains 230 gait cycles of 22 subjects. The tracking results on the basis of markerless motion capture are referenced to Vicon system, whereas the achieved accuracies of recognition are compared with the ones obtained by DTW that is based on rotational data.}, 
keywords={gait analysis;image classification;image motion analysis;image sequences;learning (artificial intelligence);object tracking;time series;transforms;multidimensional time series;motion sequences;k-nearest neighbours algorithm;skeleton-driven point clouds;classification method;markerless motion capture system;dynamic time warping transform;view-invariant gait recognition;DTW transform;markerless motion tracking;gait recognition}, 
doi={10.1049/iet-bmt.2017.0134}, 
ISSN={2047-4938}, 
month={},}
@INPROCEEDINGS{7797090, 
author={S. Z. {Gilani} and A. {Mian}}, 
booktitle={2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)}, 
title={Towards Large-Scale 3D Face Recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases.}, 
keywords={convolution;face recognition;feature extraction;image matching;learning (artificial intelligence);solid modelling;large-scale 3D face recognition;fully convolutional deep network;FCDN;seed points extraction;level set curves;single curvature dependent adaptive speed;dense correspondence;training set;surface matching;morphable model fitting;landmark localization results;FRGCv2 dataset;Bosphorus dataset;query face recognition;Three-dimensional displays;Face;Face recognition;Solid modeling;Databases;Two dimensional displays;Robustness}, 
doi={10.1109/DICTA.2016.7797090}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8373915, 
author={W. {Tian} and F. {Liu} and Q. {Zhao}}, 
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)}, 
title={Landmark-Based 3D Face Reconstruction from an Arbitrary Number of Unconstrained Images}, 
year={2018}, 
volume={}, 
number={}, 
pages={774-779}, 
abstract={In this paper, we propose a novel method for reconstructing 3D faces from 2D images. The method is characterized in three aspects. (i) It utilizes only geometric cues in the input images, i.e., 2D facial landmarks. (ii) It works for an arbitrary number of unconstrained images, both single and multiple images. (iii) It can effectively exploit complementary information in multiple images of varying poses and expressions. The method is implemented based on cascaded regression in shape space. We have evaluated the method on three databases and observed from the experimental results that (i) the reconstruction error is reduced as more images of different poses are used, (ii) the proposed method can obtain comparable reconstruction results by using state-of-the-art automated methods to detect the 2D landmarks, and (iii) the proposed method is robust to variations in facial expressions and image qualities.}, 
keywords={face recognition;image reconstruction;regression analysis;stereo image processing;cascaded regression;image qualities;facial expressions;reconstruction error;2D facial landmarks;geometric cues;unconstrained images;3D face reconstruction;Conferences;Face;Gesture recognition;3D face reconstruction;unconstrained images;landmark based;cascade regression}, 
doi={10.1109/FG.2018.00122}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7899989, 
author={D. {Kim} and J. {Choi} and J. T. {Leksut} and G. {Medioni}}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Expression invariant 3D face modeling from an RGB-D video}, 
year={2016}, 
volume={}, 
number={}, 
pages={2362-2367}, 
abstract={We aim to reconstruct an accurate neutral 3D face model from an RGB-D video in the presence of extreme expression changes. Since each depth frame, taken by a low-cost sensor, is noisy, point clouds from multiple frames can be registered and aggregated to build an accurate 3D model. However, direct aggregation of multiple data produces erroneous results in natural interaction (e.g., talking and showing expressions). We propose to analyze facial expression from an RGB frame and neutralize the corresponding 3D point cloud if needed. We first estimate the person's expression by fitting blendshape coefficients using 2D facial landmarks for each frame and calculate an expression deformity (expression score). With the estimated expression score, we determine whether an input face is neutral or non-neutral. If the face is non-neutral, we proceed to neutralize the expression of the 3D point cloud in that frame. To neutralize the 3D point cloud of a face, we deform our generic 3D face model by applying the estimated blendshape coefficients, find displacement vectors from the deformed generic face to a neutral generic face, and apply the displacement vectors to the input 3D point cloud. After preprocessing frames in a video, we rank frames based on the expression scores and register the ranked frames into a single 3D model. Our system produces a neutral 3D face model in the presence of extreme expression changes even when neutral faces do not exist in the video.}, 
keywords={data aggregation;image reconstruction;image registration;video signal processing;expression invariant 3D face modeling;RGB-D video;depth frame;noisy point clouds;low-cost sensor;multiple data direct aggregation;3D point cloud;blendshape coefficients;2D facial landmarks;displacement vectors;neutral generic face;video preprocessing frames;frame ranking;person expression estimation;Three-dimensional displays;Face;Solid modeling;Two dimensional displays;Deformable models;Computational modeling;Iterative closest point algorithm}, 
doi={10.1109/ICPR.2016.7899989}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7351660, 
author={R. J. {Arteaga} and S. J. {Ruuth}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={Laplace-Beltrami spectra for shape comparison of surfaces in 3D using the closest point method}, 
year={2015}, 
volume={}, 
number={}, 
pages={4511-4515}, 
abstract={The need to compare separate objects arises in a wide range of applications. In one approach for comparing objects, `Shape-DNA' is constructed to give a numerical fingerprint representing an individual object. Shape-DNA is a cropped set of eigenvalues of the Laplace-Beltrami operator for the surface of the object. In this paper, we compute the Shape-DNA of surfaces using the closest point method. Our approach may be applied to a variety of surface representations including triangulations, point clouds and certain analytical shapes. A 2D multidimensional scaling plot illustrates that similar objects form groups based on the Shape-DNAs. Our method has the benefit that it may be applied to surfaces defined by dense point clouds without requiring the construction of point connectivity.}, 
keywords={DNA;eigenvalues and eigenfunctions;fingerprint identification;image representation;Laplace transforms;shape recognition;Laplace-Beltrami spectra;shape comparison;closest point method;shape-DNA;numerical fingerprint representation;eigenvalues;surface representations;point clouds;2D multidimensional scaling plot;Shape;Three-dimensional displays;Eigenvalues and eigenfunctions;Standards;Interpolation;Rabbits;shape comparison;Laplace-Beltrami spectra;closest point method;point cloud}, 
doi={10.1109/ICIP.2015.7351660}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7988861, 
author={A. T. {Angonese} and P. F. {Ferreira Rosa}}, 
booktitle={2017 International Conference on Military Technologies (ICMT)}, 
title={Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform}, 
year={2017}, 
volume={}, 
number={}, 
pages={779-786}, 
abstract={This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.}, 
keywords={mobile robots;neural nets;object detection;object recognition;operating systems (computers);path planning;robot vision;SLAM (robots);multiple people detection;multiple people identification system;dynamic simultaneous localization and mapping system;autonomous mobile robotic platform;robot exploration;robot navigation;Pioneer 3DX robot;RGBD camera;Sick Lms200 sensor;robot operating system;SLAM;HOG detector;histogram of oriented gradients;deep convolutional neural network;CNN;MatLab MatConvNet library;Simultaneous localization and mapping;Libraries;Computer architecture;Feature extraction;Operating systems;People Detection;HOG;Deep Learning;CNN;Simultaneous Localization and Mapping (SLAM);robot operating system ROS}, 
doi={10.1109/MILTECHS.2017.7988861}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7391814, 
author={ and and and }, 
booktitle={2015 International Conference on 3D Imaging (IC3D)}, 
title={Robust nose tip detection for face range images based on local features in scale-space}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Being the most distinct feature point in 3D facial landmarks, nose tip plays a significant role in 3D facial studies such as face detection, face recognition, facial features extraction, face alignment, etc. Successful detection of nose tip can facilitate many tasks of 3D facial studies. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, needs not training, can handle large rotations and occlusions. To reduce computational cost, we first remove small isolated regions from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, the Multi-angle Energy (ME) of each point is computed and sorted in descending order. Then the first m points in the descending order list are obtained and hierarchical clustering method is used to cluster these points. In the first h largest clusters, we can find one point with the largest ME. For all scales of the scale-space, we get a series of such points which are treated as nose tip candidates. For these candidates, we apply hierarchical clustering again. In the obtained largest cluster, we compute the mean value of ME. The ME of nose tip will be closest to the mean value. We evaluate our method in two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The experimental results verify the robustness of our method with a high nose tip detection rate.}, 
keywords={face recognition;feature extraction;object detection;pattern clustering;nose tip detection;face range images;local features;3D facial landmarks;3D facial studies;multiangle energy;ME;range image smoothing;hierarchical clustering method;FRGC database;BOSPHORUS database;Nose;Three-dimensional displays;Face;Robustness;Training;Feature extraction;Smoothing methods;nose tip;3D faces;range images;robust smoothing;normal;scale-space;multi-angle energy;sphere fitting;least square;hierarchical clustering}, 
doi={10.1109/IC3D.2015.7391814}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7517246, 
author={P. {Azevedo} and T. O. {Dos Santos} and E. {De Aguiar}}, 
booktitle={2016 XVIII Symposium on Virtual and Augmented Reality (SVR)}, 
title={An Augmented Reality Virtual Glasses Try-On System}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper presents a virtual try-on system to correctly visualize 3D objects (e.g., glasses) in the face of a given user. By capturing the image and depth information of a user through a low-cost RGB-D camera, we apply a face tracking technique to detect specific landmarks in the facial image. These landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3D facial morphable model that fits as good as possible to the user's head and face. At the end, we deform the chosen 3D objects from its rest shape to a deformed shape matching the specific facial shape of the user. The last step projects and renders the 3D object into the original image, with enhanced precision and in proper scale, showing the selected object in the user's face. We validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. Our results demonstrate that, by fitting a facial model to the user's face, the rendered virtual 3D objects look more realistic.}, 
keywords={augmented reality;face recognition;image capture;image colour analysis;image matching;image reconstruction;image sensors;rendering (computer graphics);shape recognition;augmented reality virtual glass try-on system;3D object visualization;image capturing;depth information;low-cost RGB-D camera;facial image;point cloud reconstruction;3D facial morphable model;deformed shape matching;virtual 3D object rendering;Three-dimensional displays;Face;Solid modeling;Glass;Cameras;Shape;Augmented reality;Virtual reality;Computer graphics}, 
doi={10.1109/SVR.2016.12}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7899697, 
author={G. {Pang} and U. {Neumann}}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={3D point cloud object detection with multi-view convolutional neural network}, 
year={2016}, 
volume={}, 
number={}, 
pages={585-590}, 
abstract={Efficient detection of three dimensional (3D) objects in point clouds is a challenging problem. Performing 3D descriptor matching or 3D scanning-window search with detector are both time-consuming due to the 3-dimensional complexity. One solution is to project 3D point cloud into 2D images and thus transform the 3D detection problem into 2D space, but projection at multiple viewpoints and rotations produce a large amount of 2D detection tasks, which limit the performance and complexity of the 2D detection algorithm choice. We propose to use convolutional neural network (CNN) for the 2D detection task, because it can handle all viewpoints and rotations for the same class of object together, as well as predicting multiple classes of objects with the same network, without the need for individual detector for each object class. We further improve the detection efficiency by concatenating two extra levels of early rejection networks with binary outputs before the multi-class detection network. Experiments show that our method has competitive overall performance with at least one-order of magnitude speed-up comparing with latest 3D point cloud detection methods.}, 
keywords={neural nets;object detection;multiclass detection network;early rejection networks;CNN;convolutional neural network;2D detection algorithm;2D space;2D images;3-dimensional complexity;3D scanning-window search;3D descriptor matching;three dimensional object detection;multiview convolutional neural network;3D point cloud object detection;Three-dimensional displays;Two dimensional displays;Object detection;Training;Detectors;Complexity theory;Search problems}, 
doi={10.1109/ICPR.2016.7899697}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7869954, 
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and N. {Werghi} and M. {Linguraru}}, 
booktitle={2016 IEEE 59th International Midwest Symposium on Circuits and Systems (MWSCAS)}, 
title={Facial landmarks detection using 3D constrained local model on mesh manifold}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={This paper proposes a novel 3D Constrained Local Models (CLM) approach applied for the detection of facial landmarks in 3D images. This approach capitalizes on the properties of Independent Component Analysis (ICA) to define appropriate priors of a face Point Distribution Model. Tailored to the mesh manifold modality, this approach address the limitations of the depth images which require pose normalization and suffer from the loss of the shape information caused by 2D projection. We validate this framework through a series of experiments conducted with the public Bosporus database, whereby it demonstrates a competitive performance compared to other state of the art methods.}, 
keywords={face recognition;independent component analysis;mesh generation;object detection;facial landmarks detection;3D constrained local model;CLM approach;3D images;independent component analysis;ICA;face point distribution model;mesh manifold modality;depth images;shape information loss;2D projection;public Bosporus database;pose normalization;Shape;Principal component analysis;Face;Three-dimensional displays;Deformable models;Integrated circuit modeling;Adaptation models}, 
doi={10.1109/MWSCAS.2016.7869954}, 
ISSN={1558-3899}, 
month={Oct},}
@INPROCEEDINGS{8633125, 
author={X. {Ju} and I. R. {Garcia Júnior} and L. {De Freitas Silva} and P. {Mossey} and D. {Al-Rudainy} and A. {Ayoub} and A. M. {De Mattos}}, 
booktitle={2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={3D Head Shape Analysis of Suspected Zika Infected Infants}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={The babies infected from Zika before they are born are at risk for problems with brain development and microcephaly. 3D head images of 43 Zika cases and 43 controls were collected aiming to extract shape characteristics for diagnosis purposes. Principal component analysis (PCA) has been applied on the vaults and faces of the collected 3D images and the scores on the second principal components of the vaults and faces showed significant differences between the control and Zika groups. The shape variations from -2σ to 2σ illustrated the typical characteristics of microcephaly of the Zika babies. Canonical correlation analysis (CCA) showed a significant correlation in the first CCA variates of face and vault which indicated the potential of 3D facial imaging for Zika surveillance. Further head circumferences and distances from ear to ear were measured from the 3D images and preliminary results showed the adding ear to ear distances for classifying control and Zika children strengthened the abilities of tested classification models.}, 
keywords={diseases;image classification;medical image processing;paediatrics;principal component analysis;shape recognition;stereo image processing;suspected Zika infected infants;microcephaly;Zika surveillance;3D facial imaging;canonical correlation analysis;Zika babies;shape variations;principal component analysis;3D head images;brain development;shape analysis;Zika children;3D imaging;shape analysis;Zika}, 
doi={10.1109/CISP-BMEI.2018.8633125}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8089906, 
author={L. {Fangmin} and C. {Ke} and L. {Xinhua}}, 
booktitle={2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA)}, 
title={3D Face Reconstruction Based on Convolutional Neural Network}, 
year={2017}, 
volume={}, 
number={}, 
pages={71-74}, 
abstract={Fast and robust 3D reconstruction of facial geometric structure from a single image is a challenging task with numerous applications, but there exist two problems when applied "in the wild": the 3D estimates are unstable for different photos of the same subject; the 3D estimates are over-regularized and generic. In response, a robust method for regressing discriminative 3D morphable face models(3DMM) is described to support face recognition and 3D mask printing. Combining the local data sets with the public data sets, improving the exiting 3DMM fitting method and then using a convolutional neural network(CNN) to improve reconstruction effect. The ground truth 3D faces of the CNN are the pooled 3DMM parameters extracted from the photos of the same subject. Using CNN to regress 3DMM shape and texture parameters directly from an input photo and offering a method for generating huge numbers of labeled examples. There are two key points of the paper: one is the training data generation for the model training; the other is the training of 3D reconstruction model. Experimental results and analysis show that this method costs much less time than traditional methods of 3D face modeling, and it is improved for different races on photos with any angles than the existing methods based on deep learning, and the system has better robustness.}, 
keywords={face recognition;image morphing;image reconstruction;image texture;learning (artificial intelligence);neural nets;regression analysis;shape recognition;3D face reconstruction;facial geometric structure;robust method;face recognition;3D mask printing;local data sets;public data sets;reconstruction effect;texture parameters;training data generation;3D reconstruction model;3D face modeling;convolutional neural network;discriminative 3D morphable face models;3DMM fitting method;CNN;Face;Three-dimensional displays;Solid modeling;Image reconstruction;Shape;Robustness;Data models;3D face reconstruction;convolutional neural network(CNN);3DMM;shape;texture}, 
doi={10.1109/ICICTA.2017.23}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7428566, 
author={J. {Liu} and Q. {Zhang} and C. {Tang}}, 
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
title={CoMES: A novel method for robust nose tip detection in face range images}, 
year={2015}, 
volume={}, 
number={}, 
pages={309-315}, 
abstract={As the most distinct feature point in facial landmarks, nose tip plays a significant role in 3D facial studies. Successful detection of nose tip can facilitate many 3D facial studies tasks. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, need not training, can handle large rotations and occlusions. We first remove small isolated connected regions and noise from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, we compute multi-angle energy of each point, then we use hierarchical clustering method to cluster the points whose multi-angle energies are larger than a threshold value. In the largest cluster, we can find one point with the largest multi-angle energy. For all scales of the scale-space, we get a series of such points and apply hierarchical clustering again for these points, nose tip will have the largest multi-angle energy in the largest cluster. We evaluate our method in FRGC v2.0 3D face database and BOSPHORUS 3D face database. The experimental results verify the robustness of our method with a high nose tip detection rate.}, 
keywords={edge detection;face recognition;pattern clustering;solid modelling;CoMES;robust nose tip detection;face range images;facial landmark;3D facial studies;scale-space;multiangle energy;hierarchical clustering method;FRGC v2.0 3D face database;BOSPHORUS 3D face database;Nose;Face;Three-dimensional displays;Training;Robustness;Feature extraction;Smoothing methods;nose tip;3D faces;scale-space;multi-angle energy;hierarchical clustering;range images}, 
doi={10.1109/IAEAC.2015.7428566}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7351535, 
author={S. {Sankaranarayanan} and V. M. {Patel} and R. {Chellappa}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={3D facial model synthesis using coupled dictionaries}, 
year={2015}, 
volume={}, 
number={}, 
pages={3896-3900}, 
abstract={In this work, we propose a generative way of modeling faces, where the 3D shape of a face is generated by a supervised learning procedure involving coupled sparse feature learning. To learn dictionaries using the proposed method, we use the USF-HUMAN ID database [1]. We provide as input to our training system, paired correspondences of 2D and 3D images of individuals and aim to learn the low-level patches both in 2D and 3D domains that describe the corresponding subspaces in a sparse manner. We demonstrate the efficacy of our method by quantitative results on the 3D database and qualitative results on images drawn from the internet.}, 
keywords={face recognition;learning (artificial intelligence);3D facial model synthesis;coupled dictionaries;supervised learning procedure;coupled sparse feature learning;dictionary learning;USF-HUMAN ID database;2D images;3D images;Three-dimensional displays;Dictionaries;Solid modeling;Shape;Training;Databases;Encoding;3D Model;Face Synthesis;Coupled Sparse Coding;Cross-modal Learning}, 
doi={10.1109/ICIP.2015.7351535}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7899906, 
author={H. X. {Pham} and V. {Pavlovic} and and }, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Robust real-time performance-driven 3D face tracking}, 
year={2016}, 
volume={}, 
number={}, 
pages={1851-1856}, 
abstract={We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker capable of handling a wide range of target scene depths, whose performances are demonstrated in our extensive experiments better than those of the state-of-the-arts.}, 
keywords={cameras;computational geometry;face recognition;image colour analysis;object tracking;optimisation;solid modelling;stereo image processing;video signal processing;3D face tracking;RGBD video streams;head pose tracking;facial actions;cameras;point cloud;3D shape regressor;joint 2D+3D optimization;3D model reconstruction;Three-dimensional displays;Shape;Face;Training;Two dimensional displays;Optimization;Solid modeling}, 
doi={10.1109/ICPR.2016.7899906}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7976644, 
author={S. {Sghaier} and C. {Souani} and H. {Faeidh} and K. {Besbes}}, 
booktitle={2016 Global Summit on Computer Information Technology (GSCIT)}, 
title={Novel Technique for 3D Face Segmentation and Landmarking}, 
year={2016}, 
volume={}, 
number={}, 
pages={27-31}, 
abstract={In this paper we propose a new technique to detect and extract any part that we need from a 3D face. The regions of interest of our approach are the eyes and the nose. Moreover, we are interested in the extraction of the salient landmarks in 3D face. Therefore, in this paper a new method is proposed to detect the nose tip and eye corners from a three-dimensional face range image. Our original technique allows fully automated processing, treating incomplete and noisy input data, and automatically rejecting non-facial areas. Besides, it is robust against holes in 3D image and insensitive to facial expressions. Besides, it is stable against translation and rotation of the face, and it is suitable for different resolutions of images. All the experiments have been performed on the GAVAB and FRAV 3D databases. After applying the proposed method to the 3D face, experimental results show that it is comparable to state-of-the-art methods in terms of its accuracy and flexibility.}, 
keywords={face recognition;image segmentation;visual databases;3D face segmentation;landmark extraction;nose tip detection;eye corner detection;three-dimensional face range image;automatic nonfacial area rejection;3D image;facial expressions;image resolutions;GAVAB 3D database;FRAV 3D database;Face;Three-dimensional displays;Nose;Information technology;Noise measurement;Robustness;Image resolution;3D face;segmentation;region of interest;anthropometric;landmarks}, 
doi={10.1109/GSCIT.2016.17}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7332643, 
author={C. {Stahlschmidt} and A. {Gavriilidis} and A. {Kummert}}, 
booktitle={2015 IEEE 9th International Workshop on Multidimensional (nD) Systems (nDS)}, 
title={Classification of ascending steps and stairs using Time-of-Flight sensor data}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper proposes a method to analyse human-made environments in order to verify the presence of ascending steps or stairs. Our system is intended to assist visually impaired people by providing acoustic information about the scene in front of a low-resolution Time-of-Flight (ToF) camera that is fixed to a mobile vehicle (rollator). Detailed instructions to the user about potentially hazardous situations are provided. This paper in particular deals with a fast approach on classification of ascending steps in 3D point clouds. This method is part of a system that aims on enhancing visually impaired persons understand the environment and help prevent collisions.}, 
keywords={computer graphics;image classification;image resolution;ascending step classification;time-of-flight sensor data;human-made environment;visually impaired people;acoustic information;low-resolution time-of-flight camera;ToF camera;mobile vehicle;rollator;hazardous situation;3D point cloud;visually impaired person;Three-dimensional displays;Cameras;Data mining;Algorithm design and analysis;Gray-scale;Acoustics;Mobile communication}, 
doi={10.1109/NDS.2015.7332643}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8035340, 
author={L. {Han} and Q. {Xiao} and X. {Liang}}, 
booktitle={2017 International Conference on Computer, Information and Telecommunication Systems (CITS)}, 
title={3D face reconstruction based on progressive cascade regression}, 
year={2017}, 
volume={}, 
number={}, 
pages={297-301}, 
abstract={In order to better learn the distributions of 2D and 3D faces and the mapping between them with limited training samples, a new 3D face reconstruction method based on progressive cascade regression is proposed. Firstly, it learns the mapping between 2D and 3D facial landmarks to estimate the initial 3D facial landmarks with a coupled space learning method. Secondly, a deformed space is constructed with the difference between the estimated initial landmarks and the ground truth of training samples; and more accurate 3D facial landmarks are reconstructed by modifying the initial 3D ones with shape compensations which are calculated by minimizing an objective function. Finally, the realistic 3D faces are reconstructed by a method that is based on a simple sparse regulation and shape deformation. The results on BJUT 3D face database demonstrate the effectiveness of the proposed method. In addition, compared with some typical methods, the method can get better subjective and objective results, especially in details.}, 
keywords={face recognition;image reconstruction;learning (artificial intelligence);regression analysis;solid modelling;3D face reconstruction;progressive cascade regression;training samples;facial landmark mapping;coupled space learning;deformed space construction;sparse regulation;shape deformation;Three-dimensional displays;Face;Training;Two dimensional displays;Image reconstruction;Solid modeling;Shape}, 
doi={10.1109/CITS.2017.8035340}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8100068, 
author={W. {Peng} and Z. {Feng} and C. {Xu} and Y. {Su}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Parametric T-Spline Face Morphable Model for Detailed Fitting in Shape Subspace}, 
year={2017}, 
volume={}, 
number={}, 
pages={5515-5523}, 
abstract={Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration for the synthesis of 3D faces by assuming that faces are in a linear class. However, the human face is in a nonlinear manifold, and a new test are always not in the pre-learnt subspace accurately because of the disparity brought by ethnicity, age, gender, etc. In the paper, we propose a parametric T-spline morphable model (T-splineMM) for 3D face representation, which has great advantages of fitting data from an unknown source accurately. In the model, we describe a face by C^2 T-spline surface, and divide the face surface into several shape units (SUs), according to facial action coding system (FACS), on T-mesh instead of on the surface directly. A fitting algorithm is proposed to optimize coefficients of T-spline control point components along pre-learnt identity and expression subspaces, as well as to optimize the details in refinement progress. As any pre-learnt subspace is not complete to handle the variety and details of faces and expressions, it covers a limited span of morphing. SUs division and detail refinement make the model fitting the facial muscle deformation in a larger span of morphing subspace. We conduct experiments on face scan data, kinect data as well as the space-time data to test the performance of detail fitting, robustness to missing data and noise, and to demonstrate the effectiveness of our model. Convincing results are illustrated to demonstrate the effectiveness of our model compared with the popular methods.}, 
keywords={face recognition;feature extraction;image morphing;image reconstruction;mesh generation;muscle;solid modelling;splines (mathematics);shape units;facial action coding system;fitting algorithm;T-spline control point components;expression subspaces;face scan data;parametric T-spline face morphable model;shape subspace pre-learnt subspace;human face;parametric T-spline morphable model for 3D;T-spline surface;face surface;kinect data;space-time data;Face;Three-dimensional displays;Shape;Solid modeling;Lips;Splines (mathematics);Computational modeling}, 
doi={10.1109/CVPR.2017.585}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{8688162, 
author={F. {Cao} and F. {Yan} and Y. {Gu} and C. {Ding} and Y. {Zhuang} and W. {Wang}}, 
booktitle={2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)}, 
title={A Novel Image Model of Point Clouds and its Application in Place Recognition}, 
year={2018}, 
volume={}, 
number={}, 
pages={79-83}, 
abstract={In this paper, we present a novel panoramic image model for scattered point clouds, and apply it to the problem of place recognition. We project a point cloud onto a sphere, and then the sphere is divided into a set of individual grids by longitudes and latitudes. Each grid is regard as a pixel and its value is computed using the geometrical relationship among the points in the grid and its neighbors. For convenience, the sphere is transferred into a flat. Since point clouds are converted to 2D images, we use ORB features and bag of words technique to solve place recognition problem. Our experimental results show that our image model is a more universal one and achieve a good performance in place recognition in both accuracy and efficiency.}, 
keywords={computational geometry;image recognition;scattered point clouds;place recognition;panoramic image model}, 
doi={10.1109/CYBER.2018.8688162}, 
ISSN={2379-7711}, 
month={July},}
@INPROCEEDINGS{7301378, 
author={P. {Polewski} and W. {Yao} and M. {Heurich} and P. {Krzystek} and U. {Stilla}}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
title={Active learning approach to detecting standing dead trees from ALS point clouds combined with aerial infrared imagery}, 
year={2015}, 
volume={}, 
number={}, 
pages={10-18}, 
abstract={Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89% with less than 100 training examples, which corresponds to 10% of the pre-selected data pool.}, 
keywords={environmental factors;feature extraction;forestry;image classification;image colour analysis;infrared imaging;learning (artificial intelligence);matrix algebra;object detection;vegetation;active learning approach;standing dead tree detection;ALS point cloud;forest process;forest science;environmental science;aerial color infrared imagery;feature extraction;greedy approximation;kernelized feature matrix;error reduction;semisupervised classifier;Bavarian Forest National Park;Vegetation;Three-dimensional displays;Entropy;Training;Image segmentation;Logistics;Feature extraction}, 
doi={10.1109/CVPRW.2015.7301378}, 
ISSN={2160-7516}, 
month={June},}
@INPROCEEDINGS{7410401, 
author={A. {Sironi} and V. {Lepetit} and P. {Fua}}, 
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={Projection onto the Manifold of Elongated Structures for Accurate Extraction}, 
year={2015}, 
volume={}, 
number={}, 
pages={316-324}, 
abstract={Detection of elongated structures in 2D images and 3D image stacks is a critical prerequisite in many applications and Machine Learning-based approaches have recently been shown to deliver superior performance. However, these methods essentially classify individual locations and do not explicitly model the strong relationship that exists between neighboring ones. As a result, isolated erroneous responses, discontinuities, and topological errors are present in the resulting score maps. We solve this problem by projecting patches of the score map to their nearest neighbors in a set of ground truth training patches. Our algorithm induces global spatial consistency on the classifier score map and returns results that are provably geometrically consistent. We apply our algorithm to challenging datasets in four different domains and show that it compares favorably to state-of-the-art methods.}, 
keywords={feature extraction;image classification;learning (artificial intelligence);object detection;structural engineering computing;classifier score map;global spatial consistency;ground truth training patch;patch projection;score map;topological error;discontinuities;isolated erroneous response;individual location classification;machine learning-based approach;3D image stack;2D image;elongated structure detection;extraction;elongated structure manifold;Three-dimensional displays;Training;Manifolds;Biomembranes;Image segmentation;Feature extraction;Transforms}, 
doi={10.1109/ICCV.2015.44}, 
ISSN={2380-7504}, 
month={Dec},}
@INPROCEEDINGS{7507339, 
author={I. {Virag} and L. {Stoicu-Tivadar} and M. {Crişan-Vida}}, 
booktitle={2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI)}, 
title={Gesture-based interaction in medical interfaces}, 
year={2016}, 
volume={}, 
number={}, 
pages={519-523}, 
abstract={The latest generation of medical visualizations systems that provide gesture based interaction usually rely on closed source software modules. This paper presents a novel approach since the interaction with the rendered 3D images is done via a Web browser. The entire system is based on open source software components and this way eliminates the requirement to have a specific operating system preinstalled. Our team used a Leap Motion controller that allows the rotation, panning, scaling and selection of individual slices of a reconstructed 3D model based on a prior CT (Computed Tomography) or MRI (Magnetic Resonance Imaging) scan of a patient. The results showed that is feasible to build such a system and that the interaction with the model can be done in real-time. It was concluded that this Web oriented architecture could provide a sustainable alternative for interacting with medical images.}, 
keywords={biomedical MRI;computerised tomography;gesture recognition;image reconstruction;medical image processing;online front-ends;gesture-based interaction;medical interfaces;medical visualization system;closed source software modules;rendered 3D images;Web browser;open source software components;operating system;leap-motion controller;slice selection;slice rotation;slice panning;slice scaling;reconstructed 3D model;CT;computed tomography;MRI scan;magnetic resonance imaging;Web oriented architecture;Three-dimensional displays;Biomedical imaging;Solid modeling;Image reconstruction;Thumb;Informatics}, 
doi={10.1109/SACI.2016.7507339}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7550083, 
author={W. P. {Koppen} and W. J. {Christmas} and D. J. M. {Crouch} and W. F. {Bodmer} and J. V. {Kittler}}, 
booktitle={2016 International Conference on Biometrics (ICB)}, 
title={Extending non-negative matrix factorisation to 3D registered data}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The use of non-negative matrix factorisation (NMF) on 2D face images has been shown to result in sparse feature vectors that encode for local patches on the face, and thus provides a statistically justified approach to learning parts from wholes. However successful on 2D images, the method has so far not been extended to 3D images. The main reason for this is that 3D space is a continuum and so it is not apparent how to represent 3D coordinates in a non-negative fashion. This work compares different non-negative representations for spatial coordinates, and demonstrates that not all non-negative representations are suitable. We analyse the representational properties that make NMF a successful method to learn sparse 3D facial features. Using our proposed representation, the factorisation results in sparse and interpretable facial features.}, 
keywords={face recognition;feature extraction;image registration;image representation;learning (artificial intelligence);matrix decomposition;nonnegative matrix factorization;NMF;3D face image registration;representational property;3D facial feature learning;Face;Three-dimensional displays;Shape;Principal component analysis;Two dimensional displays;Facial features;Encoding}, 
doi={10.1109/ICB.2016.7550083}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8123040, 
author={N. {Mohsin} and S. {Payandeh}}, 
booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={Localization and identification of body extremities based on data from multiple depth sensors}, 
year={2017}, 
volume={}, 
number={}, 
pages={2736-2741}, 
abstract={This paper explores the novel use of multiple depth sensors to overcome occlusions and improve localization and tracking of body extremities. The usage of data from only depth sensors not only overcomes visual challenges associated with RGB sensors under low illumination, but also protects the identity of surveyed person with high confidentiality. For integrating depth information from multiple sources, the paper presents first an overview of a novel calibration method for multiple depth sensors. In case of occlusion of any fiducial point in the primary sensor's depth image, co-ordinates of the point can be obtained from the frame of other sensors using the calibration parameters. To localize salient body parts such as hands, head and feet, a surface triangular mesh is applied on generated 3D point cloud from the primary sensor. The geodesic extrema from the mesh coincide with body extremities. The body extremities can be identified based on those relative geodesic distances between the extremities. Once the body parts are labelled, a portion of body can be targeted and evaluated for specific gait analysis and visualization. For the performance evaluation, our calibration method has fared well in comparison to other available techniques. Also, our proposed localization of salient body parts is able to successfully tag the specific body part i.e. the head region.}, 
keywords={calibration;data visualisation;feature extraction;gait analysis;image motion analysis;image sensors;mesh generation;object detection;sensor fusion;RGB sensors;salient body parts;depth information;body extremities localization;body extremities identification;multiple depth sensor data;body extremities tracking;illumination;identity confidentiality;specific body part tagging;fiducial point occlusion;primary sensor depth image;3D point cloud;mesh geodesic extrema;relative geodesic distances;gait analysis;visualization;Sensors;Three-dimensional displays;Calibration;Extremities;Image sensors;Image segmentation;Cameras;multiple depth sensors calibration;body extremities localization;Kinect II;geodesic distances}, 
doi={10.1109/SMC.2017.8123040}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7533187, 
author={L. {Ding} and A. {Elliethy} and E. {Freedenberg} and S. A. {Wolf-Johnson} and J. {Romphf} and P. {Christensen} and G. {Sharma}}, 
booktitle={2016 IEEE International Conference on Image Processing (ICIP)}, 
title={Comparative analysis of homologous buildings using range imaging}, 
year={2016}, 
volume={}, 
number={}, 
pages={4378-4382}, 
abstract={This paper reports on a novel application of computer vision and image processing technologies to an interdisciplinary project in architectural history that seeks to help identify and visualize differences between homologous buildings constructed to a common template design. By identifying the mutations in homologous buildings, we assist humanists in giving voice to the contributions of the myriad additional “authors” for these buildings beyond their primary designers. We develop a framework for comparing 3D point cloud representations of homologous buildings captured using lidar: focusing on identifying similarities and differences, both among 3D scans of different buildings and between the 3D scans and the design specifications of architectural drawings. The framework addresses global and local alignment for highlighting gross differences as well as differences in individual structural elements and provides methods for readily highlighting the differences via suitable visualizations. The framework is demonstrated on pairs of homologous buildings selected from the Canadian and Ottoman rail networks. Results demonstrate the utility of the framework confirming differences already apparent to the humanist researchers and also revealing new differences that were not previously observed.}, 
keywords={buildings (structures);design engineering;optical radar;radar imaging;structural engineering computing;Ottoman rail networks;Canadian rail networks;structural elements;local alignment;global alignment;architectural drawings;design specifications;3D scans;lidar;3D point cloud representations;mutation identification;range imaging;homologous buildings;Laser radar;Heating;Three-dimensional displays;architectural biometrics;building difference visualization;point cloud comparison;range imaging;visual big data analytics}, 
doi={10.1109/ICIP.2016.7533187}, 
ISSN={2381-8549}, 
month={Sep.},}
@INPROCEEDINGS{7495382, 
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and H. A. {Safar} and N. {Werghi}}, 
booktitle={2016 18th Mediterranean Electrotechnical Conference (MELECON)}, 
title={Landmarks detection on 3D face scans using local histogram descriptors}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In this work, we exploit 3D Constrained Local Model (CLM) for facial landmark detection. Our approach integrates the geometric information of 3D face scans. The fast increase demand of 3D data invite to develop 3D image processing methods for many applications and especially for automatic landmark detection. The new step in this paper is the introduction of mesh histogram of gradients (meshHOG) as local descriptors around every landmark location. The proposed work is evaluated on the publicly available Bosphorus database. A comparison with the other descriptors mesh LBP and mesh SIFT are also depicted.}, 
keywords={computational geometry;face recognition;mesh generation;object detection;facial landmark detection;3D face scans;local histogram descriptors;3D constrained local model;geometric information;3D image processing methods;mesh histogram-of-gradients;meshHOG;local descriptors;publicly available Bosphorus database;Three-dimensional displays;Face;Histograms;Solid modeling;Shape;Deformable models;Mathematical model}, 
doi={10.1109/MELCON.2016.7495382}, 
ISSN={2158-8481}, 
month={April},}
@INPROCEEDINGS{7785124, 
author={F. {Maninchedda} and C. {Häne} and M. R. {Oswald} and M. {Pollefeys}}, 
booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
title={Face Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization}, 
year={2016}, 
volume={}, 
number={}, 
pages={489-498}, 
abstract={We present a system which is able to reconstruct human faces on mobile devices with only on-device processing using the sensors which are typically built into a current commodity smart phone. Such technology can for example be used for facial authentication purposes or as a fast preview for further post-processing. Our method uses recently proposed techniques which compute depth maps by passive multi-view stereo directly on the device. We propose an efficient method which recovers the geometry of the face from the typically noisy point cloud. First, we show that we can safely restrict the reconstruction to a 2.5D height map representation. Therefore we then propose a novel low dimensional height map shape model for faces which can be fitted to the input data efficiently even on a mobile phone. In order to be able to represent instance specific shape details, such as moles, we augment the reconstruction from the shape model with a distance map which can be regularized efficiently. We thoroughly evaluate our approach on synthetic and real data, thereby we use both high resolution depth data acquired using high quality multi-view stereo and depth data directly computed on mobile phones.}, 
keywords={face recognition;image reconstruction;image resolution;stereo image processing;face reconstruction;mobile devices;height map shape model;fast regularization;facial authentication;depth maps;passive multiview stereo;face geometry;noisy point cloud;low dimensional height map shape model;high resolution depth data;high quality multiview stereo;Face;Computational modeling;Cameras;Three-dimensional displays;Shape;Solid modeling;Image reconstruction}, 
doi={10.1109/3DV.2016.59}, 
ISSN={}, 
month={Oct},}
@ARTICLE{7029822, 
author={Y. {Li} and Y. {Wang} and B. {Wang} and L. {Sui}}, 
journal={IET Computer Vision}, 
title={Nose tip detection on three-dimensional faces using pose-invariant differential surface features}, 
year={2015}, 
volume={9}, 
number={1}, 
pages={75-84}, 
abstract={Three-dimensional (3D) facial data offer the potential to overcome the difficulties caused by the variation of head pose and illumination in 2D face recognition. In 3D face recognition, localisation of nose tip is essential to face normalisation, face registration and pose correction etc. Most of the existing methods of nose tip detection on 3D face deal mainly with frontal or near-frontal poses or are rotation sensitive. Many of them are training-based or model-based. In this study, a novel method of nose tip detection is proposed. Using pose-invariant differential surface features - high-order and low-order curvatures, it can detect nose tip on 3D faces under various poses automatically and accurately. Moreover, it does not require training and does not depend on any particular model. Experimental results on GavabDB verify the robustness and accuracy of the proposed method.}, 
keywords={face recognition;feature extraction;image registration;object detection;pose estimation;nose tip detection;pose invariant differential surface feature;3D face recognition;head pose variation;illumination;2D face recognition;nose tip localisation;face normalisation;face registration;pose correction;nearfrontal pose;low order curvature;high order curvature}, 
doi={10.1049/iet-cvi.2014.0070}, 
ISSN={1751-9632}, 
month={},}
@INPROCEEDINGS{7780547, 
author={T. {Hackel} and J. D. {Wegner} and K. {Schindler}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Contour Detection in Unstructured 3D Point Clouds}, 
year={2016}, 
volume={}, 
number={}, 
pages={1610-1618}, 
abstract={We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds &gt; 107 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.}, 
keywords={edge detection;feature extraction;graph theory;image classification;image coding;image representation;learning (artificial intelligence);object detection;contour detection;unstructured 3D point clouds;surface orientation;large-scale outdoor point clouds;high-quality surface;solid models;unstructured-inhomogeneous point clouds;contour extraction;two-stage discriminative learning problem;contour score;binary classifier;feature extraction;contour scores;higher-order MRF;graph cliques;Canny-style edge detection;image representation;Three-dimensional displays;Feature extraction;Solid modeling;Image edge detection;Surface reconstruction;Surface topography;Solids}, 
doi={10.1109/CVPR.2016.178}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{7587405, 
author={Y. {Wang} and X. X. {Zhu} and B. {Zeisl} and M. {Pollefeys}}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={Fusing Meter-Resolution 4-D InSAR Point Clouds and Optical Images for Semantic Urban Infrastructure Monitoring}, 
year={2017}, 
volume={55}, 
number={1}, 
pages={14-26}, 
abstract={Using synthetic aperture radar (SAR) interferometry to monitor long-term millimeter-level deformation of urban infrastructures, such as individual buildings and bridges, is an emerging and important field in remote sensing. In the state-of-the-art methods, deformation parameters are retrieved and monitored on a pixel basis solely in the SAR image domain. However, the inevitable side-looking imaging geometry of SAR results in undesired occlusion and layover in urban area, rendering the current method less competent for a semantic-level monitoring of different urban infrastructures. This paper presents a framework of a semantic-level deformation monitoring by linking the precise deformation estimates of SAR interferometry and the semantic classification labels of optical images via a 3-D geometric fusion and semantic texturing. The proposed approach provides the first “SARptical” point cloud of an urban area, which is the SAR tomography point cloud textured with attributes from optical images. This opens a new perspective of InSAR deformation monitoring. Interesting examples on bridge and railway monitoring are demonstrated.}, 
keywords={optical images;radar interferometry;synthetic aperture radar;InSAR deformation monitoring;SAR tomography point cloud;semantic texturing;3-D geometric fusion;semantic-level deformation monitoring;side-looking imaging geometry;SAR image domain;remote sensing;long-term millimeter-level deformation;SAR interferometry;synthetic aperture radar interferometry;semantic urban infrastructure monitoring;optical images;meter-resolution 4-D InSAR point clouds;Optical imaging;Optical scattering;Synthetic aperture radar;Three-dimensional displays;Optical sensors;Adaptive optics;Optical interferometry;Bridge monitoring;interferometric synthetic aperture radar (InSAR);optical InSAR fusion;railway monitoring;SAR;semantic classification}, 
doi={10.1109/TGRS.2016.2554563}, 
ISSN={0196-2892}, 
month={Jan},}
@INPROCEEDINGS{7298920, 
author={ and R. {Ji} and }, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery}, 
year={2015}, 
volume={}, 
number={}, 
pages={3013-3021}, 
abstract={Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.}, 
keywords={Boltzmann machines;feature extraction;image colour analysis;image matching;learning (artificial intelligence);object detection;3D object detection;bimodal deep Boltzmann machines;RGBD imagery;3D scenes;point clouds;3D training data labeling;cross-modality cues;cross-modality deep learning framework;3D scene object detection;cross-modality feature learning;3D detection window;3D point cloud;exemplar shape matching;3D CAD models;2D positive samples;R-CNNs;raw feature extraction;RGB domains;depth domains;RMRC dataset;bimodal based deep feature learning framework;Three-dimensional displays;Solid modeling;Training;Frequency modulation;Joints;Detectors;Feature extraction}, 
doi={10.1109/CVPR.2015.7298920}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7327791, 
author={J. B. {Sleiman} and J. B. {Perraud} and B. {Bousquet} and N. {Palka} and J. P. {Guillet} and P. {Mounaix}}, 
booktitle={2015 40th International Conference on Infrared, Millimeter, and Terahertz waves (IRMMW-THz)}, 
title={Chemical imaging and quantification of RDX/PETN mixtures by PLS applied on terahertz time-domain spectroscopy}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Chemometric analysis was applied on terahertz absorbance 3D images, in transmission. The goal is to automatically discriminate some explosives on images and quantify mixtures of RDX/PETN in the frequency range of 0.2-3 THz. Partial Least Square (PLS) was applied on THz absorbance multispectral images to quantify individual product inside pure samples and mixtures at each pixel on the image. Then the best score obtained is used to display the samples' images and provide the optimal frequencies combination for recognition purpose.}, 
keywords={chemical variables measurement;electromagnetic wave absorption;explosives;least mean squares methods;terahertz spectroscopy;terahertz wave imaging;terahertz wave spectra;chemometric analysis;terahertz absorbance 3D image;RDX/PETN mixture quantification;explosive;partial least square;PLS method;THz absorbance multispectral image;terahertz time-domain spectroscopy;chemical imaging;frequency 0.2 THz to 3 THz;Imaging;Explosives;Predictive models;Yttrium;Spectroscopy;Time-domain analysis}, 
doi={10.1109/IRMMW-THz.2015.7327791}, 
ISSN={2162-2035}, 
month={Aug},}
@INPROCEEDINGS{8453724, 
author={S. {Sihvo} and P. {Virjonen} and P. {Nevalainen} and J. {Heikkonen}}, 
booktitle={2018 Baltic Geodetic Congress (BGC Geomatics)}, 
title={Tree Detection around Forest Harvester Based on Onboard LiDAR Measurements}, 
year={2018}, 
volume={}, 
number={}, 
pages={364-367}, 
abstract={This paper proposes a new approach for the detection of tree locations around forest machines producing a situational model based on on-site terrestrial LiDAR data collected during harvesting operation. A triangularized ground model is used to planarize the point cloud in order to simplify the tree detection. The planarized ground makes the vertical cutting of the point cloud systematical. Tree stem lines detected from individual trees at individual scan views are used to guide the final alignment into global coordinates. The setup is numerically efficient and does not rely on any positioning and orientation system (POS) based e.g. on an inertial measurement unit (IMU) or global navigation satellite system (GNSS) or wheel rotation counter on the autonomous vehicle.}, 
keywords={optical radar;satellite navigation;tree detection;forest harvester;onboard LiDAR measurements;tree locations;forest machines;situational model;on-site terrestrial LiDAR data;triangularized ground model;planarized ground;tree stem lines;wheel rotation counter;Vegetation;Three-dimensional displays;Laser radar;Forestry;Semiconductor device modeling;Global navigation satellite system;Planarization;object recognition;collision avoidance;autonomous vehicles forestry}, 
doi={10.1109/BGC-Geomatics.2018.00075}, 
ISSN={}, 
month={June},}
@ARTICLE{7331662, 
author={T. v. {Landesberger} and D. {Basgier} and M. {Becker}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Comparative Local Quality Assessment of 3D Medical Image Segmentations with Focus on Statistical Shape Model-Based Algorithms}, 
year={2016}, 
volume={22}, 
number={12}, 
pages={2537-2549}, 
abstract={The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.}, 
keywords={data analysis;data visualisation;ear;image segmentation;liver;medical image processing;neurophysiology;pattern clustering;statistical analysis;comparative local quality assessment;3D medical image segmentation;statistical shape model-based algorithm;automatic 3D medical segmentation algorithm;data analysis;data visualization;segmentation quality characteristics;hierarchical clustering algorithm;connectivity criterion;landmark correspondence;liver;cochlea;facial nerve;organ region identification;outlier instance detection;Image segmentation;Three-dimensional displays;Biomedical monitoring;Visual analytics;Medical diagnostic imaging;Systematics;Statistical analysis;Clustering;Visual analytics;3D medical image segmentation quality;comparison;clustering;statistical shape models}, 
doi={10.1109/TVCG.2015.2501813}, 
ISSN={1077-2626}, 
month={Dec},}
@INPROCEEDINGS{7167461, 
author={D. {Aneja} and S. R. {Vora} and E. D. {Camci} and L. G. {Shapiro} and T. C. {Cox}}, 
booktitle={2015 IEEE 28th International Symposium on Computer-Based Medical Systems}, 
title={Automated Detection of 3D Landmarks for the Elimination of Non-Biological Variation in Geometric Morphometric Analyses}, 
year={2015}, 
volume={}, 
number={}, 
pages={78-83}, 
abstract={Landmark-based morphometric analyses are used by anthropologists, developmental and evolutionary biologists to understand shape and size differences (eg. in the cranioskeleton) between groups of specimens. The standard, laborintensive approach is for researchers to manually place landmarks on 3D image datasets. As landmark recognition is subject to inaccuracies of human perception, digitization of landmark coordinates is typically repeated (often by more than one person) and the mean coordinates are used. In an attempt to improve efficiency and reproducibility between researchers, we have developed an algorithm to locate landmarks on CT mouse hemi-mandible data. The method is evaluated on 3D meshes of 28-day old mice, and results compared to landmarks manually identified by experts. Quantitative shape comparison between two inbred mouse strains demonstrate that data obtained using our algorithm also has enhanced statistical power when compared to data obtained by manual landmarking.}, 
keywords={bone;computerised tomography;automated 3D landmark detection;nonbiological variation elimination;geometric morphometric analysis;cranioskeleton;3D image dataset;landmark recognition;human perception;landmark coordinate digitization;landmark location;CT mouse hemimandible data;computed tomography;quantitative shape comparison;3D mice meshes;inbred mouse strain;statistical power;Manuals;Shape;Mice;Surface treatment;Three-dimensional displays;Accuracy;3D landmarks;Automated landmark detection;Geometric morphometrics;Shape analysis;Mandible}, 
doi={10.1109/CBMS.2015.86}, 
ISSN={2372-9198}, 
month={June},}
@ARTICLE{7879309, 
author={H. {Zhang} and C. {Ye}}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={An Indoor Wayfinding System Based on Geometric Features Aided Graph SLAM for the Visually Impaired}, 
year={2017}, 
volume={25}, 
number={9}, 
pages={1592-1604}, 
abstract={This paper presents a 6-degree of freedom (DOF) pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired. The PE method involves two-graph simultaneous localization and mapping (SLAM) processes to reduce the accumulative pose error of the device. In the first step, the floor plane is extracted from the 3-D camera's point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch, and Zerrors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y, and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a visually impaired person in an indoor environment. The system uses the estimated pose and floor plan to locate the device user in a building and guides the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.}, 
keywords={biomedical engineering;geometry;indoor navigation;speech;vision defects;indoor wayfinding system;geometric features aided graph SLAM;visually impaired;pose estimation method;two-graph simultaneous localization;mapping processes;landmark node;yaw errors;computational time;speech interface;indoor navigation task;Simultaneous localization and mapping;RNA;Cameras;Three-dimensional displays;Navigation;Floors;Indoor environments;Blind navigation;wayfinding;robotic navigation aid;pose estimation;graph SLAM;3-D camera;Canes;Dependent Ambulation;Equipment Design;Equipment Failure Analysis;Humans;Imaging, Three-Dimensional;Patient Identification Systems;Reproducibility of Results;Self-Help Devices;Sensitivity and Specificity;Spatial Navigation;Treatment Outcome;User-Computer Interface;Visually Impaired Persons;Wireless Technology}, 
doi={10.1109/TNSRE.2017.2682265}, 
ISSN={1534-4320}, 
month={Sep.},}
@INPROCEEDINGS{8569716, 
author={T. {Akita} and Y. {Yamauchi} and H. {Fujiyoshi}}, 
booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)}, 
title={Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye Cameras}, 
year={2018}, 
volume={}, 
number={}, 
pages={1103-1108}, 
abstract={Recently, automated emergency brake systems for pedestrian have been commercialized. However, they cannot detect crossing pedestrians when turning at intersections because the field of view is not wide enough. Thus, we propose to utilize a surround view camera system becoming popular by making it into stereo vision which is robust for the pedestrian recognition. However, conventional stereo camera technologies cannot be applied due to fisheye cameras and uncalibrated camera poses. Thus we have created the new method to absorb difference of the pedestrian appearance between cameras by machine learning for the stereo vision. The method of stereo matching between image patches in each camera image was designed by combining D-Brief and NCC with SVM. Good generalization performance was achieved by it compared with individual conventional algorithms. Furthermore, feature amounts of the point cloud reconstructed by the stereo pairs are utilized with Random Forest to discriminate pedestrians. The algorithm was evaluated for the actual camera images of crossing pedestrians at various intersections, and 96.0% of pedestrian tracking rate with high position detection accuracy was achieved. They were compared with Faster R-CNN as the best pattern recognition technique, and our proposed method indicated better detection performance.}, 
keywords={cameras;image matching;image reconstruction;learning (artificial intelligence);object detection;object tracking;pedestrians;stereo image processing;support vector machines;traffic engineering computing;machine learning-based stereo vision algorithm;surround view fisheye cameras;automated emergency brake systems;crossing pedestrians;surround view camera system;pedestrian recognition;uncalibrated camera;pedestrian appearance;stereo matching;image patches;camera image;stereo pairs;pedestrian tracking rate;high position detection accuracy;detection performance;stereo camera technologies;D-Brief;SVM;random forest;Cameras;Turning;Distortion;Machine learning algorithms;Machine learning;Accidents;Feature extraction}, 
doi={10.1109/ITSC.2018.8569716}, 
ISSN={2153-0017}, 
month={Nov},}
@INPROCEEDINGS{7350744, 
author={}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={[Title page]}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={The following topics are dealt with: learning-based visual applications; machine learning and scene analysis; multimedia retrieval and indexing; object detection and tracking; pose and gesture recognition; saliency analysis; superpixel segmentation; visual biometrics; visualization and image rendering; pose estimation and human activity recognition; discriminative local features; edge and shape models; face recognition; image retrieval; image segmentation; action detection; facial features-gender-age recognition; feature detection and tracking; foreground-background separation; active contours and levelset methods; image classification; texture synthesis; video analysis; optical flow and motion estimation; video surveillance; visual content analysis; video retrieval; tomographic imaging; radar imaging; seismic and remote sensing imaging; image and video coding; video communication and networking; video streaming; 3D image processing; 3D modeling and reconstruction; multiview processing; document analysis; color imaging; 3D object and scene reconstruction; hardware, parallel, and distributed system; infrared, multispectral and hyperspectral imaging; stereo image processing; multimedia forensics; content and privacy protection; data hiding; robust hashing and counter-forensics; social and affective media; multidimensional processing; bio-inspired modeling; visual aesthetics and quality assessment; omnidirectional imaging and plenoptics; image representation; Big media Data processing; compressed sensing; and image denoising.}, 
keywords={Big Data;compressed sensing;data protection;data visualisation;document image processing;geophysical image processing;gesture recognition;hyperspectral imaging;image classification;image colour analysis;image denoising;image forensics;image representation;image segmentation;image sequences;image texture;indexing;learning (artificial intelligence);medical image processing;motion estimation;object detection;object tracking;parallel processing;pose estimation;radar imaging;remote sensing;rendering (computer graphics);seismology;social networking (online);stereo image processing;video coding;video communication;video retrieval;video surveillance;facial features-gender-age recognition;feature detection;feature tracking;foreground-background separation;active contours;levelset methods;image classification;texture synthesis;video analysis;optical flow;motion estimation;video surveillance;visual content analysis;video retrieval;tomographic imaging;radar imaging;remote sensing imaging;seismic sensing imaging;image coding;video coding;video communication;video networking;video streaming;3D image processing;3D modeling;3D reconstruction;multiview processing;document analysis;color imaging;scene reconstruction;3D object reconstruction;distributed system;parallel system;action recognition;action detection;image segmentation;image retrieval;face recognition;edge models;shape models;discriminative local features;human activity recognition;pose estimation;data visualization;image rendering;visual biometrics;superpixel segmentation;saliency analysis;pose recognition;gesture recognition;object tracking;object detection;multimedia indexing;multimedia retrieval;scene analysis;machine learning;learning-based visual applications;hardware system;hyperspectral imaging;multispectral imaging;infrared imaging;stereo image processing;multimedia forensics;privacy protection;content protection;data hiding;robust hashing;counter-forensics;social media;affective media;multidimensional processing;bio-inspired modeling;visual aesthetics;quality assessment;omnidirectional imaging;plenoptics;image representation;Big media Data processing;compressed sensing;image denoising}, 
doi={10.1109/ICIP.2015.7350744}, 
ISSN={}, 
month={Sep.},}

@ARTICLE{Shi2019455,
author={Shi, B. and Zang, H. and Zheng, R. and Zhan, S.},
title={An efficient 3D face recognition approach using Frenet feature of iso-geodesic curves},
journal={Journal of Visual Communication and Image Representation},
year={2019},
volume={59},
pages={455-460},
doi={10.1016/j.jvcir.2019.02.002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061129042&doi=10.1016%2fj.jvcir.2019.02.002&partnerID=40&md5=8f0fe3cd51ff2049c71bfe756a9bf6d6},
affiliation={School of Computer and Information, Hefei University of Technology, Hefei, 230009, China},
abstract={Extracting efficient features from the large volume of 3D facial data directly is extremely difficult in 3D face recognition (3D-FR) with the latest methods, which mostly require heavy computations and manual processing steps. This paper presents a computationally efficient 3D-FR system based on a novel Frenet frame-based feature that is derived from the 3D facial iso-geodesic curves. In terms of the evaluation of the proposed method, we conducted a number of experiments on the CASIA 3D face database, and a superior recognition performance has been achieved. The performance evaluation suggests that the pose invariance attribute of the features relieves the need of an expensive 3D face registration in the face preprocessing procedure, where we take less time to process conversely. Our experiments further demonstrate that the proposed method not only achieves competitive recognition performance when compared with some existing techniques for 3D-FR, but also is computationally efficient. © 2019 Elsevier Inc.},
author_keywords={3D face recognition;  Facial curves;  Frenet framework;  Iso-geodesic;  Pose invariant},
document_type={Article},
source={Scopus},
}

@ARTICLE{Neto2019594,
author={Neto, J.B.C. and Marana, A.N.},
title={3D face recognition with reconstructed faces from a collection of 2D images},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11401 LNCS},
pages={594-601},
doi={10.1007/978-3-030-13469-3_69},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063066921&doi=10.1007%2f978-3-030-13469-3_69&partnerID=40&md5=ae38ed3a54a47df05e22f1ec3db64720},
affiliation={São Carlos Federal University - UFSCAR, São Carlos, SP  13565-905, Brazil; UNESP - São Paulo State University, Bauru, SP  17033-360, Brazil},
abstract={Nowadays, there is an increasing need for systems that can accurately and quickly identify a person. Traditional identification methods utilize something a person knows or something a person has. This kind of methods has several drawbacks, being the main one the fact that it is impossible to detect an imposter who uses genuine credentials to pass as a genuine person. One way to solve these kinds of problems is to utilize biometric identification. The face is one of the biometric features that best suits the covert identification. However, in general, biometric systems based on 2D face recognition perform very poorly in unconstrained environments, common in covert identification scenarios, since the input images present variations in pose, illumination, and facial expressions. One way to mitigate this problem is to use 3D face data, but the current 3D scanners are expensive and require a lot of cooperation from people being identified. Therefore, in this work, we propose an approach based on local descriptors for 3D Face Recognition based on 3D face models reconstructed from collections of 2D images. Initial results show 95% in a subset of the LFW Face dataset. © Springer Nature Switzerland AG 2019.},
author_keywords={3D face recognition;  3DLBP;  Biometrics;  Face reconstruction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hajati2019936,
author={Hajati, F. and Cheraghian, A. and Ameri Sianaki, O. and Zeinali, B. and Gheisari, S.},
title={Polar Topographic Derivatives for 3D Face Recognition: Application to Internet of Things Security},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={927},
pages={936-945},
doi={10.1007/978-3-030-15035-8_92},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064875340&doi=10.1007%2f978-3-030-15035-8_92&partnerID=40&md5=a1997efd234042ace42c04b05607ab05},
affiliation={College of Engineering and Science, Victoria University Sydney, Sydney, Australia; College of Engineering and Computer Science, The Australian National University, Canberra, Australia; Electrical Engineering Department, Iran University of Science and Technology, Tehran, Iran; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, Australia},
abstract={We propose Polar Topographic Derivatives (PTD) to fuse the shape and texture information of a facial surface for 3D face recognition. Polar Average Absolute Deviations (PAADs) of the Gabor topography maps are extracted as features. High-order polar derivative patterns are obtained by encoding texture variations in a polar neighborhood. By using the and Bosphorus 3D face database, our method shows that it is robust to expression and pose variations comparing to existing state-of-the-art benchmark approaches. © 2019, Springer Nature Switzerland AG.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Peter201977,
author={Peter, M. and Minoi, J.-L. and Hipiny, I.H.M.},
title={3D face recognition using kernel-based PCA approach},
journal={Lecture Notes in Electrical Engineering},
year={2019},
volume={481},
pages={77-86},
doi={10.1007/978-981-13-2622-6_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053242189&doi=10.1007%2f978-981-13-2622-6_8&partnerID=40&md5=c4b90626b4453034107d227cbd63ef3d},
affiliation={Faculty of Computer Science and Information Technology, Universiti Malaysia SarawakSarawak, Malaysia},
abstract={Face recognition is commonly used for biometric security purposes in video surveillance and user authentications. The nature of face exhibits non-linear shapes due to appearance deformations, and face variations presented by facial expressions. Recognizing faces reliably across changes in facial expression has proved to be a more difficult problem leading to low recognition rates in many face recognition experiments. This is mainly due to the tens degree-of-freedom in a non-linear space. Recently, non-linear PCA has been revived as it posed a significant advantage for data representation in high dimensionality space. In this paper, we experimented the use of non-linear kernel approach in 3D face recognition and the results of the recognition rates have shown that the kernel method outperformed the standard PCA. © Springer Nature Singapore Pte Ltd. 2019.},
author_keywords={3D face;  Facial recognition;  Kernel PCA},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dutta2019175,
author={Dutta, K. and Bhattacharjee, D. and Nasipuri, M. and Poddar, A.},
title={3D face recognition based on volumetric representation of range image},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={883},
pages={175-189},
doi={10.1007/978-981-13-3702-4_11},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061157145&doi=10.1007%2f978-981-13-3702-4_11&partnerID=40&md5=2543d63ceec9074d39578ea3320cdfbd},
affiliation={Department of Computer Science and Engineering, Jadavpur University, Kolkata, West Bengal, India; Computer Science Engineering, Birla Institute of Technology, Mesra, Ranchi, India},
abstract={In this paper, a 3D face recognition system has been developed based on the volumetric representation of 3D range image. The main approach to build this system is to calculate volume on some distinct region of 3D range face data. The system has mainly three steps. In the very first step, seven significant facial landmarks are identified on the face. Secondly, six distinct triangular regions A to F are created on the face using any three individual landmarks where nose tip is common to all regions. Further 3D volumes of all the respective triangular regions have been calculated based on plane fitting on the input range images. Finally, KNN and SVM classifiers are considered for classification. Initially, the classification and recognition are carried out on the different volumetric region, and a further combination of all the regions is considered. The proposed approach is tested on three useful challenging databases, namely Frav3D, Bosphorous, and GavabDB. © Springer Nature Singapore Pte Ltd. 2019.},
author_keywords={3D range image;  Classification;  Facial landmark;  Plane fitting;  Volumetric representation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{ZulqarnainGilani20181896,
author={Zulqarnain Gilani, S. and Mian, A.},
title={Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2018},
pages={1896-1905},
doi={10.1109/CVPR.2018.00203},
art_number={8578301},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060207182&doi=10.1109%2fCVPR.2018.00203&partnerID=40&md5=7a2227c884147feae86f1d25240e9a7a},
affiliation={Computer Science and Software Engineering, University of Western Australia, Australia},
abstract={Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reji2018,
author={Reji, R. and Sojanlal, P.},
title={Region Based 3D Face Recognition},
journal={2017 IEEE International Conference on Computational Intelligence and Computing Research, ICCIC 2017},
year={2018},
doi={10.1109/ICCIC.2017.8524581},
art_number={8524581},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057946157&doi=10.1109%2fICCIC.2017.8524581&partnerID=40&md5=92c092fa4e2838ab444fcc0356fcdb75},
affiliation={Mahatma Gandhi University, School of Computer Sciences, Kottayam, Kerala, India; Mar-Baselious Institute of Technology Science, Kothamangalam, Kerala, India},
abstract={This paper focuses on a region based methodology for expression in sensitive 3D face recognition process. Considering facial regions that are comparatively unchanging during expressions, results shows that using fifteen sub regions on the face can attain high 3D face recognition. We use a modified face recognition algorithm along with hierarchical contour based image registration for finding the similarity score. Our method operates in two modes: verification mode and confirmation mode. Crop 100 mm of frontal face region, apply preprocessing and automatically detect nose tip, translate the face image to origin and crop fifteen sub regions. The cropped sub regions are defined by cuboids which occupy more volumetric data, Nose Tip is the most projecting point of the face with the highest value along Z-axis so consider it as origin. The modified face recognition algorithm reduces the effects caused by facial expressions and artifacts. Finally a Hierarchical contour based image registration technique is applied which yields better results. The approach is applied on Bosphorus 3D datasets and achieved a verification rate of 95.3% at 0.1% false acceptance rate. In the identification scenario 99.3% rank one recognition is achieved. © 2017 IEEE.},
author_keywords={3D face recognition;  Biometrics;  Contour based image registration;  MFRA;  Rank based Score},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Siqueira20183513,
author={Siqueira, R.S. and Alexandre, G.R. and Soares, J.M. and The, G.A.P.},
title={Triaxial slicing for 3-D face recognition from adapted rotational invariants spatial moments and minimal keypoints dependence},
journal={IEEE Robotics and Automation Letters},
year={2018},
volume={3},
number={4},
pages={3513-3520},
doi={10.1109/LRA.2018.2854295},
art_number={8408720},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063307357&doi=10.1109%2fLRA.2018.2854295&partnerID=40&md5=5205595c202573ef77808eaf4aee1626},
affiliation={Faculty of Electrical Engineering, Instituto Federal de Educação, Ciência e Tecnologia Do Ceará, Fortaleza, CE 60410-42, Brazil; Department of Teleinformatics Engineering, Universidade Federal Do Ceará, Fortaleza, CE 60020-181, Brazil},
abstract={This letter presents a multiple slicing model for three-dimensional (3-D) images of human face, using the frontal, sagittal, and transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2-D spatial moments of Hu and 3-D spatial invariant rotation moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs nonneutral ROC I experiment, applying linear discriminant analysis as classifier and more than one sample for training, achieved 98.7% of verification rate at 0.1% of false acceptance rate. By using the support vector machine as classifier the rank1 experiment recognition rates of 99% and 95.4% have been achieved for a neutral vs neutral and for a neutral vs non-neutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when anger and disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity. © 2016 IEEE.},
author_keywords={Computer vision for automation;  recognition;  surveillance systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fei2018139,
author={Fei, H. and Tu, B. and Chen, Q. and He, D. and Zhou, C. and Peng, Y.},
title={An overview of face-related technologies},
journal={Journal of Visual Communication and Image Representation},
year={2018},
volume={56},
pages={139-143},
doi={10.1016/j.jvcir.2018.09.012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053774931&doi=10.1016%2fj.jvcir.2018.09.012&partnerID=40&md5=ddfc4618812dd5bf104f59915ffc5af0},
affiliation={College of Information and Communication Engineering, School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Computer and Information, Hefei University of Technology, Hefei, China},
abstract={In recent years, information technology is developing continuously and set off a burst of artificial intelligence boom in the field of science. The development of advanced technologies such as unmanned driving and AI chips, is the extensive application of artificial intelligence. Face-related technologies have a wide range of applications because of intuitive results and good concealment. Since 3D face information can provide more comprehensive facial information than 2D face information, and it can solve many difficulties that cannot be solved in 2D face recognition. Therefore, more and more researchers have studied 3D face recognition in recent years. Under the new circumstances, the research on face are experiencing all kinds of challenges. With the tireless of many scientists, the new technology is also making a constant progress, and in the development of many technologies it still maintained its leading position. In this paper, we simply sort out the present development process of facial correlation technology, and the general evolution of this technology is outlined. Finally, the practical significance of this technology development is briefly discussed. © 2018},
author_keywords={3D face reconstruction;  Deep learning;  Face enhancement;  Face recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abbad2018525,
author={Abbad, A. and Abbad, K. and Tairi, H.},
title={3D face recognition: Multi-scale strategy based on geometric and local descriptors},
journal={Computers and Electrical Engineering},
year={2018},
volume={70},
pages={525-537},
doi={10.1016/j.compeleceng.2017.08.017},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028606046&doi=10.1016%2fj.compeleceng.2017.08.017&partnerID=40&md5=5414b01946f345f018c49654dff50957},
affiliation={LIIAN, Department of Computer Science, Faculty of Sciences Dhar El Mahraz University Sidi Mohamed Ben Abdelah, Fez, Morocco; LISA, Department of Computer Science, Faculty of Science and Technology University Sidi Mohamed Ben Abdelah, Fez, Morocco},
abstract={Most human expression variations cause a non-rigid deformation of face scans, which is a challenge today. In this article, we present a novel framework for 3D face recognition that uses a geometry and local shape descriptor in a matching process to overcome the distortions caused by expressions in faces. This algorithm consists of four major components. First, the 3D face model is presented at different scales. Second, isometric-invariant features on each scale are extracted. Third, the geometric information is obtained on the 3D surface in terms of radial and level facial curves. Fourth, the feature vectors on each scale are concatenated with their corresponding geometric information. We conducted a number of experiments using two well-known and challenging datasets, namely, the GavabDB and Bosphorus datasets, and superior recognition performance has been achieved. The new system displays an overall rank-1 identification rate of 98.9% for all faces with neutral and non-neutral expressions on the GavabDB database. © 2017 Elsevier Ltd},
author_keywords={3D face recognition;  Expression;  Facial curves;  Geometric features;  Local features},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao2018120,
author={Gao, J. and Evans, A.N.},
title={Expression robust 3D face landmarking using thresholded surface normals},
journal={Pattern Recognition},
year={2018},
volume={78},
pages={120-132},
doi={10.1016/j.patcog.2018.01.011},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042375361&doi=10.1016%2fj.patcog.2018.01.011&partnerID=40&md5=a93105edc5d4f49296e9c43d397e9898},
affiliation={Department of Medical Biochemistry and Microbiology, Uppsala University, Uppsala, Sweden; Department of Electronic and Electrical Engineering, University of Bath, Bath, United Kingdom},
abstract={3D face recognition is an increasing popular modality for biometric authentication, for example in the iPhoneX. Landmarking plays a significant role in region based face recognition algorithms. The accuracy and consistency of the landmarking will directly determine the effectiveness of feature extraction and hence the overall recognition performance. While surface normals have been shown to provide high performing features for face recognition, their use in landmarking has not been widely explored. To this end, a new 3D facial landmarking algorithm based on thresholded surface normal maps is proposed, which is applicable to widely used 3D face databases. The benefits of employing surface normals are demonstrated for both facial roll and yaw rotation calibration and nasal landmarks localization. Results on the Bosphorus, FRGC and BU-3DFE databases show that the detected landmarks possess high within-class consistency and accuracy under different expressions. For several key landmarks the performance achieved surpasses that of state-of-the-art techniques and is also training free and computationally efficient. The use of surface normals therefore provides a useful representation of the 3D surface and the proposed landmarking algorithm provides an effective approach to localising the key nasal landmarks. © 2018 Elsevier Ltd},
author_keywords={3D face landmarking;  Surface normals},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ahdid201873,
author={Ahdid, R. and Taifi, K. and Said, S. and Fakir, M. and Manaut, B.},
title={Automatic face recognition system using iso-geodesic curves in riemanian manifold},
journal={Proceedings - 2017 14th International Conference on Computer Graphics, Imaging and Visualization, CGiV 2017},
year={2018},
pages={73-78},
doi={10.1109/CGiV.2017.25},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048323921&doi=10.1109%2fCGiV.2017.25&partnerID=40&md5=73b9e65d367cee00cdcb28e7b5ef55cb},
affiliation={Sultan Moulay Slimane University, Beni Mellal, Morocco},
abstract={In this paper, we present an automatic 3D face recognition system. This system is based on the representation of human faces surfaces as collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two facial surfaces, we compute a geodesic distance between a pair of facial curves using a Riemannian geometry. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008). © 2017 IEEE.},
author_keywords={3D face recognition;  facial surfaces;  geodesic distance;  iso-geodesic curves;  Riemannian geometry},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NourbakhshKaashki201866,
author={Nourbakhsh Kaashki, N. and Safabakhsh, R.},
title={RGB-D face recognition under various conditions via 3D constrained local model},
journal={Journal of Visual Communication and Image Representation},
year={2018},
volume={52},
pages={66-85},
doi={10.1016/j.jvcir.2018.02.003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042322207&doi=10.1016%2fj.jvcir.2018.02.003&partnerID=40&md5=2b91afeff8a19c987036cf8a30b610dc},
affiliation={Department of Computer Engineering, Amirkabir University of Technology, Tehran, Iran},
abstract={This research proposes a method for 3D face recognition in various conditions using 3D constrained local model (CLM-Z). In this method, a combination of 2D images (RGBs) and depth images (Ds) captured by Kinect has been used. After detecting the face and smoothing the depth image, CLM-Z model has been used to model and detect the important points of the face. These points are described using Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), and 3D Local Binary Patterns (3DLBP). Finally, each face is recognized by a Support Vector Machine (SVM). The challenging situations are changes of lighting, facial expression and head pose. The results on CurtinFaces and IIIT-D datasets demonstrate that the proposed method outperformed state-of-the-art methods under illumination, expression and pitch pose conditions and comparable results were obtained in other cases. Additionally, our proposed method is robust even when the training data has not been carefully collected. © 2018 Elsevier Inc.},
author_keywords={3D constrained local model;  3D face recognition;  Depth image;  Face model;  Facial expression;  Feature descriptor;  Head pose;  Kinect;  Lighting},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Abbad20181,
author={Abbad, A. and Abbad, K. and Tairi, H.},
title={3D face recognition in the presence of facial expressions based on empirical mode decomposition},
journal={ACM International Conference Proceeding Series},
year={2018},
volume={2018-March},
pages={1-6},
doi={10.1145/3177148.3180087},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047112949&doi=10.1145%2f3177148.3180087&partnerID=40&md5=93db67c347fc7dadc31256f7eb69e983},
affiliation={Laboratory LIIAN, Faculty of Science, Dhar EL Mahraz, Morocco; Laboratory ISA, Faculty of Science and Technology, Morocco},
abstract={This paper presents an efficient 3D face recognition method to handle facial expression. The proposed method uses the Surfaces Empirical Mode Decomposition (SEMD), facial curves and local shape descriptor in a matching process to overcome the distortions caused by expressions in faces. The basic idea is that, the face is presented at different scales by SEMD. Then the isometric-invariant features on each scale are extracted. After that, the geometric information is obtained on the 3D surface in terms of radial and level facial curves. Finally, the feature vectors on each scale are associated with their corresponding geometric information. The presented method is validated on GavabDB database resulting a rank 1 recognition rate (RR) of 98.9% for all faces with neutral and non-neutral expressions. This result outperforms other 3D expression-invariant face recognition methods on the same database. © 2018 Association for Computing Machinery.},
author_keywords={3D face recognition;  EMD;  Expression;  Facial curves;  Geometric features;  Local features},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Guo2018,
author={Guo, Y. and Wei, R. and Liu, Y.},
title={Weighted gradient feature extraction based on multiscale sub-blocks for 3d facial recognition in bimodal images},
journal={Information (Switzerland)},
year={2018},
volume={9},
number={3},
doi={10.3390/info9030048},
art_number={48},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042711391&doi=10.3390%2finfo9030048&partnerID=40&md5=fe75121af64143005ff6ec84435ed59d},
affiliation={School of Computer Science and Engineering, Hebei University of Technology, Tianjin, 300400, China},
abstract={In this paper, we propose a bimodal 3D facial recognition method aimed at increasing the recognition rate and reducing the effect of illumination, pose, expression, ages, and occlusion on facial recognition. There are two features extracted from the multiscale sub-blocks in both the 3D mode depth map and 2D mode intensity map, which are the local gradient pattern (LGP) feature and the weighted histogram of gradient orientation (WHGO) feature. LGP and WHGO features are cascaded to form the 3D facial feature vector LGP-WHGO, and are further trained and identified by the support vector machine (SVM). Experiments on the CASIA database, FRGC v2.0 database, and Bosphorus database show that, the proposed method can efficiently extract the structure information and texture information of the facial image, and have a robustness to illumination, expression, occlusion and pose. © 2018 by the authors.},
author_keywords={3D face recognition;  Bimodal;  Depth map;  Intensitymap;  LGP-WHGO;  Multiscale sub-blocks},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Soltanpour20182811,
author={Soltanpour, S. and Wu, Q.M.J.},
title={High-order local normal derivative pattern (LNDP) for 3D face recognition},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2018},
volume={2017-September},
pages={2811-2815},
doi={10.1109/ICIP.2017.8296795},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045292850&doi=10.1109%2fICIP.2017.8296795&partnerID=40&md5=12c33512c432220af4c4c49f5116433e},
affiliation={University of Windsor, Department of Electrical and Computer Engineering, Windsor, Canada},
abstract={This paper proposes a novel descriptor based on the local derivative pattern (LDP) for 3D face recognition. Compared to the local binary pattern (LBP), LDP can capture more detailed information by encoding directional pattern features. It is based on the local derivative variations that extract high-order local information. We propose a novel discriminative facial shape descriptor, local normal derivative pattern (LNDP) that extracts LDP from the surface normal. Using surface normal, the orientation of a surface at each point is determined as a first-order surface differential. Three normal component images are extracted by estimating three components of normal vectors in x, y, and z channels. Each normal component is divided into several patches and encoded using LDP. The final descriptor is created by concatenating histograms of the LNDP on each patch. Experimental results on two famous 3D face databases, FRGC v2.0 and Bosphorus illustrate the effectiveness of the proposed descriptor. © 2017 IEEE.},
author_keywords={3D face;  High-order local pattern;  Local derivative pattern;  Surface normal},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li20181295,
author={Li, Y. and Wang, Y. and Liu, J. and Hao, W.},
title={Expression-insensitive 3D face recognition by the fusion of multiple subject-specific curves},
journal={Neurocomputing},
year={2018},
volume={275},
pages={1295-1307},
doi={10.1016/j.neucom.2017.09.070},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040674183&doi=10.1016%2fj.neucom.2017.09.070&partnerID=40&md5=ecfea39a1b4d26ab1e33c34b50b87558},
affiliation={Institute of Computer Science & Engineering, Xi'an University of Technology, No.5 South Jinhua Road, Xi'an, 710048, China},
abstract={This study proposes a 3D face recognition method using multiple subject-specific curves insensitive to intra-subject distortions caused by expression variations. Considering that most sharp variances in facial convex regions are closely related to the bone structure, the convex crest curves are first extracted as the most vital subject-specific facial curves based on the principal curvature extrema in convex local surfaces. Then, the central profile curve and the horizontal contour curve passing through the nose tip are detected by using the precise localization of the nose tip and symmetry plane. Based on their discriminative power and robustness to expression changes, the three types of curves are fused with appropriate weights at the feature-level and used for matching 3D faces with the iterative closest point algorithm. The combination of multiple expression-insensitive curves is complementary and provides sufficient and stable facial surface features for face recognition. In addition, for each convex crest curve, an expression-irrelevant factor is assigned as the adaptive weight to improve the face matching performance. The results of experiments using two public 3D databases, GavabDB and BU-3DFE, demonstrate the effectiveness of the proposed method, and its recognition rates on both databases reflect an encouraging performance. © 2017 Elsevier B.V.},
author_keywords={3D face recognition;  Expression-insensitive;  Feature-level;  Fusion;  Subject-specific curve},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kim2018133,
author={Kim, D. and Hernandez, M. and Choi, J. and Medioni, G.},
title={Deep 3D face identification},
journal={IEEE International Joint Conference on Biometrics, IJCB 2017},
year={2018},
volume={2018-January},
pages={133-142},
doi={10.1109/BTAS.2017.8272691},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046247920&doi=10.1109%2fBTAS.2017.8272691&partnerID=40&md5=9660021125b86249b9768912252c19ff},
affiliation={USC Institute for Robotics and Intelligent Systems (IRIS), University of Southern California, United States},
abstract={We propose a novel 3D face recognition algorithm using a deep convolutional neural network (DCNN) and a 3D face expression augmentation technique. The performance of 2D face recognition algorithms has significantly increased by leveraging the representational power of deep neural networks and the use of large-scale labeled training data. In this paper, we show that transfer learning from a CNN trained on 2D face images can effectively work for 3D face recognition by fine-tuning the CNN with an extremely small number of 3D facial scans. We also propose a 3D face expression augmentation technique which synthesizes a number of different facial expressions from a single 3D face scan. Our proposed method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets without using hand-crafted features. The 3D face identification using our deep features also scales well for large databases. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2018234,
author={Li, H. and Sun, J. and Chen, L.},
title={Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition},
journal={IEEE International Joint Conference on Biometrics, IJCB 2017},
year={2018},
volume={2018-January},
pages={234-242},
doi={10.1109/BTAS.2017.8272703},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046291057&doi=10.1109%2fBTAS.2017.8272703&partnerID=40&md5=e5c4065442f8bc899597efdc443ed01c},
affiliation={School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, 710049, China; Department of Mathematics and Informatics, Ecole Centrale de Lyon, Lyon, 69134, France},
abstract={This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01%, 97.60%, and 96.13% on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhao2018207,
author={Zhao, J.-L. and Wu, Z.-K. and Pan, Z.-K. and Duan, F.-Q. and Li, J.-H. and Lv, Z.-H. and Wang, K. and Chen, Y.-C.},
title={3D Face Similarity Measure by Fréchet Distances of Geodesics},
journal={Journal of Computer Science and Technology},
year={2018},
volume={33},
number={1},
pages={207-222},
doi={10.1007/s11390-018-1814-7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041342736&doi=10.1007%2fs11390-018-1814-7&partnerID=40&md5=8478bdecdc0eeeb6cb4dda3bb1ecda52},
affiliation={School of Data Science and Software Engineering, Qingdao University, Qingdao, 266071, China; College of Automation and Electrical Engineering, Qingdao University, Qingdao, 266071, China; College of Information Science and Technology, Beijing Normal University, Beijing, 100087, China; College of Computer Science and Technology, Qingdao University, Qingdao, 266071, China; School of Management, Capital Normal University, Beijing, 100048, China},
abstract={3D face similarity is a critical issue in computer vision, computer graphics and face recognition and so on. Since Fréchet distance is an effective metric for measuring curve similarity, a novel 3D face similarity measure method based on Fréchet distances of geodesics is proposed in this paper. In our method, the surface similarity between two 3D faces is measured by the similarity between two sets of 3D curves on them. Due to the intrinsic property of geodesics, we select geodesics as the comparison curves. Firstly, the geodesics on each 3D facial model emanating from the nose tip point are extracted in the same initial direction with equal angular increment. Secondly, the Fréchet distances between the two sets of geodesics on the two compared facial models are computed. At last, the similarity between the two facial models is computed based on the Fréchet distances of the geodesics obtained in the second step. We verify our method both theoretically and practically. In theory, we prove that the similarity of our method satisfies three properties: reflexivity, symmetry, and triangle inequality. And in practice, experiments are conducted on the open 3D face database GavaDB, Texas 3D Face Recognition database, and our 3D face database. After the comparison with iso-geodesic and Hausdorff distance method, the results illustrate that our method has good discrimination ability and can not only identify the facial models of the same person, but also distinguish the facial models of any two different persons. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={3D face;  Fréchet distance;  geodesic;  similarity measure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lv2018,
author={Lv, C. and Zhao, J.},
title={3D Face Recognition based on Local Conformal Parameterization and Iso-Geodesic Stripes Analysis},
journal={Mathematical Problems in Engineering},
year={2018},
volume={2018},
doi={10.1155/2018/4707954},
art_number={4707954},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056267138&doi=10.1155%2f2018%2f4707954&partnerID=40&md5=6b9608dc984f786b68071250cc5e66cb},
affiliation={College of Information Science and Technology, Beijing Normal University, Beijing, China; Engineering Research Center of Virtual Reality and Applications, Ministry of Education, Beijing Key Laboratory of Digital Preservation, Beijing, 100875, China; School of Data Science and Software Engineering, Qingdao University, Qingdao, China; College of Automation and Electrical Engineering, Qingdao University, Qingdao, China},
abstract={3D face recognition is an important topic in the field of pattern recognition and computer graphic. We propose a novel approach for 3D face recognition using local conformal parameterization and iso-geodesic stripes. In our framework, the 3D facial surface is considered as a Riemannian 2-manifold. The surface is mapped into the 2D circle parameter domain using local conformal parameterization. In the parameter domain, the geometric features are extracted from the iso-geodesic stripes. Combining the relative position measure, Chain 2D Weighted Walkthroughs (C2DWW), the 3D face matching results can be obtained. The geometric features from iso-geodesic stripes in parameter domain are robust in terms of head poses, facial expressions, and some occlusions. In the experiments, our method achieves a high recognition accuracy of 3D facial data from the Texas3D and Bosphorus3D face database. © 2018 Chenlei Lv and Junli Zhao.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ratyal20184903,
author={Ratyal, N. and Taj, I. and Bajwa, U. and Sajid, M.},
title={Pose and expression invariant alignment based multi-view 3d face recognition},
journal={KSII Transactions on Internet and Information Systems},
year={2018},
volume={12},
number={10},
pages={4903-4929},
doi={10.3837/tiis.2018.10.016},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057280833&doi=10.3837%2ftiis.2018.10.016&partnerID=40&md5=b7937933dbf1206dd6e5a81101a545c0},
affiliation={Vision and Pattern Recognition Systems Research Group, Capital University of Science and Technology (CUST), Islamabad, Pakistan; Department of Computer Science, COMSATS Institute of Information Technology, Lahore, Pakistan},
abstract={In this study, a fully automatic pose and expression invariant 3D face alignment algorithm is proposed to handle frontal and profile face images which is based on a two pass course to fine alignment strategy. The first pass of the algorithm coarsely aligns the face images to an intrinsic coordinate system (ICS) through a single 3D rotation and the second pass aligns them at fine level using a minimum nose tip-scanner distance (MNSD) approach. For facial recognition, multi-view faces are synthesized to exploit real 3D information and test the efficacy of the proposed system. Due to optimal separating hyper plane (OSH), Support Vector Machine (SVM) is employed in multi-view face verification (FV) task. In addition, a multi stage unified classifier based face identification (FI) algorithm is employed which combines results from seven base classifiers, two parallel face recognition algorithms and an exponential rank combiner, all in a hierarchical manner. The performance figures of the proposed methodology are corroborated by extensive experiments performed on four benchmark datasets: GavabDB, Bosphorus, UMB-DB and FRGC v2.0. Results show mark improvement in alignment accuracy and recognition rates. Moreover, a computational complexity analysis has been carried out for the proposed algorithm which reveals its superiority in terms of computational efficiency as well. © 2018 KSII.},
author_keywords={3D alignment;  3D FR;  Profile face;  SVM;  Unified classifier},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khan2018220,
author={Khan, M.S. and Jehanzeb, M. and Babar, M.I. and Faisal, S. and Ullah, Z. and Amin, S.Z.B.M.},
title={Face recognition analysis using 3D model},
journal={Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
year={2018},
volume={200},
pages={220-236},
doi={10.1007/978-3-319-95450-9_19},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051123902&doi=10.1007%2f978-3-319-95450-9_19&partnerID=40&md5=3b4f204837bae6e24de550e8aa4fd707},
affiliation={College of Computer Science, Sichuan University, Chengdu, 610065, China; Department of Computer Science, Army Public College of Management & Sciences (APCOMS), Rawalpindi, Punjab, Pakistan; Department of Computer Science, University of Haripur, Hattar Road Near Swat Chowk, Haripur, Khyber Pakhtunkhwa  22620, Pakistan; Federation University Australia, Mt Helen, Ballarat, VIC  3350, Australia; Western China Earthquake and Hazards Mitigation Research Centre, College of Architecture and Environment, Sichuan University, Chengdu, 610065, China},
abstract={Facial Recognition is a commonly used technology in security-related applications. It has been thoroughly studied and scrutinized for its number of practical real-world applications. On the road ahead of understanding this technology, there remain several obstacles. In this paper, methods of 3D face recognition are examined by measuring quantifiable applications and results. In facial recognition, three Dimensional Morphable Model (3DMM) techniques have attracted more and more attention as effectiveness in use increases over time. 3DMM provides automation and more accurate image rendering when compared to other traditional techniques. The accuracy in image rendering comes at a cost; as 3DMM requires more focus on texture estimation, shape-controlling limits, and extrinsic variations, accurately matching fitting models, feature tracking and precision identification. We have underlined different issues in comparison based on these methods. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018.},
author_keywords={3D model;  Morphable model;  Recognition;  Reconstruction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Banita20182325,
author={Banita and Tanwar, P.},
title={Evaluation of 3d facial paralysis using fuzzy logic},
journal={International Journal of Engineering and Technology(UAE)},
year={2018},
volume={7},
number={4},
pages={2325-2331},
doi={10.14419/ijet.v7i4.13619},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053901681&doi=10.14419%2fijet.v7i4.13619&partnerID=40&md5=f68480ef298793b35ad707b17c64ab59},
affiliation={Lingaya's University, Faridabad, India; Manav Rachana University, Faridabad, India},
abstract={Face recognition are of great interest to researchers in terms of Image processing and Computer Graphics. In recent years, various factors become popular which clearly affect the face model. Which are ageing, universal facial expressions, and muscle movement. Similarly in terms of medical terminology the facial paralysis can be peripheral or central depending on the level of motor neuron lesion which can be below the nucleus of the nerve or supra nuclear. The various medical therapy used for facial paralysis are electroaccupunture, electro-therapy, laser acupuncture, manual acupuncture which is a traditional form of acupuncture. Imaging plays a great role in evaluation of degree of paralysis and also for faces recognition. There is a wide research in terms of facial expressions and facial recognition but lim-ited research work is available in facial paralysis. House- Brackmann Grading system is one of the simplest and easiest method to evalu-ate the degree of facial paralysis. During evaluation common facial expressions are recorded and are further evaluated by considering the focal points of the left or the right side of the face. This paper presents the classification of face recognition and its respective fuzzy rules to remove uncertainty in the result after evaluation of facial paralysis. © 2018 Banita, Dr. Poonam Tanwar.},
author_keywords={3D face recognition;  CNN;  Evaluation of facial paralysis;  MAMDANI model;  Stages of face recognition},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hu2017133,
author={Hu, H. and Shah, S.A.A. and Bennamoun, M. and Molton, M.},
title={2D and 3D face recognition using convolutional neural network},
journal={IEEE Region 10 Annual International Conference, Proceedings/TENCON},
year={2017},
volume={2017-December},
pages={133-138},
doi={10.1109/TENCON.2017.8227850},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044210743&doi=10.1109%2fTENCON.2017.8227850&partnerID=40&md5=12f531513eddb3716404e891cd002a29},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA  6009, Australia},
abstract={Face recognition remains a challenge today as recognition performance is strongly affected by variability such as illumination, expressions and poses. In this work we apply Convolutional Neural Networks (CNNs) on the challenging task of both 2D and 3D face recognition. We constructed two CNN models, namely CNN-1 (two convolutional layers) and CNN-2 (one convolutional layer) for testing on 2D and 3D dataset. A comprehensive parametric study of two CNN models on face recognition is represented in which different combinations of activation function, learning rate and filter size are investigated. We find that CNN-2 has a better accuracy performance on both 2D and 3D face recognition. Our experimental results show that an accuracy of 85.15% was accomplished using CNN-2 on depth images with FRGCv2.0 dataset (4950 images with 557 objectives). An accuracy of 95% was achieved using CNN-2 on 2D raw image with the AT&T dataset (400 images with 40 objectives). The results indicate that the proposed CNN model is capable to handle complex information from facial images in different dimensions. These results provide valuable insights into further application of CNN on 3D face recognition. © 2017 IEEE.},
author_keywords={Convolutional Neural Networks;  Depth Image;  Face Recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Soltanpour2017560,
author={Soltanpour, S. and Wu, Q.M.J.},
title={Multiscale depth local derivative pattern for sparse representation based 3D face recognition},
journal={2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017},
year={2017},
volume={2017-January},
pages={560-565},
doi={10.1109/SMC.2017.8122665},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044201680&doi=10.1109%2fSMC.2017.8122665&partnerID=40&md5=5918c06bbe0b6cc90d03ba306b0d1258},
affiliation={Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada},
abstract={3D face recognition is a popular research area due to its vast application in biometrics and security. Local featurebased methods gain importance in the recent years due to their robustness under degradation conditions. In this paper, a novel high-order local pattern descriptor in combination with sparse representation based classifier (SRC) is proposed for expression robust 3D face recognition. 3D point clouds are converted to depth maps after preprocessing. Multi-directional derivatives are applied in spatial space to encode the depth maps based on the local derivative pattern (LDP) scheme. Directional pattern features are calculated according to local derivative variations. Since LDP computes spatial relationship of neighbors in a local region, it extracts distinct information from the depth map. Multiscale depth-LDP is presented as a novel descriptor for 3D face recognition. The descriptor is employed along with the SRC to increase the range data distinctiveness. A histogram on the derivative pattern creates a spatial feature descriptor that represents the distinctive micro-patterns from 3D data. We evaluate the proposed algorithm on two famous 3D face databases, FRGC v2.0 and Bosphorus. The experimental results demonstrate that the proposed approach achieves acceptable performance under facial expression. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2017,
author={Liu, Z. and Cohen, F.},
title={Synthesis and identification of three-dimensional faces from image(s) and three-dimensional generic models},
journal={Journal of Electronic Imaging},
year={2017},
volume={26},
number={6},
doi={10.1117/1.JEI.26.6.063005},
art_number={063005},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034836161&doi=10.1117%2f1.JEI.26.6.063005&partnerID=40&md5=cbf0b712fa254c1cccec3850f657b1dd},
affiliation={Kepler Group LLC, New York, NY, United States; Drexel University, Electrical and Computer Engineering, Philadelphia, PA, United States},
abstract={We describe an approach for synthesizing a three-dimensional (3-D) face structure from an image or images of a human face taken at a priori unknown poses using gender and ethnicity specific 3-D generic models. The synthesis process starts with a generic model, which is personalized as images of the person become available using preselected landmark points that are tessellated to form a high-resolution triangular mesh. From a single image, two of the three coordinates of the model are reconstructed in accordance with the given image of the person, while the third coordinate is sampled from the generic model, and the appearance is made in accordance with the image. With multiple images, all coordinates and appearance are reconstructed in accordance with the observed images. This method allows for accurate pose estimation as well as face identification in 3-D rendering of a difficult two-dimensional (2-D) face recognition problem into a much simpler 3-D surface matching problem. The estimation of the unknown pose is achieved using the Levenberg-Marquardt optimization process. Encouraging experimental results are obtained in a controlled environment with high-resolution images under a good illumination condition, as well as for images taken in an uncontrolled environment under arbitrary illumination with low-resolution cameras. © 2017 SPIE and IS&T.},
author_keywords={3-D face recognition;  3-D face synthesis;  iterative personalization;  pose estimation;  ray tracing;  subdivision},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Frikha2017,
author={Frikha, T. and Chaabane, F. and Said, B. and Drira, H. and Abid, M. and Ben Amar, C. and Lille, L.},
title={Embedded approach for a Riemannian-based framework of analyzing 3D faces},
journal={Proceedings - 3rd International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2017},
year={2017},
doi={10.1109/ATSIP.2017.8075548},
art_number={8075548},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035335463&doi=10.1109%2fATSIP.2017.8075548&partnerID=40&md5=f9c92873ac16b26563e98ecaba5f6bba},
affiliation={REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, ENIS, BP 1173, Gabes University, Higher Institute of Computer Science and Multimedia of Gabes, Sfax, 3038, Tunisia; REGIM-Lab, Sfax, 3038, Tunisia; Multimedia of Gabes, Sfax, 3038, Tunisia; CES-Laboratory Sfax Sud University, National Engineering School of Sfax, Sfax, Tunisia},
abstract={Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform. © 2017 IEEE.},
author_keywords={3D face recognition;  Curve analysis;  elastic analysis algorithm;  embedded architecture;  face detection;  Facial analysis;  Facial expressions;  Riemann geometry},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sui201719575,
author={Sui, D. and Hou, D. and Duan, X.},
title={An interpolation algorithm fitted for dynamic 3D face reconstruction},
journal={Multimedia Tools and Applications},
year={2017},
volume={76},
number={19},
pages={19575-19589},
doi={10.1007/s11042-015-3233-x},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954425638&doi=10.1007%2fs11042-015-3233-x&partnerID=40&md5=78e6cc2a30fcda0f0eb7e4d7fd616c46},
affiliation={School of Information Science and Technology, Wuhan University of Technology, Hubei, 430070, China; School of Software Engineering, Anyang Normal University, Anyang, Henan  455000, China; Department of Computer, College of Science California State Polytecnic University-Pomona, California, United States},
abstract={In order to solve the problem of low recognition accuracy in later period which is caused by the too few extracted parameters in the 3D face recognition, and the incapable formation of completed point cloud structure. An automatic iterative interpolation algorithm is proposed. The new and more accurate 3D face data points are obtained by automatic iteration. This algorithm can be used to restore the data point cloud information of 3D facial feature in 2D images by means of facial three-legged structure formed by 3D face and automatic interpolation. Thus, it can realize to shape the 3D facial dynamic model which can be recognized and has high saturability. Experimental results show that the interpolation algorithm can achieve the complete the construction of facial feature based on the facial feature after 3D dynamic reconstruction, and the validity is higher. © 2016, Springer Science+Business Media New York.},
author_keywords={3D face dynamic recognition;  Iterative interpolation algorithm;  Point cloud structure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Deng20171305,
author={Deng, X. and Da, F. and Shao, H.},
title={Adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition},
journal={Signal, Image and Video Processing},
year={2017},
volume={11},
number={7},
pages={1305-1312},
doi={10.1007/s11760-017-1087-6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017095710&doi=10.1007%2fs11760-017-1087-6&partnerID=40&md5=fc06247cc3ca7870221563a085266269},
affiliation={School of Automation, Southeast University, Nanjing, Jiangsu  210096, China; Key Laboratory of Measurement and Control for Complex System, Ministry of Education, Southeast University, Nanjing, Jiangsu  210096, China},
abstract={A novel adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition is proposed in this paper. Firstly, the novel facial coarse-to-fine landmarks localization method based on Active Shape Model and Gabor wavelets transformation is proposed to exactly and automatically locate facial landmarks in range image. Secondly, the multi-scale fusion of the pyramid local binary patterns (F-PLBP) based on the irregular segmentation associated with the located landmarks is proposed to extract the discriminative feature. Thirdly, a sparse representation-based classifier based on the adaptive feature selection (A-SRC) using the distribution of the reconstruction residual is presented to select the expression-robust feature and identify the faces. Finally, the experimental evaluation based on FRGC v2.0 indicates that the adaptive feature selection method using F-PLBP combined with the A-SRC can obtain the high recognition accuracy by performing the higher discriminative power to overcome the influence from the facial expression variations. © 2017, Springer-Verlag London.},
author_keywords={3D face recognition;  Adaptive feature selection;  Facial landmark localization;  Multi-scale fusion},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liang201784,
author={Liang, Y. and Zhang, Y. and Zeng, X.-X.},
title={Pose-invariant 3D face recognition using half face},
journal={Signal Processing: Image Communication},
year={2017},
volume={57},
pages={84-90},
doi={10.1016/j.image.2017.05.004},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020037210&doi=10.1016%2fj.image.2017.05.004&partnerID=40&md5=88180b3e1c893d79c59e39647f3ee0a2},
affiliation={Guangdong University of Technology, School of Automation, No.100, Waihuan Xi Road, Guangzhou Higher Education Mega Centre, Guangzhou, 510006, China; South China Normal University, School of Software, Nanhai Information Technology Park, Foshan, 528225, China},
abstract={Pose variations are still challenging problems in 3D face recognition because large pose variations will cause self-occlusion and result in missing data. In this paper, a new method for pose-invariant 3D face recognition is proposed to handle significant pose variations. For pose estimation and registration, a coarse-to-fine strategy is proposed to detect landmarks under large yaw variations. At the coarse search step, candidate landmarks are detected using HK curvature analysis and subdivided according to a facial geometrical structure-based classification strategy. At the fine search step, candidate landmarks are identified and labeled by comparing with a Facial Landmark Model. By using the half face matching, we perform the matching step with respect to frontal scans and side scans. Experiments carried out on the Bosphorus and UND/FRGC v2.0 databases show that our method has high accuracy and robustness to pose variations. © 2017 Elsevier B.V.},
author_keywords={3D face recognition;  Facial landmark localization;  Half face;  Iso-geodesic stripes;  Pose variation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang20171141,
author={Wang, X.-Q. and Yuan, J.-Z. and Li, Q.},
title={3D face recognition using spherical vector norms map},
journal={Journal of Information Science and Engineering},
year={2017},
volume={33},
number={5},
pages={1141-1161},
doi={10.6688/JISE.2017.33.5.3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049875700&doi=10.6688%2fJISE.2017.33.5.3&partnerID=40&md5=cd82489caa82b30e6bb8b05ea1dbaae9},
affiliation={Beijing Key Laboratory of Information Service Engineering, China; Computer Technology Institute, Beijing Union University, Beijing, 100101, China; Beijing Advanced Innovation Center for Imaging Technology Capital Normal University, Beijing, 100048, China},
abstract={In this paper, we introduce a novel, automatic method for 3D face recognition. A new feature called a spherical vector norms map of a 3D face is created using the normal vector of each point. This feature contains more detailed information than the original depth image in regions such as the eyes and nose. For certain flat areas of 3D face, such as the forehead and cheeks, this map could increase the distinguishability of different points. In addition, this feature is robust to facial expression due to an adjustment that is made in the mouth region. Then, the facial representations, which are based on Histograms of Oriented Gradients, are extracted from the spherical vector norms map and the original depth image. A new partitioning strategy is proposed to produce the histogram of eight patches of a given image, in which all of the pixels are binned based on the magnitude and direction of their gradients. In this study, SVNs map and depth image are represented compactly with two histograms of oriented gradients; this approach is completed by Linear Discriminant Analysis and a Nearest Neighbor classifier. © 2017 Institute of Information Science. All Rights Reserved.},
author_keywords={3D face recognition;  Face recognition grand challenge database;  Histograms of oriented gradients;  Linear discriminant analysis;  Spherical vector norms map},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tang2017589,
author={Tang, Y. and Chen, L.},
title={3D Facial Geometric Attributes Based Anti-Spoofing Approach against Mask Attacks},
journal={Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017},
year={2017},
pages={589-595},
doi={10.1109/FG.2017.74},
art_number={7961795},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026323809&doi=10.1109%2fFG.2017.74&partnerID=40&md5=ed89e52bd051c85dbee8198cd1d7344e},
affiliation={Universite de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, Lyon, 69134, France},
abstract={3D scanning and 3D printing techniques, as the technical impetus of 3D face recognition, also boost unconsciously the security threat against it from the spoofing attacks via manufactured mask. In order to improve the robustness of 3D face recognition system, several countermeasures against mask attacks based on photometric features have been reported in recent years. However, the anti-spoofing approach involving 3D meshed face scan and the related 3D facial features have not been studied yet. For filling this gap, in this paper, we propose to exploit the anti-spoofing performance of geometric attributes based 3D facial description. It synthesises the advantages of the selected geometric attributes, named principal curvature measures, and the meshSIFT-based feature descriptor. Specifically, the estimation of geometric attributes is coherent to the property of discrete surface, and the feature related to them can accurately describe the shape of facial surface. These characteristics are beneficial to discovering the geometry-based dissimilarity between genuine face and fraud mask. In the experiment part, the baselines of verification and anti-spoofing performance are evaluated on the Morpho database. Furthermore, for simulating a real-world scenario and testing the corresponding anti-spoofing performance, the size of genuine face set is massively extended by uniting the Morpho database and the FRGC v2.0 database to increase the ratio of genuine faces to fraud masks. The evaluation results prove that the proposed 3D face verification system can guarantee competitive verification accuracy for genuine faces and promising anti-spoofing performance against mask attacks. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Emambakhsh2017995,
author={Emambakhsh, M. and Evans, A.},
title={Nasal Patches and Curves for Expression-Robust 3D Face Recognition},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2017},
volume={39},
number={5},
pages={995-1007},
doi={10.1109/TPAMI.2016.2565473},
art_number={7467565},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018498024&doi=10.1109%2fTPAMI.2016.2565473&partnerID=40&md5=e1afb77517c631cf7da8f8d394318b02},
affiliation={Institute of Sensors, Signals and Systems, Heriot-Watt University, Edinburgh, United Kingdom; Department of Electronic and Electrical Engineering, University of Bath, Bath, United Kingdom},
abstract={The potential of the nasal region for expression robust 3D face recognition is thoroughly investigated by a novel five-step algorithm. First, the nose tip location is coarsely detected and the face is segmented, aligned and the nasal region cropped. Then, a very accurate and consistent nasal landmarking algorithm detects seven keypoints on the nasal region. In the third step, a feature extraction algorithm based on the surface normals of Gabor-wavelet filtered depth maps is utilised and, then, a set of spherical patches and curves are localised over the nasal region to provide the feature descriptors. The last step applies a genetic algorithm-based feature selector to detect the most stable patches and curves over different facial expressions. The algorithm provides the highest reported nasal region-based recognition ranks on the FRGC, Bosphorus and BU-3DFE datasets. The results are comparable with, and in many cases better than, many state-of-the-art 3D face recognition algorithms, which use the whole facial domain. The proposed method does not rely on sophisticated alignment or denoising steps, is very robust when only one sample per subject is used in the gallery, and does not require a training step for the landmarking algorithm. © 2016 IEEE.},
author_keywords={Face recognition;  facial landmarking;  feature selection;  Gabor wavelets;  nose region;  surface normals},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Naveen2017112,
author={Naveen, S. and Rugmini, K.P. and Moni, R.S.},
title={3D face reconstruction by pose correction, patch cloning and texture wrapping},
journal={2016 International Conference on Communication Systems and Networks, ComNet 2016},
year={2017},
pages={112-116},
doi={10.1109/CSN.2016.7823997},
art_number={7823997},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014911379&doi=10.1109%2fCSN.2016.7823997&partnerID=40&md5=d83e42a8548567d921eea89900252c10},
affiliation={Dept. of ECE, LBSITW, Thiruvananthapuram Kerala, India; Dept. of ECE Marian Engineering College, Thiruvananthapuram Kerala, India},
abstract={Face is being considered as one of the most commonly used biometric modality. The inaccuracy in two dimensional face recognition systems is mainly due to pose variations, occlusions, illumination etc. Among this, changes in illumination condition do not affect 3D face recognition systems. But pose variation drastically changes the appearance of face images. To solve the problems with depth map and texture images corrupted by head pose variations and the occlusions generated due to these pose variations, a reconstruction method is proposed which consist of three stages. In the first stage, the pose correction is done by Iterative Closest Point (ICP) algorithm and in the second stage the occluded region of the face is reconstructed by a resurfacing method called patch cloning. It is followed by the wrapping of reconstructed depth map by its texture to generate a 3D model. The statistical error between the original face and the reconstructed face is also evaluated. In this work, facial symmetry is used as a prior knowledge. Experiments are done with the FRAV3D database. © 2016 IEEE.},
author_keywords={Face recognition;  Face Resurfacing;  ICP algorithm;  Patch Cloning;  Pose Correction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hariri2017187,
author={Hariri, W. and Tabia, H. and Farah, N. and Declercq, D. and Benouareth, A.},
title={Geometrical and visual feature quantization for 3D face recognition},
journal={VISIGRAPP 2017 - Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
year={2017},
volume={5},
pages={187-193},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013991858&partnerID=40&md5=fd026eef10dcc4922fe7269db34feb6c},
affiliation={ETIS, ENSEA, University of Cergy-Pontoise, CNRS, UMR 8051, Cergy-Pontoise, France; Labged Laboratory, Computer Science Department, Badji Mokhtar Annaba University, Annaba, Algeria},
abstract={In this paper, we present an efficient method for 3D face recognition based on vector quantization of both geometrical and visual proprieties of the face. The method starts by describing each 3D face using a set of orderless features, and use then the Bag-of-Features paradigm to construct the face signature. We analyze the performance of three well-known classifiers: the Naïve Bayes, the Multilayer perceptron and the Random forests. The results reported on the FRGCv2 dataset show the effectiveness of our approach and prove that the method is robust to facial expression. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Bag-of-Features;  Codebook;  Depth image;  HoS;  LBP;  Term vector},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2017633,
author={Zhang, T. and Mu, Z. and Li, Y. and Liu, Q. and Zhang, Y.},
title={3D face and ear recognition based on partial mars map},
journal={ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
year={2017},
volume={2017-January},
pages={633-637},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049438191&partnerID=40&md5=142925806e53f82e08f8cdf1fe8539c6},
affiliation={School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China},
abstract={This paper proposes a 3D face recognition approach based on facial pose estimation, which is robust to large pose variations in the unconstrained scene. Deep learning method is used to facial pose estimation, and the generation of partial MARS (Multimodal fAce and eaR Spherical) map reduces the probability of feature points appearing in the deformed region. Then we extract the features from the depth and texture maps. Finally, the matching scores from two types of maps should be calculated by Bayes decision to generate the final result. In the large pose variations, the recognition rate of the method in this paper is 94.6%. The experimental results show that our approach has superior performance than the existing methods used on the MARS map, and has potential to deal with 3D face recognition in unconstrained scene. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={3d face recognition;  Deep learning;  Head pose estimation;  Partial mars map},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Deng201713,
author={Deng, X. and Da, F. and Shao, H.},
title={Expression-robust 3D face recognition based on feature-level fusion and feature-region fusion},
journal={Multimedia Tools and Applications},
year={2017},
volume={76},
number={1},
pages={13-31},
doi={10.1007/s11042-015-3012-8},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945315051&doi=10.1007%2fs11042-015-3012-8&partnerID=40&md5=5f6b0d57d4d091b3769e974b26751540},
affiliation={Department of Automation, Southeast University, Nanjing, Jiangsu  210096, China; Key Laboratory of Measurement and Control for Complex System of Ministry of Education, Southeast University, Nanjing, Jiangsu  210096, China},
abstract={3D face shape is essentially a non-rigid free-form surface, which will produce non-rigid deformation under expression variations. In terms of that problem, a promising solution named Coherent Point Drift (CPD) non-rigid registration for the non-rigid region is applied to eliminate the influence from the facial expression while guarantees 3D surface topology. In order to take full advantage of the extracted discriminative feature of the whole face under facial expression variations, the novel expression-robust 3D face recognition method using feature-level fusion and feature-region fusion is proposed. Furthermore, the Principal Component Analysis and Linear Discriminant Analysis in combination with Rotated Sparse Regression (PL-RSR) dimensionality reduction method is presented to promote the computational efficiency and provide a solution to the curse of dimensionality problem, which benefit the performance optimization. The experimental evaluation indicates that the proposed strategy has achieved the rank-1 recognition rate of 97.91 % and 96.71 % based on Face Recognition Grand Challenge (FRGC) v2.0 and Bosphorus respectively, which means the proposed approach outperforms state-of-the-art approach. © 2015, Springer Science+Business Media New York.},
author_keywords={3D face recognition;  Dimensionality reduction;  Feature-level fusion;  Feature-region fusion;  Non-rigid point set registration},
document_type={Article},
source={Scopus},
}

@ARTICLE{Keshwani2017333,
author={Keshwani, L. and Pete, D.},
title={Comparative analysis of frontal face recognition using radial curves and back propagation neural network},
journal={Advances in Intelligent Systems and Computing},
year={2017},
volume={469},
pages={333-344},
doi={10.1007/978-981-10-1678-3_32},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984944723&doi=10.1007%2f978-981-10-1678-3_32&partnerID=40&md5=8786b9edc61be7625d9311c127d87030},
affiliation={Datta Meghe College of Engineering, Airoli, Navi Mumbai, India; Electronics and Telecommunication Department, Datta Meghe College of Engineering, Airoli, Navi Mumbai, India},
abstract={Person identification using face as a cue is one of the most prominent and robust technique. This paper presents 3D face recognition system using Radial curves and Back Propagation Neural Networks (BPNN). The face images used for experimentation are under various challenges like illumination, pose variation, expression and occlusions. The features of images are extracted using Eigen vectors. These features are compared using radial curves on the face starting from center of the face to the end of the face. Each corresponding curve is matched using Euclidean Distance classifier. The BPNN is used to train the features for face matching. The proposed algorithms are tested on ORL and DMCE database. The performance analysis is based on recognition rate accuracy of the system. The proposed radial curve system yields recognition rate accuracy of 100 % for images from the ORL database and 98 % for the images from DMCE database. © Springer Science+Business Media Singapore 2017.},
author_keywords={Back propagation neural networks;  Face recognition;  ORL database;  Radial curves},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wei201766,
author={Wei, X. and Li, H. and Gu, X.D.},
title={Three Dimensional Face Recognition via Surface Harmonic Mapping and Deep Learning},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10568 LNCS},
pages={66-76},
doi={10.1007/978-3-319-69923-3_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032656852&doi=10.1007%2f978-3-319-69923-3_8&partnerID=40&md5=598d2aabb96d1b2d7f9963542a2fcddf},
affiliation={School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Department of Computer Science and Department of Mathematics, State University of New York at Stony Brook, New York, United States},
abstract={In this paper, we propose a general 3D face recognition framework by combining the idea of surface harmonic mapping and deep learning. In particular, given a 3D face scan, we first run the pre-processing pipeline and detect three main facial landmarks (i.e., nose tip and two inner eye corners). Then, harmonic mapping is employed to map the 3D coordinates and differential geometry quantities (e.g., normal vectors, curvatures) of each 3D face scan to a 2D unit disc domain, generating a group of 2D harmonic shape images (HSI). The 2D rotation of the harmonic shape images are removed by using the three detected landmarks. All these pose normalized harmonic shape images are fed into a pre-trained deep convolutional neural network (DCNN) to generate their deep representations. Finally, sparse representation classifier with score-level fusion is used for face similarity measurement and the final decision. The advantage of our method is twofold: (i) it is a general framework and can be easily extended to other surface mapping and deep learning algorithms. (ii) it is registration-free and only needs three landmarks. The effectiveness of the proposed framework was demonstrated on the BU-3DFE database, and reporting a rank-one recognition rate of 89.38% on the whole database. © 2017, Springer International Publishing AG.},
author_keywords={3D face recognition;  Deep learning;  Surface harmonic mapping},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tang201741,
author={Tang, Y. and Chen, L.},
title={Shape analysis based anti-spoofing 3D face recognition with mask attacks},
journal={Communications in Computer and Information Science},
year={2017},
volume={684},
pages={41-55},
doi={10.1007/978-3-319-60654-5_4},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025122871&doi=10.1007%2f978-3-319-60654-5_4&partnerID=40&md5=ee21d18278533c54c55ae227daf6a772},
affiliation={Université de Lyon, Ecole Centrale de Lyon, LIRIS laboratory, UMR CNRS 5205, Lyon, 69134, France},
abstract={With the growth of face recognition, the spoofing mask attacks attract more attention in biometrics research area. In recent years, the countermeasures based on the texture and depth image against spoofing mask attacks have been reported, but the research based on 3D meshed sample has not been studied yet. In this paper, we propose to apply 3D shape analysis based on principal curvature measures to describe the meshed facial surface. Meanwhile, a verification protocol based on this feature descriptor is designed to verify person identity and to evaluate the anti-spoofing performance on Morpho database. Furthermore, for simulating a real-life testing scenario, FRGCv2 database is enrolled as an extension of face scans to augment the ratio of genuine face samples to fraud mask samples. The experimental results show that our system can guarantee a high verification rate for genuine faces and the satisfactory anti-spoofing performance against spoofing mask attacks in parallel. © Springer International Publishing AG 2017.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu2016,
author={Yu, X. and Gao, Y. and Zhou, J.},
title={Boosting Radial Strings for 3D Face Recognition with Expressions and Occlusions},
journal={2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
year={2016},
doi={10.1109/DICTA.2016.7797014},
art_number={7797014},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011088593&doi=10.1109%2fDICTA.2016.7797014&partnerID=40&md5=b22aac1e03b48565af8c230b46a10822},
affiliation={School of Engineering, Griffith University, Nathan, QLD, Australia; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia},
abstract={In this paper, we present a new radial string representation and matching approach for 3D face recognition under expression variations and partial occlusions. The radial strings are an indexed collection of strings emanating from the nose tip of a face scan. The matching between two radial strings is conducted through a dynamic programming process, in which a partial matching mechanism is established to effectively find those un-occluded substrings. Moreover, the most discriminative and stable radial strings are selected optimally by the well-known AdaBoost algorithm to achieve a composite classifier for 3D face recognition under facial expression changes. Experimental results on the GavabDB and the Bosphorus databases show that the proposed approach achieves promising results for human face recognition with expressions and occlusions. © 2016 IEEE.},
author_keywords={face recognition;  facial curves;  feature selection;  machine learning;  string matching},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gilani2016,
author={Gilani, S.Z. and Mian, A.},
title={Towards Large-Scale 3D Face Recognition},
journal={2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
year={2016},
doi={10.1109/DICTA.2016.7797090},
art_number={7797090},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011072173&doi=10.1109%2fDICTA.2016.7797090&partnerID=40&md5=8e7de072eb2e5794e1830d7699cecbb6},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, Australia},
abstract={3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shah2016,
author={Shah, S.A.A. and Bennamoun, M. and Boussaid, F.},
title={Automatic 3D face landmark localization based on 3D vector field analysis},
journal={International Conference Image and Vision Computing New Zealand},
year={2016},
volume={2016-November},
doi={10.1109/IVCNZ.2015.7761526},
art_number={7761526},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006942604&doi=10.1109%2fIVCNZ.2015.7761526&partnerID=40&md5=5da7d80624be2691571e8fafab5b9dbf},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Perth, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, 35 Stirling Highway, Perth, WA, Australia},
abstract={In applications such as 3D face synthesis and animation, a prominent face landmark is required to enable 3D face normalization, pose correction, 3D face recognition and reconstruction. Due to variations in facial expressions, automatic 3D face landmark localization remains a challenge. Nose tip is one of the salient landmarks in a human face. In this paper, a novel nose tip localization technique is proposed. In the proposed approach, the rotation of the 3D vector field is analyzed for robust and efficient nose tip localization. The proposed technique has the following three characteristics: (1) it does not require any training; (2) it does not rely on any particular model; (3) it is very efficient, requiring an average time of only 1.9s for nose tip detection. We tested the proposed technique on BU3DFE and Shrec'10 datasets. Experimental results show that the proposed technique is robust to variations in facial expressions, achieving a 100% detection rate on these publicly available 3D face datasets. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li201693,
author={Li, B.Y.L. and Xue, M. and Mian, A. and Liu, W. and Krishna, A.},
title={Robust RGB-D face recognition using Kinect sensor},
journal={Neurocomputing},
year={2016},
volume={214},
pages={93-108},
doi={10.1016/j.neucom.2016.06.012},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977478475&doi=10.1016%2fj.neucom.2016.06.012&partnerID=40&md5=df18014ab4a68d40273fdbb72a7a85ae},
affiliation={Department of Computing, Curtin University, Kent Street, Perth, WA  6102, Australia; Dalian Key Lab of Digital Technology for National Culture, Dalian Minzu University, Liaoning, Dalian  116600, China; Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA  6009, Australia},
abstract={In this paper we propose a robust face recognition algorithm for low resolution RGB-D Kinect data. Many techniques are proposed for image preprocessing due to the noisy depth data. First, facial symmetry is exploited based on the 3D point cloud to obtain a canonical frontal view image irrespective of the initial pose and then depth data is converted to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are then used to project RGB to DCS (Discriminant Color Space) and normal maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust Sparse Coding method is proposed that codes the multiple channels (DCS or DNM) of a test image as a sparse combination of training samples with different pixel weighting. Weights are calculated dynamically in an iterative process to achieve robustness against variations in pose, illumination, facial expressions and disguise. In contrast to existing techniques, our multi-channel approach is more robust to variations. Reconstruction errors of the test image (DCS and DNM) are normalized and fused to decide its identity. The proposed algorithm is evaluated on four public databases. It achieves 98.4% identification rate on CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects. Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2 databases, the proposed algorithm achieves 97.6%, 95.6% and 95.2% identification rates respectively. To the best of our knowledge, these are the highest identification rates reported so far for the first three databases. © 2016 Elsevier B.V.},
author_keywords={3D face recognition;  Kinect;  Multi-channel discriminant transform;  Sparse coding},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo2016403,
author={Guo, Y. and Lei, Y. and Liu, L. and Wang, Y. and Bennamoun, M. and Sohel, F.},
title={EI3D: Expression-invariant 3D face recognition based on feature and shape matching},
journal={Pattern Recognition Letters},
year={2016},
volume={83},
pages={403-412},
doi={10.1016/j.patrec.2016.04.003},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966659227&doi=10.1016%2fj.patrec.2016.04.003&partnerID=40&md5=b09b19b7a5436b53278c02d001e93910},
affiliation={College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan  410073, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan  610065, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan  410073, China; College of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Computer Science and Software Engineering, The University of Western Australia, Perth, WA  6009, Australia; School of Engineering and Information Technology, Murdoch University, Perth, WA  6150, Australia},
abstract={This paper presents a local feature based shape matching algorithm for expression-invariant 3D face recognition. Each 3D face is first automatically detected from a raw 3D data and normalized to achieve pose invariance. The 3D face is then represented by a set of keypoints and their associated local feature descriptors to achieve robustness to expression variations. During face recognition, a probe face is compared against each gallery face using both local feature matching and 3D point cloud registration. The number of feature matches, the average distance of matched features, and the number of closest point pairs after registration are used to measure the similarity between two 3D faces. These similarity metrics are then fused to obtain the final results. The proposed algorithm has been tested on the FRGC v2 benchmark and a high recognition performance has been achieved. It obtained the state-of-the-art results by achieving an overall rank-1 identification rate of 97.0% and an average verification rate of 99.01% at 0.001 false acceptance rate for all faces with neutral and non-neutral expressions. Further, the robustness of our algorithm under different occlusions has been demonstrated on the Bosphorus dataset. © 2016 Elsevier B.V.},
author_keywords={3D face recognition;  Face identification;  Facial expression;  Keypoint detection;  Local feature;  Shape matching},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lei2016994,
author={Lei, Y. and Feng, S. and Zhou, X. and Guo, Y.},
title={An efficient 3D partial face recognition approach with single sample},
journal={Proceedings of the 2016 IEEE 11th Conference on Industrial Electronics and Applications, ICIEA 2016},
year={2016},
pages={994-999},
doi={10.1109/ICIEA.2016.7603727},
art_number={7603727},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997335714&doi=10.1109%2fICIEA.2016.7603727&partnerID=40&md5=05e7f7f4b11d4caaaace67a1698a150c},
affiliation={College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan, 610065, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, 410073, China},
abstract={3D partial face recognition under missing parts, occlusions and data corruptions is a major challenge for the practical application of the techniques of 3D face recognition. Moreover, one individual can only provide one sample for training in most practical scenarios, and thus the face recognition with single sample problem is another highly challenging task. We propose an efficient framework for 3D partial face recognition with single sample addressing both of the two problems. First, we represent a facial scan with a set of keypoint based local geometrical descriptors, which gains sufficient robustness to partial facial data along with expression/pose variations. Then, a two-step modified collaborative representation classification scheme is proposed to address the single sample recognition problem. A class-based probability estimation is given during the first classification step, and the obtained result is then incorporated into the modified collaborative representation classification as a locality constraint to improve its classification performance. Extensive experiments on the Bosphorus and FRGC v2.0 datasets demonstrate the efficiency of the proposed approach when addressing the problem of 3D partial face recognition with single sample. © 2016 IEEE.},
author_keywords={3D facial representation;  3D partial face recognition;  collaborative representation;  locality constraint;  single sample problem},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2016,
author={Zhang, J. and Huang, D. and Wang, Y. and Sun, J.},
title={Lock3DFace: A large-scale database of low-cost Kinect 3D faces},
journal={2016 International Conference on Biometrics, ICB 2016},
year={2016},
doi={10.1109/ICB.2016.7550062},
art_number={7550062},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988432113&doi=10.1109%2fICB.2016.7550062&partnerID=40&md5=b25394bb9ffda22d92409c2cd0a74cfa},
affiliation={IRIP Lab, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China},
abstract={In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kim20163011,
author={Kim, D. and Choi, J. and Leksut, J.T. and Medioni, G.},
title={Accurate 3D face modeling and recognition from RGB-D stream in the presence of large pose changes},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2016},
volume={2016-August},
pages={3011-3015},
doi={10.1109/ICIP.2016.7532912},
art_number={7532912},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006789474&doi=10.1109%2fICIP.2016.7532912&partnerID=40&md5=132fb651cb89c0d50239c14c3a7104ae},
affiliation={Institute for Robotics and Intelligent Systems, University of Southern California, 3737 Watt way PHE 101, Los Angeles, CA  90089, United States},
abstract={We propose a 3D face modeling and recognition system using an RGB-D stream in the presence of large pose changes. In the previous work, all facial data points are registered with a reference to improve the accuracy of 3D face model from a low-resolution depth sequence. This registration often fails when applied to non-frontal faces. It causes inaccurate 3D face models and poor performance of matching. We address this problem by pre-aligning each input face ('frontalization') before the registration, which avoids registration failures. For each frame, our method estimates the 3D face pose, assesses the quality of data, segments the facial region, frontalizes it, and performs an accurate registration with the previous 3D model. The 3D-3D recognition system using accurate 3D models from our method outperforms other face recognition systems and shows 100% rank 1 recognition accuracy on a dataset with 30 subjects. © 2016 IEEE.},
author_keywords={3D Face Modeling;  3D Face Recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu20163016,
author={Yu, X. and Gao, Y. and Zhou, J.},
title={3D face recognition under partial occlusions using radial strings},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2016},
volume={2016-August},
pages={3016-3020},
doi={10.1109/ICIP.2016.7532913},
art_number={7532913},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006713653&doi=10.1109%2fICIP.2016.7532913&partnerID=40&md5=dd468caaf8cc8593dad5e05e0c040b9c},
affiliation={School of Engineering, Griffith University, Nathan, QLD, Australia; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia},
abstract={3D face recognition with partial occlusions is a highly challenging problem. In this paper, we propose a novel radial string representation and matching approach to recognize 3D facial scans in the presence of partial occlusions. Here we encode 3D facial surfaces into an indexed collection of radial strings emanating from the nosetips and Dynamic Programming (DP) is then used to measure the similarity between two radial strings. In order to address the recognition problems with partial occlusions, a partial matching mechanism is established in our approach that effectively eliminates those occluded parts and finds the most discriminative parts during the matching process. Experimental results on the Bosphorus database demonstrate that the proposed approach yields superior performance on partially occluded data. © 2016 IEEE.},
author_keywords={3D face recognition;  Partial occlusions;  Radial string matching;  Structural recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Torkhani2016447,
author={Torkhani, G. and Ladgham, A. and Mansouri, M.N. and Sakly, A.},
title={Gabor-SVM applied to 3D-2D deformed mesh model},
journal={2nd International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2016},
year={2016},
pages={447-452},
doi={10.1109/ATSIP.2016.7523133},
art_number={7523133},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984607903&doi=10.1109%2fATSIP.2016.7523133&partnerID=40&md5=a94aa03e640ea81c789823dfdf4c35ed},
affiliation={CSR Research Unit, E e Laboratory, National School of Engineering, Ibn Eljazzar, Monastir, 5019, Tunisia; Electrical Departement, National School of Engineering, Ibn Eljazzar, Monastir, 5019, Tunisia},
abstract={We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate. © 2016 IEEE.},
author_keywords={3D face recognition;  deformed mesh model;  facial curvatures;  Gabor wavelet;  salient points;  SVM},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Werghi2016964,
author={Werghi, N. and Tortorici, C. and Berretti, S. and Del Bimbo, A.},
title={Boosting 3D LBP-Based Face Recognition by Fusing Shape and Texture Descriptors on the Mesh},
journal={IEEE Transactions on Information Forensics and Security},
year={2016},
volume={11},
number={5},
pages={964-979},
doi={10.1109/TIFS.2016.2515505},
art_number={7373633},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963974403&doi=10.1109%2fTIFS.2016.2515505&partnerID=40&md5=376f43714248cb873d28dcb663ac1bb5},
affiliation={Electrical and Computer Engineering Department, Khalifa University, Abu Dhabi, 127788, United Arab Emirates; Department of Information Engineering, University of Florence, Florence, 50139, Italy},
abstract={In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBPs) on a mesh for 3D face recognition. Using a recently proposed framework, we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared with its depth-image counterpart, our approach is distinguished by the following features: 1) inherits the intrinsic advantages of mesh surface (e.g., preservation of the full geometry); 2) does not require normalization; and 3) can accommodate partial matching. In addition, it allows early level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data, also in comparison to the state-of-the-art solutions. © 2016 IEEE.},
author_keywords={3D face recognition;  feature and score fusion;  mesh-LBP},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ganguly2016275,
author={Ganguly, S. and Bhattachaijee, D. and Nasipuri, M.},
title={3D face recognition from complement component range face images},
journal={2015 IEEE International Conference on Computer Graphics, Vision and Information Security, CGVIS 2015},
year={2016},
pages={275-278},
doi={10.1109/CGVIS.2015.7449936},
art_number={7449936},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966632732&doi=10.1109%2fCGVIS.2015.7449936&partnerID=40&md5=ac5698f1d98fa8b29941ffc837f84a7b},
affiliation={Department of Computer Science and Engineering, Jadavpur University, Kolkata-32, India},
abstract={Face and facial attributes represent meaningful definition about a variety of information to discriminate an individual from others and for developing a computational model for automatic face recognition purpose. However, in this work, selection of relevant features from newly created face space is the pivotal contribution of the authors. Here, authors have demonstrated a new face space 'Complement Component' that have been used to extract the four basic components along X, and Y axes in four directions. Later, authors have experimented the discriminative attributes from these face spaces for recognition purpose. Here, comparison of the proposed method has been reported by examining its success on two well accepted 3D face databases, namely: Frav3D and Texas3D. In case of 2D face images, it does not contain depth like information i.e. Z-values in X-Y plane through intensity values. Therefore, it has not been undertaken during this investigation. © 2015 IEEE.},
author_keywords={3D face image;  Complement Component;  Face recognition;  range face image},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lei2016218,
author={Lei, Y. and Guo, Y. and Hayat, M. and Bennamoun, M. and Zhou, X.},
title={A Two-Phase Weighted Collaborative Representation for 3D partial face recognition with single sample},
journal={Pattern Recognition},
year={2016},
volume={52},
pages={218-237},
doi={10.1016/j.patcog.2015.09.035},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947306090&doi=10.1016%2fj.patcog.2015.09.035&partnerID=40&md5=50ca73115b78f6746739042c1f293a0e},
affiliation={College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China; School of Computer Science and Software Engineering, University of Western Australia, Crawley, WA, Australia; IBM Research Australia, Carlton, VIC, Australia},
abstract={3D face recognition with the availability of only partial data (missing parts, occlusions and data corruptions) and single training sample is a highly challenging task. This paper presents an efficient 3D face recognition approach to address this challenge. We represent a facial scan with a set of local Keypoint-based Multiple Triangle Statistics (KMTS), which is robust to partial facial data, large facial expressions and pose variations. To address the single sample problem, we then propose a Two-Phase Weighted Collaborative Representation Classification (TPWCRC) framework. A class-based probability estimation is first calculated based on the extracted local descriptors as a prior knowledge. The resulting class-based probability estimation is then incorporated into the proposed classification framework as a locality constraint to further enhance its discriminating power. Experimental results on six challenging 3D facial datasets show that the proposed KMTS-TPWCRC framework achieves promising results for human face recognition with missing parts, occlusions, data corruptions, expressions and pose variations. © 2015 Elsevier Ltd. All rights reserved.},
author_keywords={3D face recognition;  3D representation;  Partial facial data;  Single sample problem;  Sparse representation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Echeagaray-Patron2016843,
author={Echeagaray-Patron, B.A. and Miramontes-Jaramillo, D. and Kober, V.},
title={Conformal parameterization and curvature analysis for 3D facial recognition},
journal={Proceedings - 2015 International Conference on Computational Science and Computational Intelligence, CSCI 2015},
year={2016},
pages={843-844},
doi={10.1109/CSCI.2015.133},
art_number={7424213},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964412211&doi=10.1109%2fCSCI.2015.133&partnerID=40&md5=6a3fc9e7dac95a6f25aacc425aa40af5},
affiliation={Department of Computer Science, CICESE, Ensenada, B.C., 22860, Mexico; Department of Mathematics, Chelyabinsk State University, Russian Federation},
abstract={This work proposes a new algorithm for 3D face recognition. The algorithm uses 3D shape data without color or texture information and exploits local curvature information which is a measure with high discriminant capability and robust to deformations such as rotation and scaling. In order to reduce high dimensionality of typical face surfaces our approach uses a conformal parameterization, preserving angles of original faces and simplifies the correspondence problem. Experimental results are presented and discussed using CASIA and Gavab databases. © 2015 IEEE.},
author_keywords={3D face recognition;  Conformal parameterization;  Curvature analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhou2016109,
author={Zhou, W. and Chen, J.-X. and Wang, L.},
title={A RGB-D face recognition approach without confronting the camera},
journal={Proceedings of 2015 IEEE International Conference on Computer and Communications, ICCC 2015},
year={2016},
pages={109-114},
doi={10.1109/CompComm.2015.7387550},
art_number={7387550},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963938690&doi=10.1109%2fCompComm.2015.7387550&partnerID=40&md5=d5bfca843f857fa39877a526fa1c2108},
affiliation={Key Lab of Broadband Wireless Communication and Sensor Network Technology, Ministry of Education, Nanjing, 210003, China},
abstract={Face recognition research mainly focuses on traditional 2D color images, which is extremely susceptible to be affected by external factors such as various viewpoints and has limited recognition accuracy. In order to achieve improved recognition performance, as well as the 3D face holds more abundant information than 2D, we present a 3D human face recognition algorithm using the Microsoft's Kinect. The proposed approach integrates the depth data with the RGB data to generate 3D face raw data and then extracts feature points, identifies the target via a two-level cascade classifier. Also, we build a 3D-face database including 16 individuals captured exclusively using Kinect. The experimental results indicate that the introduced algorithm can not only achieve better recognition accuracy in comparison to existing 2D and 3D face recognition algorithms when the probe face is exactly in front of Kinect sensor, but also can increase 9.3% of recognition accuracy compared to the PCA-3D algorithm when it is not confronting the camera. © 2015 IEEE.},
author_keywords={3D face recognition;  classifier;  Kinect;  RGB-D images;  XML file},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Starczewski2016210,
author={Starczewski, J.T. and Pabiasz, S. and Vladymyrska, N. and Marvuglia, A. and Napoli, C. and Wózniak, M.},
title={Self organizing maps for 3D face understanding},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9693},
pages={210-217},
doi={10.1007/978-3-319-39384-1_19},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977471207&doi=10.1007%2f978-3-319-39384-1_19&partnerID=40&md5=7f993d82b7b04337f5466c3ed735ff72},
affiliation={Institute of Computational Intelligence, Czestochowa University of Technology, Czestochowa, Poland; Radom Academy of Economics, Radom, Poland; Environmental Research and Innovation Department, Luxembourg Institute of Science and Technology, Esch-sur-Alzette, Luxembourg; Department of Mathematics and Informatics, University of Catania, Catania, Italy; Institute of Mathematics, Silesian University of Technology, Gliwice, Poland},
abstract={Landmarks are unique points that can be located on every face. Facial landmarks typically recognized by people are correlated with anthropomorphic points. Our purpose is to employ in 3D face recognition such landmarks that are easy to interpret. Face understanding is construed as identification of face characteristic points with automatic labeling of them. In this paper, we apply methods based on Self Organizing Maps to understand 3D faces. © Springer International Publishing Switzerland 2016.},
author_keywords={3D face recognition;  Self organizing maps;  Understanding of images},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Galbally2016199,
author={Galbally, J. and Satta, R.},
title={Biometric sensor interoperability: A case study in 3D face recognition},
journal={ICPRAM 2016 - Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods},
year={2016},
pages={199-204},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969988967&partnerID=40&md5=80af1e7c3fa3ef4e303e144e6927af47},
affiliation={European Commission - Joint Research Centre, IPSC, Via Enrico Fermi 2749, Ispra, 21027, Italy},
abstract={Biometric systems typically suffer a significant loss of performance when the acquisition sensor is changed between enrolment and authentication. Such a problem, commonly known as sensor interoperability, poses a serious challenge to the accuracy of matching algorithms. The present work addresses for the first time the sensor interoperability issue in 3D face recognition systems, analysing the performance of two popular and well known techniques for 3D facial authentication. For this purpose, a new gender-balanced database comprising 3D data of 26 subjects has been acquired using two devices belonging to the new generation of low-cost 3D sensors. The results show the high sensor-dependency of the tested systems and the need to develop matching algorithms robust to the variation in the sensor resolution. © Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={3D face database;  3D face recognition;  Interoperability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Song2016,
author={Song, D. and Luo, J. and Zi, C. and Tian, H.},
title={3D Face Recognition Using Anthropometric and Curvelet Features Fusion},
journal={Journal of Sensors},
year={2016},
volume={2016},
doi={10.1155/2016/6859364},
art_number={6859364},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954447014&doi=10.1155%2f2016%2f6859364&partnerID=40&md5=145d972d12039ce0cfdea217aaa57214},
affiliation={College of Electrical Engineering and Automation, Tianjin Polytechnic University, Tianjin, 300387, China; Key Laboratory of Advanced Electrical Engineering and Energy Technology, Tianjin, 300387, China; School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, Sydney, NSW  2522, Australia},
abstract={Curvelet transform can describe the signal by multiple scales, and multiple directions. In order to improve the performance of 3D face recognition algorithm, we proposed an Anthropometric and Curvelet features fusion-based algorithm for 3D face recognition (Anthropometric Curvelet Fusion Face Recognition, ACFFR). First, the eyes, nose, and mouth feature regions are extracted by the Anthropometric characteristics and curvature features of the human face. Second, Curvelet energy features of the facial feature regions at different scales and different directions are extracted by Curvelet transform. At last, Euclidean distance is used as the similarity between template and objectives. To verify the performance, the proposed algorithm is compared with Anthroface3D and Curveletface3D on the Texas 3D FR database. The experimental results have shown that the proposed algorithm performs well, with equal error rate of 1.75% and accuracy of 97.0%. The algorithm we proposed in this paper has better robustness to expression and light changes than Anthroface3D and Curveletface3D. © 2016 Dan Song et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bellil2016365,
author={Bellil, W. and Brahim, H. and Ben Amar, C.},
title={Gappy wavelet neural network for 3D occluded faces: detection and recognition},
journal={Multimedia Tools and Applications},
year={2016},
volume={75},
number={1},
pages={365-380},
doi={10.1007/s11042-014-2294-6},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953638822&doi=10.1007%2fs11042-014-2294-6&partnerID=40&md5=e5d7d42e44edd21d1de8b6e4cc7bdd48},
affiliation={REGIM: REsearch Groups on Intelligent Machines, University of Sfax, National Engineering School of Sfax (ENIS), Sfax, Tunisia},
abstract={The first handicap in 3D faces recognizing under unconstrained problem is the largest variability of the visual aspect when we use various sources. This great variability complicates the task of identifying persons from their 3D facial scans and it is the most reason that bring to face detection and recognition of the major problems in pattern recognition fields, biometrics and computer vision. We propose a new 3D face identification and recognition method based on Gappy Wavelet Neural Network (GWNN) that is able to provide better accuracy in the presence of facial occlusions. The proposed approach consists of three steps: the first step is face detection. The second step is to identify and remove occlusions. Occluded regions detection is done by considering that occlusions can be defined as local face deformations. These deformations are detected by a comparison between the input facial test wavelet coefficients and wavelet coefficients of generic face model formed by the mean data base faces. They are beneficial for neighborhood relationships between pixels rotation, dilation and translation invariant. Then, occluded regions are refined by removing wavelet coefficient above a certain threshold. Finally, the last stage of processing and retrieving is made based on wavelet neural network to recognize and to restore 3D occluded regions that gathers the most. The experimental results on this challenging database demonstrate that the proposed approach improves recognition rate performance from 93.57 to 99.45 % which represents a competitive result compared to the state of the art. © 2014, Springer Science+Business Media New York.},
author_keywords={3D face recognition; Wavelets;  Gappy data;  Occlusion detection;  Wavelet neural network},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gaonkar201615,
author={Gaonkar, A.A. and Gad, M.D. and Vetrekar, N.T. and Tilve, V.S. and Gad, R.S.},
title={Experimental evaluation of 3D kinect face database},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={10481 LNCS},
pages={15-26},
doi={10.1007/978-3-319-68124-5_2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057824711&doi=10.1007%2f978-3-319-68124-5_2&partnerID=40&md5=3a104665aebdb39c053c8b83ce35c8fb},
affiliation={Department of Electronics, Goa University, Taleigao Plateau, Goa, India; Goa Engineering College, Farmagudi, Goa, India; School of Earth and Space Exploration, Arizona State University, Tempe, United States},
abstract={3D face recognition has gain a paramount importance over 2D due to its potential to address the limitations of 2D face recognition against the variation in facial poses, angles, occlusions etc. Research in 3D face recognition has accelerated in recent years due to the development of low cost 3D Kinect camera sensor. This has leads to the development of few RGB-D database across the world. Here in this paper we introduce the base results of our 3D facial database (GU-RGBD database) comprising variation in pose (0°, 45°, 90°, −45°, −90°), expression (smile, eyes closed), occlusion (half face covered with paper) and illumination variation using Kinect. We present a proposed noise removal non-linear interpolation filter for the patches present in the depth images. The results were obtained on three face recognition algorithms and fusion at matching score level for recognition and verification rate. The obtained results indicated that the performance with our proposed filter shows improvement over pose with score level fusion using sum rule. © Springer International Publishing AG 2017.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Boukamcha2015,
author={Boukamcha, H. and Elhallek, M. and Atri, M. and Smach, F.},
title={3D face landmark auto detection},
journal={2015 World Symposium on Computer Networks and Information Security, WSCNIS 2015},
year={2015},
doi={10.1109/WSCNIS.2015.7368276},
art_number={7368276},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962430025&doi=10.1109%2fWSCNIS.2015.7368276&partnerID=40&md5=13ae6691c8afd0ea13bb469882ec0aea},
affiliation={University of Sciences of Monastir, Monastir, Tunisia; Université of Bourgogne, France},
abstract={This paper presents our methodology for Landmark Point detection to improve 3D face recognition in a presence of variant facial expression. The objective was to develop an automatic process for distinguishing and segmenting to be embedded in a 3D face recognition system using only 3D Point Distribution Model (PDM) as input. The approach used hydride method to extract this features from the surface curvature information. Landmark Localization is done on the segmented face via finding the change that decreases the deviation of the model from the mean profile. Face registering is achieved using previous anthropometric information and the localized landmarks. The results confirm that the method used is accurate and robust for the proposed application. © 2015 IEEE.},
author_keywords={3D Face;  Graph Matching;  Labelling;  Registration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lv20153635,
author={Lv, S. and Da, F. and Deng, X.},
title={A 3D face recognition method using region-based extended local binary pattern},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2015},
volume={2015-December},
pages={3635-3639},
doi={10.1109/ICIP.2015.7351482},
art_number={7351482},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956668987&doi=10.1109%2fICIP.2015.7351482&partnerID=40&md5=2c0948e184ab3751c8c447b235e50e0a},
affiliation={School of Automation, Southeast University, Key Laboratory of Measurement and Control of CSE, Ministry of Education, Nanjing, 210096, China},
abstract={A 3D face recognition method using region-based extended local binary pattern (eLBP) is proposed. First, the depth image converted from the preprocessed 3D pointclouds is normalized. Then, different regions according to their distortions under facial expressions are extracted by binary masks and represented by the uniform pattern of extended LBP. Finally, sparse representation classifier (SRC) is adopted for classification on the single region. Feature-level and score-level fusion with weight-sparse representation classifier (W-SRC) are also tested and compared, and the latter has better performance. The experiments on FRGC v2.0 database demonstrate that the proposed method is robust and efficient. © 2015 IEEE.},
author_keywords={3D face recognition;  binary mask;  depth image;  extended local binary pattern;  weight-sparse representation classifier},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tortorici20152670,
author={Tortorici, C. and Werghi, N. and Berretti, S.},
title={Boosting 3D LBP-based face recognition by fusing shape and texture descriptors on the mesh},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2015},
volume={2015-December},
pages={2670-2674},
doi={10.1109/ICIP.2015.7351287},
art_number={7351287},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956639911&doi=10.1109%2fICIP.2015.7351287&partnerID=40&md5=6ea1b56b061b463d4fc6e850dc9c85e6},
affiliation={Khalifa University, Abu Dhabi, United Arab Emirates; University of Florence, Florence, Italy},
abstract={In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBP) for 3D face recognition. Using the framework proposed in [1], we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared to its depth-image counterpart, our approach is distinguished by the following features: a) inherits the intrinsic advantages of mesh surface; b) does not require normalization; c) can accommodate partial matching. In addition, it allows early-level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data. © 2015 IEEE.},
author_keywords={3D face recognition;  fusion;  mesh-LBP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Patil2015393,
author={Patil, H. and Kothari, A. and Bhurchandi, K.},
title={3-D face recognition: features, databases, algorithms and challenges},
journal={Artificial Intelligence Review},
year={2015},
volume={44},
number={3},
pages={393-441},
doi={10.1007/s10462-015-9431-0},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941419993&doi=10.1007%2fs10462-015-9431-0&partnerID=40&md5=c90b8821f623bcb40665d0010a316ef6},
affiliation={Visvesvaraya National Institute of Technology, Nagpur, India},
abstract={Face recognition is being widely accepted as a biometric technique because of its non-intrusive nature. Despite extensive research on 2-D face recognition, it suffers from poor recognition rate due to pose, illumination, expression, ageing, makeup variations and occlusions. In recent years, the research focus has shifted toward face recognition using 3-D facial surface and shape which represent more discriminating features by the virtue of increased dimensionality. This paper presents an extensive survey of recent 3-D face recognition techniques in terms of feature detection, classifiers as well as published algorithms that address expression and occlusion variation challenges followed by our critical comments on the published work. It also summarizes remarkable 3-D face databases and their features used for performance evaluation. Finally we suggest vital steps of a robust 3-D face recognition system based on the surveyed work and identify a few possible directions for research in this area. © 2015, Springer Science+Business Media Dordrecht.},
author_keywords={3-D Face databases;  3-D faces;  Biometrics;  Classifiers;  Face matching;  Face recognition;  Feature extraction},
document_type={Article},
source={Scopus},
}

@ARTICLE{Deng20155509,
author={Deng, X. and Da, F. and Shao, H.},
title={Expression-robust 3D face recognition using region-based multiscale wavelet feature fusion},
journal={Journal of Computational Information Systems},
year={2015},
volume={11},
number={15},
pages={5509-5517},
doi={10.12733/jcis14953},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950271521&doi=10.12733%2fjcis14953&partnerID=40&md5=da8d93edfc030808fb309b097e409e94},
affiliation={Department of Automation, Southeast University, Nanjing, 210096, China; Key Laboratory of Measurement and Control for Complex System of Ministry of Education, Southeast University, Nanjing, 210096, China},
abstract={In order to eliminate the impact of facial expressions and improve the efficiency of calculation, this paper proposes a novel expression-robust 3D face recognition algorithm using region-based feature fusion technique based on multiscale wavelet transformations. The discrete wavelet transformation is applied to extract frequency component features of geometric image based on the semi-rigid face region as well as the non-rigid face region in order to reduce the influence from the facial expression using the Coherent Point Drift non-rigid point set registration. The dimensionality reduction methods are utilized to promote the computational efficiency, and the experimental results show that our algorithm outperforms state-of-the-art methods based on FRGC v2.0. Copyright © 2015 Binary Information Press.},
author_keywords={3D face recognition;  Dimensionality reduction;  Multiscale wavelet transform;  Non-rigid point set registration},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20151817,
author={Zhang, C. and Gu, Y.-Z. and Hu, K.-L. and Wang, Y.-G.},
title={Face recognition using SIFT features under 3D meshes},
journal={Journal of Central South University},
year={2015},
volume={22},
number={5},
pages={1817-1825},
doi={10.1007/s11771-015-2700-x},
art_number={2700},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930008100&doi=10.1007%2fs11771-015-2700-x&partnerID=40&md5=569ac008f1ef201b7fcbffd9e0faa163},
affiliation={Key Laboratory of Wireless Sensor Network & Communication, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai, 201800, China; Department of Computer Science and Engineering, Shaoxing University, Shaoxing, 312000, China},
abstract={Expression, occlusion, and pose variations are three main challenges for 3D face recognition. A novel method is presented to address 3D face recognition using scale-invariant feature transform (SIFT) features on 3D meshes. After preprocessing, shape index extrema on the 3D facial surface are selected as keypoints in the difference scale space and the unstable keypoints are removed after two screening steps. Then, a local coordinate system for each keypoint is established by principal component analysis (PCA). Next, two local geometric features are extracted around each keypoint through the local coordinate system. Additionally, the features are augmented by the symmetrization according to the approximate left-right symmetry in human face. The proposed method is evaluated on the Bosphorus, BU-3DFE, and Gavab databases, respectively. Good results are achieved on these three datasets. As a result, the proposed method proves robust to facial expression variations, partial external occlusions and large pose changes. © 2015, Central South University Press and Springer-Verlag Berlin Heidelberg.},
author_keywords={3D face recognition;  3D meshes;  expression;  large pose changes;  occlusion;  scale-invariant feature transform (SIFT)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Beumier20152291,
author={Beumier, C.},
title={Design of coded structured light pattern for 3D facial surface capture},
journal={European Signal Processing Conference},
year={2015},
volume={06-10-September-2004},
pages={2291-2294},
art_number={7079884},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979948896&partnerID=40&md5=b09f05fe749fab292462602ba77b999e},
affiliation={Signal and Image Centre, Royal Military Academy, Avenue de la Renaissance, 30, Brussels, B-1000, Belgium},
abstract={In the context of 3D face recognition, facial surfaces are advantageously captured by a structured light acquisition system, which is typically quick, low cost and uses off-the-shelve components. The light pattern projected, a key aspect of the structured light approach, makes the major difference between developed systems. In most of them, elements of the light pattern must be identified by a property such as element thickness or colour. We present in this paper the design of projected patterns that led to the realisation of three 3D acquisition prototypes. © 2004 EUSIPCO.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Elaiwat20151235,
author={Elaiwat, S. and Bennamoun, M. and Boussaid, F. and El-Sallam, A.},
title={A Curvelet-based approach for textured 3D face recognition},
journal={Pattern Recognition},
year={2015},
volume={48},
number={4},
pages={1235-1246},
doi={10.1016/j.patcog.2014.10.013},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920743471&doi=10.1016%2fj.patcog.2014.10.013&partnerID=40&md5=dde3d88e6412c83672a1a30bbf49b936},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia; School of Sport Science, Exercise and Health, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia},
abstract={In this paper, we present a fully automated multimodal Curvelet-based approach for textured 3D face recognition. The proposed approach relies on a novel multimodal keypoint detector capable of repeatably identifying keypoints on textured 3D face surfaces. Unique local surface descriptors are then constructed around each detected keypoint by integrating Curvelet elements of different orientations, resulting in highly descriptive rotation invariant features. Unlike previously reported Curvelet-based face recognition algorithms which extract global features from textured faces only, our algorithm extracts both texture and 3D local features. In addition, this is achieved across a number of frequency bands to achieve robust and accurate recognition under varying illumination conditions and facial expressions. The proposed algorithm was evaluated using three well-known and challenging datasets, namely FRGC v2, BU-3DFE and Bosphorus datasets. Reported results show superior performance compared to prior art, with 99.2%, 95.1% and 91% verification rates at 0.001 FAR for FRGC v2, BU-3DFE and Bosphorus datasets, respectively. © 2014 Elsevier Ltd. All rights reserved.},
author_keywords={Digital Curvelet transform;  Face recognition;  Keypoint detection;  Local features},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li201575,
author={Li, Y. and Wang, Y. and Wang, B. and Sui, L.},
title={Nose tip detection on three-dimensional faces using pose-invariant differential surface features},
journal={IET Computer Vision},
year={2015},
volume={9},
number={1},
pages={75-84},
doi={10.1049/iet-cvi.2014.0070},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988269675&doi=10.1049%2fiet-cvi.2014.0070&partnerID=40&md5=9b44b1684d50589e30f8aa93abe2e4b3},
affiliation={School of Computer Science and Engineering, Xi'an University of Technology, 5 South Jinhua Road, Xi'an, 710048, China},
abstract={Three-dimensional (3D) facial data offer the potential to overcome the difficulties caused by the variation of head pose and illumination in 2D face recognition. In 3D face recognition, localisation of nose tip is essential to face normalisation, face registration and pose correction etc. Most of the existing methods of nose tip detection on 3D face deal mainly with frontal or near-frontal poses or are rotation sensitive. Many of them are training-based or model-based. In this study, a novel method of nose tip detection is proposed. Using pose-invariant differential surface features - high-order and low-order curvatures, it can detect nose tip on 3D faces under various poses automatically and accurately. Moreover, it does not require training and does not depend on any particular model. Experimental results on GavabDB verify the robustness and accuracy of the proposed method. © The Institution of Engineering and Technology 2015.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liang2015406,
author={Liang, R. and Shen, W. and Li, X.-X. and Wang, H.},
title={Bayesian multi-distribution-based discriminative feature extraction for 3D face recognition},
journal={Information Sciences},
year={2015},
volume={320},
pages={406-417},
doi={10.1016/j.ins.2015.03.063},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937521792&doi=10.1016%2fj.ins.2015.03.063&partnerID=40&md5=ea6a6994deb813d32a7a1d39602f0d94},
affiliation={College of Information Engineering, Zhejiang University of TechnologyHangzhou, China},
abstract={Due to the difficulties associated with the collection of 3D samples, 3D face recognition technologies often have to work with smaller than desirable sample sizes. With the aim of enlarging the training number for each subject, we divide each training image into several patches. However, this immediately introduces two further problems for 3D models: high computational cost and dispersive features caused by the divided 3D image patches. We therefore first map 3D face images into 2D depth images, which greatly reduces the dimension of the samples. Though the depth images retain most of the robust features of 3D images, such as pose and illumination invariance, they lose many discriminative features of the original 3D samples. In this study, we propose a Bayesian learning framework to extract the discriminative features from the depth images. Specifically, we concentrate the features of the intra-class patches to a mean feature by maximizing the multivariate Gaussian likelihood function, and, simultaneously, enlarge the distances between the inter-class mean features by maximizing the exponential priori distribution of the mean features. For classification, we use the nearest neighbor classifier combined with the Mahalanobis distance to calculate the distance between the features of the test image and items in the training set. Experiments on two widely-used 3D face databases demonstrate the efficiency and accuracy of our proposed method compared to relevant state-of-the-art methods. © 2015 Elsevier Inc. All rights reserved.},
author_keywords={3D face recognition;  Bayesian learning;  Depth image;  Single training sample per person},
document_type={Article},
source={Scopus},
}

@ARTICLE{Quan2015199,
author={Quan, W. and Matuszewski, B.J. and Shark, L.-K.},
title={3-d face recognition using geodesic-map representation and statistical shape modelling},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9493},
pages={199-212},
doi={10.1007/978-3-319-27677-9_13},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955314982&doi=10.1007%2f978-3-319-27677-9_13&partnerID=40&md5=e85737d15347f5ea197969c838869be9},
affiliation={Applied Digital Signal and Image Processing (ADSIP) Research Centre, University of Central Lancashire, Preston, PR1 2HE, United Kingdom},
abstract={3-D face recognition research has received significant attention in the past two decades because of the rapid development in imaging technology and ever increasing security demand of modern society. One of its challenges is to cope with non-rigid deformation among faces, which is often caused by the changes of appearance and facial expression. Popular solutions to deal with this problem are to detect the deformable parts of the face and exclude them, or to represent a face in terms of sparse signature points, curves or patterns that are invariant to deformation. Such approaches, however, may lead to loss of information which is important for classification. In this paper, we propose a new geodesic-map representation with statistical shape modelling for handling the non-rigid deformation challenge in face recognition. The proposed representation captures all geometrical information from the entire 3-D face and provides a compact and expression-free map that preserves intrinsic geometrical information. As a result, the search for dense points correspondence in the face recognition task can be speeded up by using a simple image-based method instead of time-consuming, recursive closest distance search in 3-D space. An experimental investigation was conducted on 3-D face scans using publicly available databases and compared with the benchmark approaches. The experimental results demonstrate that the proposed scheme provides a highly competitive new solution for 3-D face recognition. © Springer International Publishing Switzerland 2015.},
author_keywords={3-D face recognition;  Geodesic-map representation;  Non-rigid deformation;  Shape modeling},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ming201514,
author={Ming, Y.},
title={Robust regional bounding spherical descriptor for 3D face recognition and emotion analysis},
journal={Image and Vision Computing},
year={2015},
volume={35},
pages={14-22},
doi={10.1016/j.imavis.2014.12.003},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921669860&doi=10.1016%2fj.imavis.2014.12.003&partnerID=40&md5=fe785e43fe879b32d41aadd4abc61953},
affiliation={School of Electronic Engineering, Beijing Key Laboratory of Work Safety Intelligent Monitoring, Beijing University of Posts and Telecommunications, Beijing, 100876, China},
abstract={3D face recognition and emotion analysis play important roles in many fields of communication and edutainment. An effective facial descriptor, with higher discriminating capability for face recognition and higher descriptiveness for facial emotion analysis, is a challenging issue. However, in the practical applications, the descriptiveness and discrimination are independent and contradictory to each other. 3D facial data provide a promising way to balance these two aspects. In this paper, a robust regional bounding spherical descriptor (RBSR) is proposed to facilitate 3D face recognition and emotion analysis. In our framework, we first segment a group of regions on each 3D facial point cloud by shape index and spherical bands on the human face. Then the corresponding facial areas are projected to regional bounding spheres to obtain our regional descriptor. Finally, a regional and global regression mapping (RGRM) technique is employed to the weighted regional descriptor for boosting the classification accuracy. Three largest available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the performance comparison and the experimental results show a consistently better performance for 3D face recognition and emotion analysis. © 2015 Elsevier B.V. All rights reserved.},
author_keywords={3D face recognition;  Emotion analysis;  Kullback-Leiber divergence (KLD);  Regional and global regression;  Regional bounding spherical descriptor},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang201527,
author={Wang, H. and Mu, Z. and Zeng, H. and Huang, M.},
title={3D face recognition using local features matching on sphere depth representation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9428},
pages={27-34},
doi={10.1007/978-3-319-25417-3_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950245926&doi=10.1007%2f978-3-319-25417-3_4&partnerID=40&md5=a7ea7b117fa5089c7a6e2e77750229fd},
affiliation={School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China},
abstract={This paper proposes a 3D face recognition approach using sphere depth image, which is robust to pose variations in unconstrained environments. The input 3D face point clouds is first transformed into sphere depth images, and then represented as a 3DLBP image to enhance the distinctiveness of smooth and similar facial depth images. An improved SIFT algorithm is applied in the following matching process. The improved SIFT algorithm employs the learning to rank approach to select the keypoints with higher stability and repeatability instead of manually rule-based method used by the original SIFT algorithm. The proposed face recognition method is evaluated on CASIA 3D face database. And the experimental results show our approach has superior performance than many existing methods for 3D face recognition and handles pose variations quite well. © Springer International Publishing Switzerland 2015.},
author_keywords={3D face recognition;  Learning to rank;  Local binary patterns;  Sphere depth image},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ming2015352,
author={Ming, Y. and Jin, Y.},
title={Robust 3D local SIFT features for 3D face recognition},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9246},
pages={352-359},
doi={10.1007/978-3-319-22873-0_31},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985029352&doi=10.1007%2f978-3-319-22873-0_31&partnerID=40&md5=4448fcb45dc3bae2a2c99810502b1ce7},
affiliation={School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, 100044, China},
abstract={In this paper, a robust 3D local SIFT feature is proposed for 3D face recognition. For preprocessing the original 3D face data, facial regional segmentation is first employed by fusing curvature characteristics and shape band mechanism. Then, we design a new local descriptor for the extracted regions, called 3D local Scale-Invariant Feature Transform (3D LSIFT). The key point detection based on 3D LSIFT can effectively reflect the geometric characteristic of 3D facial surface by encoding the gray and depth information captured by 3D face data. Then, 3D LSIFT descriptor extends to describe the discrimination on 3D faces. Experimental results based on the common international 3D face databases demonstrate the higher-qualified performance of our proposed algorithm with effectiveness, robustness, and universality. © Springer International Publishing Switzerland 2015.},
author_keywords={3D face recognition;  3D local Scale-Invariant feature transform;  Depth information;  Facial region segmentation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Echeagaray-Patrón2015,
author={Echeagaray-Patrón, B.A. and Kober, V.},
title={3D face recognition based on matching of facial surfaces},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9598},
doi={10.1117/12.2186695},
art_number={95980V},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951325808&doi=10.1117%2f12.2186695&partnerID=40&md5=cb5a757e08c6a66683fc0706f09faf95},
affiliation={Department of Computer Science, CICESE, Ensenada, B.C.22860, Mexico; Department of Mathematics, Chelyabinsk State University, Russian Federation},
abstract={Face recognition is an important task in pattern recognition and computer vision. In this work a method for 3D face recognition in the presence of facial expression and poses variations is proposed. The method uses 3D shape data without color or texture information. A new matching algorithm based on conformal mapping of original facial surfaces onto a Riemannian manifold followed by comparison of conformal and isometric invariants computed in the manifold is suggested. Experimental results are presented using common 3D face databases that contain significant amount of expression and pose variations. © 2015 SPIE.},
author_keywords={3D face recognition;  3D facial shape analysis;  Conformal mapping},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo2015,
author={Guo, Z. and Liu, S. and Wang, Y. and Lei, T.},
title={Learning deformation model for expression-robust 3D face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9817},
doi={10.1117/12.2228002},
art_number={98170O},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028295582&doi=10.1117%2f12.2228002&partnerID=40&md5=42fcb3ae1a1a8506c08003ac3fd73bc1},
affiliation={School of Electronics and Information, Northwestern Polytechnical University, Xi'an, 710072, China; School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, 730070, China},
abstract={Expression change is the major cause of local plastic deformation of the facial surface. The intra-class differences with large expression change somehow are larger than the inter-class differences as it's difficult to distinguish the same individual with facial expression change. In this paper, an expression-robust 3D face recognition method is proposed by learning expression deformation model. The expression of the individuals on the training set is modeled by principal component analysis, the main components are retained to construct the facial deformation model. For the test 3D face, the shape difference between the test and the neutral face in training set is used for reconstructing the expression change by the constructed deformation model. The reconstruction residual error is used for face recognition. The average recognition rate on GavabDB and self-built database reaches 85.1% and 83%, respectively, which shows strong robustness for expression changes. © 2015 SPIE.},
author_keywords={3D face recognition;  Facial deformation model;  Principal component analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Luo2015,
author={Luo, J. and Geng, S.Z. and Xiao, Z.X. and Xiu, C.B.},
title={A review of recent advances in 3d face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9443},
doi={10.1117/12.2178750},
art_number={944303},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925431201&doi=10.1117%2f12.2178750&partnerID=40&md5=f8e793b50759be140f6e5bf45667a121},
affiliation={Key Laboratory of Advanced of Electrical Engineering and Energy Technology, Tianjin Polytechnic University, Tianjin, 300387, China; College of Electrical Engineering and Automation, Tianjin Polytechnic University, Tianjin, 300387, China},
abstract={Face recognition based on machine vision has achieved great advances and been widely used in the various fields. However, there are some challenges on the face recognition, such as facial pose, variations in illumination, and facial expression. So, this paper gives the recent advances in 3D face recognition. 3D face recognition approaches are categorized into four groups: minutiae approach, space transform approach, geometric features approach, model approach. Several typical approaches are compared in detail, including feature extraction, recognition algorithm, and the performance of the algorithm. Finally, this paper summarized the challenge existing in 3D face recognition and the future trend. This paper aims to help the researches majoring on face recognition. © 2015 SPIE.},
author_keywords={3D face recognition;  geometric features;  minutiae approach.;  space transform},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Belghini2015317,
author={Belghini, N. and Ezghari, S. and Zahi, A.},
title={3D face recognition using facial curves, sparse random projection and fuzzy similarity measure},
journal={Colloquium in Information Science and Technology, CIST},
year={2015},
volume={2015-January},
number={January},
pages={317-322},
doi={10.1109/CIST.2014.7016639},
art_number={7016639},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938075627&doi=10.1109%2fCIST.2014.7016639&partnerID=40&md5=bca04cb8b40640821dbbc277a0cf6e71},
affiliation={System Intelligent and Application Laboratory (SIA) FST, Fez, Morocco},
abstract={In this paper, we propose a fuzzy similarity based classification approach for 3D face recognition. In the feature extraction method, we exploit curve concept to represent the 3D facial data, two types of curves was considered: depth-level and depth-radial curves. As the dimension of the obtained features is high, the problem 'curse of dimensionality' appears. To solve this problem, the Random Projection (RP) method was used. The proposed classifier performs Fuzzification operation using triangular membership functions for input data and ordered weighted averaging operators to measure similarity. Experiment was conducted using vrml files from 3D Database considering only one training sample per person. The obtained results are very promising for depth-level and depth-radial curves, besides the recognition rates are higher than 98%. © 2014 IEEE.},
author_keywords={3D face recognition;  facial curves;  fuzzy logic;  OWA operator;  similarity measure;  sparse random projection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tang2015466,
author={Tang, Y. and Sun, X. and Huang, D. and Morvan, J.-M. and Wang, Y. and Chen, L.},
title={3D face recognition with asymptotic cones based principal curvatures},
journal={Proceedings of 2015 International Conference on Biometrics, ICB 2015},
year={2015},
pages={466-472},
doi={10.1109/ICB.2015.7139111},
art_number={7139111},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943279293&doi=10.1109%2fICB.2015.7139111&partnerID=40&md5=bd52f57fa0f04d14b00dab8a5d4e64e2},
affiliation={Université de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, Lyon, 69134, France; King Abdullah University of Science and Technology, V.C.C. Research Center, Thuwal, 23955-6900, Saudi Arabia; IRIP, School of Computer Science and Engineering, Beihang Universtiy, Beijing, 100191, China; Université de Lyon, CNRS, Université Claude Bernard Lyon 1, ICJ UMR 5208, Villeurbanne, F-69622, France},
abstract={The classical curvatures of smooth surfaces (Gaussian, mean and principal curvatures) have been widely used in 3D face recognition (FR). However, facial surfaces resulting from 3D sensors are discrete meshes. In this paper, we present a general framework and define three principal curvatures on discrete surfaces for the purpose of 3D FR. These principal curvatures are derived from the construction of asymptotic cones associated to any Borel subset of the discrete surface. They describe the local geometry of the underlying mesh. First two of them correspond to the classical principal curvatures in the smooth case. We isolate the third principal curvature that carries out meaningful geometric shape information. The three principal curvatures in different Borel subsets scales give multi-scale local facial surface descriptors. We combine the proposed principal curvatures with the LNP-based facial descriptor and SRC for recognition. The identification and verification experiments demonstrate the practicability and accuracy of the third principal curvature and the fusion of multi-scale Borel subset descriptors on 3D face from FRGC v2.0. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ratyal2015241,
author={Ratyal, N.I. and Taj, I.A. and Bajwa, U.I. and Sajid, M.},
title={3D face recognition based on pose and expression invariant alignment},
journal={Computers and Electrical Engineering},
year={2015},
volume={46},
pages={241-255},
doi={10.1016/j.compeleceng.2015.06.007},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931030581&doi=10.1016%2fj.compeleceng.2015.06.007&partnerID=40&md5=a95f41db3d492d423152cf1d2907956a},
affiliation={Vision and Pattern Recognition Systems Research Group, Mohammad Ali Jinnah University, Islamabad, Pakistan; Department of Computer Science, COMSATS Institute of Information Technology, Lahore, Pakistan},
abstract={In this paper we present a novel pose and expression invariant approach for 3D face registration based on intrinsic coordinate system characterized by nose tip, horizontal nose plane and vertical symmetry plane of the face. It is observed that distance of nose tip from 3D scanner is reduced after pose correction which is presented as a quantifying heuristic for proposed registration scheme. In addition, motivated by the fact that a single classifier cannot be generally efficient against all face regions, a two tier ensemble classifier based 3D face recognition approach is presented which employs Principal Component Analysis (PCA) for feature extraction and Mahalanobis Cosine (MahCos) matching score for classification of facial regions with weighted Borda Count (WBC) based combination and a re-ranking stage. The performance of proposed approach is corroborated by extensive experiments performed on two databases: GavabDB and FRGC v2.0, confirming effectiveness of fusion strategies to improve performance. © 2015 Elsevier Ltd. All rights reserved.},
author_keywords={3D face recognition;  3D registration;  Ensemble classifier;  Fusion;  Intrinsic coordinate system},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wang2015192,
author={Wang, X. and Ruan, Q. and Jin, Y. and An, G.},
title={3D face recognition using closest point coordinates and spherical vector norms},
journal={IET Conference Publications},
year={2015},
volume={2015},
number={CP681},
pages={192-196},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983122121&partnerID=40&md5=4a26023e54f7fb05eaa6d68668c88d60},
affiliation={Institution of Information Science, Beijing Jiaotong University, Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, 100044, China},
abstract={In this paper, we introduce a new feature named spherical vector norms for 3D face recognition. The proposed feature is efficient, insensitive to facial expression and contains discriminatory information of 3D face. The feature extraction method is firstly finding a set of the points with the closest distance to the standard face, denoted as closest point coordinates, and then extracting the spherical vector norms of these points. This paper combines point coordinates and spherical vector norms for improving recognition. Finally this approach is finished by Linear Discriminant Analysis (LDA) and Nearest Neighbor classifier. We have performed different experiments on the Face Recognition Grand Challenge database. It achieves the verification rate of 97.11% on All vs. All experiment at 0.1% FAR and 96.64% verification rate on Neutral vs. Expression experiment.},
author_keywords={3D face recognition;  Linear discriminant analysis;  Spherical vector norms},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20153357,
author={Zhang, C. and Gu, Y. and Wang, Y. and Li, F. and Zhan, Y. and Pi, J. and Qu, L.},
title={Adaptive multiple regions matching for 3D face recognition under expression and pose variations},
journal={Journal of Computational Information Systems},
year={2015},
volume={11},
number={9},
pages={3357-3369},
doi={10.12733/jcis14297},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938320850&doi=10.12733%2fjcis14297&partnerID=40&md5=af76d7b96bb6eca2db8bc3e1babc780a},
affiliation={Key Laboratory of Wireless Sensor Network & Communication, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai, 200050, China; University of Chinese Academy of Sciences, Beijing, 100049, China; Shanghai Internet of Things Co., Ltd., Shanghai, 201800, China},
abstract={Expression and pose variations are two major challenges for 3D face recognition. This paper presents a method to cope with these two challenges by fusing the matching results of adaptive multiple regions on the 3D face. First, one approach is proposed for pose correction of 3D face based on three landmark points: nose tip, nasion, and subnasale. Then multiple regions are adaptively chosen from the facial surface, which include nose, left and right eye-forehead regions, left and right cheeks, and mouth-chin region. Next, a least trimmed square Hausdorff distance method is applied for region matching. Moreover, to obtain a better overall performance, several score-level and rank-level fusion schemes are used to fuse the contribution of each region. The proposed approach is evaluated on the Bosphorus and the BU-3DFE databases, and yields good results. The study shows that the proposed algorithm is robust to expression and pose changes. ©, 2015, Binary Information Press. All right reserved.},
author_keywords={3D face recognition;  Expression;  Pose correction;  Pose rotation;  Region matching},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chouchane2015,
author={Chouchane, A. and Belahcene, M. and Ouamane, A. and Bourennane, S.},
title={3D face recognition based on histograms of local descriptors},
journal={2014 4th International Conference on Image Processing Theory, Tools and Applications, IPTA 2014},
year={2015},
doi={10.1109/IPTA.2014.7001925},
art_number={7001925},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921721830&doi=10.1109%2fIPTA.2014.7001925&partnerID=40&md5=4bbf52130cd2484d05cce4611c376687},
affiliation={LMSE, University of Biskra, Biskra, R.P.07000, Algeria; Centre de Développement des Technologies Avancées, ASM Alger, Algeria; Institut Fresnel, UMR CNRS 7249, Ecole Centrale Marseille, France},
abstract={Face recognition in an uncontrolled condition such as illumination and expression variations is a challenging task. Local descriptor is one of the most efficient methods used to deal with these problems. In this paper, we present an automatic 3D face recognition approach based on three local descriptors, local phase quantization (LPQ), Three-Patch Local Binary Patterns (TPLBP) and Four-Patch Local Binary Patterns (TPLBP). Facial images are passing through one of the three descriptors and divided into sub-regions or rectangular blocks. The histogram of each sub-region is extracted and concatenated into a single feature vector. PCA (Principal Component Analysis) and EFM (Enhanced Fisher linear discriminant Model) are used to reduce the dimensionality of the resulting feature vectors. Finally, these vectors are sent to the classification step, when we use two methods; SVM (Support Victor Machine) and similarity measures. CASIA 3D face database is introduced to experimental evaluation. The experimental results illustrate a high recognition performance of the proposed approach. © 2014 IEEE.},
author_keywords={3D face recognition;  FPLBP;  Local phase quantization;  Locale descriptors;  Support vector machines;  TPLBP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chouchane201550,
author={Chouchane, A. and Belahcene, M. and Bourennane, S.},
title={3D and 2D face recognition using integral projection curves based depth and intensity images},
journal={International Journal of Intelligent Systems Technologies and Applications},
year={2015},
volume={14},
number={1},
pages={50-69},
doi={10.1504/IJISTA.2015.072219},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964744543&doi=10.1504%2fIJISTA.2015.072219&partnerID=40&md5=e5008a255876319ef73de7c015360bc8},
affiliation={Faculty of Science and Technology, Department of Electrical Engineering, University of Mohamed Khider, Biskra, BP 145 RP, Biskra, 07000, Algeria; Institut Fresnel, UMR CNRS 7249, Ecole Centrale Marseille, France},
abstract={This paper presents an automatic face recognition system in the presence of illumination, expressions and pose variations based on depth and intensity information. At first, the registration of 3D faces is achieved using iterative closest point (ICP). Nose tip point must be located using Maximum Intensity Method. This point usually has the largest depth value; however there is a problem with some unnecessary data such as: shoulders, hair, neck and parts of clothes; to cope with this issue, we propose the integral projection curves (IPC)-based facial area segmentation to extract the facial area. After that, the combined method principal component analysis (PCA) with enhanced fisher model (EFM) is used to obtain the feature matrix vectors. Finally, the classification is performed using distance measurement and support vector machine (SVM). The experiments are implemented on two face databases CASIA3D and GavabDB; our results show that the proposed method achieves a high recognition performance. Copyright © 2015 Inderscience Enterprises Ltd.},
author_keywords={2D and 3D face recognition;  EFM;  Enhanced fisher model;  IPC-based facial area segmentation;  Nose tip;  PCA;  Principal component analysis;  Support vector machine;  SVM},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lagorio2015,
author={Lagorio, A. and Cadoni, M. and Grosso, E. and Tistarelli, M.},
title={A 3D algorithm for unsupervised face identification},
journal={3rd International Workshop on Biometrics and Forensics, IWBF 2015},
year={2015},
doi={10.1109/IWBF.2015.7110239},
art_number={7110239},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936101701&doi=10.1109%2fIWBF.2015.7110239&partnerID=40&md5=65b942c6e6bd6ec99d853a5a632cd56f},
affiliation={VisionLab - Computer Vision Laboratory, United Kingdom},
abstract={With the increasing availability of low-cost 3D data acquisition devices, the use of 3D face data for the recognition of individuals is becoming more appealing and computationally feasible. This paper proposes a completely automatic algorithm for face registration and matching. The algorithm is based on the extraction of stable 3D facial features characterizing the face and the subsequent construction of a signature manifold. The facial features are extracted by performing a continuous-to-discrete scale-space analysis. Registration is driven from the matching of triplets of feature points and the registration error is computed as shape matching score. Conversely to most techniques in the literature, a major advantage of the proposed method is that no data pre-processing is required. Therefore all presented results have been obtained exclusively from the raw data available from the 3D acquisition device. The method has been tested on the Bosphorus 3D face database and the performances compared to the ICP baseline algorithm. Even in presence of noise in the data, the algorithm proved to be very robust and reported identification performances which are aligned to the current state of the art, but without requiring any pre-processing of the raw data. © 2015 IEEE.},
author_keywords={3D Face recognition;  Face recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Said2015,
author={Said, S. and Jemai, O. and Zaied, M. and Ben Amar, C.},
title={3D fast wavelet network model-assisted 3D face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9875},
doi={10.1117/12.2228368},
art_number={98750E},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958206236&doi=10.1117%2f12.2228368&partnerID=40&md5=e95e90acc2067f0a5a38e384b37d87d2},
affiliation={REsearch Groups in Intelligent Machines (REGIM-Lab), University of Sfax, National Engineering School of Sfax, BP 1173, Sfax, 3038, Tunisia},
abstract={In last years, the emergence of 3D shape in face recognition is due to its robustness to pose and illumination changes. These attractive benefits are not all the challenges to achieve satisfactory recognition rate. Other challenges such as facial expressions and computing time of matching algorithms remain to be explored. In this context, we propose our 3D face recognition approach using 3D wavelet networks. Our approach contains two stages: learning stage and recognition stage. For the training we propose a novel algorithm based on 3D fast wavelet transform. From 3D coordinates of the face (x,y,z), we proceed to voxelization to get a 3D volume which will be decomposed by 3D fast wavelet transform and modeled after that with a wavelet network, then their associated weights are considered as vector features to represent each training face. For the recognition stage, an unknown identity face is projected on all the training WN to obtain a new vector features after every projection. A similarity score is computed between the old and the obtained vector features. To show the efficiency of our approach, experimental results were performed on all the FRGC v.2 benchmark. © 2015 SPIE.},
author_keywords={3D face recognition;  Fast wavelet transform;  Wavelet network},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2015151,
author={Liu, S. and Mu, Z. and Huang, H.},
title={3D face recognition fusing spherical depth map and spherical texture map},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9428},
pages={151-159},
doi={10.1007/978-3-319-25417-3_19},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950277767&doi=10.1007%2f978-3-319-25417-3_19&partnerID=40&md5=828cf646ebe1b2dd0a70ea3a6fdb0939},
affiliation={School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Computing Center, Beijing Information Science and Technology University, Beijing, 100192, China},
abstract={Face recognition in unconstrained environments is often influenced by pose variations. And the problem is basically the identification that uses partial data. In this paper, a method fusing structure and texture information is proposed to solve the problem. In the register phase, the approximate 180 degree information of face is acquired, and the data used to identify individual is obtained from a random single view. Pure face is extracted from 3D data first, then convert the original data to the form of spherical depth map (SDM) and spherical texture map (STM), which are invariant to out-plane rotation, subsequently facilitating the successive alignment-free identification that is robust to pose variations. We make identification through sparse representation for its well performance with the two maps. Experiments show that our proposed method gets a high recognition rate with pose and expression variations. © Springer International Publishing Switzerland 2015.},
author_keywords={Face recognition;  Sparse representation;  Spherical Depth Map;  Spherical Texture Map},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Svoboda2015452,
author={Svoboda, J. and Bronstein, M.M. and Drahansky, M.},
title={Contactless biometric hand geometry recognition using a low-cost 3D camera},
journal={Proceedings of 2015 International Conference on Biometrics, ICB 2015},
year={2015},
pages={452-457},
doi={10.1109/ICB.2015.7139109},
art_number={7139109},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943329089&doi=10.1109%2fICB.2015.7139109&partnerID=40&md5=088b980bbb44eef1cfc61d3c229e1b06},
affiliation={Faculty of Informatics, Universita della Svizzera Italiana, Lugano, Switzerland; Faculty of Information Technology, Brno University of Technology, Brno, Czech Republic},
abstract={In the past decade, the interest in using 3D data for biometric person authentication has increased significantly, propelled by the availability of affordable 3D sensors. The adoption of 3D features has been especially successful in face recognition applications, leading to several commercial 3D face recognition products. In other biometric modalities such as hand recognition, several studies have shown the potential advantage of using 3D geometric information, however, no commercial-grade systems are currently available. In this paper, we present a contactless 3D hand recognition system based on the novel Intel RealSense camera, the first mass-produced embeddable 3D sensor. The small form factor and low cost make this sensor especially appealing for commercial biometric applications, however, they come at the price of lower resolution compared to more expensive 3D scanners used in previous research. We analyze the robustness of several existing 2D and 3D features that can be extracted from the images captured by the RealSense camera and study the use of metric learning for their fusion. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tian2015499,
author={Tian, L. and Fan, C. and Ming, Y. and Shi, J.},
title={SRDANet: An efficient deep learning algorithm for face analysis},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9244},
pages={499-510},
doi={10.1007/978-3-319-22879-2_46},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985006693&doi=10.1007%2f978-3-319-22879-2_46&partnerID=40&md5=677312dc329735134faf0a8467c2ea50},
affiliation={Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China},
abstract={In this work, we take advantage of the superiority of Spectral Graph Theory in classification application and propose a novel deep learning framework for face analysis which is called Spectral Regression Discriminant Analysis Network (SRDANet). Our SRDANet model shares the same basic architecture of Convolutional Neural Network (CNN),which comprises three basic components: convolutional filter layer, nonlinear processing layer and feature pooling layer. While it is different from traditional deep learning network that in our convolutional layer, we extract the leading eigenvectors from patches in facial image which are used as filter kernels instead of randomly initializing kernels and update them by stochastic gradient descent (SGD). And the output of all cascaded convolutional filter layers is used as the input of nonlinear processing layer. In the following nonlinear processing layer, we use hashing method for nonlinear processing. In feature pooling layer, the block-based histograms are employed to pooling output features instead of max-pooling technique. At last, the output of feature pooling layer is considered as one final feature output of our model. Different from the previous single-task research for face analysis, our proposed approach demonstrates an excellent performance in face recognition and expression recognition with 2D/3D facial images simultaneously. Extensive experiments conducted on many different face analysis databases demonstrate the efficiency of our proposed SRDANet model. Databases such as Extended Yale B, PIE, ORL are used for 2D face recognition, FRGC v2 is used for 3D face recognition and BU-3DFE is used for 3D expression recognition. © Springer International Publishing Switzerland 2015.},
author_keywords={Deep learning;  Expression recognition;  Face recognition;  Spectral regression discriminant analysis;  SRDA network},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{MatíasdiMartino2015176,
author={Matías di Martino, J. and Fernández, A. and Ferrari, J.},
title={One-shot 3D-gradient method applied to face recognition},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9423},
pages={176-183},
doi={10.1007/978-3-319-25751-8_22},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983485990&doi=10.1007%2f978-3-319-25751-8_22&partnerID=40&md5=d1ca3b0d2eb47528de1f16f2734dc918},
affiliation={Facultad de Ingeniería, Universidad de la República, Montevideo, Uruguay},
abstract={In this work we describe a novel one-shot face recognition setup. Instead of using a 3D scanner to reconstruct the face, we acquire a single photo of the face of a person while a rectangular pattern is been projected over it. Using this unique image, it is possible to extract 3D low-level geometrical features without the explicit 3D reconstruction. To handle expression variations and occlusions that may occur (e.g. wearing a scarf or a bonnet), we extract information just from the eyes-forehead and nose regions which tend to be less influenced by facial expressions. Once features are extracted, SVM hyper-planes are obtained from each subject on the database (one vs all approach), then new instances can be classified according to its distance to each of those hyper-planes. The advantage of our method with respect to other ones published in the literature, is that we do not need and explicit 3D reconstruction. Experiments with the Texas 3D Database and with new acquired data are presented, which shows the potential of the presented framework to handle different illumination conditions, pose and facial expressions. © Springer International Publishing Switzerland 2015.},
author_keywords={3D face recognition;  Differential 3D reconstruction},
document_type={Conference Paper},
source={Scopus},
}
﻿
@article{ ISI:000463151400027,
Author = {Rasouli, Maryam S. D. and Payandeh, Shahram},
Title = {{A novel depth image analysis for sleep posture estimation}},
Journal = {{JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING}},
Year = {{2019}},
Volume = {{10}},
Number = {{5, SI}},
Pages = {{1999-2014}},
Month = {{MAY}},
Abstract = {{Recognition of sleep posture and its changes are related to information
   monitoring in a number of health-related applications such as apnea
   prevention and elderly care. This paper uses a less privacy-invading
   approach to classify sleep postures of a person in various
   configurations including side and supine postures. In order to
   accomplish this, a single depth sensor has been utilized to collect
   selective depth signals and populated a dataset associated with the
   depth data. The data is then analyzed by a novel frequency-based feature
   selection approach. These extracted features were then correlated in
   order to rank their information content in various 2D scans from the 3D
   point cloud in order to train a support vector machine (SVM). The data
   of subjects are collected under two conditions. First when they were
   covered with a thin blanket and second without any blanket. In order to
   reduce the dimensionality of the feature space, a T-test approach is
   employed to determine the most dominant set of features in the frequency
   domain. The proposed recognition approach based on the frequency domain
   is also compared with an approach using feature vector defined based on
   skeleton joints. The comparative studies are performed given various
   scenarios and by a variety of datasets. Through our study, it is shown
   that our proposed method offers better performance to that of the
   joint-based method.}},
DOI = {{10.1007/s12652-018-0796-1}},
ISSN = {{1868-5137}},
EISSN = {{1868-5145}},
Unique-ID = {{ISI:000463151400027}},
}

@article{ ISI:000458711300008,
Author = {Patruno, Cosimo and Marani, Roberto and Cicirelli, Grazia and Stella,
   Ettore and D'Orazio, Tiziana},
Title = {{People re-identification using skeleton standard posture and color
   descriptors from RGB-D data}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{89}},
Pages = {{77-90}},
Month = {{MAY}},
Abstract = {{This paper tackles the problem of people re-identification by using soft
   biometrics features. The method works on RGB-D data (color point clouds)
   to determine the best matching among a database of possible users. For
   each subject under testing, skeletal information in three-dimensions is
   used to regularize the pose and to create a skeleton standard posture
   (SSP). A partition grid, whose sizes depend on the SSP, groups the
   samples of the point cloud accordingly to their position. Every group is
   then studied to build the person signature. The same grid is then used
   for the other subjects of the database to preserve information about
   possible shape differences among users. The effectiveness of this novel
   method has been tested on three public datasets. Numerical experiments
   demonstrate an improvement of results with reference to the current
   state-of-the-art, with recognition rates of 97.84\% (on a partition of
   BIWI RGBD-ID), 61.97\% (KinectREID) and 89.71\% (RGBD-ID), respectively.
   (C) 2019 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2019.01.003}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{D'Orazio, Tiziana/H-5032-2019}},
ORCID-Numbers = {{D'Orazio, Tiziana/0000-0003-1473-7110}},
Unique-ID = {{ISI:000458711300008}},
}

@article{ ISI:000457666900036,
Author = {Lv, Chenlei and Wu, Zhongke and Wang, Xingce and Zhou, Mingquan and Toh,
   Kar-Ann},
Title = {{Nasal similarity measure of 3D faces based on curve shape space}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{88}},
Pages = {{458-469}},
Month = {{APR}},
Abstract = {{We propose a novel method for measuring the nasal similarity among 3D
   faces. Firstly, we construct a representation for the nose shape, which
   is composed of a set of geodesic curves, each crosses the bridge of the
   nose. Next, using these geodesic curves, we formulate a similarity
   measure to compare among noses in the curve shape space. Under the
   Riemannian framework, the shape space is a quotient space for which the
   scaling, translation and rotation are removed. Since the nose similarity
   measure is based on the shape comparison, the proposed method has the
   following advantages: (1) the similarity measure is robust to facial
   expressions since the nose is not affected by facial expressions; (2)
   the geometric features of the nose shape match well with the human
   perception; (3) the similarity measure is independent of the mesh grid
   because the chosen nose curves are not sensitive to the triangular mesh
   model. We construct a nasal hierarchical structure for noses
   organization which is based on nose similarity measure results. In our
   experiments, we evaluate the performance of the proposed method and
   compare it with competing methods on three public face databases namely,
   FRGC2.0, Texas3D and BosphorusDB. The results show superiority of the
   proposed method in terms of both the speed and the accuracy when the
   nasal measurements are processed in the nasal hierarchical structure and
   the nasal samples with low sampling rate (5\%-25\% of original point
   cloud). (C) 2018 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.12.006}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000457666900036}},
}

@article{ ISI:000457666900042,
Author = {Pribanic, Tomislav and Petkovic, Tomislav and Donlic, Matea},
Title = {{3D registration based on the direction sensor measurements}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{88}},
Pages = {{532-546}},
Month = {{APR}},
Abstract = {{3D registration is a very active topic, spanning research areas such as
   computational geometry, computer graphics and pattern recognition. It
   aims to solve spatial transformation that aligns two point clouds. In
   this work we propose the use of a single direction sensor, such as an
   accelerometer or a magnetometer, commonly available on contemporary
   mobile platforms, such as tablets and smartphones. Both sensors have
   been heavily investigated earlier, but only for joint use with other
   sensors, such as gyroscopes and GPS. We show a time-efficient and
   accurate 3D registration method that takes advantage of only either an
   accelerometer or a magnetometer. We demonstrate a 3D reconstruction of
   individual point clouds and the proposed 3D registration method on a
   tablet equipped with an accelerometer or a magnetometer. However, we
   point out that the proposed method is not restricted to mobile
   platforms. Indeed, it can easily be applied in any 3D measurement system
   that is upgradable with some ubiquitous direction sensor, for example by
   adding a smartphone equipped with either an accelerometer or a
   magnetometer. We compare the proposed method against several
   state-of-the-art methods implemented in the open source Point Cloud
   Library (PCL). The proposed method outperforms the PCL methods tested,
   both in terms of processing time and accuracy. (C) 2018 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.12.008}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Petkovic, Tomislav/0000-0002-3054-002X
   Donlic, Matea/0000-0001-5165-6438}},
Unique-ID = {{ISI:000457666900042}},
}

@article{ ISI:000460046500034,
Author = {Alameda-Hernandez, Pedro and El Hamdouni, Rachid and Irigaray, Clemente
   and Chacon, Jose},
Title = {{Weak foliated rock slope stability analysis with ultra-close-range
   terrestrial digital photogrammetry}},
Journal = {{BULLETIN OF ENGINEERING GEOLOGY AND THE ENVIRONMENT}},
Year = {{2019}},
Volume = {{78}},
Number = {{2}},
Pages = {{1157-1171}},
Month = {{MAR}},
Abstract = {{This paper presents a review of the data acquisition procedures of
   geotechnical parameters for rock slope stability assessment and the
   proposal of some new improvements. For this purpose, a piece of research
   based on the slope mass rating classification system using close-range
   terrestrial digital photogrammetry (CR-TDP) has led to improvements in
   quality and timing of discontinuity data acquisition, and analyzes the
   suitability of each one of the parameters when applied to weak foliated
   rocks. TDP allows rapid 3D image acquisition of a rock slope, which can
   be analyzed using software to determine the geometrical parameters that
   affect stability. A fast procedure to perform the photogrammetric,
   non-contact survey in order to obtain the 3D images is shown in this
   paper. Being a rapid and single-person task, this procedure provides
   enough precision to be applied to weak foliated rock slopes with
   non-well-defined geometry. Furthermore, the study has focused on highly
   foliated rock outcrops, in which high resolution in the 3D images is
   very desirable. This research was applied to mountain road cuts, in
   which the use of TDP with a very close range was necessary. Through an
   application on weak rocks in the Alpujarras (Andalusia, Spain), this
   work analyzes the bias when applying TDP to materials such as these,
   under progressive weathering processes.}},
DOI = {{10.1007/s10064-017-1119-z}},
ISSN = {{1435-9529}},
EISSN = {{1435-9537}},
ResearcherID-Numbers = {{Hamdouni, Rachid El/L-1672-2017
   }},
ORCID-Numbers = {{Hamdouni, Rachid El/0000-0002-1271-3839
   Alameda Hernandez, Pedro Manuel/0000-0003-1830-1912}},
Unique-ID = {{ISI:000460046500034}},
}

@article{ ISI:000457692800013,
Author = {Abu, Arpah and Ngo, Chee Guan and Abu-Hassan, Nur Idayu Adira and
   Othman, Siti Adibah},
Title = {{Automated craniofacial landmarks detection on 3D image using geometry
   characteristics information}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2019}},
Volume = {{19}},
Number = {{13}},
Month = {{FEB 4}},
Note = {{17th International Conference on Bioinformatics (InCoB), Jawaharlal
   Nehru Univ, New Delhi, INDIA, SEP 26-28, 2018}},
Abstract = {{BackgroundIndirect anthropometry (IA) is one of the craniofacial
   anthropometry methods to perform the measurements on the digital facial
   images. In order to get the linear measurements, a few definable points
   on the structures of individual facial images have to be plotted as
   landmark points. Currently, most anthropometric studies use landmark
   points that are manually plotted on a 3D facial image by the examiner.
   This method is time-consuming and leads to human biases, which will vary
   from intra-examiners to inter-examiners when involving large data sets.
   Biased judgment also leads to a wider gap in measurement error. Thus,
   this work aims to automate the process of landmarks detection to help in
   enhancing the accuracy of measurement. In this work, automated
   craniofacial landmarks (ACL) on a 3D facial image system was developed
   using geometry characteristics information to identify the nasion (n),
   pronasale (prn), subnasale (sn), alare (al), labiale superius (ls),
   stomion (sto), labiale inferius (li), and chelion (ch). These landmarks
   were detected on the 3D facial image in .obj file format. The IA was
   also performed by manually plotting the craniofacial landmarks using
   Mirror software. In both methods, once all landmarks were detected, the
   eight linear measurements were then extracted. Paired t-test was
   performed to check the validity of ACL (i) between the subjects and (ii)
   between the two methods, by comparing the linear measurements extracted
   from both ACL and AI. The tests were performed on 60 subjects (30 males
   and 30 females).ResultsThe results on the validity of the ACL against IA
   between the subjects show accurate detection of n, sn, prn, sto, ls and
   li landmarks. The paired t-test showed that the seven linear
   measurements were statistically significant when p<0.05. As for the
   results on the validity of the ACL against IA between the methods, ACL
   is more accurate when p approximate to 0.03.ConclusionsIn conclusion,
   ACL has been validated with the eight landmarks and is suitable for
   automated facial recognition. ACL has proved its validity and
   demonstrated the practicability to be used as an alternative for IA, as
   it is time-saving and free from human biases.}},
DOI = {{10.1186/s12859-018-2548-9}},
Article-Number = {{548}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{OTHMAN, SITI ADIBAH BINTI/B-9784-2010
   Management Center, Dental Research/C-2478-2013}},
ORCID-Numbers = {{OTHMAN, SITI ADIBAH BINTI/0000-0002-3846-7701
   }},
Unique-ID = {{ISI:000457692800013}},
}

@article{ ISI:000460829200061,
Author = {Che, Erzhuo and Jung, Jaehoon and Olsen, Michael J.},
Title = {{Object Recognition, Segmentation, and Classification of Mobile Laser
   Scanning Point Clouds: A State of the Art Review}},
Journal = {{SENSORS}},
Year = {{2019}},
Volume = {{19}},
Number = {{4}},
Month = {{FEB 2}},
Abstract = {{Mobile Laser Scanning (MLS) is a versatile remote sensing technology
   based on Light Detection and Ranging (lidar) technology that has been
   utilized for a wide range of applications. Several previous reviews
   focused on applications or characteristics of these systems exist in the
   literature, however, reviews of the many innovative data processing
   strategies described in the literature have not been conducted in
   sufficient depth. To this end, we review and summarize the state of the
   art for MLS data processing approaches, including feature extraction,
   segmentation, object recognition, and classification. In this review, we
   first discuss the impact of the scene type to the development of an MLS
   data processing method. Then, where appropriate, we describe relevant
   generalized algorithms for feature extraction and segmentation that are
   applicable to and implemented in many processing approaches. The methods
   for object recognition and point cloud classification are further
   reviewed including both the general concepts as well as technical
   details. In addition, available benchmark datasets for object
   recognition and classification are summarized. Further, the current
   limitations and challenges that a significant portion of point cloud
   processing techniques face are discussed. This review concludes with our
   future outlook of the trends and opportunities of MLS data processing
   algorithms and applications.}},
DOI = {{10.3390/s19040810}},
Article-Number = {{810}},
ISSN = {{1424-8220}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000460829200061}},
}

@article{ ISI:000459798800005,
Author = {Sun, Jia and Huang, Di and Wang, Yunhong and Chen, Liming},
Title = {{Expression Robust 3D Facial Landmarking via Progressive Coarse-to-Fine
   Tuning}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2019}},
Volume = {{15}},
Number = {{1}},
Month = {{FEB}},
Abstract = {{Facial landmarking is a fundamental task in automatic machine-based face
   analysis. The majority of existing techniques for such a problem are
   based on 2D images; however, they suffer from illumination and pose
   variations that may largely degrade landmarking performance. The
   emergence of 3D data theoretically provides an alternative to overcome
   these weaknesses in the 2D domain. This article proposes a novel
   approach to 3D facial landmarking, which combines both the advantages of
   feature-based methods as well as model-based ones in a progressive
   three-stage coarse-to-fine manner (initial, intermediate, and fine
   stages). For the initial stage, a few fiducial landmarks (i.e., the nose
   tip and two inner eye corners) are robustly detected through curvature
   analysis, and these points are further exploited to initialize the
   subsequent stage. For the intermediate stage, a statistical model is
   learned in the feature space of three normal components of the facial
   point-cloud rather than the smooth original coordinates, namely Active
   Normal Model (ANM). For the fine stage, cascaded regression is employed
   to locally refine the landmarks according to their geometry attributes.
   The proposed approach can accurately localize dozens of fiducial points
   on each 3D face scan, greatly surpassing the feature-based ones, and it
   also improves the state of the art of the model-based ones in two
   aspects: sensitivity to initialization and deficiency in discrimination.
   The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE
   databases, and competitive results are achieved in comparison with
   counterparts in the literature, clearly demonstrating its effectiveness.}},
DOI = {{10.1145/3282833}},
Article-Number = {{21}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
Unique-ID = {{ISI:000459798800005}},
}

@article{ ISI:000456899900003,
Author = {Zhou, Zixiang and Gong, Jie and Hu, Xuan},
Title = {{Community-scale multi-level post-hurricane damage assessment of
   residential buildings using multi-temporal airborne LiDAR data}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2019}},
Volume = {{98}},
Pages = {{30-45}},
Month = {{FEB}},
Abstract = {{Building damage assessment is a critical task following major hurricane
   events. Use of remotely sensed data to support building damage
   assessment is a logical choice considering the difficulty of gaining
   ground access to the impacted areas immediately after hurricane events.
   However, a remote sensing based damage assessment approach is often only
   capable of detecting severely damaged buildings. In this study, an
   airborne LiDAR based approach is proposed to assess multi-level
   hurricane damage at the community scale. In the proposed approach,
   building clusters are first extracted using a density-based algorithm. A
   novel cluster matching algorithm is proposed to robustly match
   post-event and pre-event building clusters. Multiple features including
   roof area and volume, roof orientation, and roof shape are computed as
   building damage indicators. A hierarchical determination process is then
   employed to identify the extent of damage to each building object. The
   results of this study suggest that our proposed approach is capable of
   1) recognizing building objects, 2) extracting damage features, and 3)
   characterizing the extent of damage to individual building properties.}},
DOI = {{10.1016/j.autcon.2018.10.018}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
Unique-ID = {{ISI:000456899900003}},
}

@article{ ISI:000457939400106,
Author = {Zhang, Wuming and Wan, Peng and Wang, Tiejun and Cai, Shangshu and Chen,
   Yiming and Jin, Xiuliang and Yan, Guangjian},
Title = {{A Novel Approach for the Detection of Standing Tree Stems from
   Plot-Level Terrestrial Laser Scanning Data}},
Journal = {{REMOTE SENSING}},
Year = {{2019}},
Volume = {{11}},
Number = {{2}},
Month = {{JAN 2}},
Abstract = {{Tree stem detection is a key step toward retrieving detailed stem
   attributes from terrestrial laser scanning (TLS) data. Various
   point-based methods have been proposed for the stem point extraction at
   both individual tree and plot levels. The main limitation of the
   point-based methods is their high computing demand when dealing with
   plot-level TLS data. Although segment-based methods can reduce the
   computational burden and uncertainties of point cloud classification,
   its application is largely limited to urban scenes due to the complexity
   of the algorithm, as well as the conditions of natural forests. Here we
   propose a novel and simple segment-based method for efficient stem
   detection at the plot level, which is based on the curvature feature of
   the points and connected component segmentation. We tested our method
   using a public TLS dataset with six forest plots that were collected for
   the international TLS benchmarking project in Evo, Finland. Results
   showed that the mean accuracies of the stem point extraction were
   comparable to the state-of-art methods (>95\%). The accuracies of the
   stem mappings were also comparable to the methods tested in the
   international TLS benchmarking project. Additionally, our method was
   applicable to a wide range of stem forms. In short, the proposed method
   is accurate and simple; it is a sensible solution for the stem detection
   of standing trees using TLS data.}},
DOI = {{10.3390/rs11020211}},
Article-Number = {{211}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{Yan, Guangjian/0000-0001-5030-748X
   Wang, Tiejun/0000-0002-1138-8464}},
Unique-ID = {{ISI:000457939400106}},
}

@article{ ISI:000457037300020,
Author = {Shao, Gang and Shao, Guofan and Fei, Songlin},
Title = {{Delineation of individual deciduous trees in plantations with
   low-density LiDAR data}},
Journal = {{INTERNATIONAL JOURNAL OF REMOTE SENSING}},
Year = {{2019}},
Volume = {{40}},
Number = {{1}},
Pages = {{346-363}},
Month = {{JAN 2}},
Abstract = {{Delineation of individual deciduous trees with Light Detection and
   Ranging (LiDAR) data has long been sought for accurate forest inventory
   in temperate forests. Previous attempts mainly focused on high-density
   LiDAR data to obtain reliable delineation results, which may have
   limited applications due to the high cost and low availability of such
   data. Here, the feasibility of individual deciduous tree delineation
   with low-density LiDAR data was examined using a point-density-based
   algorithm. First a high-resolution point density model (PDM) was
   developed from low-density LiDAR point cloud to locate individual trees
   through the horizontal spatial distribution of LiDAR points. Then,
   individual tree crowns and associated attributes were delineated with a
   2D marker-controlled watershed segmentation. Additionally, the PDM-based
   approach was compared with a conventional canopy height model (CHM)
   based delineation. The results demonstrated that the PDM-based approach
   produced an 89\% detection accuracy to identify deciduous trees in our
   study area. The tree attributes derived from the PDM-based algorithm
   explained 81\% and 83\% of tree height and crown width variations of
   forest stands, respectively. The conventional CHM-based tree attributes,
   on the other hand, could explain only 71\% and 66\% of tree height and
   crown width, respectively. Our results suggest that the application of
   the PDM-based individual tree identification in deciduous forests with
   low-density LiDAR data is feasible and has relatively high accuracy to
   predict tree height and crown width, which are highly desired in
   large-scale forest inventory and analysis.}},
DOI = {{10.1080/01431161.2018.1513664}},
ISSN = {{0143-1161}},
EISSN = {{1366-5901}},
ORCID-Numbers = {{Shao, Gang/0000-0003-3198-966X}},
Unique-ID = {{ISI:000457037300020}},
}

@article{ ISI:000450379200003,
Author = {Kaminska, Agnieszka and Lisiewicz, Maciej and Sterenczak, Krzysztof and
   Kraszewski, Bartlomiej and Sadkowski, Rafal},
Title = {{Species-related single dead tree detection using multi-temporal ALS data
   and CIR imagery}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{219}},
Pages = {{31-43}},
Month = {{DEC 15}},
Abstract = {{The assessment of the health conditions of trees in forests is extremely
   important for biodiversity, forest management, global environment
   monitoring, and carbon dynamics. There is a vast amount of research
   using remote sensing (RS) techniques for the assessment of the current
   condition of a forest, but only a small number of these are concerned
   with detection and classification of dead trees. Among the available RS
   techniques, only the airborne laser scanner (ALS) enables dead tree
   detection at the single tree level with high accuracy.
   The main objective of the study was to identify spruce, pine and
   deciduous trees by alive or dead classifications. Three RS data sets
   including ALS (leaf-on and leaf-off) and color-infrared (CIR) imagery
   (leaf-on) were used for the study. We used intensity and structural
   variables from the ALS data and spectral information derived from aerial
   imagery for the classification procedure. Additionally, we tested the
   differences in the classification accuracy of all variants contained in
   the data integration. In the study, the random forest (RF) classifier
   was used. The study was carried out in the Polish part of the Bialowieia
   Forest (BF).
   In general, we can state that all classifications, with different
   combinations of ALS features and CIR, resulted in high overall accuracy
   (OA >= 90\%) and Kappa (kappa > 0.86). For the best variant
   (CIR\_ALS(WSn-FH)), the mean values of overall accuracy and Kappa were
   equal to 94.3\% and 0.93, respectively. The leaf -on point cloud
   features alone produced the lowest accuracies (OA = 75-81\% and x =
   0.68-0.76). Improvements of 0-0.04 in the Kappa coefficient and 0-3.1\%
   in the overall classification accuracy were found after the point cloud
   normalization for all variants. Full -height point cloud features (F)
   produced lower accuracies than the results based on features calculated
   for half of the tree height point clouds (H) and combined FH.
   The importance of each of the predictors for different data sets for
   tree species classification provided by the RF algorithm was
   investigated. The lists of top features were the same, independent of
   intensity normalization. For the classification based on both of the
   point clouds (leaf on and leaf-off), three structural features (a
   proportion of first returns for both half -height and full -height
   variants and the canopy relief ratio of points) and two intensity
   features from first returns and half -height variant (the coefficient of
   variation and skewness) were rated as the most important. In the
   classification based on the point cloud with CIR features, two image
   features were among the most important (the NDVI and mean value of
   reflectance in the green band).}},
DOI = {{10.1016/j.rse.2018.10.005}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Lisiewicz, Maciej/Y-3668-2018
   Kraszewski, Bartlomiej/U-9482-2017}},
ORCID-Numbers = {{Lisiewicz, Maciej/0000-0003-0676-8291
   Sterenczak, Krzysztof/0000-0002-9556-0144
   Kraszewski, Bartlomiej/0000-0001-6161-7619}},
Unique-ID = {{ISI:000450379200003}},
}

@article{ ISI:000455637600091,
Author = {Zhou, Tan and Popescu, Sorin and Malambo, Lonesome and Zhao, Kaiguang
   and Krause, Keith},
Title = {{From LiDAR Waveforms to Hyper Point Clouds: A Novel Data Product to
   Characterize Vegetation Structure}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Full waveform (FW) LiDAR holds great potential for retrieving vegetation
   structure parameters at a high level of detail, but this prospect is
   constrained by practical factors such as the lack of available handy
   processing tools and the technical intricacy of waveform processing.
   This study introduces a new product named the Hyper Point Cloud (HPC),
   derived from FW LiDAR data, and explores its potential applications,
   such as tree crown delineation using the HPC-based intensity and
   percentile height (PH) surfaces, which shows promise as a solution to
   the constraints of using FW LiDAR data. The results of the HPC present a
   new direction for handling FW LiDAR data and offer prospects for
   studying the mid-story and understory of vegetation with high point
   density (similar to 182 points/m(2)). The intensity-derived digital
   surface model (DSM) generated from the HPC shows that the ground region
   has higher maximum intensity (MAXI) and mean intensity (MI) than the
   vegetation region, while having lower total intensity (TI) and number of
   intensities (NI) at a given grid cell. Our analysis of intensity
   distribution contours at the individual tree level exhibit similar
   patterns, indicating that the MAXI and MI decrease from the tree crown
   center to the tree boundary, while a rising trend is observed for TI and
   NI. These intensity variable contours provide a theoretical
   justification for using HPC-based intensity surfaces to segment tree
   crowns and exploit their potential for extracting tree attributes. The
   HPC-based intensity surfaces and the HPC-based PH Canopy Height Models
   (CHM) demonstrate promising tree segmentation results comparable to the
   LiDAR-derived CHM for estimating tree attributes such as tree locations,
   crown widths and tree heights. We envision that products such as the HPC
   and the HPC-based intensity and height surfaces introduced in this study
   can open new perspectives for the use of FW LiDAR data and alleviate the
   technical barrier of exploring FW LiDAR data for detailed vegetation
   structure characterization.}},
DOI = {{10.3390/rs10121949}},
Article-Number = {{1949}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Popescu, Sorin C/D-5981-2015
   Malambo, Lonesome/A-5693-2015}},
ORCID-Numbers = {{Popescu, Sorin C/0000-0002-8155-8801
   Zhou, Tan/0000-0002-9193-5113
   Malambo, Lonesome/0000-0002-8102-3700}},
Unique-ID = {{ISI:000455637600091}},
}

@article{ ISI:000455069600031,
Author = {Jaafar, Wan Shafrina Wan Mohd and Woodhouse, Iain Hector and Silva,
   Carlos Alberto and Omar, Hamdan and Maulud, Khairul Nizam Abdul and
   Hudak, Andrew Thomas and Klauberg, Carine and Cardil, Adrian and Mohan,
   Midhun},
Title = {{Improving Individual Tree Crown Delineation and Attributes Estimation of
   Tropical Forests Using Airborne LiDAR Data}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Individual tree crown (ITC) segmentation is an approach to isolate
   individual tree from the background vegetation and delineate precisely
   the crown boundaries for forest management and inventory purposes. ITC
   detection and delineation have been commonly generated from canopy
   height model (CHM) derived from light detection and ranging (LiDAR)
   data. Existing ITC segmentation methods, however, are limited in their
   efficiency for characterizing closed canopies, especially in tropical
   forests, due to the overlapping structure and irregular shape of tree
   crowns. Furthermore, the potential of 3-dimensional (3D) LiDAR data is
   not fully realized by existing CHM-based methods. Thus, the aim of this
   study was to develop an efficient framework for ITC segmentation in
   tropical forests using LiDAR-derived CHM and 3D point cloud data in
   order to accurately estimate tree attributes such as the tree height,
   mean crown width and aboveground biomass (AGB). The proposed framework
   entails five major steps: (1) automatically identifying dominant tree
   crowns by implementing semi-variogram statistics and morphological
   analysis; (2) generating initial tree segments using a watershed
   algorithm based on mathematical morphology; (3) identifying problematic
   segments based on predetermined set of rules; (4) tuning the problematic
   segments using a modified distance-based algorithm (DBA); and (5)
   segmenting and counting the number of individual trees based on the 3D
   LiDAR point clouds within each of the identified segment. This approach
   was developed in a way such that the 3D LiDAR points were only examined
   on problematic segments identified for further evaluations. 209
   reference trees with diameter at breast height (DBH) 10 cm were selected
   in the field in two study areas in order to validate ITC detection and
   delineation results of the proposed framework. We computed tree crown
   metrics (e.g., maximum crown height and mean crown width) to estimate
   aboveground biomass (AGB) at tree level using previously published
   allometric equations. Accuracy assessment was performed to calculate
   percentage of correctly detected trees, omission and commission errors.
   Our method correctly identified individual tree crowns with detection
   accuracy exceeding 80 percent at both forest sites. Also, our results
   showed high agreement (R-2 > 0.64) in terms of AGB estimates using 3D
   LiDAR metrics and variables measured in the field, for both sites. The
   findings from our study demonstrate the efficacy of the proposed
   framework in delineating tree crowns, even in high canopy density areas
   such as tropical rainforests, where, usually the traditional algorithms
   are limited in their performances. Moreover, the high tree delineation
   accuracy in the two study areas emphasizes the potential robustness and
   transferability of our approach to other densely forested areas across
   the globe.}},
DOI = {{10.3390/f9120759}},
Article-Number = {{759}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{wan mohd jaafar, wan shafrina/P-3916-2018
   abdul maulud, khairul nizam/J-4136-2015
   }},
ORCID-Numbers = {{Silva, Carlos Alberto/0000-0002-7844-3560
   wan mohd jaafar, wan shafrina/0000-0002-7813-088X
   abdul maulud, khairul nizam/0000-0002-9215-2778
   Hudak, Andrew/0000-0001-7480-1458}},
Unique-ID = {{ISI:000455069600031}},
}

@article{ ISI:000451733800040,
Author = {Shen, Yueqian and Lindenbergh, Roderik and Wang, Jinguo and Ferreira,
   Vagner G.},
Title = {{Extracting Individual Bricks from a Laser Scan Point Cloud of an
   Unorganized Pile of Bricks}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Bricks are the vital component of most masonry structures. Their
   maintenance is critical to the protection of masonry buildings.
   Terrestrial Light Detection and Ranging (TLidar) systems provide massive
   point cloud data in an accurate and fast way. TLidar enables us to
   sample and store the state of a brick surface in a practical way. This
   article aims to extract individual bricks from an unorganized pile of
   bricks sampled by a dense point cloud. The method automatically segments
   and models the individual bricks. The methodology is divided into five
   main steps: Filter needless points, brick boundary points removal,
   coarse segmentation using 3D component analysis, planar segmentation and
   grouping, and brick reconstruction. A novel voting scheme is used to
   segment the planar patches in an effective way. Brick reconstruction is
   based on the geometry of single brick and its corresponding nominal size
   (length, width and height). The number of bricks reconstructed is around
   75\%. An accuracy assessment is performed by comparing 3D coordinates of
   the reconstructed vertices to the manually picked vertices. The standard
   deviations of differences along x, y and z axes are 4.55 mm, 4.53 mm and
   4.60 mm, respectively. The comparison results indicate that the accuracy
   of reconstruction based on the introduced methodology is high and
   reliable. The work presented in this paper provides a theoretical basis
   and reference for large scene applications in brick-like structures.
   Meanwhile, the high-accuracy brick reconstruction lays the foundation
   for further brick displacement estimation.}},
DOI = {{10.3390/rs10111709}},
Article-Number = {{1709}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{Lindenbergh, Roderik/0000-0001-8655-5266
   Shen, Yueqian/0000-0003-2455-9012}},
Unique-ID = {{ISI:000451733800040}},
}

@article{ ISI:000433909100002,
Author = {Song, Xiaoning and Feng, Zhen-Hua and Hu, Guosheng and Kittler, Josef
   and Wu, Xiao-Jun},
Title = {{Dictionary Integration Using 3D Morphable Face Models for Pose-Invariant
   Collaborative-Representation-Based Classification}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2018}},
Volume = {{13}},
Number = {{11}},
Pages = {{2734-2745}},
Month = {{NOV}},
Abstract = {{The paper presents a dictionary integration algorithm using 3D morphable
   face models (3DMM) for pose-invariant collaborative-representation-based
   face classification. To this end, we first fit a 3DMM to the 2D face
   images of a dictionary to reconstruct the 3D shape and texture of each
   image. The 3D faces are used to render a number of virtual 2D face
   images with arbitrary pose variations to augment the training data, by
   merging the original and rendered virtual samples to create an extended
   dictionary. Second, to reduce the information redundancy of the extended
   dictionary and improve the sparsity of reconstruction coefficient
   vectors using collaborative-representation-based classification (CRC),
   we exploit an on-line class elimination scheme to optimise the extended
   dictionary by identifying the training samples of the most
   representative classes for a given query. The final goal is to perform
   pose-invariant face classification using the proposed dictionary
   integration method and the on-line pruning strategy under the CRC
   framework. Experimental results obtained for a set of well-known face
   data sets demonstrate the merits of the proposed method, especially its
   robustness to pose variations.}},
DOI = {{10.1109/TIFS.2018.2833052}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ORCID-Numbers = {{Kittler, Josef/0000-0002-8110-9205
   Feng, Zhenhua/0000-0002-4485-4249}},
Unique-ID = {{ISI:000433909100002}},
}

@article{ ISI:000447286200001,
Author = {Song, Wei and Zou, Shuanghui and Tian, Yifei and Fong, Simon and Cho,
   Kyungeun},
Title = {{Classifying 3D objects in LiDAR point clouds with a back-propagation
   neural network}},
Journal = {{HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES}},
Year = {{2018}},
Volume = {{8}},
Month = {{OCT 12}},
Abstract = {{Due to object recognition accuracy limitations, unmanned ground vehicles
   (UGVs) must perceive their environments for local path planning and
   object avoidance. To gather high-precision information about the UGV's
   surroundings, Light Detection and Ranging (LiDAR) is frequently used to
   collect large-scale point clouds. However, the complex spatial features
   of these clouds, such as being unstructured, diffuse, and disordered,
   make it difficult to segment and recognize individual objects. This
   paper therefore develops an object feature extraction and classification
   system that uses LiDAR point clouds to classify 3D objects in urban
   environments. After eliminating the ground points via a height threshold
   method, this describes the 3D objects in terms of their geometrical
   features, namely their volume, density, and eigenvalues. A
   back-propagation neural network (BPNN) model is trained (over the course
   of many iterations) to use these extracted features to classify objects
   into five types. During the training period, the parameters in each
   layer of the BPNN model are continually changed and modified via
   back-propagation using a non-linear sigmoid function. In the system, the
   object segmentation process supports obstacle detection for autonomous
   driving, and the object recognition method provides an environment
   perception function for terrain modeling. Our experimental results
   indicate that the object recognition accuracy achieve 91.5\% in outdoor
   environment.}},
DOI = {{10.1186/s13673-018-0152-7}},
Article-Number = {{29}},
ISSN = {{2192-1962}},
ORCID-Numbers = {{Wei, Song/0000-0002-5909-9661}},
Unique-ID = {{ISI:000447286200001}},
}

@article{ ISI:000445398200037,
Author = {Thierens, Laurent A. M. and De Roo, Noemi M. C. and De Pauw, Guy A. M.
   and Brusselaers, Nele},
Title = {{Quantifying Soft Tissue Changes in Cleft Lip and Palate Using
   Nonionizing Three-Dimensional Imaging: A Systematic Review}},
Journal = {{JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{76}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{Purpose: The use of nonionizing 3-dimensional (3D) imaging in cleft lip
   and palate (CLP) research is well-established; however, general
   guidelines concerning the assessment of these images are lacking. The
   aim of the present study was to review the methods for quantification of
   soft tissue changes on 3D surface images acquired before and after an
   orthopedic or surgical intervention in CLP patients.
   Materials and Methods: A systematic literature search was performed
   using the databases MEDLINE (through PubMed), CENTRAL, Web of Science,
   and EMBASE. The literature search and eligibility assessment were
   performed by 2 independent reviewers in a nonblinded standardized
   manner. Only longitudinal studies reporting the assessment of pre- and
   postoperative 3D surface images and at least 10 CLP patients were
   considered eligible.
   Results: Fifteen unique studies (reported from 1996 to 2017) were
   identified after an eligibility assessment. The assessment of the 3D
   images was performed with landmark-dependent analyses, mostly supported
   by superimposition of the pre- and postoperative images. A wide spectrum
   of superimposition techniques has been reported. The reliability of
   these assessment methods was often not reported or was insufficiently
   reported.
   Conclusions: Soft tissue changes subsequent to a surgical or an
   orthopedic intervention can be quantified on 3D surface images using
   assessment methods that are primarily based on landmark identification,
   whether or not followed by superimposition. Operator bias is inherently
   enclosed in landmark-dependent analyses. The reliability of these
   methods has been insufficiently reported. (C) 2018 American Association
   of Oral and Maxillofacial Surgeons}},
DOI = {{10.1016/j.joms.2018.05.020}},
ISSN = {{0278-2391}},
EISSN = {{1531-5053}},
Unique-ID = {{ISI:000445398200037}},
}

@article{ ISI:000442764500004,
Author = {Cardoso, Maria Joao and Vrieling, Conny and Cardoso, Jaime S. and
   Oliveira, Helder P. and Williams, Norman R. and Dixon, J. M. and PICTURE
   Project Clinical Trial Tea and PICTURE Project Delphi Panel},
Title = {{The value of 3D images in the aesthetic evaluation of breast cancer
   conservative treatment. Results from a prospective multicentric clinical
   trial}},
Journal = {{BREAST}},
Year = {{2018}},
Volume = {{41}},
Pages = {{19-24}},
Month = {{OCT}},
Abstract = {{Purpose: BCCT.core (Breast Cancer Conservative Treatment. cosmetic
   results) is a software created for the objective evaluation of aesthetic
   result of breast cancer conservative treatment using a single patient
   frontal photography. The lack of volume information has been one
   criticism, as the use of 3D information might improve accuracy in
   aesthetic evaluation. In this study, we have evaluated the added value
   of 3D information to two methods of aesthetic evaluation: a panel of
   experts; and an augmented version of the computational model -
   BCCT.core3d.
   Material and methods: Within the scope of EU Seventh Framework Programme
   Project PICTURE, 2D and 3D images from 106 patients from three clinical
   centres were evaluated by a panel of 17 experts and the BCCT.core.
   Agreement between all methods was calculated using the kappa (K) and
   weighted kappa (wK) statistics.
   Results: Subjective agreement between 2D and 3D individual evaluation
   was fair to moderate. The agreement between the expert classification
   and the BCCT.core software with both 2D and 3D features was also fair to
   moderate.
   Conclusions: The inclusion of 3D images did not add significant
   information to the aesthetic evaluation either by the panel or the
   software. Evaluation of aesthetic outcome can be performed using of the
   BCCT.core software, with a single frontal image. (C) 2018 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.breast.2018.06.008}},
ISSN = {{0960-9776}},
EISSN = {{1532-3080}},
ResearcherID-Numbers = {{WILLIAMS, Norman/C-2002-2008
   Cardoso, Jaime/I-3286-2013
   Oliveira, Helder/M-9956-2017}},
ORCID-Numbers = {{WILLIAMS, Norman/0000-0001-6496-312X
   Cardoso, Jaime/0000-0002-3760-2473
   Cardoso, Maria Joao/0000-0002-8137-3700
   Oliveira, Helder/0000-0002-6193-8540}},
Unique-ID = {{ISI:000442764500004}},
}

@article{ ISI:000440851800017,
Author = {Siqueira, Robson S. and Alexandre, Gilderlane R. and Soares, Jose M. and
   The, George A. P.},
Title = {{Triaxial Slicing for 3-D Face Recognition From Adapted Rotational
   Invariants Spatial Moments and Minimal Keypoints Dependence}},
Journal = {{IEEE ROBOTICS AND AUTOMATION LETTERS}},
Year = {{2018}},
Volume = {{3}},
Number = {{4}},
Pages = {{3513-3520}},
Month = {{OCT}},
Abstract = {{This letter presents a multiple slicing model for three-dimensional
   (3-D) images of human face, using the frontal, sagittal, and transverse
   orthogonal planes. The definition of the segments depends on just one
   key point, the nose tip, which makes it simple and independent of the
   detection of several key points. For facial recognition, attributes
   based on adapted 2-D spatial moments of Hu and 3-D spatial invariant
   rotation moments are extracted from each segment. Tests with the
   proposed model using the Bosphorus Database for neutral vs nonneutral
   ROC I experiment, applying linear discriminant analysis as classifier
   and more than one sample for training, achieved 98.7\% of verification
   rate at 0.1\% of false acceptance rate. By using the support vector
   machine as classifier the rank1 experiment recognition rates of 99\% and
   95.4\% have been achieved for a neutral vs neutral and for a neutral vs
   non neutral, respectively. These results approach the state-of-the-art
   using Bosphorus Database and even surpasses it when anger and disgust
   expressions are evaluated. In addition, we also evaluate the
   generalization of our method using the FRGC v2.0 database and achieve
   competitive results, making the technique promising, especially for its
   simplicity.}},
DOI = {{10.1109/LRA.2018.2854295}},
ISSN = {{2377-3766}},
ORCID-Numbers = {{Marques Soares, Jose/0000-0002-5111-5794
   Alexandre, Gilderlane/0000-0002-8778-5351
   The, George/0000-0002-8064-8901}},
Unique-ID = {{ISI:000440851800017}},
}

@article{ ISI:000449993800083,
Author = {Wu, Jianwei and Yao, Wei and Polewski, Przemyslaw},
Title = {{Mapping Individual Tree Species and Vitality along Urban Road Corridors
   with LiDAR and Imaging Sensors: Point Density versus View Perspective}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{To meet a growing demand for accurate high-fidelity vegetation cover
   mapping in urban areas toward biodiversity conservation and assessing
   the impact of climate change, this paper proposes a complete approach to
   species and vitality classification at single tree level by synergistic
   use of multimodality 3D remote sensing data. So far, airborne laser
   scanning system (ALS or airborne LiDAR) has shown promising results in
   tree cover mapping for urban areas. This paper analyzes the potential of
   mobile laser scanning system/mobile mapping system (MLS/MMS)-based
   methods for recognition of urban plant species and characterization of
   growth conditions using ultra-dense LiDAR point clouds and provides an
   objective comparison with the ALS-based methods. Firstly, to solve the
   extremely intensive computational burden caused by the classification of
   ultra-dense MLS data, a new method for the semantic labeling of LiDAR
   data in the urban road environment is developed based on combining a
   conditional random field (CRF) for the context-based classification of
   3D point clouds with shape priors. These priors encode geometric
   primitives found in the scene through sample consensus segmentation.
   Then, single trees are segmented from the labelled tree points using the
   3D graph cuts algorithm. Multinomial logistic regression classifiers are
   used to determine the fine deciduous urban tree species of conversation
   concern and their growth vitality. Finally, the weight-of-evidence
   (WofE) based decision fusion method is applied to combine the
   probability outputs of classification results from the MLS and ALS data.
   The experiment results obtained in city road corridors demonstrated that
   point cloud data acquired from the airborne platform achieved even
   slightly better results in terms of tree detection rate, tree species
   and vitality classification accuracy, although the tree vitality
   distribution in the test site is less balanced compared to the species
   distribution. When combined with MLS data, overall accuracies of 78\%
   and 74\% for tree species and vitality classification can be achieved,
   which has improved by 5.7\% and 4.64\% respectively compared to the
   usage of airborne data only.}},
DOI = {{10.3390/rs10091403}},
Article-Number = {{1403}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Yao, Wei/E-8520-2017}},
ORCID-Numbers = {{Yao, Wei/0000-0001-7704-0615}},
Unique-ID = {{ISI:000449993800083}},
}

@article{ ISI:000445436100006,
Author = {Halik, Lukasz and Smaczynski, Maciej},
Title = {{Geovisualisation of Relief in a Virtual Reality System on the Basis of
   Low-Level Aerial Imagery}},
Journal = {{PURE AND APPLIED GEOPHYSICS}},
Year = {{2018}},
Volume = {{175}},
Number = {{9}},
Pages = {{3209-3221}},
Month = {{SEP}},
Abstract = {{The aim of the following paper was to present the geomatic process of
   transforming low-level aerial imagery obtained with unmanned aerial
   vehicles (UAV) into a digital terrain model (DTM) and implementing the
   model into a virtual reality system (VR). The object of the study was a
   natural aggretage heap of an irregular shape and denivelations up to 11
   m. Based on the obtained photos, three point clouds (varying in the
   level of detail) were generated for the 20,000-m(2)-area. For further
   analyses, the researchers selected the point cloud with the best ratio
   of accuracy to output file size. This choice was made based on seven
   control points of the heap surveyed in the field and the corresponding
   points in the generated 3D model. The obtained several-centimetre
   differences between the control points in the field and the ones from
   the model might testify to the usefulness of the described algorithm for
   creating large-scale DTMs for engineering purposes. Finally, the chosen
   model was implemented into the VR system, which enables the most
   lifelike exploration of 3D terrain plasticity in real time, thanks to
   the first person view mode (FPV). In this mode, the user observes an
   object with the aid of a Head- mounted display (HMD), experiencing the
   geovisualisation from the inside, and virtually analysing the terrain as
   a direct animator of the observations.}},
DOI = {{10.1007/s00024-017-1755-z}},
ISSN = {{0033-4553}},
EISSN = {{1420-9136}},
Unique-ID = {{ISI:000445436100006}},
}

@article{ ISI:000445204800002,
Author = {Wang, Jinhu and Lindenbergh, Roderik and Menenti, Massimo},
Title = {{Scalable individual tree delineation in 3D point clouds}},
Journal = {{PHOTOGRAMMETRIC RECORD}},
Year = {{2018}},
Volume = {{33}},
Number = {{163}},
Pages = {{315-340}},
Month = {{SEP}},
Abstract = {{Manually monitoring and documenting trees is labour intensive. Lidar
   provides a possible solution for automatic tree-inventory generation.
   Existing approaches for segmenting trees from original point cloud data
   lack scalable and efficient methods that separate individual trees
   sampled by different laser-scanning systems with sufficient quality
   under all circumstances. In this study a new algorithm for efficient
   individual tree delineation from lidar point clouds is presented and
   validated. The proposed algorithm first resamples the points using
   cuboid (modified voxel) cells. Consecutively connected cells are
   accumulated by vertically traversing cell layers. Trees in close
   proximity are identified, based on a novel cell-adjacency analysis. The
   scalable performance of this algorithm is validated on airborne, mobile
   and terrestrial laser-scanning point clouds. Validation against ground
   truth demonstrates an improvement from 89\% to 94\% relative to a
   state-of-the-art method while computation time is similar.
   Resume La detection et la documentation manuelle des arbres est une
   tache fastidieuse. Le lidar offre une solution possible pour
   l'inventaire automatique des arbres. Les approches existantes pour la
   segmentation des arbres dans des nuages bruts de points ne proposent pas
   de methodes efficaces et adaptees a toutes les echelles pour separer des
   arbres individuels echantillonnes par differents systemes lidar avec une
   qualite acceptable en toute circonstance. Cette etude propose et valide
   un nouvel algorithme pour la delimitation efficace d'arbres individuels
   a partir de nuages de points lidar. L'algorithme propose commence par
   reechantillonner les points dans des cellules cubiques (voxels), puis
   regroupe les cellules connexes en traversant verticalement les couches
   de cellules. Les arbres proches sont identifies grace a une nouvelle
   analyse d'adjacence de cellules. La performance de cetalgorithme en
   termes d'adaptabilite au changement d'echelle est validee a partir de
   nuages de points issus de systemes laser a balayage aerien, mobile et
   terrestre. Une validation basee sur des donnees de terrain de reference
   fait etat d'une amelioration de 89\% a 94\% par rapport a des methodes
   connues pour un temps de calcul comparable.
   Zusammenfassung Eine uberwachung und Dokumentation von Baumen ist sehr
   arbeitsaufwandig. Lidar bietet das Potential fur automatische
   Bauminventur. Es gibt Ansatze zur Segmentierung von Baumen aus
   Punktwolken, die allerdings noch nicht in der Lage sind, einzelne Baume
   in Punktwolken verschiedener Lidar-Systeme zuverlassig und mit
   ausreichender Qualitat unter vielfaltigen realen Bedingungen zu
   separieren. Diese Studie stellt einen neuen Algorithmus zur effizienten
   Erfassung von Baumen in Lidar-Punktwolken dar. Der Algorithmus bildet
   Punkte mit Hilfe von quaderformigen (Voxel) Zellen um. Nacheinander
   verbundene Zellen werden durch vertikale Traverse der Zellschichten
   akkumuliert. Baume in nachster Nachbarschaft werden durch eine neuartige
   Zellanalyse identifiziert. Der Vorteil der Skalierbarkeit des
   Algorithmus wird an flugzeuggestutzten, mobilen und terrestrischen
   Laserscanpunktwolken validiert. Anhand von Solldaten ist festzustellen,
   dass bei gleicher Rechenzeit, eine Verbesserung von 89\% bis 94\% im
   Vergleich zu aktuellen Verfahren erzielt werden kann.
   Resumen Monitorizar y documentar manualmente arboles es un trabajo
   intensivo. El lidar proporciona una posible solucion para la generacion
   automatica del inventario de arboles. Los enfoques existentes para
   segmentar arboles a partir originalmente de nubes de puntos lidar
   carecen de metodos escalables y eficientes que separen arboles
   individuales muestreados por diferentes sistemas lidar con calidad
   suficiente bajo todas las circunstancias. En este estudio, se presenta y
   valida un algoritmo nuevo para la delimitacion eficiente de arboles
   individuales a partir de nubes de puntos lidar. El algoritmo propuesto
   primero remuestrea los puntos usando celulas cuboides (voxels). Los
   voxels adyacentes se acumulan atravesando verticalmente las capas de
   voxels. Basados en un nuevo analisis de adyacencia de voxels se
   identifican arboles que estan proximos. El rendimiento escalable de este
   algoritmo se valida con nubes de puntos lidar aerotransportados, moviles
   y terrestres. La validacion con verdad terreno demuestra una mejora del
   89\% al 94\% en comparacion con un metodo de vanguardia, mientras que el
   tiempo de calculo es similar.}},
DOI = {{10.1111/phor.12247}},
ISSN = {{0031-868X}},
EISSN = {{1477-9730}},
ORCID-Numbers = {{Lindenbergh, Roderik/0000-0001-8655-5266}},
Unique-ID = {{ISI:000445204800002}},
}

@article{ ISI:000442238900004,
Author = {Switonski, Adam and Krzeszowski, Tomasz and Josinski, Henryk and Kwolek,
   Bogdan and Wojciechowski, Konrad},
Title = {{Gait recognition on the basis of markerless motion tracking and DTW
   transform}},
Journal = {{IET BIOMETRICS}},
Year = {{2018}},
Volume = {{7}},
Number = {{5}},
Pages = {{415-422}},
Month = {{SEP}},
Abstract = {{In this study, a framework for view-invariant gait recognition on the
   basis of markerless motion tracking and dynamic time warping (DTW)
   transform is presented. The system consists of a proposed markerless
   motion capture system as well as introduced classification method of
   mocap data. The markerless system estimates the three-dimensional
   locations of skeleton driven joints. Such skeleton-driven point clouds
   represent poses over time. The authors align point clouds in every pair
   of frames by calculating the minimal sum of squared distances between
   the corresponding joints. A point cloud distance measure with temporal
   context has been utilised in k-nearest neighbours algorithm to compare
   time instants of motion sequences. To enhance the generalisation of the
   recognition and to shorten the processing time, for every individual a
   single multidimensional time series among several multidimensional time
   series describing the individual's gait is established. The correct
   classification rate has been determined on the basis of a real dataset
   of human gait. It contains 230 gait cycles of 22 subjects. The tracking
   results on the basis of markerless motion capture are referenced to
   Vicon system, whereas the achieved accuracies of recognition are
   compared with the ones obtained by DTW that is based on rotational data.}},
DOI = {{10.1049/iet-bmt.2017.0134}},
ISSN = {{2047-4938}},
EISSN = {{2047-4946}},
ResearcherID-Numbers = {{Krzeszowski, Tomasz/H-7717-2019}},
ORCID-Numbers = {{Krzeszowski, Tomasz/0000-0001-7359-4637}},
Unique-ID = {{ISI:000442238900004}},
}

@article{ ISI:000440350800018,
Author = {van Veen, Martinus M. and Korteweg, Steven F. S. and Dijkstra, Pieter U.
   and Werker, Paul M. N.},
Title = {{Keeping the fat on the right spot prevents contour deformity in
   temporalis muscle transposition}},
Journal = {{JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY}},
Year = {{2018}},
Volume = {{71}},
Number = {{8}},
Pages = {{1181-1187}},
Month = {{AUG}},
Abstract = {{The temporalis muscle transposition is a reliable, one-stage reanimation
   technique for longstanding facial paralysis. In the variation described
   by Rubin, the muscle is released from the temporal bone and folded over
   the zygomatic arch towards the modiolus. This results in unsightly
   temporal hollowing and zygomatic bulging. We present a modification of
   this technique, which preserves the temporal fat pad in its anatomical
   location as well as conceals temporal hollowing and prevents zygomatic
   bulging.
   The data of 23 patients treated with this modification were analysed.
   May classification was used for evaluation of mouth reanimation. Experts
   and patients scored visibility of the contour deformity on a 100-mm
   visual analogue scale (VAS) (score 0 = poor/100 = best). 3D images of
   the face were used to measure temporal hollowing and zygomatic bulging.
   3D images were compared to those of controls with a similar gender and
   age distribution.
   After a median follow-up of 5.7 years, all patients achieved symmetry at
   rest. Eleven patients achieved symmetry while smiling with closed lips
   (May classification ``Good{''}). A median (interquartile range {[}IQR])
   VAS score of 19 (6; 41) was given by experts and 25 (5; 59) by patients
   themselves. 3D volumes of zygomatic bulging differed from those of
   control subjects, although all volume differences were small (median
   <3.3 ml) and temporal hollowing did not differ significantly.
   On the basis of our results, we conclude that our modified Rubin
   temporalis transposition technique provides an elegant way to conceal
   bulging over the zygomatic arch and prevents temporal hollowing, without
   the need for fascial extensions to reach the modiolus. (C) 2018 British
   Association of Plastic, Reconstructive and Aesthetic Surgeons. Published
   by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.bjps.2018.04.007}},
ISSN = {{1748-6815}},
EISSN = {{1878-0539}},
ResearcherID-Numbers = {{Werker, Paul/L-6293-2019}},
Unique-ID = {{ISI:000440350800018}},
}

@article{ ISI:000439703500027,
Author = {Gibelli, Daniele and Pucciarelli, Valentina and Cappella, Annalisa and
   Dolci, Claudia and Sforza, Chiarella},
Title = {{Are Portable Stereophotogrammetric Devices Reliable in Facial Imaging? A
   Validation Study of VECTRA H1 Device}},
Journal = {{JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{76}},
Number = {{8}},
Pages = {{1772-1784}},
Month = {{AUG}},
Abstract = {{Purpose: Modern 3-dimensional (3D) image acquisition systems represent a
   crucial technologic development in facial anatomy because of their
   accuracy and precision. The recently introduced portable devices can
   improve facial databases by increasing the number of applications. In
   the present study, the VECTRA H1 portable stereophotogrammetric device
   was validated to verify its applicability to 3D facial analysis.
   Materials and Methods: Fifty volunteers underwent 4 facial scans using
   portable VECTRA H1 and static VECTRA M3 devices (2 for each instrument).
   Repeatability of linear, angular, surface area, and volume measurements
   was verified within the device and between devices using the
   Bland-Altman test and the calculation of absolute and relative technical
   errors of measurement (TEM and rTEM, respectively). In addition, the 2
   scans obtained by the same device and the 2 scans obtained by different
   devices were registered and superimposed to calculate the root mean
   square (RMS; point-to-point) distance between the 2 surfaces.
   Results: Most linear, angular, and surface area measurements had high
   repeatability in M3 versus M3, H1 versus H1, and M3 versus H1
   comparisons (range, 82.2 to 98.7\%; TEM range, 0.3 to 2.0 mm, 0.4
   degrees to 1.8 degrees; rTEM range, 0.2 to 3.1\%). In contrast, volumes
   and RMS distances showed evident differences in M3 versus M3 and H1
   versus H1 comparisons and reached the maximum when scans from the 2
   different devices were compared.
   Conclusion: The portable VECTRA H1 device proved reliable for assessing
   linear measurements, angles, and surface areas; conversely, the
   influence of involuntary facial movements on volumes and RMS distances
   was more important compared with the static device. (C) 2018 American
   Association of Oral and Maxillofacial Surgeons}},
DOI = {{10.1016/j.joms.2018.01.021}},
ISSN = {{0278-2391}},
EISSN = {{1531-5053}},
ResearcherID-Numbers = {{Cappella, Annalisa/V-5586-2017
   Sforza, Chiarella/C-3008-2015}},
ORCID-Numbers = {{Sforza, Chiarella/0000-0001-6532-6464}},
Unique-ID = {{ISI:000439703500027}},
}

@article{ ISI:000435048200005,
Author = {Reiser, David and Vazquez-Arellano, Manuel and Paraforos, Dimitris S.
   and Garrido-Izard, Miguel and Griepentrog, Hans W.},
Title = {{Iterative individual plant clustering in maize with assembled 2D LiDAR
   data}},
Journal = {{COMPUTERS IN INDUSTRY}},
Year = {{2018}},
Volume = {{99}},
Pages = {{42-52}},
Month = {{AUG}},
Abstract = {{A two dimensional (2D) laser scanner was mounted at the front part of a
   small 4-wheel autonomous robot with differential steering, at an angle
   of 30 degrees pointing downwards. The machine was able to drive between
   maize rows and collect concurrent time-stamped data. A robotic total
   station tracked the position of a prism mounted on the vehicle. The
   total station and laser scanner data were fused to generate a three
   dimensional (3D) point cloud. This 3D representation was used to detect
   individual plant positions, which are of particular interest for
   applications such as phenotyping, individual plant treatment and
   precision weeding. Two different methodologies were applied to the 3D
   point cloud to estimate the position of the individual plants. The first
   methodology used the Euclidian Clustering on the entire point cloud. The
   second methodology utilised the position of an initial plant and the
   fixed plant spacing to search iteratively for the best clusters. The two
   algorithms were applied at three different plant growth stages. For the
   first method, results indicated a detection rate up to 73.7\% with a
   root mean square error of 3.6 cm. The second method was able to detect
   all plants (100\% detection rate) with an accuracy of 2.7-3.0 cm, taking
   the plant spacing of 13 cm into account.}},
DOI = {{10.1016/j.compind.2018.03.023}},
ISSN = {{0166-3615}},
EISSN = {{1872-6194}},
ORCID-Numbers = {{Paraforos, Dimitrios S./0000-0001-8275-8840
   Reiser, David/0000-0003-0158-6456}},
Unique-ID = {{ISI:000435048200005}},
}

@article{ ISI:000451673800001,
Author = {Chen, Ying and Haerdie, Wolfgang K. and He, Qiang and Majer, Piotr},
Title = {{Risk related brain regions detection and individual risk classification
   with 3D image FPCA}},
Journal = {{STATISTICS \& RISK MODELING}},
Year = {{2018}},
Volume = {{35}},
Number = {{3-4}},
Pages = {{89-110}},
Month = {{JUL}},
Abstract = {{Understanding how people make decisions from risky choices has attracted
   increasing attention of researchers in economics, psychology and
   neuroscience. While economists try to evaluate individual's risk
   preference through mathematical modeling, neuroscientists answer the
   question by exploring the neural activities of the brain. We propose a
   model-free method, 3-dimensional image functional principal component
   analysis (3DIF), to provide a connection between active risk related
   brain region detection and individual's risk preference. The 3DIF
   methodology is directly applicable to 3-dimensional image data without
   artificial vectorization or mapping and simultaneously guarantees the
   contiguity of risk related brain regions rather than discrete voxels.
   Simulation study evidences an accurate and reasonable region detection
   using the 3DIF method. In real data analysis, five important risk
   related brain regions are detected, including parietal cortex (PC),
   ventrolateral prefrontal cortex (VLPFC), lateral orbifrontal cortex
   (IOFC), anterior insula (aINS) and dorsolateral prefrontal cortex
   (DLPFC), while the alternative methods only identify limited risk
   related regions. Moreover, the 3DIF method is useful for extraction of
   subjective specific signature scores that carry explanatory power for
   individual's risk attitude. In particular, the 3DIF method perfectly
   classifies both strongly and weakly risk averse subjects for in-sample
   analysis. In out-of-sample experiment, it achieves 73\%-88\% overall
   accuracy, among which 90\%-100\% strongly risk averse subjects and
   49\%-71\% weakly risk averse subjects are correctly classified with
   leave-k-out cross validations.}},
DOI = {{10.1515/strm-2017-0011}},
ISSN = {{2193-1402}},
EISSN = {{2196-7040}},
ORCID-Numbers = {{Chen, Ying/0000-0002-2577-7348}},
Unique-ID = {{ISI:000451673800001}},
}

@article{ ISI:000441334300307,
Author = {Rymarczyk, Tomasz and Klosowski, Grzegorz and Kozlowski, Edward},
Title = {{A Non-Destructive System Based on Electrical Tomography and Machine
   Learning to Analyze the Moisture of Buildings}},
Journal = {{SENSORS}},
Year = {{2018}},
Volume = {{18}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{This article presents the results of research on a new method of spatial
   analysis of walls and buildings moisture. Due to the fact that
   destructive methods are not suitable for historical buildings of great
   architectural significance, a non-destructive method based on electrical
   tomography has been adopted. A hybrid tomograph with special sensors was
   developed for the measurements. This device enables the acquisition of
   data, which are then reconstructed by appropriately developed methods
   enabling spatial analysis of wet buildings. Special electrodes that
   ensure good contact with the surface of porous building materials such
   as bricks and cement were introduced. During the research, a group of
   algorithms enabling supervised machine learning was analyzed. They have
   been used in the process of converting input electrical values into
   conductance depicted by the output image pixels. The conductance values
   of individual pixels of the output vector made it possible to obtain
   images of the interior of building walls as both flat intersections (2D)
   and spatial (3D) images. The presented group of algorithms has a high
   application value. The main advantages of the new methods are: high
   accuracy of imaging, low costs, high processing speed, ease of
   application to walls of various thickness and irregular surface. By
   comparing the results of tomographic reconstructions, the most efficient
   algorithms were identified.}},
DOI = {{10.3390/s18072285}},
Article-Number = {{2285}},
ISSN = {{1424-8220}},
ResearcherID-Numbers = {{Klosowski, Grzegorz/B-8899-2017
   Kozlowski, Edward/A-6882-2013
   Rymarczyk, Tomasz/D-6177-2015}},
ORCID-Numbers = {{Klosowski, Grzegorz/0000-0001-7927-3674
   Kozlowski, Edward/0000-0002-7147-4903
   Rymarczyk, Tomasz/0000-0002-3524-9151}},
Unique-ID = {{ISI:000441334300307}},
}

@article{ ISI:000440122900009,
Author = {Huang, Delin and Du, Shichang and Li, Guilong and Zhao, Chen and Deng,
   Yafei},
Title = {{Detection and monitoring of defects on three-dimensional curved surfaces
   based on high-density point cloud data}},
Journal = {{PRECISION ENGINEERING-JOURNAL OF THE INTERNATIONAL SOCIETIES FOR
   PRECISION ENGINEERING AND NANOTECHNOLOGY}},
Year = {{2018}},
Volume = {{53}},
Pages = {{79-95}},
Month = {{JUL}},
Abstract = {{The surface quality of three-dimensional (3-D) curved surfaces is one of
   the most important factors that can directly influence the performance
   of the final product. This paper presents a systematic approach for
   detection and monitoring of defects on 3-D curved surfaces based on
   high-density point cloud data. Firstly, an algorithm to remove outliers
   and a boundary recognition algorithm are proposed to divide the entire
   3-D curved surface including millions of measured points into multiple
   sub-regions. Secondly, two new evaluation indexes based on wavelet
   packet entropy and normal vector are explored to represent the features
   of the multiple sub-regions to determine whether the sub-regions are
   out-of-limit (OOL) of specifications. Thirdly, three quality parameters
   representing quality characteristics of a curved surface are presented
   and their values are calculated based on the clusters of OOL
   sub-regions. Finally, three individual control charts are presented to
   monitor the three quality parameters. As long as any quality parameter
   is out of the control range, the manufacturing process of the curved
   surface is determined to be out-of-control (OOC). The results of a case
   study show that the proposed approach can effectively identify the OOC
   manufacturing process and detect defects on 3-D curved surfaces.}},
DOI = {{10.1016/j.precisioneng.2018.03.001}},
ISSN = {{0141-6359}},
EISSN = {{1873-2372}},
Unique-ID = {{ISI:000440122900009}},
}

@article{ ISI:000435372100001,
Author = {Burkus, Mate and Schlegl, Adam Tibor and O'Sullivan, Ian and Markus,
   Istvan and Vermes, Csaba and Tunyogi-Csapo, Miklos},
Title = {{Sagittal plane assessment of spino-pelvic complex in a Central European
   population with adolescent idiopathic scoliosis: a case control study}},
Journal = {{SCOLIOSIS AND SPINAL DISORDERS}},
Year = {{2018}},
Volume = {{13}},
Month = {{JUN 14}},
Abstract = {{Background: Scoliosis is a complex three-dimensional deformity. While
   the frontal profile is well understood, increasing attention has turned
   to balance in the sagittal plane. The present study evaluated changes in
   sagittal spino-pelvic parameters in a large Hungarian population with
   adolescent idiopathic scoliosis.
   Methods: EOS 2D/3D images of 458 scoliotic and 69 control cases were
   analyzed. After performing 3D reconstructions, the sagittal parameters
   were assessed as a whole and by curve type using independent sample t
   test and linear regression analysis.
   Results: Patients with scoliosis had significantly decreased thoracic
   kyphosis (p < 0.001) with values T1-T12, 34.1 +/- 17.1 degrees vs. 43.4
   +/- 12.7 degrees in control; T4-T12, 27.1 +/- 18.8 degrees vs. 37.7 +/-
   15.1 degrees in control; and T5-T12, 24.9 +/- 15.8 degrees vs. 32.9 +/-
   15. 0 degrees in control. Changes in thoracic kyphosis correlated with
   magnitude of the Cobb angle (p < 0.001). No significant change was found
   in lumbar lordosis and the pelvic parameters. After substratification
   according to the Lenke classification and individually evaluating
   subgroups, results were similar with a significant decrease in only the
   thoracic kyphosis. A strong correlation was seen between sacral slope,
   pelvic incidence, and lumbar lordosis, and between pelvic version and
   thoracic kyphosis in control and scoliotic groups, whereas pelvic
   incidence was also seen to be correlated with thoracic kyphosis in
   scoliosis patients.
   Conclusion: Adolescent idiopathic scoliosis patients showed a
   significant decrease in thoracic kyphosis, and the magnitude of the
   decrease was directly related to the Cobb angle. Changes in pelvic
   incidence were minimal but were also significantly correlated with
   thoracic changes. Changes were similar though not identical to those
   seen in other Caucasian studies and differed from those in other
   ethnicities. Scoliotic curves and their effect on pelvic balance must
   still be regarded as individual to each patient, necessitating
   individual assessment, although changes perhaps can be predicted by
   patient ethnicity.}},
DOI = {{10.1186/s13013-018-0156-0}},
Article-Number = {{10}},
ISSN = {{2397-1789}},
ORCID-Numbers = {{Schlegl, Adam Tibor/0000-0003-0349-2525}},
Unique-ID = {{ISI:000435372100001}},
}

@article{ ISI:000440272800007,
Author = {Kwiek, Bartlomiej and Ambroziak, Marcin and Osipowicz, Katarzyna and
   Kowalewski, Cezary and Rozalski, Michal},
Title = {{Treatment of Previously Treated Facial Capillary Malformations: Results
   of Single-Center Retrospective Objective 3-Dimensional Analysis of the
   Efficacy of Large Spot 532 nm Lasers}},
Journal = {{DERMATOLOGIC SURGERY}},
Year = {{2018}},
Volume = {{44}},
Number = {{6}},
Pages = {{803-813}},
Month = {{JUN}},
Abstract = {{BACKGROUND Current treatment of facial capillary malformations (CM) has
   limited efficacy.
   OBJECTIVE To assess the efficacy of large spot 532 nm lasers for the
   treatment of previously treated facial CM with the use of 3-dimensional
   (3D) image analysis.
   PATIENTS AND METHODS Forty-three white patients aged 6 to 59 were
   included in this study. Patients had 3D photography performed before and
   after treatment with a 532 nm Nd:YAG laser with large spot and contact
   cooling. Objective analysis of percentage improvement based on 3D
   digital assessment of combined color and area improvement (global
   clearance effect {[}GCE]) were performed.
   RESULTS The median maximal improvement achieved during the treatment
   (GCE(max)) was 59.1\%. The mean number of laser procedures required to
   achieve this improvement was 6.2 (range 1-16). Improvement of minimum
   25\% (GCE25) was achieved by 88.4\% of patients, a minimum of 50\%
   (GCE50) by 61.1\%, a minimum of 75\% (GCE75) by 25.6\%, and a minimum of
   90\% (GCE90) by 4.6\%. Patients previously treated with pulsed dye
   lasers had a significantly less response than those treated with other
   modalities (GCE (max) 37.3\% vs 61.8\%, respectively).
   CONCLUSION A large spot 532 nm laser is effective in previously treated
   patients with facial CM.}},
DOI = {{10.1097/DSS.0000000000001447}},
ISSN = {{1076-0512}},
EISSN = {{1524-4725}},
ORCID-Numbers = {{Kowalewski, Cezary/0000-0002-6608-9066}},
Unique-ID = {{ISI:000440272800007}},
}

@article{ ISI:000435193700027,
Author = {Wang, Di and Brunner, Jasmin and Ma, Zhenyu and Lu, Hao and Hollaus,
   Markus and Pang, Yong and Pfeifer, Norbert},
Title = {{Separating Tree Photosynthetic and Non-Photosynthetic Components from
   Point Cloud Data Using Dynamic Segment Merging}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{5}},
Month = {{MAY}},
Abstract = {{Many biophysical forest properties such as wood volume and leaf area
   index (LAI) require prior knowledge on either photosynthetic or
   non-photosynthetic components. Laser scanning appears to be a helpful
   technique in nondestructively quantifying forest structures, as it can
   acquire an accurate three-dimensional point cloud of objects. In this
   study, we propose an unsupervised geometry-based method named Dynamic
   Segment Merging (DSM) to identify non-photosynthetic components of trees
   by semantically segmenting tree point clouds, and examining the linear
   shape prior of each resulting segment. We tested our method using one
   single tree dataset and four plot-level datasets, and compared our
   results to a supervised machine learning method. We further demonstrated
   that by using an optimal neighborhood selection method that involves
   multi-scale analysis, the results were improved. Our results showed that
   the overall accuracy ranged from 81.8\% to 92.0\% with an average value
   of 87.7\%. The supervised machine learning method had an average overall
   accuracy of 86.4\% for all datasets, on account of a collection of
   manually delineated representative training data. Our study indicates
   that separating tree photosynthetic and non-photosynthetic components
   from laser scanning data can be achieved in a fully unsupervised manner
   without the need of training data and user intervention.}},
DOI = {{10.3390/f9050252}},
Article-Number = {{252}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Wang, Di/T-2571-2018
   }},
ORCID-Numbers = {{Wang, Di/0000-0003-0232-8862
   Pang, Yong/0000-0002-9760-6580
   Pfeifer, Norbert/0000-0002-2348-7929}},
Unique-ID = {{ISI:000435193700027}},
}

@article{ ISI:000430653400001,
Author = {Lopez, R. and Gantet, P. and Julian, A. and Hitzel, A. and
   Herbault-Barres, B. and Alshehri, S. and Payoux, P.},
Title = {{Value of PET/CT 3D visualization of head and neck squamous cell
   carcinoma extended to mandible}},
Journal = {{JOURNAL OF CRANIO-MAXILLOFACIAL SURGERY}},
Year = {{2018}},
Volume = {{46}},
Number = {{5}},
Pages = {{743-748}},
Month = {{MAY}},
Abstract = {{Purpose: To study an original 3D visualization of head and neck squamous
   cell carcinoma extending to the mandible by using {[}18F]-NaF PET/CT and
   {[}18F]-FDG PET/CT imaging along with a new innovative FDG and NaF image
   analysis using dedicated software. The main interest of the 3D
   evaluation is to have a better visualization of bone extension in such
   cancers and that could also avoid unsatisfying surgical treatment later
   on. Patients and methods: A prospective study was carried out from
   November 2016 to September 2017. Twenty patients with head and neck
   squamous cell carcinoma extending to the mandible (stage 4 in the UICC
   classification) underwent {[}18F]-NaF and {[}18F]-FDG PET/CT. We
   compared the delineation of 3D quantification obtained with {[}18F]-NaF
   and {[}18F]-FDG PET/CT. In order to carry out this comparison, a method
   of visualisation and quantification of PET images was developed. This
   new approach was based on a process of quantification of radioactive
   activity within the mandibular bone that objectively defined the
   significant limits of this activity on PET images and on a 3D
   visualization. Furthermore, the spatial limits obtained by analysis of
   the PET/CT 3D images were compared to those obtained by
   histopathological examination of mandibular resection which confirmed
   intraosseous extension to the mandible. Results: The {[}18F]-NaF PET/CT
   imaging confirmed the mandibular extension in 85\% of cases and was not
   shown in {[}18F]-FDG PET/CT imaging. The {[}18F]-NaF PET/CT was
   significantly more accurate than {[}18F]-FDG PET/CT in 3D assessment of
   intraosseous extension of head and neck squamous cell carcinoma. This
   new 3D information shows the importance in the imaging approach of
   cancers. All cases of mandibular extension suspected on {[}18F]-NaF
   PET/CT imaging were confirmed based on histopathological results as a
   reference. Conclusions: The {[}18F]-NaF PET/CT 3D visualization should
   be included in the pre-treatment workups of head and neck cancers. With
   the use of a dedicated software which enables objective delineation of
   radioactive activity within the bone, it gives a very encouraging
   results. The {[}18F]-FDG PET/CT appears insufficient to confirm
   mandibular extension. This new 3D simulation management is expected to
   avoid under treatment of patients with intraosseous mandibular extension
   of head and neck cancers. However, there is also a need for a further
   study that will compare the interest of PET/CT and PET/MRI in this
   indication. (C) 2018 European Association for Cranio-Maxillo-Facial
   Surgery. Published by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jcms.2018.02.007}},
ISSN = {{1010-5182}},
EISSN = {{1878-4119}},
Unique-ID = {{ISI:000430653400001}},
}

@article{ ISI:000429200400012,
Author = {Majka, Piotr and Chlodzinska, Natalia and Turlejski, Krzysztof and
   Banasik, Tomasz and Djavadian, Ruzanna L. and Weglarz, Wladyslaw P. and
   Wojcik, Daniel K.},
Title = {{A three-dimensional stereotaxic atlas of the gray short-tailed opossum
   (Monodelphis domestica) brain}},
Journal = {{BRAIN STRUCTURE \& FUNCTION}},
Year = {{2018}},
Volume = {{223}},
Number = {{4}},
Pages = {{1779-1795}},
Month = {{MAY}},
Abstract = {{The gray short-tailed opossum (Monodelphis domestica) is a small
   marsupial gaining recognition as a laboratory animal in biomedical
   research. Despite numerous studies on opossum neuroanatomy, a consistent
   and comprehensive neuroanatomical reference for this species is still
   missing. Here we present the first three-dimensional, multimodal atlas
   of the Monodelphis opossum brain. It is based on four complementary
   imaging modalities: high resolution ex vivo magnetic resonance images,
   micro-computed tomography scans of the cranium, images of the face of
   the cutting block, and series of sections stained with the Nissl method
   and for myelinated fibers. Individual imaging modalities were
   reconstructed into a three-dimensional form and then registered to the
   MR image by means of affine and deformable registration routines. Based
   on a superimposition of the 3D images, 113 anatomical structures were
   demarcated and the volumes of individual regions were measured. The
   stereotaxic coordinate system was defined using a set of cranial
   landmarks: interaural line, bregma, and lambda, which allows for easy
   expression of any location within the brain with respect to the skull.
   The atlas is released under the Creative Commons license and available
   through various digital atlasing web services.}},
DOI = {{10.1007/s00429-017-1540-x}},
ISSN = {{1863-2653}},
EISSN = {{1863-2661}},
ResearcherID-Numbers = {{Weglarz, Wladyslaw/W-5770-2018
   Wojcik, Daniel K/C-6334-2008
   }},
ORCID-Numbers = {{Weglarz, Wladyslaw/0000-0002-3390-3615
   Majka, Piotr/0000-0002-9055-8686
   Wojcik, Daniel K/0000-0003-0812-9872
   Krzysztof, Turlejski/0000-0001-6708-7815
   Djavadian, Ruzanna/0000-0002-0416-0234}},
Unique-ID = {{ISI:000429200400012}},
}

@article{ ISI:000425652100003,
Author = {Li, Zhan and Schaefer, Michael and Strahler, Alan and Schaaf, Crystal
   and Jupp, David},
Title = {{On the utilization of novel spectral laser scanning for
   three-dimensional classification of vegetation elements}},
Journal = {{INTERFACE FOCUS}},
Year = {{2018}},
Volume = {{8}},
Number = {{2}},
Month = {{APR 6}},
Abstract = {{The Dual-Wavelength Echidna Lidar (DWEL), a full waveform terrestrial
   laser scanner (TLS), has been used to scan a variety of forested and
   agricultural environments. From these scanning campaigns, we summarize
   the benefits and challenges given by DWEL's novel coaxial
   dual-wavelength scanning technology, particularly for the
   three-dimensional (3D) classification of vegetation elements.
   Simultaneous scanning at both 1064 nm and 1548 nm by DWEL instruments
   provides a new spectral dimension to TLS data that joins the 3D spatial
   dimension of lidar as an information source. Our point cloud
   classification algorithm explores the utilization of both spectral and
   spatial attributes of individual points from DWEL scans and highlights
   the strengths and weaknesses of each attribute domain. The spectral and
   spatial attributes for vegetation element classification each perform
   better in different parts of vegetation (canopy interior, fine branches,
   coarse trunks, etc.) and under different vegetation conditions (dead or
   live, leaf-on or leaf-off, water content, etc.). These environmental
   characteristics of vegetation, convolved with the lidar instrument
   specifications and lidar data quality, result in the actual capabilities
   of spectral and spatial attributes to classify vegetation elements in 3D
   space. The spectral and spatial information domains thus complement each
   other in the classification process. The joint use of both not only
   enhances the classification accuracy but also reduces its variance
   across the multiple vegetation types we have examined, highlighting the
   value of the DWEL as a new source of 3D spectral information. Wider
   deployment of the DWEL instruments is in practice currently held back by
   challenges in instrument development and the demands of data processing
   required by coaxial dual-or multi-wavelength scanning. But the
   simultaneous 3D acquisition of both spectral and spatial features,
   offered by new multispectral scanning instruments such as the DWEL,
   opens doors to study biophysical and biochemical properties of forested
   and agricultural ecosystems at more detailed scales.}},
DOI = {{10.1098/rsfs.2017.0039}},
Article-Number = {{20170039}},
ISSN = {{2042-8898}},
EISSN = {{2042-8901}},
ORCID-Numbers = {{Schaefer, Michael/0000-0001-6584-9521
   Li, Zhan/0000-0001-6307-5200}},
Unique-ID = {{ISI:000425652100003}},
}

@article{ ISI:000433517100003,
Author = {Demisse, Girum G. and Aouada, Djamila and Ottersten, Bjorn},
Title = {{Deformation-Based 3D Facial Expression Representation}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2018}},
Volume = {{14}},
Number = {{1, S}},
Month = {{APR}},
Abstract = {{We propose a deformation-based representation for analyzing expressions
   fromthree-dimensional (3D) faces. A point cloud of a 3D face is
   decomposed into an ordered deformable set of curves that start from a
   fixed point. Subsequently, a mapping function is defined to identify the
   set of curves with an element of a high-dimensional matrix Lie group,
   specifically the direct product of SE(3). Representing 3D faces as an
   element of a high-dimensional Lie group has two main advantages. First,
   using the group structure, facial expressions can be decoupled from a
   neutral face. Second, an underlying non-linear facial expression
   manifold can be captured with the Lie group and mapped to a linear
   space, Lie algebra of the group. This opens up the possibility of
   classifying facial expressions with linear models without compromising
   the underlying manifold. Alternatively, linear combinations of
   linearised facial expressions can be mapped back from the Lie algebra to
   the Lie group. The approach is tested on the Binghamton University 3D
   Facial Expression (BU-3DFE) and the Bosphorus datasets. The results show
   that the proposed approach performed comparably, on the BU-3DFE dataset,
   without using features or extensive landmark points.}},
DOI = {{10.1145/3176649}},
Article-Number = {{17}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
ResearcherID-Numbers = {{Ottersten, Bjorn/G-1005-2011}},
ORCID-Numbers = {{Ottersten, Bjorn/0000-0003-2298-6774}},
Unique-ID = {{ISI:000433517100003}},
}

@article{ ISI:000424962000005,
Author = {Czerniawski, T. and Sankaran, B. and Nahangi, M. and Haas, C. and Leite,
   F.},
Title = {{6D DBSCAN-based segmentation of building point clouds for planar object
   classification}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2018}},
Volume = {{88}},
Pages = {{44-58}},
Month = {{APR}},
Abstract = {{Due to constraints in manufacturing and construction, buildings and many
   of the manmade objects within them are often rectangular and composed of
   planar parts. Detection and analysis of planes is, therefore, central to
   processing point clouds captured in these spaces. This paper presents a
   study of the semantic information stored in the planar objects of noisy
   building point clouds. The dataset considered is the Scene Meshes
   Dataset with aNNotations (SceneNN), a collection of over 100 indoor
   scenes captured by consumer-grade depth cameras. All planar objects
   within the dataset are detected using a new point cloud segmentation
   method that applies Density Based Spatial Clustering of Applications
   with Noise (DBSCAN) in a six dimensional clustering space. With all
   planes isolated, an extensive list of features describing the planes is
   extracted and studied using feature selection. Then dimensionality
   reduction and unsupervised learning are used to explore the
   discriminative ability of the final feature set as well as emergent
   class groupings. Finally, we train a bagged decision tree classifier
   that achieves 71.2\% accuracy in predicting the object class from which
   individual planes originate.}},
DOI = {{10.1016/j.autcon.2017.12.029}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
ORCID-Numbers = {{Czerniawski, Thomas/0000-0002-7310-6522}},
Unique-ID = {{ISI:000424962000005}},
}

@article{ ISI:000430067700001,
Author = {Landschoff, Jannes and Du Plessis, Anton and Griffiths, Charles L.},
Title = {{A micro X-ray computed tomography dataset of South African hermit crabs
   (Crustacea: Decapoda: Anomura: Paguroidea) containing scans of two rare
   specimens and three recently described species}},
Journal = {{GIGASCIENCE}},
Year = {{2018}},
Volume = {{7}},
Number = {{4}},
Month = {{MAR 14}},
Abstract = {{Background: Along with the conventional deposition of physical types at
   natural history museums, the deposition of 3-dimensional (3D) image data
   has been proposed for rare and valuable museum specimens, such as
   irrepla ceable type material. Findings: Micro computed tomography (mu
   CT) scan data of 5 hermit crab species from South Africa, including rare
   specimens and type material, depicted main identification
   characteristics of calcified body parts. However, low-image contrasts,
   especially in larger (> 50 mm total length) specimens, did not allow
   sufficient 3D reconstructions of weakly calcified and fine
   characteristics, such as soft tissue of the pleon, mouthparts, gills,
   and setation. Reconstructions of soft tissue were sometimes possible,
   depending on individual sample and scanning characteristics. The raw
   data of seven scans are publicly available for download from the GigaDB
   repository. Conclusions: Calcified body parts visualized from mu CT data
   can aid taxonomic validation and provide additional, virtual deposition
   of rare specimens. The use of a nondestructive, nonstaining mu CT
   approach for taxonomy, reconstructions of soft tissue structures,
   microscopic spines, and setae depend on species characteristics.
   Constrained to these limitations, the presented dataset can be used for
   future morphological studies. However, our virtual specimens will be
   most valuable to taxonomists who can download a digital avatar for 3D
   examination. Simultaneously, in the event of physical damage to or loss
   of the original physical specimen, this dataset serves as a vital
   insurance policy.}},
DOI = {{10.1093/gigascience/giy022}},
ISSN = {{2047-217X}},
ORCID-Numbers = {{du Plessis, Anton/0000-0002-4370-8661}},
Unique-ID = {{ISI:000430067700001}},
}

@article{ ISI:000428508200022,
Author = {Alonzo, Michael and Andersen, Hans-Erik and Morton, Douglas C. and Cook,
   Bruce D.},
Title = {{Quantifying Boreal Forest Structure and Composition Using UAV Structure
   from Motion}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{The vast extent and inaccessibility of boreal forest ecosystems are
   barriers to routine monitoring of forest structure and composition. In
   this research, we bridge the scale gap between intensive but sparse plot
   measurements and extensive remote sensing studies by collecting forest
   inventory variables at the plot scale using an unmanned aerial vehicle
   (UAV) and a structure from motion (SfM) approach. At 20 Forest Inventory
   and Analysis (FIA) subplots in interior Alaska, we acquired overlapping
   imagery and generated dense, 3D, RGB (red, green, blue) point clouds. We
   used these data to model forest type at the individual crown scale as
   well as subplot-scale tree density (TD), basal area (BA), and
   aboveground biomass (AGB). We achieved 85\% cross-validation accuracy
   for five species at the crown level. Classification accuracy was
   maximized using three variables representing crown height, form, and
   color. Consistent with previous UAV-based studies, SfM point cloud data
   generated robust models of TD (r(2) = 0.91), BA (r(2) = 0.79), and AGB
   (r(2) = 0.92), using a mix of plot-and crown-scale information. Precise
   estimation of TD required either segment counts or species information
   to differentiate black spruce from mixed white spruce plots. The
   accuracy of species-specific estimates of TD, BA, and AGB at the plot
   scale was somewhat variable, ranging from accurate estimates of black
   spruce TD (+/1\%) and aspen BA (-2\%) to misallocation of aspen AGB
   (+118\%) and white spruce AGB (-50\%). These results convey the
   potential utility of SfM data for forest type discrimination in FIA
   plots and the remaining challenges to develop classification approaches
   for species-specific estimates at the plot scale that are more robust to
   segmentation error.}},
DOI = {{10.3390/f9030119}},
Article-Number = {{119}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Morton, Douglas/D-5044-2012}},
Unique-ID = {{ISI:000428508200022}},
}

@article{ ISI:000428936900023,
Author = {Yuan, Lin and Chen, Fanglin and Zeng, Ling-Li and Wang, Lubin and Hu,
   Dewen},
Title = {{Gender Identification of Human Brain Image with A Novel 3D Descriptor}},
Journal = {{IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS}},
Year = {{2018}},
Volume = {{15}},
Number = {{2}},
Pages = {{551-561}},
Month = {{MAR-APR}},
Abstract = {{Determining gender by examining the human brain is not a simple task
   because the spatial structure of the human brain is complex, and no
   obvious differences can be seen by the naked eyes. In this paper, we
   propose a novel three-dimensional feature descriptor, the
   three-dimensional weighted histogram of gradient orientation (3D WHGO)
   to describe this complex spatial structure. The descriptor combines
   local information for signal intensity and global three-dimensional
   spatial information for the whole brain. We also improve a framework to
   address the classification of three-dimensional images based on MRI.
   This framework, three-dimensional spatial pyramid, uses additional
   information regarding the spatial relationship between features. The
   proposed method can be used to distinguish gender at the individual
   level. We examine our method by using the gender identification of
   individual magnetic resonance imaging (MRI) scans of a large sample of
   healthy adults across four research sites, resulting in up to
   individual-level accuracies under the optimized parameters for
   distinguishing between females and males. Compared with previous
   methods, the proposed method obtains higher accuracy, which suggests
   that this technology has higher discriminative power. With its improved
   performance in gender identification, the proposed method may have the
   potential to inform clinical practice and aid in research on
   neurological and psychiatric disorders.}},
DOI = {{10.1109/TCBB.2015.2448081}},
ISSN = {{1545-5963}},
EISSN = {{1557-9964}},
ResearcherID-Numbers = {{Hu, Dewen/D-1978-2015}},
ORCID-Numbers = {{Hu, Dewen/0000-0001-7357-0053}},
Unique-ID = {{ISI:000428936900023}},
}

@article{ ISI:000427548400003,
Author = {Hu, Guiqing and Taylor, Dianne W. and Liu, Jun and Taylor, Kenneth A.},
Title = {{Identification of interfaces involved in weak interactions with
   application to F-actin-aldolase rafts}},
Journal = {{JOURNAL OF STRUCTURAL BIOLOGY}},
Year = {{2018}},
Volume = {{201}},
Number = {{3}},
Pages = {{199-209}},
Month = {{MAR}},
Abstract = {{Macromolecular interactions occur with widely varying affinities. Strong
   interactions form well defined interfaces but weak interactions are more
   dynamic and variable. Weak interactions can collectively lead to large
   structures such as microvilli via cooperativity and are often the
   precursors of much stronger interactions, e.g. the initial actin-myosin
   interaction during muscle contraction. Electron tomography combined with
   subvolume alignment and classification is an ideal method for the study
   of weak interactions because a 3-D image is obtained for the individual
   interactions, which subsequently are characterized collectively. Here we
   describe a method to characterize heterogeneous F-actin-aldolase
   interactions in 2-D rafts using electron tomography. By forming separate
   averages of the two constituents and fitting an atomic structure to each
   average, together with the alignment information which relates the raw
   motif to the average, an atomic model of each crosslink is determined
   and a frequency map of contact residues is computed. The approach should
   be applicable to any large structure composed of constituents that
   interact weakly and heterogeneously.}},
DOI = {{10.1016/j.jsb.2017.11.005}},
ISSN = {{1047-8477}},
EISSN = {{1095-8657}},
Unique-ID = {{ISI:000427548400003}},
}

@article{ ISI:000427313700006,
Author = {Herfort, Benjamin and Hoefle, Bernhard and Klonner, Carolin},
Title = {{3D micro-mapping: Towards assessing the quality of crowdsourcing to
   support 3D point cloud analysis}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2018}},
Volume = {{137}},
Pages = {{73-83}},
Month = {{MAR}},
Abstract = {{In this paper, we propose a method to crowdsource the task of complex
   three-dimensional information extraction from 3D point clouds. We design
   web-based 3D micro tasks tailored to assess segmented LiDAR point clouds
   of urban trees and investigate the quality of the approach in an
   empirical user study. Our results for three different experiments with
   increasing complexity indicate that a single crowd sourcing task can be
   solved in a very short time of less than five seconds on average.
   Furthermore, the results of our empirical case study reveal that the
   accuracy, sensitivity and precision of 3D crowdsourcing are high for
   most information extraction problems. For our first experiment (binary
   classification with single answer) we obtain an accuracy of 91\%, a
   sensitivity of 95\% and a precision of 92\%. For the more complex tasks
   of the second Experiment 2 (multiple answer classification) the accuracy
   ranges from 65\% to 99\% depending on the label class. Regarding the
   third experiment - the determination of the crown base height of
   individual trees - our study highlights that crowdsourcing can be a tool
   to obtain values with even higher accuracy in comparison to an automated
   computer-based approach. Finally, we found out that the accuracy of the
   crowdsourced results for all experiments is hardly influenced by
   characteristics of the input point cloud data and of the users.
   Importantly, the results' accuracy can be estimated using agreement
   among volunteers as an intrinsic indicator, which makes a broad
   application of 3D micro-mapping very promising. (C) 2018 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2018.01.009}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Hofle, Bernhard/A-4702-2010
   }},
ORCID-Numbers = {{Hofle, Bernhard/0000-0001-5849-1461
   Klonner, Carolin/0000-0003-1981-2204}},
Unique-ID = {{ISI:000427313700006}},
}

@article{ ISI:000418312200002,
Author = {Kukunda, Collins B. and Duque-Lazo, Joaquin and Gonzalez-Ferreiro,
   Eduardo and Thaden, Hauke and Kleinn, Christoph},
Title = {{Ensemble classification of individual Pinus crowns from multispectral
   satellite imagery and airborne LiDAR}},
Journal = {{INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION}},
Year = {{2018}},
Volume = {{65}},
Pages = {{12-23}},
Month = {{MAR}},
Abstract = {{Distinguishing tree species is relevant in many contexts of remote
   sensing assisted forest inventory. Accurate tree species maps support
   management and conservation planning, pest and disease control and
   biomass estimation. This study evaluated the performance of applying
   ensemble techniques with the goal of automatically distinguishing Pinus
   sylvestris L. and Pinus uncinata Mill. Ex Mirb within a 1.3 km(2)
   mountainous area in Barcelonnette (France). Three modelling schemes were
   examined, based on: (1) high-density LiDAR data (160 returns m(-2)), (2)
   Worldview-2 multispectral imagery, and (3) Worldview-2 and LiDAR in
   combination. Variables related to the crown structure and height of
   individual trees were extracted from the normalized LiDAR point cloud at
   individual-tree level, after performing individual tree crown (ITC)
   delineation. Vegetation indices and the Haralick texture indices were
   derived from Worldview-2 images and served as independent spectral
   variables. Selection of the best predictor subset was done after a
   comparison of three variable selection procedures: (1) Random Forests
   with cross validation (AUCREcv), (2) Akaike Information Criterion (AIC)
   and (3) Bayesian Information Criterion (BIC). To classify the species, 9
   regression techniques were combined using ensemble models. Predictions
   were evaluated using cross validation and an independent dataset.
   Integration of datasets and models improved individual tree species
   classification (True Skills Statistic, TSS; from 0.67 to 0.81) over
   individual techniques and maintained strong predictive power (Relative
   Operating Characteristic, ROC = 0.91). Assemblage of regression models
   and integration of the datasets provided more reliable species
   distribution maps and associated tree-scale mapping uncertainties. Our
   study highlights the potential of model and data assemblage at improving
   species classifications needed in present-day forest planning and
   management.}},
DOI = {{10.1016/j.jag.2017.09.016}},
ISSN = {{0303-2434}},
ResearcherID-Numbers = {{Duque-Lazo, Joaquin/L-3722-2019
   Gonzalez-Ferreiro, Eduardo/Q-9709-2016}},
ORCID-Numbers = {{Duque-Lazo, Joaquin/0000-0003-4223-5070
   Gonzalez-Ferreiro, Eduardo/0000-0002-4565-2155}},
Unique-ID = {{ISI:000418312200002}},
}

@article{ ISI:000427009100019,
Author = {Ye, Cang and Qian, Xiangfei},
Title = {{3-D Object Recognition of a Robotic Navigation Aid for the Visually
   Impaired}},
Journal = {{IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING}},
Year = {{2018}},
Volume = {{26}},
Number = {{2}},
Pages = {{441-450}},
Month = {{FEB}},
Abstract = {{This paper presents a 3-D object recognition method and its
   implementation on a robotic navigation aid to allow real-time detection
   of indoor structural objects for the navigation of a blind person. The
   method segments a point cloud into numerous planar patches and extracts
   their inter-plane relationships (IPRs). Based on the existing IPRs of
   the object models, the method defines six high level features (HLFs) and
   determines the HLFs for each patch. A Gaussian-mixture-model-based plane
   classifier is then devised to classify each planar patch into one
   belonging to a particular object model. Finally, a recursive plane
   clustering procedure is used to cluster the classified planes into the
   model objects. As the proposed method uses geometric context to detect
   an object, it is robust to the object's visual appearance change. As a
   result, it is ideal for detecting structural objects (e.g., stairways,
   doorways, and so on). In addition, it has high scalability and
   parallelism. The method is also capable of detecting some indoor
   nonstructural objects. Experimental results demonstrate that the
   proposed method has a high success rate in object recognition.}},
DOI = {{10.1109/TNSRE.2017.2748419}},
ISSN = {{1534-4320}},
EISSN = {{1558-0210}},
Unique-ID = {{ISI:000427009100019}},
}

@article{ ISI:000418370200123,
Author = {Li, Ye and Wang, YingHui and Liu, Jing and Hao, Wen},
Title = {{Expression-insensitive 3D face recognition by the fusion of multiple
   subject-specific curves}},
Journal = {{NEUROCOMPUTING}},
Year = {{2018}},
Volume = {{275}},
Pages = {{1295-1307}},
Month = {{JAN 31}},
Abstract = {{This study proposes a 3D face recognition method using multiple
   subject-specific curves insensitive to intra-subject distortions caused
   by expression variations. Considering that most sharp variances in
   facial convex regions are closely related to the bone structure, the
   convex crest curves are first extracted as the most vital
   subject-specific facial curves based on the principal curvature extrema
   in convex local surfaces. Then, the central profile curve and the
   horizontal contour curve passing through the nose tip are detected by
   using the precise localization of the nose tip and symmetry plane. Based
   on their discriminative power and robustness to expression changes, the
   three types of curves are fused with appropriate weights at the
   feature-level and used for matching 3D faces with the iterative closest
   point algorithm. The combination of multiple expression-insensitive
   curves is complementary and provides sufficient and stable facial
   surface features for face recognition. In addition, for each convex
   crest curve, an expression-irrelevant factor is assigned as the adaptive
   weight to improve the face matching performance. The results of
   experiments using two public 3D databases, GavabDB and BU-3DFE,
   demonstrate the effectiveness of the proposed method, and its
   recognition rates on both databases reflect an encouraging performance.
   (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2017.09.070}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000418370200123}},
}

@article{ ISI:000423097800017,
Author = {Crouch, Daniel J. M. and Winney, Bruce and Koppen, Willem P. and
   Christmas, William J. and Hutnik, Katarzyna and Day, Tammy and Meena,
   Devendra and Boumertit, Abdelhamid and Hysi, Pirro and Nessa, Ayrun and
   Spector, Tim D. and Kittler, Josef and Bodmer, Walter F.},
Title = {{Genetics of the human face: Identification of large-effect single gene
   variants}},
Journal = {{PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA}},
Year = {{2018}},
Volume = {{115}},
Number = {{4}},
Pages = {{E676-E685}},
Month = {{JAN 23}},
Abstract = {{To discover specific variants with relatively large effects on the human
   face, we have devised an approach to identifying facial features with
   high heritability. This is based on using twin data to estimate the
   additive genetic value of each point on a face, as provided by a 3D
   camera system. In addition, we have used the ethnic difference between
   East Asian and European faces as a further source of face genetic
   variation. We use principal components (PCs) analysis to provide a fine
   definition of the surface features of human faces around the eyes and of
   the profile, and chose upper and lower 10\% extremes of the most
   heritable PCs for looking for genetic associations. Using this strategy
   for the analysis of 3D images of 1,832 unique volunteers from the
   well-characterized People of the British Isles study and 1,567 unique
   twin images from the TwinsUK cohort, together with genetic data for
   500,000 SNPs, we have identified three specific genetic variants with
   notable effects on facial profiles and eyes.}},
DOI = {{10.1073/pnas.1708207114}},
ISSN = {{0027-8424}},
Unique-ID = {{ISI:000423097800017}},
}

@inproceedings{ ISI:000462163500103,
Author = {Ju, Xiangyang and Garcia Junior, Idelmo Rangel and Silva, Leonardo de
   Freitas and Mossey, Peter and Al-Rudainy, Dhelal and Ayoub, Ashraf and
   de Mattos, Adriana Marques},
Editor = {{Li, W and Li, Q and Wang, L}},
Title = {{3D Head Shape Analysis of Suspected Zika Infected Infants}},
Booktitle = {{2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018)}},
Year = {{2018}},
Note = {{11th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI), Beijing, PEOPLES R CHINA, OCT
   13-15, 2018}},
Organization = {{IEEE; IEEE Engn Med \& Biol Soc; Beijing Univ Chem Technol; Beijing Inst
   Technol; E China Normal Univ}},
Abstract = {{The babies infected from Zika before they are born are at risk for
   problems with brain development and microcephaly. 3D head images of 43
   Zika cases and 43 controls were collected aiming to extract shape
   characteristics for diagnosis purposes. Principal component analysis
   (PCA) has been applied on the vaults and faces of the collected 3D
   images and the scores on the second principal components of the vaults
   and faces showed significant differences between the control and Zika
   groups. The shape variations from -2s to 2s illustrated the typical
   characteristics of microcephaly of the Zika babies. Canonical
   correlation analysis (CCA) showed a significant correlation in the first
   CCA variates of face and vault which indicated the potential of 3D
   facial imaging for Zika surveillance. Further head circumferences and
   distances from ear to ear were measured from the 3D images and
   preliminary results showed the adding ear to ear distances for
   classifying control and Zika children strengthened the abilities of
   tested classification models.}},
ISBN = {{978-1-5386-7604-2}},
Unique-ID = {{ISI:000462163500103}},
}

@inproceedings{ ISI:000457881301017,
Author = {Akita, Tokihiko and Yamauchi, Yuji and Fujiyoshi, Hironobu},
Book-Group-Author = {{IEEE}},
Title = {{Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye
   Cameras}},
Booktitle = {{2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)}},
Series = {{IEEE International Conference on Intelligent Transportation Systems-ITSC}},
Year = {{2018}},
Pages = {{1103-1108}},
Note = {{21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC), Maui, HI, NOV 04-07, 2018}},
Organization = {{IEEE; IEEE Intelligent Transportat Syst Soc}},
Abstract = {{Recently, automated emergency brake systems for pedestrian have been
   commercialized. However, they cannot detect crossing pedestrians when
   turning at intersections because the field of view is not wide enough.
   Thus, we propose to utilize a surround view camera system becoming
   popular by making it into stereo vision which is robust for the
   pedestrian recognition. However, conventional stereo camera technologies
   cannot be applied due to fisheye cameras and uncalibrated camera poses.
   Thus we have created the new method to absorb difference of the
   pedestrian appearance between cameras by machine learning for the stereo
   vision. The method of stereo matching between image patches in each
   camera image was designed by combining D-Brief and NCC with SVM. Good
   generalization performance was achieved by it compared with individual
   conventional algorithms. Furthermore, feature amounts of the point cloud
   reconstructed by the stereo pairs are utilized with Random Forest to
   discriminate pedestrians. The algorithm was evaluated for the actual
   camera images of crossing pedestrians at various intersections, and
   96.0\% of pedestrian tracking rate with high position detection accuracy
   was achieved. They were compared with Faster R-CNN as the best pattern
   recognition technique, and our proposed method indicated better
   detection performance.}},
ISSN = {{2153-0009}},
ISBN = {{978-1-7281-0323-5}},
Unique-ID = {{ISI:000457881301017}},
}

@inproceedings{ ISI:000457843605028,
Author = {Cheng, Shiyang and Kotsia, Irene and Pantic, Maja and Zafeiriou,
   Stefanos},
Book-Group-Author = {{IEEE}},
Title = {{4DFAB: A Large Scale 4D Database for Facial Expression Analysis and
   Biometric Applications}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{5117-5126}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{The progress we are currently witnessing in many computer vision
   applications, including automatic face analysis, would not be made
   possible without tremendous efforts in collecting and annotating large
   scale visual databases. To this end, we propose 4DFAB, a new large scale
   database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes).
   4DFAB contains recordings of 180 subjects captured in four different
   sessions spanning over a five-year period. It contains 4D videos of
   subjects displaying both spontaneous and posed facial behaviours. The
   database can be used for both face and facial expression recognition, as
   well as behavioural biometrics. It can also be used to learn very
   powerful blendshapes for parametrising facial behaviour. In this paper,
   we conduct several experiments and demonstrate the usefulness of the
   database for various applications. The database will be made publicly
   available for research purposes.}},
DOI = {{10.1109/CVPR.2018.00537}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843605028}},
}

@inproceedings{ ISI:000457843609059,
Author = {Li, Jiaxin and Chen, Ben M. and Lee, Gim Hee},
Book-Group-Author = {{IEEE}},
Title = {{SO-Net: Self-Organizing Network for Point Cloud Analysis}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{9397-9406}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{This paper presents SO-Net, a permutation invariant architecture for
   deep learning with orderless point clouds. The SO-Net models the spatial
   distribution of point cloud by building a Self-Organizing Map (SOM).
   Based on the SOM, SO-Net performs hierarchical feature extraction on
   individual points and SOM nodes, and ultimately represents the input
   point cloud by a single feature vector. The receptive field of the
   network can be systematically adjusted by conducting point-to-node k
   nearest neighbor search. In recognition tasks such as point cloud
   reconstruction, classification, object part segmentation and shape
   retrieval, our proposed network demonstrates performance that is similar
   with or better than state-of-the-art approaches. In addition, the
   training speed is significantly faster than existing point cloud
   recognition networks because of the parallelizability and simplicity of
   the proposed architecture. Our code is available at the project
   website.(1)}},
DOI = {{10.1109/CVPR.2018.00979}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843609059}},
}

@inproceedings{ ISI:000455305000011,
Author = {Harikumar, A. and Paris, C. and Bovolo, F. and Bruzzone, L.},
Editor = {{Bruzzone, L and Bovolo, F}},
Title = {{A novel data-driven approach to tree species classification using high
   density multireturn airborne lidar data}},
Booktitle = {{IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXIV}},
Series = {{Proceedings of SPIE}},
Year = {{2018}},
Volume = {{10789}},
Note = {{Conference on Image and Signal Processing for Remote Sensing XXIV,
   Berlin, GERMANY, SEP 10-12, 2018}},
Organization = {{SPIE}},
Abstract = {{Tree species information is crucial for accurate forest parameter
   estimation. Small footprint high density multi-return Light Detection
   and Ranging (LiDAR) data contain a large amount of structural details
   for modelling and thus distinguishing individual tree species. To fully
   exploit the potential of these data, we propose a data-driven tree
   species classification approach based on a volumetric analysis of
   single-tree-point-cloud that extracts features that are able to
   characterize both the internal and the external crown structure. The
   method captures the spatial distribution of the LiDAR points within the
   crown by generating a feature vector representing the three-dimensional
   (3D) crown information. Each element in the feature vector uniquely
   corresponds to an Elementary Quantization Volume (EQV) of the crown.
   Three strategies have been defined to generate unique EQVs that model
   different representations of the crown components. The classification is
   performed by using a Support Vector Machines (C-SVM) classifier using
   the histogram intersection kernel that has the enhanced ability to give
   maximum preference to the key features in high dimensional feature
   space. All the experiments were performed on a set of 200 trees
   belonging to Norway Spruce, European Larch, Swiss Pine, and Silver Fir
   (i.e., 50 trees per species). The classifier is trained using 120 trees
   and tested on an independent set of 80 trees. The proposed method
   outperforms the classification performance of the state-of-the-art
   method used for comparison.}},
DOI = {{10.1117/12.2325634}},
Article-Number = {{UNSP 107890E}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-2162-6}},
ORCID-Numbers = {{Paris, Claudia/0000-0002-7189-6268}},
Unique-ID = {{ISI:000455305000011}},
}

@inproceedings{ ISI:000455343100004,
Author = {Desai, Kevin and Prabhakaran, Balakrishnan and Raghuraman, Suraj},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{Combining Skeletal Poses for 3D Human Model Generation using Multiple
   Kinects}},
Booktitle = {{PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18)}},
Year = {{2018}},
Pages = {{40-51}},
Note = {{9th ACM Multimedia Systems Conference (MMSys), Centrum Wiskunde \&
   Informatica Amsterdam, Amsterdam, NETHERLANDS, JUN 12-15, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGMM; ACM SIGOPS; ACM SIGMOBILE; ACM SIGCOMM}},
Abstract = {{RGB-D cameras, such as the Microsoft Kinect, provide us with the 3D
   information, color and depth, associated with the scene. Interactive 3D
   Tele-Immersion (i3DTI) systems use such RGB-D cameras to capture the
   person present in the scene in order to collaborate with other remote
   users and interact with the virtual objects present in the environment.
   Using a single camera, it becomes difficult to estimate an accurate
   skeletal pose and complete 3D model of the person, especially when the
   person is not in the complete view of the camera. With multiple cameras,
   even with partial views, it is possible to get a more accurate estimate
   of the skeleton of the person leading to a better and complete 3D model.
   In this paper, we present a real-time skeletal pose identification
   approach that leverages on the inaccurate skeletons of the individual
   Kinects, and provides a combined optimized skeleton. We estimate the
   Probability of an Accurate Joint (PAJ) for each joint from all of the
   Kinect skeletons. We determine the correct direction of the person and
   assign the correct joint sides for each skeleton. We then use a greedy
   consensus approach to combine the highly probable and accurate joints to
   estimate the combined skeleton. Using the individual skeletons, we
   segment the point clouds from all the cameras. We use the already
   computed PAJ values to obtain the Probability of an Accurate Bone (PAB).
   The individual point clouds are then combined one segment after another
   using the calculated PAB values. The generated combined point cloud is a
   complete and accurate 3D representation of the person present in the
   scene. We validate our estimated skeleton against two well-known methods
   by computing the error distance between the best view Kinect skeleton
   and the estimated skeleton. An exhaustive analysis is performed by using
   around 500000 skeletal frames in total, captured using 7 users and 7
   cameras. Visual analysis is performed by checking whether the estimated
   skeleton is completely present within the human model. We also develop a
   3D Holo-Bubble game to showcase the real-time performance of the
   combined skeleton and point cloud. Our results show that our method
   performs better than the state-of-the-art approaches that use multiple
   Kinects, in terms of objective error, visual quality and real-time user
   performance.}},
DOI = {{10.1145/3204949.3204958}},
ISBN = {{978-1-4503-5192-8}},
ORCID-Numbers = {{Desai, Kevin/0000-0002-2964-8981}},
Unique-ID = {{ISI:000455343100004}},
}

@article{ ISI:000455069200012,
Author = {Wang, Kaishi and Jiang, Yi and Zhang, Zhifei and Lu, Yongtian and Ni,
   Yusu},
Title = {{Extension of the Clinical Significance of the ``Cog{''}}},
Journal = {{ORL-JOURNAL FOR OTO-RHINO-LARYNGOLOGY HEAD AND NECK SURGERY}},
Year = {{2018}},
Volume = {{80}},
Number = {{5-6}},
Pages = {{317-325}},
Abstract = {{Objective: To study the clinical anatomy of the epitympanum, the attic,
   and its medial wall, to try to discover a new clinical operation-related
   anatomical landmark, and to investigate the adjacent anatomical
   relationship with this landmark. Materials and Methods: Eight donor
   temporal bone specimens were dissected endoscopically. For 29 healthy
   persons (17 males and 12 females), CT images of the temporal bone (57
   ears) were taken, 3-dimensional (3-D) reconstruction and
   multidimensional plane reconstruction were performed, and identification
   and assessment of 3-D spatial relationships between any 2 of these
   complex structures were done. Results: 3-D images of the temporal bone
   structures including the facial nerve, the cochlea, the semicircular
   canal, and the brain plate were reconstructed and shown in detail. We
   discovered a new clinical surgery-related anatomical landmark (the
   ``cog{''} tangent and the trailing edge of the cog). Based on the
   tangent and the trailing edge of the cog, we quantified the anatomical
   relationship between it and its neighboring important structures.
   Conclusion: Based on endoscopic anatomy and the temporal bone spiral CT
   3-D structure reconstruction of the epitympanum, the attic, and the
   adjacent structures, we found an extension of the clinical significance
   the cog. Quantification of the adjacent anatomical relationship of this
   landmark is very important for otology microsurgical operation. (c) 2018
   S. Karger AG, Basel}},
DOI = {{10.1159/000493012}},
ISSN = {{0301-1569}},
EISSN = {{1423-0275}},
Unique-ID = {{ISI:000455069200012}},
}

@inproceedings{ ISI:000451039807048,
Author = {Li, Shihua and Su, Lian and Liu, Yuhan and He, Ze},
Book-Group-Author = {{IEEE}},
Title = {{SEGMENTATION OF INDIVIDUAL TREES BASED ON A POINT CLOUD CLUSTERING
   METHOD USING AIRBORNE LIDAR DATA}},
Booktitle = {{IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING
   SYMPOSIUM}},
Series = {{IEEE International Symposium on Geoscience and Remote Sensing IGARSS}},
Year = {{2018}},
Pages = {{7520-7523}},
Note = {{38th IEEE International Geoscience and Remote Sensing Symposium
   (IGARSS), Valencia, SPAIN, JUL 22-27, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Geoscience
   \& Remote Sensing Soc; European Space Agcy}},
Abstract = {{The objective of this paper was to develop a new algorithm to segment
   individual trees directly by using the three-dimensional space
   characteristic of airborne light detection and ranging point cloud data.
   The local maximum method was used in the initial segmentation and the
   error identification tree exclusion. On the basis of the point cloud
   spatial distribution of individual trees and the adjacent relationship
   with the other trees, a point cloud clustering method was developed to
   decide the points belonging to the individual trees. This algorithm was
   tested by 6 forest plots in the Genhe forestry reserve. The results
   showed that this algorithm could segment individual trees quickly and
   accurately, and the overall accuracy of this algorithm was 96.3\%.}},
ISSN = {{2153-6996}},
ISBN = {{978-1-5386-7150-4}},
Unique-ID = {{ISI:000451039807048}},
}

@inproceedings{ ISI:000446394502083,
Author = {Mitash, Chaitanya and Boularias, Abdeslam and Bekris, Kostas E.},
Book-Group-Author = {{IEEE}},
Title = {{Improving 6D Pose Estimation of Objects in Clutter via Physics-aware
   Monte Carlo Tree Search}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)}},
Series = {{IEEE International Conference on Robotics and Automation ICRA}},
Year = {{2018}},
Pages = {{3331-3338}},
Note = {{IEEE International Conference on Robotics and Automation (ICRA),
   Brisbane, AUSTRALIA, MAY 21-25, 2018}},
Organization = {{IEEE; CSIRO; Australian Govt, Dept Def Sci \& Technol; DJI; Queensland
   Univ Technol; Woodside; Baidu; Bosch; Houston Mechatron; Kinova Robot;
   KUKA; Hit Robot Grp; Honda Res Inst; iRobot; Mathworks; NuTonomy;
   Ouster; Uber}},
Abstract = {{This work proposes a process for efficiently searching over combinations
   of individual object 6D pose hypotheses in cluttered scenes, especially
   in cases involving occlusions and objects resting on each other. The
   initial set of candidate object poses is generated from state-of-the-art
   object detection and global point cloud registration techniques. The
   best scored pose per object by using these techniques may not be
   accurate due to overlaps and occlusions. Nevertheless, experimental
   indications provided in this work show that object poses with lower
   ranks may be closer to the real poses than ones with high ranks
   according to registration techniques. This motivates a global
   optimization process for improving these poses by taking into account
   scene-level physical interactions between objects. It also implies that
   the Cartesian product of candidate poses for interacting objects must be
   searched so as to identify the best scene-level hypothesis. To perform
   the search efficiently, the candidate poses for each object are
   clustered so as to reduce their number but still keep a sufficient
   diversity. Then, searching over the combinations of candidate object
   poses is performed through a Monte Carlo Tree Search (MCTS) process that
   uses the similarity between the observed depth image of the scene and a
   rendering of the scene given the hypothesized pose as a score that
   guides the search procedure. MCTS handles in a principled way the
   tradeoff between fine-tuning the most promising poses and exploring new
   ones, by using the Upper Confidence Bound (UCB) technique. Experimental
   results indicate that this process is able to quickly identify in
   cluttered scenes physically-consistent object poses that are
   significantly closer to ground truth compared to poses found by point
   cloud registration methods.}},
ISSN = {{1050-4729}},
ISBN = {{978-1-5386-3081-5}},
Unique-ID = {{ISI:000446394502083}},
}

@article{ ISI:000445394200001,
Author = {Hentz, Angela M. K. and Silva, Carlos A. and Dalla Corte, Ana P. and
   Netto, Sylvio P. and Stager, Michael P. and Klauberg, Carine},
Title = {{Estimating forest uniformity in Eucalyptus spp. and Pinus taeda L.
   stands using field measurements and structure from motion point clouds
   generated from unmanned aerial vehicle (UAV) data collection}},
Journal = {{FOREST SYSTEMS}},
Year = {{2018}},
Volume = {{27}},
Number = {{2}},
Abstract = {{Aim of study: In this study we applied 3D point clouds generated by
   images obtained from an Unmanned Aerial Vehicle (UAV) to evaluate the
   uniformity of young forest stands.
   Area of study: Two commercial forest stands were selected, with two
   plots each. The forest species studied were Eucalyptus spp. and Pinus
   taeda L. and the trees had an age of 1.5 years.
   Material and methods: The individual trees were detected based on
   watershed segmentation and local maxima, using the spectral values
   stored in the point cloud. After the tree detection, the heights were
   calculated using two approaches, in the first one using the Digital
   Surface Model (DSM) and a Digital Terrain Model, and in the second using
   only the DSM. We used the UAV-derived heights to estimate an uniformity
   index.
   Main results: The trees were detected with a maximum 6\% of error.
   However, the height was underestimated in all cases, in an average of 1
   and 0.7 m for Pinus and Eucalyptus stands. We proposed to use the models
   built herein to estimate tree height, but the regression models did not
   explain the variably within the data satisfactorily. Therefore, the
   uniformity index calculated using the direct UAV-height values presented
   results close to the field inventory, reaching better results when using
   the second height approach (error ranging 2.8-7.8\%).
   Research highlights: The uniformity index using the UAV-derived height
   from the proposed methods was close to the values obtained in field. We
   noted the potential for using UAV imagery in forest monitoring.}},
DOI = {{10.5424/fs/2018272-11713}},
Article-Number = {{UNSP e005}},
ISSN = {{2171-5068}},
EISSN = {{2171-9845}},
Unique-ID = {{ISI:000445394200001}},
}

@article{ ISI:000425828200076,
Author = {Anderson, Kyle E. and Glenn, Nancy F. and Spaete, Lucas P. and
   Shinneman, Douglas J. and Pilliod, David S. and Arkle, Robert S. and
   McIlroy, Susan K. and Derryberry, DeWayne R.},
Title = {{Estimating vegetation biomass and cover across large plots in shrub and
   grass dominated drylands using terrestrial lidar and machine learning}},
Journal = {{ECOLOGICAL INDICATORS}},
Year = {{2018}},
Volume = {{84}},
Pages = {{793-802}},
Month = {{JAN}},
Abstract = {{Terrestrial laser scanning (TLS) has been shown to enable an efficient,
   precise, and non-destructive inventory of vegetation structure at ranges
   up to hundreds of meters. We developed a method that leverages TLS
   collections with machine learning techniques to model and map canopy
   cover and biomass of several classes of short-stature vegetation across
   large plots. We collected high-definition TLS scans of 26 1-ha plots in
   desert grasslands and big sagebrush shrublands in southwest Idaho, USA.
   We used the Random Forests machine learning algorithm to develop
   decision tree models predicting the biomass and canopy cover of several
   vegetation classes from statistical descriptors of the aboveground
   heights of TLS points. Manual measurements of vegetation characteristics
   collected within each plot served as training and validation data.
   Models based on five or fewer TLS descriptors of vegetation heights were
   developed to predict the canopy cover fraction of shrubs (R-2 = 0.77,
   RMSE = 7\%), annual grasses (R-2 = 0.70, RMSE = 21\%), perennial grasses
   (R-2 = 0.36, RMSE = 12\%), forbs (R-2 = 0.52, RMSE = 6\%), bare earth or
   litter (R-2 = 0.49, RMSE = 19\%), and the biomass of shrubs (R-2 = 0.71,
   RMSE = 175 g) and herbaceous vegetation (R-2 = 0.61, RMSE = 99 g) (all
   values reported are out-of-bag). Our models explained much of the
   variability between predictions and manual measurements, and yet we
   expect that future applications could produce even better results by
   reducing some of the methodological sources of error that we
   encountered. Our work demonstrates how TLS can be used efficiently to
   extend manual measurement of vegetation characteristics from small to
   large plots in grasslands and shrublands, with potential application to
   other similarly structured ecosystems. Our method shows that vegetation
   structural characteristics can be modeled without classifying and
   delineating individual plants, a challenging and time-consuming step
   common in previous methods applying TLS to vegetation inventory.
   Improving application of TLS to studies of shrub steppe ecosystems will
   serve immediate management needs by enhancing vegetation inventories,
   environmental modeling studies, and the ability to train broader
   datasets collected from air and space.}},
DOI = {{10.1016/j.ecolind.2017.09.034}},
ISSN = {{1470-160X}},
EISSN = {{1872-7034}},
Unique-ID = {{ISI:000425828200076}},
}

@article{ ISI:000424092300038,
Author = {Zhou, Tan and Popescu, Sorin C. and Lawing, A. Michelle and Eriksson,
   Marian and Strimbu, Bogdan M. and Buerkner, Paul C.},
Title = {{Bayesian and Classical Machine Learning Methods: A Comparison for Tree
   Species Classification with LiDAR Waveform Signatures}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{A plethora of information contained in full-waveform (FW) Light
   Detection and Ranging (LiDAR) data offers prospects for characterizing
   vegetation structures. This study aims to investigate the capacity of FW
   LiDAR data alone for tree species identification through the integration
   of waveform metrics with machine learning methods and Bayesian
   inference. Specifically, we first conducted automatic tree segmentation
   based on the waveform-based canopy height model (CHM) using three
   approaches including TreeVaW, watershed algorithms and the combination
   of TreeVaW and watershed (TW) algorithms. Subsequently, the Random
   forests (RF) and Conditional inference forests (CF) models were employed
   to identify important tree-level waveform metrics derived from three
   distinct sources, such as raw waveforms, composite waveforms, the
   waveform-based point cloud and the combined variables from these three
   sources. Further, we discriminated tree (gray pine, blue oak, interior
   live oak) and shrub species through the RF, CF and Bayesian multinomial
   logistic regression (BMLR) using important waveform metrics identified
   in this study. Results of the tree segmentation demonstrated that the TW
   algorithms outperformed other algorithms for delineating individual tree
   crowns. The CF model overcomes waveform metrics selection bias caused by
   the RF model which favors correlated metrics and enhances the accuracy
   of subsequent classification. We also found that composite waveforms are
   more informative than raw waveforms and waveform-based point cloud for
   characterizing tree species in our study area. Both classical machine
   learning methods (the RF and CF) and the BMLR generated satisfactory
   average overall accuracy (74\% for the RF, 77\% for the CF and 81\% for
   the BMLR) and the BMLR slightly outperformed the other two methods.
   However, these three methods suffered from low individual classification
   accuracy for the blue oak which is prone to being misclassified as the
   interior live oak due to the similar characteristics of blue oak and
   interior live oak. Uncertainty estimates from the BMLR method compensate
   for this downside by providing classification results in a probabilistic
   sense and rendering users with more confidence in interpreting and
   applying classification results to real-world tasks such as forest
   inventory. Overall, this study recommends the CF method for feature
   selection and suggests that BMLR could be a superior alternative to
   classical machining learning methods.}},
DOI = {{10.3390/rs10010039}},
Article-Number = {{39}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Lawing, Michelle/F-7453-2019
   Popescu, Sorin C/D-5981-2015}},
ORCID-Numbers = {{Lawing, Michelle/0000-0003-4041-6177
   Popescu, Sorin C/0000-0002-8155-8801}},
Unique-ID = {{ISI:000424092300038}},
}

@article{ ISI:000423587100013,
Author = {Zhao, Jun-Li and Wu, Zhong-Ke and Pan, Zhen-Kuan and Duan, Fu-Qing and
   Li, Jin-Hua and Lv, Zhi-Han and Wang, Kang and Chen, Yu-Cong},
Title = {{3D Face Similarity Measure by Fr,chet Distances of Geodesics}},
Journal = {{JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY}},
Year = {{2018}},
Volume = {{33}},
Number = {{1}},
Pages = {{207-222}},
Month = {{JAN}},
Abstract = {{3D face similarity is a critical issue in computer vision, computer
   graphics and face recognition and so on. Since Fr,chet distance is an
   effective metric for measuring curve similarity, a novel 3D face
   similarity measure method based on Fr,chet distances of geodesics is
   proposed in this paper. In our method, the surface similarity between
   two 3D faces is measured by the similarity between two sets of 3D curves
   on them. Due to the intrinsic property of geodesics, we select geodesics
   as the comparison curves. Firstly, the geodesics on each 3D facial model
   emanating from the nose tip point are extracted in the same initial
   direction with equal angular increment. Secondly, the Fr,chet distances
   between the two sets of geodesics on the two compared facial models are
   computed. At last, the similarity between the two facial models is
   computed based on the Fr,chet distances of the geodesics obtained in the
   second step. We verify our method both theoretically and practically. In
   theory, we prove that the similarity of our method satisfies three
   properties: reflexivity, symmetry, and triangle inequality. And in
   practice, experiments are conducted on the open 3D face database GavaDB,
   Texas 3D Face Recognition database, and our 3D face database. After the
   comparison with iso-geodesic and Hausdorff distance method, the results
   illustrate that our method has good discrimination ability and can not
   only identify the facial models of the same person, but also distinguish
   the facial models of any two different persons.}},
DOI = {{10.1007/s11390-018-1814-7}},
ISSN = {{1000-9000}},
EISSN = {{1860-4749}},
Unique-ID = {{ISI:000423587100013}},
}

@article{ ISI:000422943700008,
Author = {Benedek, Csaba and Galai, Bence and Nagy, Balazs and Janko, Zsolt},
Title = {{Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance
   System}},
Journal = {{IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY}},
Year = {{2018}},
Volume = {{28}},
Number = {{1}},
Pages = {{101-113}},
Month = {{JAN}},
Abstract = {{This paper presents new approaches for gait and activity analysis based
   on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed
   algorithms are embedded into an integrated 4D vision and visualization
   system, which is able to analyze and interactively display real
   scenarios in natural outdoor environments with walking pedestrians. The
   main focus of the investigations is gait-based person reidentification
   during tracking and recognition of specific activity patterns, such as
   bending, waving, making phone calls, and checking the time looking at
   wristwatches. The descriptors for training and recognition are observed
   and extracted from realistic outdoor surveillance scenarios, where
   multiple pedestrians are walking in the field of interest following
   possibly intersecting trajectories; thus, the observations might often
   be affected by occlusions or background noise. Since there is no public
   database available for such scenarios, we created and published a new
   Lidar-based outdoor gait and activity data set on our website that
   contains point cloud sequences of 28 different persons extracted and
   aggregated from 35-min-long measurements. The presented results confirm
   that both efficient gait-based identification and activity recognition
   are achievable in the sparse point clouds of a single RMB Lidar sensor.
   After extracting the people trajectories, we synthesized a
   free-viewpoint video, in which moving avatar models follow the
   trajectories of the observed pedestrians in real time, ensuring that the
   leg movements of the animated avatars are synchronized with the real
   gait cycles observed in the Lidar stream.}},
DOI = {{10.1109/TCSVT.2016.2595331}},
ISSN = {{1051-8215}},
EISSN = {{1558-2205}},
ORCID-Numbers = {{Benedek, Csaba/0000-0003-3203-0741}},
Unique-ID = {{ISI:000422943700008}},
}

@article{ ISI:000418513500004,
Author = {Sghaier, Souhir and Farhat, Wajdi and Souani, Chokri},
Title = {{Novel Technique for 3D Face Recognition Using Anthropometric Methodology}},
Journal = {{INTERNATIONAL JOURNAL OF AMBIENT COMPUTING AND INTELLIGENCE}},
Year = {{2018}},
Volume = {{9}},
Number = {{1}},
Pages = {{60-77}},
Month = {{JAN-MAR}},
Abstract = {{This manuscript presents an improved system research that can detect and
   recognize the person in 3D space automatically and without the
   interaction of the people's faces. This system is based not only on a
   quantum computation and measurements to extract the vector features in
   the phase of characterization but also on learning algorithm (using SVM)
   to classify and recognize the person. This research presents an improved
   technique for automatic 3D face recognition using anthropometric
   proportions and measurement to detect and extract the area of interest
   which is unaffected by facial expression. This approach is able to treat
   incomplete and noisy images and reject the non-facial areas
   automatically. Moreover, it can deal with the presence of holes in the
   meshed and textured 3D image. It is also stable against small
   translation and rotation of the face. All the experimental tests have
   been done with two 3D face datasets FRAV 3D and GAVAB. Therefore, the
   test's results of the proposed approach are promising because they
   showed that it is competitive comparable to similar approaches in terms
   of accuracy, robustness, and flexibility. It achieves a high recognition
   performance rate of 95.35\% for faces with neutral and non-neutral
   expressions for the identification and 98.36\% for the authentification
   with GAVAB and 100\% with some gallery of FRAV 3D datasets.}},
DOI = {{10.4018/IJACI.2018010104}},
ISSN = {{1941-6237}},
EISSN = {{1941-6245}},
ResearcherID-Numbers = {{Farhat, Wajdi/N-5341-2015
   Souani, Chokri/B-1853-2015}},
ORCID-Numbers = {{Farhat, Wajdi/0000-0003-3647-8316
   Souani, Chokri/0000-0002-8987-3582}},
Unique-ID = {{ISI:000418513500004}},
}

@article{ ISI:000413384000029,
Author = {Barnes, Chloe and Balzter, Heiko and Barrett, Kirsten and Eddy, James
   and Milrier, Sam and Suarez, Juan C.},
Title = {{Airborne laser scanning and tree crown fragmentation metrics for the
   assessment of Phytophthora ramorum infected larch forest stands}},
Journal = {{FOREST ECOLOGY AND MANAGEMENT}},
Year = {{2017}},
Volume = {{404}},
Pages = {{294-305}},
Month = {{NOV 15}},
Abstract = {{The invasive phytopathogen Phytophthora ramorum has caused extensive
   infection of larch forest across areas of the UK, particularly in
   Southwest England, South Wales and Southwest Scotland. At present,
   landscape level assessment of the disease in these areas is conducted
   manually by tree health surveyors during helicopter surveys. Airborne
   laser scanning (ALS), also known as LiDAR, has previously been applied
   to the segmentation of larch tree crowns infected by P. ramorum
   infection and the detection of insect pests in coniferous tree species.
   This study evaluates metrics from high-density discrete ALS point clouds
   (24 points/m(2)) and canopy height models (CHMs) to identify individual
   trees infected with P. ramorum and to discriminate between four disease
   severity categories (NI: not infected, 1: light, 2: moderate, 3: heavy).
   The metrics derived from ALS point clouds include canopy cover,
   skewness, and bicentiles (B60, B70, B80 and B90) calculated using both a
   static (1 m) and a variable (50\% of tree height) cut-off height.
   Significant differences are found between all disease severity
   categories, except in the case of healthy individuals (NI) and those in
   the early stages of infection (category 1). In addition, fragmentation
   metrics are shown to identify the increased patchiness and infra-crown
   height irregularities of CHMs associated with individual trees subject
   to heavy infection (category 3) of P. ramorum. Classifications using a
   k-nearest neighbour (k-NN) classifier and ALS point cloud metrics to
   classify disease presence/absence and severity yielded overall
   accuracies of 72\% and 65\% respectively. The results indicate that ALS
   can be used to identify individual tree crowns subject to moderate and
   heavy P. ramorum infection in larch forests. This information
   demonstrates the potential applications of ALS for the development of a
   targeted phytosanitary approach for the management of P. ramorum.}},
DOI = {{10.1016/j.foreco.2017.08.052}},
ISSN = {{0378-1127}},
EISSN = {{1872-7042}},
ResearcherID-Numbers = {{Balzter, Heiko/B-5976-2008}},
ORCID-Numbers = {{Balzter, Heiko/0000-0002-9053-4684}},
Unique-ID = {{ISI:000413384000029}},
}

@article{ ISI:000416554100095,
Author = {Shen, Xin and Cao, Lin},
Title = {{Tree-Species Classification in Subtropical Forests Using Airborne
   Hyperspectral and LiDAR Data}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Accurate classification of tree-species is essential for sustainably
   managing forest resources and effectively monitoring species diversity.
   In this study, we used simultaneously acquired hyperspectral and LiDAR
   data from LiCHy (Hyperspectral, LiDAR and CCD) airborne system to
   classify tree-species in subtropical forests of southeast China. First,
   each individual tree crown was extracted using the LiDAR data by a point
   cloud segmentation algorithm (PCS) and the sunlit portion of each crown
   was selected using the hyperspectral data. Second, different suites of
   hyperspectral and LiDAR metrics were extracted and selected by the
   indices of Principal Component Analysis (PCA) and the mean decrease in
   Gini index (MDG) from Random Forest (RF). Finally, both hyperspectral
   metrics (based on whole crown and sunlit crown) and LiDAR metrics were
   assessed and used as inputs to Random Forest classifier to discriminate
   five tree-species at two levels of classification. The results showed
   that the tree delineation approach (point cloud segmentation algorithm)
   was suitable for detecting individual tree in this study (overall
   accuracy = 82.9\%). The classification approach provided a relatively
   high accuracy (overall accuracy > 85.4\%) for classifying five
   tree-species in the study site. The classification using both
   hyperspectral and LiDAR metrics resulted in higher accuracies than only
   hyperspectral metrics (the improvement of overall accuracies =
   0.4-5.6\%). In addition, compared with the classification using whole
   crown metrics (overall accuracies = 85.4-89.3\%), using sunlit crown
   metrics (overall accuracies = 87.1-91.5\%) improved the overall
   accuracies of 2.3\%. The results also suggested that fewer of the most
   important metrics can be used to classify tree-species effectively
   (overall accuracies = 85.8-91.0\%).}},
DOI = {{10.3390/rs9111180}},
Article-Number = {{1180}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000416554100095}},
}

@article{ ISI:000409180500015,
Author = {Sui, Dan and Hou, Deheng and Duan, Xinyu},
Title = {{An interpolation algorithm fitted for dynamic 3D face reconstruction}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2017}},
Volume = {{76}},
Number = {{19}},
Pages = {{19575-19589}},
Month = {{OCT}},
Abstract = {{In order to solve the problem of low recognition accuracy in later
   period which is caused by the too few extracted parameters in the 3D
   face recognition, and the incapable formation of completed point cloud
   structure. An automatic iterative interpolation algorithm is proposed.
   The new and more accurate 3D face data points are obtained by automatic
   iteration. This algorithm can be used to restore the data point cloud
   information of 3D facial feature in 2D images by means of facial
   three-legged structure formed by 3D face and automatic interpolation.
   Thus, it can realize to shape the 3D facial dynamic model which can be
   recognized and has high saturability. Experimental results show that the
   interpolation algorithm can achieve the complete the construction of
   facial feature based on the facial feature after 3D dynamic
   reconstruction, and the validity is higher.}},
DOI = {{10.1007/s11042-015-3233-x}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000409180500015}},
}

@article{ ISI:000411153300086,
Author = {Liu, Ting and Lv, Jun and Qin, Yutao},
Title = {{Standardized tumor volume: an independent prognostic factor in advanced
   nasopharyngeal carcinoma}},
Journal = {{ONCOTARGET}},
Year = {{2017}},
Volume = {{8}},
Number = {{41}},
Pages = {{70299-70309}},
Month = {{SEP 19}},
Abstract = {{The study evaluated the prognostic effect of standardized tumor volume
   in patients with advanced nasopharyngeal carcinoma (NPC) treated with
   concurrent chemoradiotherapy. Between Jan 1, 2009 and December 30, 2012,
   143 patients diagnosed with NPC in UICC stage III-IVb by histopathology
   were enrolled in the study. These patients underwent intensity-modulated
   radiotherapy combined with concurrent chemotherapy. The
   three-dimensional images of tumor volume were reconstructed
   automatically by the treatment planning system. SGTVnx was calculated
   based on GTVnx/person's volume. SGTVnd was calculated based on
   GTVnd/person's volume. SGTVnx was significantly associated with the
   5-year overall survival (OS), disease-free survival (DFS), DMFS, and
   LRFS rates in univariate and multivariate analyses. Although SGTVnd was
   associated with the 5-year OS, DFS, and DMFS rates, it was not an
   independent prognostic factor for LRFS. In receiver operating
   characteristic (ROC) curve analysis, 1.091 and 0.273 were determined as
   the cut-off points for SGTVnx and SGTVnd, respectively. The 5-year OS,
   DFS, DMFS, and LRFS rates for patients with a SGTVnx > 1.091 vs. SGTVnx
   <= 1.091 was 65.4\% vs. 93.4\% (P < 0.001), 65.2\% vs. 94.8\% (P <
   0.001), 71.4\% vs. 97.4\% (P < 0.001), and 84.8\% vs. 97.3\% (P =
   0.003), respectively, for SGTVnd > 0.273 vs. SGTVnd <= 0.273 was 70.3\%
   vs. 96.5\% (P < 0.001), 70.1\% vs. 94.8\% (P < 0.001), 77.5\% vs. 98.2\%
   (P < 0.001), and 88.5\% vs. 96.6\% (P = 0.049), respectively. UICC stage
   grouping, T classification, N classification, and sex were not found to
   be independent prognostic factors for NPC. Standardized tumor volume was
   an independent prognostic factor for NPC that might improve the current
   NPC TNM classification system and provide new clinical evidence for
   personalized treatment strategies.}},
DOI = {{10.18632/oncotarget.20313}},
ISSN = {{1949-2553}},
Unique-ID = {{ISI:000411153300086}},
}

@article{ ISI:000410059200037,
Author = {Quint, S. and Christ, A. F. and Guckenberger, A. and Himbert, S. and
   Kaestner, L. and Gekle, S. and Wagner, C.},
Title = {{3D tomography of cells in micro-channels}},
Journal = {{APPLIED PHYSICS LETTERS}},
Year = {{2017}},
Volume = {{111}},
Number = {{10}},
Month = {{SEP 4}},
Abstract = {{We combine confocal imaging, microfluidics, and image analysis to record
   3D-images of cells in flow. This enables us to recover the full 3D
   representation of several hundred living cells per minute. Whereas 3D
   confocal imaging has thus far been limited to steady specimens, we
   overcome this restriction and present a method to access the 3D shape of
   moving objects. The key of our principle is a tilted arrangement of the
   micro-channel with respect to the focal plane of the microscope. This
   forces cells to traverse the focal plane in an inclined manner. As a
   consequence, individual layers of passing cells are recorded, which can
   then be assembled to obtain the volumetric representation. The full 3D
   information allows for a detailed comparison with theoretical and
   numerical predictions unfeasible with, e.g., 2D imaging. Our technique
   is exemplified by studying flowing red blood cells in a micro-channel
   reflecting the conditions prevailing in the microvasculature. We observe
   two very different types of shapes: ``croissants{''} and ``slippers.{''}
   Additionally, we perform 3D numerical simulations of our experiment to
   confirm the observations. Since 3D confocal imaging of cells in flow has
   not yet been realized, we see high potential in the field of flow
   cytometry where cell classification thus far mostly relies on 1D
   scattering and fluorescence signals. Published by AIP Publishing.}},
DOI = {{10.1063/1.4986392}},
Article-Number = {{103701}},
ISSN = {{0003-6951}},
EISSN = {{1077-3118}},
ResearcherID-Numbers = {{Wagner, Christian/A-1307-2009
   Gekle, Stephan/I-9695-2014}},
ORCID-Numbers = {{Wagner, Christian/0000-0001-7788-4594
   Gekle, Stephan/0000-0001-5597-1160}},
Unique-ID = {{ISI:000410059200037}},
}

@article{ ISI:000412378800003,
Author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Benouareth, Abdallah
   and Declercq, David},
Title = {{3D facial expression recognition using kernel methods on Riemannian
   manifold}},
Journal = {{ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE}},
Year = {{2017}},
Volume = {{64}},
Pages = {{25-32}},
Month = {{SEP}},
Abstract = {{Automatic human Facial Expressions Recognition (FER) is becoming of
   increased interest. FER finds its applications in many emerging areas
   such as affective computing and intelligent human computer interaction.
   Most of the existing work on FER has been done using 2D data which
   suffers from inherent problems of illumination changes and pose
   variations. With the development of 3D image capturing technologies, the
   acquisition of 3D data is becoming a more feasible task. The 3D data
   brings a more effective solution in addressing the issues raised by its
   2D counterpart. State-of-the-art 3D FER methods are often based on a
   single descriptor which may fail to handle the large inter-class and
   intra-class variability of the human facial expressions. In this work,
   we explore, for the first time, the usage of covariance matrices of
   descriptors, instead of the descriptors themselves, in 3D FER. Since
   covariance matrices are elements of the non-linear manifold of Symmetric
   Positive Definite (SPD) matrices, we particularly look at the
   application of manifold-based classification to the problem of 3D FER.
   We evaluate the performance of the proposed framework on the BU-3DFE and
   the Bosphorus datasets, and demonstrate its superiority compared to the
   state-of-the-art methods. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.engappai.2017.05.009}},
ISSN = {{0952-1976}},
EISSN = {{1873-6769}},
Unique-ID = {{ISI:000412378800003}},
}

@article{ ISI:000411548000002,
Author = {Jimenez-Pique, E. and Turon-Vinas, M. and Chen, H. and Trifonov, T. and
   Fair, J. and Tarres, E. and Llanes, L.},
Title = {{Focused ion beam tomography of WC-Co cemented carbides}},
Journal = {{INTERNATIONAL JOURNAL OF REFRACTORY METALS \& HARD MATERIALS}},
Year = {{2017}},
Volume = {{67}},
Pages = {{9-17}},
Month = {{SEP}},
Abstract = {{The microstructure of three different grades of WC-Co cemented carbides
   (hardmetals) has been reconstructed in three dimensions after sequential
   images obtained by focused ion beam. The three dimensional
   microstructual parameters are compared against the well-known two
   dimensional parameters of grain size, phase percentages and mean free
   path. Results show good agreement with the exception of individual grain
   recognition, which could not be univocally segmented. In the case of
   mean free path, the three-dimensional image depicts a more realistic
   description of the metal interconnections in the composite. Aiming for a
   simple example of direct application of these FIB tomography outcomes,
   reconstructed real microstructure for the coarser hardmetal grade
   studied was translated in a finite element modelling mesh, and elastic
   residual stresses were estimated from sintering to room temperature.
   Calculated thermal stresses agree with experimental results and show
   significant local variations in their value due to the complex
   microstructure of cemented carbides.}},
DOI = {{10.1016/j.ijrmhm.2017.04.007}},
ISSN = {{0263-4368}},
ResearcherID-Numbers = {{LLANES, LUIS/H-9761-2015
   }},
ORCID-Numbers = {{LLANES, LUIS/0000-0003-1054-1073
   Jimenez-Pique, Emilio/0000-0002-6950-611X}},
Unique-ID = {{ISI:000411548000002}},
}

@article{ ISI:000408398200010,
Author = {Eng, Z. H. D. and Yick, Y. Y. and Guo, Y. and Xu, H. and Reiner, M. and
   Cham, T. J. and Chen, S. H. A.},
Title = {{3D faces are recognized more accurately and faster than 2D faces, but
   with similar inversion effects}},
Journal = {{VISION RESEARCH}},
Year = {{2017}},
Volume = {{138}},
Pages = {{78-85}},
Month = {{SEP}},
Abstract = {{Recognition of faces typically occurs via holistic processing where
   individual features are combined to provide an overall facial
   representation. However, when faces are inverted, there is greater
   reliance on featural processing where faces are recognized based on
   their individual features. These findings are based on a substantial
   number of studies using 2-dimensional (2D) faces and it is unknown
   whether these results can be extended to 3-dimensional (3D) faces, which
   have more depth information that is absent in the typical 2D stimuli
   used in face recognition literature. The current study used the face
   inversion paradigm as a means to investigate how holistic and featural
   processing are differentially influenced by 2D and 3D faces. Twenty-five
   participants completed a delayed face-matching task consisting of
   upright and inverted faces that were presented as both 2D and 3D
   stereoscopic images. Recognition accuracy was significantly higher for
   3D upright faces compared to 2D upright faces, providing support that
   the enriched visual information in 3D stereoscopic images facilitates
   holistic processing that is essential for the recognition of upright
   faces. Typical face inversion effects were also obtained, regardless of
   whether the faces were presented in 2D or 3D. Moreover, recognition
   performances for 2D inverted and 3D inverted faces did not differ. Taken
   together, these results demonstrated that 3D stereoscopic effects
   influence face recognition during holistic processing but not during
   featural processing. Our findings therefore provide a novel perspective
   that furthers our understanding of face recognition mechanisms, shedding
   light on how the integration of stereoscopic information in 3D faces
   influences face recognition processes. (c) 2017 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.visres.2017.06.004}},
ISSN = {{0042-6989}},
EISSN = {{1878-5646}},
ResearcherID-Numbers = {{Chen, SH Annabel/F-3742-2011
   }},
ORCID-Numbers = {{Chen, SH Annabel/0000-0002-1540-5516
   Xu, Hong/0000-0003-1389-5408}},
Unique-ID = {{ISI:000408398200010}},
}

@article{ ISI:000407961700001,
Author = {Damon, James and Gasparovic, Ellen},
Title = {{Modeling Multi-object Configurations via Medial/Skeletal Linking
   Structures}},
Journal = {{INTERNATIONAL JOURNAL OF COMPUTER VISION}},
Year = {{2017}},
Volume = {{124}},
Number = {{3}},
Pages = {{255-272}},
Month = {{SEP}},
Abstract = {{We introduce a method for modeling a configuration of objects in 2D or
   3D images using a mathematical ``skeletal linking structure{''} which
   will simultaneously capture the individual shape features of the objects
   and their positional information relative to one another. The objects
   may either have smooth boundaries and be disjoint from the others or
   share common portions of their boundaries with other objects in a
   piecewise smooth manner. These structures include a special class of
   ``Blum medial linking structures{''}, which are intrinsically associated
   to the configuration and build upon the Blum medial axes of the
   individual objects. We give a classification of the properties of Blum
   linking structures for generic configurations. The skeletal linking
   structures add increased flexibility for modeling configurations of
   objects by relaxing the Blum conditions and they extend in a minimal way
   the individual ``skeletal structures{''} which have been previously used
   for modeling individual objects and capturing their geometric
   properties. This allows for the mathematical methods introduced for
   single objects to be significantly extended to the entire configuration
   of objects. These methods not only capture the internal shape structures
   of the individual objects but also the external structure of the
   neighboring regions of the objects. In the subsequent second paper
   (Damon and Gasparovic in Shape and positional geometry of multi-object
   configurations) we use these structures to identify specific external
   regions which capture positional information about neighboring objects,
   and we develop numerical measures for closeness of portions of objects
   and their significance for the configuration. This allows us to use the
   same mathematical structures to simultaneously analyze both the shape
   properties of the individual objects and positional properties of the
   configuration. This provides a framework for analyzing the statistical
   properties of collections of similar configurations such as for
   applications to medical imaging.}},
DOI = {{10.1007/s11263-017-1019-5}},
ISSN = {{0920-5691}},
EISSN = {{1573-1405}},
Unique-ID = {{ISI:000407961700001}},
}

@article{ ISI:000403135200018,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal and Eastwood, Peter},
Title = {{Deep, dense and accurate 3D face correspondence for generating
   population specific deformable models}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2017}},
Volume = {{69}},
Pages = {{238-250}},
Month = {{SEP}},
Abstract = {{We present a multilinear algorithm to automatically establish dense
   point-to-point correspondence over an arbitrarily large number of
   population specific 3D faces across identities, facial expressions and
   poses. The algorithm is initialized with a subset of anthropometric
   landmarks detected by our proposed Deep Landmark Identification Network
   which is trained on synthetic images. The landmarks are used to segment
   the 3D face into Voronoi regions by evolving geodesic level set curves.
   Exploiting the intrinsic features of these regions, we extract
   discriminative keypoints on the facial manifold to elastically match the
   regions across faces for establishing dense correspondence. Finally, we
   generate a Region based 3D Deformable Model which is fitted to unseen
   faces to transfer the correspondences. We evaluate our algorithm on the
   tasks of facial landmark detection and recognition using two benchmark
   datasets. Comparison with thirteen state-of-the-art techniques shows the
   efficacy of our algorithm. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2017.04.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Eastwood, Peter/0000-0002-4490-4138
   Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000403135200018}},
}

@article{ ISI:000410870200001,
Author = {Jo, Jaeik and Kim, Hyunjun and Kim, Jaihie},
Title = {{3D facial shape reconstruction using macro- and micro-level features
   from high resolution facial images}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{64}},
Pages = {{1-9}},
Month = {{AUG}},
Abstract = {{Three-dimensional (3D) facial modeling and stereo matching-based methods
   are widely used for 3D facial reconstruction from 2D single-view and
   multiple-view images. However, these methods cannot realistically
   reconstruct 3D faces because they use insufficient numbers of
   macro-level Facial Feature Points (FFPs). This paper proposes an
   accurate and person-specific 3D facial reconstruction method that uses
   ample numbers of macro and micro-level FFPs to enable coverage of all
   facial regions of high resolution facial images. Comparisons of 3D
   facial images reconstructed using the proposed method for ground-truth
   3D facial images from the Bosphorus 3D database show that the method is
   superior to a conventional Active Appearance Model-Structure from Motion
   (AAM + SfM)-based method in terms of average 3D root mean square error
   between the reconstructed and ground-truth 3D faces. Further, the
   proposed method achieved outstanding accuracy in local facial regions
   such as the cheek areas where extraction of FFPs is difficult for
   existing methods. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/jimavis.2017.05.001}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000410870200001}},
}

@article{ ISI:000410870200008,
Author = {Xia, Baiqiang and Ben Amor, Boulbaba and Daoudi, Mohamed},
Title = {{= Joint gender, ethnicity and age estimation from 3D faces An
   experimental illustration of their correlations}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{64}},
Pages = {{90-102}},
Month = {{AUG}},
Abstract = {{Humans present clear demographic traits which allow their peers to
   recognize their gender and ethnic groups as well as estimate their age.
   Abundant literature has investigated the problem of automated gender,
   ethnicity and age recognition from facial images. However, despite the
   co-existence of these traits, most of the studies have addressed them
   separately, very little attention has been given to their correlations.
   In this work, we address the problem of joint demographic estimation and
   investigate the correlation through the morphological differences in 3D
   facial shapes. To this end, a set of facial features are extracted to
   capture the 3D shape differences among the demographic groups. Then, a
   correlation-based feature selection is applied to highlight salient
   features and remove redundancy. These features are later fed to Random
   Forest for gender and ethnicity classification, and age estimation.
   Extensive experiments conducted on FRGCv2 dataset, under
   Expression-Dependent and Expression-Independent settings, demonstrate
   the effectiveness of the proposed approaches for the three traits, and
   also show the accuracy improvement when considering their correlations.
   To the best of our knowledge, this is the first study exploring the
   correlations of these facial soft-biometric traits using 3D faces. This
   is also the first work which studies the problem of age estimation from
   3D Faces.(1) (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2017.06.004}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305}},
Unique-ID = {{ISI:000410870200008}},
}

@article{ ISI:000403860600008,
Author = {Zhu, Qing and Li, Yuan and Hu, Han and Wu, Bo},
Title = {{Robust point cloud classification based on multi-level semantic
   relationships for urban scenes}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{129}},
Pages = {{86-102}},
Month = {{JUL}},
Abstract = {{The semantic classification of point clouds is a fundamental part of
   three-dimensional urban reconstruction. For datasets with high spatial
   resolution but significantly more noises, a general trend is to exploit
   more contexture information to surmount the decrease of discrimination
   of features for classification. However, previous works on adoption of
   contexture information are either too restrictive or only in a small
   region and in this paper, we propose a point cloud classification method
   based on multi-level semantic relationships, including
   point-homogeneity, supervoxel-adjacency and class-knowledge constraints,
   which is more versatile and incrementally propagate the classification
   cues from individual points to the object level and formulate them as a
   graphical model. The point-homogeneity constraint clusters points with
   similar geometric and radiometric properties into regular-shaped
   supervoxels that correspond to the vertices in the graphical model. The
   supervoxel-adjacency constraint contributes to the pairwise interactions
   by providing explicit adjacent relationships between supervoxels. The
   class knowledge constraint operates at the object level based on
   semantic rules, guaranteeing the classification correctness of
   supervoxel clusters at that level. International Society of
   Photogrammetry and Remote Sensing (ISPRS) benchmark tests have shown
   that the proposed method achieves state-of-the-art performance with an
   average per-area completeness and correctness of 93.88\% and 95.78\%,
   respectively. The evaluation of classification of photogrammetric point
   clouds and DSM generated from aerial imagery confirms the method's
   reliability in several challenging urban scenes. (C) 2017 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2017.04.022}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Wu, Bo/J-6177-2012
   Hu, Han/V-5068-2018}},
ORCID-Numbers = {{Wu, Bo/0000-0001-9530-3044
   Hu, Han/0000-0003-1137-2208}},
Unique-ID = {{ISI:000403860600008}},
}

@article{ ISI:000403031400006,
Author = {Milenkovic, Milutin and Wagner, Wolfgang and Quast, Raphael and Hollaus,
   Markus and Ressl, Camillo and Pfeifer, Norbert},
Title = {{Total canopy transmittance estimated from small-footprint, full-waveform
   airborne LiDAR}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{128}},
Pages = {{61-72}},
Month = {{JUN}},
Abstract = {{Canopy transmittance is a directional and wavelength-specific physical
   parameter that quantifies the amount of radiation attenuated when
   passing through a vegetation layer. The parameter has been estimated
   from LiDAR data in many different ways over the years. While early LiDAR
   methods treated each returned echo equally or weighted the echoes
   according to their return order, recent methods have focused more on the
   echo energy. In this study, we suggest a new method of estimating the
   total canopy transmittance considering only the energy of ground echoes.
   Therefore, this method does not require assumptions for the reflectance
   or absorption behavior of vegetation. As the oblique looking geometry of
   LiDAR is explicitly considered, canopy transmittance can be derived for
   individual laser beams and can be mapped spatially. The method was
   applied on a contemporary full-waveform LiDAR data set collected under
   leaf-off conditions and over a study site that contains two sub regions:
   one with a mixed (coniferous and deciduous) forest and another that is
   predominantly a deciduous forest in an alluvial plain. The resulting
   canopy transmittance map was analyzed for both sub regions and compared
   to aerial photos and the well-known fractional cover method. A visual
   comparison with aerial photos showed that even single trees and small
   canopy openings are visible in the canopy transmittance map. In
   comparison with the fractional cover method, the canopy transmittance
   map showed no saturation, i.e., there was better separability between
   patches with different vegetation structure. (C) 2017 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published
   by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2017.03.008}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Quast, Raphael/0000-0003-0419-4546
   Milenkovic, Milutin/0000-0003-3256-6669
   Pfeifer, Norbert/0000-0002-2348-7929}},
Unique-ID = {{ISI:000403031400006}},
}

@article{ ISI:000401888600006,
Author = {Coomes, David A. and Dalponte, Michele and Jucker, Tommaso and Asner,
   Gregory P. and Banin, Lindsay F. and Burslem, David F. R. P. and Lewis,
   Simon L. and Nilus, Reuben and Phillips, Oliver L. and Phua, Mui-How and
   Qie, Lan},
Title = {{Area-based vs tree-centric approaches to mapping forest carbon in
   Southeast Asian forests from airborne laser scanning data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2017}},
Volume = {{194}},
Pages = {{77-88}},
Month = {{JUN 1}},
Abstract = {{Tropical forests are a key component of the global carbon cycle, and
   mapping their carbon density is essential for understanding human
   influences on climate and for ecosystem-service-based payments for
   forest protection. Discrete-return airborne laser scanning (ALS) is
   increasingly recognised as a high-quality technology for mapping
   tropical forest carbon, because it generates 3D point clouds of forest
   structure from which aboveground carbon density (ACD) can be estimated.
   Area-based models are state of the art when it comes to estimating ACD
   from ALS data, but discard tree-level information contained within the
   ALS point cloud. This paper compares area based and tree-centric models
   for estimating ACD in lowland old-growth forests in Sabah, Malaysia.
   These forests are challenging to map because of their immense height. We
   compare the performance of (a) an area-based model developed by Asner
   and Mascaro (2014), and used primarily in the neotropics hitherto, with
   (b) a tree-centric approach that uses a new algorithm (itcSegment) to
   locate trees within the ALS canopy height model, measures their heights
   and crown widths, and calculates biomass from these dimensions. We find
   that Asner and Mascaro's model needed regional calibration, reflecting
   the distinctive structure of Southeast Asian forests. We also discover
   that forest basal area is closely related to canopy gap fraction
   measured by ALS, and use this finding to refine Asner and Mascaro's
   model. Finally, we show that our tree-centric approach is less accurate
   at estimating ACD than the best-performing area-based model (RMSE 18\%
   vs 13\%). Tree-centric modelling is appealing because it is based on
   summing the biomass of individual trees, but until algorithms can detect
   understory trees reliably and estimate biomass from crown dimensions
   precisely, areas-based modelling will remain the method of choice. (C)
   2017 The Authors. Published by Elsevier Inc.}},
DOI = {{10.1016/j.rse.2017.03.017}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Burslem, David FRP/F-1204-2019
   Jucker, Tommaso/S-4724-2017
   Phillips, Oliver L/A-1523-2011
   Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Burslem, David FRP/0000-0001-6033-0990
   Jucker, Tommaso/0000-0002-0751-6312
   Phillips, Oliver L/0000-0002-8993-6168
   Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000401888600006}},
}

@article{ ISI:000402350200003,
Author = {Eskandari, A. H. and Arjmand, N. and Shirazi-Adl, A. and Farahmand, F.},
Title = {{Subject-specific 2D/3D image registration and kinematics-driven
   musculoskeletal model of the spine}},
Journal = {{JOURNAL OF BIOMECHANICS}},
Year = {{2017}},
Volume = {{57}},
Pages = {{18-26}},
Month = {{MAY}},
Abstract = {{An essential input to the musculoskeletal (MS) trunk models that
   estimate muscle and spine forces is kinematics of the thorax, pelvis,
   and lumbar vertebrae. While thorax and pelvis kinematics are usually
   measured via skin motion capture devices (with inherent errors on the
   proper identification of the underlying bony landmarks and the relative
   skin-sensor-bone movements), those of the intervening lumbar vertebrae
   are commonly approximated at fixed proportions based on the
   thorax-pelvis kinematics. This study proposes an image-based kinematics
   measurement approach to drive subject-specific (musculature, geometry,
   mass, and center of masses) MS models. Kinematics of the thorax, pelvis,
   and individual lumbar vertebrae as well as disc inclinations, gravity
   loading, and musculature were all measured via different imaging
   techniques. The model estimated muscle and lumbar forces in various
   upright and flexed postures in which kinematics were obtained using
   upright fluoroscopy via 2D/3D image registration. Predictions of this
   novel image-kinematics-driven model (Img-KD) were compared with those of
   the traditional kinematics-driven (T-KD) model in which individual
   lumbar vertebral rotations were assumed based on thorax-pelvis
   orientations. Results indicated that while differences between Img-KD
   and T-KD models remained small for the force in the global muscles
   (attached to the thoracic cage) (<15\%), L4-S1 compression (<15\%), and
   shear (<20\%) forces in average for all the simulated tasks, they were
   relatively larger for the force in the local muscles (attached to the
   lumbar vertebrae). Assuming that the skin-based measurements of thorax
   and pelvis kinematics are accurate enough, the T-KD model predictions of
   spinal forces remain reliable. (C) 2017 Elsevier Ltd. All rights
   reserved,}},
DOI = {{10.1016/j.jbiomech.2017.03.011}},
ISSN = {{0021-9290}},
EISSN = {{1873-2380}},
ResearcherID-Numbers = {{Farahmand, Farzam/B-3921-2011}},
ORCID-Numbers = {{Farahmand, Farzam/0000-0001-8900-7003}},
Unique-ID = {{ISI:000402350200003}},
}

@article{ ISI:000390884300006,
Author = {Jones, Scott P. and Dwyer, Dominic M. and Lewis, Michael B.},
Title = {{The utility of multiple synthesized views in the recognition of
   unfamiliar faces}},
Journal = {{QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY}},
Year = {{2017}},
Volume = {{70}},
Number = {{5}},
Pages = {{906-918}},
Month = {{MAY}},
Abstract = {{The ability to recognize an unfamiliar individual on the basis of prior
   exposure to a photograph is notoriously poor and prone to errors, but
   recognition accuracy is improved when multiple photographs are
   available. In applied situations, when only limited real images are
   available (e.g., from a mugshot or CCTV image), the generation of new
   images might provide a technological prosthesis for otherwise fallible
   human recognition. We report two experiments examining the effects of
   providing computer-generated additional views of a target face. In
   Experiment 1, provision of computer-generated views supported better
   target face recognition than exposure to the target image alone and
   equivalent performance to that for exposure of multiple photograph
   views. Experiment 2 replicated the advantage of providing generated
   views, but also indicated an advantage for multiple viewings of the
   single target photograph. These results strengthen the claim that
   identifying a target face can be improved by providing multiple
   synthesized views based on a single target image. In addition, our
   results suggest that the degree of advantage provided by synthesized
   views may be affected by the quality of synthesized material.}},
DOI = {{10.1080/17470218.2016.1158302}},
ISSN = {{1747-0218}},
EISSN = {{1747-0226}},
ResearcherID-Numbers = {{Dwyer, Dominic Michael/D-1498-2009
   }},
ORCID-Numbers = {{Dwyer, Dominic Michael/0000-0001-8069-5508
   Lewis, Michael/0000-0002-5735-5318
   Jones, Scott/0000-0001-5516-4385}},
Unique-ID = {{ISI:000390884300006}},
}

@article{ ISI:000425868600001,
Author = {Wang, Lamei and Chen, Wenfeng and Li, Hong},
Title = {{Use of 3D faces facilitates facial expression recognition in children}},
Journal = {{SCIENTIFIC REPORTS}},
Year = {{2017}},
Volume = {{7}},
Month = {{APR 3}},
Abstract = {{This study assessed whether presenting 3D face stimuli could facilitate
   children's facial expression recognition. Seventy-one children aged
   between 3 and 6 participated in the study. Their task was to judge
   whether a face presented in each trial showed a happy or fearful
   expression. Half of the face stimuli were shown with 3D representations,
   whereas the other half of the images were shown as 2D pictures. We
   compared expression recognition under these conditions. The results
   showed that the use of 3D faces improved the speed of facial expression
   recognition in both boys and girls. Moreover, 3D faces improved boys'
   recognition accuracy for fearful expressions. Since fear is the most
   difficult facial expression for children to recognize, the facilitation
   effect of 3D faces has important practical implications for children
   with difficulties in facial expression recognition. The potential
   benefits of 3D representation for other expressions also have
   implications for developing more realistic assessments of children's
   expression recognition.}},
DOI = {{10.1038/srep45464}},
Article-Number = {{45464}},
ISSN = {{2045-2322}},
ResearcherID-Numbers = {{Chen, Wenfeng/H-2424-2012
   }},
ORCID-Numbers = {{Chen, Wenfeng/0000-0002-4271-8366
   wang, la mei/0000-0002-9203-5539}},
Unique-ID = {{ISI:000425868600001}},
}

@article{ ISI:000401097300027,
Author = {Santoro, Valeria and Lubelli, Sergio and De Donno, Antonio and
   Inchingolo, Alessio and Lavecchia, Fulvio and Introna, Francesco},
Title = {{Photogrammetric 3D skull/photo superimposition: A pilot study}},
Journal = {{FORENSIC SCIENCE INTERNATIONAL}},
Year = {{2017}},
Volume = {{273}},
Pages = {{168-174}},
Month = {{APR}},
Abstract = {{The identification of bodies through the examination of skeletal remains
   holds a prominent place in the field of forensic investigations.
   Technological advancements in 3D facial acquisition techniques have led
   to the proposal of a new body identification technique that involves a
   combination of craniofacial superimposition and photogrammetry. The aim
   of this study was to test the method by superimposing various
   computerized 3D images of skulls onto various photographs of missing
   people taken while they were still alive in cases when there was a
   suspicion that the skulls in question belonged to them. The technique is
   divided into four phases: preparatory phase, 3d acquisition phase,
   superimposition phase, and metric image analysis 3d.
   The actual superimposition of the images was carried out in the fourth
   step. and was done so by comparing the skull images with the selected
   photos.
   Using a specific software, the two images (i.e. the 3D avatar and the
   photo of the missing person) were superimposed. Cross-comparisons of 5
   skulls discovered in a mass grave, and of 2 skulls retrieved in the
   crawlspace of a house were performed. The morphologyc phase reveals a
   full overlap between skulls and photos of disappeared persons. Metric
   phase reveals that correlation coefficients of this values, higher than
   0.998-0,997 allow to confirm identification hypothesis. (C) 2017
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.forsciint.2017.02.006}},
ISSN = {{0379-0738}},
EISSN = {{1872-6283}},
Unique-ID = {{ISI:000401097300027}},
}

@article{ ISI:000398720100091,
Author = {Weinmann, Martin and Weinmann, Michael and Mallet, Clement and Bredif,
   Mathieu},
Title = {{A Classification-Segmentation Framework for the Detection of Individual
   Trees in Dense MMS Point Cloud Data Acquired in Urban Areas}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{In this paper, we present a novel framework for detecting individual
   trees in densely sampled 3D point cloud data acquired in urban areas.
   Given a 3D point cloud, the objective is to assign point-wise labels
   that are both class-aware and instance-aware, a task that is known as
   instance-level segmentation. To achieve this, our framework addresses
   two successive steps. The first step of our framework is given by the
   use of geometric features for a binary point-wise semantic
   classification with the objective of assigning semantic class labels to
   irregularly distributed 3D points, whereby the labels are defined as
   ``tree points{''} and ``other points{''}. The second step of our
   framework is given by a semantic segmentation with the objective of
   separating individual trees within the ``tree points{''}. This is
   achieved by applying an efficient adaptation of the mean shift algorithm
   and a subsequent segment-based shape analysis relying on semantic rules
   to only retain plausible tree segments. We demonstrate the performance
   of our framework on a publicly available benchmark dataset, which has
   been acquired with a mobile mapping system in the city of Delft in the
   Netherlands. This dataset contains 10.13 M labeled 3D points among which
   17.6\% are labeled as ``tree points{''}. The derived results clearly
   reveal a semantic classification of high accuracy (up to 90.77\%) and an
   instance-level segmentation of high plausibility, while the simplicity,
   applicability and efficiency of the involved methods even allow applying
   the complete framework on a standard laptop computer with a reasonable
   processing time (less than 2.5 h).}},
DOI = {{10.3390/rs9030277}},
Article-Number = {{277}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Bredif, Mathieu/T-3029-2018
   }},
ORCID-Numbers = {{Bredif, Mathieu/0000-0003-0228-1232
   Mallet, Clement/0000-0002-2675-165X
   Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000398720100091}},
}

@article{ ISI:000399162400002,
Author = {Lindberg, Eva and Holmgren, Johan},
Title = {{Individual Tree Crown Methods for 3D Data from Remote Sensing}},
Journal = {{CURRENT FORESTRY REPORTS}},
Year = {{2017}},
Volume = {{3}},
Number = {{1}},
Pages = {{19-31}},
Month = {{MAR}},
Abstract = {{Purpose of Review The rapid development of remote sensing technology
   hasmade dense 3D data available from airborne laser scanning and
   recently also photogrammetric point clouds. This paper reviews methods
   for extraction of individual trees from 3D data and their applications
   in forestry and ecology.
   Recent Findings Methods for analysis of 3D data at tree level have been
   developed since the turn of the century. The first algorithms were based
   on 2D surface models of the upper contours of tree crowns. These methods
   are robust and provide information about the trees in the top-most
   canopy. There are also methods that use the complete 3D data. However,
   development of these 3D methods is still needed to include use of
   geometric properties. To detect a large fraction of the tallest trees, a
   surface model method generally gives the best results, but detection of
   smaller trees below the top-most canopy requires methods utilizing the
   whole point cloud. Several new sensors are now available with capability
   to describe the upper part of the canopy, which can be used to
   frequently update vegetation maps. Highly sensitive laser photo
   detectors have become available for civilian applications, which will
   enable acquisition of high-resolution 3D laser data for large areas to
   much lower costs.
   Summary Methods for ITC delineation from 3D data provide information
   about a large fraction of the trees, but there is still a challenge to
   make optimal use of the information from whole point cloud. Newly
   developed sensors might make ITC methods cheaper and feasible for large
   areas.}},
DOI = {{10.1007/s40725-017-0051-6}},
ISSN = {{2198-6436}},
Unique-ID = {{ISI:000399162400002}},
}

@article{ ISI:000398720100002,
Author = {Nevalainen, Olli and Honkavaara, Eija and Tuominen, Sakari and Viljanen,
   Niko and Hakala, Teemu and Yu, Xiaowei and Hyyppa, Juha and Saari,
   Heikki and Polonen, Ilkka and Imai, Nilton N. and Tommaselli, Antonio M.
   G.},
Title = {{Individual Tree Detection and Classification with UAV-Based
   Photogrammetric Point Clouds and Hyperspectral Imaging}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{Small unmanned aerial vehicle (UAV) based remote sensing is a rapidly
   evolving technology. Novel sensors and methods are entering the market,
   offering completely new possibilities to carry out remote sensing tasks.
   Three-dimensional (3D) hyperspectral remote sensing is a novel and
   powerful technology that has recently become available to small UAVs.
   This study investigated the performance of UAV-based photogrammetry and
   hyperspectral imaging in individual tree detection and tree species
   classification in boreal forests. Eleven test sites with 4151 reference
   trees representing various tree species and developmental stages were
   collected in June 2014 using a UAV remote sensing system equipped with a
   frame format hyperspectral camera and an RGB camera in highly variable
   weather conditions. Dense point clouds were measured photogrammetrically
   by automatic image matching using high resolution RGB images with a 5 cm
   point interval. Spectral features were obtained from the hyperspectral
   image blocks, the large radiometric variation of which was compensated
   for by using a novel approach based on radiometric block adjustment with
   the support of in-flight irradiance observations. Spectral and 3D point
   cloud features were used in the classification experiment with various
   classifiers. The best results were obtained with Random Forest and
   Multilayer Perceptron (MLP) which both gave 95\% overall accuracies and
   an F-score of 0.93. Accuracy of individual tree identification from the
   photogrammetric point clouds varied between 40\% and 95\%, depending on
   the characteristics of the area. Challenges in reference measurements
   might also have reduced these numbers. Results were promising,
   indicating that hyperspectral 3D remote sensing was operational from a
   UAV platform even in very difficult conditions. These novel methods are
   expected to provide a powerful tool for automating various environmental
   close-range remote sensing tasks in the very near future.}},
DOI = {{10.3390/rs9030185}},
Article-Number = {{185}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Imai, Nilton/O-8909-2018
   }},
ORCID-Numbers = {{Imai, Nilton/0000-0003-0516-0567
   Nevalainen, Olli/0000-0002-4826-2929
   Honkavaara, Eija/0000-0002-7236-2145}},
Unique-ID = {{ISI:000398720100002}},
}

@article{ ISI:000398720100102,
Author = {Ni, Huan and Lin, Xiangguo and Zhang, Jixian},
Title = {{Classification of ALS Point Cloud with Improved Point Cloud Segmentation
   and Random Forests}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{This paper presents an automated and effective framework for classifying
   airborne laser scanning (ALS) point clouds. The framework is composed of
   four stages: (i) step-wise point cloud segmentation, (ii) feature
   extraction, (iii) Random Forests (RF) based feature selection and
   classification, and (iv) post-processing. First, a step-wise point cloud
   segmentation method is proposed to extract three kinds of segments,
   including planar, smooth and rough surfaces. Second, a segment, rather
   than an individual point, is taken as the basic processing unit to
   extract features. Third, RF is employed to select features and classify
   these segments. Finally, semantic rules are employed to optimize the
   classification result. Three datasets provided by Open Topography are
   utilized to test the proposed method. Experiments show that our method
   achieves a superior classification result with an overall classification
   accuracy larger than 91.17\%, and kappa coefficient larger than 83.79\%.}},
DOI = {{10.3390/rs9030288}},
Article-Number = {{288}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000398720100102}},
}

@article{ ISI:000397032500012,
Author = {Yao, Fei and Wang, Jian and Yao, Ju and Hang, Fangrong and Lei, Xu and
   Cao, Yongke},
Title = {{Three-dimensional image reconstruction with free open-source OsiriX
   software in video-assisted thoracoscopic lobectomy and segmentectomy}},
Journal = {{INTERNATIONAL JOURNAL OF SURGERY}},
Year = {{2017}},
Volume = {{39}},
Pages = {{16-22}},
Month = {{MAR}},
Abstract = {{Objective: The aim of this retrospective study was to evaluate the
   practice and the feasibility of Osirix, a free and open-source medical
   imaging software, in performing accurate video-assisted thoracoscopic
   lobectomy and segmentectomy.
   Methods: From July 2014 to April 2016, 63 patients received anatomical
   video-assisted thoracoscopic surgery (VATS), either lobectomy or
   segmentectomy, in our department. Three-dimensional (3D) reconstruction
   images of 61 (96.8\%) patients were preoperatively obtained with
   contrast-enhanced computed tomography (CT). Preoperative resection
   simulations were accomplished with patient-individual reconstructed 3D
   images. For lobectomy, pulmonary lobar veins, arteries and bronchi were
   identified meticulously by carefully reviewing the 3D images on the
   display. For segmentectomy, the intrasegmental veins in the affected
   segment for division and the intersegmental veins to be preserved were
   identified on the 3D images. Patient preoperative characteristics,
   surgical outcomes and postoperative data were reviewed from a
   prospective database.
   Results: The study cohort of 63 patients included 33 (52.4\%) men and 30
   (47.6\%) women, of whom 46 (73.0\%) underwent VATS lobectomy and 17
   (27.0\%) underwent VATS segmentectomy. There was 1 conversion from VATS
   lobectomy to open thoracotomy because of fibrocalcified lymph nodes. A
   VATS lobectomy was performed in 1 case after completing the
   segmentectomy because invasive adenocarcinoma was detected by
   intraoperative frozen-section analysis. There were no 30-day or 90-day
   operative mortalities
   Conclusions: The free, simple, and user-friendly software program Osirix
   can provide a 3D anatomic structure of pulmonary vessels and a clear
   vision into the space between the lesion and adjacent tissues, which
   allows surgeons to make preoperative simulations and improve the
   accuracy and safety of actual surgery. (C) 2017 IJS Publishing Group
   Ltd. Published by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.ijsu.2017.01.079}},
ISSN = {{1743-9191}},
EISSN = {{1743-9159}},
Unique-ID = {{ISI:000397032500012}},
}

@article{ ISI:000395034500015,
Author = {Gibelli, Daniele and De Angelis, Danilo and Poppa, Pasquale and Sforza,
   Chiarella and Cattaneo, Cristina},
Title = {{An Assessment of How Facial Mimicry Can Change Facial Morphology:
   Implications for Identification}},
Journal = {{JOURNAL OF FORENSIC SCIENCES}},
Year = {{2017}},
Volume = {{62}},
Number = {{2}},
Pages = {{405-410}},
Month = {{MAR}},
Abstract = {{The assessment of facial mimicry is important in forensic anthropology;
   in addition, the application of modern 3D image acquisition systems may
   help for the analysis of facial surfaces. This study aimed at exposing a
   novel method for comparing 3D profiles in different facial expressions.
   Ten male adults, aged between 30 and 40 years, underwent acquisitions by
   stereophotogrammetry (VECTRA-3D (R)) with different expressions
   (neutral, happy, sad, angry, surprised). The acquisition of each
   individual was then superimposed on the neutral one according to nine
   landmarks, and the root mean square (RMS) value between the two
   expressions was calculated. The highest difference in comparison with
   the neutral standard was shown by the happy expression (RMS 4.11 mm),
   followed by the surprised (RMS 2.74 mm), sad (RMS 1.3 mm), and angry
   ones (RMS 1.21 mm). This pilot study shows that the 3D-3D
   superimposition may provide reliable results concerning facial
   alteration due to mimicry.}},
DOI = {{10.1111/1556-4029.13295}},
ISSN = {{0022-1198}},
EISSN = {{1556-4029}},
ORCID-Numbers = {{De Angelis, Danilo/0000-0001-6388-9415}},
Unique-ID = {{ISI:000395034500015}},
}

@article{ ISI:000395034500021,
Author = {Gibelli, Daniele and De Angelis, Danilo and Poppa, Pasquale and Sforza,
   Chiarella and Cattaneo, Cristina},
Title = {{A View to the Future: A Novel Approach for 3D-3D Superimposition and
   Quantification of Differences for Identification from Next-Generation
   Video Surveillance Systems}},
Journal = {{JOURNAL OF FORENSIC SCIENCES}},
Year = {{2017}},
Volume = {{62}},
Number = {{2}},
Pages = {{457-461}},
Month = {{MAR}},
Abstract = {{Techniques of 2D-3D superimposition are widely used in cases of personal
   identification from video surveillance systems. However, the progressive
   improvement of 3D image acquisition technology will enable operators to
   perform also 3D-3D facial superimposition. This study aims at analyzing
   the possible applications of 3D-3D superimposition to personal
   identification, although from a theoretical point of view. Twenty
   subjects underwent a facial 3D scan by stereophotogrammetry twice at
   different time periods. Scans were superimposed two by two according to
   nine landmarks, and root-mean-square (RMS) value of point-to-point
   distances was calculated. When the two superimposed models belonged to
   the same individual, RMS value was 2.10 mm, while it was 4.47 mm in
   mismatches with a statistically significant difference (p < 0.0001).
   This experiment shows the potential of 3D-3D superimposition: Further
   studies are needed to ascertain technical limits which may occur in
   practice and to improve methods useful in the forensic practice.}},
DOI = {{10.1111/1556-4029.13290}},
ISSN = {{0022-1198}},
EISSN = {{1556-4029}},
ResearcherID-Numbers = {{Sforza, Chiarella/C-3008-2015
   }},
ORCID-Numbers = {{Sforza, Chiarella/0000-0001-6532-6464
   De Angelis, Danilo/0000-0001-6388-9415}},
Unique-ID = {{ISI:000395034500021}},
}

@article{ ISI:000395521200012,
Author = {Chen, Jingdao and Fang, Yihai and Cho, Yong K. and Kim, Changwan},
Title = {{Principal Axes Descriptor for Automated Construction-Equipment
   Classification from Point Clouds}},
Journal = {{JOURNAL OF COMPUTING IN CIVIL ENGINEERING}},
Year = {{2017}},
Volume = {{31}},
Number = {{2}},
Month = {{MAR}},
Abstract = {{Recognizing construction assets (e.g.,materials, equipment, labor) from
   point cloud data of construction environments provides essential
   information for engineering and management applications including
   progress monitoring, safety management, supply-chain management, and
   quality control. This study introduces a novel principal axes descriptor
   (PAD) for construction-equipment classification from point cloud data.
   Scattered as-is point clouds are first processed with downsampling,
   segmentation, and clustering steps to obtain individual instances of
   construction equipment. A geometric descriptor consisting of dimensional
   variation, occupancy distribution, shape profile, and plane counting
   features is then calculated to encode three-dimensional (3D)
   characteristics of each equipment category. Using the derived features,
   machine learning methods such as k-nearest neighbors and support vector
   machine are employed to determine class membership among major
   construction-equipment categories such as backhoe loader, bulldozer,
   dump truck, excavator, and front loader. Construction-equipment
   classification with the proposed PAD was validated using computer-aided
   design (CAD)-generated point clouds as training data and laser-scanned
   point clouds from an equipment yard as testing data. The recognition
   performance was further evaluated using point clouds from a construction
   site as well as a pose variation data set. PAD was shown to achieve a
   higher recall rate and lower computation time compared to competing 3D
   descriptors. The results indicate that the proposed descriptor is a
   viable solution for construction-equipment classification from point
   cloud data. (C) 2016 American Society of Civil Engineers.}},
DOI = {{10.1061/(ASCE)CP.1943-5487.0000628}},
Article-Number = {{04016058}},
ISSN = {{0887-3801}},
EISSN = {{1943-5487}},
ORCID-Numbers = {{Fang, Yihai/0000-0002-9451-4947}},
Unique-ID = {{ISI:000395521200012}},
}

@article{ ISI:000397013700050,
Author = {Hu, Xingbo and Chen, Wei and Xu, Weiyang},
Title = {{Adaptive Mean Shift-Based Identification of Individual Trees Using
   Airborne LiDAR Data}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Identifying individual trees and delineating their canopy structures
   from the forest point cloud data acquired by an airborne LiDAR (Light
   Detection And Ranging) has significant implications in forestry
   inventory. Once accurately identified, tree structural attributes such
   as tree height, crown diameter, canopy based height and diameter at
   breast height can be derived. This paper focuses on a novel
   computationally efficient method to adaptively calibrate the kernel
   bandwidth of a computational scheme based on mean shift-a non-parametric
   probability density-based clustering technique-to segment the 3D
   (three-dimensional) forest point clouds and identify individual tree
   crowns. The basic concept of this method is to partition the 3D space
   over each test plot into small vertical units (irregular columns
   containing 3D spatial features from one or more trees) first, by using a
   fixed bandwidth mean shift procedure and a small square grouping
   technique, and then rough estimation of crown sizes for distinct trees
   within a unit, based on an original 2D (two-dimensional) incremental
   grid projection technique, is applied to provide a basis for dynamical
   calibration of the kernel bandwidth for an adaptive mean shift procedure
   performed in each partition. The adaptive mean shift-based scheme, which
   incorporates our proposed bandwidth calibration method, is validated on
   10 test plots of a dense, multi-layered evergreen broad-leaved forest
   located in South China. Experimental results reveal that this approach
   can work effectively and when compared to the conventional point-based
   approaches (e.g., region growing, k-means clustering, fixed bandwidth or
   multi-scale mean shift), its accuracies are relatively high: it detects
   86 percent of the trees ({''}recall{''}) and 92 percent of the
   identified trees are correct ({''}precision{''}), showing good potential
   for use in the area of forest inventory.}},
DOI = {{10.3390/rs9020148}},
Article-Number = {{148}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{xu, weiyang/0000-0002-8980-6005}},
Unique-ID = {{ISI:000397013700050}},
}

@article{ ISI:000397013700010,
Author = {Yu, Xiaowei and Hyyppa, Juha and Litkey, Paula and Kaartinen, Harri and
   Vastaranta, Mikko and Holopainen, Markus},
Title = {{Single-Sensor Solution to Tree Species Classification Using
   Multispectral Airborne Laser Scanning}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{This paper investigated the potential of multispectral airborne laser
   scanning (ALS) data for individual tree detection and tree species
   classification. The aim was to develop a single-sensor solution for
   forest mapping that is capable of providing species-specific
   information, required for forest management and planning purposes.
   Experiments were conducted using 1903 ground measured trees from 22
   sample plots and multispectral ALS data, acquired with an Optech Titan
   scanner over a boreal forest, mainly consisting of Scots pine (Pinus
   Sylvestris), Norway spruce (Picea Abies), and birch (Betula sp.), in
   southern Finland. ALS-features used as predictors for tree species were
   extracted from segmented tree objects and used in random forest
   classification. Different combinations of features, including point
   cloud features, and intensity features of single and multiple channels,
   were tested. Among the field-measured trees, 61.3\% were correctly
   detected. The best overall accuracy (OA) of tree species classification
   achieved for correctly-detected trees was 85.9\% (Kappa = 0.75), using a
   point cloud and single-channel intensity features combination, which was
   not significantly different from the ones that were obtained either
   using all features (OA = 85.6\%, Kappa = 0.75), or single-channel
   intensity features alone (OA = 85.4\%, Kappa = 0.75). Point cloud
   features alone achieved the lowest accuracy, with an OA of 76.0\%.
   Field-measured trees were also divided into four categories. An
   examination of the classification accuracy for four categories of trees
   showed that isolated and dominant trees can be detected with a detection
   rate of 91.9\%, and classified with a high overall accuracy of 90.5\%.
   The corresponding detection rate and accuracy were 81.5\% and 89.8\% for
   a group of trees, 26.4\% and 79.1\% for trees next to a larger tree, and
   7.2\% and 53.9\% for trees situated under a larger tree, respectively.
   The results suggest that Channel 2 (1064 nm) contains more information
   for separating pine, spruce, and birch, followed by channel 1 (1550 nm)
   and channel 3 (532 nm) with an overall accuracy of 81.9\%, 78.3\%, and
   69.1\%, respectively. Our results indicate that the use of multispectral
   ALS data has great potential to lead to a single-sensor solution for
   forest mapping.}},
DOI = {{10.3390/rs9020108}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Kaartinen, Harri/B-1474-2015
   Vastaranta, Mikko/K-9656-2018}},
ORCID-Numbers = {{Vastaranta, Mikko/0000-0001-6552-9122}},
Unique-ID = {{ISI:000397013700010}},
}

@article{ ISI:000395844700002,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Title = {{Statistical non-rigid ICP algorithm and its application to 3D face
   alignment}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{58}},
Pages = {{3-12}},
Month = {{FEB}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means Of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region. (C) 2016 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.imavis.2016.10.007}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000395844700002}},
}

@article{ ISI:000391965900001,
Author = {Dinh-Cuong Hoang and Liang-Chia Chen and Thanh-Hung Nguyen},
Title = {{Sub-OBB based object recognition and localization algorithm using range
   images}},
Journal = {{MEASUREMENT SCIENCE AND TECHNOLOGY}},
Year = {{2017}},
Volume = {{28}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{This paper presents a novel approach to recognize and estimate pose of
   the 3D objects in cluttered range images. The key technical breakthrough
   of the developed approach can enable robust object recognition and
   localization under undesirable condition such as environmental
   illumination variation as well as optical occlusion to viewing the
   object partially. First, the acquired point clouds are segmented into
   individual object point clouds based on the developed 3D object
   segmentation for randomly stacked objects. Second, an efficient
   shape-matching algorithm called Sub-OBB based object recognition by
   using the proposed oriented bounding box (OBB) regional area-based
   descriptor is performed to reliably recognize the object. Then, the 3D
   position and orientation of the object can be roughly estimated by
   aligning the OBB of segmented object point cloud with OBB of matched
   point cloud in a database generated from CAD model and 3D virtual
   camera. To detect accurate pose of the object, the iterative closest
   point (ICP) algorithm is used to match the object model with the
   segmented point clouds. From the feasibility test of several scenarios,
   the developed approach is verified to be feasible for object pose
   recognition and localization.}},
DOI = {{10.1088/1361-6501/aa513a}},
Article-Number = {{025401}},
ISSN = {{0957-0233}},
EISSN = {{1361-6501}},
Unique-ID = {{ISI:000391965900001}},
}

@inproceedings{ ISI:000425498402048,
Author = {Liu, Liu and Li, Hongdong and Dai, Yuchao},
Book-Group-Author = {{IEEE}},
Title = {{Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale
   3D Map}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)}},
Series = {{IEEE International Conference on Computer Vision}},
Year = {{2017}},
Pages = {{2391-2400}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{Given an image of a street scene in a city, this paper develops a new
   method that can quickly and precisely pinpoint at which location (as
   well as viewing direction) the image was taken, against a pre-stored
   large-scale 3D point-cloud map of the city. We adopt the recently
   developed 2D-3D direct feature matching framework for this task
   {[}23,31,32,42-44]. This is a challenging task especially for
   large-scale problems. As the map size grows bigger, many 3D points in
   the wider geographical area can be visually very similar-or even
   identical-causing severe ambiguities in 2D-3D feature matching. The key
   is to quickly and unambiguously find the correct matches between a query
   image and the large 3D map. Existing methods solve this problem mainly
   via comparing individual features' visual similarities in a local and
   per feature manner, thus only local solutions can be found, inadequate
   for large-scale applications.
   In this paper, we introduce a global method which harnesses global
   contextual information exhibited both within the query image and among
   all the 3D points in the map. This is achieved by a novel global ranking
   algorithm, applied to a Markov network built upon the 3D map, which
   takes account of not only visual similarities between individual 2D-3D
   matches, but also their global compatibilities (as measured by
   co-visibility) among all matching pairs found in the scene. Tests on
   standard benchmark datasets show that our method achieved both higher
   precision and comparable recall, compared with the state-of-the-art.}},
DOI = {{10.1109/ICCV.2017.260}},
ISSN = {{1550-5499}},
ISBN = {{978-1-5386-1032-9}},
ORCID-Numbers = {{Liu, Liu/0000-0002-5880-5974}},
Unique-ID = {{ISI:000425498402048}},
}

@article{ ISI:000397995100002,
Author = {Ahmed, Oumer S. and Shemrock, Adam and Chabot, Dominique and Dillon,
   Chris and Williams, Griffin and Wasson, Rachel and Franklin, Steven E.},
Title = {{Hierarchical land cover and vegetation classification using
   multispectral data acquired from an unmanned aerial vehicle}},
Journal = {{INTERNATIONAL JOURNAL OF REMOTE SENSING}},
Year = {{2017}},
Volume = {{38}},
Number = {{8-10}},
Pages = {{2037-2052}},
Note = {{Conference on Small Unmanned Aerial Systems (sUAS) for Environmental
   Research, Univ Worcester, Worcester, ENGLAND, JUN, 2016}},
Abstract = {{The use of multispectral cameras deployed on unmanned aerial vehicles
   (UAVs) in land cover and vegetation mapping applications continues to
   improve and receive increasing recognition and adoption by resource
   management and forest survey practitioners. Comparisons of different
   camera data and platform performance characteristics are an important
   contribution in understanding the role and operational capability of
   this technology. In this article, object-based classification accuracies
   for different cover types and vegetation species of interest in central
   Ontario were examined using data from three UAV-based multispectral
   cameras. Five land-cover classes (forest, shrub, herbaceous, bare soil,
   and built-up) were determined to be up to 95\% correct overall with
   calibrated multispectral Parrot Sequoia digital camera data compared to
   independent field observations. The levels of classification accuracy
   decreased approximately 10-15\% when spectrally less capable
   consumer-grade RGB sensors were used. Multispectral Parrot Sequoia
   classification accuracy was approximately 89\% when more detailed
   vegetation classes, including individual deciduous tree species, shrub
   communities and agricultural crops, were analysed. Additional work is
   suggested in the use of such UAV multispectral and point cloud data in
   ash tree discrimination to support emerald ash borer infestation
   detection and management, and in analysis of functional and structural
   vegetation characteristics (e.g. leaf area index).}},
DOI = {{10.1080/01431161.2017.1294781}},
ISSN = {{0143-1161}},
EISSN = {{1366-5901}},
Unique-ID = {{ISI:000397995100002}},
}

@inproceedings{ ISI:000434278900149,
Author = {Swetha, K. M. and Suja, P.},
Editor = {{Niranjan, SK and Manvi, SS and Kodabagi, MM and Hulipalled, VR}},
Title = {{A Geometric Approach for Recognizing Emotions From 3D Images with Pose
   Variations}},
Booktitle = {{PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES
   FOR SMART NATION (SMARTTECHCON)}},
Year = {{2017}},
Pages = {{805-809}},
Note = {{International Conference On Smart Technologies For Smart Nation
   (SmartTechCon), REVA Univ, Bengaluru, INDIA, AUG 17-19, 2017}},
Organization = {{IEEE; IEEE Bangalore Sect; IEEE Computat Intelligence Soc, Bangalore
   Chapter; CSIR}},
Abstract = {{Emotions are an incredibly important aspect of human life. Research on
   emotion recognition for the past few decades have resulted in
   development of several fields. In the current scenario, it is necessary
   that machines/robots need to identify human emotions and respond
   accordingly. Applications in this field can be seen in security,
   entertainment and Human Machine Interface/Human Robot Interface. Recent
   works on 3D images have gained importance due to its accuracy in real
   life applications as emotions can be recognised at different head poses.
   The intention of this work has been to develop an algorithm for
   recognition of emotion from facial expressions, which recognizes 6 basic
   emotions, which are anger, fear, happy, disgust, sad and surprise from
   3D images in 7 yaw angles (+45 degrees to -45 degrees) and 3 pitch
   angles (+15 degrees, 0 degrees, -15 degrees). Most of the reported work
   considers + yaw angles. While in the current work, both positive as well
   as negative pitch and yaw angles are considered. BU3DFE database is used
   for the implementation. The proposed method resulted in improved
   accuracy and is comparable with the literature.}},
ISBN = {{978-1-5386-0569-1}},
Unique-ID = {{ISI:000434278900149}},
}

@inproceedings{ ISI:000432373000248,
Author = {Varga, Robert and Costea, Arthur and Florea, Horatiu and Giosan, Ion and
   Nedevschi, Sergiu},
Book-Group-Author = {{IEEE}},
Title = {{Super-sensor for 360-degree Environment Perception: Point Cloud
   Segmentation Using Image Features}},
Booktitle = {{2017 IEEE 20TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION
   SYSTEMS (ITSC)}},
Series = {{IEEE International Conference on Intelligent Transportation Systems-ITSC}},
Year = {{2017}},
Note = {{20th IEEE International Conference on Intelligent Transportation Systems
   (ITSC), Yokohama, JAPAN, OCT 16-19, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper describes a super-sensor that enables 360-degree environment
   perception for automated vehicles in urban traffic scenarios. We use
   four fisheye cameras, four 360 degree LIDARs and a GPS/IMU sensor
   mounted on an automated vehicle to build a super-sensor that offers an
   enhanced low-level representation of the environment by harmonizing all
   the available sensor measurements. Individual sensors cannot provide a
   robust 360-degree perception due to their limitations: field of view,
   range, orientation, number of scanning rays, etc. The novelty of this
   work consists of segmenting the 3D LIDAR point cloud by associating it
   with the 2D image semantic segmentation. Another contribution is the
   sensor configuration that enables 360-degree environment perception. The
   following steps are involved in the process: calibration, timestamp
   synchronization, fisheye image unwarping, motion correction of LIDAR
   points, point cloud projection onto the images and semantic segmentation
   of images. The enhanced low-level representation will improve the
   high-level perception environment tasks such as object detection,
   classification and tracking.}},
ISSN = {{2153-0009}},
ISBN = {{978-1-5386-1526-3}},
Unique-ID = {{ISI:000432373000248}},
}

@inproceedings{ ISI:000432372100099,
Author = {Torkhani, Ghada and Ladgham, Anis and Sakly, Anis},
Book-Group-Author = {{IEEE}},
Title = {{3D Gabor-Edge Filters Applied to Face Depth Images}},
Booktitle = {{2017 18TH INTERNATIONAL CONFERENCE ON SCIENCES AND TECHNIQUES OF
   AUTOMATIC CONTROL AND COMPUTER ENGINEERING (STA)}},
Series = {{International Conference on Sciences and Techniques of Automatic Control
   and Computer Engineering}},
Year = {{2017}},
Pages = {{578-582}},
Note = {{18th International Conference on Sciences and Techniques of Automatic
   Control and Computer Engineering (STA), Monastir, TUNISIA, DEC 21-23,
   2017}},
Organization = {{IEEE Tunisia Sect; Tunisian Assoc Numer Tech \& Automat; Univ Sfax, Natl
   Engn Sch Sfax, Lab Sci \& Tech Automat Control \& Comp Engn; Tunisia
   Sect Control Syst Soc Chapter; Tunisia Sect Robot \& Automat Soc
   Chapter; Tunisia Sect Signal Proc Soc Chapter; Tunisia Sect Circuits \&
   Syst Soc Chapter; Tunisia Sect Solid State Circuits Soc Chapter}},
Abstract = {{This manuscript introduces a novel 3D face authentication system
   inspired from the advantageous capacities of Gabor-Edge filters. The
   approach studies 3D face difficulties such as expression variety,
   different rotations and exposure to illuminations. The proposed systems
   starts by preprocessing the 3D face images to resolve acquisition
   problems. Then, a filtering process is performed by implanting our 3D
   Gabor-Edge technique extended based on the classic 3D Gabor masks. The
   next step is to achieve the classification of facial features from the
   edge saliency by the artificial Neural Network Classifier (NNC). The
   evaluation of the adopted system is achieved by exporting common
   datasets from GavabDB database. Experimental results are reported to
   prove the high accuracy rates of our method compared to the recent
   researches in the same biometric field.}},
ISSN = {{2378-7163}},
ISBN = {{978-1-5386-1084-8}},
Unique-ID = {{ISI:000432372100099}},
}

@inproceedings{ ISI:000427598702135,
Author = {Mohsin, Nasreen and Payandeh, Shahram},
Book-Group-Author = {{IEEE}},
Title = {{Localization and Identification of Body Extremities Based on Data from
   Multiple Depth Sensors}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)}},
Series = {{IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings}},
Year = {{2017}},
Pages = {{2736-2741}},
Note = {{IEEE International Conference on Systems, Man, and Cybernetics (SMC),
   Banff, CANADA, OCT 05-08, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper explores the novel use of multiple depth sensors to overcome
   occlusions and improve localization and tracking of body extremities.
   The usage of data from only depth sensors not only overcomes visual
   challenges associated with RGB sensors under low illumination, but also
   protects the identity of surveyed person with high confidentiality. For
   integrating depth information from multiple sources, the paper presents
   first an overview of a novel calibration method for multiple depth
   sensors. In case of occlusion of any fiducial point in the primary
   sensor's depth image, co-ordinates of the point can be obtained from the
   frame of other sensors using the calibration parameters. To localize
   salient body parts such as hands, head and feet, a surface triangular
   mesh is applied on generated 3D point cloud from the primary sensor. The
   geodesic extrema from the mesh coincide with body extremities. The body
   extremities can be identified based on those relative geodesic distances
   between the extremities. Once the body parts are labelled, a portion of
   body can be targeted and evaluated for specific gait analysis and
   visualization. For the performance evaluation, our calibration method
   has fared well in comparison to other available techniques. Also, our
   proposed localization of salient body parts is able to successfully tag
   the specific body part i.e. the head region.}},
ISSN = {{1062-922X}},
ISBN = {{978-1-5386-1645-1}},
Unique-ID = {{ISI:000427598702135}},
}

@inproceedings{ ISI:000427083300058,
Author = {Jazouli, Maha and Majda, Aicha and Zarghili, Arsalane},
Editor = {{Nfaoui, E and Boumhidi, J and Sabri, A and Yahyaouy, A and Bouchaffra, D}},
Title = {{A \$P Recognizer for Automatic Facial Emotion Recognition using Kinect
   Sensor}},
Booktitle = {{2017 INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV)}},
Year = {{2017}},
Note = {{Intelligent Systems and Computer Vision (ISCV), Fez, MOROCCO, APR 17-19,
   2017}},
Organization = {{IEEE; Univ Sidi Mohamed ben Abdellah; IEEE Comp Soc; IEEE Morocco Sect}},
Abstract = {{Autism is a developmental disorder involving qualitative impairments in
   social interaction. One source of those impairments are difficulties
   with facial expressions of emotion. Autistic people often have
   difficulty to recognize or to understand other people's emotions and
   feelings, or expressing their own. This work proposes a method to
   automatically recognize seven basic emotions among autistic children in
   real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and
   Neutral. The method uses the Microsoft Kinect sensor to track and
   identify points of interest from the 3D face model and it is based on
   the \$P point-cloud recognizer to identify multi-stroke emotions as
   point-clouds. The experimental results show that our system can achieve
   above 94.28\% recognition rate. Our study provides a novel clinical tool
   to help children with autism to assisting doctors in operating rooms.}},
ISBN = {{978-1-5090-4062-9}},
Unique-ID = {{ISI:000427083300058}},
}

@inproceedings{ ISI:000427293600008,
Author = {Frikha, Tarek and Chaabane, Faten and Said, Boukhchim and Drira, Hassen
   and Abid, Mohamed and Ben Amar, Chokri},
Editor = {{ElHassouni, M and Karim, M and BenHamida, A and BenSlima, A and Solaiman, B}},
Title = {{Embedded approach for a Riemannian-based framework of analyzing 3D faces}},
Booktitle = {{2017 3RD INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)}},
Year = {{2017}},
Pages = {{43-47}},
Note = {{3rd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP), Fez, MOROCCO, MAY 22-24, 2017}},
Organization = {{Univ Sidi Mohamed Ben Abdellah; Fac Sci; Fac Med \& Pharm; CNRST; TICSM;
   IEEE Morocco Sect; IEEE Signal Proc Soc Morocco Chapter}},
Abstract = {{Developing multimedia embedded applications continues to flourish. In
   fact, a biometric facial recognition system can be used not only on PCs
   abut also in embedded systems, it is a potential enhancer to meet
   security and surveillance needs. The analysis of facial recognition
   consists offoursteps: face analysis, face expressions' recognition,
   missing data completion and full face recognition.
   This paper proposes a hardware architecture based on an adaptation
   approach foran algorithm which has proven good face detection and
   recognition in 3D space. The proposed application was tested using a co
   design technique based on a mixed Hardware Software architecture: the
   FPGA platform.}},
ISBN = {{978-1-5386-0551-6}},
Unique-ID = {{ISI:000427293600008}},
}

@inproceedings{ ISI:000426973200029,
Author = {Li, Huibin and Sun, Jian and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Location-Sensitive Sparse Representation of Deep Normal Patterns for
   Expression-robust 3D Face Recognition}},
Booktitle = {{2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)}},
Year = {{2017}},
Pages = {{234-242}},
Note = {{IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
   OCT 01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper presents a straight-forward yet efficient, and
   expression-robust 3D face recognition approach by exploring location
   sensitive sparse representation of deep normal patterns (DNP). In
   particular given raw 3D facial surfaces, we first run 3D face
   pre-processing pipeline, including nose tip detection, face region
   cropping, and pose normalization. The 3D coordinates of each normalized
   3D facial surface are then projected into 2D plane to generate geometry
   images, from which three images of facial surface normal components are
   estimated. Each normal image is then fed into a pre-trained deep face
   net to generate deep representations of facial surface normals, i.e.,
   deep normal patterns. Considering the importance of different facial
   locations, we propose a location sensitive sparse representation
   classifier (LS-SRC) for similarity measure among deep normal patterns
   associated with different 3D faces. Finally, simple score-level fusion
   of different normal components are used for the final decision. The
   proposed approach achieves significantly high performance, and reporting
   rank-one scores of 98.01\%, 97.60\%, and 96.13\% on the FRGC v2.0,
   Bosphorus, and BU-3DFE databases when only one sample per subject is
   used in the gallery. These experimental results reveals that the
   performance of 3D face recognition would be constantly improved with the
   aid of training deep models from massive 2D face images, which opens the
   door for future directions of 3D face recognition.}},
ISBN = {{978-1-5386-1124-1}},
Unique-ID = {{ISI:000426973200029}},
}

@inproceedings{ ISI:000426886000048,
Author = {Lopez, G. and Pallas, B. and Martinez, S. and Lauri, P. E. and Regnard,
   J. L. and Durel, C. E. and Costes, E.},
Editor = {{Marsal, J and Girona, J}},
Title = {{High-throughput phenotyping of an apple core collection: identification
   of genotypes with high water use efficiency}},
Booktitle = {{VIII INTERNATIONAL SYMPOSIUM ON IRRIGATION OF HORTICULTURAL CROPS}},
Series = {{Acta Horticulturae}},
Year = {{2017}},
Volume = {{1150}},
Pages = {{335-340}},
Note = {{8th International Symposium on Irrigation of Horticultural Crops,
   Lleida, SPAIN, JUN 08-11, 2015}},
Organization = {{Int Soc Horticultural Sci}},
Abstract = {{To detect genotypes with high water use efficiency (WUE) in apple (Malus
   x domestica), 193 genotypes from an INRA core collection were evaluated
   in 2014. Eight grafted replicates per genotype grown as one-year-old
   scions were studied in a high-throughput phenotyping platform
   (PhenoArch). Individual pot weight was recorded twice a day and
   irrigation was scheduled for 46 days according to two irrigation
   treatments: well-watered (WW), maintaining soil water content (SWC) at
   1.4 g g(-1); and water stress (WS), reducing SWC until 0.7 g g(-1) and
   maintaining this value for ten days. For each genotype, half of the
   replicates were WW while the other half were grown under WS. Plant 3D
   images were automatically acquired every two days. Analysis of images
   and pot weight differences allowed the estimation of the accumulated
   whole-plant biomass (A\_Bio) and transpiration (Plant\_T) during the
   experiment. WUE was calculated as the ratio A\_Bio/Plant\_T. A\_Bio and
   WUE had a higher genetic variation than Plant\_T under WW and WS
   conditions. The genetic variation in WUE is a promising result,
   indicating that available genetic resources such as the INRA core
   collection could be useful to improve apple plant material for the use
   of water. WS reduced A\_Bio and Plant\_T but the reduction was less
   evident in WUE. Some genotypes had similar WUE values under WW and WS
   conditions. We identified of a group of 38 genotypes with high WUE under
   WW and WS. The existence of genotypes with high WUE whatever the water
   regime in apple may encourage apple breeders to consider the use of
   these genotypes as potential parents for improving apple plant material
   for the use of water.}},
DOI = {{10.17660/ActaHortic.2017.1150.48}},
ISSN = {{0567-7572}},
EISSN = {{2406-6168}},
ISBN = {{978-94-62611-45-0}},
Unique-ID = {{ISI:000426886000048}},
}

@inproceedings{ ISI:000425931000007,
Author = {Asl, Azin Shabani and Oskoei, Mohammadreza Asghari},
Book-Group-Author = {{IEEE}},
Title = {{Depth Dependent Invariant Features Applied to Person Detection Using 3D
   Camera}},
Booktitle = {{2017 5TH IRANIAN JOINT CONGRESS ON FUZZY AND INTELLIGENT SYSTEMS (CFIS)}},
Year = {{2017}},
Pages = {{29-34}},
Note = {{5th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS),
   Qazvin Islam Azad Univ, Tehran, IRAN, MAR 07-09, 2017}},
Abstract = {{This paper is about detection and tracking a person by mobile robots in
   in-door environments, such as shopping center and hospital. It uses
   vision based approaches to recognize texture of clothes. The paper
   proposes a method to use depth (distance) reference along with scale
   invariant features (SIFT) to recognize patterns in various orientation,
   distance and illumination. SIFT is an important feature detection
   algorithm that is robust against rotation, translation, and scaling in
   2D images and to some extent against variations in lighting conditions.
   But it suffers inadequate performance for visual patterns rotated in 3D
   space. To overcome this issue, reference inputs given to the algorithm
   was extended to include images taken from different angles. The proposed
   algorithm showed considerably improved performance in detection for
   real-time applications.}},
ISBN = {{978-1-5090-4008-7}},
Unique-ID = {{ISI:000425931000007}},
}

@inproceedings{ ISI:000425238900023,
Author = {Kaur, Rajwant and Sharma, Dolly and Verma, Amit},
Editor = {{Sood, M and Jain, S}},
Title = {{An Advance 2D Face Recognition by Feature Extraction (ICA) and Optimize
   Multilayer Architecture}},
Booktitle = {{PROCEEDINGS OF 4TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING,
   COMPUTING AND CONTROL (ISPCC 2K17)}},
Series = {{IEEE International Conference on Signal Processing Computing and Control}},
Year = {{2017}},
Pages = {{122-129}},
Note = {{4th IEEE International Conference on Signal Processing, Computing and
   Control (ISPCC), Jaypee Univ Informat Technol, Dept Elect \& Commun
   Engn, Solan, INDIA, SEP 21-23, 2017}},
Organization = {{IEEE; IEEE Delhi Sect; IEEE Jaypee Univ Informat Technol Student Branch;
   GENTECH; AD Instruments; ARK}},
Abstract = {{Facial recognition has most significant real-life requests like
   investigation and access control. It is associated through the issue of
   appropriately verifying face pictures and transmit them person in a
   database. In a past years face study has been emerging active topic.
   Most of the face detector techniques could be classified into feature
   based methods and image based also. Feature based techniques adds
   low-level analysis, feature analysis, etc. Facial recognition is a
   system capable of verifying / identifying a human after 3D images. By
   evaluating selected facial unique features from the image and face
   dataset. Design from transformation method given vector dimensional
   illustration of individual face in a prepared set of images, Principle
   component analysis inclines to search a dimensional sub-space whose
   normal vector features correspond to the maximum variance direction in
   the real image space. The PCA algorithm evaluates the feature
   extraction, data, i.e. Eigen Values and vectors of the scatter matrix.
   In literature survey, Face recognition is a design recognition mission
   performed exactly on faces. It can be described as categorizing a facial
   either ``known{''} or ``unknown{''}, after comparing it with deposits
   known individuals. It is also necessary to need a system that has the
   capability of knowledge to recognize indefinite faces. Computational
   representations of facial recognition must statement various difficult
   issues. After existing work, we study the SIFT structures for the
   gratitude method. The novel technique is compared with well settled
   facial recognition methods, name component analysis and eigenvalues and
   vector. This algorithm is called PCA and ICA (Independent Component
   Analysis). In research work, we implement the novel approach to detect
   the face in minimum time and evaluate the better accuracy based on Back
   Propagation Neural Networks. We design the framework in face recognition
   using MATLAB 2013a simulation tool. Evaluate the performance parameters,
   i.e. the FAR (false acceptance rate), FRR (False rejection Rate) and
   Accuracy and compare the existing performance parameters i.e. accuracy.}},
ISSN = {{2376-5461}},
ISBN = {{978-1-5090-5838-9}},
Unique-ID = {{ISI:000425238900023}},
}

@inproceedings{ ISI:000423869700004,
Author = {Haufel, Gisela and Bulatov, Dimitri and Solbrig, Peter},
Editor = {{Michel, U and Schulz, K and Nikolakopoulos, KG and Civco, D}},
Title = {{Sensor Data Fusion for Textured Reconstruction and Virtual
   Representation of Alpine Scenes}},
Booktitle = {{EARTH RESOURCES AND ENVIRONMENTAL REMOTE SENSING/GIS APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10428}},
Note = {{17th SPIE Conference on Earth Resources and Environmental Remote
   Sensing/GIS Applications, Warsaw, POLAND, SEP 12-14, 2017}},
Organization = {{SPIE}},
Abstract = {{The concept of remote sensing is to provide information about a
   wide-range area without making physical contact with this area. If,
   additionally to satellite imagery, images and videos taken by drones
   provide a more up-to-date data at a higher resolution, or accurate
   vector data is downloadable from the Internet, one speaks of sensor data
   fusion. The concept of sensor data fusion is relevant for many
   applications, such as virtual tourism, automatic navigation, hazard
   assessment, etc. In this work, we describe sensor data fusion aiming to
   create a semantic 3D model of an extremely interesting yet challenging
   dataset: An alpine region in Southern Germany. A particular challenge of
   this work is that rock faces including overhangs are present in the
   input airborne laser point cloud. The proposed procedure for
   identification and reconstruction of overhangs from point clouds
   comprises four steps: Point cloud preparation, filtering out vegetation,
   mesh generation and texturing. Further object types are extracted in
   several interesting subsections of the dataset: Building models with
   textures from UAV (Unmanned Aerial Vehicle) videos, hills reconstructed
   as generic surfaces and textured by the orthophoto, individual trees
   detected by the watershed algorithm, as well as the vector data for
   roads retrieved from openly available shape files and GPS-device tracks.
   We pursue geo-specific reconstruction by assigning texture and width to
   roads of several pre-determined types and modeling isolated trees and
   rocks using commercial software. For visualization and simulation of the
   area, we have chosen the simulation system Virtual Battlespace 3 (VBS3).
   It becomes clear that the proposed concept of sensor data fusion allows
   a coarse reconstruction of a large scene and, at the same time, an
   accurate and up-to-date representation of its relevant subsections, in
   which simulation can take place.}},
DOI = {{10.1117/12.2278237}},
Article-Number = {{UNSP 1042805}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1321-8; 978-1-5106-1320-1}},
ORCID-Numbers = {{Bulatov, Dimitri/0000-0002-0560-2591}},
Unique-ID = {{ISI:000423869700004}},
}

@inproceedings{ ISI:000418793200017,
Author = {Hammer, Marcus and Hebel, Marcus and Arens, Michael},
Editor = {{Kamerman, G and Steinvall, O}},
Title = {{Person detection and tracking with a 360 degrees LiDAR system}},
Booktitle = {{ELECTRO-OPTICAL REMOTE SENSING XI}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10434}},
Note = {{Conference on Electro-Optical Remote Sensing XI, Warsaw, POLAND, SEP
   11-12, 2017}},
Organization = {{SPIE}},
Abstract = {{Today it is easily possible to generate dense point clouds of the sensor
   environment using 360 degrees LiDAR (Light Detection and Ranging)
   sensors which are available since a number of years. The interpretation
   of these data is much more challenging. For the automated data
   evaluation the detection and classification of objects is a fundamental
   task. Especially in urban scenarios moving objects like persons or
   vehicles are of particular interest, for instance in automatic collision
   avoidance, for mobile sensor platforms or surveillance tasks.
   In literature there are several approaches for automated person
   detection in point clouds. While most techniques show acceptable results
   in object detection, the computation time is often crucial. The runtime
   can be problematic, especially due to the amount of data in the
   panoramic 360 degrees point clouds. On the other hand, for most
   applications an object detection and classification in real time is
   needed.
   The paper presents a proposal for a fast, real-time capable algorithm
   for person detection, classification and tracking in panoramic point
   clouds.}},
DOI = {{10.1117/12.2278215}},
Article-Number = {{UNSP 104340L}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1333-1; 978-1-5106-1332-4}},
Unique-ID = {{ISI:000418793200017}},
}

@article{ ISI:000418758100001,
Author = {Dai, Yucheng and Gong, Jianhua and Li, Yi and Feng, Quanlong},
Title = {{Building segmentation and outline extraction from UAV image-derived
   point clouds by a line growing algorithm}},
Journal = {{INTERNATIONAL JOURNAL OF DIGITAL EARTH}},
Year = {{2017}},
Volume = {{10}},
Number = {{11}},
Pages = {{1077-1097}},
Abstract = {{This paper presents an approach to process raw unmanned aircraft vehicle
   (UAV) image-derived point clouds for automatically detecting, segmenting
   and regularizing buildings of complex urban landscapes. For
   regularizing, we mean the extraction of the building footprints with
   precise position and details. In the first step, vegetation points were
   extracted using a support vector machine (SVM) classifier based on
   vegetation indexes calculated from color information, then the
   traditional hierarchical stripping classification method was applied to
   classify and segment individual buildings. In the second step, we first
   determined the building boundary points with a modified convex hull
   algorithm. Then, we further segmented these points such that each point
   was assigned to a fitting line using a line growing algorithm. Then, two
   mutually perpendicular directions of each individual building were
   determined through a W-k-means clustering algorithm which used the slop
   information and principal direction constraints. Eventually, the
   building edges were regularized to form the final building footprints.
   Qualitative and quantitative measures were used to evaluate the
   performance of the proposed approach by comparing the digitized results
   from ortho images.}},
DOI = {{10.1080/17538947.2016.1269841}},
ISSN = {{1753-8947}},
EISSN = {{1753-8955}},
Unique-ID = {{ISI:000418758100001}},
}

@inproceedings{ ISI:000418371405064,
Author = {Peng, Weilong and Feng, Zhiyong and Xu, Chao and Su, Yong},
Book-Group-Author = {{IEEE}},
Title = {{Parametric T-spline Face Morphable Model for Detailed Fitting in Shape
   Subspace}},
Booktitle = {{30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR
   2017)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{5515-5523}},
Note = {{30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Honolulu, HI, JUL 21-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration
   for the synthesis of 3D faces by assuming that faces are in a linear
   class. However, the human face is in a nonlinear manifold, and a new
   test are always not in the pre-learnt subspace accurately because of the
   disparity brought by ethnicity, age, gender, etc. In the paper, we
   propose a parametric T-spline morphable model (T-splineMM) for 3D face
   representation, which has great advantages of fitting data from an
   unknown source accurately. In the model, we describe a face by C-2
   T-spline surface, and divide the face surface into several shape units
   (SUs), according to facial action coding system (FACS), on T-mesh
   instead of on the surface directly. A fitting algorithm is proposed to
   optimize coefficients of T-spline control point components along
   pre-learnt identity and expression subspaces, as well as to optimize the
   details in refinement progress. As any pre-learnt subspace is not
   complete to handle the variety and details of faces and expressions, it
   covers a limited span of morphing. SUs division and detail refinement
   make the model fitting the facial muscle deformation in a larger span of
   morphing subspace. We conduct experiments on face scan data, kinect data
   as well as the space-time data to test the performance of detail
   fitting, robustness to missing data and noise, and to demonstrate the
   effectiveness of our model. Convincing results are illustrated to
   demonstrate the effectiveness of our model compared with the popular
   methods.}},
DOI = {{10.1109/CVPR.2017.585}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-0457-1}},
Unique-ID = {{ISI:000418371405064}},
}

@inproceedings{ ISI:000418397700008,
Author = {Richter, Julia and Wiede, Christian and Dayangac, Enes and Shahenshah,
   Ahsan and Hirtz, Gangolf},
Editor = {{Fred, A and DeMarsico, M and DiBaja, GS}},
Title = {{Activity Recognition for Elderly Care by Evaluating Proximity to Objects
   and Human Skeleton Data}},
Booktitle = {{PATTERN RECOGNITION APPLICATIONS AND METHODS, ICPRAM 2016}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10163}},
Pages = {{139-155}},
Note = {{5th International conference on Pattern Recognition Applications and
   Methods (ICPRAM), Rome, ITALY, FEB 24-26, 2016}},
Abstract = {{Recently, researchers have shown an increased interest in the detection
   of activities of daily living (ADLs) for ambient assisted living (AAL)
   applications. In this study, we present an algorithm that detects
   activities related to personal hygiene. The approach is based on the
   evaluation of pose information and a person's proximity to objects
   belonging to the typical equipment of bathrooms, such as sink, toilet
   and shower. In addition to this high-level reasoning, we developed a
   skeleton-based algorithm that recognises actions using a supervised
   learning model. Therefore, we analysed several feature vectors,
   especially with regard to the representation of joint trajectories in
   the frequency domain. The results gave evidence that this high-level
   reasoning algorithm can reliably recognise hygiene-related activities.
   An evaluation of the skeleton-based algorithm shows that the defined
   actions were successfully classified with a rate of 96.66\%.}},
DOI = {{10.1007/978-3-319-53375-9\_8}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-53375-9; 978-3-319-53374-2}},
Unique-ID = {{ISI:000418397700008}},
}

@inproceedings{ ISI:000417429000016,
Author = {Li Fangmin and Chen Ke and Liu Xinhua},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Reconstruction Based on Convolutional Neural Network}},
Booktitle = {{2017 10TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTATION TECHNOLOGY
   AND AUTOMATION (ICICTA 2017)}},
Series = {{International Conference on Intelligent Computation Technology and
   Automation}},
Year = {{2017}},
Pages = {{71-74}},
Note = {{10th International Conference on Intelligent Computation Technology and
   Automation (ICICTA), Changsha, PEOPLES R CHINA, OCT 09-10, 2017}},
Organization = {{Changsha Univ Sci \& Technol, Commun Res Inst; Cent S Univ, Shenzhen Res
   Inst; Hunan City Coll, Dept Urban Management}},
Abstract = {{Fast and robust 3D reconstruction of facial geometric structure from a
   single image is a challenging task with numerous applications, but there
   exist two problems when applied ``in the wild{''}: the 3D estimates are
   unstable for different photos of the same subject; the 3D estimates are
   over-regularized and generic. In response, a robust method for
   regressing discriminative 3D morphable face models(3DMM) is described to
   support face recognition and 3D mask printing. Combining the local data
   sets with the public data sets, improving the exiting 3DMM fitting
   method and then using a convolutional neural network(CNN) to improve
   reconstruction effect. The ground truth 3D faces of the CNN are the
   pooled 3DMM parameters extracted from the photos of the same subject.
   Using CNN to regress 3DMM shape and texture parameters directly from an
   input photo and offering a method for generating huge numbers of labeled
   examples. There are two key points of the paper: one is the training
   data generation for the model training; the other is the training of 3D
   reconstruction model. Experimental results and analysis show that this
   method costs much less time than traditional methods of 3D face
   modeling, and it is improved for different races on photos with any
   angles than the existing methods based on deep learning, and the system
   has better robustness.}},
DOI = {{10.1109/ICICTA.2017.23}},
ISSN = {{1949-1263}},
ISBN = {{978-1-5386-1230-9}},
Unique-ID = {{ISI:000417429000016}},
}

@article{ ISI:000416603000002,
Author = {El Sayed, Abdul Rahman and El Chakik, Abdallah and Alabboud, Hassan and
   Yassine, Adnan},
Title = {{3D face detection based on salient features extraction and skin colour
   detection using data mining}},
Journal = {{IMAGING SCIENCE JOURNAL}},
Year = {{2017}},
Volume = {{65}},
Number = {{7}},
Pages = {{393-408}},
Abstract = {{Face detection has an essential role in many applications. In this
   paper, we propose an efficient and robust method for face detection on a
   3D point cloud represented by a weighted graph. This method classifies
   graph vertices as skin and non-skin regions based on a data mining
   predictive model. Then, the saliency degree of vertices is computed to
   identify the possible candidate face features. Finally, the matching
   between non-skin regions representing eyes, mouth and eyebrows and
   salient regions is done by detecting collisions between polytopes,
   representing these two regions. This method extracts faces from
   situations where pose variation and change of expressions can be found.
   The robustness is showed through different experimental results.
   Moreover, we study the stability of our method according to noise.
   Furthermore, we show that our method deals with 2D images.}},
DOI = {{10.1080/13682199.2017.1358528}},
ISSN = {{1368-2199}},
EISSN = {{1743-131X}},
Unique-ID = {{ISI:000416603000002}},
}

@inproceedings{ ISI:000412830800019,
Author = {Mizoguchi, Tomohiro and Ishii, Akira and Nakamura, Hiroyuki and Inoue,
   Tsuyoshi and Takamatsu, Hisashi},
Editor = {{Remondino, F and Shortis, MR}},
Title = {{Lidar-based Individual Tree Species Classification using Convolutional
   Neural Network}},
Booktitle = {{VIDEOMETRICS, RANGE IMAGING, AND APPLICATIONS XIV}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10332}},
Note = {{Conference on Videometrics, Range Imaging, and Applications XIV, Munich,
   GERMANY, JUN 26-27, 2017}},
Organization = {{SPIE}},
Abstract = {{Terrestrial lidar is commonly used for detailed documentation in the
   field of forest inventory investigation. Recent improvements of point
   cloud processing techniques enabled efficient and precise computation of
   an individual tree shape parameters, such as breast-height diameter,
   height, and volume. However, tree species are manually specified by
   skilled workers to date. Previous works for automatic tree species
   classification mainly focused on aerial or satellite images, and few
   works have been reported for classification techniques using
   ground-based sensor data. Several candidate sensors can be considered
   for classification, such as RGB or multi/hyper spectral cameras. Above
   all candidates, we use terrestrial lidar because it can obtain high
   resolution point cloud in the dark forest. We selected bark texture for
   the classification criteria, since they clearly represent unique
   characteristics of each tree and do not change their appearance under
   seasonable variation and aged deterioration. In this paper, we propose a
   new method for automatic individual tree species classification based on
   terrestrial lidar using Convolutional Neural Network (CNN). The key
   component is the creation step of a depth image which well describe the
   characteristics of each species from a point cloud. We focus on Japanese
   cedar and cypress which cover the large part of domestic forest. Our
   experimental results demonstrate the effectiveness of our proposed
   method.}},
DOI = {{10.1117/12.2270123}},
Article-Number = {{UNSP 103320O}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1110-8; 978-1-5106-1109-2}},
Unique-ID = {{ISI:000412830800019}},
}

@inproceedings{ ISI:000413068300005,
Author = {Prathusha, Sai S. and Suja, P. and Tripathi, Shikha and Louis, R.},
Editor = {{Basu, A and Das, S and Horain, P and Bhattacharya, S}},
Title = {{Emotion Recognition from Facial Expressions of 4D Videos Using Curves
   and Surface Normals}},
Booktitle = {{INTELLIGENT HUMAN COMPUTER INTERACTION, IHCI 2016}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10127}},
Pages = {{51-64}},
Note = {{8th International Conference on Intelligent Human Computer Interaction
   (IHCI), Pilani, INDIA, DEC 12-13, 2016}},
Organization = {{Council Sci \& Ind Res, Cent Elect Engn Res Inst Pilani; Birla Inst
   Technol \& Sci Pilani; Indian Inst Informat Technol}},
Abstract = {{In this paper, we propose and compare three methods for recognizing
   emotions from facial expressions using 4D videos. In the first two
   methods, the 3D faces are re-sampled by using curves to extract the
   feature information. Two different methods are presented to resample the
   faces in an intelligent way using parallel curves and radial curves. The
   movement of the face is measured through these curves using two frames:
   neutral and peak frame. The deformation matrix is formed by computing
   the distance point to point on the corresponding curves of the neutral
   frame and peak frame. This matrix is used to create the feature vector
   that will be used for classification using Support Vector Machine (SVM).
   The third method proposed is to extract the feature information from the
   face by using surface normals. At every point on the frame, surface
   normals are extracted. The deformation matrix is formed by computing the
   Euclidean distances between the corresponding normals at a point on
   neutral and peak frames. This matrix is used to create the feature
   vector that will be used for classification of emotions using SVM. The
   proposed methods are analyzed and they showed improvement over existing
   literature.}},
DOI = {{10.1007/978-3-319-52503-7\_5}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-52503-7; 978-3-319-52502-0}},
Unique-ID = {{ISI:000413068300005}},
}

@inproceedings{ ISI:000408273100011,
Author = {Rajeev, Srijith and Kamath, Shreyas K. M. and Panetta, Karen and Agaian,
   Sos},
Book-Group-Author = {{IEEE}},
Title = {{3-D Palmprint Modeling for Biometric Verification}},
Booktitle = {{2017 IEEE INTERNATIONAL SYMPOSIUM ON TECHNOLOGIES FOR HOMELAND SECURITY
   (HST)}},
Year = {{2017}},
Note = {{IEEE International Symposium on Technologies for Homeland Security
   (HST), Waltham, MA, APR 25-26, 2017}},
Organization = {{IEEE}},
Abstract = {{Palmprint is a very unique and distinctive biometric trait because of
   features such as a person's inimitable principal lines, wrinkles, delta
   points, and minutiae. These constitute the main reasons why palmprint
   verification is considered as one of the most reliable personal
   identification methods. However, a clear majority of the research on
   palm-prints are concentrated on 2-D palmprint images irrespective of the
   fact that the human palm is a 3D-surface. While 2-D palmprint
   recognition has proved to be efficient in terms of verification rate, it
   has some essential downsides. These restrictions can adversely affect
   the performance and robustness of the palmprint recognition system. One
   of the possible solutions to resolve the limitations associated with 2-D
   palm print authentication systems is (i) to use a 3-D scanning system
   and to produce high quality 3-D images with depth information; (ii) to
   map 3-D palm-print images into 2-D images which may support the usage of
   3-D images with both biometric palmprint 2-D image databases and 2-D
   palmprint recognition tools. The bloom of 3-D technologies has made it
   easier to capture and store 3-D images. The problem of a direct mapping
   approach is that a large section of the palm is hard-pressed on the
   scanner surface during 2-D based acquisition. This paper proposes a
   novel technique to unravel/map 3-D palm images to its equivalent 2-D
   palm-print image. This image can be then used to perform efficient and
   accurate 2-D identification/verification. Experimental results and
   discussions will also be presented.}},
ISBN = {{978-1-5090-6356-7}},
Unique-ID = {{ISI:000408273100011}},
}

@inproceedings{ ISI:000407106200038,
Author = {Fraser, Alex and Dallaire, Michael and Godmaire, Xavier P.},
Editor = {{Ratvik, AP}},
Title = {{Laser Marking and 3D Imaging of Aluminum Products}},
Booktitle = {{LIGHT METALS 2017}},
Series = {{Minerals Metals \& Materials Series}},
Year = {{2017}},
Pages = {{289-292}},
Note = {{146th TMS Annual Meeting and Exhibition / Conference on Light Metals,
   San Diego, CA, FEB 26-MAR 02, 2017}},
Organization = {{Minerals Metals \& Mat Soc}},
Abstract = {{Most industrial products have (challenging) 3D shapes, many of them
   require traceability and individual marking. Although some laser marking
   systems on the market have 3D capabilities, they require the 3D shape to
   be loaded in the laser controller and the part to be precisely located.
   However, many industrial processes requiring direct part identification
   cannot fulfill those precise positioning requirements. To overcome these
   limitations, a 3D laser marker with integrated 3D imaging system was
   developed. This imaging system obtains the 3D image of the piece, and
   then the laser controller starts the marking process so that the focus
   fits on the part surface. The whole 3D data acquisition and transfer
   takes less than 3 s. This solves the problem of part positioning and
   simplifies the integration, while also providing 3D data of the surface
   that can be used for quality control.}},
DOI = {{10.1007/978-3-319-51541-0\_38}},
ISSN = {{2367-1181}},
ISBN = {{978-3-319-51541-0; 978-3-319-51540-3}},
Unique-ID = {{ISI:000407106200038}},
}

@inproceedings{ ISI:000406996500085,
Author = {Carraro, Marco and Munaro, Matteo and Roitberg, Alina and Menegatti,
   Emanuele},
Editor = {{Chen, W and Hosoda, K and Menegatti, E and Shimizu, M and Wang, H}},
Title = {{Improved Skeleton Estimation by Means of Depth Data Fusion from Multiple
   Depth Cameras}},
Booktitle = {{INTELLIGENT AUTONOMOUS SYSTEMS 14}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{531}},
Pages = {{1155-1167}},
Note = {{14th International Conference on Intelligent Autonomous Systems (IAS),
   Shanghai Jiao Tong Univ, Shanghai, PEOPLES R CHINA, JUL 03-07, 2016}},
Abstract = {{In this work, we address the problem of human skeleton estimation when
   multiple depth cameras are available. We propose a system that takes
   advantage of the knowledge of the camera poses to create a collaborative
   virtual depth image of the person in the scene which consists of points
   from all the cameras and that represents the person in a frontal pose.
   This depth image is fed as input to the open-source body part detector
   in the Point Cloud Library. A further contribution of this work is the
   improvement of this detector obtained by introducing two new components:
   as a pre-processing, a people detector is applied to remove the
   background from the depth map before estimating the skeleton, while an
   alpha-beta tracking is added as a post-processing step for filtering the
   obtained joint positions over time. The overall system has been proven
   to effectively improve the skeleton estimation on two sequences of
   people in different poses acquired from two first-generation Microsoft
   Kinect.}},
DOI = {{10.1007/978-3-319-48036-7\_85}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-48036-7; 978-3-319-48035-0}},
Unique-ID = {{ISI:000406996500085}},
}

@inproceedings{ ISI:000406534300014,
Author = {McIver, Charles A. and Metcalf, Jeremy P. and Olsen, Richard C.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Spectral LiDAR Analysis for Terrain Classification}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10191}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Anaheim, CA,
   APR 11-12, 2017}},
Organization = {{SPIE}},
Abstract = {{Data from the Optech Titan airborne laser scanner were collected over
   Monterey, CA, in three wavelengths (532 nm, 1064 nm, and 1550 nm), in
   May 2016, by the National Center for Airborne LiDAR Mapping (NCALM).
   Analysis techniques have been developed using spectral technology
   largely derived from the analysis of spectral imagery. Data are analyzed
   as individual points, vs techniques that emphasize spatial binning. The
   primary tool which allows for this exploitation is the N-Dimensional
   Visualizer contained in the ENVI software package. The results allow for
   significant improvement in classification accuracy compared to results
   obtained from techniques derived from standard LiDAR analysis tools.}},
DOI = {{10.1117/12.2276658}},
Article-Number = {{UNSP 101910J}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0883-2; 978-1-5106-0884-9}},
ResearcherID-Numbers = {{Olsen, Richard C/O-2699-2015}},
ORCID-Numbers = {{Olsen, Richard C/0000-0002-8344-9297}},
Unique-ID = {{ISI:000406534300014}},
}

@inproceedings{ ISI:000405560700088,
Author = {Manit, Jirapong and Bremer, Christina and Schweikard, Achim and Ernst,
   Floris},
Editor = {{Webster, RJ and Fei, B}},
Title = {{Patient identification using a near-infrared lasers canner}},
Booktitle = {{MEDICAL IMAGING 2017: IMAGE-GUIDED PROCEDURES, ROBOTIC INTERVENTIONS,
   AND MODELING}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10135}},
Note = {{Conference on Medical Imaging - Image-Guided Procedures, Robotic
   Interventions, and Modeling, Orlando, FL, FEB 14-16, 2017}},
Organization = {{SPIE; Alpin Med Syst; Siemens Healthineers; No Digital Inc}},
Abstract = {{We propose a new biometric approach where the tissue thickness of a
   person's forehead is used as a biometric feature. Given that the spatial
   registration of two 3D laser scans of the same human face usually
   produces a low error value, the principle of point cloud registration
   and its error metric can be applied to human classification techniques.
   However, by only considering the spatial error, it is not possible to
   reliably verify a person's identity. We propose to use a novel
   near-infrared laser-based head tracking system to determine an
   additional feature, the tissue thickness, and include this in the error
   metric. Using MRI as a ground truth, data from the foreheads of 30
   subjects was collected from which a 4D reference point cloud was created
   for each subject. The measurements from the near-infrared system were
   registered with all reference point clouds using the ICP algorithm.
   Afterwards, the spatial and tissue thickness errors were extracted,
   forming a 2D feature space. For all subjects, the lowest feature
   distance resulted from the registration of a measurement and the
   reference point cloud of the same person.
   The combined registration error features yielded two clusters in the
   feature space, one from the same subject and another from the other
   subjects. When only the tissue thickness error was considered, these
   clusters were less distinct but still present. These findings could help
   to raise safety standards for head and neck cancer patients and lays the
   foundation for a future human identification technique.}},
DOI = {{10.1117/12.2254963}},
Article-Number = {{UNSP 101352L}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0715-6; 978-1-5106-0716-3}},
Unique-ID = {{ISI:000405560700088}},
}

@inproceedings{ ISI:000402657200006,
Author = {Bobulski, Janusz},
Editor = {{Choras, RS}},
Title = {{Face Recognition with 3D Face Asymmetry}},
Booktitle = {{IMAGE PROCESSING AND COMMUNICATIONS CHALLENGES 8}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{525}},
Pages = {{53-60}},
Note = {{8th International Conference on Image Processing and Communications
   (IP\&C), UTP Univ Technol \& Sci, Inst Telecommunicat \& Comp Sci,
   Bydgoszcz, POLAND, SEP 07-09, 2016}},
Organization = {{UTP Univ Technol \& Sci}},
Abstract = {{Using of 3D images for the identification was in a field of the interest
   of many researchers which developed a few methods offering good results.
   However, there are few techniques exploiting the 3D asymmetry amongst
   these methods. We propose fast algorithm for rough extraction face
   asymmetry that is used to 3D face recognition with hidden Markov models.
   This paper presents conception of fast method for determine 3D face
   asymmetry. The research results indicate that face recognition with 3D
   face asymmetry may be used in biometrics systems.}},
DOI = {{10.1007/978-3-319-47274-4\_6}},
ISSN = {{2194-5357}},
EISSN = {{2194-5365}},
ISBN = {{978-3-319-47274-4; 978-3-319-47273-7}},
ResearcherID-Numbers = {{Bobulski, Janusz/C-4998-2014}},
ORCID-Numbers = {{Bobulski, Janusz/0000-0003-3345-604X}},
Unique-ID = {{ISI:000402657200006}},
}

@article{ ISI:000391527900002,
Author = {Wang, Yuanyuan and Zhu, Xiao Xiang and Zeisl, Bernhard and Pollefeys,
   Marc},
Title = {{Fusing Meter-Resolution 4-D InSAR Point Clouds and Optical Images for
   Semantic Urban Infrastructure Monitoring}},
Journal = {{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING}},
Year = {{2017}},
Volume = {{55}},
Number = {{1}},
Pages = {{14-26}},
Month = {{JAN}},
Abstract = {{Using synthetic aperture radar (SAR) interferometry to monitor long-term
   millimeter-level deformation of urban infrastructures, such as
   individual buildings and bridges, is an emerging and important field in
   remote sensing. In the state-of-the-art methods, deformation parameters
   are retrieved and monitored on a pixel basis solely in the SAR image
   domain. However, the inevitable side-looking imaging geometry of SAR
   results in undesired occlusion and layover in urban area, rendering the
   current method less competent for a semantic-level monitoring of
   different urban infrastructures. This paper presents a framework of a
   semantic-level deformation monitoring by linking the precise deformation
   estimates of SAR interferometry and the semantic classification labels
   of optical images via a 3-D geometric fusion and semantic texturing. The
   proposed approach provides the first ``SARptical{''} point cloud of an
   urban area, which is the SAR tomography point cloud textured with
   attributes from optical images. This opens a new perspective of InSAR
   deformation monitoring. Interesting examples on bridge and railway
   monitoring are demonstrated.}},
DOI = {{10.1109/TGRS.2016.2554563}},
ISSN = {{0196-2892}},
EISSN = {{1558-0644}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000391527900002}},
}

@article{ ISI:000397373000001,
Author = {Jin, Hai and Wang, Xun and Zhong, Zichun and Hua, Jing},
Title = {{Robust 3D face modeling and reconstruction from frontal and side images}},
Journal = {{COMPUTER AIDED GEOMETRIC DESIGN}},
Year = {{2017}},
Volume = {{50}},
Pages = {{1-13}},
Month = {{JAN}},
Abstract = {{Robust and effective capture and reconstruction of 3D face models
   directly by smartphone users enables many applications. This paper
   presents a novel 3D face modeling and reconstruction solution that
   robustly and accurately acquire 3D face models from a couple of images
   captured by a single smartphone camera. Two selfie photos of a subject
   taken from the front and side are first used to guide our Non-Negative
   Matrix Factorization (NMF) induced part-based face model to iteratively
   reconstruct an initial 3D face of the subject. Then, an iterative detail
   updating method is applied to the initial generated 3D face to
   reconstruct facial details through optimizing lighting parameters and
   local depths. Our iterative 3D face reconstruction method permits fully
   automatic registration of a part based face representation to the
   acquired face data and the detailed 2D/3D features to build a
   high-quality 3D face model. The NMF part-based face representation
   learned from a 3D face database facilitates effective global and
   adaptive local detail data fitting alternatively. Our system is flexible
   and it allows users to conduct the capture in any uncontrolled
   environment. We demonstrate the capability of our method by allowing
   users to capture and reconstruct their 3D faces by themselves. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.cagd.2016.11.001}},
ISSN = {{0167-8396}},
EISSN = {{1879-2332}},
ResearcherID-Numbers = {{Smith, Nash/R-3104-2017}},
ORCID-Numbers = {{Smith, Nash/0000-0002-4406-0043}},
Unique-ID = {{ISI:000397373000001}},
}

@article{ ISI:000401423700003,
Author = {Kramer, Heather A. and Collins, Brandon M. and Gallagher, Claire V. and
   Keane, John J. and Stephens, Scott L. and Kelly, Maggi},
Title = {{Accessible light detection and ranging: estimating large tree density
   for habitat identification}},
Journal = {{ECOSPHERE}},
Year = {{2016}},
Volume = {{7}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Large trees are important to a wide variety of wildlife, including many
   species of conservation concern, such as the California spotted owl
   (Strix occidentalis occidentalis). Light detection and ranging (LiDAR)
   has been successfully utilized to identify the density of large-diameter
   trees, either by segmenting the LiDAR point cloud into individual trees,
   or by building regression models between variables extracted from the
   LiDAR point cloud and field data. Neither of these methods is easily
   accessible for most land managers due to the reliance on specialized
   software, and much available LiDAR data are being underutilized due to
   the steep learning curve required for advanced processing using these
   programs. This study derived a simple, yet effective method for
   estimating the density of large-stemmed trees from the LiDAR canopy
   height model, a standard raster product derived from the LiDAR point
   cloud that is often delivered with the LiDAR and is easy to process by
   personnel trained in geographic information systems (GIS). Ground plots
   needed to be large (1 ha) to build a robust model, but the spatial
   accuracy of plot center was less crucial to model accuracy. We also
   showed that predicted large tree density is positively linked to
   California spotted owl nest sites.}},
DOI = {{10.1002/ecs2.1593}},
Article-Number = {{e01593}},
ISSN = {{2150-8925}},
Unique-ID = {{ISI:000401423700003}},
}

@article{ ISI:000396382500043,
Author = {Magnard, Christophe and Morsdorf, Felix and Small, David and Stilla, Uwe
   and Schaepman, Michael E. and Meier, Erich},
Title = {{Single tree identification using airborne multibaseline SAR
   interferometry data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2016}},
Volume = {{186}},
Pages = {{567-580}},
Month = {{DEC 1}},
Abstract = {{Remote sensing data allow large scale observation of forested
   ecosystems. Forest assessment benefits from information about individual
   trees. Multibaseline SAR interferometry (InSAR) is able to generate
   dense point clouds of forest canopies, similar to airborne laser
   scanning (ALS). This type of point cloud was generated using data from
   the Ka-band MEMPHIS system, acquired over a mainly coniferous forest
   near Vordemwald in the Swiss Midlands. This point cloud was segmented
   using an advanced clustering technique to detect individual trees and
   derive their positions, heights, and crown diameters. To evaluate the
   InSAR point cloud properties and limitations, it was compared to
   products derived from ALS and stereo-photogrammetry. All point clouds
   showed similar geolocation accuracies with 02-0.3 m relative shifts.
   Both InSAR and photogrammetry techniques yielded points predominantly
   located in the upper levels of the forest vegetation, while ALS provided
   points from the top of the canopy down to the understory and forest
   floor. The canopy height models agreed very well with each other, with
   R-2 values between 0.84 and 0.89. The detected trees and their estimated
   physical and structural parameters were validated by comparing them to
   reference forestry data. A detection rate of similar to 90\% was
   achieved for larger trees, corresponding to half of the reference trees.
   The smaller trees were detected with a success rate of similar to 50\%.
   The tree height was slightly underestimated, with a R-2 value of 0.63.
   The estimated crown diameter agreed on an average sense, however with a
   relatively low R-2 value of 0.19. Very high success rates (>90\%) were
   obtained when matching the trees detected from the InSAR-data with those
   detected from the ALS- and photogrammetry-data. There, InSAR tree
   heights were in the mean 1-1.5 m lower, with high R-2 values ranging
   between 0.8 and 0.9. Our results demonstrate the use of millimeter wave
   SAR interferometry data as an alternative to ALS- and
   photogrammetry-based data for forest monitoring. (C) 2016 Elsevier Inc.
   All rights reserved.}},
DOI = {{10.1016/j.rse.2016.09.018}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Schaepman, Michael/B-9213-2009
   }},
ORCID-Numbers = {{Schaepman, Michael/0000-0002-9627-9565
   Magnard, Christophe/0000-0002-1473-8650}},
Unique-ID = {{ISI:000396382500043}},
}

@article{ ISI:000391303000155,
Author = {Rose, Johann Christian and Kicherer, Anna and Wieland, Markus and
   Klingbeil, Lasse and Toepfer, Reinhard and Kuhlmann, Heiner},
Title = {{Towards Automated Large-Scale 3D Phenotyping of Vineyards under Field
   Conditions}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{In viticulture, phenotypic data are traditionally collected directly in
   the field via visual and manual means by an experienced person. This
   approach is time consuming, subjective and prone to human errors. In
   recent years, research therefore has focused strongly on developing
   automated and non-invasive sensor-based methods to increase data
   acquisition speed, enhance measurement accuracy and objectivity and to
   reduce labor costs. While many 2D methods based on image processing have
   been proposed for field phenotyping, only a few 3D solutions are found
   in the literature. A track-driven vehicle consisting of a camera system,
   a real-time-kinematic GPS system for positioning, as well as hardware
   for vehicle control, image storage and acquisition is used to visually
   capture a whole vine row canopy with georeferenced RGB images. In the
   first post-processing step, these images were used within a
   multi-view-stereo software to reconstruct a textured 3D point cloud of
   the whole grapevine row. A classification algorithm is then used in the
   second step to automatically classify the raw point cloud data into the
   semantic plant components, grape bunches and canopy. In the third step,
   phenotypic data for the semantic objects is gathered using the
   classification results obtaining the quantity of grape bunches, berries
   and the berry diameter.}},
DOI = {{10.3390/s16122136}},
Article-Number = {{2136}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000391303000155}},
}

@article{ ISI:000386741300011,
Author = {Li, Billy Y. L. and Xue, Mingliang and Mian, Ajmal and Liu, Wanquan and
   Krishna, Aneesh},
Title = {{Robust RGB-D face recognition using Kinect sensor}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{214}},
Pages = {{93-108}},
Month = {{NOV 19}},
Abstract = {{In this paper we propose a robust face recognition algorithm for low
   resolution RGB-D Kinect data. Many techniques are proposed for image
   preprocessing due to the noisy depth data. First, facial symmetry is
   exploited based on the 3D point cloud to obtain a canonical frontal view
   image irrespective of the initial pose and then depth data is converted
   to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are
   then used to project RGB to DCS (Discriminant Color Space) and normal
   maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust
   Sparse Coding method is proposed that codes the multiple channels (DCS
   or DNM) of a test image as a sparse combination of training samples with
   different pixel weighting. Weights are calculated dynamically in an
   iterative process to achieve robustness against variations in pose,
   illumination, facial expressions and disguise. In contrast to existing
   techniques, our multi-channel approach is more robust to variations.
   Reconstruction errors of the test image (DCS and DNM) are normalized and
   fused to decide its identity. The proposed algorithm is evaluated on
   four public databases. It achieves 98.4\% identification rate on
   CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects.
   Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2
   databases, the proposed algorithm achieves 97.6\%, 95.6\% and 95.2\%
   identification rates respectively. To the best of our knowledge, these
   are the highest identification rates reported so far for the first three
   databases. (C) 2016 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2016.06.012}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
ORCID-Numbers = {{Krishna, Aneesh/0000-0001-8637-5732
   liu, wanquan/0000-0003-4910-353X}},
Unique-ID = {{ISI:000386741300011}},
}

@article{ ISI:000386910000105,
Author = {Nakamura, Tomoya and Matsumoto, Jumpei and Nishimaru, Hiroshi and
   Bretas, Rafael Vieira and Takamura, Yusaku and Hori, Etsuro and Ono,
   Taketoshi and Nishijo, Hisao},
Title = {{A Markerless 3D Computerized Motion Capture System Incorporating a
   Skeleton Model for Monkeys}},
Journal = {{PLOS ONE}},
Year = {{2016}},
Volume = {{11}},
Number = {{11}},
Month = {{NOV 3}},
Abstract = {{In this study, we propose a novel markerless motion capture system (MCS)
   for monkeys, in which 3D surface images of monkeys were reconstructed by
   integrating data from four depth cameras, and a skeleton model of the
   monkey was fitted onto 3D images of monkeys in each frame of the video.
   To validate the MCS, first, estimated 3D positions of body parts were
   compared between the 3D MCS-assisted estimation and manual estimation
   based on visual inspection when a monkey performed a shuttling behavior
   in which it had to avoid obstacles in various positions. The mean
   estimation error of the positions of body parts (3-14 cm) and of head
   rotation (35-43 degrees) between the 3D MCS-assisted and manual
   estimation were comparable to the errors between two different
   experimenters performing manual estimation. Furthermore, the MCS could
   identify specific monkey actions, and there was no false positive nor
   false negative detection of actions compared with those in manual
   estimation. Second, to check the reproducibility of MCS-assisted
   estimation, the same analyses of the above experiments were repeated by
   a different user. The estimation errors of positions of most body parts
   between the two experimenters were significantly smaller in the
   MCS-assisted estimation than in the manual estimation. Third, effects of
   methamphetamine (MAP) administration on the spontaneous behaviors of
   four monkeys were analyzed using the MCS. MAP significantly increased
   head movements, tended to decrease locomotion speed, and had no
   significant effect on total path length. The results were comparable to
   previous human clinical data. Furthermore, estimated data following MAP
   injection (total path length, walking speed, and speed of head rotation)
   correlated significantly between the two experimenters in the
   MCS-assisted estimation (r = 0.863 to 0.999). The results suggest that
   the presented MCS in monkeys is useful in investigating neural
   mechanisms underlying various psychiatric disorders and developing
   pharmacological interventions.}},
DOI = {{10.1371/journal.pone.0166154}},
Article-Number = {{e0166154}},
ISSN = {{1932-6203}},
ORCID-Numbers = {{Matsumoto, Jumpei/0000-0003-4729-2816}},
Unique-ID = {{ISI:000386910000105}},
}

@article{ ISI:000386995100031,
Author = {Wolff, Antje and Gotz, Yvonne},
Title = {{4D phenotyping of germinating seeds and seedlings as a tool to
   objectively measure seed quality and improve field establishment and
   yield of sugar beets}},
Journal = {{INTERNATIONAL SUGAR JOURNAL}},
Year = {{2016}},
Volume = {{118}},
Number = {{1415}},
Pages = {{836-839}},
Month = {{NOV}},
Abstract = {{The plant breeding company Strube, in cooperation with the German
   Fraunhofer Institute for non-destructive testing, has developed an
   automated high-throughput germination test for sugar beet seeds. The
   phenoTest permits objective measurement and classification of
   germinating seeds and resulting seedlings. It is therefore more accurate
   and provides more information than the conventional ISTA (International
   Seed Testing Association)-germination test, which relies purely on
   visual assessment and classification into the categories ``normal{''} or
   ``abnormal{''}. This differentiation is difficult to standardise, and
   is, to a significant degree, subjective. The phenoTest is based on
   three-dimensional (3D) X-ray images. Repeated tests of the same plants
   enable an objective assessment of seedling development over the course
   of time (4D phenotyping). The individual organs of each plant (radicle,
   hypocotyl and cotyledons) are automatically identified and measured. The
   method provides detailed information on germinating capacity and vigour,
   as well as the homogeneity of a seed lot. Results are documented as
   measurement values and 3D-images of each individual plant at different
   time points. The data is used to compare seed lots concerning their
   natural germination capacity and especially vigour, the processing or
   priming technologies they experienced, the pelleting and seed treatment
   applied etc. in order to predict their field emergence potential even
   under difficult growing conditions. These analyses also helps to
   optimise all these processes in commercial seed production to obtain a
   quick, homogeneous and complete field emergence, making full use of the
   genetic yield potential of sugar beet.}},
ISSN = {{0020-8841}},
Unique-ID = {{ISI:000386995100031}},
}

@article{ ISI:000386874900020,
Author = {Guo, Yulan and Lei, Yinjie and Liu, Li and Wang, Yan and Bennamoun,
   Mohammed and Sohelf, Ferdous},
Title = {{EI3D: Expression-invariant 3D face recognition based on feature and
   shape matching}},
Journal = {{PATTERN RECOGNITION LETTERS}},
Year = {{2016}},
Volume = {{83}},
Number = {{3}},
Pages = {{403-412}},
Month = {{NOV 1}},
Abstract = {{This paper presents a local feature based shape matching algorithm for
   expression-invariant 3D face recognition. Each 3D face is first
   automatically detected from a raw 3D data and normalized to achieve pose
   invariance. The 3D face is then represented by a set of keypoints and
   their associated local feature descriptors to achieve robustness to
   expression variations. During face recognition, a probe face is compared
   against each gallery face using both local feature matching and 3D point
   cloud registration. The number of feature matches, the average distance
   of matched features, and the number of closest point pairs after
   registration are used to measure the similarity between two 3D faces.
   These similarity metrics are then fused to obtain the final results. The
   proposed algorithm has been tested on the FRGC v2 benchmark and a high
   recognition performance has been achieved. It obtained the
   state-of-the-art results by achieving an overall rank- 1 identification
   rate of 97.0\% and an average verification rate of 99.01\% at 0.001
   false acceptance rate for all faces with neutral and non-neutral
   expressions. Further, the robustness of our algorithm under different
   occlusions has been demonstrated on the Bosphorus dataset. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.patrec.2016.04.003}},
ISSN = {{0167-8655}},
EISSN = {{1872-7344}},
ResearcherID-Numbers = {{Sohel, Ferdous/C-2428-2013
   }},
ORCID-Numbers = {{Sohel, Ferdous/0000-0003-1557-4907
   Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000386874900020}},
}

@article{ ISI:000387670700012,
Author = {Dalponte, Michele and Coomes, David A.},
Title = {{Tree-centric mapping of forest carbon density from airborne laser
   scanning and hyperspectral data}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2016}},
Volume = {{7}},
Number = {{10}},
Pages = {{1236-1245}},
Month = {{OCT}},
Abstract = {{1. Forests are a major component of the global carbon cycle, and
   accurate estimation of forest carbon stocks and fluxes is important in
   the context of anthropogenic global change. Airborne laser scanning
   (ALS) data sets are increasingly recognized as outstanding data sources
   for high-fidelity mapping of carbon stocks at regional scales.
   2. We develop a tree-centric approach to carbon mapping, based on
   identifying individual tree crowns (ITCs) and species from airborne
   remote sensing data, from which individual tree carbon stocks are
   calculated. We identify ITCs from the laser scanning point cloud using a
   region-growing algorithm and identifying species from airborne
   hyperspectral data by machine learning. For each detected tree, we
   predict stem diameter from its height and crown-width estimate. From
   that point on, we use well-established approaches developed for
   field-based inventories: above-ground biomasses of trees are estimated
   using published allometries and summed within plots to estimate carbon
   density.
   3. We show this approach is highly reliable: tests in the Italian Alps
   demonstrated a close relationship between field-and ALS-based estimates
   of carbon stocks (r(2) = 0.98). Small trees are invisible from the air,
   and a correction factor is required to accommodate this effect.
   4. An advantage of the tree-centric approach over existing area-based
   methods is that it can produce maps at any scale and is fundamentally
   based on field-based inventory methods, making it intuitive and
   transparent. Airborne laser scanning, hyperspectral sensing and
   computational power are all advancing rapidly, making it increasingly
   feasible to use ITC approaches for effective mapping of forest carbon
   density also inside wider carbon mapping programs like REDD++.}},
DOI = {{10.1111/2041-210X.12575}},
ISSN = {{2041-210X}},
EISSN = {{2041-2096}},
ResearcherID-Numbers = {{Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000387670700012}},
}

@article{ ISI:000385597700004,
Author = {Li, Lin and Li, Dalin and Zhu, Haihong and Li, You},
Title = {{A dual growing method for the automatic extraction of individual trees
   from mobile laser scanning data}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{120}},
Pages = {{37-52}},
Month = {{OCT}},
Abstract = {{Street trees interlaced with other objects in cluttered point clouds of
   urban scenes inhibit the automatic extraction of individual trees. This
   paper proposes a method for the automatic extraction of individual trees
   from mobile laser scanning data, according to the general constitution
   of trees. Two components of each individual tree - a trunk and a crown
   can be extracted by the dual growing method. This method consists of
   coarse classification, through which most of artifacts are removed; the
   automatic selection of appropriate seeds for individual trees, by which
   the common manual initial setting is avoided; a dual growing process
   that separates one tree from others by circumscribing a trunk in an
   adaptive growing radius and segmenting a crown in constrained growing
   regions; and a refining process that draws a singular trunk from the
   interlaced other objects. The method is verified by two datasets with
   over 98\% completeness and over 96\% correctness. The low mean absolute
   percentage errors in capturing the morphological parameters of
   individual trees indicate that this method can output individual trees
   with high precision. (C) 2016 International Society for Photogrammetry
   and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.07.009}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
Unique-ID = {{ISI:000385597700004}},
}

@article{ ISI:000388855500007,
Author = {Ximena Bastidas-Rodriguez, Maria and Prieto-Ortiz, Flavio A. and
   Espejo-Mora, Edgar},
Title = {{Fractographic classification in metallic materials by using 3D
   processing and computer vision techniques}},
Journal = {{REVISTA FACULTAD DE INGENIERIA, UNIVERSIDAD PEDAGOGICA Y TECNOLOGICA DE
   COLOMBIA}},
Year = {{2016}},
Volume = {{25}},
Number = {{43}},
Pages = {{83-96}},
Month = {{SEP-DEC}},
Abstract = {{Failure analysis aims at collecting information about how and why a
   failure is produced. The first step in this process is a visual
   inspection on the flaw surface that will reveal the features, marks, and
   texture, which characterize each type of fracture. This is generally
   carried out by personnel with no experience that usually lack the
   knowledge to do it. This paper proposes a classification method for
   three kinds of fractures in crystalline materials: brittle, fatigue, and
   ductile. The method uses 3D vision, and it is expected to support
   failure analysis. The features used in this work were: i) Haralick's
   features and ii) the fractal dimension. These features were applied to
   3D images obtained from a confocal laser scanning microscopy Zeiss LSM
   700. For the classification, we evaluated two classifiers: Artificial
   Neural Networks and Support Vector Machine. The performance evaluation
   was made by extracting four marginal relations from the confusion
   matrix: accuracy, sensitivity, specificity, and precision, plus three
   evaluation methods: Receiver Operating Characteristic space, the
   Individual Classification Success Index, and the Jaccard's coefficient.
   Despite the classification percentage obtained by an expert is better
   than the one obtained with the algorithm, the algorithm achieves a
   classification percentage near or exceeding the 60 \% accuracy for the
   analyzed failure modes. The results presented here provide a good
   approach to address future research on texture analysis using 3D data.}},
DOI = {{10.19053/01211129.v25.n43.2016.5301}},
ISSN = {{0121-1129}},
EISSN = {{2357-5328}},
Unique-ID = {{ISI:000388855500007}},
}

@article{ ISI:000385488000039,
Author = {Cao, Lin and Gao, Sha and Li, Pinghao and Yun, Ting and Shen, Xin and
   Ruan, Honghua},
Title = {{Aboveground Biomass Estimation of Individual Trees in a Coastal Planted
   Forest Using Full-Waveform Airborne Laser Scanning Data}},
Journal = {{REMOTE SENSING}},
Year = {{2016}},
Volume = {{8}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{The accurate estimation of individual tree level aboveground biomass
   (AGB) is critical for understanding the carbon cycle, detecting
   potential biofuels and managing forest ecosystems. In this study, we
   assessed the capability of the metrics of point clouds, extracted from
   the full-waveform Airborne Laser Scanning (ALS) data, and of composite
   waveforms, calculated based on a voxel-based approach, for estimating
   tree level AGB individually and in combination, over a planted forest in
   the coastal region of east China. To do so, we investigated the
   importance of point cloud and waveform metrics for estimating tree-level
   AGB by all subsets models and relative weight indices. We also assessed
   the capability of the point cloud and waveform metrics based models and
   combo model (including the combination of both point cloud and waveform
   metrics) for tree-level AGB estimation and evaluated the accuracies of
   these models. The results demonstrated that most of the waveform metrics
   have relatively low correlation coefficients (<0.60) with other metrics.
   The combo models (Adjusted R-2 = 0.78-0.89), including both point cloud
   and waveform metrics, have a relatively higher performance than the
   models fitted by point cloud metrics-only (Adjusted R-2 = 0.74-0.86) and
   waveform metrics-only (Adjusted R-2 = 0.72-0.84), with the mostly
   selected metrics of the 95th percentile height (H-95), mean of height of
   median energy (HOME) and mean of the height/median ratio (HTMR). Based
   on the relative weights (i.e., the percentage of contribution for R-2)
   of the mostly selected metrics for all subsets, the metric of 95th
   percentile height (H-95) has the highest relative importance for AGB
   estimation (19.23\%), followed by 75th percentile height (H-75)
   (18.02\%) and coefficient of variation of heights (H-cv) (15.18\%) in
   the point cloud metrics based models. For the waveform metrics based
   models, the metric of mean of height of median energy (HOME) has the
   highest relative importance for AGB estimation (17.86\%), followed by
   mean of the height/median ratio (HTMR) (16.23\%) and standard deviation
   of height of median energy (HOME sigma) (14.78\%). This study
   demonstrated benefits of using full-waveform ALS data for estimating
   biomass at tree level, for sustainable forest management and mitigating
   climate change by planted forest, as China has the largest area of
   planted forest in the world, and these forests contribute to a large
   amount of carbon sequestration in terrestrial ecosystems.}},
DOI = {{10.3390/rs8090729}},
Article-Number = {{729}},
ISSN = {{2072-4292}},
Unique-ID = {{ISI:000385488000039}},
}

@article{ ISI:000385150400002,
Author = {Qiu, Luwen and Zhou, Zhongwei and Guo, Jixiang and Lv, Jiancheng},
Title = {{An Automatic Registration Algorithm for 3D Maxillofacial Model}},
Journal = {{3D RESEARCH}},
Year = {{2016}},
Volume = {{7}},
Number = {{3}},
Month = {{SEP}},
Abstract = {{3D image registration aims at aligning two 3D data sets in a common
   coordinate system, which has been widely used in computer vision,
   pattern recognition and computer assisted surgery. One challenging
   problem in 3D registration is that point-wise correspondences between
   two point sets are often unknown apriori. In this work, we develop an
   automatic algorithm for 3D maxillofacial models registration including
   facial surface model and skull model. Our proposed registration
   algorithm can achieve a good alignment result between partial and whole
   maxillofacial model in spite of ambiguous matching, which has a
   potential application in the oral and maxillofacial reparative and
   reconstructive surgery. The proposed algorithm includes three steps: (1)
   3D-SIFT features extraction and FPFH descriptors construction; (2)
   feature matching using SAC-IA; (3) coarse rigid alignment and refinement
   by ICP. Experiments on facial surfaces and mandible skull models
   demonstrate the efficiency and robustness of our algorithm.}},
DOI = {{10.1007/s13319-016-0083-x}},
Article-Number = {{UNSP 20}},
ISSN = {{2092-6731}},
Unique-ID = {{ISI:000385150400002}},
}

@article{ ISI:000384777300027,
Author = {Yang, Bisheng and Huang, Ronggang and Dong, Zhen and Zang, Yufu and Li,
   Jianping},
Title = {{Two-step adaptive extraction method for ground points and breaklines
   from lidar point clouds}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{119}},
Pages = {{373-389}},
Month = {{SEP}},
Abstract = {{The extraction of ground points and breaklines is a crucial step during
   generation of high quality digital elevation models (DEMs) from airborne
   LiDAR point clouds. In this study, we propose a novel automated method
   for this task. To overcome the disadvantages of applying a single
   filtering method in areas with various types of terrain, the proposed
   method first classifies the points into a set of segments and one set of
   individual points, which are filtered by segment-based filtering and
   multi-scale morphological filtering, respectively. In the process of
   multi-scale morphological filtering, the proposed method removes
   amorphous objects from the set of individual points to decrease the
   effect of the maximum scale on the filtering result. The proposed method
   then extracts the breaklines from the ground points, which provide a
   good foundation for generation of a high quality DEM. Finally, the
   experimental results demonstrate that the proposed method extracts
   ground points in a robust manner while preserving the breaklines. (C)
   2016 International Society for Photogrammetry and Remote Sensing, Inc.
   (ISPRS). Published by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.07.002}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Yang, Bisheng/A-4642-2013}},
ORCID-Numbers = {{Yang, Bisheng/0000-0001-7736-0803}},
Unique-ID = {{ISI:000384777300027}},
}

@article{ ISI:000382679900012,
Author = {Bagchi, Parama and Bhattacharjee, Debotosh and Nasipuri, Mita},
Title = {{A robust analysis, detection and recognition of facial features in 2.5D
   images}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{75}},
Number = {{18}},
Pages = {{11059-11096}},
Month = {{SEP}},
Abstract = {{A robust technique for recognition of 3D faces which performs well with
   face images with various poses, expressions and occlusions. In this
   method, the face images represented in 3D mesh format are smoothed using
   trilinear interpolation and then converted to 2.5D image or range
   images. Nose-tip which is the most prominent feature on human face is
   detected first on the corner points selected by 3D Harris corner and
   curvedness at those corner points. K-Means clustering is applied to
   group those corner points in 2 groups. The cluster of points with larger
   curvedness values represents the possible locations of nose-tip.
   Nose-tip is finally localized using Mean-Gaussian curvature values of
   the prospective corner points in that cluster. Using the nose-tip
   location, other facial landmarks namely corners of the eyes and mouth
   are located and a facial graph is generated. The dimensionality of 2.5D
   feature space is that, depth values are stored at each (x, y) grid of
   the 2.5D image, so a 3D face image uses some function to map the depth
   value at any pixel position to the intensity with which that pixel will
   be displayed. Here finally extracted features for each subject is of
   dimensionality {[}1x21], taking into account the Euclidean distances in
   three dimensional form between each feature points detected
   automatically. Taking Euclidean distances between all pairs of landmark
   points as features, face images are classified using Multilayer
   Perceptron (MLP), as well as Support Vector Machines (SVM). Maximum
   recognition rates of 75 and 87.5 \% have been obtained in case of
   Bosphorus Databases, 62.5 and 87.5 \% in case of GavabDB databases, 75
   and 87.5 \% in case of Frav3D Databases by Multilayer Perceptron and
   Support Vector Machines respectively.}},
DOI = {{10.1007/s11042-015-2835-7}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000382679900012}},
}

@article{ ISI:000384777300010,
Author = {Mahmoudabadi, Hamid and Olsen, Michael J. and Todorovic, Sinisa},
Title = {{Efficient terrestrial laser scan segmentation exploiting data structure}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{119}},
Pages = {{135-150}},
Month = {{SEP}},
Abstract = {{New technologies such as lidar enable the rapid collection of massive
   datasets to model a 3D scene as a point cloud. However, while hardware
   technology continues to advance, processing 3D point clouds into
   informative models remains complex and time consuming. A common approach
   to increase processing efficiently is to segment the point cloud into
   smaller sections. This paper proposes a novel approach for point cloud
   segmentation using computer vision algorithms to analyze panoramic
   representations of individual laser scans. These panoramas can be
   quickly created using an inherent neighborhood structure that is
   established during the scanning process, which scans at fixed angular
   increments in a cylindrical or spherical coordinate system. In the
   proposed approach, a selected image segmentation algorithm is applied on
   several input layers exploiting this angular structure including laser
   intensity, range, normal vectors, and color information. These segments
   are then mapped back to the 3D point cloud so that modeling can be
   completed more efficiently. This approach does not depend on pre-defined
   mathematical models and consequently setting parameters for them. Unlike
   common geometrical point cloud segmentation methods, the proposed method
   employs the colorimetric and intensity data as another source of
   information. The proposed algorithm is demonstrated on several datasets
   encompassing variety of scenes and objects. Results show a very high
   perceptual (visual) level of segmentation and thereby the feasibility of
   the proposed algorithm. The proposed method is also more efficient
   compared to Random Sample Consensus (RANSAC), which is a common approach
   for point cloud segmentation. (C) 2016 International Society for
   Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2016.05.015}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000384777300010}},
}

@article{ ISI:000376708000002,
Author = {Alashkar, Taleb and Ben Amor, Boulbaba and Daoudi, Mohamed and Berretti,
   Stefano},
Title = {{A Grassmann framework for 4D facial shape analysis}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2016}},
Volume = {{57}},
Pages = {{21-30}},
Month = {{SEP}},
Abstract = {{In this paper, we investigate the contribution of dynamic evolution of
   3D faces to identity recognition. To this end, we adopt a subspace
   representation of the flow of curvature-maps computed on 3D facial
   frames of a sequence, after normalizing their pose. Such representation
   allows us to embody the shape as well as its temporal evolution within
   the same subspace representation. Dictionary learning and sparse coding
   over the space of fixed-dimensional subspaces, called Grassmann
   manifold, have been used to perform face recognition. We have conducted
   extensive experiments on the BU-4DFE dataset. The obtained results of
   the proposed approach provide promising results. (C) 2016 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.patcog.2016.03.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018
   Daoudi, Mohammed/H-5935-2013}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305
   Berretti, Stefano/0000-0003-1219-4386
   Daoudi, Mohammed/0000-0003-4219-7860}},
Unique-ID = {{ISI:000376708000002}},
}

@article{ ISI:000379266300013,
Author = {Quan, Wei and Matuszewski, Bogdan J. and Shark, Lik-Kwan},
Title = {{Statistical shape modelling for expression-invariant face analysis and
   recognition}},
Journal = {{PATTERN ANALYSIS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{19}},
Number = {{3}},
Pages = {{765-781}},
Month = {{AUG}},
Abstract = {{Paper introduces a 3-D shape representation scheme for automatic face
   analysis and identification, and demonstrates its invariance to facial
   expression. The core of this scheme lies on the combination of
   statistical shape modelling and non-rigid deformation matching. While
   the former matches 3-D faces with facial expression, the latter provides
   a low-dimensional feature vector that controls the deformation of model
   for matching the shape of new input, thereby enabling robust
   identification of 3-D faces. The proposed scheme is also able to handle
   the pose variation without large part of missing data. To assist the
   establishment of dense point correspondences, a modified
   free-form-deformation based on B-spline warping is applied with the help
   of extracted landmarks. The hybrid iterative closest point method is
   introduced for matching the models and new data. The feasibility and
   effectiveness of the proposed method was investigated using standard
   publicly available Gavab and BU-3DFE datasets, which contain faces with
   expression and pose changes. The performance of the system was compared
   with that of nine benchmark approaches. The experimental results
   demonstrate that the proposed scheme provides a competitive solution for
   face recognition.}},
DOI = {{10.1007/s10044-014-0439-x}},
ISSN = {{1433-7541}},
EISSN = {{1433-755X}},
Unique-ID = {{ISI:000379266300013}},
}

@article{ ISI:000381843600009,
Author = {Ekizoglu, Oguzhan and Hocaoglu, Elif and Inci, Ercan and Can, Ismail
   Ozgur and Solmaz, Dilek and Aksoy, Sema and Buran, Cudi Ferat and Sayin,
   Ibrahim},
Title = {{Assessment of sex in a modern Turkish population using cranial
   anthropometric parameters}},
Journal = {{LEGAL MEDICINE}},
Year = {{2016}},
Volume = {{21}},
Pages = {{45-52}},
Month = {{JUL}},
Abstract = {{The utilization of radiological imaging methods in anthropometric
   studies is being expanded by the application of modern imaging methods,
   leading to a decrease in costs, a decrease in the time required for
   analysis and the ability to create three-dimensional images. This
   retrospective study investigated 400 patients within the 18-45-years age
   group (mean age: 30.7 +/- 11.2 years) using cranial computed tomography
   images. We measured 14 anthropometric parameters (basion-bregma height,
   basion-prosthion length, maximum cranial length and cranial base
   lengths, maximum cranial breadth, bizygomatic diameter, upper facial
   breadth, bimastoid diameter, orbital breadth, orbital length, biorbital
   breadth, interorbital breadth, foramen magnum breadth and foramen magnum
   length) of cranial measurements. The intra- and inter-observer
   repeatability and consistency were good. From the results of logistic
   regression analysis using morphometric measurements, the most
   conspicuous measurements in terms of dimorphism were maximum cranial
   length, bizygomatic diameter, basion-bregma height, and cranial base
   length. The most dimorphic structure was the bizygomatic diameter with
   an accuracy rate of 83\% in females and 77\% in males. In this study,
   87.5\% of females and 87.0\% of males were classified accurately by this
   model including four parameters with a sensitivity of 91.5\% and
   specificity of 85.0\%. In conclusion, CT cranial morphometric analysis
   may be reliable for the assessment of sex in the Turkish population and
   is recommended for comparison of data of modern populations with those
   of former populations. Additionally, cranial morphometric data that we
   obtained from modern Turkish population may reveal population specific
   data, which may help current criminal investigations and identification
   of disaster victims. (C) 2016 Elsevier Ireland Ltd. All rights reserved.}},
DOI = {{10.1016/j.legalmed.2016.06.001}},
ISSN = {{1344-6223}},
ORCID-Numbers = {{Ekizoglu, Oguzhan/0000-0002-0194-595X
   Buran, Ferat/0000-0002-7858-0194}},
Unique-ID = {{ISI:000381843600009}},
}

@article{ ISI:000380771500016,
Author = {Tanhuanpaa, Topi and Saarinen, Ninni and Kankare, Ville and Nurminen,
   Kimmo and Vastaranta, Mikko and Honkavaara, Eija and Karjalainen, Mika
   and Yu, Xiaowei and Holopainen, Markus and Hyyppa, Juha},
Title = {{Evaluating the Performance of High-Altitude Aerial Image-Based Digital
   Surface Models in Detecting Individual Tree Crowns in Mature Boreal
   Forests}},
Journal = {{FORESTS}},
Year = {{2016}},
Volume = {{7}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{Height models based on high-altitude aerial images provide a low-cost
   means of generating detailed 3D models of the forest canopy. In this
   study, the performance of these height models in the detection of
   individual trees was evaluated in a commercially managed boreal forest.
   Airborne digital stereo imagery (DSI) was captured from a flight
   altitude of 5 km with a ground sample distance of 50 cm and corresponds
   to regular national topographic airborne data capture programs operated
   in many countries. Tree tops were detected from smoothed canopy height
   models (CHM) using watershed segmentation. The relative amount of
   detected trees varied between 26\% and 140\%, and the RMSE of plot-level
   arithmetic mean height between 2.2 m and 3.1 m. Both the dominant tree
   species and the filter used for smoothing affected the results. Even
   though the spatial resolution of DSI-based CHM was sufficient, detecting
   individual trees from the data proved to be demanding because of the
   shading effect of the dominant trees and the limited amount of data from
   lower canopy levels and near the ground.}},
DOI = {{10.3390/f7070143}},
Article-Number = {{143}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Saarinen, Ninni/K-4296-2019
   Vastaranta, Mikko/K-9656-2018
   Karjalainen, Mika/E-3348-2017
   }},
ORCID-Numbers = {{Saarinen, Ninni/0000-0003-2730-8892
   Vastaranta, Mikko/0000-0001-6552-9122
   Karjalainen, Mika/0000-0003-4320-8007
   Tanhuanpaa, Topi/0000-0002-5509-6922
   Honkavaara, Eija/0000-0002-7236-2145
   Nurminen, Kimmo/0000-0001-8036-9446}},
Unique-ID = {{ISI:000380771500016}},
}

@article{ ISI:000373271800001,
Author = {Hojris, Bo and Christensen, Sarah Christine Boesgaard and Albrechtsen,
   Hans-Jorgen and Smith, Christian and Dahlqvist, Mathis},
Title = {{A novel, optical, on-line bacteria sensor for monitoring drinking water
   quality}},
Journal = {{SCIENTIFIC REPORTS}},
Year = {{2016}},
Volume = {{6}},
Month = {{APR 4}},
Abstract = {{Today, microbial drinking water quality is monitored through either
   time-consuming laboratory methods or indirect on-line measurements.
   Results are thus either delayed or insufficient to support proactive
   action. A novel, optical, on-line bacteria sensor with a 10-minute time
   resolution has been developed. The sensor is based on 3D image
   recognition, and the obtained pictures are analyzed with algorithms
   considering 59 quantified image parameters. The sensor counts individual
   suspended particles and classifies them as either bacteria or abiotic
   particles. The technology is capable of distinguishing and quantifying
   bacteria and particles in pure and mixed suspensions, and the
   quantification correlates with total bacterial counts. Several field
   applications have demonstrated that the technology can monitor changes
   in the concentration of bacteria, and is thus well suited for rapid
   detection of critical conditions such as pollution events in drinking
   water.}},
DOI = {{10.1038/srep23935}},
Article-Number = {{23935}},
ISSN = {{2045-2322}},
ResearcherID-Numbers = {{Hojris, Bo/H-1350-2018
   Albrechtsen, Hans-Jorgen/J-1229-2014}},
ORCID-Numbers = {{Hojris, Bo/0000-0003-4129-2794
   Christensen, Sarah Christine Boesgaard/0000-0001-6183-6045
   Albrechtsen, Hans-Jorgen/0000-0003-3483-7709}},
Unique-ID = {{ISI:000373271800001}},
}

@article{ ISI:000422927300005,
Author = {Ahranjani, Behnaz Asadi and Shojaei, Bahador and Tootian, Zahra and
   Masoudifard, Madjid and Rostami, Amir},
Title = {{Anatomical, radiographical and computed tomographic study of the limbs
   skeleton of the Euphrates soft shell turtle (Rafetus euphraticus)}},
Journal = {{VETERINARY RESEARCH FORUM}},
Year = {{2016}},
Volume = {{7}},
Number = {{2}},
Pages = {{117-124}},
Month = {{SPR}},
Abstract = {{Euphrates turtle is the only soft shell turtle of Iran, and
   unfortunately is in danger of extinction due to multiple reasons.
   Imaging techniques, in addition to their importance in diagnosis of
   injuries to animals, have been used as non-invasive methods to provide
   normal anatomic views. A few studies have been conducted to understand
   body structure of the Euphrates turtle. Since there is only general
   information about the anatomy of turtle limbs, the normal skeleton of
   the Euphrates limbs was studied. For this purpose four adult Euphrates
   turtles were used. Digital radiographic examination was performed by
   computed radiographic (CR) in dorsoventral (DV) and lateral (L)
   positions. Spiral CT-scanning was done and 3D images of the bones were
   reconstructed for anatomical evaluation. For skeletal preparation, the
   skeleton was cleaned by a combination of boiling and mealworm methods
   and limbs' bones were examined anatomically. In the present study,
   simultaneous anatomic, radiographic and CT studies of bones in
   individual turtles made us possible to describe bones anatomically and
   provided comparable and complementary conditions to represent the
   abilities of the radiography and CT for better understanding of the
   anatomy. Arrangement and the number of carpal and tarsal bones are used
   in turtles' classification. Among the studied species, Euphrates turtle
   carpal and tarsal bones show the most similarities to the Apolone
   spinifera. (c) 2016 Urmia University. All rights reserved.}},
ISSN = {{2008-8140}},
EISSN = {{2322-3618}},
Unique-ID = {{ISI:000422927300005}},
}

@article{ ISI:000368511500001,
Author = {Mirshojaei, Seyedeh Fatemeh and Ahmadi, Amirhossein and Morales-Avila,
   Enrique and Ortiz-Reynoso, Mariana and Reyes-Perez, Horacio},
Title = {{Radiolabelled nanoparticles: novel classification of
   radiopharmaceuticals for molecular imaging of cancer}},
Journal = {{JOURNAL OF DRUG TARGETING}},
Year = {{2016}},
Volume = {{24}},
Number = {{2}},
Pages = {{91-101}},
Month = {{FEB 7}},
Abstract = {{Nanotechnology has been used for every single modality in the molecular
   imaging arena for imaging purposes. Synergic advantages can be explored
   when multiple molecular imaging modalities are combined with respect to
   single imaging modalities. Multifunctional nanoparticles have large
   surface areas, where multiple functional moieties can be incorporated,
   including ligands for site-specific targeting and radionuclides, which
   can be detected to create 3D images. Recently, radiolabeled
   nanoparticles with individual properties have attracted great interest
   regarding their use in multimodality tumor imaging. Multifunctional
   nanoparticles can combine diagnostic and therapeutic capabilities for
   both target-specific diagnosis and the treatment of a given disease. The
   future of nanomedicine lies in multifunctional nanoplatforms that
   combine the diagnostic ability and therapeutic effects using appropriate
   ligands, drugs, responses and technological devices, which together are
   collectively called theranostic drugs. Co-delivery of radiolabeled
   nanoparticles is useful in multifunctional molecular imaging areas
   because it comprises several advantages based on nanoparticles
   architecture, pharmacokinetics and pharmacodynamic properties.}},
DOI = {{10.3109/1061186X.2015.1048516}},
ISSN = {{1061-186X}},
EISSN = {{1029-2330}},
ResearcherID-Numbers = {{Ahmadi, Amirhossein/H-2136-2011
   }},
ORCID-Numbers = {{Ahmadi, Amirhossein/0000-0002-9737-3633
   Reyes-Perez, Horacio/0000-0001-9018-1105}},
Unique-ID = {{ISI:000368511500001}},
}

@article{ ISI:000383905800007,
Author = {de Jong, Markus A. and Wollstein, Andreas and Ruff, Clifford and
   Dunaway, David and Hysi, Pirro and Spector, Tim and Liu, Fan and
   Niessen, Wiro and Koudstaal, Maarten J. and Kayser, Manfred and Wolvius,
   Eppo B. and Bohringer, Stefan},
Title = {{An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}},
Journal = {{IEEE TRANSACTIONS ON IMAGE PROCESSING}},
Year = {{2016}},
Volume = {{25}},
Number = {{2}},
Pages = {{580-588}},
Month = {{FEB}},
Abstract = {{In this paper, we present a novel approach to automatic 3D facial
   landmarking using 2D Gabor wavelets. Our algorithm considers the face to
   be a surface and uses map projections to derive 2D features from raw
   data. Extracted features include texture, relief map, and
   transformations thereof. We extend an established 2D landmarking method
   for simultaneous evaluation of these data. The method is validated by
   performing landmarking experiments on two data sets using 21 landmarks
   and compared with an active shape model implementation. On average,
   landmarking error for our method was 1.9 mm, whereas the active shape
   model resulted in an average landmarking error of 2.3 mm. A second study
   investigating facial shape heritability in related individuals concludes
   that automatic landmarking is on par with manual landmarking for some
   landmarks. Our algorithm can be trained in 30 min to automatically
   landmark 3D facial data sets of any size, and allows for fast and robust
   landmarking of 3D faces.}},
DOI = {{10.1109/TIP.2015.2496183}},
ISSN = {{1057-7149}},
EISSN = {{1941-0042}},
ResearcherID-Numbers = {{Boehringer, Stefan/Y-2442-2018
   Liu, Fan/B-8833-2013
   }},
ORCID-Numbers = {{Boehringer, Stefan/0000-0001-9108-9212
   Liu, Fan/0000-0001-9241-8161
   Niessen, Wiro/0000-0002-5822-1995
   Dunaway, David/0000-0001-5063-9943}},
Unique-ID = {{ISI:000383905800007}},
}

@article{ ISI:000371787800087,
Author = {Alletto, Stefano and Abati, Davide and Serra, Giuseppe and Cucchiara,
   Rita},
Title = {{Exploring Architectural Details Through a Wearable Egocentric Vision
   Device}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Augmented user experiences in the cultural heritage domain are in
   increasing demand by the new digital native tourists of 21st century. In
   this paper, we propose a novel solution that aims at assisting the
   visitor during an outdoor tour of a cultural site using the unique first
   person perspective of wearable cameras. In particular, the approach
   exploits computer vision techniques to retrieve the details by proposing
   a robust descriptor based on the covariance of local features. Using a
   lightweight wearable board, the solution can localize the user with
   respect to the 3D point cloud of the historical landmark and provide him
   with information about the details at which he is currently looking.
   Experimental results validate the method both in terms of accuracy and
   computational effort. Furthermore, user evaluation based on real-world
   experiments shows that the proposal is deemed effective in enriching a
   cultural experience.}},
DOI = {{10.3390/s16020237}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000371787800087}},
}

@article{ ISI:000370350100005,
Author = {Ma, Lixia and Zheng, Guang and Eitel, Jan U. H. and Moskal, L. Monika
   and He, Wei and Huang, Huabing},
Title = {{Improved Salient Feature-Based Approach for Automatically Separating
   Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar
   Point Cloud Data of Forest Canopies}},
Journal = {{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{54}},
Number = {{2}},
Pages = {{679-696}},
Month = {{FEB}},
Abstract = {{Accurate separation of photosynthetic and nonphotosynthetic components
   in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a
   challenging but of key importance to understand the spatial distribution
   of the radiation regime, photosynthetic processes, and carbon and water
   exchanges of the forest canopy. The objective of this paper was to
   improve current methods for separating photosynthetic and
   nonphotosynthetic components in TLS data of forest canopies by adding
   two additional filters only based on its geometric information. By
   comparing the proposed approach with the eigenvalues plus color
   information-based method, we found that the proposed approach could
   effectively improve the overall producer's accuracy from 62.12\% to
   95.45\%, and the overall classification producer's accuracy would
   increase from 84.28\% to 97.80\% as the forest leaf area index (LAI)
   decreases from 4.15 to 3.13. In addition, variations in tree species had
   negligible effects on the final classification accuracy, as shown by the
   overall producer's accuracy for coniferous (93.09\%) and broadleaf
   (94.96\%) trees. To remove quantitatively the effects of the woody
   materials in a forest canopy for improving TLS-based LAI estimates, we
   also computed the ``woody-to-total area ratio{''} based on the
   classified linear class points from an individual tree. Automatic
   classification of the forest point cloud data set will facilitate the
   application of TLS on retrieving 3-D forest canopy structural
   parameters, including LAI and leaf and woody area ratios.}},
DOI = {{10.1109/TGRS.2015.2459716}},
ISSN = {{0196-2892}},
EISSN = {{1558-0644}},
ResearcherID-Numbers = {{Moskal, L. Monika/F-8715-2010
   }},
ORCID-Numbers = {{Moskal, L. Monika/0000-0003-1563-6506
   He, Wei/0000-0003-0779-2496}},
Unique-ID = {{ISI:000370350100005}},
}

@article{ ISI:000369200900006,
Author = {Shendryk, Iurii and Broich, Mark and Tulbure, Mirela G. and Alexandrov,
   Sergey V.},
Title = {{Bottom-up delineation of individual trees from full-waveform airborne
   laser scans in a structurally complex eucalypt forest}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2016}},
Volume = {{173}},
Pages = {{69-83}},
Month = {{FEB}},
Note = {{2014 ForestSAT Conference, Riva del Garda, ITALY, NOV 04-07, 2014}},
Abstract = {{Full-waveform airborne laser scanning (ALS) is a powerful tool for
   characterizing and monitoring forest structure over large areas at the
   individual tree level. Most of the existing ALS-based algorithms for
   individual tree delineation from the point cloud are top-down, which are
   accurate for delineating cone-shaped conifers, but have lower
   delineation accuracies over more structurally complex broad-leaf
   forests. Therefore, in this study we developed a new bottom-up algorithm
   for detecting trunks and delineating individual trees with complex
   shapes, such as eucalypts. Experiments were conducted in the largest
   river red gum forest in the world, located in the southeast of
   Australia, that experienced severe dieback over the past six decades.
   For detection of individual tree trunks, we used a novel approach based
   on conditional Euclidean distance clustering that takes advantage of
   spacing between laser returns. Overall, the algorithm developed in our
   study was able to detect up to 67\% of field-measured trees with
   diameter larger than or equal to 13 cm. By filtering ALS based on the
   intensity, return number and returned pulse width values, we were able
   to differentiate between woody and leaf tree components, thus improving
   the accuracy of tree trunk detections by 5\% as compared to non-filtered
   ALS. The detected trunks were used to seed random walks on graph
   algorithm for tree crown delineation. The accuracy of tree crown
   delineation for different ALS point cloud densities was assessed in
   terms of tree height and crown width and resulted in up to 68\% of
   field-measured trees being correctly delineated. The double increase in
   point density from similar to 12 points/m(2) to similar to 24
   points/m(2) resulted in tree trunk detection increase of 11\% (from 56\%
   to 67\%) and percentage of correctly delineated crowns increase of 13\%
   (from 55\% to 68\%). Our results confirm an algorithm that can be used
   to accurately delineate individual trees with complex structures (e.g.
   eucalypts and other broad leaves) and highlight the importance of
   full-waveform ALS for individual tree delineation. (C) 2015 Elsevier
   Inc. All rights reserved.}},
DOI = {{10.1016/j.rse.2015.11.008}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Tulbure, Mirela/B-3030-2012
   Tulbure, Mirela G/M-1212-2019
   }},
ORCID-Numbers = {{Tulbure, Mirela/0000-0003-1456-183X
   Tulbure, Mirela G/0000-0003-1456-183X
   Shendryk, Iurii/0000-0003-1657-1361}},
Unique-ID = {{ISI:000369200900006}},
}

@article{ ISI:000368956300004,
Author = {Sener, Emre and Mumcuoglu, Erkan U. and Hamcan, Salih},
Title = {{Bayesian segmentation of human facial tissue using 3D MR-CT information
   fusion, resolution enhancement and partial volume modelling}},
Journal = {{COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE}},
Year = {{2016}},
Volume = {{124}},
Pages = {{31-44}},
Month = {{FEB}},
Abstract = {{Background: Accurate segmentation of human head on medical images is an
   important process in a wide array of applications such as diagnosis,
   facial surgery planning, prosthesis design, and forensic identification.
   Objectives: In this study, a Bayesian method for segmentation of facial
   tissues is presented. Segmentation classes include muscle, bone, fat,
   air and skin.
   Methods: The method presented incorporates information fusion from
   multiple modalities, modelling of image resolution (measurement
   blurring), image noise, two priors helping to reduce noise and partial
   volume. Image resolution modelling employed facilitates resolution
   enhancement and superresolution capabilities during image segmentation.
   Regularization based on isotropic and directional Markov Random Field
   priors is integrated. The Bayesian model is solved iteratively yielding
   tissue class labels at every voxel of the image. Sub methods as
   variations of the main method are generated by using a combination of
   the models.
   Results: Testing of the sub-methods is performed on two patients using
   single modality three-dimensional (3D) image (magnetic resonance, MR or
   computerized tomography, CT) as well as registered MR-CT images with
   information fusion. Numerical, visual and statistical analyses of the
   methods are conducted. High segmentation accuracy values are obtained by
   the use of image resolution and partial volume models as well as
   information fusion from MR and CT images. The methods are also compared
   with our Bayesian segmentation method proposed in a previous study. The
   performance is found to be similar to our previous Bayesian approach,
   but the presented methods here eliminates ad hoc parameter tuning needed
   by the previous approach which is system and data acquisition setting
   dependent.
   Conclusions: The Bayesian approach presented provides resolution
   enhanced segmentation of very thin structures of the human head.
   Meanwhile, free parameters of the algorithm can be adjusted for
   different imaging systems and data acquisition settings in a more
   systematic way as compared with our previous study. (C) 2015 Elsevier
   Ireland Ltd. All rights reserved.}},
DOI = {{10.1016/j.cmpb.2015.10.009}},
ISSN = {{0169-2607}},
EISSN = {{1872-7565}},
ResearcherID-Numbers = {{Mumcuoglu, Erkan/B-5480-2012}},
Unique-ID = {{ISI:000368956300004}},
}

@article{ ISI:000367113200011,
Author = {Almansa, Julio and Salvat-Pujol, Francesc and Diaz-Londono, Gloria and
   Carnicer, Artur and Lallena, Antonio M. and Salvat, Francesc},
Title = {{PENGEOM-A general-purpose geometry package for Monte Carlo simulation of
   radiation transport in material systems defined by quadric surfaces}},
Journal = {{COMPUTER PHYSICS COMMUNICATIONS}},
Year = {{2016}},
Volume = {{199}},
Pages = {{102-113}},
Month = {{FEB}},
Abstract = {{The Fortran subroutine package PENGEOM provides a complete set of tools
   to handle quadric geometries in Monte Carlo simulations of radiation
   transport. The material structure where radiation propagates is assumed
   to consist of homogeneous bodies limited by quadric surfaces. The
   PENGEOM subroutines (a subset of the PENELOPE code) track particles
   through the material structure, independently of the details of the
   physics models adopted to describe the interactions. Although these
   subroutines are designed for detailed simulations of photon and electron
   transport, where all individual interactions are simulated sequentially,
   they can also be used in mixed (class II) schemes for simulating the
   transport of high-energy charged particles, where the effect of soft
   interactions is described by the random-hinge method. The definition of
   the geometry and the details of the tracking algorithm are tailored to
   optimize simulation speed. The use of fuzzy quadric surfaces minimizes
   the impact of round-off errors. The provided software includes a Java
   graphical user interface for editing and debugging the geometry
   definition file and for visualizing the material structure. Images of
   the structure are generated by using the tracking subroutines and,
   hence, they describe the geometry actually passed to the simulation
   code.
   Program summary
   Program title: Pengeom
   Catalogue identifier: AEYH\_v1\_0
   Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYH\_v1\_0.html
   Program obtainable from: CPC Program Library, Queen's University,
   Belfast, N. Ireland
   Licensing provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.html
   No. of lines in distributed program, including test data, etc.: 89390
   No. of bytes in distributed program, including test data, etc.: 5062646
   Distribution format: tar.gz
   Programming language: Fortran, Java.
   Computer: PC with Java Runtime Environment installed.
   Operating system: Windows, Linux.
   RAM: 210 MiB
   Classification: 21.1, 14.
   Nature of problem: The Fortran subroutines perform all geometry
   operations in Monte Carlo simulations of radiation transport with
   arbitrary interaction models. They track particles through material
   systems consisting of homogeneous bodies limited by quadric surfaces.
   Particles are moved in steps (free flights) of a given length, which is
   dictated by the simulation program, and are halted when they cross an
   interface between media of different compositions or when they enter
   selected bodies.
   Solution method: The pengeom subroutines are tailored to optimize
   simulation speed and accuracy. Fast tracking is accomplished by the use
   of quadric surfaces, which facilitate the calculation of ray
   intersections, and of modules (connected volumes limited by quadric
   surfaces) organized in a hierarchical structure. Optimal accuracy is
   obtained by considering fuzzy surfaces, with the aid of a simple
   algorithm that keeps control of multiple intersections of a ray and a
   surface. The Java GUI PenGeomJar provides a geometry toolbox; it allows
   building and debugging the geometry definition file, as well as
   visualizing the resulting geometry in two and three dimensions.
   Restrictions: By default pengeom can handle systems with up to 5000
   bodies and 10,000 surfaces. These numbers can be increased by editing
   the Fortran source file.
   Unusual features: All geometrical operations are performed internally.
   The connection between the steering main program and the tracking
   routines is through a Fortran module, which contains the state variables
   of the transported particle, and the input-output arguments of the
   subroutine step. Rendering of two- and three-dimensional images is
   performed by using the pengeom subroutines, so that displayed images
   correspond to the definitions passed to the simulation program.
   Additional comments: Java editor and viewer (PenGeomJar), geometry
   examples, translator to POV-Ray (TM) format, detailed manual. The
   Fortran subroutine package pengeom is part of the penelope code system
   {[}1].
   Running time: The running time much depends on the complexity of the
   material system. The most complicated example provided, phantom, an
   anthropomorphic phantom, has 264 surfaces and 169 bodies and modules,
   with different levels of grouping; the largest module contains 51
   daughters. The rendering of a 3D image of phantom with 1680x1050 pixels
   takes about 25 s (i.e., about 1.5 . 10(-5) seconds per ray) on an Intel
   Core 17-3520M CPU, with Windows 7 and subroutines compiled with
   gfortran.
   References:
   {[}1] F. Salvat, PENELOPE-2014: A Code System for Monte Carlo Simulation
   of Electron and Photon Transport, OECD/NEA Data Bank,
   Issy-les-Moulineaux, France, 2015. Available from
   http://www.nea.fr/lists/penelope.html.(C) 2015 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.cpc.2015.09.019}},
ISSN = {{0010-4655}},
EISSN = {{1879-2944}},
ResearcherID-Numbers = {{Carnicer, Artur/B-1442-2013
   Diaz-Londono, Gloria/W-2639-2018
   Salvat, Francesc/F-8255-2016
   }},
ORCID-Numbers = {{Carnicer, Artur/0000-0002-4936-5778
   Diaz-Londono, Gloria/0000-0002-3235-1193
   Salvat, Francesc/0000-0002-6162-8841
   Lallena Rojo, Antonio M./0000-0003-1962-6217}},
Unique-ID = {{ISI:000367113200011}},
}

@inproceedings{ ISI:000406771300100,
Author = {Pang, Guan and Neumann, Ulrich},
Book-Group-Author = {{IEEE}},
Title = {{3D Point Cloud Object Detection with Multi-View Convolutional Neural
   Network}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{585-590}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Efficient detection of three dimensional (3D) objects in point clouds is
   a challenging problem. Performing 3D descriptor matching or 3D
   scanning-window search with detector are both time-consuming due to the
   3-dimensional complexity. One solution is to project 3D point cloud into
   2D images and thus transform the 3D detection problem into 2D space, but
   projection at multiple viewpoints and rotations produce a large amount
   of 2D detection tasks, which limit the performance and complexity of the
   2D detection algorithm choice. We propose to use convolutional neural
   network (CNN) for the 2D detection task, because it can handle all
   viewpoints and rotations for the same class of object together, as well
   as predicting multiple classes of objects with the same network, without
   the need for individual detector for each object class. We further
   improve the detection efficiency by concatenating two extra levels of
   early rejection networks with binary outputs before the multi-class
   detection network. Experiments show that our method has competitive
   overall performance with at least one-order of magnitude speedup
   comparing with latest 3D point cloud detection methods.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771300100}},
}

@inproceedings{ ISI:000406771301004,
Author = {Tian, Guifen and Mori, Tatusya and Okuda, Yuji},
Book-Group-Author = {{IEEE}},
Title = {{Spoofing Detection for Embedded Face Recognition System using A Low Cost
   Stereo Camera}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{1017-1022}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Spoofing detection is essential for practical face recognition system.
   Based on the fact that genuine face has special geometric curvatures
   across surface, this paper brings forward an ultra-fast yet accurate
   spoofing detection approach using a low-cost stereo camera. To obtain
   curvatures, the three dimensional shapes of selected facial landmarks
   are analyzed, by fitting point cloud around each landmark to a specific
   partial face surface. Spoofing detection is then performed by evaluating
   curvatures of each landmark and integrating them together. Experiments
   verify that the approach is able to detect spoofed faces in printed
   photographs without or with various bending at FAR equal to 0.00\%.
   Meanwhile, genuine faces have a trivial opportunity to be falsely
   rejected: FRR is 0.59\% for near frontal faces and less than 5\% for
   faces with large varying poses. Detection time is 51 milliseconds when
   executed on a single processor {[}1] running at a clock frequency of
   266M Hz, this makes the detection very suitable for embedded face
   recognition system.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771301004}},
}

@inproceedings{ ISI:000406771302059,
Author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni,
   Gerard},
Book-Group-Author = {{IEEE}},
Title = {{Expression Invariant 3D Face Modeling from an RGB-D Video}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{2362-2367}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{We aim to reconstruct an accurate neutral 3D face model from an RGB-D
   video in the presence of extreme expression changes. Since each depth
   frame, taken by a low-cost sensor, is noisy, point clouds from multiple
   frames can be registered and aggregated to build an accurate 3D model.
   However, direct aggregation of multiple data produces erroneous results
   in natural interaction (e.g., talking and showing expressions). We
   propose to analyze facial expression from an RGB frame and neutralize
   the corresponding 3D point cloud if needed. We first estimate the
   person's expression by fitting blend-shape coefficients using 2D facial
   landmarks for each frame and calculate an expression deformity
   (expression score). With the estimated expression score, we determine
   whether an input face is neutral or non-neutral. If the face is
   non-neutral, we proceed to neutralize the expression of the 3D point
   cloud in that frame. To neutralize the 3D point cloud of a face, we
   deform our generic 3D face model by applying the estimated blendshape
   coefficients, find displacement vectors from the deformed generic face
   to a neutral generic face, and apply the displacement vectors to the
   input 3D point cloud. After preprocessing frames in a video, we rank
   frames based on the expression scores and register the ranked frames
   into a single 3D model. Our system produces a neutral 3D face model in
   the presence of extreme expression changes even when neutral faces do
   not exist in the video.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771302059}},
}

@inproceedings{ ISI:000405706400151,
Author = {Seo, Masataka and Chen, Yen-Wei},
Editor = {{Wang, Y and An, J and Wang, L and Li, Q and Yan, G and Chang, Q}},
Title = {{Joint Subspace Learning for Reconstruction of 3D Facial Dynamic
   Expression from Single Image}},
Booktitle = {{2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016)}},
Year = {{2016}},
Pages = {{820-824}},
Note = {{9th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI), Datong, PEOPLES R CHINA, OCT
   15-17, 2016}},
Organization = {{IEEE; IEEE Engn Med \& Biol Soc; Taiyuan Univ Technol; E China Normal
   Univ}},
Abstract = {{Recently, the synthesis of 3D dynamic expressions has become an
   important concern in computer graphics, facial recognition, etc. In this
   study, we propose a regression based joint subspace learning method for
   the automatic synthesis of 3D dynamic expression images. This method
   synthesizes 3D dynamic expression images from a single 2D facial image.
   We use two subspaces (the view subspace and the frame subspace) to
   synthesize a 3D image. First, we use the view subspace to estimate
   multi-view facial images from a front image. Next, we construct a 3D
   image using the estimated multi-view facial images. Finally, we estimate
   the 3D images in different frames by using the frame subspace to
   synthesis 3D dynamic expression images. This approach is unlike the
   conventional joint subspace learning in which, the coefficients
   estimated by the input image are directly used for synthesis.
   Furthermore, we propose using textural information to improve the
   accuracy of synthesized images.}},
ISBN = {{978-1-5090-3710-0}},
Unique-ID = {{ISI:000405706400151}},
}

@inproceedings{ ISI:000401716400003,
Author = {Naveen, S. and Ahalya, R. K. and Moni, R. S.},
Book-Group-Author = {{IEEE}},
Title = {{Multimodal Face Recognition using Spectral Transformation by LBP and
   Polynomial Coefficients}},
Booktitle = {{PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION
   SYSTEMS AND NETWORKS (COMNET)}},
Series = {{International Conference on Communication Systems and Networks}},
Year = {{2016}},
Pages = {{13-17}},
Note = {{International Conference on Communication Systems and Networks (ComNet),
   Trivandrum, INDIA, JUL 21-23, 2016}},
Organization = {{IEEE}},
Abstract = {{This paper presents a multimodal face recognition using spectral
   transformation by Local Binary Pattern (LBP) and Polynomial
   Coefficients. Here 2D image and 3D image are combined to get multimodal
   face recognition. In this method a novel feature extraction is done
   using LBP and Polynomial Coefficients. Then these features are
   spectrally transformed using Discrete Fourier Transform (DFT). These
   spectrally transformed features extracted from texture image using the
   two methods are combined at the score level. Similarly this is done in
   depth image. Finally feature information from texture and depth are
   combined at the score level which gives better results than the
   individual results.}},
ISSN = {{2155-2487}},
ISBN = {{978-1-5090-3349-2}},
Unique-ID = {{ISI:000401716400003}},
}

@inproceedings{ ISI:000401510000148,
Author = {Amin, Rafiul and Shams, A. Farhan and Rahman, S. M. Mahbubur and
   Hatzinakos, Dimitrios},
Book-Group-Author = {{IEEE}},
Title = {{Evaluation of Discrimination Power of Facial Parts from 3D Point Cloud
   Data}},
Booktitle = {{2016 9TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING
   (ICECE)}},
Series = {{International Conference on Computer and Electrical Engineering ICCEE}},
Year = {{2016}},
Pages = {{602-605}},
Note = {{9th International Conference on Electrical and Computer Engineering
   (ICECE), Dhaka, BANGLADESH, DEC 20-22, 2016}},
Organization = {{Bangladesh Univ Engn \& Technol, Dept Elect \& Elect Engn; Inst Elect \&
   Elect Engineers; Energypac; PARADISE Cables ltd; BTCL; Summit Communicat
   Ltd; Dhaka Power Distribut Co Ltd}},
Abstract = {{Feature selection from facial regions is a well-known approach to
   increase the performance of 2D image-based face recognition systems. In
   case of 3D modality, the approach of region-based feature selection for
   face recognition is relatively new. In this context, this paper presents
   an approach to evaluate the discrimination power of different regions of
   a 3D facial surface for its potential use in face recognition systems.
   We propose the use of weighted average of unit normal vector on the
   facial surface as the feature for region-based face recognition from 3D
   point cloud data (PCD). The iterative closest point algorithm is
   employed for the registration of segmented regions of facial point
   clouds. A metric based on angular distance between normals is introduced
   to indicate the similarity between two surfaces of same facial region.
   Finally, the intra class correlation based discrimination score is
   formulated to find out the key facial regions such as the eyes, nose,
   and mouth that are significant while recognizing a person with facial
   surface PCD.}},
ISBN = {{978-1-5090-2963-1}},
ResearcherID-Numbers = {{Amin, Rafiul/L-8633-2019}},
Unique-ID = {{ISI:000401510000148}},
}

@inproceedings{ ISI:000400012304105,
Author = {Bolkart, Timo and Wuhrer, Stefanie},
Book-Group-Author = {{IEEE}},
Title = {{A Robust Multilinear Model Learning Framework for 3D Faces}},
Booktitle = {{2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2016}},
Pages = {{4911-4919}},
Note = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Seattle, WA, JUN 27-30, 2016}},
Organization = {{IEEE Comp Soc; Comp Vis Fdn}},
Abstract = {{Multilinear models are widely used to represent the statistical
   variations of 3D human faces as they decouple shape changes due to
   identity and expression. Existing methods to learn a multilinear face
   model degrade if not every person is captured in every expression, if
   face scans are noisy or partially occluded, if expressions are
   erroneously labeled, or if the vertex correspondence is inaccurate.
   These limitations impose requirements on the training data that
   disqualify large amounts of available 3D face data from being usable to
   learn a multilinear model. To overcome this, we introduce the first
   framework to robustly learn a multilinear model from 3D face databases
   with missing data, corrupt data, wrong semantic correspondence, and
   inaccurate vertex correspondence. To achieve this robustness to
   erroneous training data, our framework jointly learns a multilinear
   model and fixes the data. We evaluate our framework on two publicly
   available 3D face databases, and show that our framework achieves a data
   completion accuracy that is comparable to state-of-the-art tensor
   completion methods. Our method reconstructs corrupt data more accurately
   than state-of-the-art methods, and improves the quality of the learned
   model significantly for erroneously labeled expressions.}},
DOI = {{10.1109/CVPR.2016.531}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-8851-1}},
Unique-ID = {{ISI:000400012304105}},
}

@inproceedings{ ISI:000400688200019,
Author = {Starczewski, Janusz T. and Pabiasz, Sebastian and Vladymyrska, Natalia
   and Marvuglia, Antonino and Napoli, Christian and Wozniak, Marcin},
Editor = {{Rutkowski, L and Korytkowski, M and Scherer, R and Tadeusiewicz, R and Zadeh, LA and Zurada, JM}},
Title = {{Self Organizing Maps for 3D Face Understanding}},
Booktitle = {{ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING, (ICAISC 2016), PT II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9693}},
Pages = {{210-217}},
Note = {{15th International Conference on Artificial Intelligence and Soft
   Computing (ICAISC), Zakopane, POLAND, JUN 12-16, 2016}},
Organization = {{Polish Neural Network Soc; Univ Social Sci; Czestochowa Univ Technol,
   Inst Computat Intelligence}},
Abstract = {{Landmarks are unique points that can be located on every face. Facial
   landmarks typically recognized by people are correlated with
   anthropomorphic points. Our purpose is to employ in 3D face recognition
   such landmarks that are easy to interpret. Face understanding is
   construed as identification of face characteristic points with automatic
   labeling of them. In this paper, we apply methods based on Self
   Organizing Maps to understand 3D faces.}},
DOI = {{10.1007/978-3-319-39384-1\_19}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-39384-1}},
ResearcherID-Numbers = {{Marvuglia, Antonino/J-2595-2019
   Wozniak, Marcin/L-6640-2013
   }},
ORCID-Numbers = {{Wozniak, Marcin/0000-0002-9073-5347
   Starczewski, Janusz/0000-0003-4694-7868
   Napoli, Christian/0000-0002-3336-5853}},
Unique-ID = {{ISI:000400688200019}},
}

@inproceedings{ ISI:000388114601158,
Author = {Gevaert, Caroline and Persello, Claudio and Sliuzas, Richard and
   Vosselman, George},
Book-Group-Author = {{IEEE}},
Title = {{INTEGRATION OF 2D AND 3D FEATURES FROM UAV IMAGERY FOR INFORMAL
   SETTLEMENT CLASSIFICATION USING MULTIPLE KERNEL LEARNING}},
Booktitle = {{2016 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS)}},
Series = {{IEEE International Symposium on Geoscience and Remote Sensing IGARSS}},
Year = {{2016}},
Pages = {{1508-1511}},
Note = {{36th IEEE International Geoscience and Remote Sensing Symposium
   (IGARSS), Beijing, PEOPLES R CHINA, JUL 10-15, 2016}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers, Geoscience
   \& Remote Sensing Soc; NSSC}},
Abstract = {{Informal settlement upgrading projects require highresolution and
   up-to-date thematic maps in order to plan and design effective
   interventions. To this end, Unmanned Aerial Vehicles (UAVs) provide the
   opportunity to obtain very high resolution 2D orthomosaics and 3D point
   clouds where and when needed. The heterogeneous, dense structures which
   typically make up an informal settlement motivate the importance of
   integrating complex 2D and 3D features obtained from UAV data into a
   single classification problem. Multiple Kernel Learning (MKL) Support
   Vector Machines (SVMs) maintain the distinct characteristics of the
   different feature spaces by optimizing individual kernels for specific
   feature groups which are later combined into a single kernel used for
   classification. Both the kernel parameters and kernel weights can be
   optimized by considering the alignment between the kernel and an ideal
   kernel which would perfectly classify the samples. This paper
   demonstrates how extracting high-level features from both the 2D
   orthomosaic as well as the 3D point cloud (obtained by an UAV), and
   integrating them through a MKL approach, can obtain an Overall Accuracy
   of 90.29\%, a 4\% increase over the results obtained using single kernel
   methods.}},
DOI = {{10.1109/IGARSS.2016.7729385}},
ISSN = {{2153-6996}},
ISBN = {{978-1-5090-3332-4}},
ResearcherID-Numbers = {{Persello, Claudio/L-5713-2015
   Vosselman, George/D-3985-2009
   Gevaert, Caroline/H-6195-2019
   Sliuzas, Richard/K-5323-2013}},
ORCID-Numbers = {{Persello, Claudio/0000-0003-3742-5398
   Vosselman, George/0000-0001-8813-8028
   Sliuzas, Richard/0000-0001-5243-4431}},
Unique-ID = {{ISI:000388114601158}},
}

@inproceedings{ ISI:000393154600016,
Author = {Harikumar, A. and Bovolo, F. and Bruzzone, L.},
Editor = {{Bruzzone, L and Bovolo, F}},
Title = {{A novel approach to internal crown characterization for coniferous tree
   species classification}},
Booktitle = {{IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXII}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{10004}},
Note = {{Conference on Image and Signal Processing for Remote Sensing XXII,
   Edinburgh, SCOTLAND, SEP 26-28, 2016}},
Organization = {{SPIE}},
Abstract = {{The knowledge about individual trees in forest is highly beneficial in
   forest management. High density small foot- print multi-return airborne
   Light Detection and Ranging (LiDAR) data can provide a very accurate
   information about the structural properties of individual trees in
   forests. Every tree species has a unique set of crown structural
   characteristics that can be used for tree species classification. In
   this paper, we use both the internal and external crown structural
   information of a conifer tree crown, derived from a high density small
   foot-print multi-return LiDAR data acquisition for species
   classification. Considering the fact that branches are the major
   building blocks of a conifer tree crown, we obtain the internal crown
   structural information using a branch level analysis. The structure of
   each conifer branch is represented using clusters in the LiDAR point
   cloud. We propose the joint use of the k-means clustering and geometric
   shape fitting, on the LiDAR data projected onto a novel 3-dimensional
   space, to identify branch clusters. After mapping the identified
   clusters back to the original space, six internal geometric features are
   estimated using a branch-level analysis. The external crown
   characteristics are modeled by using six least correlated features based
   on cone fitting and convex hull. Species classification is performed
   using a sparse Support Vector Machines (sparse SVM) classifier.}},
DOI = {{10.1117/12.2241452}},
Article-Number = {{100040H}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0412-4; 978-1-5106-0413-1}},
Unique-ID = {{ISI:000393154600016}},
}

@inproceedings{ ISI:000390287300035,
Author = {Pan, Zhenglin and Polceanu, Mihai and Lisetti, Christine},
Editor = {{Traum, D and Swartout, W and Khooshabeh, P and Kopp, S and Scherer, S and Leuski, A}},
Title = {{On Constrained Local Model Feature Normalization for Facial Expression
   Recognition}},
Booktitle = {{INTELLIGENT VIRTUAL AGENTS, IVA 2016}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{10011}},
Pages = {{369-372}},
Note = {{16th International Conference on Intelligent Virtual Agents (IVA), Los
   Angeles, CA, SEP 20-23, 2016}},
Organization = {{Alelo; Springer; Univ So Calif, Inst Creat Technologies}},
Abstract = {{Real time user independent facial expression recognition is important
   for virtual agents but challenging. However, since in real time
   recognition users are not necessarily presenting all the emotions, some
   proposed methods are not applicable. In this paper, we present a new
   approach that instead of using the traditional base face normalization
   on whole face shapes, performs normalization on the point cloud of each
   landmark. The result shows that our method outperforms the other two
   when the user input does not contain all six universal emotions.}},
DOI = {{10.1007/978-3-319-47665-0\_35}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-47665-0; 978-3-319-47664-3}},
Unique-ID = {{ISI:000390287300035}},
}

@inproceedings{ ISI:000392743800047,
Author = {Boehm, J. and Bredif, M. and Gierlinger, T. and Kraemer, M. and
   Lindenbergh, R. and Liu, K. and Michel, F. and Sirmacek, B.},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND
   IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{301-307}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Current 3D data capturing as implemented on for example airborne or
   mobile laser scanning systems is able to efficiently sample the surface
   of a city by billions of unselective points during one working day. What
   is still difficult is to extract and visualize meaningful information
   hidden in these point clouds with the same efficiency. This is where the
   FP7 IQmulus project enters the scene. IQmulus is an interactive facility
   for processing and visualizing big spatial data. In this study the
   potential of IQmulus is demonstrated on a laser mobile mapping point
   cloud of 1 billion points sampling similar to 10 km of street
   environment in Toulouse, France. After the data is uploaded to the
   IQmulus Hadoop Distributed File System, a workflow is defined by the
   user consisting of retiling the data followed by a PCA driven local
   dimensionality analysis, which runs efficiently on the IQmulus cloud
   facility using a Spark implementation. Points scattering in 3 directions
   are clustered in the tree class, and are separated next into individual
   trees. Five hours of processing at the 12 node computing cluster results
   in the automatic identification of 4000+ urban trees. Visualization of
   the results in the IQmulus fat client helps users to appreciate the
   results, and developers to identify remaining flaws in the processing
   workflow.}},
DOI = {{10.5194/isprsarchives-XLI-B3-301-2016}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Boehm, Jan/K-2336-2012
   Bredif, Mathieu/T-3029-2018
   }},
ORCID-Numbers = {{Boehm, Jan/0000-0003-2190-0449
   Bredif, Mathieu/0000-0003-0228-1232
   Lindenbergh, Roderik/0000-0001-8655-5266}},
Unique-ID = {{ISI:000392743800047}},
}

@inproceedings{ ISI:000392743800052,
Author = {Moradi, A. and Satari, M. and Momeni, M.},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{INDIVIDUAL TREE OF URBAN FOREST EXTRACTION FROM VERY HIGH DENSITY LIDAR
   DATA}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{337-343}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Airborne LiDAR (Light Detection and Ranging) data have a high potential
   to provide 3D information from trees. Most proposed methods to extract
   individual trees detect points of tree top or bottom firstly and then
   using them as starting points in a segmentation algorithm. Hence, in
   these methods, the number and the locations of detected peak points
   heavily effect on the process of detecting individual trees. In this
   study, a new method is presented to extract individual tree segments
   using LiDAR points with 10cm point density. In this method, a two-step
   strategy is performed for the extraction of individual tree LiDAR
   points: finding deterministic segments of individual trees points and
   allocation of other LiDAR points based on these segments. This research
   is performed on two study areas in Zeebrugge, Bruges, Belgium (51.33
   degrees N, 3.20 degrees E). The accuracy assessment of this method
   showed that it could correctly classified 74.51\% of trees with 21.57\%
   and 3.92\% under- and over-segmentation errors respectively.}},
DOI = {{10.5194/isprsarchives-XLI-B3-337-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Momeni, Mehdi/0000-0003-3705-1787}},
Unique-ID = {{ISI:000392743800052}},
}

@inproceedings{ ISI:000392743800092,
Author = {Kadamen, Jayren and Sithole, George},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{AUTOMATICALLY DETERMINING SCALE WITHIN UNSTRUCTU RED POINT CLOUDS}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{617-624}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Three dimensional models obtained from imagery have an arbitrary scale
   and therefore have to be scaled. Automatically scaling these models
   requires the detection of objects in these models which can be
   computationally intensive. Real-time object detection may pose problems
   for applications such as indoor navigation. This investigation poses the
   idea that relational cues, specifically height ratios, within indoor
   environments may offer an easier means to obtain scales for models
   created using imagery. The investigation aimed to show two things, (a)
   that the size of objects, especially the height off ground is consistent
   within an environment, and (b) that based on this consistency, objects
   can be identified and their general size used to scale a model. To test
   the idea a hypothesis is first tested on a terrestrial lidar scan of an
   indoor environment. Later as a proof of concept the same test is applied
   to a model created using imagery. The most notable finding was that the
   detection of objects can be more readily done by studying the ratio
   between the dimensions of objects that have their dimensions defined by
   human physiology. For example the dimensions of desks and chairs are
   related to the height of an average person. In the test, the difference
   between generalised and actual dimensions of objects were assessed. A
   maximum difference of 3.96\% (2.93cm) was observed from automated
   scaling. By analysing the ratio between the heights (distance from the
   floor) of the tops of objects in a room, identification was also
   achieved.}},
DOI = {{10.5194/isprsarchives-XLI-B3-617-2016}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000392743800092}},
}

@inproceedings{ ISI:000392739800107,
Author = {Zhou, K. and Gorte, B. and Zlatanova, S.},
Editor = {{Halounova, L and Safar, V and Remondino, F and Hodac, J and Pavelka, K and Shortis, M and Rinaudo, F and Scaioni, M and Boehm, J and RiekeZapp, D}},
Title = {{EXPLORING REGULARITIES FOR IMPROVING FACADE RECONSTRUCTION FROM POINT
   CLOUDS}},
Booktitle = {{XXIII ISPRS Congress, Commission V}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B5}},
Pages = {{749-755}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{(Semi)-automatic facade reconstruction from terrestrial LiDAR point
   clouds is often affected by both quality of point cloud itself and
   imperfectness of object recognition algorithms. In this paper, we employ
   regularities, which exist on facades, to mitigate these problems. For
   example, doors, windows and balconies often have orthogonal and parallel
   boundaries. Many windows are constructed with the same shape. They may
   be arranged at the same lines and distance intervals, so do different
   windows. By identifying regularities among objects with relatively poor
   quality, these can be applied to calibrate the objects and improve their
   quality. The paper focuses on the regularities among the windows, which
   is the majority of objects on the wall. Regularities are classified into
   three categories: within an individual window, among similar windows and
   among different windows. Nine cases are specified as a reference for
   exploration. A hierarchical clustering method is employed to identify
   and apply regularities in a feature space, where regularities can be
   identified from clusters. To find the corresponding features in the nine
   cases of regularities, two phases are distinguished for similar and
   different windows. In the first phase, ICP (iterative closest points) is
   used to identify groups of similar windows. The registered points and a
   number of transformation matrices are used to identify and apply
   regularities among similar windows. In the second phase, features are
   extracted from the boundaries of the different windows. When applying
   regularities by relocating windows, the connections, called chains,
   established among the similar windows in the first phase are preserved.
   To test the performance of the algorithms, two datasets from terrestrial
   LiDAR point clouds are used. Both show good effects on the reconstructed
   model, while still matching with original point cloud, preventing over
   or under-regularization.}},
DOI = {{10.5194/isprsarchives-XLI-B5-749-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Gorte, Bernardus/0000-0001-8953-6394}},
Unique-ID = {{ISI:000392739800107}},
}

@inproceedings{ ISI:000391534900098,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal},
Editor = {{Liew, AWC and Lovell, B and Fookes, C and Zhou, J and Gao, Y and Blumenstein, M and Wang, Z}},
Title = {{Towards Large-scale 3D Face Recognition}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND
   APPLICATIONS (DICTA)}},
Year = {{2016}},
Pages = {{682-689}},
Note = {{International Conference on Digital Image Computing - Techniques and
   Applications (DICTA), Gold Coast, AUSTRALIA, NOV 30-DEC 02, 2016}},
Organization = {{Australian Govt, Dept Defence, Defence Sci \& Technol Grp; IAPR; Canon
   Informat Syst Res Australia; IEEE; Griffith Univ; APRS}},
Abstract = {{3D face recognition holds great promise in achieving robustness to pose,
   expressions and occlusions. However, 3D face recognition algorithms are
   still far behind their 2D counterparts due to the lack of large-scale
   datasets. We present a model based algorithm for 3D face recognition and
   test its performance by combining two large public datasets of 3D faces.
   We propose a Fully Convolutional Deep Network (FCDN) to initialize our
   algorithm. Reliable seed points are then extracted from each 3D face by
   evolving level set curves with a single curvature dependent adaptive
   speed function. We then establish dense correspondence between the faces
   in the training set by matching the surface around the seed points on a
   template face to the ones on the target faces. A morphable model is then
   fitted to probe faces and face recognition is performed by matching the
   parameters of the probe and gallery faces. Our algorithm achieves state
   of the art landmark localization results. Face recognition results on
   the combined FRGCv2 and Bosphorus datasets show that our method is
   affective in recognizing query faces with real world variations in pose
   and expression, and with occlusion and missing data despite a huge
   gallery. Comparing results of individual and combined datasets show that
   the recognition accuracy drops when the size of the gallery increases.}},
ISBN = {{978-1-5090-2896-2}},
Unique-ID = {{ISI:000391534900098}},
}

@inproceedings{ ISI:000391015300024,
Author = {Parmehr, Ebadat G. and Amati, Marco and Fraser, Clive S.},
Editor = {{Halounova, L and Sunar, F and Potuckova, M and Patkova, L and Yoshimura, M and Soergel, U}},
Title = {{MAPPING URBAN TREE CANOPY COVER USING FUSED AIRBORNE LIDAR AND SATELLITE
   IMAGERY DATA}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION VII}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{3}},
Number = {{7}},
Pages = {{181-186}},
Note = {{23rd ISPRS Congress, Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Urban green spaces, particularly urban trees, play a key role in
   enhancing the liveability of cities. The availability of accurate and
   up-to-date maps of tree canopy cover is important for sustainable
   development of urban green spaces. LiDAR point clouds are widely used
   for the mapping of buildings and trees, and several LiDAR point cloud
   classification techniques have been proposed for automatic mapping.
   However, the effectiveness of point cloud classification techniques for
   automated tree extraction from LiDAR data can be impacted to the point
   of failure by the complexity of tree canopy shapes in urban areas.
   Multispectral imagery, which provides complementary information to LiDAR
   data, can improve point cloud classification quality. This paper
   proposes a reliable method for the extraction of tree canopy cover from
   fused LiDAR point cloud and multispectral satellite imagery data. The
   proposed method initially associates each LiDAR point with spectral
   information from the co-registered satellite imagery data. It calculates
   the normalised difference vegetation index (NDVI) value for each LiDAR
   point and corrects tree points which have been misclassified as
   buildings. Then, region growing of tree points, taking the NDVI value
   into account, is applied. Finally, the LiDAR points classified as tree
   points are utilised to generate a canopy cover map. The performance of
   the proposed tree canopy cover mapping method is experimentally
   evaluated on a data set of airborne LiDAR and WorldView 2 imagery
   covering a suburb in Melbourne, Australia.}},
DOI = {{10.5194/isprsannals-III-7-181-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Amati, Marco/0000-0002-9600-5572}},
Unique-ID = {{ISI:000391015300024}},
}

@inproceedings{ ISI:000390841700083,
Author = {Torkhani, Ghada and Ladgham, Anis and Mansouri, Mohamed Nejib and Sakly,
   Anis},
Book-Group-Author = {{IEEE}},
Title = {{Gabor-SVM Applied to 3D-2D Deformed Mesh Model}},
Booktitle = {{2016 2ND INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)}},
Year = {{2016}},
Pages = {{447-452}},
Note = {{2nd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP), Monastir, TUNISIA, MAR 21-23, 2016}},
Organization = {{IEEE; IEEE Tunisia Sect; ATMS Lab; ATSI; IEEE Explore; EMB; ENIS Sch;
   Telecom Paris; Supelec; CESBIO; Telecom SudParis; ENIT; Univ Paris Sud;
   IEEE Signal Proc Soc Tunisia Chapter; ISAAM Inst; Minist Higher Educ
   Res; IEEE EMP Tunisia Chapter; Novartis Company}},
Abstract = {{We propose a robust method for 3D face recognition using 3D to 2D
   modeling and facial curvatures detection. The 3D2D algorithm permits to
   transform 3D images into 3D triangular mesh, then the mesh model is
   deformed and fitted to the 2D space in order to obtain a 2D smoother
   mesh. Then, we apply Gabor wavelets to the deformed model in order to
   exploit surface curves in the detection of salient face features. The
   classification of the final Gabor facial model is performed using the
   support vector machines (SVM). To demonstrate the quality of our
   technique, we give some experiments using the 3D AJMAL faces database.
   The experimental results prove that the proposed method is able to give
   a good recognition quality and a high accuracy rate.}},
ISBN = {{978-1-4673-8526-8}},
Unique-ID = {{ISI:000390841700083}},
}

@inproceedings{ ISI:000390841200037,
Author = {Koppen, W. P. and Christmas, W. J. and Crouch, D. J. M. and Bodmer, W.
   F. and Kittler, J. V.},
Editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun, J}},
Title = {{Extending Non-negative Matrix Factorisation to 3D Registered Data}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2016}},
Note = {{9th International Conference on Biometrics (ICB), Halmstad Univ,
   Halmstad, SWEDEN, JUN 13-16, 2016}},
Organization = {{IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB;
   Safran Ident \& Secur; EU Horizon 2020 Project IDENT; Speed Ident AB;
   Cognitec}},
Abstract = {{The use of non-negative matrix factorisation (NMF) on 2D face images has
   been shown to result in sparse feature vectors that encode for local
   patches on the face, and thus provides a statistically justified
   approach to learning parts from wholes. However successful on 2D images,
   the method has so far not been extended to 3D images. The main reason
   for this is that 3D space is a continuum and so it is not apparent how
   to represent 3D coordinates in a non-negative fashion. This work
   compares different non-negative representations for spatial coordinates,
   and demonstrates that not all non-negative representations are suitable.
   We analyse the representational properties that make NMF a successful
   method to learn sparse 3D facial features. Using our proposed
   representation, the factorisation results in sparse and interpretable
   facial features.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-5090-1869-7}},
Unique-ID = {{ISI:000390841200037}},
}

@inproceedings{ ISI:000390841200018,
Author = {Zhang, Jinjin and Huang, Di and Wang, Yunhong and Sun, Jia},
Editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun, J}},
Title = {{Lock3DFace: A Large-Scale Database of Low-Cost Kinect 3D Faces}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2016}},
Note = {{9th International Conference on Biometrics (ICB), Halmstad Univ,
   Halmstad, SWEDEN, JUN 13-16, 2016}},
Organization = {{IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB;
   Safran Ident \& Secur; EU Horizon 2020 Project IDENT; Speed Ident AB;
   Cognitec}},
Abstract = {{In this paper, we present a large-scale database consisting of low cost
   Kinect 3D face videos, namely Lock3DFace, for 3D face analysis,
   particularly for 3D Face Recognition (FR). To the best of our knowledge,
   Lock3DFace is currently the largest low cost 3D face database for public
   academic use. The 3D samples are highly noisy and contain a diversity of
   variations in expression, pose, occlusion, time lapse, and their
   corresponding texture and near infrared channels have changes in
   lighting condition and radiation intensity, allowing for evaluating FR
   methods in complex situations. Furthermore, based on Lock3DFace, we
   design the standard experimental protocol for low-cost 3D FR, and give
   the baseline performance of individual subsets belonging to different
   scenarios for fair comparison in the future.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-5090-1869-7}},
Unique-ID = {{ISI:000390841200018}},
}

@inproceedings{ ISI:000387187800290,
Author = {Liu, Shuming and Chen, Xiaopeng and Fan, Di and Chen, Xu and Meng, Fei
   and Huang, Qiang},
Book-Group-Author = {{IEEE}},
Title = {{3D Smiling Facial Expression Recognition Based on SVM}},
Booktitle = {{2016 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION}},
Year = {{2016}},
Pages = {{1661-1666}},
Note = {{IEEE International Conference on Mechatronics and Automation, Harbin,
   PEOPLES R CHINA, AUG 07-10, 2016}},
Organization = {{IEEE}},
Abstract = {{Using Kinect acquired RGB-D image to obtain a face feature parameters
   and three-dimensional coordinates of the characteristic parameters, and
   to select the characteristic parameter Facial by Candide-3 model, and
   feature extraction and normalization. Smile face expression data
   collection through Kinect, SVM collected to smiley face data classify
   and output the result of recognition, and the results compared with
   two-dimensional image of smiling face expression recognition results.
   Experimental results show that three-dimensional image of smiling face
   expression recognition accuracy than the two-dimensional image of
   smiling face. This research has important significance for the research
   and application of facial expression recognition technology.}},
ISBN = {{978-1-5090-2396-7}},
Unique-ID = {{ISI:000387187800290}},
}

@inproceedings{ ISI:000389381200037,
Author = {Trung Truong and Ngoc Ly},
Editor = {{Nguyen, NT and Trawinski, B and Fujita, H and Hong, TP}},
Title = {{Building the Facial Expressions Recognition System Based on RGB-D Images
   in High Performance}},
Booktitle = {{Intelligent Information and Database Systems, ACIIDS 2016, Pt II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9622}},
Pages = {{377-387}},
Note = {{8th Asian Conference on Intelligent Information and Database Systems
   (ACIIDS), Da Nang, VIETNAM, MAR 14-16, 2016}},
Organization = {{Vietnam Korea Friendship Informat Technol Coll; Wroclaw Univ Technol;
   IEEE SMC Tech Comm Computat Collect Intelligence; Bina Nusantara Univ;
   Ton Duc Thang Univ; Quang Binh Univ}},
Abstract = {{In this paper, we propose a novel idea for automatic facial expression
   analysis with the aim of resolving the existing challenges in 2D images.
   The subtle combination of the geometry-based method with the
   appearance-based features in depth and color images contributes to
   increasing in distinguishable features among various facial expressions.
   Particular functions are utilised to calculate the correlation between
   expressions in order to determine the exact facial expression. Our
   approach consists of a sequence of steps including estimating the normal
   vector of facial surface, then extracting the geometric features such as
   the orientation of normal vector in the point cloud. The useful color
   information is known as LBP. According to the result of the experiment,
   we demonstrate that the effective fusion scheme of texture and shape
   feature on color and depth images. In comparison with the non fusion
   scheme, our fusion scheme has resulted in the increase of recognition
   under low and high illuminated light, about 19.84\% and 1.59\%,
   respectively.}},
DOI = {{10.1007/978-3-662-49390-8\_37}},
ISSN = {{0302-9743}},
ISBN = {{978-3-662-49390-8; 978-3-662-49389-2}},
ORCID-Numbers = {{Truong, Quang Trung/0000-0002-6242-2191}},
Unique-ID = {{ISI:000389381200037}},
}

@article{ ISI:000385343000017,
Author = {Abd Rahman, Siti Zaharah and Abdullah, Siti Norul Huda Sheikh and Hao,
   Lim Eng and Abdulameer, Mohammed Hasan and Zamani, Nazri Ahmad and
   Darus, Mohammad Zaharudin A.},
Title = {{MAPPING 2D TO 3D FORENSIC FACIAL RECOGNITION VIA BIO-INSPIRED ACTIVE
   APPEARANCE MODEL}},
Journal = {{JURNAL TEKNOLOGI}},
Year = {{2016}},
Volume = {{78}},
Number = {{2-2}},
Pages = {{121-129}},
Abstract = {{This research done is to solve the problems faced by digital forensic
   analysts in identifying a suspect captured on their CCTV. Identifying
   the suspect through the CCTV video footage is a very challenging task
   for them as it involves tedious rounds of processes to match the facial
   information in the video footage to a set of suspect's images. The
   biggest problem faced by digital forensic analysis is modeling 2D model
   extracted from CCTV video as the model does not provide enough
   information to carry out the identification process. Problems occur when
   a suspect in the video is not facing the camera, the image extracted is
   the side image of the suspect and it is difficult to make a matching
   with portrait image in the database. There are also many factors that
   contribute to the process of extracting facial information from a video
   to be difficult, such as low-quality video. Through 2D to 3D image model
   mapping, any partial face information that is incomplete can be matched
   more efficiently with 3D data by rotating it to matched position. The
   first methodology in this research is data collection; any data obtained
   through video recorder. Then, the video will be converted into an image.
   Images are used to develop the Active Appearance Model (the 2D face
   model is AAM) 2D and AAM 3D. AAM is used as an input for learning and
   testing process involving three classifiers, which are Random Forest,
   Support Vector Machine (SVM), and Neural Networks classifier. The
   experimental results show that the 3D model is more suitable for use in
   face recognition as the percentage of the recognition is higher compared
   with the 2D model.}},
ISSN = {{0127-9696}},
EISSN = {{2180-3722}},
ORCID-Numbers = {{Rahman, Syed Ziaur/0000-0002-3460-1993}},
Unique-ID = {{ISI:000385343000017}},
}

@inproceedings{ ISI:000385794300020,
Author = {Khatiwada, Bikalpa and Budge, Scott E.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Three-dimensional image reconstruction using bundle adjustment applied
   to multiple texel images}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXI}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9832}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Baltimore,
   MD, APR 19-20, 2016}},
Organization = {{SPIE}},
Abstract = {{The importance of creating 3D imagery is increasing and has many
   applications in the field of disaster response, digital elevation
   models, object recognition, and cultural heritage. Several methods have
   been proposed to register texel images, which consist of fused lidar and
   digital imagery. The previous methods were limited to registering up to
   two texel images or multiple texel swaths having only one strip of lidar
   data per swath. One area of focus still remains to register multiple
   texel images to create a 3D model.
   The process of creating true 3D images using multiple texel images is
   described. The texel camera fuses the 2D digital image and calibrated 3D
   lidar data to form a texel image. The images are then taken from several
   perspectives and registered. The advantage of using multiple full frame
   texel images over 3D- or 2D-only methods is that there will be better
   registration between images because of the overlapping 3D points as well
   as 2D texture used in the joint registration process. The individual
   position and rotation mapping to a common world coordinate frame is
   calculated for each image and optimized. The proposed methods
   incorporate bundle adjustment for jointly optimizing the registration of
   multiple images. Sparsity is exploited as there is a lack of interaction
   between parameters of different cameras. Examples of the 3D model are
   shown and analyzed for numerical accuracy.}},
DOI = {{10.1117/12.2223259}},
Article-Number = {{UNSP 98320S}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0073-7}},
ORCID-Numbers = {{Budge, Scott/0000-0002-6138-3602}},
Unique-ID = {{ISI:000385794300020}},
}

@inproceedings{ ISI:000385794300011,
Author = {Magruder, Lori A. and Leigh, Holly W. and Soderlund, Alexander and
   Clymer, Bradley and Baer, Jessica and Neuenschwander, Amy L.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Automated feature extraction for 3-dimensional point clouds}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXI}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9832}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Baltimore,
   MD, APR 19-20, 2016}},
Organization = {{SPIE}},
Abstract = {{Light detection and ranging (LIDAR) technology offers the capability to
   rapidly capture high-resolution, 3-dimensional surface data with
   centimeter-level accuracy for a large variety of applications. Due to
   the foliage-penetrating properties of LIDAR systems, these geospatial
   data sets can detect ground surfaces beneath trees, enabling the
   production of high-fidelity bare earth elevation models. Precise
   characterization of the ground surface allows for identification of
   terrain and non-terrain points within the point cloud, and facilitates
   further discernment between natural and man-made objects based solely on
   structural aspects and relative neighboring parameterizations. A
   framework is presented here for automated extraction of natural and
   man-made features that does not rely on coincident ortho-imagery or
   point RGB attributes. The TEXAS (Terrain EXtraction And Segmentation)
   algorithm is used first to generate a bare earth surface from a lidar
   survey, which is then used to classify points as terrain or non-terrain.
   Further classifications are assigned at the point level by leveraging
   local spatial information. Similarly classed points are then clustered
   together into regions to identify individual features. Descriptions of
   the spatial attributes of each region are generated, resulting in the
   identification of individual tree locations, forest extents, building
   footprints, and 3-dimensional building shapes, among others. Results of
   the fully-automated feature extraction algorithm are then compared to
   ground truth to assess completeness and accuracy of the methodology.}},
DOI = {{10.1117/12.2223845}},
Article-Number = {{UNSP 98320F}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0073-7}},
Unique-ID = {{ISI:000385794300011}},
}

@inproceedings{ ISI:000384248300039,
Author = {Ding, Yifu and Tavolara, Thomas and Cheng, Keith},
Editor = {{Gurcan, MN and Madabhushi, A}},
Title = {{Automated Detection of Retinal Cell Nuclei in 3D Micro-CT Images of
   Zebrafish using Support Vector Machine Classification}},
Booktitle = {{MEDICAL IMAGING 2016: DIGITAL PATHOLOGY}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9791}},
Note = {{Conference on Medical Imaging - Digital Pathology, San Diego, CA, MAR
   02-03, 2016}},
Organization = {{SPIE; Modus Med Devices Inc; Bruker; Poco Graphite; ImXPAD}},
Abstract = {{Our group is developing a method to examine biological specimens in
   cellular detail using synchrotron microCT. The method can acquire 3D
   images of tissue at micrometer-scale resolutions, allowing for
   individual cell types to be visualized in the context of the entire
   specimen. For model organism research, this tool will enable the rapid
   characterization of tissue architecture and cellular morphology from
   every organ system. This characterization is critical for proposed and
   ongoing ``phenome{''} projects that aim to phenotype whole-organism
   mutants and diseased tissues from different organisms including humans.
   With the envisioned collection of hundreds to thousands of images for a
   phenome project, it is important to develop quantitative image analysis
   tools for the automated scoring of organism phenotypes across organ
   systems. Here we present a first step towards that goal, demonstrating
   the use of support vector machines (SVM) in detecting retinal cell
   nuclei in 3D images of wild-type zebrafish. In addition, we apply the
   SVM classifier on a mutant zebrafish to examine whether SVMs can be used
   to capture phenotypic differences in these images. The long-term goal of
   this work is to allow cellular and tissue morphology to be characterized
   quantitatively for many organ systems, at the level of the
   whole-organism.}},
DOI = {{10.1117/12.2216940}},
Article-Number = {{97911A}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0026-3}},
ORCID-Numbers = {{Ding, Yifu/0000-0002-4629-5858}},
Unique-ID = {{ISI:000384248300039}},
}

@inproceedings{ ISI:000381427400036,
Author = {Naveen, S. and Moni, R. S.},
Editor = {{Berretti, S and Thampi, SM and Srivastava, PR}},
Title = {{Contourlet and Fourier Transform Features Based 3D Face Recognition
   System}},
Booktitle = {{INTELLIGENT SYSTEMS TECHNOLOGIES AND APPLICATIONS, VOL 1}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2016}},
Volume = {{384}},
Pages = {{411-425}},
Note = {{International Symposium on Intelligent Systems Technologies and
   Applications (ISTA), SCMS Grp Inst, SCMS Sch Engn \& Technol, Kochi,
   INDIA, AUG 10-13, 2015}},
Abstract = {{Human face recognition based on geometrical structure has been an area
   of interest among researchers for the past few decades especially in
   pattern recognition. 3D Face recognition systems are of interest in this
   context. The main advantage of 3D Face recognition is the availability
   of geometrical information of the face structure which is more or less
   unique for a subject. This paper focuses on the problems of person
   identification using 3D Face data. Use of unregistered 3D Face data for
   feature extraction significantly increases the operational speed of the
   system with huge database enrollment. In this work, unregistered Face
   data, i.e. both texture and depth is fed to a classifier in spectral
   representations of the same data. 2-D Discrete Contourlet Transform and
   2-D Discrete Fourier Transform is used here for the spectral
   representation which forms the feature matrix. Fusion of texture and
   depth statistical information of face is proposed in this paper since
   the individual schemes are of lower performance. Application of
   statistical method seems to degrade the performance of the system when
   applied to texture data and was effective in the case of depth data.
   Fusion of the matching scores proves that the recognition accuracy can
   be improved significantly by fusion of scores of multiple
   representations. FRAV3D database is used for testing the algorithm.}},
DOI = {{10.1007/978-3-319-23036-8\_36}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-23036-8; 978-3-319-23035-1}},
Unique-ID = {{ISI:000381427400036}},
}

@article{ ISI:000369518500015,
Author = {Ouamane, A. and Belahcene, M. and Benakcha, A. and Bourennane, S. and
   Taleb-Ahmed, A.},
Title = {{Robust multimodal 2D and 3D face authentication using local feature
   fusion}},
Journal = {{SIGNAL IMAGE AND VIDEO PROCESSING}},
Year = {{2016}},
Volume = {{10}},
Number = {{1}},
Pages = {{129-137}},
Month = {{JAN}},
Abstract = {{In this work, we present a robust face authentication approach merging
   multiple descriptors and exploiting both 3D and 2D information. First,
   we correct the heads rotation in 3D by iterative closest point
   algorithm, followed by an efficient preprocessing phase. Then, we
   extract different features namely: multi-scale local binary patterns
   (MSLBP), novel statistical local features (SLF), Gabor wavelets, and
   scale invariant feature transform (SIFT). The principal component
   analysis followed by enhanced fisher linear discriminant model is used
   for dimensionality reduction and classification. Finally, fusion at the
   score level is carried out using two-class support vector machines.
   Extensive experiments are conducted on the CASIA 3D faces database. The
   evaluation of individual descriptors clearly showed the superiority of
   the proposed SLF features. In addition, applying the (3D + 2D)
   multimodal score level fusion, the best result is obtained by combining
   the SLF with the MSLBP + SIFT descriptor yielding in an equal error rate
   of 0.98\% and a recognition rate of RR = 97.22 \%.}},
DOI = {{10.1007/s11760-014-0712-x}},
ISSN = {{1863-1703}},
EISSN = {{1863-1711}},
Unique-ID = {{ISI:000369518500015}},
}

@article{ ISI:000367856500018,
Author = {Bellil, Wajdi and Brahim, Hajer and Ben Amar, Chokri},
Title = {{Gappy wavelet neural network for 3D occluded faces: detection and
   recognition}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{75}},
Number = {{1}},
Pages = {{365-380}},
Month = {{JAN}},
Abstract = {{The first handicap in 3D faces recognizing under unconstrained problem
   is the largest variability of the visual aspect when we use various
   sources. This great variability complicates the task of identifying
   persons from their 3D facial scans and it is the most reason that bring
   to face detection and recognition of the major problems in pattern
   recognition fields, biometrics and computer vision. We propose a new 3D
   face identification and recognition method based on Gappy Wavelet Neural
   Network (GWNN) that is able to provide better accuracy in the presence
   of facial occlusions. The proposed approach consists of three steps: the
   first step is face detection. The second step is to identify and remove
   occlusions. Occluded regions detection is done by considering that
   occlusions can be defined as local face deformations. These deformations
   are detected by a comparison between the input facial test wavelet
   coefficients and wavelet coefficients of generic face model formed by
   the mean data base faces. They are beneficial for neighborhood
   relationships between pixels rotation, dilation and translation
   invariant. Then, occluded regions are refined by removing wavelet
   coefficient above a certain threshold. Finally, the last stage of
   processing and retrieving is made based on wavelet neural network to
   recognize and to restore 3D occluded regions that gathers the most. The
   experimental results on this challenging database demonstrate that the
   proposed approach improves recognition rate performance from 93.57 to
   99.45 \% which represents a competitive result compared to the state of
   the art.}},
DOI = {{10.1007/s11042-014-2294-6}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000367856500018}},
}

@article{ ISI:000367181400024,
Author = {Westphalen, Antonio C. and Noworolski, Susan M. and Harisinghani, Mukesh
   and Jhaveri, Kartik S. and Raman, Steve S. and Rosenkrantz, Andrew B.
   and Wang, Zhen J. and Zagoria, Ronald J. and Kurhanewicz, John},
Title = {{High-Resolution 3-T Endorectal Prostate MRI: A Multireader Study of
   Radiologist Preference and Perceived Interpretive Quality of 2D and 3D
   T2-Weighted Fast Spin-Echo MR Images}},
Journal = {{AMERICAN JOURNAL OF ROENTGENOLOGY}},
Year = {{2016}},
Volume = {{206}},
Number = {{1}},
Pages = {{86-91}},
Month = {{JAN}},
Abstract = {{OBJECTIVE. The goal of this study was to compare the perceived quality
   of 3-T axial T2-weighted high-resolution 2D and high-resolution 3D fast
   spin-echo (FSE) endorectal MR images of the prostate.
   MATERIALS AND METHODS. Six radiologists independently reviewed paired
   3-T axial T2-weighted high-resolution 2D and 3D FSE endorectal MR images
   of the prostates of 85 men in two sessions. In the first session (n =
   85), each reader selected his or her preferred images; in the second
   session (n = 28), they determined their confidence in tumor
   identification and compared the depiction of the prostatic anatomy,
   tumor conspicuity, and subjective intrinsic image quality of images. A
   meta-analysis using a random-effects model, logistic regression, and the
   paired Wilcoxon rank-sum test were used for statistical analyses.
   RESULTS. Three readers preferred the 2D acquisition (67-89\%), and the
   other three preferred the 3D images (70-80\%). The option for one of the
   techniques was not associated with any of the predictor variables. The
   2D FSE images were significantly sharper than 3D FSE (p < 0.001) and
   significantly more likely to exhibit other (nonmotion) artifacts (p =
   0.002). No other statistically significant differences were found.
   CONCLUSION. Our results suggest that there are strong individual
   preferences for the 2D or 3D FSE MR images, but there was a wide
   variability among radiologists. There were differences in image quality
   (image sharpness and presence of artifacts not related to motion) but
   not in the sequences' ability to delineate the glandular anatomy and
   depict a cancerous tumor.}},
DOI = {{10.2214/AJR.14.14065}},
ISSN = {{0361-803X}},
EISSN = {{1546-3141}},
ORCID-Numbers = {{Zagoria, Ronald/0000-0001-6926-4627}},
Unique-ID = {{ISI:000367181400024}},
}

@inproceedings{ ISI:000400012301070,
Author = {Hackel, Timo and Wegner, Jan D. and Schindler, Konrad},
Book-Group-Author = {{IEEE}},
Title = {{Contour detection in unstructured 3D point clouds}},
Booktitle = {{2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2016}},
Pages = {{1610-1618}},
Note = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Seattle, WA, JUN 27-30, 2016}},
Organization = {{IEEE Comp Soc; Comp Vis Fdn}},
Abstract = {{We describe a method to automatically detect contours, i.e. lines along
   which the surface orientation sharply changes, in large-scale outdoor
   point clouds. Contours are important intermediate features for
   structuring point clouds and converting them into high-quality surface
   or solid models, and are extensively used in graphics and mapping
   applications. Yet, detecting them in unstructured, inhomogeneous point
   clouds turns out to be surprisingly difficult, and existing line
   detection algorithms largely fail. We approach contour extraction as a
   two-stage discriminative learning problem. In the first stage, a contour
   score for each individual point is predicted with a binary classifier,
   using a set of features extracted from the point's neighborhood. The
   contour scores serve as a basis to construct an overcomplete graph of
   candidate contours. The second stage selects an optimal set of contours
   from the candidates. This amounts to a further binary classification in
   a higher-order MRF, whose cliques encode a preference for connected
   contours and penalize loose ends. The method can handle point clouds >
   10(7) points in a couple of minutes, and vastly outperforms a baseline
   that performs Canny-style edge detection on a range image representation
   of the point cloud.}},
DOI = {{10.1109/CVPR.2016.178}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-8851-1}},
Unique-ID = {{ISI:000400012301070}},
}

@article{ ISI:000368942000004,
Author = {Bargoti, Suchet and Underwood, James P. and Nieto, Juan I. and
   Sukkarieh, Salah},
Title = {{A Pipeline for Trunk Detection in Trellis Structured Apple Orchards}},
Journal = {{JOURNAL OF FIELD ROBOTICS}},
Year = {{2015}},
Volume = {{32}},
Number = {{8, SI}},
Pages = {{1075-1094}},
Month = {{DEC}},
Note = {{9th International Conference on Field and Service Robotics (FSR),
   Brisbane, AUSTRALIA, NOV 09-11, 2013}},
Abstract = {{The ability of robots to meticulously cover large areas while gathering
   sensor data has widespread applications in precision agriculture. For
   autonomous operations in orchards, a suitable information management
   system is required, within which we can gather and process data relating
   to the state and performance of the crop over time, such as distinct
   yield count, canopy volume, and crop health. An efficient way to
   structure an information system is to discretize it to the individual
   tree, for which tree segmentation/detection is a key component. This
   paper presents a tree trunk detection pipeline for identifying
   individual trees in a trellis structured apple orchard, using
   ground-based lidar and image data. A coarse observation of trunk
   candidates is initially made using a Hough transformation on point cloud
   lidar data. These candidates are projected into the camera images, where
   pixelwise classification is used to update their likelihood of being a
   tree trunk. Detection is achieved by using a hidden semi-Markov model to
   leverage from contextual information provided by the repetitive
   structure of an orchard. By repeating this over individual orchard rows,
   we are able to build a tree map over the farm, which can be either GPS
   localized or represented topologically by the row and tree number. The
   pipeline was evaluated at a commercial apple orchard near Melbourne,
   Australia. Data were collected at different times of year, covering an
   area of 1.6 ha containing different apple varieties planted on two types
   of trellis systems: a vertical I-trellis structure and a Guttingen
   V-trellis structure. The results show good trunk detection performance
   for both apple varieties and trellis structures during the preharvest
   season (87-96\% accuracy) and near perfect trunk detection performance
   (99\% accuracy) during the flowering season. (C) 2015 Wiley Periodicals,
   Inc.}},
DOI = {{10.1002/rob.21583}},
ISSN = {{1556-4959}},
EISSN = {{1556-4967}},
ORCID-Numbers = {{Underwood, James/0000-0003-0189-0706}},
Unique-ID = {{ISI:000368942000004}},
}

@article{ ISI:000365704000006,
Author = {St-Onge, Benoit and Audet, Felix-Antoine and Begin, Jean},
Title = {{Characterizing the Height Structure and Composition of a Boreal Forest
   Using an Individual Tree Crown Approach Applied to Photogrammetric Point
   Clouds}},
Journal = {{FORESTS}},
Year = {{2015}},
Volume = {{6}},
Number = {{11}},
Pages = {{3899-3922}},
Month = {{NOV}},
Abstract = {{Photogrammetric point clouds (PPC) obtained by stereomatching of aerial
   photographs now have a resolution sufficient to discern individual
   trees. We have produced such PPCs of a boreal forest and delineated
   individual tree crowns using a segmentation algorithm applied to the
   canopy height model derived from the PPC and a lidar terrain model. The
   crowns were characterized in terms of height and species (spruce, fir,
   and deciduous). Species classification used the 3D shape of the single
   crowns and their reflectance properties. The same was performed on a
   lidar dataset. Results show that the quality of PPC data generally
   approaches that of airborne lidar. For pixel-based canopy height models,
   viewing geometry in aerial images, forest structure (dense vs. open
   canopies), and composition (deciduous vs. conifers) influenced the
   quality of the 3D reconstruction of PPCs relative to lidar.
   Nevertheless, when individual tree height distributions were analyzed,
   PPC-based results were very similar to those extracted from lidar. The
   random forest classification (RF) of individual trees performed better
   in the lidar case when only 3D metrics were used (83\% accuracy for
   lidar, 79\% for PPC). However, when 3D and intensity or multispectral
   data were used together, the accuracy of PPCs (89\%) surpassed that of
   lidar (86\%).}},
DOI = {{10.3390/f6113899}},
ISSN = {{1999-4907}},
Unique-ID = {{ISI:000365704000006}},
}

@article{ ISI:000359029900025,
Author = {Liang, Ronghua and Shen, Wenjia and Li, Xiao-Xin and Wang, Haixia},
Title = {{Bayesian multi-distribution-based discriminative feature extraction for
   3D face recognition}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2015}},
Volume = {{320}},
Pages = {{406-417}},
Month = {{NOV 1}},
Abstract = {{Due to the difficulties associated with the collection of 3D samples, 3D
   face recognition technologies often have to work with smaller than
   desirable sample sizes. With the aim of enlarging the training number
   for each subject, we divide each training image into several patches.
   However, this immediately introduces two further problems for 3D models:
   high computational cost and dispersive features caused by the divided 3D
   image patches. We therefore first map 3D face images into 2D depth
   images, which greatly reduces the dimension of the samples. Though the
   depth images retain most of the robust features of 3D images, such as
   pose and illumination invariance, they lose many discriminative features
   of the original 3D samples. In this study, we propose a Bayesian
   learning framework to extract the discriminative features from the depth
   images. Specifically, we concentrate the features of the intra-class
   patches to a mean feature by maximizing the multivariate Gaussian
   likelihood function, and, simultaneously, enlarge the distances between
   the inter-class mean features by maximizing the exponential priori
   distribution of the mean features. For classification, we use the
   nearest neighbor classifier combined with the Mahalanobis distance to
   calculate the distance between the features of the test image and items
   in the training set. Experiments on two widely-used 3D face databases
   demonstrate the efficiency and accuracy of our proposed method compared
   to relevant state-of-the-art methods. Published by Elsevier Inc.}},
DOI = {{10.1016/j.ins.2015.03.063}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
Unique-ID = {{ISI:000359029900025}},
}

@article{ ISI:000363075300013,
Author = {Kankare, Ville and Liang, Xinlian and Vastaranta, Mikko and Yu, Xiaowei
   and Holopainen, Markus and Hyyppa, Juha},
Title = {{Diameter distribution estimation with laser scanning based multisource
   single tree inventory}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2015}},
Volume = {{108}},
Pages = {{161-171}},
Month = {{OCT}},
Abstract = {{Tree detection and tree species recognition are bottlenecks of the
   airborne remote sensing-based single tree inventories. The effect of
   these factors in forest attribute estimation can be reduced if airborne
   measurements are aided with tree mapping information that is collected
   from the ground. The main objective here was to demonstrate the use of
   terrestrial laser scanning-derived (TLS) tree maps in aiding airborne
   laser scanning-based (ALS) single tree inventory (multisource single
   tree inventory, MS-STI) and its capability in predicting diameter
   distribution in various forest conditions. Automatic measurement of TLS
   point clouds provided the tree maps and the required reference
   information from the tree attributes. The study area was located in Evo,
   Finland, and the reference data was acquired from 27 different sample
   plots with varying forest conditions. The workflow of MS-STI included:
   (1) creation of automatic tree map from TLS point clouds, (2) automatic
   diameter at breast height (DBH) measurement from TLS point clouds, (3)
   individual tree detection (ITD) based on ALS, (4) matching the ITD
   segments to the field-measured reference, (5) ALS point cloud metric
   extraction from the single tree segments and (6) DBH estimation based on
   the derived metrics. MS-STI proved to be accurate and efficient method
   for DBH estimation and predicting diameter distribution. The overall
   accuracy (root mean squared error, RMSE) of the DBH was 36.9 mm. Results
   showed that the DBH accuracy decreased if the tree density (trees/ha)
   increased. The highest accuracies were found in old-growth forests (tree
   densities less than 500 stems/ha). MS-STI resulted in the best
   accuracies regarding Norway spruce (Picea abies (L.) H.
   Karst.)-dominated forests (RMSE of 29.9 mm). Diameter distributions were
   predicted with low error indices, thereby resulting in a good fit
   compared to the reference. Based on the results, diameter distribution
   estimation with MS-STI is highly dependent on the forest structure and
   the accuracy of the tree maps that are used. The most important
   development step in the future for the MS-STI and automatic measurements
   of the TLS point cloud is to develop tree species recognition methods
   and further develop tree detection techniques. The possibility of using
   MLS or harvester data as a basis for the required tree maps should also
   be assessed in the future. (C) 2015 International Society for
   Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2015.07.007}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Vastaranta, Mikko/K-9656-2018}},
ORCID-Numbers = {{Vastaranta, Mikko/0000-0001-6552-9122}},
Unique-ID = {{ISI:000363075300013}},
}

@article{ ISI:000360999400005,
Author = {Patil, Hemprasad and Kothari, Ashwin and Bhurchandi, Kishor},
Title = {{3-D face recognition: features, databases, algorithms and challenges}},
Journal = {{ARTIFICIAL INTELLIGENCE REVIEW}},
Year = {{2015}},
Volume = {{44}},
Number = {{3}},
Pages = {{393-441}},
Month = {{OCT}},
Abstract = {{Face recognition is being widely accepted as a biometric technique
   because of its non-intrusive nature. Despite extensive research on 2-D
   face recognition, it suffers from poor recognition rate due to pose,
   illumination, expression, ageing, makeup variations and occlusions. In
   recent years, the research focus has shifted toward face recognition
   using 3-D facial surface and shape which represent more discriminating
   features by the virtue of increased dimensionality. This paper presents
   an extensive survey of recent 3-D face recognition techniques in terms
   of feature detection, classifiers as well as published algorithms that
   address expression and occlusion variation challenges followed by our
   critical comments on the published work. It also summarizes remarkable
   3-D face databases and their features used for performance evaluation.
   Finally we suggest vital steps of a robust 3-D face recognition system
   based on the surveyed work and identify a few possible directions for
   research in this area.}},
DOI = {{10.1007/s10462-015-9431-0}},
ISSN = {{0269-2821}},
EISSN = {{1573-7462}},
Unique-ID = {{ISI:000360999400005}},
}

@article{ ISI:000358893300004,
Author = {Korez, Robert and Ibragimov, Bulat and Likar, Bostjan and Pernus, Franjo
   and Vrtovec, Tomaz},
Title = {{A Framework for Automated Spine and Vertebrae Interpolation-Based
   Detection and Model-Based Segmentation}},
Journal = {{IEEE TRANSACTIONS ON MEDICAL IMAGING}},
Year = {{2015}},
Volume = {{34}},
Number = {{8, SI}},
Pages = {{1649-1662}},
Month = {{AUG}},
Abstract = {{Automated and semi-automated detection and segmentation of spinal and
   vertebral structures from computed tomography (CT) images is a
   challenging task due to a relatively high degree of anatomical
   complexity, presence of unclear boundaries and articulation of vertebrae
   with each other, as well as due to insufficient image spatial
   resolution, partial volume effects, presence of image artifacts,
   intensity variations and low signal-to-noise ratio. In this paper, we
   describe a novel framework for automated spine and vertebrae detection
   and segmentation from 3-D CT images. A novel optimization technique
   based on interpolation theory is applied to detect the location of the
   whole spine in the 3-D image and, using the obtained location of the
   whole spine, to further detect the location of individual vertebrae
   within the spinal column. The obtained vertebra detection results
   represent a robust and accurate initialization for the subsequent
   segmentation of individual vertebrae, which is performed by an improved
   shape-constrained deformable model approach. The framework was evaluated
   on two publicly available CT spine image databases of 50 lumbar and 170
   thoracolumbar vertebrae. Quantitative comparison against corresponding
   reference vertebra segmentations yielded an overall mean
   centroid-to-centroid distance of 1.1 mm and Dice coefficient of 83.6\%
   for vertebra detection, and an overall mean symmetric surface distance
   of 0.3 mm and Dice coefficient of 94.6\% for vertebra segmentation. The
   results indicate that by applying the proposed automated detection and
   segmentation framework, vertebrae can be successfully detected and
   accurately segmented in 3-D from CT spine images.}},
DOI = {{10.1109/TMI.2015.2389334}},
ISSN = {{0278-0062}},
EISSN = {{1558-254X}},
Unique-ID = {{ISI:000358893300004}},
}

@article{ ISI:000357545400014,
Author = {Schmitt, Michael and Shahzad, Muhammad and Zhu, Xiao Xiang},
Title = {{Reconstruction of individual trees from multi-aspect TomoSAR data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2015}},
Volume = {{165}},
Pages = {{175-185}},
Month = {{AUG}},
Abstract = {{The localization and reconstruction of individual trees as well as the
   extraction of their geometrical parameters is an important field of
   research in both forestry and remote sensing. While the current
   state-of-the-art mostly focuses on the exploitation of optical imagery
   and airborne LiDAR data, modern SAR sensors have not yet met the
   interest of the research community in that regard. This paper presents a
   prototypical processing chain for the reconstruction of individual
   deciduous trees: First, single-pass multi-baseline InSAR data acquired
   from multiple aspect angles are used for the generation of a layover-
   and shadow-free 3D point cloud by tomographic SAR processing. The
   resulting point cloud is then segmented by unsupervised mean shift
   clustering, before ellipsoid models are fitted to the points of each
   cluster. From these 3D ellipsoids the relevant geometrical tree
   parameters are extracted. Evaluation with respect to a manually derived
   reference dataset prove that almost 74\% of all trees are successfully
   segmented and reconstructed, thus providing a promising perspective for
   further research toward individual tree recognition from SAR data. (C)
   2015 The Authors. Published by Elsevier Inc This is an open access
   article under the CC BY-NC-ND license.}},
DOI = {{10.1016/j.rse.2015.05.012}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000357545400014}},
}

@article{ ISI:000356107200001,
Author = {Wang, Chao and Cho, Yong K. and Kim, Changwan},
Title = {{Automatic BIM component extraction from point clouds of existing
   buildings for sustainability applications}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2015}},
Volume = {{56}},
Pages = {{1-13}},
Month = {{AUG}},
Abstract = {{Building information models (BIMs) are increasingly being applied
   throughout a building's lifecycle for various applications, such as
   progressive construction monitoring and defect detection, building
   renovation, energy simulation, and building system analysis in the
   Architectural, Engineering, Construction, and Facility Management
   (AEC/FM) domains. In conventional approaches, as-is BIM is primarily
   manually created from point clouds, which is labor-intensive, costly,
   and time consuming. This paper proposes a method for automatically
   extracting building geometries from unorganized point clouds. The
   collected raw data undergo data downsizing, boundary detection, and
   building component categorization, resulting in the building components
   being recognized as individual objects and their visualization as
   polygons. The results of tests conducted on three collected as-is
   building data to validate the technical feasibility and evaluate the
   performance of the proposed method indicate that it can simplify and
   accelerate the as-is building model from the point cloud creation
   process. (C) 2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.autcon.2015.04.001}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
ResearcherID-Numbers = {{Jeong, Yongwook/N-7413-2016}},
Unique-ID = {{ISI:000356107200001}},
}

@article{ ISI:000355894900023,
Author = {Weinmann, Martin and Jutzi, Boris and Hinz, Stefan and Mallet, Clement},
Title = {{Semantic point cloud interpretation based on optimal neighborhoods,
   relevant features and efficient classifiers}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2015}},
Volume = {{105}},
Pages = {{286-304}},
Month = {{JUL}},
Abstract = {{3D scene analysis in terms of automatically assigning 3D points a
   respective semantic label has become a topic of great importance in
   photogrammetiy, remote sensing, computer vision and robotics. In this
   paper, we address the issue of how to increase the distinctiveness of
   geometric features and select the most relevant ones among these for 3D
   scene analysis. We present a new, fully automated and versatile
   framework composed of four components: (i) neighborhood selection, (ii)
   feature extraction, (iii) feature selection and (iv) classification. For
   each component, we consider a variety of approaches which allow
   applicability in terms of simplicity, efficiency and reproducibility, so
   that end-users can easily apply the different components and do not
   require expert knowledge in the respective domains. In a detailed
   evaluation involving 7 neighborhood definitions, 21 geometric features,
   7 approaches for feature selection, 10 classifiers and 2 benchmark
   datasets, we demonstrate that the selection of optimal neighborhoods for
   individual 3D points significantly improves the results of 3D scene
   analysis. Additionally, we show that the selection of adequate feature
   subsets may even further increase the quality of the derived results
   while significantly reducing both processing time and memory
   consumption. (C) 2015 International Society for Photogrammetry and
   Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.isprsjprs.2015.01.016}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ORCID-Numbers = {{Weinmann, Martin/0000-0002-8654-7546
   Mallet, Clement/0000-0002-2675-165X}},
Unique-ID = {{ISI:000355894900023}},
}

@article{ ISI:000448059300007,
Author = {Rodriguez, Julian S. and Prieto, Flavio},
Title = {{Analysis and comparison of the cone curvature descriptor in facial
   gesture recognition tasks}},
Journal = {{INGENIERIA}},
Year = {{2015}},
Volume = {{20}},
Number = {{2}},
Pages = {{261-275}},
Month = {{JUL-DEC}},
Abstract = {{This article presents the results of analyzing the behavior of the Cone
   Curvature shape descriptor (CC) in the task of recognition of facial
   expressions in 3D images. The CC descriptor is a representation of the
   3D model computed from a set of waves modeling for each vertex of a
   polygon mesh. The 3D Facial Expression Database (BU-3DFE) was used,
   which contains images with six facial expressions. With the use of the
   CC descriptor, the expressions were recognized in an average percentage
   of 76.67\% with a neural network, and of 78.88\% with a Bayesian
   classifier. By combining the CC descriptor with other descriptors such
   as DESIRE and Spherical Spin Image, it was achieved an average
   percentage of gesture recognition of 90.27\% and 97.2\%, using the
   mentioned classifiers.}},
DOI = {{10.14483/udistrital.jour.reving.2015.2.a06}},
ISSN = {{0121-750X}},
EISSN = {{2344-8393}},
Unique-ID = {{ISI:000448059300007}},
}

@article{ ISI:000357869200002,
Author = {Duan, Zhugeng and Zhao, Dan and Zeng, Yuan and Zhao, Yujin and Wu,
   Bingfang and Zhu, Jianjun},
Title = {{Assessing and Correcting Topographic Effects on Forest Canopy Height
   Retrieval Using Airborne LiDAR Data}},
Journal = {{SENSORS}},
Year = {{2015}},
Volume = {{15}},
Number = {{6}},
Pages = {{12133-12155}},
Month = {{JUN}},
Abstract = {{Topography affects forest canopy height retrieval based on airborne
   Light Detection and Ranging (LiDAR) data a lot. This paper proposes a
   method for correcting deviations caused by topography based on
   individual tree crown segmentation. The point cloud of an individual
   tree was extracted according to crown boundaries of isolated individual
   trees from digital orthophoto maps (DOMs). Normalized canopy height was
   calculated by subtracting the elevation of centres of gravity from the
   elevation of point cloud. First, individual tree crown boundaries are
   obtained by carrying out segmentation on the DOM. Second, point clouds
   of the individual trees are extracted based on the boundaries. Third,
   precise DEM is derived from the point cloud which is classified by a
   multi-scale curvature classification algorithm. Finally, a height
   weighted correction method is applied to correct the topological
   effects. The method is applied to LiDAR data acquired in South China,
   and its effectiveness is tested using 41 field survey plots. The results
   show that the terrain impacts the canopy height of individual trees in
   that the downslope side of the tree trunk is elevated and the upslope
   side is depressed. This further affects the extraction of the location
   and crown of individual trees. A strong correlation was detected between
   the slope gradient and the proportions of returns with height
   differences more than 0.3, 0.5 and 0.8 m in the total returns, with
   coefficient of determination R-2 of 0.83, 0.76, and 0.60 (n = 41),
   respectively.}},
DOI = {{10.3390/s150612133}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000357869200002}},
}

@article{ ISI:000356741800007,
Author = {Weinmann, M. and Urban, S. and Hinz, S. and Jutzi, B. and Mallet, C.},
Title = {{Distinctive 2D and 3D features for automated large-scale scene analysis
   in urban areas}},
Journal = {{COMPUTERS \& GRAPHICS-UK}},
Year = {{2015}},
Volume = {{49}},
Pages = {{47-57}},
Month = {{JUN}},
Abstract = {{We propose a new methodology for large-scale urban 3D scene analysis in
   terms of automatically assigning 3D points the respective semantic
   labels. The methodology focuses on simplicity and reproducibility of the
   involved components as well as performance in terms of accuracy and
   computational efficiency. Exploiting a variety of low-level 2D and 3D
   geometric features, we further improve their distinctiveness by
   involving individual neighborhoods of optimal size. Due to the use of
   individual neighborhoods, the methodology is not tailored to a specific
   dataset, but in principle designed to process point clouds with a few
   millions of 3D points. Consequently, an extension has to be introduced
   for analyzing huge 3D point clouds with possibly billions of points for
   a whole city. For this purpose, we propose an extension which is based
   on an appropriate partitioning of the scene and thus allows a successive
   processing in a reasonable time without affecting the quality of the
   classification results. We demonstrate the performance of our
   methodology on two labeled benchmark datasets with respect to
   robustness, efficiency, and scalability. (C) 2015 Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.cag.2015.01.006}},
ISSN = {{0097-8493}},
EISSN = {{1873-7684}},
ORCID-Numbers = {{Mallet, Clement/0000-0002-2675-165X
   Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000356741800007}},
}

@article{ ISI:000355003600024,
Author = {Lindsay, Kaitlin E. and Ruehli, Frank J. and Deleon, Valerie Burke},
Title = {{Revealing the Face of an Ancient Egyptian: Synthesis of Current and
   Traditional Approaches to Evidence-Based Facial Approximation}},
Journal = {{ANATOMICAL RECORD-ADVANCES IN INTEGRATIVE ANATOMY AND EVOLUTIONARY
   BIOLOGY}},
Year = {{2015}},
Volume = {{298}},
Number = {{6}},
Pages = {{1144-1161}},
Month = {{JUN}},
Abstract = {{The technique of forensic facial approximation, or reconstruction, is
   one of many facets of the field of mummy studies. Although far from a
   rigorous scientific technique, evidence-based visualization of
   antemortem appearance may supplement radiological, chemical,
   histological, and epidemiological studies of ancient remains. Published
   guidelines exist for creating facial approximations, but few
   approximations are published with documentation of the specific process
   and references used. Additionally, significant new research has taken
   place in recent years which helps define best practices in the field.
   This case study records the facial approximation of a 3,000-year-old
   ancient Egyptian woman using medical imaging data and the digital
   sculpting program, ZBrush. It represents a synthesis of current
   published techniques based on the most solid anatomical and/or
   statistical evidence. Through this study, it was found that although
   certain improvements have been made in developing repeatable,
   evidence-based guidelines for facial approximation, there are many
   proposed methods still awaiting confirmation from comprehensive studies.
   This study attempts to assist artists, anthropologists, and forensic
   investigators working in facial approximation by presenting the
   recommended methods in a chronological and usable format. Anat Rec,
   298:1144-1161, 2015. (c) 2015 Wiley Periodicals, Inc.}},
DOI = {{10.1002/ar.23146}},
ISSN = {{1932-8486}},
EISSN = {{1932-8494}},
Unique-ID = {{ISI:000355003600024}},
}

@article{ ISI:000354623800005,
Author = {Mehanna, Ahmed Mohamed and Baki, Fatthi Abdel and Eid, Mohamed and Negm,
   Magdy},
Title = {{Comparison of different computed tomography post-processing modalities
   in assessment of various middle ear disorders}},
Journal = {{EUROPEAN ARCHIVES OF OTO-RHINO-LARYNGOLOGY}},
Year = {{2015}},
Volume = {{272}},
Number = {{6}},
Pages = {{1357-1370}},
Month = {{JUN}},
Abstract = {{Several anatomic structures of the middle ear are not optimally depicted
   in the standard axial and coronal planes. Several 2D and 3D
   image-processing modalities are currently available for CT examinations
   in clinical radiology departments. Till now 3D reconstructions of the
   temporal bone have not been widely used yet, and attracted only academic
   interest. The aim of this study was to compare axial (source images), 2D
   and 3DCT post-processing modalities, and to evaluate the value of 3D
   reconstructed images/virtual endoscopy (VE) in assessment of various
   middle ear disorders for identification of the best modality/view for
   assessment of a particular middle ear structure or pathology. 40
   patients with various middle ear disorders, planned for surgical
   intervention were included in prospective study. Multi-slice CT was
   performed for all patients. Scans were acquired in the axial plane. The
   axial source datasets were utilized for generation of 2D reformations
   and 3D reconstructed images. All studied images were divided into three
   categories: axial (source images), 2D reformations (MPR and
   sliding-thin-slab MIP) and 3D reconstruction (virtual endoscopy). The
   visibility of middle ear structures and pathologies with each modality
   were scored qualitatively using three-point scoring system in reference
   to operative findings. Stapes superstructure and footplate,
   incudostapedial joint, oval and round windows, tympanic segment of the
   facial nerve and tegmen were not optimally depicted in the axial plane.
   Sinus tympani and facial recess were best visualized with axial images
   or VE. 3D reconstruction/VE allowed good visualization of all parts of
   ossicular chain except stapes superstructure. Regarding pathologic
   changes, 2D reformations and 3D reconstructed images allowed better
   visualization of erosion of ossicles and tegmen. 3D reconstruction/VE
   did not allow detection of foci of otospongiosis. 2D reformations can be
   considered the mainstay in assessment of most middle ear structures and
   pathologies. 3D reconstruction/VE seems to provide a useful method for a
   preoperative general overview of the middle ear anatomy, particularly
   for the ossicular chain, round window and retrotympanum.}},
DOI = {{10.1007/s00405-014-2920-y}},
ISSN = {{0937-4477}},
EISSN = {{1434-4726}},
ORCID-Numbers = {{Eid, Mohamed/0000-0002-4830-9010}},
Unique-ID = {{ISI:000354623800005}},
}

@article{ ISI:000355288200010,
Author = {Vuollo, Ville and Sidlauskas, Mantas and Sidlauskas, Antanas and Harila,
   Virpi and Salomskiene, Loreta and Zhurov, Alexei and Holmstrom, Lasse
   and Pirttiniemi, Pertti and Heikkinen, Tuomo},
Title = {{Comparing Facial 3D Analysis With DNA Testing to Determine Zygosities of
   Twins}},
Journal = {{TWIN RESEARCH AND HUMAN GENETICS}},
Year = {{2015}},
Volume = {{18}},
Number = {{3}},
Pages = {{306-313}},
Month = {{JUN}},
Abstract = {{The aim of this study was to compare facial 3D analysis to DNA testing
   in twin zygosity determinations. Facial 3D images of 106 pairs of young
   adult Lithuanian twins were taken with a stereophotogrammetric device
   (3dMD, Atlanta, Georgia) and zygosity was determined according to
   similarity of facial form. Statistical pattern recognition methodology
   was used for classification. The results showed that in 75\% to 90\% of
   the cases, zygosity determinations were similar to DNA-based results.
   There were 81 different classification scenarios, including 3 groups, 3
   features, 3 different scaling methods, and 3 threshold levels. It
   appeared that coincidence with 0.5 mm tolerance is the most suitable
   feature for classification. Also, leaving out scaling improves results
   in most cases. Scaling was expected to equalize the magnitude of
   differences and therefore lead to better recognition performance. Still,
   better classification features and a more effective scaling method or
   classification in different facial areas could further improve the
   results. In most of the cases, male pair zygosity recognition was at a
   higher level compared with females. Erroneously classified twin pairs
   appear to be obvious outliers in the sample. In particular, faces of
   young dizygotic (DZ) twins may be so similar that it is very hard to
   define a feature that would help classify the pair as DZ.
   Correspondingly, monozygotic (MZ) twins may have faces with quite
   different shapes. Such anomalous twin pairs are interesting exceptions,
   but they form a considerable portion in both zygosity groups.}},
DOI = {{10.1017/thg.2015.16}},
ISSN = {{1832-4274}},
EISSN = {{1839-2628}},
ResearcherID-Numbers = {{Zhurov, Alexei/P-4410-2014
   }},
ORCID-Numbers = {{Zhurov, Alexei/0000-0002-5594-0740
   Pirttiniemi, Pertti/0000-0003-4514-836X}},
Unique-ID = {{ISI:000355288200010}},
}

@article{ ISI:000352571800033,
Author = {Mills, Graham and Fotopoulos, Georgia},
Title = {{Rock Surface Classification in a Mine Drift Using Multiscale Geometric
   Features}},
Journal = {{IEEE GEOSCIENCE AND REMOTE SENSING LETTERS}},
Year = {{2015}},
Volume = {{12}},
Number = {{6}},
Pages = {{1322-1326}},
Month = {{JUN}},
Abstract = {{Scale-dependent statistical depictions of surface morphology offer the
   potential to parameterize complex geometrical scaling relationships with
   greater detail than traditional fractal measures. Using multiscale
   operators, it is possible to identify points belonging to rough
   discontinuous surfaces in noisy point clouds solely on the basis of
   their local geometry. Many strategies for point cloud feature
   classification have been developed since the proliferation of laser
   scanning systems. Most of the techniques which are applicable to natural
   scenes employ external data sources such as hyperspectral imagery,
   return pulse intensity, and waveform data. In this letter, multiscale
   geometric parameters are used to identify individual point observations
   corresponding to rock surfaces in point clouds acquired by terrestrial
   laser scanning in scenes with man-made clutter and scanning artifacts.
   Three multiscale operators, namely, the approximate shape and density of
   a defined neighborhood and the distance of its mean point from its
   geometric center, are fused into a single feature vector. The procedure
   is demonstrated using real point cloud data acquired in a mine drift,
   with the goal of identifying points belonging to the rock face obscured
   by an overlying wire support mesh. Using the extra-trees classifier,
   extraneous returns caused by the mesh were excluded from the point cloud
   with a 97\% success rate, while 87\% of the desired surface points were
   retained.}},
DOI = {{10.1109/LGRS.2015.2398814}},
ISSN = {{1545-598X}},
EISSN = {{1558-0571}},
Unique-ID = {{ISI:000352571800033}},
}

@article{ ISI:000353891500010,
Author = {Kashani, Alireza G. and Crawford, Patrick S. and Biswas, Sufal K. and
   Graettinger, Andrew J. and Grau, David},
Title = {{Automated Tornado Damage Assessment and Wind Speed Estimation Based on
   Terrestrial Laser Scanning}},
Journal = {{JOURNAL OF COMPUTING IN CIVIL ENGINEERING}},
Year = {{2015}},
Volume = {{29}},
Number = {{3}},
Month = {{MAY}},
Abstract = {{There are more than 1,000 tornadoes in the United States each year, yet
   engineers do not typically design for tornadoes because of insufficient
   information about wind loads. Collecting building-level damage data in
   the aftermath of tornadoes can improve the understanding of tornado
   winds, but these data are difficult to collect because of safety, time,
   and access constraints. This study presents and tests an automated
   geographic information system (GIS) method using postevent point cloud
   data collected by terrestrial scanners and preevent aerial images to
   calculate the percentage of roof and wall damage and estimate wind
   speeds at an individual building scale. Simulations determined that for
   typical point cloud density (>25points/m2), a GIS raster cell size of
   40-50cm resulted in less than 10\% error in damaged roof and wall
   detection. Data collected after recent tornadoes were used to correlate
   wind speed estimates and the percent of detected damage. The developed
   method estimated wind speeds from damage data collected after the 2011
   Tuscaloosa, AL tornado at finer scales than the typical large-scale
   assessments done by reconnaissance engineers.}},
DOI = {{10.1061/(ASCE)CP.1943-5487.0000389}},
Article-Number = {{04014051}},
ISSN = {{0887-3801}},
EISSN = {{1943-5487}},
Unique-ID = {{ISI:000353891500010}},
}

@article{ ISI:000353807700021,
Author = {Hoevenaren, Inge A. and Maal, Thomas J. J. and Krikken, E. and de Haan,
   A. F. J. and Berge, S. J. and Ulrich, D. J. O.},
Title = {{Development of a three-dimensional hand model using 3D
   stereophotogrammetry: Evaluation of landmark reproducibility}},
Journal = {{JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY}},
Year = {{2015}},
Volume = {{68}},
Number = {{5}},
Pages = {{709-716}},
Month = {{MAY}},
Abstract = {{BACKGROUND: Using three-dimensional (3D) photography, exact images of
   the human body can be produced. Over the last few years, this technique
   is mainly being developed in the field of maxillofacial reconstructive
   surgery, creating fusion images with computed tomography (CT) data for
   accurate planning and prediction of treatment outcome. However, in hand
   surgery, 3D photography is not yet being used in clinical settings.
   METHODS: The aim of this study was to develop a valid method for imaging
   the hand using 3D stereophotogrammetry. The reproducibility of 30 soft
   tissue landmarks was determined using 3D stereophotogrammetric images.
   Analysis was performed by two observers on 20 3D photographs.
   Reproducibility and reliability of the landmark identification were
   determined using statistical analysis.
   RESULTS: The intra-and interobserver reproducibility of the landmarks
   were high. This study showed a high reliability coefficient for
   intraobserver (1.00) and interobserver reliability (0.99).
   Identification of the landmarks on the palmar aspect of individual
   fingers was more precise than the identification of landmarks of the
   thumb.
   CONCLUSIONS: This study shows that 3D photography can safely produce
   accurate and reproducible images of the hand, which makes the technique
   a reliable method for soft tissue analysis. 3D images can be a helpful
   tool in pre- and postoperative evaluation of reconstructive trauma
   surgery, in aesthetic surgery of the hand, and for educational purposes.
   The use in everyday practice of hand surgery and the concept of fusing
   3D photography images with radiologic images of the interior hand
   structures needs to be further explored. (C) 2014 British Association of
   Plastic, Reconstructive and Aesthetic Surgeons. Published by Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.bjps.2014.12.025}},
ISSN = {{1748-6815}},
EISSN = {{1878-0539}},
ResearcherID-Numbers = {{Ulrich, D.J.O./H-8099-2014
   Maal, Thomas/L-4497-2015
   de Haan, Antonius/L-4344-2015
   Berge, S.J./H-8011-2014}},
ORCID-Numbers = {{Maal, Thomas/0000-0002-1702-4733
   }},
Unique-ID = {{ISI:000353807700021}},
}

@article{ ISI:000352441700012,
Author = {Urbanova, Petra and Hejna, Petr and Jurda, Mikolas},
Title = {{Testing photogrammetry-based techniques for three-dimensional surface
   documentation in forensic pathology}},
Journal = {{FORENSIC SCIENCE INTERNATIONAL}},
Year = {{2015}},
Volume = {{250}},
Pages = {{77-86}},
Month = {{MAY}},
Abstract = {{Three-dimensional surface technologies particularly close range
   photogrammetry and optical surface scanning have recently advanced into
   affordable, flexible and accurate techniques. Forensic postmortem
   investigation as performed on a daily basis, however, has not yet fully
   benefited from their potentials. In the present paper, we tested two
   approaches to 3D external body documentation - digital camera-based
   photogrammetry combined with commercial Agisoft PhotoScan (R) software
   and stereophotogrammetry-based Vectra H1 (R), a portable handheld
   surface scanner. In order to conduct the study three human subjects were
   selected, a living person, a 25-year-old female, and two forensic cases
   admitted for postmortem examination at the Department of Forensic
   Medicine, Hradec Kralove, Czech Republic (both 63-year-old males), one
   dead to traumatic, self-inflicted, injuries (suicide by hanging), the
   other diagnosed with the heart failure.
   All three cases were photographed in 3608 manner with a Nikon 7000
   digital camera and simultaneously documented with the handheld scanner.
   In addition to having recorded the pre-autopsy phase of the forensic
   cases, both techniques were employed in various stages of autopsy. The
   sets of collected digital images (approximately 100 per case) were
   further processed to generate point clouds and 3D meshes. Final 3D
   models (a pair per individual) were counted for numbers of points and
   polygons, then assessed visually and compared quantitatively using ICP
   alignment algorithm and a cloud point comparison technique based on
   closest point to point distances.
   Both techniques were proven to be easy to handle and equally laborious.
   While collecting the images at autopsy took around 20 min, the
   post-processing was much more time-demanding and required up to 10 h of
   computation time. Moreover, for the full-body scanning the
   post-processing of the handheld scanner required rather time-consuming
   manual image alignment. In all instances the applied approaches produced
   high-resolution photorealistic, real sized or easy to calibrate 3D
   surface models. Both methods equally failed when the scanned body
   surface was covered with body hair or reflective moist areas. Still, it
   can be concluded that single camera close range photogrammetry and
   optical surface scanning using Vectra H1 scanner represent relatively
   low-cost solutions which were shown to be beneficial for postmortem body
   documentation in forensic pathology. (C) 2015 Elsevier Ireland Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.forsciint.2015.03.005}},
ISSN = {{0379-0738}},
EISSN = {{1872-6283}},
ORCID-Numbers = {{Urbanova, Petra/0000-0001-9321-3360}},
Unique-ID = {{ISI:000352441700012}},
}

@article{ ISI:000350839700012,
Author = {Tamrin, K. F. and Rahmatullah, B. and Samuri, S. M.},
Title = {{An experimental investigation of three-dimensional particle aggregation
   using digital holographic microscopy}},
Journal = {{OPTICS AND LASERS IN ENGINEERING}},
Year = {{2015}},
Volume = {{68}},
Pages = {{93-103}},
Month = {{MAY}},
Abstract = {{The tendency of particles to aggregate depends on particle-particle and
   particle-fluid interactions. These interactions can be characterized but
   it requires accurate 3D measurements of particle distributions. We
   introduce the application of an off-axis digital holographic microscopy
   for measuring distributions of dense micrometer (2 mu m) particles in a
   liquid solution. We demonstrate that digital holographic microscopy is
   capable of recording the instantaneous 3D position of particles in a
   flow volume. A new reconstruction method that aids identification of
   particle images was used in this work. About 62\% of the expected number
   of particles within the interrogated flow volume was detected. Based on
   the 3D position of individual particles, the tendency of particle to
   aggregate is investigated. Results show that relatively few particles
   (around 5-10 of a cohort of 1500) were aggregates. This number did not
   change significantly with time. (C) 2014 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.optlaseng.2014.12.011}},
ISSN = {{0143-8166}},
EISSN = {{1873-0302}},
Unique-ID = {{ISI:000350839700012}},
}

@article{ ISI:000349271500019,
Author = {Azazi, Amal and Lotfi, Syaheerah Lebai and Venkat, Ibrahim and
   Fernandez-Martinez, Fernando},
Title = {{Towards a robust affect recognition: Automatic facial expression
   recognition in 3D faces}},
Journal = {{EXPERT SYSTEMS WITH APPLICATIONS}},
Year = {{2015}},
Volume = {{42}},
Number = {{6}},
Pages = {{3056-3066}},
Month = {{APR 15}},
Abstract = {{Facial expressions are a powerful tool that communicates a person's
   emotional state and subsequently his/her intentions. Compared to 2D face
   images, 3D face images offer more granular cues that are not available
   in the 2D images. However, one major setback of 3D faces is that they
   impose a higher dimensionality than 2D faces. In this paper, we attempt
   to address this problem by proposing a fully automatic 3D facial
   expression recognition model that tackles the high dimensionality
   problem in a twofold solution. First, we transform the 3D faces into the
   2D plane using conformal mapping. Second, we propose a Differential
   Evolution (DE) based optimization algorithm to select the optimal facial
   feature set and the classifier parameters simultaneously. The optimal
   features are selected from a pool of Speed Up Robust Features (SURF)
   descriptors of all the prospective facial points. The proposed model
   yielded an average recognition accuracy of 79\% using the Bosphorus
   database and 79.36\% using the BU-3DFE database. In addition, we exploit
   the facial muscular movements to enhance the probability estimation (PE)
   of Support Vector Machine (SVM). Joint application of feature selection
   with the proposed enhanced PE (EPE) yielded an average recognition
   accuracy of 84\% using the Bosphorus database and 85.81\% using the
   BU-3DFE database, which is statistically significantly better (at p <
   0.01 and p < 0.001, respectively) if compared to the individual exploit
   of the optimal features only. (C) 2014 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.eswa.2014.10.042}},
ISSN = {{0957-4174}},
EISSN = {{1873-6793}},
ResearcherID-Numbers = {{Fernandez-Martinez, Fernando/M-2935-2014
   }},
ORCID-Numbers = {{Fernandez-Martinez, Fernando/0000-0003-3877-0089
   Lebai Lutfi, Syaheerah/0000-0001-7349-0061}},
Unique-ID = {{ISI:000349271500019}},
}

@article{ ISI:000351880000193,
Author = {Amaral, Carlos P. and Simoes, Marco A. and Castelo-Branco, Miguel S.},
Title = {{Neural Signals Evoked by Stimuli of Increasing Social Scene Complexity
   Are Detectable at the Single-Trial Level and Right Lateralized}},
Journal = {{PLOS ONE}},
Year = {{2015}},
Volume = {{10}},
Number = {{3}},
Month = {{MAR 25}},
Abstract = {{Classification of neural signals at the single-trial level and the study
   of their relevance in affective and cognitive neuroscience are still in
   their infancy. Here we investigated the neurophysiological correlates of
   conditions of increasing social scene complexity using 3D human models
   as targets of attention, which may also be important in autism research.
   Challenging single-trial statistical classification of EEG neural
   signals was attempted for detection of oddball stimuli with increasing
   social scene complexity. Stimuli had an oddball structure and were as
   follows: 1) flashed schematic eyes, 2) simple 3D faces flashed between
   averted and non-averted gaze (only eye position changing), 3) simple 3D
   faces flashed between averted and non-averted gaze (head and eye
   position changing), 4) animated avatar alternated its gaze direction to
   the left and to the right (head and eye position), 5) environment with 4
   animated avatars all of which change gaze and one of which is the target
   of attention. We found a late (> 300 ms) neurophysiological oddball
   correlate for all conditions irrespective of their complexity as
   assessed by repeated measures ANOVA. We attempted single-trial detection
   of this signal with automatic classifiers and obtained a significant
   balanced accuracy classification of around 79\%, which is noteworthy
   given the amount of scene complexity. Lateralization analysis showed a
   specific right lateralization only for more complex realistic social
   scenes. In sum, complex ecological animations with social content elicit
   neurophysiological events which can be characterized even at the
   single-trial level. These signals are right lateralized. These finding
   paves the way for neuroscientific studies in affective neuroscience
   based on complex social scenes, and given the detectability at the
   single trial level this suggests the feasibility of brain computer
   interfaces that can be applied to social cognition disorders such as
   autism.}},
DOI = {{10.1371/journal.pone.0121970}},
Article-Number = {{e0121970}},
ISSN = {{1932-6203}},
ResearcherID-Numbers = {{Amaral, Carlos/J-4282-2019
   Castelo-Branco, Miguel/F-3866-2019
   }},
ORCID-Numbers = {{Amaral, Carlos/0000-0002-0493-9192
   Castelo-Branco, Miguel/0000-0003-4364-6373
   Simoes, Marco/0000-0003-3713-2464}},
Unique-ID = {{ISI:000351880000193}},
}

@article{ ISI:000351796000002,
Author = {Ming, Yue},
Title = {{Robust regional bounding spherical descriptor for 3D face recognition
   and emotion analysis}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2015}},
Volume = {{35}},
Pages = {{14-22}},
Month = {{MAR}},
Abstract = {{3D face recognition and emotion analysis play important roles in many
   fields of communication and edutainment An effective facial descriptor,
   with higher discriminating capability for face recognition and higher
   descriptiveness for facial emotion analysis, is a challenging issue.
   However, in the practical applications, the descriptiveness and
   discrimination are independent and contradictory to each other. 3D
   facial data provide a promising way to balance these two aspects. In
   this paper, a robust regional bounding spherical descriptor (RBSR) is
   proposed to facilitate 3D face recognition and emotion analysis. In our
   framework, we first segment a group of regions on each 3D facial point
   cloud by shape index and spherical bands on the human face. Then the
   corresponding facial areas are projected to regional bounding spheres to
   obtain our regional descriptor. Finally, a regional and global
   regression mapping (RGRM) technique is employed to the weighted regional
   descriptor for boosting the classification accuracy. Three largest
   available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the
   performance comparison and the experimental results show a consistently
   better performance for 3D face recognition and emotion analysis. (C)
   2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2014.12.003}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000351796000002}},
}

@article{ ISI:000351134600001,
Author = {Mengesha, Taye and Hawkins, Michael and Nieuwenhuis, Maarten},
Title = {{Validation of terrestrial laser scanning data using conventional forest
   inventory methods}},
Journal = {{EUROPEAN JOURNAL OF FOREST RESEARCH}},
Year = {{2015}},
Volume = {{134}},
Number = {{2}},
Pages = {{211-222}},
Month = {{MAR}},
Abstract = {{The application of terrestrial laser scanning (TLS) in capturing forest
   inventory parameters such as diameter at breast height, height and
   diameters along stem profiles, and in monitoring forest growth, was
   investigated and validated by comparison with conventionally measured
   individual tree parameters and plot-level forest growth in a stand of
   Sitka spruce (Picea sitchensis (Bong.) Carr.) in Ireland. The data
   acquisition for all the plots with different tree sizes and different
   slopes was carried out using a terrestrial laser scanner (FARO LS 800
   HE80) in November 2007 and November 2009, using the same plot centres
   and measurement procedures. The point cloud data were processed with
   Autostem (TM) software. The results showed that TLS enables the
   acquisition of forest stand parameters with an acceptable accuracy.
   Pruning of the lower branches did not improve tree recognition and the
   number of (partly) occluded trees stayed the same. Over the 2-year
   period, the average difference between the volume increment of the trees
   visible to the scanner derived using the conventional method and
   Autostem (TM) was 4.77 m(3) ha(-1) and resulted in scanner-derived
   estimates that were lower than the estimates obtained by conventional
   method by 6.1 \%. Using a simple correction factor to account for
   occlusion in the laser scanner data, the difference between these
   estimates for all trees in the stand became an over-estimation by 6.96
   m(3) ha(-1) (8.1 \%). At heights up along the stems > 15 m, the errors
   in stem diameter estimates started to escalate.}},
DOI = {{10.1007/s10342-014-0844-0}},
ISSN = {{1612-4669}},
EISSN = {{1612-4677}},
Unique-ID = {{ISI:000351134600001}},
}

@article{ ISI:000349588900008,
Author = {Bolkart, Timo and Wuhrer, Stefanie},
Title = {{3D faces in motion: Fully automatic registration and statistical
   analysis}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2015}},
Volume = {{131}},
Pages = {{100-115}},
Month = {{FEB}},
Abstract = {{This paper presents a representation of 3D facial motion sequences that
   allows performing statistical analysis of 3D face shapes in motion. The
   resulting statistical analysis is applied to automatically generate
   realistic facial animations and to recognize dynamic facial expressions.
   To perform statistical analysis of 3D facial shapes in motion over
   different subjects and different motion sequences, a large database of
   motion sequences needs to be brought in full correspondence. Existing
   algorithms that compute correspondences between 3D facial motion
   sequences either require manual input or suffer from instabilities
   caused by drift. For large databases, algorithms that require manual
   interaction are not practical. We propose an approach to robustly
   compute correspondences between a large set of facial motion sequences
   in a fully automatic way using a multilinear model as statistical prior.
   In order to register the motion sequences, a good initialization is
   needed. We obtain this initialization by introducing a landmark
   prediction method for 3D motion sequences based on Markov Random Fields.
   Using this motion sequence registration, we find a compact
   representation of each motion sequence consisting of one vector of
   coefficients for identity and a high dimensional curve for expression.
   Based on this representation, we synthesize new motion sequences and
   perform expression recognition. We show experimentally that the obtained
   registration is of high quality, where 56\% of all vertices are at
   distance at most 1 mm from the input data, and that our synthesized
   motion sequences look realistic. (C) 2014 Elsevier Inc. All rights
   reserved.}},
DOI = {{10.1016/j.cviu.2014.06.013}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
Unique-ID = {{ISI:000349588900008}},
}

@article{ ISI:000349308600007,
Author = {Li, Ye and Wang, YingHui and Wang, BingBo and Sui, LianSheng},
Title = {{Nose tip detection on three-dimensional faces using pose-invariant
   differential surface features}},
Journal = {{IET COMPUTER VISION}},
Year = {{2015}},
Volume = {{9}},
Number = {{1}},
Pages = {{75-84}},
Month = {{FEB}},
Abstract = {{Three-dimensional (3D) facial data offer the potential to overcome the
   difficulties caused by the variation of head pose and illumination in 2D
   face recognition. In 3D face recognition, localisation of nose tip is
   essential to face normalisation, face registration and pose correction
   etc. Most of the existing methods of nose tip detection on 3D face deal
   mainly with frontal or near-frontal poses or are rotation sensitive.
   Many of them are training-based or model-based. In this study, a novel
   method of nose tip detection is proposed. Using pose-invariant
   differential surface features - high-order and low-order curvatures, it
   can detect nose tip on 3D faces under various poses automatically and
   accurately. Moreover, it does not require training and does not depend
   on any particular model. Experimental results on GavabDB verify the
   robustness and accuracy of the proposed method.}},
DOI = {{10.1049/iet-cvi.2014.0070}},
ISSN = {{1751-9632}},
EISSN = {{1751-9640}},
Unique-ID = {{ISI:000349308600007}},
}

@inproceedings{ ISI:000382327100024,
Author = {Wang, Yuanyuan and Zhu, Xiao Xiang},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane, G}},
Title = {{SEMANTIC INTERPRETATION OF INSAR ESTIMATES USING OPTICAL IMAGES WITH
   APPLICATION TO URBAN INFRASTRUCTURE MONITORING}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W3}},
Pages = {{153-160}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{Synthetic aperture radar interferometry (InSAR) has been an established
   method for long term large area monitoring. Since the launch of
   meter-resolution spaceborne SAR sensors, the InSAR community has shown
   that even individual buildings can be monitored in high level of detail.
   However, the current deformation analysis still remains at a primitive
   stage of pixel-wise motion parameter inversion and manual identification
   of the regions of interest. We are aiming at developing an automatic
   urban infrastructure monitoring approach by combining InSAR and the
   semantics derived from optical images, so that the deformation analysis
   can be done systematically in the semantic/object level. This paper
   explains how we transfer the semantic meaning derived from optical image
   to the InSAR point clouds, and hence different semantic classes in the
   InSAR point cloud can be automatically extracted and monitored. Examples
   on bridges and railway monitoring are demonstrated.}},
DOI = {{10.5194/isprsarchives-XL-3-W3-153-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000382327100024}},
}

@inproceedings{ ISI:000382327100086,
Author = {Gorte, Ben and Elberink, Sander Oude and Sirmacek, Beril and Wang, Jinhu},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane, G}},
Title = {{IQPC 2015 TRACK: TREE SEPARATION AND CLASSIFICATION IN MOBILE MAPPING
   LIDAR DATA}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W3}},
Pages = {{607-612}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{The European FP7 project IQmulus yearly organizes several processing
   contests, where submissions are requested for novel algorithms for point
   cloud and other big geodata processing. This paper describes the set-up
   and execution of a contest having the purpose to evaluate
   state-of-the-art algorithms for Mobile Mapping System point clouds, in
   order to detect and identify (individual) trees. By the nature of MMS
   these are trees in the vicinity of the road network (rather than in
   forests). Therefore, part of the challenge is distinguishing between
   trees and other objects, such as buildings, street furniture, cars etc.
   Three submitted segmentation and classification algorithms are thus
   evaluated.}},
DOI = {{10.5194/isprsarchives-XL-3-W3-607-2015}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Elberink, Sander Oude/D-3829-2009
   }},
ORCID-Numbers = {{Wang, Jinhu/0000-0001-7473-4152
   Gorte, Bernardus/0000-0001-8953-6394}},
Unique-ID = {{ISI:000382327100086}},
}

@inproceedings{ ISI:000380533900053,
Author = {Song, Soohwan and Jo, Sungho},
Editor = {{Kim, JH and Yang, W and Jo, J and Sincak, P and Myung, H}},
Title = {{Traversability Classification Using Super-voxel Method in Unstructured
   Terrain}},
Booktitle = {{ROBOT INTELLIGENCE TECHNOLOGY ANDAPPLICATIONS 3}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{345}},
Pages = {{595-604}},
Note = {{3rd International Conference on Robot Intelligence Technology and
   Applications, Beijing, PEOPLES R CHINA, NOV 06-08, 2014}},
Abstract = {{Estimating the traversability of terrain in an unstructured outdoor
   environment is one of the challenging issues in autonomous vehicles.
   When dealing with a large 3D point cloud, the computational cost of
   processing all of the individual points is very high. Thus voxelization
   methods are used extensively. In this paper, we propose a more
   fine-grained voxelization algorithm in the context of unstructured
   terrain classification. While the current shape of a voxel is a
   fixed-length cubic, we construct a flexible shape voxel which has
   spatial and geometrical properties. Furthermore, we propose a new shape
   histogram feature that represents the statistical characteristics of 3D
   points. The proposed method was tested using data obtained from
   unstructured outdoor environments for performance evaluation.}},
DOI = {{10.1007/978-3-319-16841-8\_53}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-16841-8; 978-3-319-16840-1}},
Unique-ID = {{ISI:000380533900053}},
}

@inproceedings{ ISI:000382326300052,
Author = {Shahzad, M. and Zhu, X. X.},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Rottensteiner, F and Yang, M and Christophe, S and Coltekin, A and Bredif, M}},
Title = {{RECONSTRUCTION OF BUILDING FOOTPRINTS USING SPACEBORNE TOMOSAR POINT
   CLOUDS}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{II-3}},
Number = {{W5}},
Pages = {{385-392}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{This paper presents an approach that automatically (but parametrically)
   reconstructs 2-D/3-D building footprints using 3-D synthetic aperture
   radar (SAR) tomography (TomoSAR) point clouds. These point clouds are
   generated by processing SAR image stacks via SAR tomographic inversion.
   The proposed approach reconstructs the building outline by exploiting
   both the roof and facade points. Initial building footprints are derived
   by applying the alpha shapes method on pre-segmented point clusters of
   individual buildings. A recursive angular deviation based refinement is
   then carried out to obtain refined/smoothed 2-D polygonal boundaries. A
   robust fusion framework then fuses the information pertaining to
   building facades to the smoothed polygons. Afterwards, a rectilinear
   building identification procedure is adopted and constraints are added
   to yield geometrically correct and visually aesthetic building shapes.
   The proposed approach is illustrated and validated using TomoSAR point
   clouds generated from a stack of TerraSAR-X high-resolution spotlight
   images from ascending orbit covering approximately 1.5 km(2) area in the
   city of Berlin, Germany.}},
DOI = {{10.5194/isprsannals-II-3-W5-385-2015}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000382326300052}},
}

@inproceedings{ ISI:000380558900020,
Author = {Pang, Guan and Qiu, Rongqi and Huang, Jing and You, Suya and Neumann,
   Ulrich},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Industrial Point Cloud Modeling and Recognition}},
Booktitle = {{2015 14th IAPR International Conference on Machine Vision Applications
   (MVA)}},
Year = {{2015}},
Pages = {{22-25}},
Note = {{14th IAPR International Conference on Machine Vision Applications (MVA),
   Tokyo, JAPAN, MAY 18-22, 2015}},
Organization = {{MVA Organisation; IAPR TC-8; National Institute of Advanced Industrial
   Science and Technology (AIST); The Telecommunications Advancement
   Foundation; KDDI Foundation}},
Abstract = {{3D modeling of point clouds is an important but time-consuming process,
   inspiring extensive research in automatic methods. Prior efforts focus
   on primitive geometry, street structures or indoor objects, but
   industrial data has rarely been pursued. Our work presents a method for
   automatic modeling and recognition of 3D industrial site point clouds,
   dividing the task into 3 separate sub-problems: pipe modeling, plane
   classification, and object recognition. The results are integrated to
   obtain the complete model, revealing some issues during the integration,
   solved by utilizing information gained from each individual process.
   Experiments show that the presented method automatically models large
   and complex industrial scenes with a quality that outperforms leading
   commercial modeling software and is comparable to professional hand-made
   models.}},
ISBN = {{978-4-9011-2214-6}},
Unique-ID = {{ISI:000380558900020}},
}

@inproceedings{ ISI:000380475300697,
Author = {Arora, Sourabh and Chawla, Shikha},
Book-Group-Author = {{IEEE}},
Title = {{An Intensified Approach to Face Recognition through Average Half Face}},
Booktitle = {{2015 ANNUAL IEEE INDIA CONFERENCE (INDICON)}},
Series = {{Annual IEEE India Conference}},
Year = {{2015}},
Note = {{12 IEEE Int C Elect Energy Env Communications Computer Control, New
   Delhi, INDIA, DEC 17-20, 2015}},
Abstract = {{Face recognition has broad excitement in the latest trend in image
   processing. Face recognition refers to identify a specific individual in
   digital image by analyzing and comparing patterns. It has numerous
   benefits which attract every sector but there are some issues such as
   more time consumption and lesser accuracy which degrade the user
   services. To solve this problem we proposed a highly accurate and fast
   method to reduce the execution time. The proposed method uses average
   half face approach because overall system's accuracy is better in it
   rather than using the original full face image. The proposed method can
   be used to recognize both 2D and 3D images. It mainly includes the
   average half face creation, feature detection, full face recognition
   through average half face using distance metrics and finally checking
   system's accuracy along with time consumption. The proposed method is
   based on eye, nose and mouth detection.}},
ISSN = {{2325-940X}},
ISBN = {{978-1-4673-6540-6}},
Unique-ID = {{ISI:000380475300697}},
}

@inproceedings{ ISI:000380475300183,
Author = {Suja, P. and Krishnasri, D. and Tripathi, Shikha},
Book-Group-Author = {{IEEE}},
Title = {{Pose Invariant Method for Emotion Recognition from 3D Images}},
Booktitle = {{2015 ANNUAL IEEE INDIA CONFERENCE (INDICON)}},
Series = {{Annual IEEE India Conference}},
Year = {{2015}},
Note = {{12 IEEE Int C Elect Energy Env Communications Computer Control, New
   Delhi, INDIA, DEC 17-20, 2015}},
Abstract = {{Information about the emotional state of a person can be inferred from
   facial expressions. Emotion recognition has become an active research
   area in recent years in various fields such as Human Robot Interaction (
   HRI), medicine, intelligent vehicle, etc., The challenges in emotion
   recognition from images with pose variations, motivates researchers to
   explore further. In this paper, we have proposed a method based on
   geometric features, considering images of 7 yaw angles (-45 degrees,-30
   degrees,-15 degrees, 0 degrees,+15 degrees,+30 degrees,+45 degrees) from
   BU3DFE database. Most of the work that has been reported considered only
   positive yaw angles. In this work, we have included both positive and
   negative yaw angles. In the proposed method, feature extraction is
   carried out by concatenating distance and angle vectors between the
   feature points, and classification is performed using neural network.
   The results obtained for images with pose variations are encouraging and
   comparable with literature where work has been performed on pitch and
   yaw angles. Using our proposed method non-frontal views achieve similar
   accuracy when compared to frontal view thus making it pose invariant.
   The proposed method may be implemented for pitch and yaw angles in
   future.}},
ISSN = {{2325-940X}},
ISBN = {{978-1-4673-6540-6}},
Unique-ID = {{ISI:000380475300183}},
}

@inproceedings{ ISI:000380605400006,
Author = {Rai, Marwa Chendeb E. L. and Werghi, Naoufel and Al Muhairi, Hassan and
   Alsafar, Habiba},
Book-Group-Author = {{IEEE}},
Title = {{Using facial images for the diagnosis of genetic syndromes: A survey}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, SIGNAL PROCESSING, AND
   THEIR APPLICATIONS (ICCSPA'15)}},
Year = {{2015}},
Note = {{2015 International Conference on Communications, Signal Processing, and
   their Applications (ICCSPA'15), Sharjah, U ARAB EMIRATES, FEB 17-19,
   2015}},
Organization = {{Amer Univ Sharjah; IEEE}},
Abstract = {{The analysis of facial appearance is significant to an early diagnosis
   of medical genetic diseases. The fast development of image processing
   and machine learning techniques facilitates the detection of facial
   dysmorphic features. This paper is a survey of the recent studies
   developed for the screening of genetic abnormalities across the facial
   features obtained from two dimensional and three dimensional images.}},
ISBN = {{978-1-4799-6532-8}},
ResearcherID-Numbers = {{Werghi, Naoufel/D-2398-2018}},
ORCID-Numbers = {{Werghi, Naoufel/0000-0002-5542-448X}},
Unique-ID = {{ISI:000380605400006}},
}

@inproceedings{ ISI:000380407300092,
Author = {Pawar, Asmita A. and Patil, Nitin N.},
Book-Group-Author = {{IEEE}},
Title = {{Recognition of 3-D Faces with Missing Parts and Line Scratch Removal
   using New Technique}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)}},
Year = {{2015}},
Note = {{International Conference on Pervasive Computing (ICPC), Pune, INDIA, JAN
   08-10, 2015}},
Organization = {{IEEE Pune Sect; IEEE Comp Soc; Savitribai Phule Pune Univ; IEEE Commun
   Soc Pune Chapter; Sinhgad Inst; Sakal Times}},
Abstract = {{This paper presents an implementation of face recognition, which is a
   very important task of human face identification. Line scratch detection
   in images is a highly challenging situation because of various
   characteristics of this defect. Few characteristics are considered with
   the different texture and geometry of images. We propose a useful
   algorithm for frame-by-frame line scratch detection in face image which
   deals with 3D approach and a filtering of detection. The temporary
   filtering algorithm can be used to remove false detection due to thin
   vertical structures by detecting the scratches on an image. Experimental
   evaluation can be detecting the lines and scratches on a face image and
   they used to solve this difficult approach. Our method is used with
   missing parts in an image. Three-dimensional face recognition is an
   extended method of facial recognition is considered according with the
   geometry and texture of a face. It has been elaborated that 3D face
   recognition methods can provide high accuracy as well as high detection
   with a comparison of 2D recognition. 3D avoids such mismatch effect of
   2D face recognition algorithms. Additionally, most 3D scanners achieve
   both a 3D mesh and the texture of a face image. This allows combining
   the output of pure 3D matches with the more traditional algorithms of 2D
   face recognition, thus producing better performance.}},
ISBN = {{978-1-4799-6272-3}},
Unique-ID = {{ISI:000380407300092}},
}

@inproceedings{ ISI:000380584900016,
Author = {Lin, Xiangguo and Zhang, Jixian},
Editor = {{Zhang, J and Lu, Z and Zeng, Y}},
Title = {{SEGMENTATION-BASED GROUND POINTS DETECTION FROM MOBILE LASER SCANNING
   POINT CLOUD}},
Booktitle = {{IWIDF 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{47}},
Number = {{W4}},
Pages = {{99-102}},
Note = {{International Workshop on Image and Data Fusion (IWIDF), Kona, HI, JUL
   21-23, 2015}},
Abstract = {{In most Mobile Laser Scanning (MLS) applications, filtering is a
   necessary step. In this paper, a segmentation-based filtering method is
   proposed for MLS point cloud, where a segment rather than an individual
   point is the basic processing unit. Particularly, the MLS point cloud in
   some blocks are clustered into segments by a surface growing algorithm,
   then the object segments are detected and removed. A segment-based
   filtering method is employed to detect the ground segments. Two MLS
   point cloud datasets are used to evaluate the proposed method.
   Experiments indicate that, compared with the classic progressive TIN
   (Triangulated Irregular Network) densification algorithm, the proposed
   method is capable of reducing the omission error, the commission error
   and total error by 3.62\%, 7.87\% and 5.54\% on average, respectively.}},
DOI = {{10.5194/isprsarchives-XL-7-W4-99-2015}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000380584900016}},
}

@inproceedings{ ISI:000380388000083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 4}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380388000083}},
}

@inproceedings{ ISI:000380388000012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 4}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380388000012}},
}

@inproceedings{ ISI:000380379900083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 1}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380379900083}},
}

@inproceedings{ ISI:000380379900012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 1}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380379900012}},
}

@inproceedings{ ISI:000380393900083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 2}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380393900083}},
}

@inproceedings{ ISI:000380393900012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 2}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380393900012}},
}

@inproceedings{ ISI:000380377400083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 3}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380377400083}},
}

@inproceedings{ ISI:000380377400012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 3}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380377400012}},
}

@inproceedings{ ISI:000380390600083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 5}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380390600083}},
}

@inproceedings{ ISI:000380390600012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 5}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380390600012}},
}

@inproceedings{ ISI:000380483800005,
Author = {Liu, Jian and Zhang, Quan and Zhang, Chen and Tang, Chaojing},
Book-Group-Author = {{IEEE}},
Title = {{ROBUST NOSE TIP DETECTION FOR FACE RANGE IMAGES BASED ON LOCAL FEATURES
   IN SCALE- SPACE}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON 3D IMAGING (IC3D)}},
Series = {{International Conference on 3D Imaging}},
Year = {{2015}},
Note = {{International Conference on 3D Imaging (IC3D), Liege, BELGIUM, DEC
   14-15, 2015}},
Organization = {{The Inst of Elect and Elect Engn; Signal Proc Soc; 3D Stereo Media}},
Abstract = {{Being the most distinct feature point in 3D facial landmarks, nose tip
   plays a significant role in 3D facial studies such as face detection,
   face recognition, facial features extraction, face alignment, etc.
   Successful detection of nose tip can facilitate many tasks of 3D facial
   studies. In this paper, we propose a novel method to detect nose tip
   robustly. The method is robust to noise, needs not training, can handle
   large rotations and occlusions. To reduce computational cost, we first
   remove small isolated regions from the input range image, then establish
   scale-space by robust smoothing the preprocessed range image. In each
   scale of the scale-space, the Multi-angle Energy (ME) of each point is
   computed and sorted in descending order. Then the first. points in the
   descending order list are obtained and hierarchical clustering method is
   used to cluster these points. In the first h largest clusters, we can
   find one point with the largest ME. For all scales of the scale-space,
   we get a series of such points which are treated as nose tip candidates.
   For these candidates, we apply hierarchical clustering again. In the
   obtained largest cluster, we compute the mean value of ME. The ME of
   nose tip will be closest to the mean value. We evaluate our method in
   two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The
   experimental results verify the robustness of our method with a high
   nose tip detection rate.}},
ISSN = {{2379-1772}},
ISBN = {{978-1-5090-1265-7}},
Unique-ID = {{ISI:000380483800005}},
}

@inproceedings{ ISI:000378887900113,
Author = {Polewski, Przemyslaw and Yao, Wei and Heurich, Marco and Krzystek, Peter
   and Stilla, Uwe},
Book-Group-Author = {{IEEE}},
Title = {{Active learning approach to detecting standing dead trees from ALS point
   clouds combined with aerial infrared imagery}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   WORKSHOPS (CVPRW)}},
Series = {{IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops}},
Year = {{2015}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{Due to their role in certain essential forest processes, dead trees are
   an interesting object of study within the environmental and forest
   sciences. This paper describes an active learning-based approach to
   detecting individual standing dead trees, known as snags, from ALS point
   clouds and aerial color infrared imagery. We first segment individual
   trees within the 3D point cloud and subsequently find an approximate
   bounding polygon for each tree within the image. We utilize these
   polygons to extract features based on the pixel intensity values in the
   visible and infrared bands, which forms the basis for classifying the
   associated trees as either dead or living. We define a two-step scheme
   of selecting a small subset of training examples from a large initially
   unlabeled set of objects. In the first step, a greedy approximation of
   the kernelized feature matrix is conducted, yielding a smaller pool of
   the most representative objects. We then perform active learning on this
   moderate-sized pool, using expected error reduction as the basic method.
   We explore how the use of semi-supervised classifiers with minimum
   entropy regularizers can benefit the learning process. Based on
   validation with reference data manually labeled on images from the
   Bavarian Forest National Park, our method attains an overall accuracy of
   up to 89\% with less than 100 training examples, which corresponds to
   10\% of the pre-selected data pool.}},
ISSN = {{2160-7508}},
ISBN = {{978-1-4673-6759-2}},
ResearcherID-Numbers = {{Heurich, Marco/O-4653-2014
   Yao, Wei/J-7423-2019}},
ORCID-Numbers = {{Heurich, Marco/0000-0003-0051-2930
   Yao, Wei/0000-0001-7704-0615}},
Unique-ID = {{ISI:000378887900113}},
}

@inproceedings{ ISI:000376674000399,
Author = {Sleiman, J. Bou and Perraud, J. B. and Bousquet, B. and Palka, N. and
   Guillet, J. P. and Mounaix, P.},
Book-Group-Author = {{IEEE}},
Title = {{Chemical imaging and quantification of RDX/PETN mixtures by PLS applied
   on terahertz time-domain spectroscopy}},
Booktitle = {{2015 40TH INTERNATIONAL CONFERENCE ON INFRARED, MILLIMETER AND TERAHERTZ
   WAVES (IRMMW-THZ)}},
Series = {{International Conference on Infrared Millimeter and Terahertz Waves}},
Year = {{2015}},
Note = {{40th International Conference on Infrared, Millimeter, and Terahertz
   Waves (IRMMW-THz), Chinese Univ Hong Kong, Hong Kong, PEOPLES R CHINA,
   AUG 23-28, 2015}},
Organization = {{IEEE; IEEE Microwave Theory \& Tech Soc; Virginal Diodes Inc; TeraView;
   Microtech Instruments Inc; Hong Kong Univ Sci \& Technol, Dept Elect \&
   Comp Engn; Croucher Fdn; Capital Normal Univ; K C Wong Educ Fdn;
   Meetings \& Exhibit Hong Kong; Army Res Off; NSF}},
Abstract = {{Chemometric analysis was applied on terahertz absorbance 3D images, in
   transmission. The goal is to automatically discriminate some explosives
   on images and quantify mixtures of RDX/PETN in the frequency range of
   0.2 - 3 THz. Partial Least Square (PLS) was applied on THz absorbance
   multispectral images to quantify individual product inside pure samples
   and mixtures at each pixel on the image. Then the best score obtained is
   used to display the samples' images and provide the optimal frequencies
   combination for recognition purpose.}},
ISSN = {{2162-2027}},
ISBN = {{978-1-4799-8272-1}},
ResearcherID-Numbers = {{Palka, Norbert/G-9652-2018
   Mounaix, Patrick/E-1653-2012}},
ORCID-Numbers = {{Palka, Norbert/0000-0002-1931-876X
   }},
Unique-ID = {{ISI:000376674000399}},
}

@inproceedings{ ISI:000371977803080,
Author = {Batabyal, Tamal and Vaccari, Andrea and Acton, Scott T.},
Book-Group-Author = {{IEEE}},
Title = {{UGraSP: A UNIFIED FRAMEWORK FOR ACTIVITY RECOGNITION AND PERSON
   IDENTIFICATION USING GRAPH SIGNAL PROCESSING}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{3270-3274}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{With the growing availability and wide distribution of low-cost,
   high-performance 3D imaging sensors, the image analysis community has
   witnessed an increased demand for solutions to the challenges of
   activity recognition and person identification. We propose an integrated
   framework, based on graph signal processing, that simultaneously
   performs both tasks using a single set of features. The novelty of our
   approach is based on the fact that the set of features used for activity
   recognition accommodates person identification without additional
   computation. The analysis is based on the extracted structure-invariant
   graph (skeleton). The Laplacian of the skeleton is used both to identify
   the person and recognize the performed activity. While person
   identification is achieved directly from the analysis of the Laplacian,
   activity recognition is obtained after transformation, into the graph
   spectral domain, of the vectorized form of the skeletal joints 3D
   coordinates. Feature vectors for activity recognition are then derived,
   in this domain, from the covariance matrices evaluated over fixed-length
   sequential video segments. Both classification tasks are implemented
   using linear support vector machines (SVM). When applied to real
   activity datasets, our approach shows an improved performance over the
   existing state-of-the-art.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977803080}},
}

@inproceedings{ ISI:000371977804130,
Author = {Arteaga, Reynaldo J. and Ruuth, Steven J.},
Book-Group-Author = {{IEEE}},
Title = {{LAPLACE-BELTRAMI SPECTRA FOR SHAPE COMPARISON OF SURFACES IN 3D USING
   THE CLOSEST POINT METHOD}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{4511-4515}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{The need to compare separate objects arises in a wide range of
   applications. In one approach for comparing objects, `ShapeDNA' is
   constructed to give a numerical fingerprint representing an individual
   object. Shape-DNA is a cropped set of eigenvalues of the
   Laplace-Beltrami operator for the surface of the object. In this paper,
   we compute the Shape-DNA of surfaces using the closest point method. Our
   approach may be applied to a variety of surface representations
   including triangulations, point clouds and certain analytical shapes. A
   2D multidimensional scaling plot illustrates that similar objects form
   groups based on the Shape-DNAs. Our method has the benefit that it may
   be applied to surfaces defined by dense point clouds without requiring
   the construction of point connectivity.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977804130}},
}

@inproceedings{ ISI:000352725200030,
Author = {Shahzad, M. and Schmitt, M. and Zhu, X. X.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{SEGMENTATION AND CROWN PARAMETER EXTRACTION OF INDIVIDUAL TREES IN AN
   AIRBORNE TOMOSAR POINT CLOUD}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W2}},
Pages = {{205-209}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{The analysis of individual trees is an important field of research in
   the forest remote sensing community. While the current state-of-the-art
   mostly focuses on the exploitation of optical imagery and airborne LiDAR
   data, modern SAR sensors have not yet met the interest of the research
   community in that regard. This paper describes how several critical
   parameters of individual deciduous trees can be extraced from airborne
   multi-aspect TomoSAR point clouds: First, the point cloud is segmented
   by unsupervised mean shift clustering. Then ellipsoid models are fitted
   to the points of each cluster. Finally, from these 3D ellipsoids the
   geometrical tree parameters location, height and crown radius are
   extracted. Evaluation with respect to a manually derived reference
   dataset prove that almost 86\% of all trees are localized, thus
   providing a promising perspective for further research towards
   individual tree recognition from SAR data.}},
DOI = {{10.5194/isprsarchives-XL-3-W2-205-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000352725200030}},
}

@inproceedings{ ISI:000352725200038,
Author = {Vetrivel, A. and Gerke, M. and Kerle, N. and Vosselman, G.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{Segmentation of UAV-based images incorporating 3D point cloud
   information}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W2}},
Pages = {{261-268}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{Numerous applications related to urban scene analysis demand automatic
   recognition of buildings and distinct sub-elements. For example, if
   LiDAR data is available, only 3D information could be leveraged for the
   segmentation. However, this poses several risks, for instance, the
   in-plane objects cannot be distinguished from their surroundings. On the
   other hand, if only image based segmentation is performed, the geometric
   features (e.g., normal orientation, planarity) are not readily
   available. This renders the task of detecting the distinct sub-elements
   of the building with similar radiometric characteristic infeasible. In
   this paper the individual sub-elements of buildings are recognized
   through sub-segmentation of the building using geometric and radiometric
   characteristics jointly. 3D points generated from Unmanned Aerial
   Vehicle (UAV) images are used for inferring the geometric
   characteristics of roofs and facades of the building. However, the
   image-based 3D points are noisy, error prone and often contain gaps.
   Hence the segmentation in 3D space is not appropriate. Therefore, we
   propose to perform segmentation in image space using geometric features
   from the 3D point cloud along with the radiometric features. The initial
   detection of buildings in 3D point cloud is followed by the segmentation
   in image space using the region growing approach by utilizing various
   radiometric and 3D point cloud features. The developed method was tested
   using two data sets obtained with UAV images with a ground resolution of
   around 1-2 cm. The developed method accurately segmented most of the
   building elements when compared to the plane-based segmentation using 3D
   point cloud alone.}},
DOI = {{10.5194/isprsarchives-XL-3-W2-261-2015}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Vosselman, George/D-3985-2009
   Gerke, Markus/A-8791-2012
   Kerle, Norman/A-5508-2010}},
ORCID-Numbers = {{Vosselman, George/0000-0001-8813-8028
   Gerke, Markus/0000-0002-2221-6182
   }},
Unique-ID = {{ISI:000352725200038}},
}

@inproceedings{ ISI:000370974903006,
Author = {Linder, Timm and Wehner, Sven and Arras, Kai O.},
Book-Group-Author = {{IEEE}},
Title = {{Real-Time Full-Body Human Gender Recognition in (RGB)-D Data}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)}},
Series = {{IEEE International Conference on Robotics and Automation ICRA}},
Year = {{2015}},
Pages = {{3039-3045}},
Note = {{IEEE International Conference on Robotics and Automation (ICRA),
   Seattle, WA, MAY 26-30, 2015}},
Organization = {{IEEE}},
Abstract = {{Understanding social context is an important skill for robots that share
   a space with humans. In this paper, we address the problem of
   recognizing gender, a key piece of information when interacting with
   people and understanding human social relations and rules. Unlike
   previous work which typically considered faces or frontal body views in
   image data, we address the problem of recognizing gender in RGB-D data
   from side and back views as well. We present a large, gender-balanced,
   annotated, multi-perspective RGB-D dataset with full-body views of over
   a hundred different persons captured with both the Kinect v1 and Kinect
   v2 sensor. We then learn and compare several classifiers on the Kinect
   v2 data using a HOG baseline, two state-of-the-art deep-learning
   methods, and a recent tessellation-based learning approach. Originally
   developed for person detection in 3D data, the latter is able to learn
   the best selection, location and scale of a set of simple point cloud
   features. We show that for gender recognition, it outperforms the other
   approaches for both standing and walking people while being very
   efficient to compute with classification rates up to 150 Hz.}},
ISSN = {{1050-4729}},
ISBN = {{978-1-4799-6923-4}},
Unique-ID = {{ISI:000370974903006}},
}

@inproceedings{ ISI:000370814200017,
Author = {Bull, Geoff and Gao, Junbin and Antolovich, Michael},
Editor = {{Battiato, S and Coquillart, S and Pettre, J and Laramee, RS and Kerren, A and Braz, J}},
Title = {{Rock Fragment Boundary Detection Using Compressed Random Features}},
Booktitle = {{COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS - THEORY AND
   APPLICATIONS, VISIGRAPP 2014}},
Series = {{Communications in Computer and Information Science}},
Year = {{2015}},
Volume = {{550}},
Pages = {{273-286}},
Note = {{International Joint Conference on Computer Vision, Imaging and Computer
   Graphics Theory and Applications (VISIGRAPP), Lisbon, PORTUGAL, JAN
   05-08, 2014}},
Organization = {{Inst Syst \& Technologies Informat, Control \& Commun; Eurographics;
   IEEE Comp Soc; IEEE VGMT; IEEE TCMC}},
Abstract = {{Sections of the mining industry depend on regular analysis of rock
   fragmentation to detect trends that may affect safety or production. The
   limitations inherent in 2D imaging analysis mean that human input is
   typically needed for delineating individual rock fragments. Although
   recent advances in 3D image processing have diminished the need for
   human input, it is often infeasible for many mines to upgrade their
   existing 2D imaging systems to 3D. Hence there is still a need to
   improve delineation in 2D images. This paper proposes a method for
   delineating rock fragments by classifying compressed Haar-like features
   extracted from small image patches. The optimum size of the image
   patches and the number of compressed features are determined
   empirically. Experimental results show the proposed method gives
   superior results to the commonly used watershed algorithm, and
   compressing features improves computational efficiency such that a
   machine learning approach is practical.}},
DOI = {{10.1007/978-3-319-25117-2\_17}},
ISSN = {{1865-0929}},
ISBN = {{978-3-319-25117-2; 978-3-319-25116-5}},
ResearcherID-Numbers = {{Bull, Geoff/L-2805-2018
   Antolovich, Michael/C-1656-2012
   Gao, Junbin/A-1766-2009}},
ORCID-Numbers = {{Bull, Geoff/0000-0002-9818-5132
   Antolovich, Michael/0000-0003-2601-8332
   Gao, Junbin/0000-0001-9803-0256}},
Unique-ID = {{ISI:000370814200017}},
}

@inproceedings{ ISI:000369099700018,
Author = {Aneja, D. and Vora, S. R. and Camci, E. D. and Shapiro, L. G. and Cox,
   T. C.},
Editor = {{Traina, C and Rodrigues, PP and Kane, B and Marques, PMD and Traina, AJM}},
Title = {{Automated Detection of 3D Landmarks for the Elimination of
   Non-Biological Variation in Geometric Morphometric Analyses}},
Booktitle = {{2015 IEEE 28TH INTERNATIONAL SYMPOSIUM ON COMPUTER-BASED MEDICAL SYSTEMS
   (CBMS)}},
Series = {{IEEE International Symposium on Computer-Based Medical Systems}},
Year = {{2015}},
Pages = {{78-83}},
Note = {{28th IEEE International Symposium on Computer-Based Medical Systems
   (CBMS), Univ Sao Paulo, Sao Paulo, BRAZIL, JUN 22-25, 2015}},
Organization = {{IEEE; IEEE Comp Soc; ICMC; SUS; Ministerio Saude; Governo Fed Patria
   Educadora Brasil; CNPq; Google Brasil; FAPESP}},
Abstract = {{Landmark-based morphometric analyses are used by anthropologists,
   developmental and evolutionary biologists to understand shape and size
   differences (eg. in the cranioskeleton) between groups of specimens. The
   standard, labor intensive approach is for researchers to manually place
   landmarks on 3D image datasets. As landmark recognition is subject to
   inaccuracies of human perception, digitization of landmark coordinates
   is typically repeated (often by more than one person) and the mean
   coordinates are used. In an attempt to improve efficiency and
   reproducibility between researchers, we have developed an algorithm to
   locate landmarks on CT mouse hemi-mandible data. The method is evaluated
   on 3D meshes of 28-day old mice, and results compared to landmarks
   manually identified by experts. Quantitative shape comparison between
   two inbred mouse strains demonstrate that data obtained using our
   algorithm also has enhanced statistical power when compared to data
   obtained by manual landmarking.}},
DOI = {{10.1109/CBMS.2015.86}},
ISSN = {{1063-7125}},
ISBN = {{978-1-4673-6775-2}},
Unique-ID = {{ISI:000369099700018}},
}

@inproceedings{ ISI:000364991200038,
Author = {De Giorgis, Nikolas and Rocca, Luigi and Puppo, Enrico},
Editor = {{Murino, V and Puppo, E}},
Title = {{Scale-Space Techniques for Fiducial Points Extraction from 3D Faces}},
Booktitle = {{IMAGE ANALYSIS AND PROCESSING - ICIAP 2015, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9279}},
Pages = {{421-431}},
Note = {{18th International Conference on Image Analysis and Processing (ICIAP),
   Genoa, ITALY, SEP 07-11, 2015}},
Organization = {{Datalogic; Google Inc; Centro Studi Gruppo Orizzonti Holding; Ansaldo
   Energia; EBIT Esaote; Softeco; eVS embedded Vis Syst S r l; 3DFlow S r
   l; Camelot Biomed Syst S r l; Ist Italiano Tecnologia, Pattern Anal \&
   Comp Vis Dept; Univ Genova; Univ Verona; Camera Commercio Genova; Comune
   Genova}},
Abstract = {{We propose a method for extracting fiducial points from human faces that
   uses 3D information only and is based on two key steps: multi-scale
   curvature analysis, and the reliable tracking of features in a
   scale-space based on curvature. Our scale-space analysis, coupled to
   careful use of prior information based on variability boundaries of
   anthropometric facial proportions, does not require a training step,
   because it makes direct use of morphological characteristics of the
   analyzed surface. The proposed method precisely identifies important
   fiducial points and is able to extract new fiducial points that were
   previously unrecognized, thus paving the way to more effective
   recognition algorithms.}},
DOI = {{10.1007/978-3-319-23231-7\_38}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-23231-7; 978-3-319-23230-0}},
ORCID-Numbers = {{Puppo, Enrico/0000-0001-9780-5283}},
Unique-ID = {{ISI:000364991200038}},
}

@inproceedings{ ISI:000365181700036,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal, JK}},
Title = {{Range Face Image Registration Using ERFI from 3D Images}},
Booktitle = {{PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF
   INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{328}},
Pages = {{323-333}},
Note = {{3rd International Conference on Frontiers in Intelligent Computing -
   Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014}},
Organization = {{Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol \& Sci, CSI Student
   Branch}},
Abstract = {{In this paper, we present a novel and robust approach for 3D faces
   registration based on Energy Range Face Image (ERFI). ERFI is the
   frontal face model for the individual people from the database. It can
   be considered as a mean frontal range face image for each person. Thus,
   the total energy of the frontal range face images has been preserved by
   ERFI. For registration purpose, an interesting point or a land mark,
   which is the nose tip (or `pronasal') from face surface is extracted.
   Then, this landmark is exploited to correct the oriented faces by
   applying the 3D geometrical rotation technique with respect to the ERFI
   model for registration purpose. During the error calculation phase,
   Manhattan distance metric between the localized `pronasal' landmark on
   face image and that of ERFI model is determined on Euclidian space. The
   accuracy is quantified with selection of cut-points `T' on measured
   Manhattan distances along yaw, pitch and roll. The proposed method has
   been tested on Frav3D database and achieved 82.5\% accurate pose
   registration.}},
DOI = {{10.1007/978-3-319-12012-6\_36}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-12012-6; 978-3-319-12011-9}},
ResearcherID-Numbers = {{Bhattacharjee, Debotosh/L-8521-2015
   }},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-4483-706X
   Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000365181700036}},
}

@inproceedings{ ISI:000365181700047,
Author = {Sivasankar, C. and Srinivasan, A.},
Editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal, JK}},
Title = {{A Framework for Human Recognition Based on Locomotive Object Extraction}},
Booktitle = {{PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF
   INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{328}},
Pages = {{431-439}},
Note = {{3rd International Conference on Frontiers in Intelligent Computing -
   Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014}},
Organization = {{Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol \& Sci, CSI Student
   Branch}},
Abstract = {{Moving Object detection based on video, of late has gained momentum in
   the field of research. Moving object detection has extensive application
   areas and is used for monitoring intelligence interaction between human
   and computer, transportation of intelligence, and navigating visual
   robotics, clarity in steering systems. It is also used in various other
   fields for diagnosing, compressing images, reconstructing 3D images,
   retrieving video images and so on. Since surveillance of human movement
   detection is subjective, the human objects are precisely detected to the
   framework proposed for human detection based on the Locomotive Object
   Extraction. The issue of illumination changes and crowded human image is
   discriminated. The image is detected through the detection feature that
   identifies head and shoulder and is the loci for the proposed framework.
   The detection of individual objects has been revamped appreciably over
   the recent years but even now environmental factors and crowd-scene
   detection remains significantly difficult for detection of moving
   object. The proposed framework subtracts the background through Gaussian
   mixture model and the area of significance is extracted. The area of
   significance is transformed to white and black picture by picture
   binarization. Then, Wiener filter is employed to scale the background
   level for optimizing the results of the object in motion. The object is
   finally identified. The performance in every stage is measured and is
   evaluated. The result in each stage is compared and the performance of
   the proposed framework is that of the existing system proves
   satisfactory.}},
DOI = {{10.1007/978-3-319-12012-6\_47}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-12012-6; 978-3-319-12011-9}},
Unique-ID = {{ISI:000365181700047}},
}

@inproceedings{ ISI:000363756900031,
Author = {Ming, Yue and Jin, Yi},
Editor = {{Liu, H and Kubota, N and Zhu, X and Dillmann, R and Zhou, D}},
Title = {{Robust 3D Local SIFT Features for 3D Face Recognition}},
Booktitle = {{INTELLIGENT ROBOTICS AND APPLICATIONS (ICIRA 2015), PT III}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2015}},
Volume = {{9246}},
Pages = {{352-359}},
Note = {{8th International Conference on Intelligent Robotics and Applications
   (ICIRA), Portsmouth, ENGLAND, AUG 24-27, 2015}},
Abstract = {{In this paper, a robust 3D local SIFT feature is proposed for 3D face
   recognition. For preprocessing the original 3D face data, facial
   regional segmentation is first employed by fusing curvature
   characteristics and shape band mechanism. Then, we design a new local
   descriptor for the extracted regions, called 3D local Scale-Invariant
   Feature Transform (3D LSIFT). The key point detection based on 3D LSIFT
   can effectively reflect the geometric characteristic of 3D facial
   surface by encoding the gray and depth information captured by 3D face
   data. Then, 3D LSIFT descriptor extends to describe the discrimination
   on 3D faces. Experimental results based on the common international 3D
   face databases demonstrate the higher-qualified performance of our
   proposed algorithm with effectiveness, robustness, and universality.}},
DOI = {{10.1007/978-3-319-22873-0\_31}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-22873-0; 978-3-319-22872-3}},
Unique-ID = {{ISI:000363756900031}},
}

@inproceedings{ ISI:000361841100052,
Author = {Lin, Yizhou and Hua, Gang and Mordohai, Philippos},
Editor = {{Agapito, L and Bronstein, MM and Rother, C}},
Title = {{Egocentric Object Recognition Leveraging the 3D Shape of the Grasping
   Hand}},
Booktitle = {{COMPUTER VISION - ECCV 2014 WORKSHOPS, PT III}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{8927}},
Pages = {{746-762}},
Note = {{13th European Conference on Computer Vision (ECCV), Zurich, SWITZERLAND,
   SEP 06-12, 2014}},
Abstract = {{We present a systematic study on the relationship between the 3D shape
   of a hand that is about to grasp an object and recognition of the object
   to be grasped. In this paper, we investigate the direction from the
   shape of the hand to object recognition for unimpaired users. Our work
   shows that the 3D shape of a grasping hand from an egocentric point of
   view can help improve recognition of the objects being grasped. Previous
   work has attempted to exploit hand interactions or gaze information in
   the egocentric setting to guide object segmentation. However, all such
   analyses are conducted in 2D. We hypothesize that the 3D shape of a
   grasping hand is highly correlated to the physical attributes of the
   object being grasped. Hence, it can provide very beneficial visual
   information for object recognition. We validate this hypothesis by first
   building a 3D, egocentric vision pipeline to segment and reconstruct
   dense 3D point clouds of the grasping hands. Then, visual descriptors
   are extracted from the point cloud and subsequently fed into an object
   recognition system to recognize the object being grasped. Our
   experiments demonstrate that the 3D hand shape can indeed greatly help
   improve the visual recognition accuracy, when compared with the baseline
   where only 2D image features are utilized.}},
DOI = {{10.1007/978-3-319-16199-0\_52}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-16199-0; 978-3-319-16198-3}},
Unique-ID = {{ISI:000361841100052}},
}

@inproceedings{ ISI:000360175900188,
Author = {Naveen, S. and Moni, R. S.},
Editor = {{Samuel, P}},
Title = {{Multimodal Face Recognition System using Spectral Transformation of 2D
   Texture feature and Statistical processing of Face Range Images}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION AND
   COMMUNICATION TECHNOLOGIES, ICICT 2014}},
Series = {{Procedia Computer Science}},
Year = {{2015}},
Volume = {{46}},
Pages = {{1537-1545}},
Note = {{International Conference on Information and Communication Technologies
   (ICICT), Kochi, INDIA, DEC 03-05, 2014}},
Organization = {{Cochin Uni Sci \& Technol, Sch Engn; TEQIP Phase II}},
Abstract = {{3D Face recognition has been an area of interest for the past few
   decades in pattern recognition. This paper focuses on problems of person
   identification using 3D Face data. Here unregistered Face data, i.e.
   both texture and depth is fed to classifier in spectral representations
   of data. 2D Discrete Fourier Transform (DFT) is used for spectral
   representation. Fusion of scores improves the recognition accuracy
   significantly since use of depth information alone in spectral
   representation was not sufficient to increase accuracy. Statistical
   method seems to degrade performance of system when applied to texture
   data and was effective for depth data. (C) 2015 The Authors. Published
   by Elsevier B.V.}},
DOI = {{10.1016/j.procs.2015.02.078}},
ISSN = {{1877-0509}},
Unique-ID = {{ISI:000360175900188}},
}

@inproceedings{ ISI:000359292400017,
Author = {Karagoz, Burcu and Altan, Hakan and Kamburoglu, Kivanc},
Editor = {{Lilge, LD and Sroka, R}},
Title = {{Terahertz pulsed imaging study of dental caries}},
Booktitle = {{MEDICAL LASER APPLICATIONS AND LASER-TISSUE INTERACTIONS VII}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9542}},
Note = {{Conference on Medical Laser Applications and Laser-Tissue Interactions
   VII, Munich, GERMANY, JUN 21-23, 2015}},
Organization = {{SPIE; Opt Soc America}},
Abstract = {{Current diagnostic techniques in dentistry rely predominantly on X-rays
   to monitor dental caries. Terahertz Pulsed Imaging (TPI) has great
   potential for medical applications since it is a nondestructive imaging
   method. It does not cause any ionization hazard on biological samples
   due to low energy of THz radiation. Even though it is strongly absorbed
   by water which exhibits very unique chemical and physical properties
   that contribute to strong interaction with THz radiation, teeth can
   still be investigated in three dimensions. Recent investigations suggest
   that this method can be used in the early identification of dental
   diseases and imperfections in the tooth structure without the hazards of
   using techniques which rely on x-rays. We constructed a continuous wave
   (CW) and time-domain reflection mode raster scan THz imaging system that
   enables us to investigate various teeth samples in two or three
   dimensions. The samples comprised of either slices of individual tooth
   samples or rows of teeth embedded in wax, and the imaging was done by
   scanning the sample across the focus of the THz beam. 2D images were
   generated by acquiring the intensity of the THz radiation at each pixel,
   while 3D images were generated by collecting the amplitude of the
   reflected signal at each pixel. After analyzing the measurements in both
   the spatial and frequency domains, the results suggest that the THz
   pulse is sensitive to variations in the structure of the samples that
   suggest that this method can be useful in detecting the presence of
   caries.}},
DOI = {{10.1117/12.2183673}},
Article-Number = {{95420N}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-707-4}},
ResearcherID-Numbers = {{Altan, Hakan/A-4036-2017}},
ORCID-Numbers = {{Altan, Hakan/0000-0002-2456-8827}},
Unique-ID = {{ISI:000359292400017}},
}

@article{ ISI:000358942000004,
Author = {Dalponte, Michele and Reyes, Francesco and Kandare, Kaja and Gianelle,
   Damiano},
Title = {{Delineation of Individual Tree Crowns from ALS and Hyperspectral data: a
   comparison among four methods}},
Journal = {{EUROPEAN JOURNAL OF REMOTE SENSING}},
Year = {{2015}},
Volume = {{48}},
Pages = {{365-382}},
Abstract = {{In this paper four different delineation methods based on airborne laser
   scanning (ALS) and hyperspectral data are compared over a forest area in
   the Italian Alps. The comparison was carried out in terms of detected
   trees, while the ALS based methods are compared also in terms of
   attributes estimated (e.g. height). From the experimental results
   emerged that ALS methods outperformed hyperspectral one in terms of tree
   detection rate in two of three cases. The best results were achieved
   with a method based on region growing on an ALS image, and by one based
   on clustering of raw ALS point cloud. Regarding the estimates of the
   tree attributes all the ALS methods provided good results with very high
   accuracies when considering only big trees.}},
DOI = {{10.5721/EuJRS20154821}},
ISSN = {{2279-7254}},
ResearcherID-Numbers = {{Gianelle, Damiano/G-9437-2011
   Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Gianelle, Damiano/0000-0001-7697-5793
   Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000358942000004}},
}

@inproceedings{ ISI:000355583800011,
Author = {Varney, Nina M. and Asari, Vijayan K.},
Editor = {{Casasent, D and Alam, MS}},
Title = {{Volume component analysis for classification of LiDAR data}},
Booktitle = {{OPTICAL PATTERN RECOGNITION XXVI}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9477}},
Note = {{Conference on Optical Pattern Recognition XXVI, Baltimore, MA, APR
   22-23, 2015}},
Organization = {{SPIE}},
Abstract = {{One of the most difficult challenges of working with LiDAR data is the
   large amount of data points that are produced. Analysing these large
   data sets is an extremely time consuming process. For this reason,
   automatic perception of LiDAR scenes is a growing area of research.
   Currently, most LiDAR feature extraction relies on geometrical features
   specific to the point cloud of interest. These geometrical features are
   scene-specific, and often rely on the scale and orientation of the
   object for classification. This paper proposes a robust method for
   reduced dimensionality feature extraction of 3D objects using a volume
   component analysis (VCA) approach.1
   This VCA approach is based on principal component analysis (PCA). PCA is
   a method of reduced feature extraction that computes a covariance matrix
   from the original input vector. The eigenvectors corresponding to the
   largest eigenvalues of the covariance matrix are used to describe an
   image. Block-based PCA is an adapted method for feature extraction in
   facial images because PCA, when performed in local areas of the image,
   can extract more significant features than can be extracted when the
   entire image is considered. The image space is split into several of
   these blocks, and PCA is computed individually for each block.
   This VCA proposes that a LiDAR point cloud can be represented as a
   series of voxels whose values correspond to the point density within
   that relative location. From this voxelized space, block-based PCA is
   used to analyze sections of the space where the sections, when combined,
   will represent features of the entire 3-D object. These features are
   then used as the input to a support vector machine which is trained to
   identify four classes of objects, vegetation, vehicles, buildings and
   barriers with an overall accuracy of 93.8\%}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-593-3}},
Unique-ID = {{ISI:000355583800011}},
}

@inproceedings{ ISI:000353328200021,
Author = {Qu, Chengchao and Monari, Eduardo and Schuchert, Tobias and Beyerer,
   Juergen},
Editor = {{Lam, EY and Niel, KS}},
Title = {{Realistic Texture Extraction for 3D Face Models Robust to Self-Occlusion}},
Booktitle = {{IMAGE PROCESSING: MACHINE VISION APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9405}},
Note = {{Conference on Image Processing - Machine Vision Applications VIII, San
   Francisco, CA, FEB 10-11, 2015}},
Organization = {{Soc Imaging Sci \& Technol; SPIE}},
Abstract = {{In the context of face modeling, probably the most well-known approach
   to represent 3D faces is the 3D Morphable Model (3DMM). When 3DMM is
   fitted to a 2D image, the shape as well as the texture and illumination
   parameters are simultaneously estimated. However, if real facial texture
   is needed, texture extraction from the 2D image is necessary. This paper
   addresses the possible problems in texture extraction of a single image
   caused by self-occlusion. Unlike common approaches that leverage the
   symmetric property of the face by mirroring the visible facial part,
   which is sensitive to inhomogeneous illumination, this work first
   generates a virtual texture map for the skin area iteratively by
   averaging the color of neighbored vertices. Although this step creates
   unrealistic, overly smoothed texture, illumination stays constant
   between the real and virtual texture. In the second pass, the mirrored
   texture is gradually blended with the real or generated texture
   according to the visibility. This scheme ensures a gentle handling of
   illumination and yet yields realistic texture. Because the blending area
   only relates to non-informative area, main facial features still have
   unique appearance in different face halves. Evaluation results reveal
   realistic rendering in novel poses robust to challenging illumination
   conditions and small registration errors.}},
Article-Number = {{94050P}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-495-0}},
Unique-ID = {{ISI:000353328200021}},
}

@inproceedings{ ISI:000352727000035,
Author = {Weinmann, M. and Schmidt, A. and Mallet, C. and Hinz, S. and
   Rottensteiner, F. and Jutzi, B.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{CONTEXTUAL CLASSIFICATION OF POINT CLOUD DATA BY EXPLOITING INDIVIDUAL
   3D NEIGBOURHOODS}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. II}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{2-3}},
Number = {{W4}},
Pages = {{271-278}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{The fully automated analysis of 3D point clouds is of great importance
   in photogrammetry, remote sensing and computer vision. For reliably
   extracting objects such as buildings, road inventory or vegetation, many
   approaches rely on the results of a point cloud classification, where
   each 3D point is assigned a respective semantic class label. Such an
   assignment, in turn, typically involves statistical methods for feature
   extraction and machine learning. Whereas the different components in the
   processing workflow have extensively, but separately been investigated
   in recent years, the respective connection by sharing the results of
   crucial tasks across all components has not yet been addressed. This
   connection not only encapsulates the interrelated issues of neighborhood
   selection and feature extraction, but also the issue of how to involve
   spatial context in the classification step. In this paper, we present a
   novel and generic approach for 3D scene analysis which relies on (i)
   individually optimized 3D neighborhoods for (ii) the extraction of
   distinctive geometric features and (iii) the contextual classification
   of point cloud data. For a labeled benchmark dataset, we demonstrate the
   beneficial impact of involving contextual information in the
   classification process and that using individual 3D neighborhoods of
   optimal size significantly increases the quality of the results for both
   pointwise and contextual classification.}},
DOI = {{10.5194/isprsannals-II-3-W4-271-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000352727000035}},
}

@article{ ISI:000347672900002,
Author = {da Neiva, Marcelo Baiao and Soares, Alvaro Cavalheiro and Lisboa,
   Cinthia de Oliveira and Vilella, Oswaldo de Vasconcellos and Motta,
   Alexandre Trindade},
Title = {{Evaluation of cephalometric landmark identification on CBCT multiplanar
   and 3D reconstructions}},
Journal = {{ANGLE ORTHODONTIST}},
Year = {{2015}},
Volume = {{85}},
Number = {{1}},
Pages = {{11-17}},
Month = {{JAN}},
Abstract = {{Objective: To evaluate the reliability of three-dimensional (3D)
   landmark identification in cone-beam computed tomography (CBCT) using
   two different visualization techniques.
   Materials and Methods: Twelve CBCT images were randomly selected. Three
   observers independently repeated three times the identification of 30
   landmarks using 3D reconstructions and 28 landmarks using multiplanar
   views. The values of the coordinates X, Y, and Z of each point were
   obtained and the intraclass correlation coefficient (ICC) was
   calculated.
   Results: The ICC of the 3D visualization was rated >0.90 in 67.76\% and
   45.56\%, and <= 0.45 in 13.33\% and 14.46\% of the intraobserver and
   interobserver assessments, respectively. The ICC of the multiplanar
   visualization was rated >0.90 in 82.16\% and 78.56\% and <= 0.45 in only
   16.7\% and 8.33\% of the intraobserver and interobserver assessments,
   respectively. An individual landmark classification was done according
   to ICC values.
   Conclusions: The frequency of highly reliable values was greater for
   multiplanar than 3D reconstructions. Overall, lower reliability was
   found for points on the condyle and higher reliability for those on the
   midsagittal plane. Depending on the anatomic region, the observer must
   choose the most reliable type of image visualization.}},
DOI = {{10.2319/120413-891.1}},
ISSN = {{0003-3219}},
EISSN = {{1945-7103}},
Unique-ID = {{ISI:000347672900002}},
}

@inproceedings{ ISI:000377348700061,
Author = {Liu, Jian and Zhang, Quan and Tang, Chaojing},
Book-Author = {{Xu, B}},
Title = {{CoMES: A Novel Method for Robust Nose Tip Detection in Face Range Images}},
Booktitle = {{2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION
   CONTROL CONFERENCE (IAEAC)}},
Year = {{2015}},
Pages = {{309-315}},
Note = {{IEEE Advanced Information Technology, Electronic and Automation Control
   Conference (IAEAC), Chongqing, PEOPLES R CHINA, DEC 19-20, 2015}},
Organization = {{IEEE; IEEE Beijing Sect; Global Union Acad Sci \& Technol; Chongqing
   Global Union Acad Sci \& Technol}},
Abstract = {{As the most distinct feature point in facial landmarks, nose tip plays a
   significant role in 3D facial studies. Successful detection of nose tip
   can facilitate many 3D facial studies tasks. In this paper, we propose a
   novel method to detect nose tip robustly. The method is robust to noise,
   need not training, can handle large rotations and occlusions. We first
   remove small isolated connected regions and noise from the input range
   image, then establish scale-space by robust smoothing the preprocessed
   range image. In each scale of the scale-space, we compute multi-angle
   energy of each point, then we use hierarchical clustering method to
   cluster the points whose multi-angle energies are larger than a
   threshold value. In the largest cluster, we can find one point with the
   largest multi-angle energy. For all scales of the scale-space, we get a
   series of such points and apply hierarchical clustering again for these
   points, nose tip will have the largest multi-angle energy in the largest
   cluster. We evaluate our method in FRGC v2.0 3D face database and
   BOSPHORUS 3D face database. The experimental results verify the
   robustness of our method with a high nose tip detection rate.}},
ISBN = {{978-1-4799-1980-2}},
Unique-ID = {{ISI:000377348700061}},
}

@inproceedings{ ISI:000387959204074,
Author = {Gilani, Syed Zulqarnain and Shafait, Faisal and Mian, Ajmal},
Book-Group-Author = {{IEEE}},
Title = {{Shape-based Automatic Detection of a Large Number of 3D Facial Landmarks}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2015}},
Pages = {{4639-4648}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{We present an algorithm for automatic detection of a large number of
   anthropometric landmarks on 3D faces. Our approach does not use texture
   and is completely shape based in order to detect landmarks that are
   morphologically significant. The proposed algorithm evolves level set
   curves with adaptive geometric speed functions to automatically extract
   effective seed points for dense correspondence. Correspondences are
   established by minimizing the bending energy between patches around seed
   points of given faces to those of a reference face. Given its
   hierarchical structure, our algorithm is capable of establishing
   thousands of correspondences between a large number of faces. Finally, a
   morphable model based on the dense corresponding points is fitted to an
   unseen query face for transfer of correspondences and hence automatic
   detection of landmarks. The proposed algorithm can detect any number of
   pre-defined landmarks including subtle landmarks that are even difficult
   to detect manually. Extensive experimental comparison on two benchmark
   databases containing 6, 507 scans shows that our algorithm outperforms
   six state of the art algorithms.}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-6964-0}},
ORCID-Numbers = {{Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000387959204074}},
}

@article{ ISI:000215156100010,
Author = {Fernandez-Cervantes, Victor and Garcia, Arturo and Antonio Ramos, Marco
   and Mendez, Andres},
Title = {{Facial Geometry Identification through Fuzzy Patterns with RGBD Sensor}},
Journal = {{COMPUTACION Y SISTEMAS}},
Year = {{2015}},
Volume = {{19}},
Number = {{3}},
Pages = {{529-546}},
Abstract = {{Automatic human facial recognition is an important and complicated task;
   it is necessary to design algorithms capable of recognizing the constant
   patterns in the face and to use computing resources efficiently. In this
   paper we present a novel algorithm to recognize the human face in real
   time; the system's input is the depth and color data from the Microsoft
   KinectTM device. The algorithm recognizes patterns/shapes on the point
   cloud topography. The template of the face is based in facial geometry;
   the forensic theory classifies the human face with respect to constant
   patterns: cephalometric points, lines, and areas of the face. The
   topography, relative position, and symmetry are directly related to the
   craniometric points. The similarity between a point cloud cluster and a
   pattern description is measured by a fuzzy pattern theory algorithm. The
   face identification is composed by two phases: the first phase
   calculates the face pattern hypothesis of the facial points, configures
   each point shape, the related location in the areas, and lines of the
   face. Then, in the second phase, the algorithm performs a search on
   these face point configurations.}},
DOI = {{10.13053/CyS-19-3-2015}},
ISSN = {{1405-5546}},
EISSN = {{2007-9737}},
ORCID-Numbers = {{Ramos Corchado, Marco Antonio/0000-0003-3982-6988}},
Unique-ID = {{ISI:000215156100010}},
}

@inproceedings{ ISI:000380453200062,
Author = {Sindhuja, C. and Mala, K.},
Book-Group-Author = {{IEEE}},
Title = {{LANDMARK IDENTIFICATION IN 3D IMAGE FOR FACIAL EXPRESSION RECOGNITION}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTING AND
   COMMUNICATIONS TECHNOLOGIES (ICCCT 15)}},
Year = {{2015}},
Pages = {{338-343}},
Note = {{International Conference on Computing and Communications Technologies
   ((ICCCT), Chennai, INDIA, FEB 26-27, 2015}},
Organization = {{Dept Informat Technol Sri Sai Ram Engn Coll Chennai}},
Abstract = {{Facial expression recognition plays a major role in non verbal
   communication. Recognition by machine is still a challenging problem. To
   automate the recognition for human machine interaction, a system is
   proposed in this paper. The proposed system uses shape descriptors to
   identify twelve land marks which mainly contribute to the facial
   expression recognition. From the location and the size or boundary of
   the land marks by matching with Facial Landmark Model (FLM), basic
   expressions are identified. The experimental results show that the shape
   descriptors and post processing correctly identifies landmarks
   automatically. The architectural distortion of action units is used to
   identify the basic facial expressions and tested on Bosphorous data set.}},
ISBN = {{978-1-4799-7623-2}},
Unique-ID = {{ISI:000380453200062}},
}

@article{ ISI:000344204000007,
Author = {Jo, Jaeik and Choi, Heeseung and Kim, Ig-Jae and Kim, Jaihie},
Title = {{Single-view-based 3D facial reconstruction method robust against pose
   variations}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{48}},
Number = {{1}},
Pages = {{73-85}},
Month = {{JAN}},
Abstract = {{The 3D Morphable Model (3DMM) and the Structure from Motion (SfM)
   methods are widely used for 3D facial reconstruction from 2D single-view
   or multiple-view images. However, model-based methods suffer from
   disadvantages such as high computational costs and vulnerability to
   local minima and head pose variations. The SfM-based methods require
   multiple facial images in various poses. To overcome these
   disadvantages, we propose a single-view-based 3D facial reconstruction
   method that is person-specific and robust to pose variations. Our
   proposed method combines the simplified 3DMM and the SfM methods. First,
   2D initial frontal Facial Feature Points (FFPs) are estimated from a
   preliminary 3D facial image that is reconstructed by the simplified
   3DMM. Second, a bilateral symmetric facial image and its corresponding
   FFPs are obtained from the original side-view image and corresponding
   FFPs by using the mirroring technique. Finally, a more accurate the 3D
   facial shape is reconstructed by the SfM using the frontal, original,
   and bilateral symmetric FFPs. We evaluated the proposed method using
   facial images in 35 different poses. The reconstructed facial images and
   the ground-truth 3D facial shapes obtained from the scanner were
   compared. The proposed method proved more robust to pose variations than
   3DMM. The average 3D Root Mean Square Error (RMSE) between the
   reconstructed and ground-truth 3D faces was less than 2.6 mm when 2D
   FFPs were manually annotated, and less than 3.5 mm when automatically
   annotated. (C) 2014 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2014.07.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000344204000007}},
}
