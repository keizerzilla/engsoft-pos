@INPROCEEDINGS{7292772, 
author={C. {Sindhuja} and K. {Mala}}, 
booktitle={2015 International Conference on Computing and Communications Technologies (ICCCT)}, 
title={Landmark identification in 3D image for facial expression recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={338-343}, 
abstract={Facial expression recognition plays a major role in non verbal communication. Recognition by machine is still a challenging problem. To automate the recognition for human machine interaction, a system is proposed in this paper. The proposed system uses shape descriptors to identify twelve land marks which mainly contribute to the facial expression recognition. From the location and the size or boundary of the land marks by matching with Facial Landmark Model (FLM), basic expressions are identified. The experimental results show that the shape descriptors and post processing correctly identifies landmarks automatically. The architectural distortion of action units is used to identify the basic facial expressions and tested on Bosphorous data set.}, 
keywords={emotion recognition;face recognition;landmark identification;3D image;facial expression recognition;nonverbal communication;human machine interaction;shape descriptors;facial landmark model;FLM;post processing;architectural distortion;facial expression identification;Bosphorous data set;Shape;Indexes;Face recognition;Nose;Mouth;Three-dimensional displays;Feature extraction;Landmark detection;Facial Landmark Model;Shape index}, 
doi={10.1109/ICCCT2.2015.7292772}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7558813, 
author={S. {Liu} and X. {Chen} and D. {Fan} and X. {Chen} and F. {Meng} and Q. {Huang}}, 
booktitle={2016 IEEE International Conference on Mechatronics and Automation}, 
title={3D smiling facial expression recognition based on SVM}, 
year={2016}, 
volume={}, 
number={}, 
pages={1661-1666}, 
abstract={Using Kinect acquired RGB-D image to obtain a face feature parameters and three-dimensional coordinates of the characteristic parameters, and to select the characteristic parameter Facial by Candide-3 model, and feature extraction and normalization. Smile face expression data collection through Kinect, SVM collected to smiley face data classify and output the result of recognition, and the results compared with two-dimensional image of smiling face expression recognition results. Experimental results show that three-dimensional image of smiling face expression recognition accuracy than the two-dimensional image of smiling face. This research has important significance for the research and application of facial expression recognition technology.}, 
keywords={emotion recognition;face recognition;feature extraction;image classification;interactive devices;support vector machines;3D smiling facial expression recognition;SVM;Kinect;RGB-D image;face feature parameters;three-dimensional coordinates;characteristic parameters;Candide-3 model;feature extraction;normalization;smile face expression data collection;smiley face data classification;smiling face expression recognition two-dimensional image;smiling face expression recognition three-dimensional image;facial expression recognition technology;Feature extraction;Face;Face recognition;Support vector machines;Image recognition;Training;Data mining;Facial Expression Recognition;Feature Extraction;Support Vector Machine;Kinect}, 
doi={10.1109/ICMA.2016.7558813}, 
ISSN={2152-744X}, 
month={Aug},}
@INPROCEEDINGS{7087053, 
author={A. A. {Pawar} and N. N. {Patil}}, 
booktitle={2015 International Conference on Pervasive Computing (ICPC)}, 
title={Recognition of 3-D faces with missing parts and line scratch removal using new technique}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={This paper presents an implementation of face recognition, which is a very important task of human face identification. Line scratch detection in images is a highly challenging situation because of various characteristics of this defect. Few characteristics are considered with the different texture and geometry of images. We propose a useful algorithm for frame-by-frame line scratch detection in face image which deals with 3D approach and a filtering of detection. The temporary filtering algorithm can be used to remove false detection due to thin vertical structures by detecting the scratches on an image. Experimental evaluation can be detecting the lines and scratches on a face image and they used to solve this difficult approach. Our method is used with missing parts in an image. Three-dimensional face recognition is an extended method of facial recognition is considered according with the geometry and texture of a face. It has been elaborated that 3D face recognition methods can provide high accuracy as well as high detection with a comparison of 2D recognition. 3D avoids such mismatch effect of 2D face recognition algorithms. Additionally, most 3D scanners achieve both a 3D mesh and the texture of a face image. This allows combining the output of pure 3D matches with the more traditional algorithms of 2D face recognition, thus producing better performance.}, 
keywords={computational geometry;face recognition;filtering theory;image matching;image texture;mesh generation;object detection;3D face recognition;missing parts;line scratch removal;human face identification;frame-by-frame line scratch detection;image texture;image geometry;detection filtering;false detection removal;3D scanners;3D mesh;pure 3D matches;Face recognition;Three-dimensional displays;Transforms;Filtering;Image recognition;Noise;Films;3D Images;Adaptive detection;Face mask;Hough transforms;ICP algorithm;Line scratches;Missing parts;RANSAC;SIFT}, 
doi={10.1109/PERVASIVE.2015.7087053}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7443288, 
author={ and and S. {Tripathi}}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={Pose invariant method for emotion recognition from 3D images}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Information about the emotional state of a person can be inferred from facial expressions. Emotion recognition has become an active research area in recent years in various fields such as Human Robot Interaction (HRI), medicine, intelligent vehicle, etc., The challenges in emotion recognition from images with pose variations, motivates researchers to explore further. In this paper, we have proposed a method based on geometric features, considering images of 7 yaw angles (-45°,-30°,-15°,0°,+15°,+30°,+45°) from BU3DFE database. Most of the work that has been reported considered only positive yaw angles. In this work, we have included both positive and negative yaw angles. In the proposed method, feature extraction is carried out by concatenating distance and angle vectors between the feature points, and classification is performed using neural network. The results obtained for images with pose variations are encouraging and comparable with literature where work has been performed on pitch and yaw angles. Using our proposed method non-frontal views achieve similar accuracy when compared to frontal view thus making it pose invariant. The proposed method may be implemented for pitch and yaw angles in future.}, 
keywords={emotion recognition;feature extraction;image classification;neural nets;pose estimation;visual databases;pose invariant method;emotion recognition;3D image;person emotional state;facial expressions;pose variation;geometric features;BU3DFE database;positive yaw angles;negative yaw angles;feature extraction;concatenating distance;angle vectors;feature points;classification;neural network;nonfrontal views;Emotion recognition;Databases;Three-dimensional displays;Feature extraction;Eyebrows;Euclidean distance;Mouth;BU3DFE database;feature points;feature extraction;classification;neural network}, 
doi={10.1109/INDICON.2015.7443288}, 
ISSN={2325-9418}, 
month={Dec},}
@INPROCEEDINGS{7351408, 
author={T. {Batabyal} and A. {Vaccari} and S. T. {Acton}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={UGraSP: A unified framework for activity recognition and person identification using graph signal processing}, 
year={2015}, 
volume={}, 
number={}, 
pages={3270-3274}, 
abstract={With the growing availability and wide distribution of low-cost, high-performance 3D imaging sensors, the image analysis community has witnessed an increased demand for solutions to the challenges of activity recognition and person identification. We propose an integrated framework, based on graph signal processing, that simultaneously performs both tasks using a single set of features. The novelty of our approach is based on the fact that the set of features used for activity recognition accommodates person identification without additional computation. The analysis is based on the extracted structure-invariant graph (skeleton). The Laplacian of the skeleton is used both to identify the person and recognize the performed activity. While person identification is achieved directly from the analysis of the Laplacian, activity recognition is obtained after transformation, into the graph spectral domain, of the vectorized form of the skeletal joints 3D coordinates. Feature vectors for activity recognition are then derived, in this domain, from the covariance matrices evaluated over fixed-length sequential video segments. Both classification tasks are implemented using linear support vector machines (SVM). When applied to real activity datasets, our approach shows an improved performance over the existing state-of-the-art.}, 
keywords={covariance matrices;feature extraction;graph theory;image classification;object detection;object recognition;support vector machines;video signal processing;UGraSP;unified framework;image analysis community;activity recognition;person identification;integrated framework;graph signal processing;feature tasks;structure-invariant graph extraction;graph skeleton;skeleton Laplacian;graph spectral domain;vectorized form;skeletal joints 3D coordinates;feature vectors;covariance matrices;fixed-length sequential video segments;classification tasks;linear support vector machines;SVM;real activity datasets;performance improvement;Laplace equations;Skeleton;Three-dimensional displays;Motion segmentation;Image recognition;Sensors;Support vector machines;Laplacian;Adjacency Matrix;Graph Signal Processing;Graph Fourier Transform;activity Recognition;Person Identification;Point cloud datasets}, 
doi={10.1109/ICIP.2015.7351408}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8571963, 
author={S. {Yamada} and H. {Lu} and J. K. {Tan} and H. {Kim} and N. {Kimura} and T. {Okawachi} and E. {Nozoe} and N. {Nakamura}}, 
booktitle={2018 18th International Conference on Control, Automation and Systems (ICCAS)}, 
title={Extraction of Median Plane from Facial 3D Point Cloud Based on Symmetry Analysis Using ICP Algorithm}, 
year={2018}, 
volume={}, 
number={}, 
pages={1347-1350}, 
abstract={Cleft lip is a kind of congenital facial morphological abnormality. In the clinical field of cleft lip, it is necessary to analyze symmetric shape. However, there is no method to analyze the cleft lip technique based on symmetrical viewpoints. On the other hand, in our previous method to find a symmetric axis using a 2D image, since the middle line is extracted only from the front view of the face moire image. There was a problem that low accuracy was obtained by slight rotation of the face and it was not possible to consider 3D information. In this paper, we propose a method to extract the median plane of the face by analyzing based on bilateral symmetry by using 3D point cloud on the face of front. By extracting the median plane, we believe that not only surgical assistance of doctor be possible but also become a clue to development of simulation software which is the end goal.}, 
keywords={biomechanics;face recognition;feature extraction;image reconstruction;medical image processing;surgery;symmetric shape;clinical field;congenital facial morphological abnormality;ICP algorithm;symmetry analysis;facial 3D point cloud;bilateral symmetry;median plane;problem that low accuracy;face moire image;middle line;symmetric axis;symmetrical viewpoints;cleft lip technique;Three-dimensional displays;Lips;Face;Surgery;Iterative closest point algorithm;Two dimensional displays;Nose;Cleft lip;ICP algorithm;3D point cloud;Point Cloud Library;Facial median plane;Symmetry analysis.}, 
doi={}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7910452, 
author={S. G. {Gunanto} and M. {Hariadi} and E. M. {Yuniarno}}, 
booktitle={2016 2nd International Conference of Industrial, Mechanical, Electrical, and Chemical Engineering (ICIMECE)}, 
title={Computer facial animation with synthesize marker on 3D faces surface}, 
year={2016}, 
volume={}, 
number={}, 
pages={260-263}, 
abstract={An animated character has its own characteristics and behaviour. The animator needs to be skilled enough to make a complex animation, especially on making face expression. a Well defined facial expression can represent the emotional condition and make the animation more expressing the mood. But most facial animation is done by manually, frame-by-frame. It is very time-consuming. This research proposed a combination methods for handling the facial animation based on marker location and face surface from the 3D character. The motion data captured based on the location and movement of the marker then implemented on 3D face model to generate the motion guidance. As a guidance, this marker data role as a centroid of a vertex cluster. The cluster provided by implementing segmentation fp-NN Clustering method based on surface and can visualize the deformation using linear blend skinning methods. The result from this research shows that this system can automatically generate facial animations based on the marker data and the surface segmentation. The visualization of deformation arranges accordingly to the motion captured data and organized sequentially.}, 
keywords={computer animation;data visualisation;face recognition;image segmentation;neural nets;pattern clustering;computer facial animation;synthesize marker;3D face surface;facial expression;3D character;motion data capture;motion guidance;vertex cluster;segmentation fp-NN Clustering method;linear blend skinning methods;Facial animation;Three-dimensional displays;Solid modeling;Motion segmentation;Surface treatment;Interpolation;facial animation;feature marker;surface}, 
doi={10.1109/ICIMECE.2016.7910452}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7153124, 
author={G. {Pang} and R. {Qiu} and J. {Huang} and S. {You} and U. {Neumann}}, 
booktitle={2015 14th IAPR International Conference on Machine Vision Applications (MVA)}, 
title={Automatic 3D industrial point cloud modeling and recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={22-25}, 
abstract={3D modeling of point clouds is an important but time-consuming process, inspiring extensive research in automatic methods. Prior efforts focus on primitive geometry, street structures or indoor objects, but industrial data has rarely been pursued. Our work presents a method for automatic modeling and recognition of 3D industrial site point clouds, dividing the task into 3 separate sub-problems: pipe modeling, plane classification, and object recognition. The results are integrated to obtain the complete model, revealing some issues during the integration, solved by utilizing information gained from each individual process. Experiments show that the presented method automatically models large and complex industrial scenes with a quality that outperforms leading commercial modeling software and is comparable to professional hand-made models.}, 
keywords={image classification;mechanical engineering computing;object recognition;pipes;automatic 3D industrial point cloud modeling;automatic 3D industrial point cloud recognition;primitive geometry;street structures;indoor objects;plane classification;pipe modeling;object recognition;Three-dimensional displays;Solid modeling;Data models;Object recognition;Libraries;Software;Surface treatment}, 
doi={10.1109/MVA.2015.7153124}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7378673, 
author={ and and }, 
booktitle={2015 23rd International Conference on Geoinformatics}, 
title={A method of extracting human facial feature points based on 3D laser scanning point cloud data}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-3}, 
abstract={Three-dimensional body measurement is determined by measuring the size of each part of the body and body shape, in order to study the morphological characteristics of the human body, and provide basic information for human industrial design, ergonomics, engineering design, anthropological research, and medicine. This paper presents a three-dimensional point cloud data extraction method of facial feature points by doing nose points with examples, using this method can accurately locate the feature points, and there is no specific stations toward requirements in the process of scanning point cloud of the human body, it is convenient and quick. The results show that this method can meet the ergonomics and other features to quickly locate and measure the body size requirements.}, 
keywords={computer graphics;face recognition;feature extraction;optical scanners;human facial feature point extraction;3D laser scanning point cloud data;three-dimensional body measurement;morphological characteristics;human body;human industrial design;ergonomics;engineering design;anthropological research;medicine;three-dimensional point cloud data extraction method;nose point;Atmospheric modeling;Ergonomics;Biomedical imaging;The face feature point;Nose point;Measurement;Three-dimensional point cloud;Three-dimensional laser scanning}, 
doi={10.1109/GEOINFORMATICS.2015.7378673}, 
ISSN={2161-024X}, 
month={June},}
@INPROCEEDINGS{8054955, 
author={M. {Jazouli} and A. {Majda} and A. {Zarghili}}, 
booktitle={2017 Intelligent Systems and Computer Vision (ISCV)}, 
title={A $P recognizer for automatic facial emotion recognition using Kinect sensor}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Autism is a developmental disorder involving qualitative impairments in social interaction. One source of those impairments are difficulties with facial expressions of emotion. Autistic people often have difficulty to recognize or to understand other people's emotions and feelings, or expressing their own. This work proposes a method to automatically recognize seven basic emotions among autistic children in real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and Neutral. The method uses the Microsoft Kinect sensor to track and identify points of interest from the 3D face model and it is based on the $P point-cloud recognizer to identify multi-stroke emotions as point-clouds. The experimental results show that our system can achieve above 94.28% recognition rate. Our study provides a novel clinical tool to help children with autism to assisting doctors in operating rooms.}, 
keywords={emotion recognition;face recognition;feature extraction;multistroke emotions;point-clouds;recognition rate;autism;$P recognizer;automatic facial emotion recognition;developmental disorder;qualitative impairments;social interaction;autistic people;autistic children;Microsoft Kinect sensor;3D face model;$P point-cloud recognizer;Face recognition;Emotion recognition;Face;Three-dimensional displays;Autism;Support vector machines;Algorithm design and analysis;ASD;Autism;emotion;face expression;Kinect;$P Recognizer}, 
doi={10.1109/ISACV.2017.8054955}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8687254, 
author={T. {Terada} and Y. {Chen} and R. {Kimura}}, 
booktitle={2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
title={3D Facial Landmark Detection Using Deep Convolutional Neural Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={390-393}, 
abstract={Various applications have been developed in facial research to aid the recognition of personal attributes, analysis of race, and personal authentication for the security industry and other research fields. Thus, it is possible to identify the differences in facial shape based on place of birth or country. The present study analyzes facial shape using scanned 3D facial images and investigates ways to extract facial landmarks from the 3D facial images. The detection of the facial landmark requires the normalization of the facial scale and position among in the 3D image data to analyze the facial shape. Therefore, it is difficult to obtain accurate facial landmarks from 3D facial images. Our method decomposes the task into the following three parts: (a) conversion of data from the 3D facial image to a 2D image, (b) extraction of facial landmarks from the 3D image using Convolutional Neural Network (CNN), (c) inversion of the identified facial landmarks from 2D to 3D images. In experiments, we compared the accuracy of this model for facial landmark detection.}, 
keywords={convolutional neural nets;face recognition;facial landmark detection;3D facial image;personal authentication;convolutional neural network;security industry;CNN;component;landmarks detection;3d facial image;point cloud;facial analysis;cnn}, 
doi={10.1109/FSKD.2018.8687254}, 
ISSN={}, 
month={July},}
@ARTICLE{7130613, 
author={L. {Yuan} and F. {Chen} and L. {Zeng} and L. {Wang} and D. {Hu}}, 
journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
title={Gender Identification of Human Brain Image with A Novel 3D Descriptor}, 
year={2018}, 
volume={15}, 
number={2}, 
pages={551-561}, 
abstract={Determining gender by examining the human brain is not a simple task because the spatial structure of the human brain is complex, and no obvious differences can be seen by the naked eyes. In this paper, we propose a novel three-dimensional feature descriptor, the three-dimensional weighted histogram of gradient orientation (3D WHGO) to describe this complex spatial structure. The descriptor combines local information for signal intensity and global three-dimensional spatial information for the whole brain. We also improve a framework to address the classification of three-dimensional images based on MRI. This framework, three-dimensional spatial pyramid, uses additional information regarding the spatial relationship between features. The proposed method can be used to distinguish gender at the individual level. We examine our method by using the gender identification of individual magnetic resonance imaging (MRI) scans of a large sample of healthy adults across four research sites, resulting in up to individual-level accuracies under the optimized parameters for distinguishing between females and males. Compared with previous methods, the proposed method obtains higher accuracy, which suggests that this technology has higher discriminative power. With its improved performance in gender identification, the proposed method may have the potential to inform clinical practice and aid in research on neurological and psychiatric disorders.}, 
keywords={biomedical MRI;brain;image classification;medical image processing;neurophysiology;local information;signal intensity;three-dimensional spatial information;three-dimensional images;MRI;three-dimensional spatial pyramid;spatial relationship;individual level;gender identification;individual magnetic resonance imaging scans;individual-level accuracies;human brain image;naked eyes;three-dimensional feature descriptor;three-dimensional weighted histogram;gradient orientation;3D WHGO;complex spatial structure;Three-dimensional displays;Histograms;Kernel;Spatial resolution;Magnetic resonance imaging;Computational biology;Three-dimensional descriptor;gender identification;neuroimaging data analysis;pattern classification}, 
doi={10.1109/TCBB.2015.2448081}, 
ISSN={1545-5963}, 
month={March},}
@INPROCEEDINGS{8518089, 
author={S. {Li} and L. {Su} and Y. {Liu} and Z. {He}}, 
booktitle={IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium}, 
title={Segmentation of Individual Trees Based on a Point Cloud Clustering Method Using Airborne Lidar Data}, 
year={2018}, 
volume={}, 
number={}, 
pages={7520-7523}, 
abstract={The objective of this paper was to develop a new algorithm to segment individual trees directly by using the three-dimensional space characteristic of airborne light detection and ranging point cloud data. The local maximum method was used in the initial segmentation and the error identification tree exclusion. On the basis of the point cloud spatial distribution of individual trees and the adjacent relationship with the other trees, a point cloud clustering method was developed to decide the points belonging to the individual trees. This algorithm was tested by 6 forest plots in the Genhe forestry reserve. The results showed that this algorithm could segment individual trees quickly and accurately, and the overall accuracy of this algorithm was 96.3%.}, 
keywords={forestry;geophysical image processing;image segmentation;optical radar;pattern clustering;remote sensing by laser beam;vegetation mapping;segment individual trees;point cloud clustering method;airborne lidar data;three-dimensional space characteristic;airborne light detection;ranging point cloud data;local maximum method;initial segmentation;error identification tree exclusion;point cloud spatial distribution;Genhe forestry reserve;Vegetation;Three-dimensional displays;Forestry;Laser radar;Clustering algorithms;Remote sensing;Lasers;LiDAR;segmentation;tree;clustering}, 
doi={10.1109/IGARSS.2018.8518089}, 
ISSN={2153-7003}, 
month={July},}
@INPROCEEDINGS{7338568, 
author={F. {Ergüner} and P. O. {Durdu}}, 
booktitle={2015 9th International Conference on Application of Information and Communication Technologies (AICT)}, 
title={Multimodal natural interaction for 3D images}, 
year={2015}, 
volume={}, 
number={}, 
pages={305-309}, 
abstract={Due to the improvements in computer technology, interaction between computer and human has been evolved from command-line based interfaces to natural user interfaces that enables interaction in a more human-human way such as by speech, hand and body gestures, facial expressions and eye gaze. In this study controlling three dimensional images with gestures and speech using a three dimensional depth camera is realized in order to ensure human computer interaction in a more natural way. For this purpose realized system allows starting and closing the application and interaction with the three dimensional images using only speech and gestures but not using keyboard and mouse. System allows three different speech commands to start, close the application and reset the three dimensional image. Furthermore gesture-based commands are used to rotate, pan and zoom the three dimensional image.}, 
keywords={gesture recognition;human computer interaction;keyboards;mouse controllers (computers);user interfaces;multimodal natural interaction;human computer interaction;command-line based interfaces;natural user interfaces;facial expressions;body gestures;hand gestures;eye gaze;three dimensional images;keyboard;mouse;gesture-based commands;Three-dimensional displays;Speech;Biomedical imaging;Software;User interfaces;Cameras;Computers;Human Computer Interaction;Multimodal Natural Interaction;Kinect;Natural User Interface}, 
doi={10.1109/ICAICT.2015.7338568}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7163090, 
author={X. {Yang} and D. {Huang} and Y. {Wang} and L. {Chen}}, 
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
title={Automatic 3D facial expression recognition using geometric scattering representation}, 
year={2015}, 
volume={1}, 
number={}, 
pages={1-6}, 
abstract={Facial Expression Recognition (FER) is one of the most active topics in the domain of computer vision and pattern recognition, and it has received increasing attention for its wide application potentials as well as attractive scientific challenges. In this paper, we present a novel method to automatic 3D FER based on geometric scattering representation. A set of maps of shape features in terms of multiple order differential quantities, i.e. the Normal Maps (NOM) and the Shape Index Maps (SIM), are first jointly adopted to comprehensively describe geometry attributes of the facial surface. The scattering operator is then introduced to further highlight expression related cues on these maps, thereby constructing geometric scattering representations of 3D faces for classification. The scattering descriptor not only encodes distinct local shape changes of various expressions as by several milestone descriptors, such as SIFT, HOG, etc., but also captures subtle information hidden in high frequencies, which is quite crucial to better distinguish expressions that are easily confused. We evaluate the proposed approach on the BU-3DFE database, and the performance is up to 84.8% and 82.7% with two commonly used protocols respectively which is superior to the state of the art ones.}, 
keywords={emotion recognition;face recognition;image classification;image representation;shape recognition;automatic 3D facial expression recognition;automatic 3D FER;BU-3DFE database;local shape changes;3D face classification;scattering operator;facial surface geometry attributes;SIM;shape index maps;NOM;normal maps;multiple order differential quantities;shape feature maps;geometric scattering representation;Three-dimensional displays;Shape;Scattering;Indexes;Support vector machines;Solid modeling;Feature extraction}, 
doi={10.1109/FG.2015.7163090}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8358484, 
author={K. M. {Swetha} and P. {Suja}}, 
booktitle={2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon)}, 
title={A geometric approach for recognizing emotions from 3D images with pose variations}, 
year={2017}, 
volume={}, 
number={}, 
pages={805-809}, 
abstract={Emotions are an incredibly important aspect of human life. Research on emotion recognition for the past few decades have resulted in development of several fields. In the current scenario, it is necessary that machines/robots need to identify human emotions and respond accordingly. Applications in this field can be seen in security, entertainment and Human Machine Interface/Human Robot Interface. Recent works on 3D images have gained importance due to its accuracy in real life applications as emotions can be recognised at different head poses. The intention of this work has been to develop an algorithm for recognition of emotion from facial expressions, which recognizes 6 basic emotions, which are anger, fear, happy, disgust, sad and surprise from 3D images in 7 yaw angles (+45° to -45°) and 3 pitch angles (+15°,0°, -15°). Most of the reported work considers + yaw angles. While in the current work, both positive as well as negative pitch and yaw angles are considered. BU3DFE database is used for the implementation. The proposed method resulted in improved accuracy and is comparable with the literature.}, 
keywords={emotion recognition;face recognition;geometry;pose estimation;BU3DFE database;geometric approach;pose variations;emotion recognition;human emotions;head poses;yaw angles;pitch angles;3D image;facial expressions;Basic emotions;feature points;BU3DFE database;classification}, 
doi={10.1109/SmartTechCon.2017.8358484}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8525654, 
author={M. P. {Zapf} and A. {Gupta} and L. Y. {Morales Saiki} and M. {Kawanabe}}, 
booktitle={2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}, 
title={Data-Driven, 3-D Classification of Person-Object Relationships and Semantic Context Clustering for Robotics and AI Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={180-187}, 
abstract={We introduce a framework for detection and classification of spatio-temporal person-object interactions. Our method clusters similar semantic contexts from interactions detected from RGB-D data. 2-D object detection (YOLO) is run on RGB data from a Kinect v2 sensor on a mobile robot navigating an office and observing persons and desk spaces. Person and object detections are converted into 3-D point cloud time series via RGB-Depth co-registration and successive Euclidean and k-means spatial clustering. 3-D person and object point cloud streams are used to create time-series occupancy maps and person-object co-localization maps. From these maps, spatiotemporal correlations between persons and distinct objects are computed. Correlation patterns are clustered using k-means to obtain distinct human-object interactions, i.e. segment semantic context over time. We evaluated the performance of our approach to detect person-object correlations and cluster semantic context by recording 90 30-second RGB-D data episodes, with three persons handling representative objects (books, cups, bottles). Experimental results show that our framework is able to consistently assign semantic context to the same cluster in > 79% of cases (scene frames). Semantic contexts in visual scenes can be distinguished without the need to provide prior information, allowing mobile agents to learn and explore in new environments.}, 
keywords={feature extraction;image classification;image colour analysis;image motion analysis;image registration;image segmentation;mobile robots;object detection;path planning;pattern clustering;robot vision;time series;data-driven;3D classification;k-means spatial clustering;spatio-temporal person-object interaction detection;spatio-temporal person-object interaction classification;2D object detection;3D point cloud time series;RGB-depth coregistration;Euclidean clustering;person-object colocalization maps;correlation pattern clustering;semantic context segmentation;mobile agents;RGB-D data episodes;person-object correlations;human-object interactions;spatiotemporal correlations;time-series occupancy maps;object point cloud streams;mobile robot;Kinect v2 sensor;RGB data;semantic context clustering;person-object relationships;Three-dimensional displays;Robot sensing systems;Semantics;Object detection;Feature extraction;Correlation}, 
doi={10.1109/ROMAN.2018.8525654}, 
ISSN={1944-9437}, 
month={Aug},}
@INPROCEEDINGS{7081271, 
author={M. C. {EL Rai} and N. {Werghi} and H. {Al Muhairi} and H. {Alsafar}}, 
booktitle={2015 International Conference on Communications, Signal Processing, and their Applications (ICCSPA'15)}, 
title={Using facial images for the diagnosis of genetic syndromes: A survey}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The analysis of facial appearance is significant to an early diagnosis of medical genetic diseases. The fast development of image processing and machine learning techniques facilitates the detection of facial dysmorphic features. This paper is a survey of the recent studies developed for the screening of genetic abnormalities across the facial features obtained from two dimensional and three dimensional images.}, 
keywords={diseases;face recognition;learning (artificial intelligence);medical image processing;patient diagnosis;facial images;genetic syndromes diagnosis;facial appearance analysis;medical genetic diseases;image processing;machine learning techniques;facial dysmorphic features;genetic abnormalities;facial features;two dimensional images;three dimensional images;Three-dimensional displays;Face;Genetics;Feature extraction;Principal component analysis;Accuracy;Databases;Facial images;2D imaging;3D imaging;Landmarks;Dysmorphology;Classification}, 
doi={10.1109/ICCSPA.2015.7081271}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8075548, 
author={T. {Frikha} and F. {Chaabane} and B. {Said} and H. {Drira} and M. {Abid} and C. {Ben Amar} and L. {Lille}}, 
booktitle={2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
title={Embedded approach for a Riemannian-based framework of analyzing 3D faces}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform.}, 
keywords={biometrics (access control);embedded systems;face recognition;feature extraction;field programmable gate arrays;hardware-software codesign;software architecture;stereo image processing;tensors;biometric facial recognition system;embedded systems;data completion;full face recognition;hardware architecture;adaptation approach;face detection;Riemannian-based framework;3D face analysis;multimedia embedded applications;face expressions recognition;mixed hardware software architecture;codesign technique;FPGA platform;Computer architecture;Shape;Face recognition;Multimedia communication;Three-dimensional displays;Embedded systems;Measurement;Facial analysis;face detection;Facial expressions;3D face recognition;embedded architecture;elastic analysis algorithm;Riemann geometry;Curve analysis}, 
doi={10.1109/ATSIP.2017.8075548}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8578635, 
author={S. {Cheng} and I. {Kotsia} and M. {Pantic} and S. {Zafeiriou}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={5117-5126}, 
abstract={The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.}, 
keywords={biometrics (access control);computer vision;emotion recognition;face recognition;image capture;image resolution;stereo image processing;visual databases;high-resolution 3D faces;4DFAB;facial behaviour;facial expression recognition;behavioural biometrics;computer vision applications;automatic face analysis;scale visual databases;face recognition;Databases;Three-dimensional displays;Face;Face recognition;Cameras;Two dimensional displays;Task analysis}, 
doi={10.1109/CVPR.2018.00537}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{7853992, 
author={R. {Amin} and A. F. {Shams} and S. M. M. {Rahman} and D. {Hatzinakos}}, 
booktitle={2016 9th International Conference on Electrical and Computer Engineering (ICECE)}, 
title={Evaluation of discrimination power of facial parts from 3D point cloud data}, 
year={2016}, 
volume={}, 
number={}, 
pages={602-605}, 
abstract={Feature selection from facial regions is a well-known approach to increase the performance of 2D image-based face recognition systems. In case of 3D modality, the approach of region-based feature selection for face recognition is relatively new. In this context, this paper presents an approach to evaluate the discrimination power of different regions of a 3D facial surface for its potential use in face recognition systems. We propose the use of weighted average of unit normal vector on the facial surface as the feature for region-based face recognition from 3D point cloud data (PCD). The iterative closest point algorithm is employed for the registration of segmented regions of facial point clouds. A metric based on angular distance between normals is introduced to indicate the similarity between two surfaces of same facial region. Finally, the intra class correlation based discrimination score is formulated to find out the key facial regions such as the eyes, nose, and mouth that are significant while recognizing a person with facial surface PCD.}, 
keywords={computational geometry;correlation methods;face recognition;feature selection;image registration;image segmentation;discrimination power evaluation;facial parts;2D image-based face recognition systems;3D modality;region-based feature selection;3D facial surface;3D point cloud data;3D PCD;iterative closest point algorithm;segmented region registration;angular distance;intra class correlation;discrimination score;Three-dimensional displays;Face;Face recognition;Measurement;Two dimensional displays;Nose;Databases}, 
doi={10.1109/ICECE.2016.7853992}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7852823, 
author={ and }, 
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={Joint subspace learning for reconstruction of 3D facial dynamic expression from single image}, 
year={2016}, 
volume={}, 
number={}, 
pages={820-824}, 
abstract={Recently, the synthesis of 3D dynamic expressions has become an important concern in computer graphics, facial recognition, etc. In this study, we propose a regression based joint subspace learning method for the automatic synthesis of 3D dynamic expression images. This method synthesizes 3D dynamic expression images from a single 2D facial image. We use two subspaces (the view subspace and the frame subspace) to synthesize a 3D image. First, we use the view subspace to estimate multi-view facial images from a front image. Next, we construct a 3D image using the estimated multi-view facial images. Finally, we estimate the 3D images in different frames by using the frame subspace to synthesis 3D dynamic expression images. This approach is unlike the conventional joint subspace learning in which, the coefficients estimated by the input image are directly used for synthesis. Furthermore, we propose using textural information to improve the accuracy of synthesized images.}, 
keywords={computer graphics;estimation theory;face recognition;image reconstruction;learning (artificial intelligence);regression analysis;stereo image processing;3D facial dynamic expression reconstruction;single image;computer graphics;facial recognition;regression based joint subspace learning;multiview facial image estimation;Three-dimensional displays;Shape;Image reconstruction;Two dimensional displays;Principal component analysis;Training;Joints;3D dynamic expressions;multi-view facial shape;joint subspace learning;regression}, 
doi={10.1109/CISP-BMEI.2016.7852823}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7428562, 
author={J. {Liu} and Q. {Zhang} and C. {Tang}}, 
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
title={Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information}, 
year={2015}, 
volume={}, 
number={}, 
pages={276-284}, 
abstract={3D facial landmarks play a significant role in many 3D facial studies. Due to the complex geometry of high resolution 3D human face, landmark detection remains to be a challenge problem. This paper proposes a novel method to detect landmarks automatically for high resolution 3D faces without learning or training, solely using geometric information. For high resolution 3D faces, geodesic remeshing is used to reduce the vertices number, then the remeshed 3D faces are parameterized and interpolated on a regular grid, which enable us to compute the differential geometric features more efficiently and accurately. To detect landmarks, we extract global and local constraints for each landmark by considering relative positions of landmarks and differential geometric features, then landmarks are detected by combine these constraints. We evaluate our method and compare it with other methods which are mostly related to ours in a large scale publicly available 3D face data set, BJUT-3D face database. The experimental results show that our method is robust and effective, and achieves better performance than existing methods.}, 
keywords={face recognition;feature extraction;image resolution;interpolation;mesh generation;object detection;automatic landmark detection;high resolution nonrigid 3D faces;geometric information;3D facial landmarks;geodesic remeshing;vertices number;parameterization;interpolation;regular grid;differential geometric features;global constraints extraction;local constraints extraction;Three-dimensional displays;Indexes;Shape;Training;Feature extraction;Facial animation;Mesh generation;3D faces;landmarks;geometric information;geodesic remeshing;differential geometric features}, 
doi={10.1109/IAEAC.2015.7428562}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8272703, 
author={H. {Li} and J. {Sun} and L. {Chen}}, 
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)}, 
title={Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition}, 
year={2017}, 
volume={}, 
number={}, 
pages={234-242}, 
abstract={This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01%, 97.60%, and 96.13% on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition.}, 
keywords={face recognition;feature extraction;geometry;image classification;image representation;pose estimation;location-sensitive sparse representation;expression-robust 3D face recognition approach;given raw 3D facial surfaces;3D face pre-processing pipeline;face region cropping;normalized 3D facial surface;facial surface normal components;deep face net;facial surface normals;different facial locations;location sensitive sparse representation classifier;different 3D faces;different normal components;BU-3DFE databases;massive 2D face images;Three-dimensional displays;Face;Face recognition;Solid modeling;Two dimensional displays;Shape;Deformable models}, 
doi={10.1109/BTAS.2017.8272703}, 
ISSN={2474-9699}, 
month={Oct},}
@INPROCEEDINGS{8579077, 
author={J. {Li} and B. M. {Chen} and G. H. {Lee}}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={SO-Net: Self-Organizing Network for Point Cloud Analysis}, 
year={2018}, 
volume={}, 
number={}, 
pages={9397-9406}, 
abstract={This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website1.}, 
keywords={computer vision;feature extraction;learning (artificial intelligence);nearest neighbour methods;self-organising feature maps;deep learning;orderless point clouds;SO-Net models;spatial distribution;individual points;SOM nodes;input point cloud;single feature vector;point cloud reconstruction;part segmentation;shape retrieval;point cloud recognition networks;point cloud analysis;permutation invariant architecture;self-organizing map;self-organizing network;point-to-node k nearest neighbor search;SO-Net;hierarchical feature extraction;Three-dimensional displays;Feature extraction;Training;Shape;Two dimensional displays;Graphical models}, 
doi={10.1109/CVPR.2018.00979}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{8269662, 
author={R. {Kaur} and D. {Sharma} and A. {Verma}}, 
booktitle={2017 4th International Conference on Signal Processing, Computing and Control (ISPCC)}, 
title={An advance 2D face recognition by feature extraction (ICA) and optimize multilayer architecture}, 
year={2017}, 
volume={}, 
number={}, 
pages={122-129}, 
abstract={Facial recognition has most significant real-life requests like investigation and access control. It is associated through the issue of appropriately verifying face pictures and transmit them person in a database. In a past years face study has been emerging active topic. Most of the face detector techniques could be classified into feature based methods and image based also. Feature based techniques adds low-level analysis, feature analysis, etc. Facial recognition is a system capable of verifying / identifying a human after 3D images. By evaluating selected facial unique features from the image and face dataset. Design from transformation method given vector dimensional illustration of individual face in a prepared set of images, Principle component analysis inclines to search a dimensional sub-space whose normal vector features correspond to the maximum variance direction in the real image space. The PCA algorithm evaluates the feature extraction, data, i.e. Eigen Values and vectors of the scatter matrix. In literature survey, Face recognition is a design recognition mission performed exactly on faces. It can be described as categorizing a facial either “known” or “unknown”, after comparing it with deposits known individuals. It is also necessary to need a system that has the capability of knowledge to recognize indefinite faces. Computational representations of facial recognition must statement various difficult issues. After existing work, we study the SIFT structures for the gratitude method. The novel technique is compared with well settled facial recognition methods, name component analysis and eigenvalues and vector. This algorithm is called PCA and ICA (Independent Component Analysis). In research work, we implement the novel approach to detect the face in minimum time and evaluate the better accuracy based on Back Propagation Neural Networks. We design the framework in face recognition using MATLAB 2013a simulation tool. Evaluate the performance parameters, i.e. the FAR (false acceptance rate), FRR (False rejection Rate) and Accuracy and compare the existing performance parameters i.e. accuracy.}, 
keywords={backpropagation;eigenvalues and eigenfunctions;face recognition;feature extraction;independent component analysis;principal component analysis;face recognition;feature extraction;appropriately verifying face pictures;face detector techniques;feature based techniques;low-level analysis;face dataset;individual face;image space;indefinite faces;settled facial recognition methods;independent component analysis;principle component analysis;vector dimensional illustration;normal vector features;backpropagation neural networks;Face recognition;Face;Feature extraction;Algorithm design and analysis;Signal processing algorithms;Videos;Databases;Face recognition;Features of face;Eigen values and Vectors;Neural Network}, 
doi={10.1109/ISPCC.2017.8269662}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8003592, 
author={A. S. {Asl} and M. A. {Oskoei}}, 
booktitle={2017 5th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)}, 
title={Depth dependent invariant features applied to person detection using 3D camera}, 
year={2017}, 
volume={}, 
number={}, 
pages={29-34}, 
abstract={This paper is about detection and tracking a person by mobile robots in in-door environments, such as shopping center and hospital. It uses vision based approaches to recognize texture of clothes. The paper proposes a method to use depth (distance) reference along with scale invariant features (SIFT) to recognize patterns in various orientation, distance and illumination. SIFT is an important feature detection algorithm that is robust against rotation, translation, and scaling in 2D images and to some extent against variations in lighting conditions. But it suffers inadequate performance for visual patterns rotated in 3D space. To overcome this issue, reference inputs given to the algorithm was extended to include images taken from different angles. The proposed algorithm showed considerably improved performance in detection for real-time applications.}, 
keywords={feature extraction;image sensors;image texture;mobile robots;object recognition;object tracking;robot vision;transforms;depth dependent invariant features;person detection;3D camera;person tracking;mobile robots;shopping center;hospital;vision based approaches;cloth texture recognition;depth reference;distance reference;scale invariant features;SIFT;pattern recognition;orientation;distance;illumination;feature detection algorithm;Feature extraction;Cameras;Three-dimensional displays;Robot kinematics;Detection algorithms;Robot sensing systems;Scale invariant features;SIFT;3D images;Person detection;mobile robot}, 
doi={10.1109/CFIS.2017.8003592}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8346387, 
author={H. M. R. {Afzal} and S. {Luo} and M. K. {Afzal}}, 
booktitle={2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)}, 
title={Reconstruction of 3D facial image using a single 2D image}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In many scenarios related to image processing, 3D face reconstruction is considered an essential part. A robust and fast method of 3D face reconstruction from a single 2D image is presented. This method consists of three main steps including extraction of features from single image, calculating the depth of image and adjustment of a 3D model on the direction of Z-axis. In the first step, the features of image are extracted by using supervised descent method (SDM). Using SDM, face regions like facial components (eyes, nose lips) and face contours are detected. Second step consists of depth prediction by implementation of multivariate Gaussian distribution. Finally, 3D face is constructed with the help of features and the depth information and 3D database. The proposed method has been verified by conducting several experiments depicted in evaluation section. Our method is robust in nature and gives good results even using a single image, comparing to other methods that use multiple images for reconstruction of 3D images.}, 
keywords={face recognition;feature extraction;Gaussian distribution;gradient methods;image reconstruction;stereo image processing;3D face reconstruction;supervised descent method;SDM;face regions;3D model;facial components;face contours;depth prediction;multivariate Gaussian distribution;image processing;single 2D image;3D facial image;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Shape;Two dimensional displays;Solid modeling;3D face reconstruction;features extraction;Gaussain distribution;facial modeling}, 
doi={10.1109/ICOMET.2018.8346387}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7523133, 
author={G. {Torkhani} and A. {Ladgham} and M. N. {Mansouri} and A. {Sakly}}, 
booktitle={2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
title={Gabor-SVM applied to 3D-2D deformed mesh model}, 
year={2016}, 
volume={}, 
number={}, 
pages={447-452}, 
abstract={We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate.}, 
keywords={face recognition;feature extraction;Gabor filters;mesh generation;support vector machines;visual databases;wavelet transforms;3D-2D deformed mesh model;Gabor-SVM;3D face recognition;2D modeling;3D modeling;facial curvatures detection;3D image transformation;3D triangular mesh;2D space;2D smoother mesh;Gabor wavelets;surface curves;salient face feature detection;Gabor facial model;support vector machines;3D AJMAL face database;Three-dimensional displays;Solid modeling;Face recognition;Face;Deformable models;Feature extraction;Databases;3D face recognition;salient points;deformed mesh model;facial curvatures;Gabor wavelet;SVM}, 
doi={10.1109/ATSIP.2016.7523133}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7443802, 
author={S. {Arora} and S. {Chawla}}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={An intensified approach to face recognition through average half face}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Face recognition has broad excitement in the latest trend in image processing. Face recognition refers to identify a specific individual in digital image by analyzing and comparing patterns. It has numerous benefits which attract every sector but there are some issues such as more time consumption and lesser accuracy which degrade the user services. To solve this problem we proposed a highly accurate and fast method to reduce the execution time. The proposed method uses average half face approach because overall system's accuracy is better in it rather than using the original full face image. The proposed method can be used to recognize both 2D and 3D images. It mainly includes the average half face creation, feature detection, full face recognition through average half face using distance metrics and finally checking system's accuracy along with time consumption. The proposed method is based on eye, nose and mouth detection.}, 
keywords={face recognition;feature extraction;face recognition;average half face approach;digital image processing;half face creation;feature detection;distance metrics;eye detection;nose detection;mouth detection;Face;Face recognition;Nose;Databases;Mouth;Three-dimensional displays;Image processing;Face recognition;Image processing;Accuracy;Average half face;Distance metrics}, 
doi={10.1109/INDICON.2015.7443802}, 
ISSN={2325-9418}, 
month={Dec},}
@INPROCEEDINGS{7823978, 
author={S. {Naveen} and R. K. {Ahalya} and R. S. {Moni}}, 
booktitle={2016 International Conference on Communication Systems and Networks (ComNet)}, 
title={Multimodal face recognition using spectral transformation by LBP and polynomial coefficients}, 
year={2016}, 
volume={}, 
number={}, 
pages={13-17}, 
abstract={This paper presents a multimodal face recognition using spectral transformation by Local Binary Pattern (LBP) and Polynomial Coefficients. Here 2D image and 3D image are combined to get multimodal face recognition. In this method a novel feature extraction is done using LBP and Polynomial Coefficients. Then these features are spectrally transformed using Discrete Fourier Transform (DFT). These spectrally transformed features extracted from texture image using the two methods are combined at the score level. Similarly this is done in depth image. Finally feature information from texture and depth are combined at the score level which gives better results than the individual results.}, 
keywords={discrete Fourier transforms;face recognition;feature extraction;image texture;multimodal face recognition;spectral transformation;LBP;polynomial coefficients;local binary pattern;2D image;3D image;feature extraction;discrete Fourier transform;DFT;texture image;depth image;feature information;Feature extraction;Face recognition;Discrete Fourier transforms;Discrete cosine transforms;Databases;Face;Two dimensional displays;Texture;Depth;Local Binary Pattern (LBP);Polynomial Coefficients;Multimodal;Discrete Fourier Transform (DFT)}, 
doi={10.1109/CSN.2016.7823978}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7899769, 
author={G. {Tian} and T. {Mori} and Y. {Okuda}}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Spoofing detection for embedded face recognition system using a low cost stereo camera}, 
year={2016}, 
volume={}, 
number={}, 
pages={1017-1022}, 
abstract={Spoofing detection is essential for practical face recognition system. Based on the fact that genuine face has special geometric curvatures across surface, this paper brings forward an ultra-fast yet accurate spoofing detection approach using a low-cost stereo camera. To obtain curvatures, the three dimensional shapes of selected facial landmarks are analyzed, by fitting point cloud around each landmark to a specific partial face surface. Spoofing detection is then performed by evaluating curvatures of each landmark and integrating them together. Experiments verify that the approach is able to detect spoofed faces in printed photographs without or with various bending at FAR equal to 0.00%. Meanwhile, genuine faces have a trivial opportunity to be falsely rejected: FRR is 0.59% for near frontal faces and less than 5% for faces with large varying poses. Detection time is 51 milliseconds when executed on a single processor [1] running at a clock frequency of 266M Hz, this makes the detection very suitable for embedded face recognition system.}, 
keywords={face recognition;stereo image processing;spoofing detection;embedded face recognition system;low cost stereo camera;facial landmark 3D shapes;frequency 266 MHz;Face;Three-dimensional displays;Nose;Face recognition;Cameras;Surface fitting;Fitting;spoof detection;point cloud;surface fitting;stereo vision}, 
doi={10.1109/ICPR.2016.7899769}, 
ISSN={}, 
month={Dec},}
@ARTICLE{8408720, 
author={R. S. {Siqueira} and G. R. {Alexandre} and J. M. {Soares} and G. A. P. {Thé}}, 
journal={IEEE Robotics and Automation Letters}, 
title={Triaxial Slicing for 3-D Face Recognition From Adapted Rotational Invariants Spatial Moments and Minimal Keypoints Dependence}, 
year={2018}, 
volume={3}, 
number={4}, 
pages={3513-3520}, 
abstract={This letter presents a multiple slicing model for threedimensional (3-D) images of human face, using the frontal, sagittal, and transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2-D spatial moments of Hu and 3-D spatial invariant rotation moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs nonneutral ROC I experiment, applying linear discriminant analysis as classifier and more than one sample for training, achieved 98.7% of verification rate at 0.1% of false acceptance rate. By using the support vector machine as classifier the rank1 experiment recognition rates of 99% and 95.4% have been achieved for a neutral vs neutral and for a neutral vs nonneutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when anger and disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity.}, 
keywords={face recognition;feature extraction;image classification;support vector machines;verification rate;false acceptance rate;support vector machine;rank1 experiment recognition rates;Bosphorus Database;3-D face recognition;multiple slicing model;human face;orthogonal planes;nose tip;facial recognition;3-D spatial invariant rotation moments;neutral vs nonneutral ROC;linear discriminant analysis;adapted rotational invariant spatial moments;minimal keypoint dependence;Three-dimensional displays;Face;Feature extraction;Nose;Two dimensional displays;Robustness;Iterative closest point algorithm;Computer vision for automation;recognition;surveillance systems}, 
doi={10.1109/LRA.2018.2854295}, 
ISSN={2377-3766}, 
month={Oct},}
@ARTICLE{7265032, 
author={L. {Ma} and G. {Zheng} and J. U. H. {Eitel} and L. M. {Moskal} and W. {He} and H. {Huang}}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={Improved Salient Feature-Based Approach for Automatically Separating Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar Point Cloud Data of Forest Canopies}, 
year={2016}, 
volume={54}, 
number={2}, 
pages={679-696}, 
abstract={Accurate separation of photosynthetic and nonphotosynthetic components in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a challenging but of key importance to understand the spatial distribution of the radiation regime, photosynthetic processes, and carbon and water exchanges of the forest canopy. The objective of this paper was to improve current methods for separating photosynthetic and nonphotosynthetic components in TLS data of forest canopies by adding two additional filters only based on its geometric information. By comparing the proposed approach with the eigenvalues plus color information-based method, we found that the proposed approach could effectively improve the overall producer's accuracy from 62.12% to 95.45%, and the overall classification producer's accuracy would increase from 84.28% to 97.80% as the forest leaf area index (LAI) decreases from 4.15 to 3.13. In addition, variations in tree species had negligible effects on the final classification accuracy, as shown by the overall producer's accuracy for coniferous (93.09%) and broadleaf (94.96%) trees. To remove quantitatively the effects of the woody materials in a forest canopy for improving TLS-based LAI estimates, we also computed the “woody-to-total area ratio” based on the classified linear class points from an individual tree. Automatic classification of the forest point cloud data set will facilitate the application of TLS on retrieving 3-D forest canopy structural parameters, including LAI and leaf and woody area ratios.}, 
keywords={forestry;optical radar;photosynthesis;remote sensing by radar;salient feature-based approach;photosynthetic components;nonphotosynthetic components;terrestrial lidar point cloud data;forest canopies;TLS data;geometric information;eigenvalues;information-based method;forest leaf area index;Three-dimensional displays;Vegetation;Laser radar;Eigenvalues and eigenfunctions;Accuracy;Surface emitting lasers;Probability density function;Light detection and ranging (lidar);pattern recognition;point classification;terrestrial laser scanning (TLS);woody-to-total area ratio;Light detection and ranging (lidar);pattern recognition;point classification;terrestrial laser scanning (TLS);woody-to-total area ratio}, 
doi={10.1109/TGRS.2015.2459716}, 
ISSN={0196-2892}, 
month={Feb},}
@INPROCEEDINGS{8661454, 
author={A. {Camarena-Ibarrola} and M. {Castro-Coria} and K. {Figueroa}}, 
booktitle={2018 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)}, 
title={Cloud Point Matching for Text-Independent Speaker Identification}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In Text-Independent speaker identification, the individual that produced some captured speech signal has to be identified without his collaboration, he might not even know that he is being the subject of an identification process. The system could not ask the individual to utter some specific word or phrase, which is precisely what is done in Text-Dependent speaker recognition. Text-Independent speaker identification is far more complicated since we cannot simply measure the similarity of an utterance of a word or phrase to another utterance made by the same speaker of the same word or phrase in which case we could use the dynamics of the speech signal. In this paper we search in the speech signal looking for voiced speech segments and estimate its first three formants, so we end up with a three-dimensional point cloud for each speaker of the collection of known speakers. To identify a speaker we have to measure the similarity of a point-cloud from an unknown speaker to the point-clouds that belong to known speakers, we do that by searching for local structures in the cloud in a way that is highly scalable and robust. We performed tests with both a collection of our own in Spanish and with the English Language Speech Database for Speaker Recognition (ELSDSR) from the Technical University of Denmark achieving results that improve recent published work with ELSDSR.}, 
keywords={speaker recognition;speech processing;cloud point matching;captured speech signal;three-dimensional point cloud;text-independent speaker identification;text-dependent speaker recognition;English Language Speech Database for Speaker Recognition;Technical University of Denmark;Three-dimensional displays;Speaker recognition;Feature extraction;Correlation;Mel frequency cepstral coefficient;Speech processing;Databases}, 
doi={10.1109/ROPEC.2018.8661454}, 
ISSN={2573-0770}, 
month={Nov},}
@ARTICLE{7524772, 
author={C. {Benedek} and B. {Gálai} and B. {Nagy} and Z. {Jankó}}, 
journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
title={Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance System}, 
year={2018}, 
volume={28}, 
number={1}, 
pages={101-113}, 
abstract={This paper presents new approaches for gait and activity analysis based on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed algorithms are embedded into an integrated 4D vision and visualization system, which is able to analyze and interactively display real scenarios in natural outdoor environments with walking pedestrians. The main focus of the investigations is gait-based person reidentification during tracking and recognition of specific activity patterns, such as bending, waving, making phone calls, and checking the time looking at wristwatches. The descriptors for training and recognition are observed and extracted from realistic outdoor surveillance scenarios, where multiple pedestrians are walking in the field of interest following possibly intersecting trajectories; thus, the observations might often be affected by occlusions or background noise. Since there is no public database available for such scenarios, we created and published a new Lidar-based outdoor gait and activity data set on our website that contains point cloud sequences of 28 different persons extracted and aggregated from 35-min-long measurements. The presented results confirm that both efficient gait-based identification and activity recognition are achievable in the sparse point clouds of a single RMB Lidar sensor. After extracting the people trajectories, we synthesized a free-viewpoint video, in which moving avatar models follow the trajectories of the observed pedestrians in real time, ensuring that the leg movements of the animated avatars are synchronized with the real gait cycles observed in the Lidar stream.}, 
keywords={gait analysis;image motion analysis;image recognition;optical radar;gait analysis;activity recognition;4D surveillance system;walking pedestrians;person reidentification;point cloud sequences;sparse point clouds;gait cycles;Lidar stream;multibeam Lidar sensor;RMB Lidar sensor;Laser radar;Three-dimensional displays;Trajectory;Surveillance;Legged locomotion;Databases;Visualization;4D reconstruction;activity recognition;gait recognition;multibeam Lidar}, 
doi={10.1109/TCSVT.2016.2595331}, 
ISSN={1051-8215}, 
month={Jan},}
@INPROCEEDINGS{8389776, 
author={G. {Geetha} and M. {Safa} and C. {Fancy} and K. {Chittal}}, 
booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)}, 
title={3D face recognition using Hadoop}, 
year={2017}, 
volume={}, 
number={}, 
pages={1882-1885}, 
abstract={Face Recognition is one of the biometric technique to vestige the given faces. We present a 3D face recognition method using Hadoop to recognize 3D faces under varying expressions, lighting and different poses to overcome the challenges of the 2D face recognition. In this paper, a threshold facial region of an image is detected and preprocessing is done based through the image excellence. If the selected face is frontal face with good lighting, extract the prerequisite features and do the necessary comparison steps to recognize the faces. In case, if the selected face is in bad lighting, then perform histogram equalization and normalization to increase the contrast. Different Poses and expressions are the very challenging zones which require surplus pre-processing to improve the performance of the face recognition system. Hence an enhanced normalization method called 3D Morphable Model are used as a pre- processing technique to create a frontal view from a non-frontal view and also merge images with different views in to a single frontal view. Next to diminish the number of features used for recognition process; we emulate the linear discriminant analysis method for further classification. Eventually, we used an open-source Hadoop Image Processing Interface (HIPI) to act as an interface for MapReduce technology for recognition.}, 
keywords={biometrics (access control);face recognition;feature extraction;parallel processing;frontal face;biometric technique;3D face recognition method;histogram equalization;surplus preprocessing;enhanced normalization method;3D morphable model;single frontal view;nonfrontal view;linear discriminant analysis method;open-source Hadoop image processing interface;HIPI;MapReduce technology;Face;Face recognition;Three-dimensional displays;Lighting;Feature extraction;Solid modeling;Hadoop;Image Processing;Map Reduce;Linear Discriminant analysis}, 
doi={10.1109/ICECDS.2017.8389776}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8125644, 
author={A. V. {Kumar} and V. V. R. {Prasad} and K. M. {Bhurchandi} and V. R. {Satpute} and L. {Pious} and S. {Kar}}, 
booktitle={2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
title={Dense reconstruction of 3D human face using 5 images and no reference model}, 
year={2017}, 
volume={}, 
number={}, 
pages={1185-1190}, 
abstract={3D reconstruction of human faces is of high importance in applications like forensics, facial features based human tracking and realistic reconstruction of human faces in virtual reality applications. The published algorithms so far require either a large number of views of a human face or a 3D facial reference model. This paper proposes a technique for 3D human face reconstruction using only five views without any reference model unlike most of the contemporary facial reconstruction techniques. Face localization is done followed by facial feature point extraction in the facial images. The features are tracked in the other images using the Kanade-Lucas-Tomasi (KLT) object tracking algorithm. 3D location of each feature is calculated in all the images using triangulation and a 3D point cloud is formed. The Point cloud is processed to remove outliers and holes. The 3D face is then formed by interpolating and meshing the point cloud. This computationally efficient and low cost technique outperforms commercial and online available software and published algorithms.}, 
keywords={face recognition;feature extraction;image reconstruction;object tracking;solid modelling;virtual reality;dense reconstruction;facial features;human tracking;3D facial reference model;contemporary facial reconstruction techniques;face localization;facial feature point extraction;facial images;point cloud;3D human face;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Skin;Image color analysis;Cameras;3D Reconstruction;meshing;point cloud;tracking;triangulation}, 
doi={10.1109/CoDIT.2017.8125644}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7943450, 
author={S. {Rajeev} and K. K. M. {Shreyas} and K. {Panetta} and S. {Agaian}}, 
booktitle={2017 IEEE International Symposium on Technologies for Homeland Security (HST)}, 
title={3-D palmprint modeling for biometric verification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Palmprint is a very unique and distinctive biometric trait because of features such as a person's inimitable principal lines, wrinkles, delta points, and minutiae. These constitute the main reasons why palmprint verification is considered as one of the most reliable personal identification methods. However, a clear majority of the research on palm-prints are concentrated on 2-D palmprint images irrespective of the fact that the human palm is a 3D-surface. While 2-D palmprint recognition has proved to be efficient in terms of verification rate, it has some essential downsides. These restrictions can adversely affect the performance and robustness of the palmprint recognition system. One of the possible solutions to resolve the limitations associated with 2-D palm print authentication systems is (i) to use a 3-D scanning system and to produce high quality 3-D images with depth information; (ii) to map 3-D palm-print images into 2-D images which may support the usage of 3-D images with both biometric palmprint 2-D image databases and 2-D palmprint recognition tools. The bloom of 3-D technologies has made it easier to capture and store 3-D images. The problem of a direct mapping approach is that a large section of the palm is hard-pressed on the scanner surface during 2-D based acquisition. This paper proposes a novel technique to unravel/map 3-D palm images to its equivalent 2-D palm-print image. This image can be then used to perform efficient and accurate 2-D identification/ verification. Experimental results and discussions will also be presented.}, 
keywords={authorisation;biometrics (access control);computer graphics;palmprint recognition;biometric verification;biometric trait;2-D palmprint images;2-D palmprint recognition;2-D palmprint authentication systems;3-D palmprint images;biometric palmprint 2-D image databases;2-D palmprint recognition tools;2-D identification-verification;Fingerprint recognition;Databases;Solid modeling;Mathematical model;Biomedical imaging;Feature extraction;biometric;palm-print;2-D;3-D;authentication;verification;identification;unrolling;mapping;modeling;security}, 
doi={10.1109/THS.2017.7943450}, 
ISSN={}, 
month={April},}
@ARTICLE{7050293, 
author={G. {Mills} and G. {Fotopoulos}}, 
journal={IEEE Geoscience and Remote Sensing Letters}, 
title={Rock Surface Classification in a Mine Drift Using Multiscale Geometric Features}, 
year={2015}, 
volume={12}, 
number={6}, 
pages={1322-1326}, 
abstract={Scale-dependent statistical depictions of surface morphology offer the potential to parameterize complex geometrical scaling relationships with greater detail than traditional fractal measures. Using multiscale operators, it is possible to identify points belonging to rough discontinuous surfaces in noisy point clouds solely on the basis of their local geometry. Many strategies for point cloud feature classification have been developed since the proliferation of laser scanning systems. Most of the techniques which are applicable to natural scenes employ external data sources such as hyperspectral imagery, return pulse intensity, and waveform data. In this letter, multiscale geometric parameters are used to identify individual point observations corresponding to rock surfaces in point clouds acquired by terrestrial laser scanning in scenes with man-made clutter and scanning artifacts. Three multiscale operators, namely, the approximate shape and density of a defined neighborhood and the distance of its mean point from its geometric center, are fused into a single feature vector. The procedure is demonstrated using real point cloud data acquired in a mine drift, with the goal of identifying points belonging to the rock face obscured by an overlying wire support mesh. Using the extra-trees classifier, extraneous returns caused by the mesh were excluded from the point cloud with a 97% success rate, while 87% of the desired surface points were retained.}, 
keywords={feature extraction;geometry;geophysical signal processing;mining;remote sensing by laser beam;rocks;surface morphology;rock surface classification;mine drift;multiscale geometric features;scale-dependent statistical depictions;surface morphology;complex geometrical scaling relationships;traditional fractal measures;multiscale operators;rough discontinuous surfaces;noisy point clouds;local geometry;point cloud feature classification;laser scanning systems;natural scenes;external data sources;hyperspectral imagery;return pulse intensity;waveform data;multiscale geometric parameters;terrestrial laser scanning;man-made clutter;scanning artifacts;defined neighborhood;single feature vector;real point cloud data;rock face;overlying wire support mesh;extra-trees classifier;Three-dimensional displays;Rocks;Rough surfaces;Surface roughness;Fractals;Surface morphology;Classification;geology;mining industry;point clouds;terrestrial LiDAR;Classification;geology;mining industry;point clouds;terrestrial LiDAR}, 
doi={10.1109/LGRS.2015.2398814}, 
ISSN={1545-598X}, 
month={June},}
@INPROCEEDINGS{8634657, 
author={S. A. {Ali Shah} and M. {Bennamoun} and M. {Molton}}, 
booktitle={2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)}, 
title={A Fully Automatic Framework for Prediction of 3D Facial Rejuvenation}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={How will I look afterwards? is a common question asked by the patients undergoing a cosmetic procedure. Cosmetic practitioners at present can only offer subjective and descriptive replies. This subjective prediction is a serious concern for patients undergoing cosmetic treatment and therefore necessitates the development of automatic techniques for facial quantification. This paper proposes a novel machine learning approach to quantify and predict the outcome of 3D facial rejuvenation prior to actual cosmetic procedure. The facial rejuvenation prediction results are achieved by estimating the dermal filler volume in 3D faces. This involves estimation of structural changes in 3D face images and to learn underlying structural mapping. Our preliminary experimental results show that the proposed model achieves superior prediction accuracy on real world dataset compared to baseline methods. The computational time analysis shows that the proposed technique is very efficient (at test time) which makes it suitable for real time applications.}, 
keywords={cosmetics;face recognition;learning (artificial intelligence);superior prediction accuracy;fully automatic framework;3D facial rejuvenation;common question;cosmetic practitioners;cosmetic treatment;automatic techniques;facial quantification;actual cosmetic procedure;facial rejuvenation prediction results;3D face images;Three-dimensional displays;Prediction algorithms;Solid modeling;Predictive models;Training;Machine learning;Australia;Facial rejuvenation;Predictive Modeling;Machine Learning}, 
doi={10.1109/IVCNZ.2018.8634657}, 
ISSN={2151-2205}, 
month={Nov},}
@INPROCEEDINGS{7780900, 
author={T. {Bolkart} and S. {Wuhrer}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Robust Multilinear Model Learning Framework for 3D Faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={4911-4919}, 
abstract={Multilinear models are widely used to represent the statistical variations of 3D human faces as they decouple shape changes due to identity and expression. Existing methods to learn a multilinear face model degrade if not every person is captured in every expression, if face scans are noisy or partially occluded, if expressions are erroneously labeled, or if the vertex correspondence is inaccurate. These limitations impose requirements on the training data that disqualify large amounts of available 3D face data from being usable to learn a multilinear model. To overcome this, we introduce the first framework to robustly learn a multilinear model from 3D face databases with missing data, corrupt data, wrong semantic correspondence, and inaccurate vertex correspondence. To achieve this robustness to erroneous training data, our framework jointly learns a multilinear model and fixes the data. We evaluate our framework on two publicly available 3D face databases, and show that our framework achieves a data completion accuracy that is comparable to state-of-the-art tensor completion methods. Our method reconstructs corrupt data more accurately than state-of-the-art methods, and improves the quality of the learned model significantly for erroneously labeled expressions.}, 
keywords={computer graphics;face recognition;learning (artificial intelligence);visual databases;robust multilinear model learning framework;3D faces;statistical variations;3D human faces;multilinear face model;face scans;3D face databases;missing data;corrupt data;semantic correspondence;inaccurate vertex correspondence;erroneous training data;data completion accuracy;tensor completion methods;Data models;Computational modeling;Three-dimensional displays;Solid modeling;Semantics;Robustness;Shape}, 
doi={10.1109/CVPR.2016.531}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{8099972, 
author={L. {Sheng} and J. {Cai} and T. {Cham} and V. {Pavlovic} and K. N. {Ngan}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Generative Model for Depth-Based Robust 3D Facial Pose Tracking}, 
year={2017}, 
volume={}, 
number={}, 
pages={4598-4607}, 
abstract={We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face models visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.}, 
keywords={face recognition;object tracking;pose estimation;ray tracing;statistical analysis;generative model;arbitrary facial expression variations;generative framework;face models visibility;depth-based robust 3D facial pose tracking;unconstrained scenarios;face model adaptation;statistical 3D face model;uncertainty prediction;distribution prediction;ray visibility constraint;Biwi datasets;ICT-3DHP datasets;Face;Three-dimensional displays;Solid modeling;Adaptation models;Robustness;Shape;Probabilistic logic}, 
doi={10.1109/CVPR.2017.489}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{7139616, 
author={T. {Linder} and S. {Wehner} and K. O. {Arras}}, 
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Real-time full-body human gender recognition in (RGB)-D data}, 
year={2015}, 
volume={}, 
number={}, 
pages={3039-3045}, 
abstract={Understanding social context is an important skill for robots that share a space with humans. In this paper, we address the problem of recognizing gender, a key piece of information when interacting with people and understanding human social relations and rules. Unlike previous work which typically considered faces or frontal body views in image data, we address the problem of recognizing gender in RGB-D data from side and back views as well. We present a large, gender-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the Kinect v1 and Kinect v2 sensor. We then learn and compare several classifiers on the Kinect v2 data using a HOG baseline, two state-of-the-art deep-learning methods, and a recent tessellation-based learning approach. Originally developed for person detection in 3D data, the latter is able to learn the best selection, location and scale of a set of simple point cloud features. We show that for gender recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 Hz.}, 
keywords={human-robot interaction;image classification;image sensors;learning (artificial intelligence);object detection;object recognition;real-time full-body human gender recognition;RGB-D data;human social relations;human social rules;image data;multiperspective RGB-D dataset;gender-balanced RGB-D dataset;annotated RGB-D dataset;Kinect v2 sensor;Kinect v1 sensor;HOG baseline;deep-learning methods;tessellation-based learning approach;person detection;point cloud features;Three-dimensional displays;Robot sensing systems;Training;Legged locomotion;Accuracy;Support vector machines}, 
doi={10.1109/ICRA.2015.7139616}, 
ISSN={1050-4729}, 
month={May},}
@ARTICLE{7312454, 
author={M. A. {de Jong} and A. {Wollstein} and C. {Ruff} and D. {Dunaway} and P. {Hysi} and T. {Spector} and F. {Liu} and W. {Niessen} and M. J. {Koudstaal} and M. {Kayser} and E. B. {Wolvius} and S. {Böhringer}}, 
journal={IEEE Transactions on Image Processing}, 
title={An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}, 
year={2016}, 
volume={25}, 
number={2}, 
pages={580-588}, 
abstract={In this paper, we present a novel approach to automatic 3D facial landmarking using 2D Gabor wavelets. Our algorithm considers the face to be a surface and uses map projections to derive 2D features from raw data. Extracted features include texture, relief map, and transformations thereof. We extend an established 2D landmarking method for simultaneous evaluation of these data. The method is validated by performing landmarking experiments on two data sets using 21 landmarks and compared with an active shape model implementation. On average, landmarking error for our method was 1.9 mm, whereas the active shape model resulted in an average landmarking error of 2.3 mm. A second study investigating facial shape heritability in related individuals concludes that automatic landmarking is on par with manual landmarking for some landmarks. Our algorithm can be trained in 30 min to automatically landmark 3D facial data sets of any size, and allows for fast and robust landmarking of 3D faces.}, 
keywords={face recognition;feature extraction;Gabor filters;wavelet transforms;automatic 3D facial landmarking algorithm;2D Gabor wavelets;map projections;feature extraction;data sets;active shape model;landmarking error;facial shape heritability;automatic landmarking;Three-dimensional displays;Face;Ellipsoids;Accuracy;Training;Solid modeling;Nose;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Algorithms;Anatomic Landmarks;Face;Humans;Imaging, Three-Dimensional;Pattern Recognition, Automated;Wavelet Analysis}, 
doi={10.1109/TIP.2015.2496183}, 
ISSN={1057-7149}, 
month={Feb},}
@ARTICLE{8024025, 
author={C. {Ye} and X. {Qian}}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired}, 
year={2018}, 
volume={26}, 
number={2}, 
pages={441-450}, 
abstract={This paper presents a 3-D object recognition method and its implementation on a robotic navigation aid to allow real-time detection of indoor structural objects for the navigation of a blind person. The method segments a point cloud into numerous planar patches and extracts their inter-plane relationships (IPRs).Based on the existing IPRs of the object models, the method defines six high level features (HLFs) and determines the HLFs for each patch. A Gaussian-mixture-model-based plane classifier is then devised to classify each planar patch into one belonging to a particular object model. Finally, a recursive plane clustering procedure is used to cluster the classified planes into the model objects. As the proposed method uses geometric context to detect an object, it is robust to the object's visual appearance change. As a result, it is ideal for detecting structural objects (e.g., stairways, doorways, and so on). In addition, it has high scalability and parallelism. The method is also capable of detecting some indoor non-structural objects. Experimental results demonstrate that the proposed method has a high success rate in object recognition.}, 
keywords={feature extraction;Gaussian processes;geometry;handicapped aids;image segmentation;mobile robots;object recognition;path planning;pattern clustering;robot vision;nonstructural objects;robotic navigation aid;real-time detection;indoor structural objects;numerous planar patches;object models;HLFs;Gaussian-mixture-model;plane classifier;planar patch;particular object model;recursive plane clustering procedure;point cloud;3D-object recognition;interplane relationships;Object recognition;Three-dimensional displays;Feature extraction;Visualization;Navigation;Cameras;RNA;Robotic navigation aid;visually impaired;3D object recognition;geometric context;Gaussian mixture model}, 
doi={10.1109/TNSRE.2017.2748419}, 
ISSN={1534-4320}, 
month={Feb},}
@INPROCEEDINGS{7163161, 
author={S. {Cheng} and I. {Marras} and S. {Zafeiriou} and M. {Pantic}}, 
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
title={Active nonrigid ICP algorithm}, 
year={2015}, 
volume={1}, 
number={}, 
pages={1-8}, 
abstract={The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15-20 years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region.}, 
keywords={face recognition;statistical analysis;facial surfaces;iterative closest point algorithm;nonrigid iterative closest point algorithms;3D face fitting;mouth region;statistical 3D model;3D facial deformation;single facial model;3D facial shape;3D mesh;3D facial model;active nonrigid ICP algorithm;Face;Three-dimensional displays;Iterative closest point algorithm;Solid modeling;Deformable models;Shape;Mouth}, 
doi={10.1109/FG.2015.7163161}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7299095, 
author={S. Z. {Gilani} and F. {Shafait} and A. {Mian}}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Shape-based automatic detection of a large number of 3D facial landmarks}, 
year={2015}, 
volume={}, 
number={}, 
pages={4639-4648}, 
abstract={We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morphologically significant. The proposed algorithm evolves level set curves with adaptive geometric speed functions to automatically extract effective seed points for dense correspondence. Correspondences are established by minimizing the bending energy between patches around seed points of given faces to those of a reference face. Given its hierarchical structure, our algorithm is capable of establishing thousands of correspondences between a large number of faces. Finally, a morphable model based on the dense corresponding points is fitted to an unseen query face for transfer of correspondences and hence automatic detection of landmarks. The proposed algorithm can detect any number of pre-defined landmarks including subtle landmarks that are even difficult to detect manually. Extensive experimental comparison on two benchmark databases containing 6, 507 scans shows that our algorithm outperforms six state of the art algorithms.}, 
keywords={face recognition;feature extraction;geometry;minimisation;shape recognition;shape-based automatic detection;3D facial landmark;geometric speed function;bending energy minimization;Three-dimensional displays;Shape;Databases;Level set;Algorithm design and analysis;Feature extraction;Mathematical model}, 
doi={10.1109/CVPR.2015.7299095}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{8314888, 
author={G. {Torkhani} and A. {Ladgham} and A. {Sakly}}, 
booktitle={2017 18th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering (STA)}, 
title={3D Gabor-Edge filters applied to face depth images}, 
year={2017}, 
volume={}, 
number={}, 
pages={578-582}, 
abstract={This manuscript introduces a novel 3D face authentication system inspired from the advantageous capacities of Gabor-Edge filters. The approach studies 3D face difficulties such as expression variety, different rotations and exposure to illuminations. The proposed systems starts by preprocessing the 3D face images to resolve acquisition problems. Then, a filtering process is performed by implanting our 3D Gabor-Edge technique extended based on the classic 3D Gabor masks. The next step is to achieve the classification of facial features from the edge saliency by the artificial Neural Network Classifier (NNC). The evaluation of the adopted system is achieved by exporting common datasets from GavabDB database. Experimental results are reported to prove the high accuracy rates of our method compared to the recent researches in the same biometric field.}, 
keywords={biometrics (access control);edge detection;face recognition;feature extraction;Gabor filters;image classification;neural nets;3D face difficulties;3D face images;3D Gabor-Edge technique;classic 3D Gabor masks;edge saliency;Gabor-edge filters;3D face authentication system;acquisition problems;facial feature classification;artificial neural network classifier;GavabDB database;biometric field;Three-dimensional displays;Face;Feature extraction;Authentication;Gabor filters;Face recognition;face authentication;Gabor filtering;3D images;saliency}, 
doi={10.1109/STA.2017.8314888}, 
ISSN={2573-539X}, 
month={Dec},}
@INPROCEEDINGS{7852696, 
author={F. {Li} and C. {Lai} and S. {Jin} and Y. {Peng}}, 
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={Automatic calibration of 3D human faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={135-139}, 
abstract={This paper presents a method for correcting the posture of 3D face with unknown position and posture based on the standard face pose. We define a standard posture of face which is not affected by the head gesture. Then the algorithms of principal component analysis and iterative closest point are used to achieve the preliminary adjustment of the target model. For the precise calibration, we use the method of depth values iterative searching to find the point of nose precisely, and propose an algorithm of searching along a straight line to find the ridge line of nose. Ultimately we determine the difference between the posture of the target model and standard posture to obtain accurate calculation of the target model posture.}, 
keywords={calibration;computer graphics;face recognition;principal component analysis;automatic calibration;3D human faces;standard face pose;standard posture;head gesture;principal component analysis;iterative closest point;precise calibration;iterative searching;target model posture;Three-dimensional displays;Calibration;Nose;Standards;Iterative closest point algorithm;Solid modeling;Principal component analysis;3D Faces;Posture Correction;Face Calibration;Facial Feature Points}, 
doi={10.1109/CISP-BMEI.2016.7852696}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8317846, 
author={R. {Varga} and A. {Costea} and H. {Florea} and I. {Giosan} and S. {Nedevschi}}, 
booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)}, 
title={Super-sensor for 360-degree environment perception: Point cloud segmentation using image features}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={This paper describes a super-sensor that enables 360-degree environment perception for automated vehicles in urban traffic scenarios. We use four fisheye cameras, four 360-degree LIDARs and a GPS/IMU sensor mounted on an automated vehicle to build a super-sensor that offers an enhanced low-level representation of the environment by harmonizing all the available sensor measurements. Individual sensors cannot provide a robust 360-degree perception due to their limitations: field of view, range, orientation, number of scanning rays, etc. The novelty of this work consists of segmenting the 3D LIDAR point cloud by associating it with the 2D image semantic segmentation. Another contribution is the sensor configuration that enables 360-degree environment perception. The following steps are involved in the process: calibration, timestamp synchronization, fisheye image unwarping, motion correction of LIDAR points, point cloud projection onto the images and semantic segmentation of images. The enhanced low-level representation will improve the high-level perception environment tasks such as object detection, classification and tracking.}, 
keywords={image classification;image motion analysis;image representation;image resolution;image segmentation;image sensors;object detection;object tracking;optical radar;360-degree environment perception;point cloud segmentation;automated vehicle;360-degree LIDARs;GPS/IMU sensor;3D LIDAR point cloud;sensor measurements;image semantic segmentation;Conferences;Intelligent transportation systems;automated driving;environment perception;fisheye images;3D LIDAR points;360-degree perception;super-sensor}, 
doi={10.1109/ITSC.2017.8317846}, 
ISSN={2153-0017}, 
month={Oct},}
@INPROCEEDINGS{7550062, 
author={ and D. {Huang} and Y. {Wang} and J. {Sun}}, 
booktitle={2016 International Conference on Biometrics (ICB)}, 
title={Lock3DFace: A large-scale database of low-cost Kinect 3D faces}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future.}, 
keywords={emotion recognition;face recognition;image texture;pose estimation;video databases;video signal processing;large-scale database;low-cost Kinect 3D face videos;Lock3DFace database;3D face analysis;3D face recognition;3D FR;expression analysis;pose analysis;occlusion analysis;time lapse analysis;texture analysis;near infrared channels;lighting condition;radiation intensity;Three-dimensional displays;Databases;Videos;Solid modeling;Two dimensional displays;Lighting;Sensors}, 
doi={10.1109/ICB.2016.7550062}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7729385, 
author={C. {Gevaert} and C. {Persello} and R. {Sliuzas} and G. {Vosselman}}, 
booktitle={2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)}, 
title={Integration of 2D and 3D features from UAV imagery for informal settlement classification using Multiple Kernel Learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={1508-1511}, 
abstract={Informal settlement upgrading projects require high-resolution and up-to-date thematic maps in order to plan and design effective interventions. To this end, Unmanned Aerial Vehicles (UAVs) provide the opportunity to obtain very high resolution 2D orthomosaics and 3D point clouds where and when needed. The heterogeneous, dense structures which typically make up an informal settlement motivate the importance of integrating complex 2D and 3D features obtained from UAV data into a single classification problem. Multiple Kernel Learning (MKL) Support Vector Machines (SVMs) maintain the distinct characteristics of the different feature spaces by optimizing individual kernels for specific feature groups which are later combined into a single kernel used for classification. Both the kernel parameters and kernel weights can be optimized by considering the alignment between the kernel and an ideal kernel which would perfectly classify the samples. This paper demonstrates how extracting high-level features from both the 2D orthomosaic as well as the 3D point cloud (obtained by an UAV), and integrating them through a MKL approach, can obtain an Overall Accuracy of 90.29%, a 4% increase over the results obtained using single kernel methods.}, 
keywords={autonomous aerial vehicles;geophysical image processing;image classification;land use;remote sensing;support vector machines;informal settlement classification;multiple kernel learning;unmanned aerial vehicle;UAV imagery;2D orthomosaics;3D point cloud;support vector machine;Kernel;Three-dimensional displays;Feature extraction;Support vector machines;Two dimensional displays;Unmanned aerial vehicles;Vegetation mapping;informal settlements;image classification;unmanned aerial vehicles;support vector machines;multiple kernel learning}, 
doi={10.1109/IGARSS.2016.7729385}, 
ISSN={2153-7003}, 
month={July},}
@ARTICLE{8440881, 
author={A. {Switonski} and T. {Krzeszowski} and H. {Josinski} and B. {Kwolek} and K. {Wojciechowski}}, 
journal={IET Biometrics}, 
title={Gait recognition on the basis of markerless motion tracking and DTW transform}, 
year={2018}, 
volume={7}, 
number={5}, 
pages={415-422}, 
abstract={In this study, a framework for view-invariant gait recognition on the basis of markerless motion tracking and dynamic time warping (DTW) transform is presented. The system consists of a proposed markerless motion capture system as well as introduced classification method of mocap data. The markerless system estimates the three-dimensional locations of skeleton driven joints. Such skeleton-driven point clouds represent poses over time. The authors align point clouds in every pair of frames by calculating the minimal sum of squared distances between the corresponding joints. A point cloud distance measure with temporal context has been utilised ink-nearest neighbours algorithm to compare time instants of motion sequences. To enhance the generalisation of the recognition and to shorten the processing time, for every individual a single multidimensional time series among several multidimensional time series describing the individual's gait is established. The correct classification rate has been determined on the basis of a real dataset of human gait. It contains 230 gait cycles of 22 subjects. The tracking results on the basis of markerless motion capture are referenced to Vicon system, whereas the achieved accuracies of recognition are compared with the ones obtained by DTW that is based on rotational data.}, 
keywords={gait analysis;image classification;image motion analysis;image sequences;learning (artificial intelligence);object tracking;time series;transforms;multidimensional time series;motion sequences;k-nearest neighbours algorithm;skeleton-driven point clouds;classification method;markerless motion capture system;dynamic time warping transform;view-invariant gait recognition;DTW transform;markerless motion tracking;gait recognition}, 
doi={10.1049/iet-bmt.2017.0134}, 
ISSN={2047-4938}, 
month={},}
@INPROCEEDINGS{7797090, 
author={S. Z. {Gilani} and A. {Mian}}, 
booktitle={2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)}, 
title={Towards Large-Scale 3D Face Recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases.}, 
keywords={convolution;face recognition;feature extraction;image matching;learning (artificial intelligence);solid modelling;large-scale 3D face recognition;fully convolutional deep network;FCDN;seed points extraction;level set curves;single curvature dependent adaptive speed;dense correspondence;training set;surface matching;morphable model fitting;landmark localization results;FRGCv2 dataset;Bosphorus dataset;query face recognition;Three-dimensional displays;Face;Face recognition;Solid modeling;Databases;Two dimensional displays;Robustness}, 
doi={10.1109/DICTA.2016.7797090}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8373915, 
author={W. {Tian} and F. {Liu} and Q. {Zhao}}, 
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)}, 
title={Landmark-Based 3D Face Reconstruction from an Arbitrary Number of Unconstrained Images}, 
year={2018}, 
volume={}, 
number={}, 
pages={774-779}, 
abstract={In this paper, we propose a novel method for reconstructing 3D faces from 2D images. The method is characterized in three aspects. (i) It utilizes only geometric cues in the input images, i.e., 2D facial landmarks. (ii) It works for an arbitrary number of unconstrained images, both single and multiple images. (iii) It can effectively exploit complementary information in multiple images of varying poses and expressions. The method is implemented based on cascaded regression in shape space. We have evaluated the method on three databases and observed from the experimental results that (i) the reconstruction error is reduced as more images of different poses are used, (ii) the proposed method can obtain comparable reconstruction results by using state-of-the-art automated methods to detect the 2D landmarks, and (iii) the proposed method is robust to variations in facial expressions and image qualities.}, 
keywords={face recognition;image reconstruction;regression analysis;stereo image processing;cascaded regression;image qualities;facial expressions;reconstruction error;2D facial landmarks;geometric cues;unconstrained images;3D face reconstruction;Conferences;Face;Gesture recognition;3D face reconstruction;unconstrained images;landmark based;cascade regression}, 
doi={10.1109/FG.2018.00122}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7899989, 
author={D. {Kim} and J. {Choi} and J. T. {Leksut} and G. {Medioni}}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Expression invariant 3D face modeling from an RGB-D video}, 
year={2016}, 
volume={}, 
number={}, 
pages={2362-2367}, 
abstract={We aim to reconstruct an accurate neutral 3D face model from an RGB-D video in the presence of extreme expression changes. Since each depth frame, taken by a low-cost sensor, is noisy, point clouds from multiple frames can be registered and aggregated to build an accurate 3D model. However, direct aggregation of multiple data produces erroneous results in natural interaction (e.g., talking and showing expressions). We propose to analyze facial expression from an RGB frame and neutralize the corresponding 3D point cloud if needed. We first estimate the person's expression by fitting blendshape coefficients using 2D facial landmarks for each frame and calculate an expression deformity (expression score). With the estimated expression score, we determine whether an input face is neutral or non-neutral. If the face is non-neutral, we proceed to neutralize the expression of the 3D point cloud in that frame. To neutralize the 3D point cloud of a face, we deform our generic 3D face model by applying the estimated blendshape coefficients, find displacement vectors from the deformed generic face to a neutral generic face, and apply the displacement vectors to the input 3D point cloud. After preprocessing frames in a video, we rank frames based on the expression scores and register the ranked frames into a single 3D model. Our system produces a neutral 3D face model in the presence of extreme expression changes even when neutral faces do not exist in the video.}, 
keywords={data aggregation;image reconstruction;image registration;video signal processing;expression invariant 3D face modeling;RGB-D video;depth frame;noisy point clouds;low-cost sensor;multiple data direct aggregation;3D point cloud;blendshape coefficients;2D facial landmarks;displacement vectors;neutral generic face;video preprocessing frames;frame ranking;person expression estimation;Three-dimensional displays;Face;Solid modeling;Two dimensional displays;Deformable models;Computational modeling;Iterative closest point algorithm}, 
doi={10.1109/ICPR.2016.7899989}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7351660, 
author={R. J. {Arteaga} and S. J. {Ruuth}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={Laplace-Beltrami spectra for shape comparison of surfaces in 3D using the closest point method}, 
year={2015}, 
volume={}, 
number={}, 
pages={4511-4515}, 
abstract={The need to compare separate objects arises in a wide range of applications. In one approach for comparing objects, `Shape-DNA' is constructed to give a numerical fingerprint representing an individual object. Shape-DNA is a cropped set of eigenvalues of the Laplace-Beltrami operator for the surface of the object. In this paper, we compute the Shape-DNA of surfaces using the closest point method. Our approach may be applied to a variety of surface representations including triangulations, point clouds and certain analytical shapes. A 2D multidimensional scaling plot illustrates that similar objects form groups based on the Shape-DNAs. Our method has the benefit that it may be applied to surfaces defined by dense point clouds without requiring the construction of point connectivity.}, 
keywords={DNA;eigenvalues and eigenfunctions;fingerprint identification;image representation;Laplace transforms;shape recognition;Laplace-Beltrami spectra;shape comparison;closest point method;shape-DNA;numerical fingerprint representation;eigenvalues;surface representations;point clouds;2D multidimensional scaling plot;Shape;Three-dimensional displays;Eigenvalues and eigenfunctions;Standards;Interpolation;Rabbits;shape comparison;Laplace-Beltrami spectra;closest point method;point cloud}, 
doi={10.1109/ICIP.2015.7351660}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7988861, 
author={A. T. {Angonese} and P. F. {Ferreira Rosa}}, 
booktitle={2017 International Conference on Military Technologies (ICMT)}, 
title={Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform}, 
year={2017}, 
volume={}, 
number={}, 
pages={779-786}, 
abstract={This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.}, 
keywords={mobile robots;neural nets;object detection;object recognition;operating systems (computers);path planning;robot vision;SLAM (robots);multiple people detection;multiple people identification system;dynamic simultaneous localization and mapping system;autonomous mobile robotic platform;robot exploration;robot navigation;Pioneer 3DX robot;RGBD camera;Sick Lms200 sensor;robot operating system;SLAM;HOG detector;histogram of oriented gradients;deep convolutional neural network;CNN;MatLab MatConvNet library;Simultaneous localization and mapping;Libraries;Computer architecture;Feature extraction;Operating systems;People Detection;HOG;Deep Learning;CNN;Simultaneous Localization and Mapping (SLAM);robot operating system ROS}, 
doi={10.1109/MILTECHS.2017.7988861}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7391814, 
author={ and and and }, 
booktitle={2015 International Conference on 3D Imaging (IC3D)}, 
title={Robust nose tip detection for face range images based on local features in scale-space}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Being the most distinct feature point in 3D facial landmarks, nose tip plays a significant role in 3D facial studies such as face detection, face recognition, facial features extraction, face alignment, etc. Successful detection of nose tip can facilitate many tasks of 3D facial studies. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, needs not training, can handle large rotations and occlusions. To reduce computational cost, we first remove small isolated regions from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, the Multi-angle Energy (ME) of each point is computed and sorted in descending order. Then the first m points in the descending order list are obtained and hierarchical clustering method is used to cluster these points. In the first h largest clusters, we can find one point with the largest ME. For all scales of the scale-space, we get a series of such points which are treated as nose tip candidates. For these candidates, we apply hierarchical clustering again. In the obtained largest cluster, we compute the mean value of ME. The ME of nose tip will be closest to the mean value. We evaluate our method in two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The experimental results verify the robustness of our method with a high nose tip detection rate.}, 
keywords={face recognition;feature extraction;object detection;pattern clustering;nose tip detection;face range images;local features;3D facial landmarks;3D facial studies;multiangle energy;ME;range image smoothing;hierarchical clustering method;FRGC database;BOSPHORUS database;Nose;Three-dimensional displays;Face;Robustness;Training;Feature extraction;Smoothing methods;nose tip;3D faces;range images;robust smoothing;normal;scale-space;multi-angle energy;sphere fitting;least square;hierarchical clustering}, 
doi={10.1109/IC3D.2015.7391814}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7517246, 
author={P. {Azevedo} and T. O. {Dos Santos} and E. {De Aguiar}}, 
booktitle={2016 XVIII Symposium on Virtual and Augmented Reality (SVR)}, 
title={An Augmented Reality Virtual Glasses Try-On System}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper presents a virtual try-on system to correctly visualize 3D objects (e.g., glasses) in the face of a given user. By capturing the image and depth information of a user through a low-cost RGB-D camera, we apply a face tracking technique to detect specific landmarks in the facial image. These landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3D facial morphable model that fits as good as possible to the user's head and face. At the end, we deform the chosen 3D objects from its rest shape to a deformed shape matching the specific facial shape of the user. The last step projects and renders the 3D object into the original image, with enhanced precision and in proper scale, showing the selected object in the user's face. We validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. Our results demonstrate that, by fitting a facial model to the user's face, the rendered virtual 3D objects look more realistic.}, 
keywords={augmented reality;face recognition;image capture;image colour analysis;image matching;image reconstruction;image sensors;rendering (computer graphics);shape recognition;augmented reality virtual glass try-on system;3D object visualization;image capturing;depth information;low-cost RGB-D camera;facial image;point cloud reconstruction;3D facial morphable model;deformed shape matching;virtual 3D object rendering;Three-dimensional displays;Face;Solid modeling;Glass;Cameras;Shape;Augmented reality;Virtual reality;Computer graphics}, 
doi={10.1109/SVR.2016.12}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7899697, 
author={G. {Pang} and U. {Neumann}}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={3D point cloud object detection with multi-view convolutional neural network}, 
year={2016}, 
volume={}, 
number={}, 
pages={585-590}, 
abstract={Efficient detection of three dimensional (3D) objects in point clouds is a challenging problem. Performing 3D descriptor matching or 3D scanning-window search with detector are both time-consuming due to the 3-dimensional complexity. One solution is to project 3D point cloud into 2D images and thus transform the 3D detection problem into 2D space, but projection at multiple viewpoints and rotations produce a large amount of 2D detection tasks, which limit the performance and complexity of the 2D detection algorithm choice. We propose to use convolutional neural network (CNN) for the 2D detection task, because it can handle all viewpoints and rotations for the same class of object together, as well as predicting multiple classes of objects with the same network, without the need for individual detector for each object class. We further improve the detection efficiency by concatenating two extra levels of early rejection networks with binary outputs before the multi-class detection network. Experiments show that our method has competitive overall performance with at least one-order of magnitude speed-up comparing with latest 3D point cloud detection methods.}, 
keywords={neural nets;object detection;multiclass detection network;early rejection networks;CNN;convolutional neural network;2D detection algorithm;2D space;2D images;3-dimensional complexity;3D scanning-window search;3D descriptor matching;three dimensional object detection;multiview convolutional neural network;3D point cloud object detection;Three-dimensional displays;Two dimensional displays;Object detection;Training;Detectors;Complexity theory;Search problems}, 
doi={10.1109/ICPR.2016.7899697}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7869954, 
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and N. {Werghi} and M. {Linguraru}}, 
booktitle={2016 IEEE 59th International Midwest Symposium on Circuits and Systems (MWSCAS)}, 
title={Facial landmarks detection using 3D constrained local model on mesh manifold}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={This paper proposes a novel 3D Constrained Local Models (CLM) approach applied for the detection of facial landmarks in 3D images. This approach capitalizes on the properties of Independent Component Analysis (ICA) to define appropriate priors of a face Point Distribution Model. Tailored to the mesh manifold modality, this approach address the limitations of the depth images which require pose normalization and suffer from the loss of the shape information caused by 2D projection. We validate this framework through a series of experiments conducted with the public Bosporus database, whereby it demonstrates a competitive performance compared to other state of the art methods.}, 
keywords={face recognition;independent component analysis;mesh generation;object detection;facial landmarks detection;3D constrained local model;CLM approach;3D images;independent component analysis;ICA;face point distribution model;mesh manifold modality;depth images;shape information loss;2D projection;public Bosporus database;pose normalization;Shape;Principal component analysis;Face;Three-dimensional displays;Deformable models;Integrated circuit modeling;Adaptation models}, 
doi={10.1109/MWSCAS.2016.7869954}, 
ISSN={1558-3899}, 
month={Oct},}
@INPROCEEDINGS{8633125, 
author={X. {Ju} and I. R. {Garcia Júnior} and L. {De Freitas Silva} and P. {Mossey} and D. {Al-Rudainy} and A. {Ayoub} and A. M. {De Mattos}}, 
booktitle={2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
title={3D Head Shape Analysis of Suspected Zika Infected Infants}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={The babies infected from Zika before they are born are at risk for problems with brain development and microcephaly. 3D head images of 43 Zika cases and 43 controls were collected aiming to extract shape characteristics for diagnosis purposes. Principal component analysis (PCA) has been applied on the vaults and faces of the collected 3D images and the scores on the second principal components of the vaults and faces showed significant differences between the control and Zika groups. The shape variations from -2σ to 2σ illustrated the typical characteristics of microcephaly of the Zika babies. Canonical correlation analysis (CCA) showed a significant correlation in the first CCA variates of face and vault which indicated the potential of 3D facial imaging for Zika surveillance. Further head circumferences and distances from ear to ear were measured from the 3D images and preliminary results showed the adding ear to ear distances for classifying control and Zika children strengthened the abilities of tested classification models.}, 
keywords={diseases;image classification;medical image processing;paediatrics;principal component analysis;shape recognition;stereo image processing;suspected Zika infected infants;microcephaly;Zika surveillance;3D facial imaging;canonical correlation analysis;Zika babies;shape variations;principal component analysis;3D head images;brain development;shape analysis;Zika children;3D imaging;shape analysis;Zika}, 
doi={10.1109/CISP-BMEI.2018.8633125}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8089906, 
author={L. {Fangmin} and C. {Ke} and L. {Xinhua}}, 
booktitle={2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA)}, 
title={3D Face Reconstruction Based on Convolutional Neural Network}, 
year={2017}, 
volume={}, 
number={}, 
pages={71-74}, 
abstract={Fast and robust 3D reconstruction of facial geometric structure from a single image is a challenging task with numerous applications, but there exist two problems when applied "in the wild": the 3D estimates are unstable for different photos of the same subject; the 3D estimates are over-regularized and generic. In response, a robust method for regressing discriminative 3D morphable face models(3DMM) is described to support face recognition and 3D mask printing. Combining the local data sets with the public data sets, improving the exiting 3DMM fitting method and then using a convolutional neural network(CNN) to improve reconstruction effect. The ground truth 3D faces of the CNN are the pooled 3DMM parameters extracted from the photos of the same subject. Using CNN to regress 3DMM shape and texture parameters directly from an input photo and offering a method for generating huge numbers of labeled examples. There are two key points of the paper: one is the training data generation for the model training; the other is the training of 3D reconstruction model. Experimental results and analysis show that this method costs much less time than traditional methods of 3D face modeling, and it is improved for different races on photos with any angles than the existing methods based on deep learning, and the system has better robustness.}, 
keywords={face recognition;image morphing;image reconstruction;image texture;learning (artificial intelligence);neural nets;regression analysis;shape recognition;3D face reconstruction;facial geometric structure;robust method;face recognition;3D mask printing;local data sets;public data sets;reconstruction effect;texture parameters;training data generation;3D reconstruction model;3D face modeling;convolutional neural network;discriminative 3D morphable face models;3DMM fitting method;CNN;Face;Three-dimensional displays;Solid modeling;Image reconstruction;Shape;Robustness;Data models;3D face reconstruction;convolutional neural network(CNN);3DMM;shape;texture}, 
doi={10.1109/ICICTA.2017.23}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7428566, 
author={J. {Liu} and Q. {Zhang} and C. {Tang}}, 
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
title={CoMES: A novel method for robust nose tip detection in face range images}, 
year={2015}, 
volume={}, 
number={}, 
pages={309-315}, 
abstract={As the most distinct feature point in facial landmarks, nose tip plays a significant role in 3D facial studies. Successful detection of nose tip can facilitate many 3D facial studies tasks. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, need not training, can handle large rotations and occlusions. We first remove small isolated connected regions and noise from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, we compute multi-angle energy of each point, then we use hierarchical clustering method to cluster the points whose multi-angle energies are larger than a threshold value. In the largest cluster, we can find one point with the largest multi-angle energy. For all scales of the scale-space, we get a series of such points and apply hierarchical clustering again for these points, nose tip will have the largest multi-angle energy in the largest cluster. We evaluate our method in FRGC v2.0 3D face database and BOSPHORUS 3D face database. The experimental results verify the robustness of our method with a high nose tip detection rate.}, 
keywords={edge detection;face recognition;pattern clustering;solid modelling;CoMES;robust nose tip detection;face range images;facial landmark;3D facial studies;scale-space;multiangle energy;hierarchical clustering method;FRGC v2.0 3D face database;BOSPHORUS 3D face database;Nose;Face;Three-dimensional displays;Training;Robustness;Feature extraction;Smoothing methods;nose tip;3D faces;scale-space;multi-angle energy;hierarchical clustering;range images}, 
doi={10.1109/IAEAC.2015.7428566}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7351535, 
author={S. {Sankaranarayanan} and V. M. {Patel} and R. {Chellappa}}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={3D facial model synthesis using coupled dictionaries}, 
year={2015}, 
volume={}, 
number={}, 
pages={3896-3900}, 
abstract={In this work, we propose a generative way of modeling faces, where the 3D shape of a face is generated by a supervised learning procedure involving coupled sparse feature learning. To learn dictionaries using the proposed method, we use the USF-HUMAN ID database [1]. We provide as input to our training system, paired correspondences of 2D and 3D images of individuals and aim to learn the low-level patches both in 2D and 3D domains that describe the corresponding subspaces in a sparse manner. We demonstrate the efficacy of our method by quantitative results on the 3D database and qualitative results on images drawn from the internet.}, 
keywords={face recognition;learning (artificial intelligence);3D facial model synthesis;coupled dictionaries;supervised learning procedure;coupled sparse feature learning;dictionary learning;USF-HUMAN ID database;2D images;3D images;Three-dimensional displays;Dictionaries;Solid modeling;Shape;Training;Databases;Encoding;3D Model;Face Synthesis;Coupled Sparse Coding;Cross-modal Learning}, 
doi={10.1109/ICIP.2015.7351535}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7899906, 
author={H. X. {Pham} and V. {Pavlovic} and and }, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Robust real-time performance-driven 3D face tracking}, 
year={2016}, 
volume={}, 
number={}, 
pages={1851-1856}, 
abstract={We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker capable of handling a wide range of target scene depths, whose performances are demonstrated in our extensive experiments better than those of the state-of-the-arts.}, 
keywords={cameras;computational geometry;face recognition;image colour analysis;object tracking;optimisation;solid modelling;stereo image processing;video signal processing;3D face tracking;RGBD video streams;head pose tracking;facial actions;cameras;point cloud;3D shape regressor;joint 2D+3D optimization;3D model reconstruction;Three-dimensional displays;Shape;Face;Training;Two dimensional displays;Optimization;Solid modeling}, 
doi={10.1109/ICPR.2016.7899906}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7976644, 
author={S. {Sghaier} and C. {Souani} and H. {Faeidh} and K. {Besbes}}, 
booktitle={2016 Global Summit on Computer Information Technology (GSCIT)}, 
title={Novel Technique for 3D Face Segmentation and Landmarking}, 
year={2016}, 
volume={}, 
number={}, 
pages={27-31}, 
abstract={In this paper we propose a new technique to detect and extract any part that we need from a 3D face. The regions of interest of our approach are the eyes and the nose. Moreover, we are interested in the extraction of the salient landmarks in 3D face. Therefore, in this paper a new method is proposed to detect the nose tip and eye corners from a three-dimensional face range image. Our original technique allows fully automated processing, treating incomplete and noisy input data, and automatically rejecting non-facial areas. Besides, it is robust against holes in 3D image and insensitive to facial expressions. Besides, it is stable against translation and rotation of the face, and it is suitable for different resolutions of images. All the experiments have been performed on the GAVAB and FRAV 3D databases. After applying the proposed method to the 3D face, experimental results show that it is comparable to state-of-the-art methods in terms of its accuracy and flexibility.}, 
keywords={face recognition;image segmentation;visual databases;3D face segmentation;landmark extraction;nose tip detection;eye corner detection;three-dimensional face range image;automatic nonfacial area rejection;3D image;facial expressions;image resolutions;GAVAB 3D database;FRAV 3D database;Face;Three-dimensional displays;Nose;Information technology;Noise measurement;Robustness;Image resolution;3D face;segmentation;region of interest;anthropometric;landmarks}, 
doi={10.1109/GSCIT.2016.17}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7332643, 
author={C. {Stahlschmidt} and A. {Gavriilidis} and A. {Kummert}}, 
booktitle={2015 IEEE 9th International Workshop on Multidimensional (nD) Systems (nDS)}, 
title={Classification of ascending steps and stairs using Time-of-Flight sensor data}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper proposes a method to analyse human-made environments in order to verify the presence of ascending steps or stairs. Our system is intended to assist visually impaired people by providing acoustic information about the scene in front of a low-resolution Time-of-Flight (ToF) camera that is fixed to a mobile vehicle (rollator). Detailed instructions to the user about potentially hazardous situations are provided. This paper in particular deals with a fast approach on classification of ascending steps in 3D point clouds. This method is part of a system that aims on enhancing visually impaired persons understand the environment and help prevent collisions.}, 
keywords={computer graphics;image classification;image resolution;ascending step classification;time-of-flight sensor data;human-made environment;visually impaired people;acoustic information;low-resolution time-of-flight camera;ToF camera;mobile vehicle;rollator;hazardous situation;3D point cloud;visually impaired person;Three-dimensional displays;Cameras;Data mining;Algorithm design and analysis;Gray-scale;Acoustics;Mobile communication}, 
doi={10.1109/NDS.2015.7332643}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8035340, 
author={L. {Han} and Q. {Xiao} and X. {Liang}}, 
booktitle={2017 International Conference on Computer, Information and Telecommunication Systems (CITS)}, 
title={3D face reconstruction based on progressive cascade regression}, 
year={2017}, 
volume={}, 
number={}, 
pages={297-301}, 
abstract={In order to better learn the distributions of 2D and 3D faces and the mapping between them with limited training samples, a new 3D face reconstruction method based on progressive cascade regression is proposed. Firstly, it learns the mapping between 2D and 3D facial landmarks to estimate the initial 3D facial landmarks with a coupled space learning method. Secondly, a deformed space is constructed with the difference between the estimated initial landmarks and the ground truth of training samples; and more accurate 3D facial landmarks are reconstructed by modifying the initial 3D ones with shape compensations which are calculated by minimizing an objective function. Finally, the realistic 3D faces are reconstructed by a method that is based on a simple sparse regulation and shape deformation. The results on BJUT 3D face database demonstrate the effectiveness of the proposed method. In addition, compared with some typical methods, the method can get better subjective and objective results, especially in details.}, 
keywords={face recognition;image reconstruction;learning (artificial intelligence);regression analysis;solid modelling;3D face reconstruction;progressive cascade regression;training samples;facial landmark mapping;coupled space learning;deformed space construction;sparse regulation;shape deformation;Three-dimensional displays;Face;Training;Two dimensional displays;Image reconstruction;Solid modeling;Shape}, 
doi={10.1109/CITS.2017.8035340}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8100068, 
author={W. {Peng} and Z. {Feng} and C. {Xu} and Y. {Su}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Parametric T-Spline Face Morphable Model for Detailed Fitting in Shape Subspace}, 
year={2017}, 
volume={}, 
number={}, 
pages={5515-5523}, 
abstract={Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration for the synthesis of 3D faces by assuming that faces are in a linear class. However, the human face is in a nonlinear manifold, and a new test are always not in the pre-learnt subspace accurately because of the disparity brought by ethnicity, age, gender, etc. In the paper, we propose a parametric T-spline morphable model (T-splineMM) for 3D face representation, which has great advantages of fitting data from an unknown source accurately. In the model, we describe a face by C^2 T-spline surface, and divide the face surface into several shape units (SUs), according to facial action coding system (FACS), on T-mesh instead of on the surface directly. A fitting algorithm is proposed to optimize coefficients of T-spline control point components along pre-learnt identity and expression subspaces, as well as to optimize the details in refinement progress. As any pre-learnt subspace is not complete to handle the variety and details of faces and expressions, it covers a limited span of morphing. SUs division and detail refinement make the model fitting the facial muscle deformation in a larger span of morphing subspace. We conduct experiments on face scan data, kinect data as well as the space-time data to test the performance of detail fitting, robustness to missing data and noise, and to demonstrate the effectiveness of our model. Convincing results are illustrated to demonstrate the effectiveness of our model compared with the popular methods.}, 
keywords={face recognition;feature extraction;image morphing;image reconstruction;mesh generation;muscle;solid modelling;splines (mathematics);shape units;facial action coding system;fitting algorithm;T-spline control point components;expression subspaces;face scan data;parametric T-spline face morphable model;shape subspace pre-learnt subspace;human face;parametric T-spline morphable model for 3D;T-spline surface;face surface;kinect data;space-time data;Face;Three-dimensional displays;Shape;Solid modeling;Lips;Splines (mathematics);Computational modeling}, 
doi={10.1109/CVPR.2017.585}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{8688162, 
author={F. {Cao} and F. {Yan} and Y. {Gu} and C. {Ding} and Y. {Zhuang} and W. {Wang}}, 
booktitle={2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)}, 
title={A Novel Image Model of Point Clouds and its Application in Place Recognition}, 
year={2018}, 
volume={}, 
number={}, 
pages={79-83}, 
abstract={In this paper, we present a novel panoramic image model for scattered point clouds, and apply it to the problem of place recognition. We project a point cloud onto a sphere, and then the sphere is divided into a set of individual grids by longitudes and latitudes. Each grid is regard as a pixel and its value is computed using the geometrical relationship among the points in the grid and its neighbors. For convenience, the sphere is transferred into a flat. Since point clouds are converted to 2D images, we use ORB features and bag of words technique to solve place recognition problem. Our experimental results show that our image model is a more universal one and achieve a good performance in place recognition in both accuracy and efficiency.}, 
keywords={computational geometry;image recognition;scattered point clouds;place recognition;panoramic image model}, 
doi={10.1109/CYBER.2018.8688162}, 
ISSN={2379-7711}, 
month={July},}
@INPROCEEDINGS{7301378, 
author={P. {Polewski} and W. {Yao} and M. {Heurich} and P. {Krzystek} and U. {Stilla}}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
title={Active learning approach to detecting standing dead trees from ALS point clouds combined with aerial infrared imagery}, 
year={2015}, 
volume={}, 
number={}, 
pages={10-18}, 
abstract={Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89% with less than 100 training examples, which corresponds to 10% of the pre-selected data pool.}, 
keywords={environmental factors;feature extraction;forestry;image classification;image colour analysis;infrared imaging;learning (artificial intelligence);matrix algebra;object detection;vegetation;active learning approach;standing dead tree detection;ALS point cloud;forest process;forest science;environmental science;aerial color infrared imagery;feature extraction;greedy approximation;kernelized feature matrix;error reduction;semisupervised classifier;Bavarian Forest National Park;Vegetation;Three-dimensional displays;Entropy;Training;Image segmentation;Logistics;Feature extraction}, 
doi={10.1109/CVPRW.2015.7301378}, 
ISSN={2160-7516}, 
month={June},}
@INPROCEEDINGS{7410401, 
author={A. {Sironi} and V. {Lepetit} and P. {Fua}}, 
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={Projection onto the Manifold of Elongated Structures for Accurate Extraction}, 
year={2015}, 
volume={}, 
number={}, 
pages={316-324}, 
abstract={Detection of elongated structures in 2D images and 3D image stacks is a critical prerequisite in many applications and Machine Learning-based approaches have recently been shown to deliver superior performance. However, these methods essentially classify individual locations and do not explicitly model the strong relationship that exists between neighboring ones. As a result, isolated erroneous responses, discontinuities, and topological errors are present in the resulting score maps. We solve this problem by projecting patches of the score map to their nearest neighbors in a set of ground truth training patches. Our algorithm induces global spatial consistency on the classifier score map and returns results that are provably geometrically consistent. We apply our algorithm to challenging datasets in four different domains and show that it compares favorably to state-of-the-art methods.}, 
keywords={feature extraction;image classification;learning (artificial intelligence);object detection;structural engineering computing;classifier score map;global spatial consistency;ground truth training patch;patch projection;score map;topological error;discontinuities;isolated erroneous response;individual location classification;machine learning-based approach;3D image stack;2D image;elongated structure detection;extraction;elongated structure manifold;Three-dimensional displays;Training;Manifolds;Biomembranes;Image segmentation;Feature extraction;Transforms}, 
doi={10.1109/ICCV.2015.44}, 
ISSN={2380-7504}, 
month={Dec},}
@INPROCEEDINGS{7507339, 
author={I. {Virag} and L. {Stoicu-Tivadar} and M. {Crişan-Vida}}, 
booktitle={2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI)}, 
title={Gesture-based interaction in medical interfaces}, 
year={2016}, 
volume={}, 
number={}, 
pages={519-523}, 
abstract={The latest generation of medical visualizations systems that provide gesture based interaction usually rely on closed source software modules. This paper presents a novel approach since the interaction with the rendered 3D images is done via a Web browser. The entire system is based on open source software components and this way eliminates the requirement to have a specific operating system preinstalled. Our team used a Leap Motion controller that allows the rotation, panning, scaling and selection of individual slices of a reconstructed 3D model based on a prior CT (Computed Tomography) or MRI (Magnetic Resonance Imaging) scan of a patient. The results showed that is feasible to build such a system and that the interaction with the model can be done in real-time. It was concluded that this Web oriented architecture could provide a sustainable alternative for interacting with medical images.}, 
keywords={biomedical MRI;computerised tomography;gesture recognition;image reconstruction;medical image processing;online front-ends;gesture-based interaction;medical interfaces;medical visualization system;closed source software modules;rendered 3D images;Web browser;open source software components;operating system;leap-motion controller;slice selection;slice rotation;slice panning;slice scaling;reconstructed 3D model;CT;computed tomography;MRI scan;magnetic resonance imaging;Web oriented architecture;Three-dimensional displays;Biomedical imaging;Solid modeling;Image reconstruction;Thumb;Informatics}, 
doi={10.1109/SACI.2016.7507339}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7550083, 
author={W. P. {Koppen} and W. J. {Christmas} and D. J. M. {Crouch} and W. F. {Bodmer} and J. V. {Kittler}}, 
booktitle={2016 International Conference on Biometrics (ICB)}, 
title={Extending non-negative matrix factorisation to 3D registered data}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The use of non-negative matrix factorisation (NMF) on 2D face images has been shown to result in sparse feature vectors that encode for local patches on the face, and thus provides a statistically justified approach to learning parts from wholes. However successful on 2D images, the method has so far not been extended to 3D images. The main reason for this is that 3D space is a continuum and so it is not apparent how to represent 3D coordinates in a non-negative fashion. This work compares different non-negative representations for spatial coordinates, and demonstrates that not all non-negative representations are suitable. We analyse the representational properties that make NMF a successful method to learn sparse 3D facial features. Using our proposed representation, the factorisation results in sparse and interpretable facial features.}, 
keywords={face recognition;feature extraction;image registration;image representation;learning (artificial intelligence);matrix decomposition;nonnegative matrix factorization;NMF;3D face image registration;representational property;3D facial feature learning;Face;Three-dimensional displays;Shape;Principal component analysis;Two dimensional displays;Facial features;Encoding}, 
doi={10.1109/ICB.2016.7550083}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8123040, 
author={N. {Mohsin} and S. {Payandeh}}, 
booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={Localization and identification of body extremities based on data from multiple depth sensors}, 
year={2017}, 
volume={}, 
number={}, 
pages={2736-2741}, 
abstract={This paper explores the novel use of multiple depth sensors to overcome occlusions and improve localization and tracking of body extremities. The usage of data from only depth sensors not only overcomes visual challenges associated with RGB sensors under low illumination, but also protects the identity of surveyed person with high confidentiality. For integrating depth information from multiple sources, the paper presents first an overview of a novel calibration method for multiple depth sensors. In case of occlusion of any fiducial point in the primary sensor's depth image, co-ordinates of the point can be obtained from the frame of other sensors using the calibration parameters. To localize salient body parts such as hands, head and feet, a surface triangular mesh is applied on generated 3D point cloud from the primary sensor. The geodesic extrema from the mesh coincide with body extremities. The body extremities can be identified based on those relative geodesic distances between the extremities. Once the body parts are labelled, a portion of body can be targeted and evaluated for specific gait analysis and visualization. For the performance evaluation, our calibration method has fared well in comparison to other available techniques. Also, our proposed localization of salient body parts is able to successfully tag the specific body part i.e. the head region.}, 
keywords={calibration;data visualisation;feature extraction;gait analysis;image motion analysis;image sensors;mesh generation;object detection;sensor fusion;RGB sensors;salient body parts;depth information;body extremities localization;body extremities identification;multiple depth sensor data;body extremities tracking;illumination;identity confidentiality;specific body part tagging;fiducial point occlusion;primary sensor depth image;3D point cloud;mesh geodesic extrema;relative geodesic distances;gait analysis;visualization;Sensors;Three-dimensional displays;Calibration;Extremities;Image sensors;Image segmentation;Cameras;multiple depth sensors calibration;body extremities localization;Kinect II;geodesic distances}, 
doi={10.1109/SMC.2017.8123040}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7533187, 
author={L. {Ding} and A. {Elliethy} and E. {Freedenberg} and S. A. {Wolf-Johnson} and J. {Romphf} and P. {Christensen} and G. {Sharma}}, 
booktitle={2016 IEEE International Conference on Image Processing (ICIP)}, 
title={Comparative analysis of homologous buildings using range imaging}, 
year={2016}, 
volume={}, 
number={}, 
pages={4378-4382}, 
abstract={This paper reports on a novel application of computer vision and image processing technologies to an interdisciplinary project in architectural history that seeks to help identify and visualize differences between homologous buildings constructed to a common template design. By identifying the mutations in homologous buildings, we assist humanists in giving voice to the contributions of the myriad additional “authors” for these buildings beyond their primary designers. We develop a framework for comparing 3D point cloud representations of homologous buildings captured using lidar: focusing on identifying similarities and differences, both among 3D scans of different buildings and between the 3D scans and the design specifications of architectural drawings. The framework addresses global and local alignment for highlighting gross differences as well as differences in individual structural elements and provides methods for readily highlighting the differences via suitable visualizations. The framework is demonstrated on pairs of homologous buildings selected from the Canadian and Ottoman rail networks. Results demonstrate the utility of the framework confirming differences already apparent to the humanist researchers and also revealing new differences that were not previously observed.}, 
keywords={buildings (structures);design engineering;optical radar;radar imaging;structural engineering computing;Ottoman rail networks;Canadian rail networks;structural elements;local alignment;global alignment;architectural drawings;design specifications;3D scans;lidar;3D point cloud representations;mutation identification;range imaging;homologous buildings;Laser radar;Heating;Three-dimensional displays;architectural biometrics;building difference visualization;point cloud comparison;range imaging;visual big data analytics}, 
doi={10.1109/ICIP.2016.7533187}, 
ISSN={2381-8549}, 
month={Sep.},}
@INPROCEEDINGS{7495382, 
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and H. A. {Safar} and N. {Werghi}}, 
booktitle={2016 18th Mediterranean Electrotechnical Conference (MELECON)}, 
title={Landmarks detection on 3D face scans using local histogram descriptors}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In this work, we exploit 3D Constrained Local Model (CLM) for facial landmark detection. Our approach integrates the geometric information of 3D face scans. The fast increase demand of 3D data invite to develop 3D image processing methods for many applications and especially for automatic landmark detection. The new step in this paper is the introduction of mesh histogram of gradients (meshHOG) as local descriptors around every landmark location. The proposed work is evaluated on the publicly available Bosphorus database. A comparison with the other descriptors mesh LBP and mesh SIFT are also depicted.}, 
keywords={computational geometry;face recognition;mesh generation;object detection;facial landmark detection;3D face scans;local histogram descriptors;3D constrained local model;geometric information;3D image processing methods;mesh histogram-of-gradients;meshHOG;local descriptors;publicly available Bosphorus database;Three-dimensional displays;Face;Histograms;Solid modeling;Shape;Deformable models;Mathematical model}, 
doi={10.1109/MELCON.2016.7495382}, 
ISSN={2158-8481}, 
month={April},}
@INPROCEEDINGS{7785124, 
author={F. {Maninchedda} and C. {Häne} and M. R. {Oswald} and M. {Pollefeys}}, 
booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
title={Face Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization}, 
year={2016}, 
volume={}, 
number={}, 
pages={489-498}, 
abstract={We present a system which is able to reconstruct human faces on mobile devices with only on-device processing using the sensors which are typically built into a current commodity smart phone. Such technology can for example be used for facial authentication purposes or as a fast preview for further post-processing. Our method uses recently proposed techniques which compute depth maps by passive multi-view stereo directly on the device. We propose an efficient method which recovers the geometry of the face from the typically noisy point cloud. First, we show that we can safely restrict the reconstruction to a 2.5D height map representation. Therefore we then propose a novel low dimensional height map shape model for faces which can be fitted to the input data efficiently even on a mobile phone. In order to be able to represent instance specific shape details, such as moles, we augment the reconstruction from the shape model with a distance map which can be regularized efficiently. We thoroughly evaluate our approach on synthetic and real data, thereby we use both high resolution depth data acquired using high quality multi-view stereo and depth data directly computed on mobile phones.}, 
keywords={face recognition;image reconstruction;image resolution;stereo image processing;face reconstruction;mobile devices;height map shape model;fast regularization;facial authentication;depth maps;passive multiview stereo;face geometry;noisy point cloud;low dimensional height map shape model;high resolution depth data;high quality multiview stereo;Face;Computational modeling;Cameras;Three-dimensional displays;Shape;Solid modeling;Image reconstruction}, 
doi={10.1109/3DV.2016.59}, 
ISSN={}, 
month={Oct},}
@ARTICLE{7029822, 
author={Y. {Li} and Y. {Wang} and B. {Wang} and L. {Sui}}, 
journal={IET Computer Vision}, 
title={Nose tip detection on three-dimensional faces using pose-invariant differential surface features}, 
year={2015}, 
volume={9}, 
number={1}, 
pages={75-84}, 
abstract={Three-dimensional (3D) facial data offer the potential to overcome the difficulties caused by the variation of head pose and illumination in 2D face recognition. In 3D face recognition, localisation of nose tip is essential to face normalisation, face registration and pose correction etc. Most of the existing methods of nose tip detection on 3D face deal mainly with frontal or near-frontal poses or are rotation sensitive. Many of them are training-based or model-based. In this study, a novel method of nose tip detection is proposed. Using pose-invariant differential surface features - high-order and low-order curvatures, it can detect nose tip on 3D faces under various poses automatically and accurately. Moreover, it does not require training and does not depend on any particular model. Experimental results on GavabDB verify the robustness and accuracy of the proposed method.}, 
keywords={face recognition;feature extraction;image registration;object detection;pose estimation;nose tip detection;pose invariant differential surface feature;3D face recognition;head pose variation;illumination;2D face recognition;nose tip localisation;face normalisation;face registration;pose correction;nearfrontal pose;low order curvature;high order curvature}, 
doi={10.1049/iet-cvi.2014.0070}, 
ISSN={1751-9632}, 
month={},}
@INPROCEEDINGS{7780547, 
author={T. {Hackel} and J. D. {Wegner} and K. {Schindler}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Contour Detection in Unstructured 3D Point Clouds}, 
year={2016}, 
volume={}, 
number={}, 
pages={1610-1618}, 
abstract={We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds &gt; 107 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.}, 
keywords={edge detection;feature extraction;graph theory;image classification;image coding;image representation;learning (artificial intelligence);object detection;contour detection;unstructured 3D point clouds;surface orientation;large-scale outdoor point clouds;high-quality surface;solid models;unstructured-inhomogeneous point clouds;contour extraction;two-stage discriminative learning problem;contour score;binary classifier;feature extraction;contour scores;higher-order MRF;graph cliques;Canny-style edge detection;image representation;Three-dimensional displays;Feature extraction;Solid modeling;Image edge detection;Surface reconstruction;Surface topography;Solids}, 
doi={10.1109/CVPR.2016.178}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{7587405, 
author={Y. {Wang} and X. X. {Zhu} and B. {Zeisl} and M. {Pollefeys}}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={Fusing Meter-Resolution 4-D InSAR Point Clouds and Optical Images for Semantic Urban Infrastructure Monitoring}, 
year={2017}, 
volume={55}, 
number={1}, 
pages={14-26}, 
abstract={Using synthetic aperture radar (SAR) interferometry to monitor long-term millimeter-level deformation of urban infrastructures, such as individual buildings and bridges, is an emerging and important field in remote sensing. In the state-of-the-art methods, deformation parameters are retrieved and monitored on a pixel basis solely in the SAR image domain. However, the inevitable side-looking imaging geometry of SAR results in undesired occlusion and layover in urban area, rendering the current method less competent for a semantic-level monitoring of different urban infrastructures. This paper presents a framework of a semantic-level deformation monitoring by linking the precise deformation estimates of SAR interferometry and the semantic classification labels of optical images via a 3-D geometric fusion and semantic texturing. The proposed approach provides the first “SARptical” point cloud of an urban area, which is the SAR tomography point cloud textured with attributes from optical images. This opens a new perspective of InSAR deformation monitoring. Interesting examples on bridge and railway monitoring are demonstrated.}, 
keywords={optical images;radar interferometry;synthetic aperture radar;InSAR deformation monitoring;SAR tomography point cloud;semantic texturing;3-D geometric fusion;semantic-level deformation monitoring;side-looking imaging geometry;SAR image domain;remote sensing;long-term millimeter-level deformation;SAR interferometry;synthetic aperture radar interferometry;semantic urban infrastructure monitoring;optical images;meter-resolution 4-D InSAR point clouds;Optical imaging;Optical scattering;Synthetic aperture radar;Three-dimensional displays;Optical sensors;Adaptive optics;Optical interferometry;Bridge monitoring;interferometric synthetic aperture radar (InSAR);optical InSAR fusion;railway monitoring;SAR;semantic classification}, 
doi={10.1109/TGRS.2016.2554563}, 
ISSN={0196-2892}, 
month={Jan},}
@INPROCEEDINGS{7298920, 
author={ and R. {Ji} and }, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery}, 
year={2015}, 
volume={}, 
number={}, 
pages={3013-3021}, 
abstract={Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.}, 
keywords={Boltzmann machines;feature extraction;image colour analysis;image matching;learning (artificial intelligence);object detection;3D object detection;bimodal deep Boltzmann machines;RGBD imagery;3D scenes;point clouds;3D training data labeling;cross-modality cues;cross-modality deep learning framework;3D scene object detection;cross-modality feature learning;3D detection window;3D point cloud;exemplar shape matching;3D CAD models;2D positive samples;R-CNNs;raw feature extraction;RGB domains;depth domains;RMRC dataset;bimodal based deep feature learning framework;Three-dimensional displays;Solid modeling;Training;Frequency modulation;Joints;Detectors;Feature extraction}, 
doi={10.1109/CVPR.2015.7298920}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7327791, 
author={J. B. {Sleiman} and J. B. {Perraud} and B. {Bousquet} and N. {Palka} and J. P. {Guillet} and P. {Mounaix}}, 
booktitle={2015 40th International Conference on Infrared, Millimeter, and Terahertz waves (IRMMW-THz)}, 
title={Chemical imaging and quantification of RDX/PETN mixtures by PLS applied on terahertz time-domain spectroscopy}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Chemometric analysis was applied on terahertz absorbance 3D images, in transmission. The goal is to automatically discriminate some explosives on images and quantify mixtures of RDX/PETN in the frequency range of 0.2-3 THz. Partial Least Square (PLS) was applied on THz absorbance multispectral images to quantify individual product inside pure samples and mixtures at each pixel on the image. Then the best score obtained is used to display the samples' images and provide the optimal frequencies combination for recognition purpose.}, 
keywords={chemical variables measurement;electromagnetic wave absorption;explosives;least mean squares methods;terahertz spectroscopy;terahertz wave imaging;terahertz wave spectra;chemometric analysis;terahertz absorbance 3D image;RDX/PETN mixture quantification;explosive;partial least square;PLS method;THz absorbance multispectral image;terahertz time-domain spectroscopy;chemical imaging;frequency 0.2 THz to 3 THz;Imaging;Explosives;Predictive models;Yttrium;Spectroscopy;Time-domain analysis}, 
doi={10.1109/IRMMW-THz.2015.7327791}, 
ISSN={2162-2035}, 
month={Aug},}
@INPROCEEDINGS{8453724, 
author={S. {Sihvo} and P. {Virjonen} and P. {Nevalainen} and J. {Heikkonen}}, 
booktitle={2018 Baltic Geodetic Congress (BGC Geomatics)}, 
title={Tree Detection around Forest Harvester Based on Onboard LiDAR Measurements}, 
year={2018}, 
volume={}, 
number={}, 
pages={364-367}, 
abstract={This paper proposes a new approach for the detection of tree locations around forest machines producing a situational model based on on-site terrestrial LiDAR data collected during harvesting operation. A triangularized ground model is used to planarize the point cloud in order to simplify the tree detection. The planarized ground makes the vertical cutting of the point cloud systematical. Tree stem lines detected from individual trees at individual scan views are used to guide the final alignment into global coordinates. The setup is numerically efficient and does not rely on any positioning and orientation system (POS) based e.g. on an inertial measurement unit (IMU) or global navigation satellite system (GNSS) or wheel rotation counter on the autonomous vehicle.}, 
keywords={optical radar;satellite navigation;tree detection;forest harvester;onboard LiDAR measurements;tree locations;forest machines;situational model;on-site terrestrial LiDAR data;triangularized ground model;planarized ground;tree stem lines;wheel rotation counter;Vegetation;Three-dimensional displays;Laser radar;Forestry;Semiconductor device modeling;Global navigation satellite system;Planarization;object recognition;collision avoidance;autonomous vehicles forestry}, 
doi={10.1109/BGC-Geomatics.2018.00075}, 
ISSN={}, 
month={June},}
@ARTICLE{7331662, 
author={T. v. {Landesberger} and D. {Basgier} and M. {Becker}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Comparative Local Quality Assessment of 3D Medical Image Segmentations with Focus on Statistical Shape Model-Based Algorithms}, 
year={2016}, 
volume={22}, 
number={12}, 
pages={2537-2549}, 
abstract={The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.}, 
keywords={data analysis;data visualisation;ear;image segmentation;liver;medical image processing;neurophysiology;pattern clustering;statistical analysis;comparative local quality assessment;3D medical image segmentation;statistical shape model-based algorithm;automatic 3D medical segmentation algorithm;data analysis;data visualization;segmentation quality characteristics;hierarchical clustering algorithm;connectivity criterion;landmark correspondence;liver;cochlea;facial nerve;organ region identification;outlier instance detection;Image segmentation;Three-dimensional displays;Biomedical monitoring;Visual analytics;Medical diagnostic imaging;Systematics;Statistical analysis;Clustering;Visual analytics;3D medical image segmentation quality;comparison;clustering;statistical shape models}, 
doi={10.1109/TVCG.2015.2501813}, 
ISSN={1077-2626}, 
month={Dec},}
@INPROCEEDINGS{7167461, 
author={D. {Aneja} and S. R. {Vora} and E. D. {Camci} and L. G. {Shapiro} and T. C. {Cox}}, 
booktitle={2015 IEEE 28th International Symposium on Computer-Based Medical Systems}, 
title={Automated Detection of 3D Landmarks for the Elimination of Non-Biological Variation in Geometric Morphometric Analyses}, 
year={2015}, 
volume={}, 
number={}, 
pages={78-83}, 
abstract={Landmark-based morphometric analyses are used by anthropologists, developmental and evolutionary biologists to understand shape and size differences (eg. in the cranioskeleton) between groups of specimens. The standard, laborintensive approach is for researchers to manually place landmarks on 3D image datasets. As landmark recognition is subject to inaccuracies of human perception, digitization of landmark coordinates is typically repeated (often by more than one person) and the mean coordinates are used. In an attempt to improve efficiency and reproducibility between researchers, we have developed an algorithm to locate landmarks on CT mouse hemi-mandible data. The method is evaluated on 3D meshes of 28-day old mice, and results compared to landmarks manually identified by experts. Quantitative shape comparison between two inbred mouse strains demonstrate that data obtained using our algorithm also has enhanced statistical power when compared to data obtained by manual landmarking.}, 
keywords={bone;computerised tomography;automated 3D landmark detection;nonbiological variation elimination;geometric morphometric analysis;cranioskeleton;3D image dataset;landmark recognition;human perception;landmark coordinate digitization;landmark location;CT mouse hemimandible data;computed tomography;quantitative shape comparison;3D mice meshes;inbred mouse strain;statistical power;Manuals;Shape;Mice;Surface treatment;Three-dimensional displays;Accuracy;3D landmarks;Automated landmark detection;Geometric morphometrics;Shape analysis;Mandible}, 
doi={10.1109/CBMS.2015.86}, 
ISSN={2372-9198}, 
month={June},}
@ARTICLE{7879309, 
author={H. {Zhang} and C. {Ye}}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={An Indoor Wayfinding System Based on Geometric Features Aided Graph SLAM for the Visually Impaired}, 
year={2017}, 
volume={25}, 
number={9}, 
pages={1592-1604}, 
abstract={This paper presents a 6-degree of freedom (DOF) pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired. The PE method involves two-graph simultaneous localization and mapping (SLAM) processes to reduce the accumulative pose error of the device. In the first step, the floor plane is extracted from the 3-D camera's point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch, and Zerrors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y, and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a visually impaired person in an indoor environment. The system uses the estimated pose and floor plan to locate the device user in a building and guides the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.}, 
keywords={biomedical engineering;geometry;indoor navigation;speech;vision defects;indoor wayfinding system;geometric features aided graph SLAM;visually impaired;pose estimation method;two-graph simultaneous localization;mapping processes;landmark node;yaw errors;computational time;speech interface;indoor navigation task;Simultaneous localization and mapping;RNA;Cameras;Three-dimensional displays;Navigation;Floors;Indoor environments;Blind navigation;wayfinding;robotic navigation aid;pose estimation;graph SLAM;3-D camera;Canes;Dependent Ambulation;Equipment Design;Equipment Failure Analysis;Humans;Imaging, Three-Dimensional;Patient Identification Systems;Reproducibility of Results;Self-Help Devices;Sensitivity and Specificity;Spatial Navigation;Treatment Outcome;User-Computer Interface;Visually Impaired Persons;Wireless Technology}, 
doi={10.1109/TNSRE.2017.2682265}, 
ISSN={1534-4320}, 
month={Sep.},}
@INPROCEEDINGS{8569716, 
author={T. {Akita} and Y. {Yamauchi} and H. {Fujiyoshi}}, 
booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)}, 
title={Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye Cameras}, 
year={2018}, 
volume={}, 
number={}, 
pages={1103-1108}, 
abstract={Recently, automated emergency brake systems for pedestrian have been commercialized. However, they cannot detect crossing pedestrians when turning at intersections because the field of view is not wide enough. Thus, we propose to utilize a surround view camera system becoming popular by making it into stereo vision which is robust for the pedestrian recognition. However, conventional stereo camera technologies cannot be applied due to fisheye cameras and uncalibrated camera poses. Thus we have created the new method to absorb difference of the pedestrian appearance between cameras by machine learning for the stereo vision. The method of stereo matching between image patches in each camera image was designed by combining D-Brief and NCC with SVM. Good generalization performance was achieved by it compared with individual conventional algorithms. Furthermore, feature amounts of the point cloud reconstructed by the stereo pairs are utilized with Random Forest to discriminate pedestrians. The algorithm was evaluated for the actual camera images of crossing pedestrians at various intersections, and 96.0% of pedestrian tracking rate with high position detection accuracy was achieved. They were compared with Faster R-CNN as the best pattern recognition technique, and our proposed method indicated better detection performance.}, 
keywords={cameras;image matching;image reconstruction;learning (artificial intelligence);object detection;object tracking;pedestrians;stereo image processing;support vector machines;traffic engineering computing;machine learning-based stereo vision algorithm;surround view fisheye cameras;automated emergency brake systems;crossing pedestrians;surround view camera system;pedestrian recognition;uncalibrated camera;pedestrian appearance;stereo matching;image patches;camera image;stereo pairs;pedestrian tracking rate;high position detection accuracy;detection performance;stereo camera technologies;D-Brief;SVM;random forest;Cameras;Turning;Distortion;Machine learning algorithms;Machine learning;Accidents;Feature extraction}, 
doi={10.1109/ITSC.2018.8569716}, 
ISSN={2153-0017}, 
month={Nov},}
@INPROCEEDINGS{7350744, 
author={}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={[Title page]}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={The following topics are dealt with: learning-based visual applications; machine learning and scene analysis; multimedia retrieval and indexing; object detection and tracking; pose and gesture recognition; saliency analysis; superpixel segmentation; visual biometrics; visualization and image rendering; pose estimation and human activity recognition; discriminative local features; edge and shape models; face recognition; image retrieval; image segmentation; action detection; facial features-gender-age recognition; feature detection and tracking; foreground-background separation; active contours and levelset methods; image classification; texture synthesis; video analysis; optical flow and motion estimation; video surveillance; visual content analysis; video retrieval; tomographic imaging; radar imaging; seismic and remote sensing imaging; image and video coding; video communication and networking; video streaming; 3D image processing; 3D modeling and reconstruction; multiview processing; document analysis; color imaging; 3D object and scene reconstruction; hardware, parallel, and distributed system; infrared, multispectral and hyperspectral imaging; stereo image processing; multimedia forensics; content and privacy protection; data hiding; robust hashing and counter-forensics; social and affective media; multidimensional processing; bio-inspired modeling; visual aesthetics and quality assessment; omnidirectional imaging and plenoptics; image representation; Big media Data processing; compressed sensing; and image denoising.}, 
keywords={Big Data;compressed sensing;data protection;data visualisation;document image processing;geophysical image processing;gesture recognition;hyperspectral imaging;image classification;image colour analysis;image denoising;image forensics;image representation;image segmentation;image sequences;image texture;indexing;learning (artificial intelligence);medical image processing;motion estimation;object detection;object tracking;parallel processing;pose estimation;radar imaging;remote sensing;rendering (computer graphics);seismology;social networking (online);stereo image processing;video coding;video communication;video retrieval;video surveillance;facial features-gender-age recognition;feature detection;feature tracking;foreground-background separation;active contours;levelset methods;image classification;texture synthesis;video analysis;optical flow;motion estimation;video surveillance;visual content analysis;video retrieval;tomographic imaging;radar imaging;remote sensing imaging;seismic sensing imaging;image coding;video coding;video communication;video networking;video streaming;3D image processing;3D modeling;3D reconstruction;multiview processing;document analysis;color imaging;scene reconstruction;3D object reconstruction;distributed system;parallel system;action recognition;action detection;image segmentation;image retrieval;face recognition;edge models;shape models;discriminative local features;human activity recognition;pose estimation;data visualization;image rendering;visual biometrics;superpixel segmentation;saliency analysis;pose recognition;gesture recognition;object tracking;object detection;multimedia indexing;multimedia retrieval;scene analysis;machine learning;learning-based visual applications;hardware system;hyperspectral imaging;multispectral imaging;infrared imaging;stereo image processing;multimedia forensics;privacy protection;content protection;data hiding;robust hashing;counter-forensics;social media;affective media;multidimensional processing;bio-inspired modeling;visual aesthetics;quality assessment;omnidirectional imaging;plenoptics;image representation;Big media Data processing;compressed sensing;image denoising}, 
doi={10.1109/ICIP.2015.7350744}, 
ISSN={}, 
month={Sep.},}
