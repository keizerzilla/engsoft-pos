
@article{ ISI:000464920200012,
Author = {Soltanpour, Sima and Wu, Qing Ming Jonathan},
Title = {{Weighted Extreme Sparse Classifier and Local Derivative Pattern for 3D
   Face Recognition}},
Journal = {{IEEE TRANSACTIONS ON IMAGE PROCESSING}},
Year = {{2019}},
Volume = {{28}},
Number = {{6}},
Pages = {{3020-3033}},
Month = {{JUN}},
Abstract = {{A novel weighted hybrid classifier and a high-order, local normal
   derivative pattern descriptor are proposed for 3D face recognition. The
   local derivative pattern (LDP) captures the detailed information based
   on the local derivative variation in different directions. The LDP is
   computed on three normal maps in x-, y-, and z-directions and on
   different scales. The surface normal captures the orientation of a
   surface at each point of 3D data. More informative local shape
   information is extracted using the surface normal, as compared to depth.
   The nth-order LDP on the surface normal is proposed to encode the more
   detailed features from the (n-1)th-order's local derivative direction
   variations. An extreme learning machine (ELM)-based autoencoder, using a
   multilayer network structure, is employed to select more discriminant
   features and to provide a faster training speed. A weighted hybrid
   framework is proposed to handle facial challenges using a combination of
   the ELM and the sparse representation classifier (SRC). The advantage of
   speed for the ELM and the accuracy for the SRC in a weighted scheme is
   used to enhance the performance of the recognition system. Experimental
   results regarding four famous 3D face databases illustrate the
   generalization and effectiveness of the proposed method in terms of both
   computational cost and recognition accuracy.}},
DOI = {{10.1109/TIP.2019.2893524}},
ISSN = {{1057-7149}},
EISSN = {{1941-0042}},
Unique-ID = {{ISI:000464920200012}},
}

@article{ ISI:000463151400027,
Author = {Rasouli, Maryam S. D. and Payandeh, Shahram},
Title = {{A novel depth image analysis for sleep posture estimation}},
Journal = {{JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING}},
Year = {{2019}},
Volume = {{10}},
Number = {{5, SI}},
Pages = {{1999-2014}},
Month = {{MAY}},
Abstract = {{Recognition of sleep posture and its changes are related to information
   monitoring in a number of health-related applications such as apnea
   prevention and elderly care. This paper uses a less privacy-invading
   approach to classify sleep postures of a person in various
   configurations including side and supine postures. In order to
   accomplish this, a single depth sensor has been utilized to collect
   selective depth signals and populated a dataset associated with the
   depth data. The data is then analyzed by a novel frequency-based feature
   selection approach. These extracted features were then correlated in
   order to rank their information content in various 2D scans from the 3D
   point cloud in order to train a support vector machine (SVM). The data
   of subjects are collected under two conditions. First when they were
   covered with a thin blanket and second without any blanket. In order to
   reduce the dimensionality of the feature space, a T-test approach is
   employed to determine the most dominant set of features in the frequency
   domain. The proposed recognition approach based on the frequency domain
   is also compared with an approach using feature vector defined based on
   skeleton joints. The comparative studies are performed given various
   scenarios and by a variety of datasets. Through our study, it is shown
   that our proposed method offers better performance to that of the
   joint-based method.}},
DOI = {{10.1007/s12652-018-0796-1}},
ISSN = {{1868-5137}},
EISSN = {{1868-5145}},
Unique-ID = {{ISI:000463151400027}},
}

@article{ ISI:000458711300008,
Author = {Patruno, Cosimo and Marani, Roberto and Cicirelli, Grazia and Stella,
   Ettore and D'Orazio, Tiziana},
Title = {{People re-identification using skeleton standard posture and color
   descriptors from RGB-D data}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{89}},
Pages = {{77-90}},
Month = {{MAY}},
Abstract = {{This paper tackles the problem of people re-identification by using soft
   biometrics features. The method works on RGB-D data (color point clouds)
   to determine the best matching among a database of possible users. For
   each subject under testing, skeletal information in three-dimensions is
   used to regularize the pose and to create a skeleton standard posture
   (SSP). A partition grid, whose sizes depend on the SSP, groups the
   samples of the point cloud accordingly to their position. Every group is
   then studied to build the person signature. The same grid is then used
   for the other subjects of the database to preserve information about
   possible shape differences among users. The effectiveness of this novel
   method has been tested on three public datasets. Numerical experiments
   demonstrate an improvement of results with reference to the current
   state-of-the-art, with recognition rates of 97.84\% (on a partition of
   BIWI RGBD-ID), 61.97\% (KinectREID) and 89.71\% (RGBD-ID), respectively.
   (C) 2019 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2019.01.003}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{D'Orazio, Tiziana/H-5032-2019}},
ORCID-Numbers = {{D'Orazio, Tiziana/0000-0003-1473-7110}},
Unique-ID = {{ISI:000458711300008}},
}

@article{ ISI:000464484600009,
Author = {Pala, Pietro and Seidenari, Lorenzo and Berretti, Stefano and Del Bimbo,
   Alberto},
Title = {{Enhanced skeleton and face 3D data for person re-identification from
   depth cameras}},
Journal = {{COMPUTERS \& GRAPHICS-UK}},
Year = {{2019}},
Volume = {{79}},
Pages = {{69-80}},
Month = {{APR}},
Abstract = {{Person re-identification is typically performed using 2D still images or
   videos, where photometric appearance is the main visual cue used to
   discover the presence of a target subject when switching from different
   camera views across time. This invalidates any application where a
   person may change dress across subsequent acquisitions as can be the
   case of patients monitoring at home. Differently from RGB data, 3D
   information as acquired by depth cameras can open the way to person
   re-identification based on biometric cues such as distinguishing traits
   of the body or face. However, the accuracy of skeleton and face geometry
   extracted from depth data is not always adequate to enable person
   recognition, since both these features are affected by the pose of the
   subject and the distance from the camera. In this paper, we propose a
   method to derive a robust skeleton representation from a depth sequence
   and to complement it with a highly discriminative face feature. This is
   obtained by selecting skeleton and face samples based on their quality
   and using the temporal redundancy across the sequence to derive and
   refine cumulated models for both of them. Extracting skeleton and face
   features from such cumulated models and combining them for the
   recognition allow us to improve rank-1 re-identification accuracy
   compared to individual cues. A comparative evaluation on three benchmark
   datasets also shows results at the state-of-the-art. (C) 2019 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.cag.2019.01.003}},
ISSN = {{0097-8493}},
EISSN = {{1873-7684}},
Unique-ID = {{ISI:000464484600009}},
}

@article{ ISI:000463672800006,
Author = {Jin, Hai and Wang, Xun and Lian, Yuanfeng and Hua, Jing},
Title = {{Emotion information visualization through learning of 3D morphable face
   model}},
Journal = {{VISUAL COMPUTER}},
Year = {{2019}},
Volume = {{35}},
Number = {{4}},
Pages = {{535-548}},
Month = {{APR}},
Abstract = {{Analysis and visualization of human facial expressions and its
   applications are useful but challenging. This paper presents a novel
   approach to analyze the facial expressions from images through learning
   of a 3D morphable face model and a quantitative information
   visualization scheme for exploring this type of visual data. More
   specifically, a 3D face database with various facial expressions is
   employed to build a nonnegative matrix factorization (NMF) part-based
   morphable 3D face model. From an input image, a 3D face with expression
   can be reconstructed iteratively by using the NMF morphable 3D face
   model as a priori knowledge, from which basis parameters and a
   displacement map are extracted as features for facial emotion analysis
   and visualization. Based upon the features, two support vector
   regressions are trained to determine the fuzzy valence-arousal (VA)
   values to quantify the emotions. The continuously changing emotion
   status can be intuitively analyzed by visualizing the VA values in VA
   space. Our emotion analysis and visualization system, based on 3D NMF
   morphable face model, detect expressions robustly from various head
   poses, face sizes and lighting conditions and is fully automatic to
   compute the VA values from images or a sequence of video with various
   facial expressions. To evaluate our novel method, we test our system on
   publicly available databases and evaluate the emotion analysis and
   visualization results. We also apply our method to quantifying emotion
   changes during motivational interviews. These experiments and
   applications demonstrate the effectiveness and accuracy of our method.}},
DOI = {{10.1007/s00371-018-1482-1}},
ISSN = {{0178-2789}},
EISSN = {{1432-2315}},
Unique-ID = {{ISI:000463672800006}},
}

@article{ ISI:000457666900036,
Author = {Lv, Chenlei and Wu, Zhongke and Wang, Xingce and Zhou, Mingquan and Toh,
   Kar-Ann},
Title = {{Nasal similarity measure of 3D faces based on curve shape space}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{88}},
Pages = {{458-469}},
Month = {{APR}},
Abstract = {{We propose a novel method for measuring the nasal similarity among 3D
   faces. Firstly, we construct a representation for the nose shape, which
   is composed of a set of geodesic curves, each crosses the bridge of the
   nose. Next, using these geodesic curves, we formulate a similarity
   measure to compare among noses in the curve shape space. Under the
   Riemannian framework, the shape space is a quotient space for which the
   scaling, translation and rotation are removed. Since the nose similarity
   measure is based on the shape comparison, the proposed method has the
   following advantages: (1) the similarity measure is robust to facial
   expressions since the nose is not affected by facial expressions; (2)
   the geometric features of the nose shape match well with the human
   perception; (3) the similarity measure is independent of the mesh grid
   because the chosen nose curves are not sensitive to the triangular mesh
   model. We construct a nasal hierarchical structure for noses
   organization which is based on nose similarity measure results. In our
   experiments, we evaluate the performance of the proposed method and
   compare it with competing methods on three public face databases namely,
   FRGC2.0, Texas3D and BosphorusDB. The results show superiority of the
   proposed method in terms of both the speed and the accuracy when the
   nasal measurements are processed in the nasal hierarchical structure and
   the nasal samples with low sampling rate (5\%-25\% of original point
   cloud). (C) 2018 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.12.006}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000457666900036}},
}

@article{ ISI:000457666900042,
Author = {Pribanic, Tomislav and Petkovic, Tomislav and Donlic, Matea},
Title = {{3D registration based on the direction sensor measurements}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2019}},
Volume = {{88}},
Pages = {{532-546}},
Month = {{APR}},
Abstract = {{3D registration is a very active topic, spanning research areas such as
   computational geometry, computer graphics and pattern recognition. It
   aims to solve spatial transformation that aligns two point clouds. In
   this work we propose the use of a single direction sensor, such as an
   accelerometer or a magnetometer, commonly available on contemporary
   mobile platforms, such as tablets and smartphones. Both sensors have
   been heavily investigated earlier, but only for joint use with other
   sensors, such as gyroscopes and GPS. We show a time-efficient and
   accurate 3D registration method that takes advantage of only either an
   accelerometer or a magnetometer. We demonstrate a 3D reconstruction of
   individual point clouds and the proposed 3D registration method on a
   tablet equipped with an accelerometer or a magnetometer. However, we
   point out that the proposed method is not restricted to mobile
   platforms. Indeed, it can easily be applied in any 3D measurement system
   that is upgradable with some ubiquitous direction sensor, for example by
   adding a smartphone equipped with either an accelerometer or a
   magnetometer. We compare the proposed method against several
   state-of-the-art methods implemented in the open source Point Cloud
   Library (PCL). The proposed method outperforms the PCL methods tested,
   both in terms of processing time and accuracy. (C) 2018 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.12.008}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Petkovic, Tomislav/0000-0002-3054-002X
   Donlic, Matea/0000-0001-5165-6438}},
Unique-ID = {{ISI:000457666900042}},
}

@article{ ISI:000462540400122,
Author = {Quintana, Marcos and Karaoglu, Sezer and Alvarez, Federico and Manuel
   Menendez, Jose and Gevers, Theo},
Title = {{Three-D Wide Faces (3DWF): Facial Landmark Detection and 3D
   Reconstruction over a New RGB-D Multi-Camera Dataset}},
Journal = {{SENSORS}},
Year = {{2019}},
Volume = {{19}},
Number = {{5}},
Month = {{MAR 1}},
Abstract = {{Latest advances of deep learning paradigm and 3D imaging systems have
   raised the necessity for more complete datasets that allow exploitation
   of facial features such as pose, gender or age. In our work, we propose
   a new facial dataset collected with an innovative RGB-D multi-camera
   setup whose optimization is presented and validated. 3DWF includes 3D
   raw and registered data collection for 92 persons from low-cost RGB-D
   sensing devices to commercial scanners with great accuracy. 3DWF
   provides a complete dataset with relevant and accurate visual
   information for different tasks related to facial properties such as
   face tracking or 3D face reconstruction by means of annotated density
   normalized 2K clouds and RGB-D streams. In addition, we validate the
   reliability of our proposal by an original data augmentation method from
   a massive set of face meshes for facial landmark detection in 2D domain,
   and by head pose classification through common Machine Learning
   techniques directed towards proving alignment of collected data.}},
DOI = {{10.3390/s19051103}},
Article-Number = {{1103}},
ISSN = {{1424-8220}},
ORCID-Numbers = {{ALVAREZ GARCIA, FEDERICO/0000-0001-7400-9591}},
Unique-ID = {{ISI:000462540400122}},
}

@article{ ISI:000444795500006,
Author = {Mokhayeri, Fania and Granger, Eric and Bilodeau, Guillaume-Alexandre},
Title = {{Domain-Specific Face Synthesis for Video Face Recognition From a Single
   Sample Per Person}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2019}},
Volume = {{14}},
Number = {{3}},
Pages = {{757-772}},
Month = {{MAR}},
Abstract = {{In video surveillance, face recognition (FR) systems are employed to
   detect individuals of interest appearing over a distributed network of
   cameras. The performance of still-tovideo FR systems can decline
   significantly because faces captured in unconstrained operational domain
   (OD) over multiple video cameras have a different underlying data
   distribution compared to faces captured under controlled conditions in
   the enrollment domain with a still camera. This is particularly true
   when individuals are enrolled to the system using a single reference
   still. To improve the robustness of these systems, it is possible to
   augment the reference set by generating synthetic faces based on the
   original still. However, without the knowledge of the OD, many synthetic
   images must be generated to account for all possible capture conditions.
   FR systems may, therefore, require complex implementations and yield
   lower accuracy when training on many less relevant images. This paper
   introduces an algorithm for domain-specific face synthesis (DSFS) that
   exploits the representative intra-class variation information available
   from the OD. Prior to operation (during camera calibration), a compact
   set of faces from unknown persons appearing in the OD is selected
   through affinity propagation clustering in the captured condition space
   (defined by pose and illumination estimation). The domain-specific
   variations of these face images are then projected onto the reference
   still of each individual by integrating an image-based face relighting
   technique inside the 3-D reconstruction framework. A compact set of
   synthetic faces is generated that resemble individuals of interest under
   the capture conditions relevant to the OD. In a particular
   implementation based on sparse representation classification, the
   synthetic faces generated with the DSFS are employed to form a
   cross-domain dictionary that accounts for structured sparsity, where the
   dictionary blocks combine the original and synthetic faces of each
   individual. Experimental results obtained with videos from the
   Chokepoint and COX-S2V data sets reveal that augmenting the reference
   gallery set of still-to-video FR systems using the proposed DSFS
   approach can provide a significantly higher level of accuracy compared
   with the state-of-the-art approaches, with only a moderate increase in
   its computational complexity.}},
DOI = {{10.1109/TIFS.2018.2866295}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ORCID-Numbers = {{Bilodeau, Guillaume-Alexandre/0000-0003-3227-5060
   mokhayeri, fania/0000-0002-9756-5416}},
Unique-ID = {{ISI:000444795500006}},
}

@article{ ISI:000460829200061,
Author = {Che, Erzhuo and Jung, Jaehoon and Olsen, Michael J.},
Title = {{Object Recognition, Segmentation, and Classification of Mobile Laser
   Scanning Point Clouds: A State of the Art Review}},
Journal = {{SENSORS}},
Year = {{2019}},
Volume = {{19}},
Number = {{4}},
Month = {{FEB 2}},
Abstract = {{Mobile Laser Scanning (MLS) is a versatile remote sensing technology
   based on Light Detection and Ranging (lidar) technology that has been
   utilized for a wide range of applications. Several previous reviews
   focused on applications or characteristics of these systems exist in the
   literature, however, reviews of the many innovative data processing
   strategies described in the literature have not been conducted in
   sufficient depth. To this end, we review and summarize the state of the
   art for MLS data processing approaches, including feature extraction,
   segmentation, object recognition, and classification. In this review, we
   first discuss the impact of the scene type to the development of an MLS
   data processing method. Then, where appropriate, we describe relevant
   generalized algorithms for feature extraction and segmentation that are
   applicable to and implemented in many processing approaches. The methods
   for object recognition and point cloud classification are further
   reviewed including both the general concepts as well as technical
   details. In addition, available benchmark datasets for object
   recognition and classification are summarized. Further, the current
   limitations and challenges that a significant portion of point cloud
   processing techniques face are discussed. This review concludes with our
   future outlook of the trends and opportunities of MLS data processing
   algorithms and applications.}},
DOI = {{10.3390/s19040810}},
Article-Number = {{810}},
ISSN = {{1424-8220}},
ORCID-Numbers = {{Olsen, Michael/0000-0002-2989-5309}},
Unique-ID = {{ISI:000460829200061}},
}

@article{ ISI:000463462600049,
Author = {Shi, Biao and Zang, Huaijuan and Zheng, Rongsheng and Zhan, Shu},
Title = {{An efficient 3D face recognition approach using Frenet feature of
   iso-geodesic curves}},
Journal = {{JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION}},
Year = {{2019}},
Volume = {{59}},
Pages = {{455-460}},
Month = {{FEB}},
Abstract = {{Extracting efficient features from the large volume of 3D facial data
   directly is extremely difficult in 3D face recognition (3D-FR) with the
   latest methods, which mostly require heavy computations and manual
   processing steps. This paper presents a computationally efficient 3D-FR
   system based on a novel Frenet frame-based feature that is derived from
   the 3D facial iso-geodesic curves. In terms of the evaluation of the
   proposed method, we conducted a number of experiments on the CASIA 3D
   face database, and a superior recognition performance has been achieved.
   The performance evaluation suggests that the pose invariance attribute
   of the features relieves the need of an expensive 3D face registration
   in the face preprocessing procedure, where we take less time to process
   conversely. Our experiments further demonstrate that the proposed method
   not only achieves competitive recognition performance when compared with
   some existing techniques for 3D-FR, but also is computationally
   efficient. (C) 2019 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.jvcir.2019.02.002}},
ISSN = {{1047-3203}},
EISSN = {{1095-9076}},
Unique-ID = {{ISI:000463462600049}},
}

@article{ ISI:000459941200022,
Author = {Tian, Liang and Liu, Jing and Guo, Wei},
Title = {{Three-Dimensional Face Reconstruction Using Multi-View-Based Bilinear
   Model}},
Journal = {{SENSORS}},
Year = {{2019}},
Volume = {{19}},
Number = {{3}},
Month = {{FEB 1}},
Abstract = {{Face reconstruction is a popular topic in 3D vision system. However,
   traditional methods often depend on monocular cues, which contain few
   feature pixels and only use their location information while ignoring a
   lot of textural information. Furthermore, they are affected by the
   accuracy of the feature extraction method and occlusion. Here, we
   propose a novel facial reconstruction framework that accurately extracts
   the 3D shapes and poses of faces from images captured at multi-views. It
   extends the traditional method using the monocular bilinear model to the
   multi-view-based bilinear model by incorporating the feature prior
   constraint and the texture constraint, which are learned from multi-view
   images. The feature prior constraint is used as a shape prior to
   allowing us to estimate accurate 3D facial contours. Furthermore, the
   texture constraint extracts a high-precision 3D facial shape where
   traditional methods fail because of their limited number of feature
   points or the mostly texture-less and texture-repetitive nature of the
   input images. Meanwhile, it fully explores the implied 3D information of
   the multi-view images, which also enhances the robustness of the
   results. Additionally, the proposed method uses only two or more
   uncalibrated images with an arbitrary baseline, estimating calibration
   and shape simultaneously. A comparison with the state-of-the-art
   monocular bilinear model-based method shows that the proposed method has
   a significantly higher level of accuracy.}},
DOI = {{10.3390/s19030459}},
Article-Number = {{459}},
ISSN = {{1424-8220}},
ResearcherID-Numbers = {{Wei, Leyi/Q-5699-2018
   }},
ORCID-Numbers = {{Wei, Leyi/0000-0003-1444-190X
   Liu, Jing/0000-0002-2217-0372}},
Unique-ID = {{ISI:000459941200022}},
}

@article{ ISI:000459798800005,
Author = {Sun, Jia and Huang, Di and Wang, Yunhong and Chen, Liming},
Title = {{Expression Robust 3D Facial Landmarking via Progressive Coarse-to-Fine
   Tuning}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2019}},
Volume = {{15}},
Number = {{1}},
Month = {{FEB}},
Abstract = {{Facial landmarking is a fundamental task in automatic machine-based face
   analysis. The majority of existing techniques for such a problem are
   based on 2D images; however, they suffer from illumination and pose
   variations that may largely degrade landmarking performance. The
   emergence of 3D data theoretically provides an alternative to overcome
   these weaknesses in the 2D domain. This article proposes a novel
   approach to 3D facial landmarking, which combines both the advantages of
   feature-based methods as well as model-based ones in a progressive
   three-stage coarse-to-fine manner (initial, intermediate, and fine
   stages). For the initial stage, a few fiducial landmarks (i.e., the nose
   tip and two inner eye corners) are robustly detected through curvature
   analysis, and these points are further exploited to initialize the
   subsequent stage. For the intermediate stage, a statistical model is
   learned in the feature space of three normal components of the facial
   point-cloud rather than the smooth original coordinates, namely Active
   Normal Model (ANM). For the fine stage, cascaded regression is employed
   to locally refine the landmarks according to their geometry attributes.
   The proposed approach can accurately localize dozens of fiducial points
   on each 3D face scan, greatly surpassing the feature-based ones, and it
   also improves the state of the art of the model-based ones in two
   aspects: sensitivity to initialization and deficiency in discrimination.
   The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE
   databases, and competitive results are achieved in comparison with
   counterparts in the literature, clearly demonstrating its effectiveness.}},
DOI = {{10.1145/3282833}},
Article-Number = {{21}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
Unique-ID = {{ISI:000459798800005}},
}

@article{ ISI:000456899900003,
Author = {Zhou, Zixiang and Gong, Jie and Hu, Xuan},
Title = {{Community-scale multi-level post-hurricane damage assessment of
   residential buildings using multi-temporal airborne LiDAR data}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2019}},
Volume = {{98}},
Pages = {{30-45}},
Month = {{FEB}},
Abstract = {{Building damage assessment is a critical task following major hurricane
   events. Use of remotely sensed data to support building damage
   assessment is a logical choice considering the difficulty of gaining
   ground access to the impacted areas immediately after hurricane events.
   However, a remote sensing based damage assessment approach is often only
   capable of detecting severely damaged buildings. In this study, an
   airborne LiDAR based approach is proposed to assess multi-level
   hurricane damage at the community scale. In the proposed approach,
   building clusters are first extracted using a density-based algorithm. A
   novel cluster matching algorithm is proposed to robustly match
   post-event and pre-event building clusters. Multiple features including
   roof area and volume, roof orientation, and roof shape are computed as
   building damage indicators. A hierarchical determination process is then
   employed to identify the extent of damage to each building object. The
   results of this study suggest that our proposed approach is capable of
   1) recognizing building objects, 2) extracting damage features, and 3)
   characterizing the extent of damage to individual building properties.}},
DOI = {{10.1016/j.autcon.2018.10.018}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
Unique-ID = {{ISI:000456899900003}},
}

@article{ ISI:000454941500027,
Author = {Chong, Lee Ying and Ong, Thian Song and Teoh, Andrew Beng Jin},
Title = {{Feature fusions for 2.5D face recognition in Random Maxout Extreme
   Learning Machine}},
Journal = {{APPLIED SOFT COMPUTING}},
Year = {{2019}},
Volume = {{75}},
Pages = {{358-372}},
Month = {{FEB}},
Abstract = {{Contemporary face recognition system is often based on either 2D
   (texture) or 3D (texture + shape) face modality. An alternative modality
   that utilizes range (depth) facial images, namely 2.5D face recognition
   emerges. In this paper, we propose a 2.5D face descriptor that based on
   the Regional Covariance Matrix (RCM), a powerful means of feature fusion
   technique and a novel classifier dubbed Random Maxout Extreme Learning
   Machine (RMELM). The RCM of interest is constructed based on the
   Principal Component Analysis (PCA) filters responses of facial texture
   and/or range image, wherein the PCA filters are learned from a two-layer
   PCA network. The RMELM is an ELM variant where the activation function
   is based on the locally linear maxout function, in place of typical
   global non-linear functions in ELM. Since the RCM is a special case of
   symmetric positive definite matrix that resides on the Tensor manifold;
   a gap exists in between RCM and RMELM, which is a vector-based
   classifier. To bridge the gap, we flatten the manifold by transforming
   the RCM to a feature vector via a matrix logarithm operator.
   Experimental results from two public 3D face databases, FRGC v2.O
   database and Gavab database, validated our proposed method is promising
   in 2.5D face recognition. (C) 2018 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.asoc.2018.11.024}},
ISSN = {{1568-4946}},
EISSN = {{1872-9681}},
ResearcherID-Numbers = {{Teoh, Andrew Beng Jin/F-4422-2010}},
ORCID-Numbers = {{Teoh, Andrew Beng Jin/0000-0001-5063-9484}},
Unique-ID = {{ISI:000454941500027}},
}

@article{ ISI:000457037300020,
Author = {Shao, Gang and Shao, Guofan and Fei, Songlin},
Title = {{Delineation of individual deciduous trees in plantations with
   low-density LiDAR data}},
Journal = {{INTERNATIONAL JOURNAL OF REMOTE SENSING}},
Year = {{2019}},
Volume = {{40}},
Number = {{1}},
Pages = {{346-363}},
Month = {{JAN 2}},
Abstract = {{Delineation of individual deciduous trees with Light Detection and
   Ranging (LiDAR) data has long been sought for accurate forest inventory
   in temperate forests. Previous attempts mainly focused on high-density
   LiDAR data to obtain reliable delineation results, which may have
   limited applications due to the high cost and low availability of such
   data. Here, the feasibility of individual deciduous tree delineation
   with low-density LiDAR data was examined using a point-density-based
   algorithm. First a high-resolution point density model (PDM) was
   developed from low-density LiDAR point cloud to locate individual trees
   through the horizontal spatial distribution of LiDAR points. Then,
   individual tree crowns and associated attributes were delineated with a
   2D marker-controlled watershed segmentation. Additionally, the PDM-based
   approach was compared with a conventional canopy height model (CHM)
   based delineation. The results demonstrated that the PDM-based approach
   produced an 89\% detection accuracy to identify deciduous trees in our
   study area. The tree attributes derived from the PDM-based algorithm
   explained 81\% and 83\% of tree height and crown width variations of
   forest stands, respectively. The conventional CHM-based tree attributes,
   on the other hand, could explain only 71\% and 66\% of tree height and
   crown width, respectively. Our results suggest that the application of
   the PDM-based individual tree identification in deciduous forests with
   low-density LiDAR data is feasible and has relatively high accuracy to
   predict tree height and crown width, which are highly desired in
   large-scale forest inventory and analysis.}},
DOI = {{10.1080/01431161.2018.1513664}},
ISSN = {{0143-1161}},
EISSN = {{1366-5901}},
ORCID-Numbers = {{Shao, Gang/0000-0003-3198-966X}},
Unique-ID = {{ISI:000457037300020}},
}

@article{ ISI:000461160000001,
Author = {Shah, Syed Afaq Ali and Bennamoun, Mohammed and Molton, Michael K.},
Title = {{Machine Learning Approaches for Prediction of Facial Rejuvenation Using
   Real and Synthetic Data}},
Journal = {{IEEE ACCESS}},
Year = {{2019}},
Volume = {{7}},
Pages = {{23779-23787}},
Abstract = {{This paper proposes a novel machine learning approaches to predict the
   outcome of facial rejuvenation prior to a cosmetic procedure. This is
   achieved by estimating the required amount of dermal filler volume that
   needs to be applied on the face by learning the underlying structural
   mapping from the pretreatment and posttreatment 3D face images. We
   develop and train our proposed deep neural network, called Rejuv3DNet,
   designed specifically to predict the dermal filler volume. We also
   propose the kernel regression (KR)-based model to validate and improve
   our volume estimation results using regression. Our other contributions
   include the development of the first 3D face cosmetic dataset, which
   consists of real-world pretreatment and posttreatment 3D face images and
   a novel technique for the generation of synthetic cosmetic treatment 3D
   face images. Our experimental results show that the proposed Rejuv3DNet
   and the KR model achieve 62.5\% and 66.67\%, respectively, on real-world
   data, while these techniques achieve a prediction accuracy of 75.2\% and
   89.5\%, and 77.2\% and 90.1\% on our two different synthetic datasets.
   Our proposed techniques have been found to be computationally efficient,
   achieving near real-time prediction performance. The reported accuracies
   are our preliminary results for proof of concept, which can be improved
   with more data. The proposed approach has the potential for further
   investigation in the cosmetic surgery domain.}},
DOI = {{10.1109/ACCESS.2019.2899379}},
ISSN = {{2169-3536}},
ORCID-Numbers = {{Bennamoun, Mohammed/0000-0002-6603-3257
   Shah, Syed Afaq Ali/0000-0003-2181-8445}},
Unique-ID = {{ISI:000461160000001}},
}

@article{ ISI:000460647500026,
Author = {Dubey, Arun Kumar and Jain, Vanita},
Title = {{A review of face recognition methods using deep learning network}},
Journal = {{JOURNAL OF INFORMATION \& OPTIMIZATION SCIENCES}},
Year = {{2019}},
Volume = {{40}},
Number = {{2, SI}},
Pages = {{547-558}},
Abstract = {{Face recognition could be a technology capable of distinguishing or
   confirming an individual from a digital image or a video frame from a
   video supply. Face recognition technology is employed in wide selection
   of applications like authentication, access management, and police
   investigation. It is finding applications in all industries ranging from
   retail, advertising to banking etc. It is to this extent that Large
   retailers are using facial recognition to recognize customers and
   present offers, they also use it to catch shoplifters. Deep learning
   Network is influencing every aspect of computer vision technology and
   research. In this paper, we are depicting the role and achievements of
   different deep models for face recognition in images and videos, we have
   also compared recent algorithms for face recognition.}},
DOI = {{10.1080/02522667.2019.1582875}},
ISSN = {{0252-2667}},
EISSN = {{2169-0103}},
Unique-ID = {{ISI:000460647500026}},
}

@article{ ISI:000458017400007,
Author = {Jin, Hai and Lian, Yuanfeng and Hua, Jing},
Title = {{Learning Facial Expressions with 3D Mesh Convolutional Neural Network}},
Journal = {{ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY}},
Year = {{2019}},
Volume = {{10}},
Number = {{1, SI}},
Month = {{JAN}},
Abstract = {{Making machines understand human expressions enables various useful
   applications in human-machine interaction. In this article, we present a
   novel facial expression recognition approach with 3D Mesh Convolutional
   Neural Networks (3DMCNN) and a visual analytics-guided 3DMCNN design and
   optimization scheme. From an RGBD camera, we first reconstruct a 3D face
   model of a subject with facial expressions and then compute the
   geometric properties of the surface. Instead of using regular
   Convolutional Neural Networks (CNNs) to learn intensities of the facial
   images, we convolve the geometric properties on the surface of the 3D
   model using 3DMCNN. We design a geodesic distance-based convolution
   method to overcome the difficulties raised from the irregular sampling
   of the face surface mesh. We further present interactive visual
   analytics for the purpose of designing and modifying the networks to
   analyze the learned features and cluster similar nodes in 3DMCNN. By
   removing low-activity nodes in the network, the performance of the
   network is greatly improved. We compare our method with the regular
   CNN-based method by interactively visualizing each layer of the networks
   and analyze the effectiveness of our method by studying representative
   cases. Testing on public datasets, our method achieves a higher
   recognition accuracy than traditional image-based CNN and other 3D CNNs.
   The proposed framework, including 3DMCNN and interactive visual
   analytics of the CNN, can be extended to other applications.}},
DOI = {{10.1145/3200572}},
Article-Number = {{7}},
ISSN = {{2157-6904}},
EISSN = {{2157-6912}},
Unique-ID = {{ISI:000458017400007}},
}

@article{ ISI:000440782400006,
Author = {Neves, Joao and Proenca, Hugo},
Title = {{``A Leopard Cannot Change Its Spots{''}: Improving Face Recognition
   Using 3D-Based Caricatures}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2019}},
Volume = {{14}},
Number = {{1}},
Pages = {{151-161}},
Month = {{JAN}},
Abstract = {{Caricatures refer to a representation of a person, in which the
   distinctive features are deliberately exaggerated, with several studies
   showing that humans perform better at recognizing people from
   caricatures than using original images. Inspired by this observation,
   this paper introduces the first fully automated caricature-based face
   recognition approach capable of working with data acquired in the wild.
   Our approach leverages the 3D face structure from a single 2D image and
   compares it with a reference model for obtaining a compact
   representation of face features deviations. This descriptor is
   subsequently deformed using a ``measure locally, weight globally{''}
   strategy to resemble the caricature drawing process. The deformed
   deviations are incorporated in the 3D model using the Laplacian mesh
   deformation algorithm, and the 2D face caricature image is obtained by
   projecting the deformed model in the original camera view. To
   demonstrate the advantages of caricature-based face recognition, we
   train the VGG-face network from scratch using either original face
   images (baseline) or caricatured images and use these models for
   extracting face descriptors from the LFW, IJB-A, and MegaFace data sets.
   The experiments show an increase in the recognition accuracy when using
   caricatures rather than original images. Moreover, our approach achieves
   competitive results with the state-of-the-art face recognition methods,
   even without explicitly tuning the network for any of the evaluation
   sets.}},
DOI = {{10.1109/TIFS.2018.2846617}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ResearcherID-Numbers = {{Neves, Joao/G-6477-2016}},
ORCID-Numbers = {{Neves, Joao/0000-0003-0139-2213}},
Unique-ID = {{ISI:000440782400006}},
}

@article{ ISI:000450379200003,
Author = {Kaminska, Agnieszka and Lisiewicz, Maciej and Sterenczak, Krzysztof and
   Kraszewski, Bartlomiej and Sadkowski, Rafal},
Title = {{Species-related single dead tree detection using multi-temporal ALS data
   and CIR imagery}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{219}},
Pages = {{31-43}},
Month = {{DEC 15}},
Abstract = {{The assessment of the health conditions of trees in forests is extremely
   important for biodiversity, forest management, global environment
   monitoring, and carbon dynamics. There is a vast amount of research
   using remote sensing (RS) techniques for the assessment of the current
   condition of a forest, but only a small number of these are concerned
   with detection and classification of dead trees. Among the available RS
   techniques, only the airborne laser scanner (ALS) enables dead tree
   detection at the single tree level with high accuracy.
   The main objective of the study was to identify spruce, pine and
   deciduous trees by alive or dead classifications. Three RS data sets
   including ALS (leaf-on and leaf-off) and color-infrared (CIR) imagery
   (leaf-on) were used for the study. We used intensity and structural
   variables from the ALS data and spectral information derived from aerial
   imagery for the classification procedure. Additionally, we tested the
   differences in the classification accuracy of all variants contained in
   the data integration. In the study, the random forest (RF) classifier
   was used. The study was carried out in the Polish part of the Bialowieia
   Forest (BF).
   In general, we can state that all classifications, with different
   combinations of ALS features and CIR, resulted in high overall accuracy
   (OA >= 90\%) and Kappa (kappa > 0.86). For the best variant
   (CIR\_ALS(WSn-FH)), the mean values of overall accuracy and Kappa were
   equal to 94.3\% and 0.93, respectively. The leaf -on point cloud
   features alone produced the lowest accuracies (OA = 75-81\% and x =
   0.68-0.76). Improvements of 0-0.04 in the Kappa coefficient and 0-3.1\%
   in the overall classification accuracy were found after the point cloud
   normalization for all variants. Full -height point cloud features (F)
   produced lower accuracies than the results based on features calculated
   for half of the tree height point clouds (H) and combined FH.
   The importance of each of the predictors for different data sets for
   tree species classification provided by the RF algorithm was
   investigated. The lists of top features were the same, independent of
   intensity normalization. For the classification based on both of the
   point clouds (leaf on and leaf-off), three structural features (a
   proportion of first returns for both half -height and full -height
   variants and the canopy relief ratio of points) and two intensity
   features from first returns and half -height variant (the coefficient of
   variation and skewness) were rated as the most important. In the
   classification based on the point cloud with CIR features, two image
   features were among the most important (the NDVI and mean value of
   reflectance in the green band).}},
DOI = {{10.1016/j.rse.2018.10.005}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Lisiewicz, Maciej/Y-3668-2018
   Kraszewski, Bartlomiej/U-9482-2017}},
ORCID-Numbers = {{Lisiewicz, Maciej/0000-0003-0676-8291
   Sterenczak, Krzysztof/0000-0002-9556-0144
   Kraszewski, Bartlomiej/0000-0001-6161-7619}},
Unique-ID = {{ISI:000450379200003}},
}

@article{ ISI:000454376800005,
Author = {Dou, Pengfei and Kakadiaris, Ioannis A.},
Title = {{Multi-view 3D face reconstruction with deep recurrent neural networks}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2018}},
Volume = {{80}},
Pages = {{80-91}},
Month = {{DEC}},
Abstract = {{Image-based 3D face reconstruction has great potential in different
   areas, such as facial recognition, facial analysis, and facial
   animation. Due to the variations in image quality, single-image-based 3D
   face reconstruction might not be sufficient to accurately reconstruct a
   3D face. To overcome this limitation, multi-view 3D face reconstruction
   uses multiple images of the same subject and aggregates complementary
   information for better accuracy. Though appealing, there are multiple
   challenges in practice. Among these challenges, the most significant is
   the difficulty to establish coherent and accurate correspondence among a
   set of images, especially when these images are captured under
   unconstrained in-the-wild condition. This work proposes a method, Deep
   Recurrent 3D FAce Reconstruction (DRFAR), to solve the task of
   multi-view 3D face reconstruction using a subspace representation of the
   3D facial shape and a deep recurrent neural network that consists of
   both a deep convolutional neural network (DCNN) and a recurrent neural
   network (RNN). The DCNN disentangles the facial identity and the facial
   expression components for each single image independently, while the RNN
   fuses identity-related features from the DCNN and aggregates the
   identity specific contextual information, or the identity signal, from
   the whole set of images to estimate the facial identity parameter, which
   is robust to variations in image quality and is consistent over the
   whole set of images. Experimental results indicate significant
   improvement over state-of-the-art in both the accuracy and the
   consistency of 3D face reconstruction. Moreover, face recognition
   results on 11B-A with the UR2D face recognition pipeline indicate that,
   compared to single-view 3D face reconstruction, the proposed multi-view
   3D face reconstruction algorithm can improve the face identification
   accuracy of UR2D by two percentage points in Rank-1 identification rate.
   (C) 2018 Published by Elsevier B.V.}},
DOI = {{10.1016/j.imavis.2018.09.004}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000454376800005}},
}

@article{ ISI:000448786500006,
Author = {Qian, Kun and Su, Kehua and Zhang, Jialing and Li, Yinghua},
Title = {{A 3D face registration algorithm based on conformal mapping}},
Journal = {{CONCURRENCY AND COMPUTATION-PRACTICE \& EXPERIENCE}},
Year = {{2018}},
Volume = {{30}},
Number = {{22, SI}},
Month = {{NOV 25}},
Note = {{International Conference of Intelligence Computation and Evolutionary
   Computation (ICEC), Wuhan, PEOPLES R CHINA, DEC 07-10, 2017}},
Organization = {{Informat Technol \& Ind Engn Res Ctr}},
Abstract = {{Recently, 3D facial datasets are more easily available, and at the same
   time, the research of 3D face becomes more and more important. One of
   the most important research fields is 3D face registration, which plays
   an important role in face recognition, face shape analysis, and face
   animation. However, one of the most challenging issues in 3D face
   registration is to obtain a unique mapping for faces with different
   expression and landmark constraints. In this paper, we propose a novel
   conformal mapping algorithm to deal with the 3D face registration.
   Besides, the calculation is about harmonic energy, which makes our
   method applicable to low-quality meshes. We begin with a harmonic
   mapping, then minimize the harmonic energy by a specific boundary
   condition on surfaces, and to obtain the conformal mapping, finally, we
   use a landmark-constrained surface registration algorithm to register
   faces. Numerical experiments on various surfaces demonstrate the
   efficiency and robustness of our method.}},
DOI = {{10.1002/cpe.4654}},
Article-Number = {{e4654}},
ISSN = {{1532-0626}},
EISSN = {{1532-0634}},
Unique-ID = {{ISI:000448786500006}},
}

@article{ ISI:000451316500001,
Author = {Zhou, Song and Xiao, Sheng},
Title = {{3D face recognition: a survey}},
Journal = {{HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES}},
Year = {{2018}},
Volume = {{8}},
Month = {{NOV 24}},
Abstract = {{3D face recognition has become a trending research direction in both
   industry and academia. It inherits advantages from traditional 2D face
   recognition, such as the natural recognition process and a wide range of
   applications. Moreover, 3D face recognition systems could accurately
   recognize human faces even under dim lights and with variant facial
   positions and expressions, in such conditions 2D face recognition
   systems would have immense difficulty to operate. This paper summarizes
   the history and the most recent progresses in 3D face recognition
   research domain. The frontier research results are introduced in three
   categories: pose-invariant recognition, expression-invariant
   recognition, and occlusion-invariant recognition. To promote future
   research, this paper collects information about publicly available 3D
   face databases. This paper also lists important open problems.}},
DOI = {{10.1186/s13673-018-0157-2}},
Article-Number = {{35}},
ISSN = {{2192-1962}},
Unique-ID = {{ISI:000451316500001}},
}

@article{ ISI:000451733800040,
Author = {Shen, Yueqian and Lindenbergh, Roderik and Wang, Jinguo and Ferreira,
   Vagner G.},
Title = {{Extracting Individual Bricks from a Laser Scan Point Cloud of an
   Unorganized Pile of Bricks}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Bricks are the vital component of most masonry structures. Their
   maintenance is critical to the protection of masonry buildings.
   Terrestrial Light Detection and Ranging (TLidar) systems provide massive
   point cloud data in an accurate and fast way. TLidar enables us to
   sample and store the state of a brick surface in a practical way. This
   article aims to extract individual bricks from an unorganized pile of
   bricks sampled by a dense point cloud. The method automatically segments
   and models the individual bricks. The methodology is divided into five
   main steps: Filter needless points, brick boundary points removal,
   coarse segmentation using 3D component analysis, planar segmentation and
   grouping, and brick reconstruction. A novel voting scheme is used to
   segment the planar patches in an effective way. Brick reconstruction is
   based on the geometry of single brick and its corresponding nominal size
   (length, width and height). The number of bricks reconstructed is around
   75\%. An accuracy assessment is performed by comparing 3D coordinates of
   the reconstructed vertices to the manually picked vertices. The standard
   deviations of differences along x, y and z axes are 4.55 mm, 4.53 mm and
   4.60 mm, respectively. The comparison results indicate that the accuracy
   of reconstruction based on the introduced methodology is high and
   reliable. The work presented in this paper provides a theoretical basis
   and reference for large scene applications in brick-like structures.
   Meanwhile, the high-accuracy brick reconstruction lays the foundation
   for further brick displacement estimation.}},
DOI = {{10.3390/rs10111709}},
Article-Number = {{1709}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{Lindenbergh, Roderik/0000-0001-8655-5266
   Shen, Yueqian/0000-0003-2455-9012}},
Unique-ID = {{ISI:000451733800040}},
}

@article{ ISI:000449893800008,
Author = {Derkach, Dmytro and Sukno, Federico M.},
Title = {{Automatic local shape spectrum analysis for 3D facial expression
   recognition}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2018}},
Volume = {{79}},
Pages = {{86-98}},
Month = {{NOV}},
Note = {{12th IEEE International Conference on Automatic Face and Gesture
   Recognition (FG), Washington, DC, MAY 30-JUN 03, 2017}},
Organization = {{IEEE; Baidu; Mitsubishi Elect Res Labs Inc; 3dMD; DI4D; Syst \& Technol
   Res; ObjectVideo Labs; MUKH Technologies; IEEE Comp Soc}},
Abstract = {{We investigate the problem of Facial Expression Recognition (FER) using
   3D data. Building from one of the most successful frameworks for facial
   analysis using exclusively 3D geometry, we extend the analysis from a
   curve-based representation into a spectral representation, which allows
   a complete description of the underlying surface that can be further
   tuned to the desired level of detail. Spectral representations are based
   on the decomposition of the geometry in its spatial frequency
   components, much like a Fourier transform, which are related to
   intrinsic characteristics of the surface. In this work, we propose the
   use of Graph Laplacian Features (GLFs), which result from the projection
   of local surface patches into a common basis obtained from the Graph
   Laplacian eigenspace. We extract patches around facial landmarks and
   include a state-of-the-art localization algorithm to allow for
   fully-automatic operation. The proposed approach is tested on the three
   most popular databases for 3D FER (BU-3DFE, Bosphorus and BU-4DFE) in
   terms of expression and AU recognition. Our results show that the
   proposed GLFs consistently outperform the curves-based approach as well
   as the most popular alternative for spectral representation, Shape-DNA,
   which is based on the Laplace Beltrami Operator and cannot provide a
   stable basis that guarantee that the extracted signatures for the
   different patches are directly comparable. Interestingly, the accuracy
   improvement brought by GLFs is obtained also at a lower computational
   cost. Considering the extraction of patches as a common step between the
   three compared approaches, the curves-based framework requires a costly
   elastic deformation between corresponding curves (e.g. based on splines)
   and Shape-DNA requires computing an eigen-decomposition of every new
   patch to be analyzed. In contrast, GLFs only require the projection of
   the patch geometry into the Graph Laplacian eigenspace, which is common
   to all patches and can therefore be pre-computed off-line. We also show
   that 14 automatically detected landmarks are enough to achieve high FER
   and AU detection rates, only slightly below those obtained when using
   sets of manually annotated landmarks. (C) 2018 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.imavis.2018.09.007}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000449893800008}},
}

@article{ ISI:000433909100002,
Author = {Song, Xiaoning and Feng, Zhen-Hua and Hu, Guosheng and Kittler, Josef
   and Wu, Xiao-Jun},
Title = {{Dictionary Integration Using 3D Morphable Face Models for Pose-Invariant
   Collaborative-Representation-Based Classification}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2018}},
Volume = {{13}},
Number = {{11}},
Pages = {{2734-2745}},
Month = {{NOV}},
Abstract = {{The paper presents a dictionary integration algorithm using 3D morphable
   face models (3DMM) for pose-invariant collaborative-representation-based
   face classification. To this end, we first fit a 3DMM to the 2D face
   images of a dictionary to reconstruct the 3D shape and texture of each
   image. The 3D faces are used to render a number of virtual 2D face
   images with arbitrary pose variations to augment the training data, by
   merging the original and rendered virtual samples to create an extended
   dictionary. Second, to reduce the information redundancy of the extended
   dictionary and improve the sparsity of reconstruction coefficient
   vectors using collaborative-representation-based classification (CRC),
   we exploit an on-line class elimination scheme to optimise the extended
   dictionary by identifying the training samples of the most
   representative classes for a given query. The final goal is to perform
   pose-invariant face classification using the proposed dictionary
   integration method and the on-line pruning strategy under the CRC
   framework. Experimental results obtained for a set of well-known face
   data sets demonstrate the merits of the proposed method, especially its
   robustness to pose variations.}},
DOI = {{10.1109/TIFS.2018.2833052}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ORCID-Numbers = {{Kittler, Josef/0000-0002-8110-9205
   Feng, Zhenhua/0000-0002-4485-4249}},
Unique-ID = {{ISI:000433909100002}},
}

@article{ ISI:000448833400016,
Author = {Ratyal, Naeem and Taj, Imtiaz and Bajwa, Usama and Sajid, Muhammad},
Title = {{Pose and Expression Invariant Alignment based Multi-View 3D Face
   Recognition}},
Journal = {{KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS}},
Year = {{2018}},
Volume = {{12}},
Number = {{10}},
Pages = {{4903-4929}},
Month = {{OCT 31}},
Abstract = {{In this study, a fully automatic pose and expression invariant 3D face
   alignment algorithm is proposed to handle frontal and profile face
   images which is based on a two pass course to fine alignment strategy.
   The first pass of the algorithm coarsely aligns the face images to an
   intrinsic coordinate system (ICS) through a single 3D rotation and the
   second pass aligns them at fine level using a minimum nose tip-scanner
   distance (MNSD) approach. For facial recognition, multi-view faces are
   synthesized to exploit real 3D information and test the efficacy of the
   proposed system. Due to optimal separating hyper plane (OSH), Support
   Vector Machine (SVM) is employed in multi-view face verification (FV)
   task. In addition, a multi stage unified classifier based face
   identification (FI) algorithm is employed which combines results from
   seven base classifiers, two parallel face recognition algorithms and an
   exponential rank combiner, all in a hierarchical manner. The performance
   figures of the proposed methodology are corroborated by extensive
   experiments performed on four benchmark datasets: GavabDB, Bosphorus,
   UMB-DB and FRGC v2.0. Results show mark improvement in alignment
   accuracy and recognition rates. Moreover, a computational complexity
   analysis has been carried out for the proposed algorithm which reveals
   its superiority in terms of computational efficiency as well.}},
DOI = {{10.3837/tiis.2018.10.016}},
ISSN = {{1976-7277}},
Unique-ID = {{ISI:000448833400016}},
}

@article{ ISI:000447286200001,
Author = {Song, Wei and Zou, Shuanghui and Tian, Yifei and Fong, Simon and Cho,
   Kyungeun},
Title = {{Classifying 3D objects in LiDAR point clouds with a back-propagation
   neural network}},
Journal = {{HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES}},
Year = {{2018}},
Volume = {{8}},
Month = {{OCT 12}},
Abstract = {{Due to object recognition accuracy limitations, unmanned ground vehicles
   (UGVs) must perceive their environments for local path planning and
   object avoidance. To gather high-precision information about the UGV's
   surroundings, Light Detection and Ranging (LiDAR) is frequently used to
   collect large-scale point clouds. However, the complex spatial features
   of these clouds, such as being unstructured, diffuse, and disordered,
   make it difficult to segment and recognize individual objects. This
   paper therefore develops an object feature extraction and classification
   system that uses LiDAR point clouds to classify 3D objects in urban
   environments. After eliminating the ground points via a height threshold
   method, this describes the 3D objects in terms of their geometrical
   features, namely their volume, density, and eigenvalues. A
   back-propagation neural network (BPNN) model is trained (over the course
   of many iterations) to use these extracted features to classify objects
   into five types. During the training period, the parameters in each
   layer of the BPNN model are continually changed and modified via
   back-propagation using a non-linear sigmoid function. In the system, the
   object segmentation process supports obstacle detection for autonomous
   driving, and the object recognition method provides an environment
   perception function for terrain modeling. Our experimental results
   indicate that the object recognition accuracy achieve 91.5\% in outdoor
   environment.}},
DOI = {{10.1186/s13673-018-0152-7}},
Article-Number = {{29}},
ISSN = {{2192-1962}},
ORCID-Numbers = {{Wei, Song/0000-0002-5909-9661}},
Unique-ID = {{ISI:000447286200001}},
}

@article{ ISI:000449578500012,
Author = {Fei, Hongyan and Tu, Bing and Chen, Ququ and He, Danbing and Zhou,
   Chengle and Peng, Yishu},
Title = {{An overview of face-related technologies}},
Journal = {{JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION}},
Year = {{2018}},
Volume = {{56}},
Pages = {{139-143}},
Month = {{OCT}},
Abstract = {{In recent years, information technology is developing continuously and
   set off a burst of artificial intelligence boom in the field of science.
   The development of advanced technologies such as unmanned driving and AI
   chips, is the extensive application of artificial intelligence.
   Face-related technologies have a wide range of applications because of
   intuitive results and good concealment. Since 3D face information can
   provide more comprehensive facial information than 2D face information,
   and it can solve many difficulties that cannot be solved in 2D face
   recognition. Therefore, more and more researchers have studied 3D face
   recognition in recent years. Under the new circumstances, the research
   on face are experiencing all kinds of challenges. With the tireless of
   many scientists, the new technology is also making a constant progress,
   and in the development of many technologies it still maintained its
   leading position. In this paper, we simply sort out the present
   development process of facial correlation technology, and the general
   evolution of this technology is outlined. Finally, the practical
   significance of this technology development is briefly discussed. (C)
   2018 Published by Elsevier Inc.}},
DOI = {{10.1016/j.jvcir.2018.09.012}},
ISSN = {{1047-3203}},
EISSN = {{1095-9076}},
Unique-ID = {{ISI:000449578500012}},
}

@article{ ISI:000443025400016,
Author = {ElSayed, Ahmed and Kongar, Elif and Mahmood, Ausif and Sobh, Tarek},
Title = {{Unsupervised face recognition in the wild using high-dimensional
   features under super-resolution and 3D alignment effect}},
Journal = {{SIGNAL IMAGE AND VIDEO PROCESSING}},
Year = {{2018}},
Volume = {{12}},
Number = {{7}},
Pages = {{1353-1360}},
Month = {{OCT}},
Abstract = {{Face recognition algorithms customarily utilize query faces captured
   from uncontrolled, in the wild, environments. The quality of these
   facial images is affected by various internal factors, including the
   quality of sensors used in outdoor cameras as well as external ones,
   such as the quality and direction of light. These factors adversely
   affect the overall quality of the captured images often causing blurring
   and/or low resolution, a phenomena commonly referred to as image
   degradation. Super-resolution algorithms are highly effective in
   improving the resolution of degraded images, more so if the captured
   face is small requiring scaling up. With this motivation, this research
   aims at demonstrating the effect of one of the state-of-the-art image
   super-resolution algorithms on the labeled faces in the wild (lfw)
   dataset. In this regard, several cases are analyzed to demonstrate the
   effectiveness of the super-resolution algorithm. Each case is then
   investigated independently comparing the order of execution before or
   after the 3D face alignment step. Following this, resulting images are
   tested on a closed set face recognition protocol using unsupervised
   algorithms with high-dimensional extracted features. The inclusion of
   super-resolution resulted in improvement in the recognition rate
   compared to unsupervised algorithm results reported in the literature.}},
DOI = {{10.1007/s11760-018-1289-6}},
ISSN = {{1863-1703}},
EISSN = {{1863-1711}},
ORCID-Numbers = {{ElSayed, Ahmed/0000-0003-4746-9095}},
Unique-ID = {{ISI:000443025400016}},
}

@article{ ISI:000440851800017,
Author = {Siqueira, Robson S. and Alexandre, Gilderlane R. and Soares, Jose M. and
   The, George A. P.},
Title = {{Triaxial Slicing for 3-D Face Recognition From Adapted Rotational
   Invariants Spatial Moments and Minimal Keypoints Dependence}},
Journal = {{IEEE ROBOTICS AND AUTOMATION LETTERS}},
Year = {{2018}},
Volume = {{3}},
Number = {{4}},
Pages = {{3513-3520}},
Month = {{OCT}},
Abstract = {{This letter presents a multiple slicing model for three-dimensional
   (3-D) images of human face, using the frontal, sagittal, and transverse
   orthogonal planes. The definition of the segments depends on just one
   key point, the nose tip, which makes it simple and independent of the
   detection of several key points. For facial recognition, attributes
   based on adapted 2-D spatial moments of Hu and 3-D spatial invariant
   rotation moments are extracted from each segment. Tests with the
   proposed model using the Bosphorus Database for neutral vs nonneutral
   ROC I experiment, applying linear discriminant analysis as classifier
   and more than one sample for training, achieved 98.7\% of verification
   rate at 0.1\% of false acceptance rate. By using the support vector
   machine as classifier the rank1 experiment recognition rates of 99\% and
   95.4\% have been achieved for a neutral vs neutral and for a neutral vs
   non neutral, respectively. These results approach the state-of-the-art
   using Bosphorus Database and even surpasses it when anger and disgust
   expressions are evaluated. In addition, we also evaluate the
   generalization of our method using the FRGC v2.0 database and achieve
   competitive results, making the technique promising, especially for its
   simplicity.}},
DOI = {{10.1109/LRA.2018.2854295}},
ISSN = {{2377-3766}},
ORCID-Numbers = {{Marques Soares, Jose/0000-0002-5111-5794
   Alexandre, Gilderlane/0000-0002-8778-5351
   The, George/0000-0002-8064-8901}},
Unique-ID = {{ISI:000440851800017}},
}

@article{ ISI:000449993800083,
Author = {Wu, Jianwei and Yao, Wei and Polewski, Przemyslaw},
Title = {{Mapping Individual Tree Species and Vitality along Urban Road Corridors
   with LiDAR and Imaging Sensors: Point Density versus View Perspective}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{To meet a growing demand for accurate high-fidelity vegetation cover
   mapping in urban areas toward biodiversity conservation and assessing
   the impact of climate change, this paper proposes a complete approach to
   species and vitality classification at single tree level by synergistic
   use of multimodality 3D remote sensing data. So far, airborne laser
   scanning system (ALS or airborne LiDAR) has shown promising results in
   tree cover mapping for urban areas. This paper analyzes the potential of
   mobile laser scanning system/mobile mapping system (MLS/MMS)-based
   methods for recognition of urban plant species and characterization of
   growth conditions using ultra-dense LiDAR point clouds and provides an
   objective comparison with the ALS-based methods. Firstly, to solve the
   extremely intensive computational burden caused by the classification of
   ultra-dense MLS data, a new method for the semantic labeling of LiDAR
   data in the urban road environment is developed based on combining a
   conditional random field (CRF) for the context-based classification of
   3D point clouds with shape priors. These priors encode geometric
   primitives found in the scene through sample consensus segmentation.
   Then, single trees are segmented from the labelled tree points using the
   3D graph cuts algorithm. Multinomial logistic regression classifiers are
   used to determine the fine deciduous urban tree species of conversation
   concern and their growth vitality. Finally, the weight-of-evidence
   (WofE) based decision fusion method is applied to combine the
   probability outputs of classification results from the MLS and ALS data.
   The experiment results obtained in city road corridors demonstrated that
   point cloud data acquired from the airborne platform achieved even
   slightly better results in terms of tree detection rate, tree species
   and vitality classification accuracy, although the tree vitality
   distribution in the test site is less balanced compared to the species
   distribution. When combined with MLS data, overall accuracies of 78\%
   and 74\% for tree species and vitality classification can be achieved,
   which has improved by 5.7\% and 4.64\% respectively compared to the
   usage of airborne data only.}},
DOI = {{10.3390/rs10091403}},
Article-Number = {{1403}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Yao, Wei/E-8520-2017}},
ORCID-Numbers = {{Yao, Wei/0000-0001-7704-0615}},
Unique-ID = {{ISI:000449993800083}},
}

@article{ ISI:000445204800002,
Author = {Wang, Jinhu and Lindenbergh, Roderik and Menenti, Massimo},
Title = {{Scalable individual tree delineation in 3D point clouds}},
Journal = {{PHOTOGRAMMETRIC RECORD}},
Year = {{2018}},
Volume = {{33}},
Number = {{163}},
Pages = {{315-340}},
Month = {{SEP}},
Abstract = {{Manually monitoring and documenting trees is labour intensive. Lidar
   provides a possible solution for automatic tree-inventory generation.
   Existing approaches for segmenting trees from original point cloud data
   lack scalable and efficient methods that separate individual trees
   sampled by different laser-scanning systems with sufficient quality
   under all circumstances. In this study a new algorithm for efficient
   individual tree delineation from lidar point clouds is presented and
   validated. The proposed algorithm first resamples the points using
   cuboid (modified voxel) cells. Consecutively connected cells are
   accumulated by vertically traversing cell layers. Trees in close
   proximity are identified, based on a novel cell-adjacency analysis. The
   scalable performance of this algorithm is validated on airborne, mobile
   and terrestrial laser-scanning point clouds. Validation against ground
   truth demonstrates an improvement from 89\% to 94\% relative to a
   state-of-the-art method while computation time is similar.
   Resume La detection et la documentation manuelle des arbres est une
   tache fastidieuse. Le lidar offre une solution possible pour
   l'inventaire automatique des arbres. Les approches existantes pour la
   segmentation des arbres dans des nuages bruts de points ne proposent pas
   de methodes efficaces et adaptees a toutes les echelles pour separer des
   arbres individuels echantillonnes par differents systemes lidar avec une
   qualite acceptable en toute circonstance. Cette etude propose et valide
   un nouvel algorithme pour la delimitation efficace d'arbres individuels
   a partir de nuages de points lidar. L'algorithme propose commence par
   reechantillonner les points dans des cellules cubiques (voxels), puis
   regroupe les cellules connexes en traversant verticalement les couches
   de cellules. Les arbres proches sont identifies grace a une nouvelle
   analyse d'adjacence de cellules. La performance de cetalgorithme en
   termes d'adaptabilite au changement d'echelle est validee a partir de
   nuages de points issus de systemes laser a balayage aerien, mobile et
   terrestre. Une validation basee sur des donnees de terrain de reference
   fait etat d'une amelioration de 89\% a 94\% par rapport a des methodes
   connues pour un temps de calcul comparable.
   Zusammenfassung Eine uberwachung und Dokumentation von Baumen ist sehr
   arbeitsaufwandig. Lidar bietet das Potential fur automatische
   Bauminventur. Es gibt Ansatze zur Segmentierung von Baumen aus
   Punktwolken, die allerdings noch nicht in der Lage sind, einzelne Baume
   in Punktwolken verschiedener Lidar-Systeme zuverlassig und mit
   ausreichender Qualitat unter vielfaltigen realen Bedingungen zu
   separieren. Diese Studie stellt einen neuen Algorithmus zur effizienten
   Erfassung von Baumen in Lidar-Punktwolken dar. Der Algorithmus bildet
   Punkte mit Hilfe von quaderformigen (Voxel) Zellen um. Nacheinander
   verbundene Zellen werden durch vertikale Traverse der Zellschichten
   akkumuliert. Baume in nachster Nachbarschaft werden durch eine neuartige
   Zellanalyse identifiziert. Der Vorteil der Skalierbarkeit des
   Algorithmus wird an flugzeuggestutzten, mobilen und terrestrischen
   Laserscanpunktwolken validiert. Anhand von Solldaten ist festzustellen,
   dass bei gleicher Rechenzeit, eine Verbesserung von 89\% bis 94\% im
   Vergleich zu aktuellen Verfahren erzielt werden kann.
   Resumen Monitorizar y documentar manualmente arboles es un trabajo
   intensivo. El lidar proporciona una posible solucion para la generacion
   automatica del inventario de arboles. Los enfoques existentes para
   segmentar arboles a partir originalmente de nubes de puntos lidar
   carecen de metodos escalables y eficientes que separen arboles
   individuales muestreados por diferentes sistemas lidar con calidad
   suficiente bajo todas las circunstancias. En este estudio, se presenta y
   valida un algoritmo nuevo para la delimitacion eficiente de arboles
   individuales a partir de nubes de puntos lidar. El algoritmo propuesto
   primero remuestrea los puntos usando celulas cuboides (voxels). Los
   voxels adyacentes se acumulan atravesando verticalmente las capas de
   voxels. Basados en un nuevo analisis de adyacencia de voxels se
   identifican arboles que estan proximos. El rendimiento escalable de este
   algoritmo se valida con nubes de puntos lidar aerotransportados, moviles
   y terrestres. La validacion con verdad terreno demuestra una mejora del
   89\% al 94\% en comparacion con un metodo de vanguardia, mientras que el
   tiempo de calculo es similar.}},
DOI = {{10.1111/phor.12247}},
ISSN = {{0031-868X}},
EISSN = {{1477-9730}},
ORCID-Numbers = {{Lindenbergh, Roderik/0000-0001-8655-5266}},
Unique-ID = {{ISI:000445204800002}},
}

@article{ ISI:000442238900004,
Author = {Switonski, Adam and Krzeszowski, Tomasz and Josinski, Henryk and Kwolek,
   Bogdan and Wojciechowski, Konrad},
Title = {{Gait recognition on the basis of markerless motion tracking and DTW
   transform}},
Journal = {{IET BIOMETRICS}},
Year = {{2018}},
Volume = {{7}},
Number = {{5}},
Pages = {{415-422}},
Month = {{SEP}},
Abstract = {{In this study, a framework for view-invariant gait recognition on the
   basis of markerless motion tracking and dynamic time warping (DTW)
   transform is presented. The system consists of a proposed markerless
   motion capture system as well as introduced classification method of
   mocap data. The markerless system estimates the three-dimensional
   locations of skeleton driven joints. Such skeleton-driven point clouds
   represent poses over time. The authors align point clouds in every pair
   of frames by calculating the minimal sum of squared distances between
   the corresponding joints. A point cloud distance measure with temporal
   context has been utilised in k-nearest neighbours algorithm to compare
   time instants of motion sequences. To enhance the generalisation of the
   recognition and to shorten the processing time, for every individual a
   single multidimensional time series among several multidimensional time
   series describing the individual's gait is established. The correct
   classification rate has been determined on the basis of a real dataset
   of human gait. It contains 230 gait cycles of 22 subjects. The tracking
   results on the basis of markerless motion capture are referenced to
   Vicon system, whereas the achieved accuracies of recognition are
   compared with the ones obtained by DTW that is based on rotational data.}},
DOI = {{10.1049/iet-bmt.2017.0134}},
ISSN = {{2047-4938}},
EISSN = {{2047-4946}},
ResearcherID-Numbers = {{Krzeszowski, Tomasz/H-7717-2019}},
ORCID-Numbers = {{Krzeszowski, Tomasz/0000-0001-7359-4637}},
Unique-ID = {{ISI:000442238900004}},
}

@article{ ISI:000436350700038,
Author = {Dou, Pengfei and Wu, Yuhang and Shah, Shishir K. and Kakadiaris, Ioannis
   A.},
Title = {{Monocular 3D facial shape reconstruction from a single 2D image with
   coupled-dictionary learning and sparse coding}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2018}},
Volume = {{81}},
Pages = {{515-527}},
Month = {{SEP}},
Abstract = {{Monocular 3D face reconstruction from a single image has been an active
   research topic due to its wide applications. It has been demonstrated
   that the 3D face can be reconstructed efficiently using a PCA-based
   subspace model for facial shape representation and facial landmarks for
   model parameter estimation. However, due to the limited expressiveness
   of the subspace model and the inaccuracy of landmark detection, most
   existing methods are not robust to pose and illumination variation. To
   overcome this limitation, this work proposes a coupled-dictionary model
   for parametric facial shape representation and a two-stage framework for
   3D face reconstruction from a single 2D image by using facial landmarks.
   Motivated by image super-resolution, the proposed coupled-model consists
   of two dictionaries for the sparse and the dense 3D facial shapes,
   respectively. In the first stage, the sparse 3D face is estimated from
   facial landmarks by using partial least-squares regression. In the
   second stage, the dense 3D face is reconstructed by 3D super-resolution
   on the estimated sparse 3D face. Comprehensive experimental evaluations
   demonstrate that the proposed coupled-dictionary model outperforms the
   PCA-based subspace model in 3D face modeling accuracy and that the
   proposed framework achieves much lower reconstruction error on facial
   images with pose and illumination variations compared to
   state-of-the-art algorithms. Moreover, qualitative analysis demonstrates
   that the proposed method is generalizable to different types of data,
   including facial images, portraits, and facial sketches. (C) 2018
   Published by Elsevier Ltd.}},
DOI = {{10.1016/j.patcog.2018.03.002}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000436350700038}},
}

@article{ ISI:000446151100037,
Author = {Abbad, Abdelghafour and Abbad, Khalid and Tairi, Hamid},
Title = {{3D face recognition: Multi-scale strategy based on geometric and local
   descriptors}},
Journal = {{COMPUTERS \& ELECTRICAL ENGINEERING}},
Year = {{2018}},
Volume = {{70}},
Pages = {{525-537}},
Month = {{AUG}},
Abstract = {{Most human expression variations cause a non-rigid deformation of face
   scans, which is a challenge today. In this article, we present a novel
   framework for 3D face recognition that uses a geometry and local shape
   descriptor in a matching process to overcome the distortions caused by
   expressions in faces. This algorithm consists of four major components.
   First, the 3D face model is presented at different scales. Second,
   isometric-invariant features on each scale are extracted. Third, the
   geometric information is obtained on the 3D surface in terms of radial
   and level facial curves. Fourth, the feature vectors on each scale are
   concatenated with their corresponding geometric information. We
   conducted a number of experiments using two well-known and challenging
   datasets, namely, the GavabDB and Bosphorus datasets, and superior
   recognition performance has been achieved. The new system displays an
   overall rank-1 identification rate of 98.9\% for all faces with neutral
   and non-neutral expressions on the GavabDB database. (C) 2017 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.compeleceng.2017.08.017}},
ISSN = {{0045-7906}},
EISSN = {{1879-0755}},
ORCID-Numbers = {{khalid, abbad/0000-0002-4929-1967}},
Unique-ID = {{ISI:000446151100037}},
}

@article{ ISI:000441233300018,
Author = {Chouchane, Ammar and Ouamane, Abdelmalik and Boutellaa, Elhocine and
   Belahcene, Mebarka and Bourennane, Salah},
Title = {{3D face verification across pose based on euler rotation and tensors}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2018}},
Volume = {{77}},
Number = {{16}},
Pages = {{20697-20714}},
Month = {{AUG}},
Abstract = {{In this paper, we propose a new approach for 3D face verification based
   on tensor representation. Face challenges, such as illumination,
   expression and pose, are modeled as a multilinear algebra problem where
   facial images are represented as high order tensors. Particularly, to
   account for head pose variations, several pose scans are generated from
   a single depth image using Euler transformation. Multi-bloc local phase
   quantization (MBLPQ) histogram features are extracted from depth face
   images and arranged as a third order tensor. The dimensionality of the
   tensor is reduced based on the higher-order singular value decomposition
   (HOSVD). HOSVD projects the input tensor in a new subspace in which the
   dimension of each tensor mode is reduced. To discriminate faces of
   different persons, we utilize the Enhanced Fisher Model (EFM).
   Experimental evaluations on CASIA-3D database, which contains large head
   pose variations, demonstrate the effectiveness of the proposed approach.
   A verification rate of 98.60\% is obtained.}},
DOI = {{10.1007/s11042-017-5478-z}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000441233300018}},
}

@article{ ISI:000435048200005,
Author = {Reiser, David and Vazquez-Arellano, Manuel and Paraforos, Dimitris S.
   and Garrido-Izard, Miguel and Griepentrog, Hans W.},
Title = {{Iterative individual plant clustering in maize with assembled 2D LiDAR
   data}},
Journal = {{COMPUTERS IN INDUSTRY}},
Year = {{2018}},
Volume = {{99}},
Pages = {{42-52}},
Month = {{AUG}},
Abstract = {{A two dimensional (2D) laser scanner was mounted at the front part of a
   small 4-wheel autonomous robot with differential steering, at an angle
   of 30 degrees pointing downwards. The machine was able to drive between
   maize rows and collect concurrent time-stamped data. A robotic total
   station tracked the position of a prism mounted on the vehicle. The
   total station and laser scanner data were fused to generate a three
   dimensional (3D) point cloud. This 3D representation was used to detect
   individual plant positions, which are of particular interest for
   applications such as phenotyping, individual plant treatment and
   precision weeding. Two different methodologies were applied to the 3D
   point cloud to estimate the position of the individual plants. The first
   methodology used the Euclidian Clustering on the entire point cloud. The
   second methodology utilised the position of an initial plant and the
   fixed plant spacing to search iteratively for the best clusters. The two
   algorithms were applied at three different plant growth stages. For the
   first method, results indicated a detection rate up to 73.7\% with a
   root mean square error of 3.6 cm. The second method was able to detect
   all plants (100\% detection rate) with an accuracy of 2.7-3.0 cm, taking
   the plant spacing of 13 cm into account.}},
DOI = {{10.1016/j.compind.2018.03.023}},
ISSN = {{0166-3615}},
EISSN = {{1872-6194}},
ORCID-Numbers = {{Paraforos, Dimitrios S./0000-0001-8275-8840
   Reiser, David/0000-0003-0158-6456}},
Unique-ID = {{ISI:000435048200005}},
}

@article{ ISI:000439601100001,
Author = {Mohammadi, Shahram and Gervei, Omid},
Title = {{Using nonlocal filtering and feature extraction approaches in
   three-dimensional face recognition by Kinect}},
Journal = {{INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS}},
Year = {{2018}},
Volume = {{15}},
Number = {{4}},
Month = {{JUL 13}},
Abstract = {{To use low-cost depth sensors such as Kinect for three-dimensional face
   recognition with an acceptable rate of recognition, the challenges of
   filling up nonmeasured pixels and smoothing of noisy data need to be
   addressed. The main goal of this article is presenting solutions for
   aforementioned challenges as well as offering feature extraction methods
   to reach the highest level of accuracy in the presence of different
   facial expressions and occlusions. To use this method, a domestic
   database was created. First, the noisy pixels-called holes-of depth
   image is removed by solving multiple linear equations resulted from the
   values of the surrounding pixels of the holes. Then, bilateral and block
   matching 3-D filtering approaches, as representatives of local and
   nonlocal filtering approaches, are used for depth image smoothing.
   Curvelet transform as a well-known nonlocal feature extraction technique
   applied on both RGB and depth images. Two unsupervised dimension
   reduction techniques, namely, principal component analysis and
   independent component analysis, are used to reduce the dimension of
   extracted features. Finally, support vector machine is used for
   classification. Experimental results show a recognition rate of 90\% for
   just depth images and 100\% when combining RGB and depth data of a
   Kinect sensor which is much higher than other recently proposed
   algorithms.}},
DOI = {{10.1177/1729881418787743;1729881418787743}},
ISSN = {{1729-8814}},
Unique-ID = {{ISI:000439601100001}},
}

@article{ ISI:000438224000004,
Author = {Sharma, Sahil and Kumar, Vijay},
Title = {{Performance evaluation of 2D face recognition techniques under image
   processing attacks}},
Journal = {{MODERN PHYSICS LETTERS B}},
Year = {{2018}},
Volume = {{32}},
Number = {{19}},
Month = {{JUL 10}},
Abstract = {{Face recognition is a vastly researched topic in the field of computer
   vision. A lot of work have been done for facial recognition in two
   dimensions and three dimensions. The amount of work done with face
   recognition invariant of image processing attacks is very limited. This
   paper presents a total of three classes of image processing attacks on
   face recognition system, namely image enhancement attacks, geometric
   attacks and the image noise attacks. The well-known machine learning
   techniques have been used to train and test the face recognition system
   using two different databases namely Bosphorus Database and University
   of Milano Bicocca three-dimensional (3D) Face Database (UMBDB). Three
   classes of classification models, namely discriminant analysis, support
   vector machine and k-nearest neighbor along with ensemble techniques
   have been implemented. The significance of machine learning techniques
   has been mentioned. The visual verification has been done with multiple
   image processing attacks.}},
DOI = {{10.1142/S0217984918502123}},
Article-Number = {{1850212}},
ISSN = {{0217-9849}},
EISSN = {{1793-6640}},
ORCID-Numbers = {{Sharma, Sahil/0000-0002-6694-3365}},
Unique-ID = {{ISI:000438224000004}},
}

@article{ ISI:000441334300102,
Author = {Li, Jing and Qiu, Tao and Wen, Chang and Xie, Kai and Wen, Fang-Qing},
Title = {{Robust Face Recognition Using the Deep C2D-CNN Model Based on
   Decision-Level Fusion}},
Journal = {{SENSORS}},
Year = {{2018}},
Volume = {{18}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{Given that facial features contain a wide range of identification
   information and cannot be completely represented by a single feature,
   the fusion of multiple features is particularly significant for
   achieving a robust face recognition performance, especially when there
   is a big difference between the test sets and the training sets. This
   has been proven in both traditional and deep learning approaches. In
   this work, we proposed a novel method named C2D-CNN (color 2-dimensional
   principal component analysis (2DPCA)-convolutional neural network).
   C2D-CNN combines the features learnt from the original pixels with the
   image representation learnt by CNN, and then makes decision-level
   fusion, which can significantly improve the performance of face
   recognition. Furthermore, a new CNN model is proposed: firstly, we
   introduce a normalization layer in CNN to speed up the network
   convergence and shorten the training time. Secondly, the layered
   activation function is introduced to make the activation function
   adaptive to the normalized data. Finally, probabilistic max-pooling is
   applied so that the feature information is preserved to the maximum
   extent while maintaining feature invariance. Experimental results show
   that compared with the state-of-the-art method, our method shows better
   performance and solves low recognition accuracy caused by the difference
   between test and training datasets.}},
DOI = {{10.3390/s18072080}},
Article-Number = {{2080}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000441334300102}},
}

@article{ ISI:000440385500001,
Author = {Hamdan, Bensenane and Mokhtar, Keche},
Title = {{The detection of spoofing by 3D mask in a 2D identity recognition system}},
Journal = {{EGYPTIAN INFORMATICS JOURNAL}},
Year = {{2018}},
Volume = {{19}},
Number = {{2}},
Pages = {{75-82}},
Month = {{JUL}},
Abstract = {{Nowadays face recognition systems are facing a new problem after having
   won the challenge of reliability. The problem is that these systems have
   become vulnerable to attacks by identity theft. In order to deceive the
   recognition systems hackers use several methods, such as the use of face
   images or videos of people belonging to the system database. Luckily,
   this type of attack is thwarted by the use of adapted systems. But
   unfortunately another type of attack that uses 3D face masks appeared.
   This type of attack is very efficient, since as will be shown, a high
   percentage of hackers who use 3D masks can mislead a good facial
   recognition system, like the one used in our investigation. In this
   paper, a new method is proposed for the detection of hackers that use 3D
   masks to deceive face recognition systems. This method uses the Angular
   Radial Transformation (ART) to extract pertinent features that are fed
   into a classifier to decide whether the captured image represents a face
   image. The performance of the proposed method was evaluated using a
   public 3D Mask Attack Database (3DMAD). The obtained results show the
   efficiency of the proposed method, since it can reduce the error rate in
   discriminating between a real face and a face mask down to 0.90\%. (C)
   2017 Production and hosting by Elsevier B.V. on behalf of Faculty of
   Computers and Information, Cairo University. This is an open access
   article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).}},
DOI = {{10.1016/j.eij.2017.10.001}},
ISSN = {{1110-8665}},
EISSN = {{2090-4754}},
Unique-ID = {{ISI:000440385500001}},
}

@article{ ISI:000440122900009,
Author = {Huang, Delin and Du, Shichang and Li, Guilong and Zhao, Chen and Deng,
   Yafei},
Title = {{Detection and monitoring of defects on three-dimensional curved surfaces
   based on high-density point cloud data}},
Journal = {{PRECISION ENGINEERING-JOURNAL OF THE INTERNATIONAL SOCIETIES FOR
   PRECISION ENGINEERING AND NANOTECHNOLOGY}},
Year = {{2018}},
Volume = {{53}},
Pages = {{79-95}},
Month = {{JUL}},
Abstract = {{The surface quality of three-dimensional (3-D) curved surfaces is one of
   the most important factors that can directly influence the performance
   of the final product. This paper presents a systematic approach for
   detection and monitoring of defects on 3-D curved surfaces based on
   high-density point cloud data. Firstly, an algorithm to remove outliers
   and a boundary recognition algorithm are proposed to divide the entire
   3-D curved surface including millions of measured points into multiple
   sub-regions. Secondly, two new evaluation indexes based on wavelet
   packet entropy and normal vector are explored to represent the features
   of the multiple sub-regions to determine whether the sub-regions are
   out-of-limit (OOL) of specifications. Thirdly, three quality parameters
   representing quality characteristics of a curved surface are presented
   and their values are calculated based on the clusters of OOL
   sub-regions. Finally, three individual control charts are presented to
   monitor the three quality parameters. As long as any quality parameter
   is out of the control range, the manufacturing process of the curved
   surface is determined to be out-of-control (OOC). The results of a case
   study show that the proposed approach can effectively identify the OOC
   manufacturing process and detect defects on 3-D curved surfaces.}},
DOI = {{10.1016/j.precisioneng.2018.03.001}},
ISSN = {{0141-6359}},
EISSN = {{1873-2372}},
Unique-ID = {{ISI:000440122900009}},
}

@article{ ISI:000438934400005,
Author = {Dagnes, Nicole and Vezzetti, Enrico and Marcolin, Federica and
   Tornincasa, Stefano},
Title = {{Occlusion detection and restoration techniques for 3D face recognition:
   a literature review}},
Journal = {{MACHINE VISION AND APPLICATIONS}},
Year = {{2018}},
Volume = {{29}},
Number = {{5}},
Pages = {{789-813}},
Month = {{JUL}},
Abstract = {{Methodologies for 3D face recognition which work in the presence of
   occlusions are core for the current needs in the field of identification
   of suspects, as criminals try to take advantage of the weaknesses among
   the implemented security systems by camouflaging themselves and
   occluding their face with eyeglasses, hair, hands, or covering their
   face with scarves and hats. Recent occlusion detection and restoration
   strategies for recognition purposes of 3D partially occluded faces with
   unforeseen objects are here presented in a literature review. The
   research community has worked on face recognition systems under
   controlled environments, but uncontrolled conditions have been
   investigated in a lesser extent. The paper details the experiments and
   databases used to handle the problem of occlusion and the results
   obtained by different authors. Lastly, a comparison of various
   techniques is presented and some conclusions are drawn referring to the
   best outcomes.}},
DOI = {{10.1007/s00138-018-0933-z}},
ISSN = {{0932-8092}},
EISSN = {{1432-1769}},
ORCID-Numbers = {{Marcolin, Federica/0000-0002-4360-6905}},
Unique-ID = {{ISI:000438934400005}},
}

@article{ ISI:000438827800001,
Author = {Nazim, Khalid S. A. and Harsha, S. and Bhaskar, N. and Al Quhayz, Hani},
Title = {{Facial Image Reconstruction From A Single Frontal Image Using Intensity
   Histograms and 3-D Mapping}},
Journal = {{INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND NETWORK SECURITY}},
Year = {{2018}},
Volume = {{18}},
Number = {{6}},
Pages = {{1-7}},
Month = {{JUN 30}},
Abstract = {{Face Recognition {[}Pentland et al 1991] confronts innumerable hurdles
   in the form of variations in lighting conditions during image capture,
   Occlusions, damage in facial portions due to accidents etc. Hence
   recovery of the complete picture of a human face from partially occluded
   images is quite a challenge in Image Processing. Facial imaging proposed
   in this paper is the method of generating 3-D face mask of a subject.
   This process involves receiving one frontal image of the subject from a
   digital imaging device (Cameras in Smartphone, tablet or PC) and
   applying algorithms for edge recovery of the face. Once the face edges
   are detected, the light intensity incident on the face is mapped as a 3D
   histogram. The intensity gradients formed due to the contour {[}Kass et
   al 1998] of the face are then mapped on to the face as depth points in a
   grid-based space. Finally, the representation determined from the
   database is used to generate 3D facial data of the subject based on the
   best mapping.}},
ISSN = {{1738-7906}},
Unique-ID = {{ISI:000438827800001}},
}

@article{ ISI:000428490900009,
Author = {Gao, Jiangning and Evans, Adrian N.},
Title = {{Expression robust 3D face landmarking using thresholded surface normals}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2018}},
Volume = {{78}},
Pages = {{120-132}},
Month = {{JUN}},
Abstract = {{3D face recognition is an increasing popular modality for biometric
   authentication, for example in the iPhoneX. Landmarking plays a
   significant role in region based face recognition algorithms. The
   accuracy and consistency of the landmarking will directly determine the
   effectiveness of feature extraction and hence the overall recognition
   performance. While surface normals have been shown to provide high
   performing features for face recognition, their use in landmarking has
   not been widely explored. To this end, a new 3D facial landmarking
   algorithm based on thresholded surface normal maps is proposed, which is
   applicable to widely used 3D face databases. The benefits of employing
   surface normals are demonstrated for both facial roll and yaw rotation
   calibration and nasal landmarks localization. Results on the Bosphorus,
   FRGC and BU-3DFE databases show that the detected landmarks possess high
   within class consistency and accuracy under different expressions. For
   several key landmarks the performance achieved surpasses that of
   state-of-the-art techniques and is also training free and
   computationally efficient. The use of surface normals therefore provides
   a useful representation of the 3D surface and the proposed landmarking
   algorithm provides an effective approach to localising the key nasal
   landmarks. (C) 2018 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2018.01.011}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Gao, Jiangning/0000-0001-5255-8135
   Evans, Adrian/0000-0001-8586-8295}},
Unique-ID = {{ISI:000428490900009}},
}

@article{ ISI:000436283000050,
Author = {Kendrick, Connah and Tan, Kevin and Walker, Kevin and Yap, Moi Hoon},
Title = {{Towards Real-Time Facial Landmark Detection in Depth Data Using
   Auxiliary Information}},
Journal = {{SYMMETRY-BASEL}},
Year = {{2018}},
Volume = {{10}},
Number = {{6}},
Month = {{JUN}},
Abstract = {{Modern facial motion capture systems employ a two-pronged approach for
   capturing and rendering facial motion. Visual data (2D) is used for
   tracking the facial features and predicting facial expression, whereas
   Depth (3D) data is used to build a series of expressions on 3D face
   models. An issue with modern research approaches is the use of a single
   data stream that provides little indication of the 3D facial structure.
   We compare and analyse the performance of Convolutional Neural Networks
   (CNN) using visual, Depth and merged data to identify facial features in
   real-time using a Depth sensor. First, we review the facial landmarking
   algorithms and its datasets for Depth data. We address the limitation of
   the current datasets by introducing the Kinect One Expression Dataset
   (KOED). Then, we propose the use of CNNs for the single data stream and
   merged data streams for facial landmark detection. We contribute to
   existing work by performing a full evaluation on which streams are the
   most effective for the field of facial landmarking. Furthermore, we
   improve upon the existing work by extending neural networks to predict
   into 3D landmarks in real-time with additional observations on the
   impact of using 2D landmarks as auxiliary information. We evaluate the
   performance by using Mean Square Error (MSE) and Mean Average Error
   (MAE). We observe that the single data stream predicts accurate facial
   landmarks on Depth data when auxiliary information is used to train the
   network. The codes and dataset used in this paper will be made
   available.}},
DOI = {{10.3390/sym10060230}},
Article-Number = {{230}},
ISSN = {{2073-8994}},
ORCID-Numbers = {{Yap, Moi Hoon/0000-0001-7681-4287
   kendrick, connah/0000-0002-3623-6598}},
Unique-ID = {{ISI:000436283000050}},
}

@article{ ISI:000434382900048,
Author = {Vezzetti, Enrico and Marcolin, Federica and Tornincasa, Stefano and
   Ulrich, Luca and Dagnes, Nicole},
Title = {{3D geometry-based automatic landmark localization in presence of facial
   occlusions}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2018}},
Volume = {{77}},
Number = {{11}},
Pages = {{14177-14205}},
Month = {{JUN}},
Abstract = {{This study proposes a novel automatic method for facial landmark
   localization relying on geometrical properties of 3D facial surface
   working both on complete faces displaying different emotions and in
   presence of occlusions. In particular, 12 descriptors coming from
   Differential Geometry including the coefficients of the fundamental
   forms, Gaussian, mean, principal curvatures, shape index and curvedness
   are extracted as facial features and their local geometric properties
   are exploited to localize 13 soft-tissue landmarks from eye and nose
   areas. The method is deterministic and is backboned by a thresholding
   technique designed by studying the behaviour of each geometrical
   descriptor in correspondence to the locus of each landmark. Occlusions
   are managed by a detection algorithm based on geometrical properties
   which allows to proceed with the landmark localization avoiding the
   covered areas. Experimentations were carried out on 3132 faces of the
   Bosphorus database and of a 230-sized internal database, including
   expressive and occluded ones (mouth, eye, and eyeglasses occlusions),
   obtaining 4.75 mm mean localization error.}},
DOI = {{10.1007/s11042-017-5025-y}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
ORCID-Numbers = {{Ulrich, Luca/0000-0001-8407-0660
   Dagnes, Nicole/0000-0003-4690-7567}},
Unique-ID = {{ISI:000434382900048}},
}

@article{ ISI:000431281900081,
Author = {Miranda, Geraldo Elias and Wilkinson, Caroline and Roughley, Mark and
   Beaini, Thiago Leite and Haltenhoff Melani, Rodolfo Francisco},
Title = {{Assessment of accuracy and recognition of three-dimensional computerized
   forensic craniofacial reconstruction}},
Journal = {{PLOS ONE}},
Year = {{2018}},
Volume = {{13}},
Number = {{5}},
Month = {{MAY 2}},
Abstract = {{Facial reconstruction is a technique that aims to reproduce the
   individual facial characteristics based on interpretation of the skull,
   with the objective of recognition leading to identification. The aim of
   this paper was to evaluate the accuracy and recognition level of
   three-dimensional (3D) computerized forensic craniofacial reconstruction
   (CCFR) performed in a blind test on open-source software using computed
   tomography (CT) data from live subjects. Four CCFRs were produced by one
   of the researchers, who was provided with information concerning the
   age, sex, and ethnic group of each subject. The CCFRs were produced
   using Blender (R) with 3D models obtained from the CT data and templates
   from the MakeHuman (R) program. The evaluation of accuracy was carried
   out in CloudCompare, by geometric comparison of the CCFR to the subject
   3D face model (obtained from the CT data). A recognition level was
   performed using the Picasa (R) recognition tool with a frontal
   standardized photography, images of the subject CT face model and the
   CCFR. Soft-tissue depth and nose, ears and mouth were based on published
   data, observing Brazilian facial parameters. The results were presented
   from all the points that form the CCFR model, with an average for each
   comparison between 63\% and 74\% with a distance -2.5 <= x <= 2.5 mm
   from the skin surface. The average distances were 1.66 to 0.33 mm and
   greater distances were observed around the eyes, cheeks, mental and
   zygomatic regions. Two of the four CCFRs were correctly matched by the
   Picasa (R) tool. Free software programs are capable of producing 3D
   CCFRs with plausible levels of accuracy and recognition and therefore
   indicate their value for use in forensic applications.}},
DOI = {{10.1371/journal.pone.0196770}},
Article-Number = {{e0196770}},
ISSN = {{1932-6203}},
ORCID-Numbers = {{MIRANDA, GERALDO/0000-0003-1240-3256
   Roughley, Mark/0000-0002-7483-859X}},
Unique-ID = {{ISI:000431281900081}},
}

@article{ ISI:000434085600046,
Author = {Zollhoefer, M. and Thies, J. and Garrido, P. and Bradley, D. and Beeler,
   T. and Perez, P. and Stamminger, M. and Niessner, M. and Theobalt, C.},
Title = {{State of the Art on Monocular 3D Face Reconstruction, Tracking, and
   Applications}},
Journal = {{COMPUTER GRAPHICS FORUM}},
Year = {{2018}},
Volume = {{37}},
Number = {{2}},
Pages = {{523-550}},
Month = {{MAY}},
Note = {{39th Annual Conference of the European-Association-for-Computer-Graphics
   (EUROGRAPHICS), Delft, NETHERLANDS, APR 16-20, 2018}},
Organization = {{European Assoc Comp Graph; TU Delft; Tomtom; Adobe; Disney Res;
   StyleShoots; Activision; KAUST; Vrvis}},
Abstract = {{The computer graphics and vision communities have dedicated long
   standing efforts in building computerized tools for reconstructing,
   tracking, and analyzing human faces based on visual input. Over the past
   years rapid progress has been made, which led to novel and powerful
   algorithms that obtain impressive results even in the very challenging
   case of reconstruction from a single RGB or RGB-D camera. The range of
   applications is vast and steadily growing as these technologies are
   further improving in speed, accuracy, and ease of use. Motivated by this
   rapid progress, this state-of-the-art report summarizes recent trends in
   monocular facial performance capture and discusses its applications,
   which range from performance-based animation to real-time facial
   reenactment. We focus our discussion on methods where the central task
   is to recover and track a three dimensional model of the human face
   using optimization-based reconstruction algorithms. We provide an
   in-depth overview of the underlying concepts of real-world image
   formation, and we discuss common assumptions and simplifications that
   make these algorithms practical. In addition, we extensively cover the
   priors that are used to better constrain the under-constrained monocular
   reconstruction problem, and discuss the optimization techniques that are
   employed to recover dense, photo-geometric 3D face models from monocular
   2D data. Finally, we discuss a variety of use cases for the reviewed
   algorithms in the context of motion capture, facial animation, as well
   as image and video editing.}},
DOI = {{10.1111/cgf.13382}},
ISSN = {{0167-7055}},
EISSN = {{1467-8659}},
Unique-ID = {{ISI:000434085600046}},
}

@article{ ISI:000435193700027,
Author = {Wang, Di and Brunner, Jasmin and Ma, Zhenyu and Lu, Hao and Hollaus,
   Markus and Pang, Yong and Pfeifer, Norbert},
Title = {{Separating Tree Photosynthetic and Non-Photosynthetic Components from
   Point Cloud Data Using Dynamic Segment Merging}},
Journal = {{FORESTS}},
Year = {{2018}},
Volume = {{9}},
Number = {{5}},
Month = {{MAY}},
Abstract = {{Many biophysical forest properties such as wood volume and leaf area
   index (LAI) require prior knowledge on either photosynthetic or
   non-photosynthetic components. Laser scanning appears to be a helpful
   technique in nondestructively quantifying forest structures, as it can
   acquire an accurate three-dimensional point cloud of objects. In this
   study, we propose an unsupervised geometry-based method named Dynamic
   Segment Merging (DSM) to identify non-photosynthetic components of trees
   by semantically segmenting tree point clouds, and examining the linear
   shape prior of each resulting segment. We tested our method using one
   single tree dataset and four plot-level datasets, and compared our
   results to a supervised machine learning method. We further demonstrated
   that by using an optimal neighborhood selection method that involves
   multi-scale analysis, the results were improved. Our results showed that
   the overall accuracy ranged from 81.8\% to 92.0\% with an average value
   of 87.7\%. The supervised machine learning method had an average overall
   accuracy of 86.4\% for all datasets, on account of a collection of
   manually delineated representative training data. Our study indicates
   that separating tree photosynthetic and non-photosynthetic components
   from laser scanning data can be achieved in a fully unsupervised manner
   without the need of training data and user intervention.}},
DOI = {{10.3390/f9050252}},
Article-Number = {{252}},
ISSN = {{1999-4907}},
ResearcherID-Numbers = {{Wang, Di/T-2571-2018
   }},
ORCID-Numbers = {{Wang, Di/0000-0003-0232-8862
   Pang, Yong/0000-0002-9760-6580
   Pfeifer, Norbert/0000-0002-2348-7929}},
Unique-ID = {{ISI:000435193700027}},
}

@article{ ISI:000426222800032,
Author = {Deng, Weihong and Hu, Jiani and Wu, Zhongjun and Guo, Jun},
Title = {{From one to many: Pose-Aware Metric Learning for single-sample face
   recognition}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2018}},
Volume = {{77}},
Pages = {{426-437}},
Month = {{MAY}},
Abstract = {{Pose and illumination variations are very challenging for face
   recognition with a single sample per person (SSPP). In this paper, we
   address this issue by a Pose-Aware Metric Learning (PAML) approach. Our
   primary idea is ``from one to many{''}: Synthesizing many images of
   sufficient pose and illumination variability from the single training
   image, based on which metric learning approach is applied to reduce
   these ``synthesized{''} variations at each quantified pose. For this
   purpose, given a single frontal training image, a multi-depth generic
   elastic model and an extended generic elastic model are developed to
   synthesize facial images of the target pose with varying 3D shape
   (depth) and illumination variations respectively. To reduce these
   ``synthesized{''} variability, Pose-Aware Metric spaces are separately
   learnt by linear regression analysis at each quantized pose, and
   pose-invariant recognition is performed in the corresponding metric
   space. By preserving the detailed texture and reducing the shape
   variability, the PAML method achieves an 100\% accuracy on the Multi-PIE
   database under the test setting across poses, which is significantly
   better than the traditional methods that use a large generic image
   ensemble to learn the cross-pose transformations. On the more
   challenging setting across both poses and illuminations, PAML
   outperforms the recent deep learning approaches by over 10\% accuracy.
   (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2017.10.020}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000426222800032}},
}

@article{ ISI:000433517100002,
Author = {Berretti, Stefano and Daoudi, Mohamed and Turaga, Pavan and Basu, Anup},
Title = {{Representation, Analysis, and Recognition of 3D Humans: A Survey}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2018}},
Volume = {{14}},
Number = {{1, S}},
Month = {{APR}},
Abstract = {{Computer Vision and Multimedia solutions are now offering an increasing
   number of applications ready for use by end users in everyday life. Many
   of these applications are centered for detection, representation, and
   analysis of face and body. Methods based on 2D images and videos are the
   most widespread, but there is a recent trend that successfully extends
   the study to 3D human data as acquired by a new generation of 3D
   acquisition devices. Based on these premises, in this survey, we provide
   an overview on the newly designed techniques that exploit 3D human data
   and also prospect the most promising current and future research
   directions. In particular, we first propose a taxonomy of the
   representation methods, distinguishing between spatial and temporal
   modeling of the data. Then, we focus on the analysis and recognition of
   3D humans from 3D static and dynamic data, considering many applications
   for body and face.}},
DOI = {{10.1145/3182179}},
Article-Number = {{16}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
Unique-ID = {{ISI:000433517100002}},
}

@article{ ISI:000433517100005,
Author = {Azzakhnini, Safaa and Ballihi, Lahoucine and Aboutajdine, Driss},
Title = {{Combining Facial Parts For Learning Gender, Ethnicity, and Emotional
   State Based on RGB-D Information}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2018}},
Volume = {{14}},
Number = {{1, S}},
Month = {{APR}},
Abstract = {{With the success of emerging RGB-D cameras such as the Kinect sensor,
   combining the shape (depth) and texture information to improve the
   quality of recognition became a trend among computer vision researchers.
   In this work, we address the problem of face classification in the
   context of RGB images and depth data. Inspired by the psychological
   results for human face perception, this article focuses on (i) finding
   out which facial parts are most effective at making the difference for
   some social aspects of face perception (gender, ethnicity, and emotional
   state), (ii) determining the optimal decision by combining the decision
   rendered by the individual parts, and (iii) extracting the promising
   features from RGB-D faces to exploit all the potential that this data
   provide. Experimental results on EurecomKinect Face and CurtinFaces
   databases show that the proposed approach improves the recognition
   quality in many use cases.}},
DOI = {{10.1145/3152125}},
Article-Number = {{19}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
Unique-ID = {{ISI:000433517100005}},
}

@article{ ISI:000433517100003,
Author = {Demisse, Girum G. and Aouada, Djamila and Ottersten, Bjorn},
Title = {{Deformation-Based 3D Facial Expression Representation}},
Journal = {{ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS}},
Year = {{2018}},
Volume = {{14}},
Number = {{1, S}},
Month = {{APR}},
Abstract = {{We propose a deformation-based representation for analyzing expressions
   fromthree-dimensional (3D) faces. A point cloud of a 3D face is
   decomposed into an ordered deformable set of curves that start from a
   fixed point. Subsequently, a mapping function is defined to identify the
   set of curves with an element of a high-dimensional matrix Lie group,
   specifically the direct product of SE(3). Representing 3D faces as an
   element of a high-dimensional Lie group has two main advantages. First,
   using the group structure, facial expressions can be decoupled from a
   neutral face. Second, an underlying non-linear facial expression
   manifold can be captured with the Lie group and mapped to a linear
   space, Lie algebra of the group. This opens up the possibility of
   classifying facial expressions with linear models without compromising
   the underlying manifold. Alternatively, linear combinations of
   linearised facial expressions can be mapped back from the Lie algebra to
   the Lie group. The approach is tested on the Binghamton University 3D
   Facial Expression (BU-3DFE) and the Bosphorus datasets. The results show
   that the proposed approach performed comparably, on the BU-3DFE dataset,
   without using features or extensive landmark points.}},
DOI = {{10.1145/3176649}},
Article-Number = {{17}},
ISSN = {{1551-6857}},
EISSN = {{1551-6865}},
ResearcherID-Numbers = {{Ottersten, Bjorn/G-1005-2011}},
ORCID-Numbers = {{Ottersten, Bjorn/0000-0003-2298-6774}},
Unique-ID = {{ISI:000433517100003}},
}

@article{ ISI:000433351100010,
Author = {Alashkar, Taleb and Ben Amor, Boulbaba and Daoudi, Mohamed and Berretti,
   Stefano},
Title = {{Spontaneous Expression Detection from 3D Dynamic Sequences by Analyzing
   Trajectories on Grassmann Manifolds}},
Journal = {{IEEE TRANSACTIONS ON AFFECTIVE COMPUTING}},
Year = {{2018}},
Volume = {{9}},
Number = {{2}},
Pages = {{271-284}},
Month = {{APR-JUN}},
Abstract = {{In this paper, we propose a framework for online spontaneous emotion
   detection, such as happiness or physical pain, from depth videos. Our
   approach consists on mapping the video streams onto a Grassmann manifold
   (i.e., space of k-dimensional linear subspaces) to form
   time-parameterized trajectories. To this end, depth videos are
   decomposed into short-time subsequences, each approximated by a
   k-dimensional linear subspace, which is in turn a point on the Grassmann
   manifold. Then, the temporal evolution of subspaces gives rise to a
   precise mathematical representation of trajectories on the underlying
   manifold. In the final step, extracted spatio-temporal features based on
   computing the velocity vectors along the trajectories, termed Geometric
   Motion History (GMH), are fed to an early event detector based on
   Structured Output SVM, which enables online emotion detection from
   partially-observed data. Experimental results obtained on the publicly
   available Cam3D Kinect and BP4D-spontaneous databases validate the
   proposed solution. The first database has served to exemplify the
   proposed framework using depth sequences of the upper part of the body
   collected using depth-consumer cameras, while the second database
   allowed the application of the same framework to physical pain detection
   from high-resolution and long 3D-face sequences.}},
DOI = {{10.1109/TAFFC.2016.2623718}},
ISSN = {{1949-3045}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018
   Daoudi, Mohammed/H-5935-2013}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305
   Berretti, Stefano/0000-0003-1219-4386
   Daoudi, Mohammed/0000-0003-4219-7860}},
Unique-ID = {{ISI:000433351100010}},
}

@article{ ISI:000429630300007,
Author = {Kaashki, Nastaran Nourbakhsh and Safabakhsh, Reza},
Title = {{RGB-D face recognition under various conditions via 3D constrained local
   model}},
Journal = {{JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION}},
Year = {{2018}},
Volume = {{52}},
Pages = {{66-85}},
Month = {{APR}},
Abstract = {{This research proposes a method for 3D face recognition in various
   conditions using 3D constrained local model (CLM-Z). In this method, a
   combination of 2D images (RGBs) and depth images (Ds) captured by Kinect
   has been used. After detecting the face and smoothing the depth image,
   CLM-Z model has been used to model and detect the important points of
   the face. These points are described using Histogram of Oriented
   Gradients (HOG), Local Binary Patterns (LBP), and 3D Local Binary
   Patterns (3DLBP). Finally, each face is recognized by a Support Vector
   Machine (SVM). The challenging situations are changes of lighting,
   facial expression and head pose. The results on CurtinFaces and IIIT-D
   datasets demonstrate that the proposed method outperformed
   state-of-the-art methods under illumination, expression and pitch pose
   conditions and comparable results were obtained in other cases.
   Additionally, our proposed method is robust even when the training data
   has not been carefully collected.}},
DOI = {{10.1016/j.jvcir.2018.02.003}},
ISSN = {{1047-3203}},
EISSN = {{1095-9076}},
ORCID-Numbers = {{Safabakhsh, Reza/0000-0002-4937-8026}},
Unique-ID = {{ISI:000429630300007}},
}

@article{ ISI:000428320200008,
Author = {Menke, Leonie A. and Gardeitchik, Thatjana and Hammond, Peter and
   Heimdal, Ketil R. and Houge, Gunnar and Hufnagel, Sophia B. and Ji,
   Jianling and Johansson, Stefan and Kant, Sarina G. and Kinning, Esther
   and Leon, Eyby L. and Newbury-Ecob, Ruth and Paolacci, Stefano and
   Pfundt, Rolph and Ragge, Nicola K. and Rinne, Tuula and Ruivenkamp,
   Claudia and Saitta, Sulagna C. and Sun, Yu and Tartaglia, Marco and
   Terhal, Paulien A. and van Essen, Anthony J. and Vigeland, Magnus D. and
   Xiao, Bing and Hennekam, Raoul C. and DDD Study},
Title = {{Further delineation of an entity caused by CREBBP and EP300 mutations
   but not resembling Rubinstein-Taybi syndrome}},
Journal = {{AMERICAN JOURNAL OF MEDICAL GENETICS PART A}},
Year = {{2018}},
Volume = {{176}},
Number = {{4}},
Pages = {{862-876}},
Month = {{APR}},
Abstract = {{In 2016, we described that missense variants in parts of exons 30 and 31
   of CREBBP can cause a phenotype that differs from Rubinstein-Taybi
   syndrome (RSTS). Here we report on another 11 patients with variants in
   this region of CREBBP (between bp 5,128 and 5,614) and two with variants
   in the homologous region of EP300. None of the patients show
   characteristics typical for RSTS. The variants were detected by exome
   sequencing using a panel for intellectual disability in all but one
   individual, in whom Sanger sequencing was performed upon clinical
   recognition of the entity. The main characteristics of the patients are
   developmental delay (90\%), autistic behavior (65\%), short stature
   (42\%), and microcephaly (43\%). Medical problems include feeding
   problems (75\%), vision (50\%), and hearing (54\%) impairments,
   recurrent upper airway infections (42\%), and epilepsy (21\%). Major
   malformations are less common except for cryptorchidism (46\% of males),
   and cerebral anomalies (70\%). Individuals with variants between bp
   5,595 and 5,614 of CREBBP show a specific phenotype (ptosis, telecanthi,
   short and upslanted palpebral fissures, depressed nasal ridge, short
   nose, anteverted nares, short columella, and long philtrum). 3D face
   shape demonstrated resemblance to individuals with a duplication of
   16p13.3 (the region that includes CREBBP), possibly indicating a gain of
   function. The other affected individuals show a less specific phenotype.
   We conclude that there is now more firm evidence that variants in these
   specific regions of CREBBP and EP300 result in a phenotype that differs
   from RSTS, and that this phenotype may be heterogeneous.}},
DOI = {{10.1002/ajmg.a.38626}},
ISSN = {{1552-4825}},
EISSN = {{1552-4833}},
ResearcherID-Numbers = {{Kant, Sarina/F-8596-2010
   Heimdal, Ketil Riddervold/H-3038-2019
   Johansson, Stefan/C-4394-2011
   Menke, Leonie/K-2515-2019
   }},
ORCID-Numbers = {{Johansson, Stefan/0000-0002-2298-7008
   Vigeland, Magnus Dehli/0000-0002-9134-4962
   Menke, Leonie/0000-0002-8312-7730
   Heimdal, Ketil/0000-0002-8911-3508}},
Unique-ID = {{ISI:000428320200008}},
}

@article{ ISI:000428416300007,
Author = {Gao, Wanshun and Zhao, Xi and An, Jun and Zou, Jianhua},
Title = {{Multi-pose 3D facial texture refinement for face recognition}},
Journal = {{INTERNATIONAL JOURNAL OF WAVELETS MULTIRESOLUTION AND INFORMATION
   PROCESSING}},
Year = {{2018}},
Volume = {{16}},
Number = {{2, SI}},
Month = {{MAR}},
Abstract = {{In this paper, we propose a novel approach for 3D face reconstruction
   from multi-facial images. Given original pose-variant images, coarse 3D
   face templates are initialized to reconstruct a refined 3D face mesh in
   an iteration manner. Then, we warp original facial images to the 2D
   meshes projected from 3D using Sparse Mesh Affine Warp (SMAW). Finally,
   we weight the face patches in each view respectively and map the patch
   with higher weight to a canonical UV space. For facial images with
   arbitrary pose, their invisible regions are filled with the
   corresponding UV patches. Poisson editing is applied to blend different
   patches seamlessly. We evaluate the proposed method on LFW dataset in
   terms of texture refinement and face recognition. The results
   demonstrate competitive performance compared to state-of-the-art
   methods.}},
DOI = {{10.1142/S0219691318400064}},
Article-Number = {{1840006}},
ISSN = {{0219-6913}},
EISSN = {{1793-690X}},
Unique-ID = {{ISI:000428416300007}},
}

@article{ ISI:000428416300008,
Author = {Kai, Wang and An, Jun and Zhao, Xi and Zou, Jianhua},
Title = {{Accurate landmarking from 3D facial scans by CNN and cascade regression}},
Journal = {{INTERNATIONAL JOURNAL OF WAVELETS MULTIRESOLUTION AND INFORMATION
   PROCESSING}},
Year = {{2018}},
Volume = {{16}},
Number = {{2, SI}},
Month = {{MAR}},
Abstract = {{Facial landmarking locates the key facial feature points on facial data,
   which provides not only information on semantic facial structures, but
   also prior knowledge for other types of facial analysis. However, most
   of the existing works still focus on the 2D facial image which is quite
   sensitive to the lighting condition changes. In order to address this
   limitation, this paper proposed a coarse-to-fine method only based on
   the 3D facial scan data extracted from professional equipment to
   automatically and accurately estimate the landmark localization.
   Specifically, we firstly trained a convolutional neural network (CNN) to
   initialize the face landmarks instead of the mean shape. Then the
   proposed cascade regression networks learn the mapping function between
   3D facial geometry feature and landmarks location. Tested on Bosphorus
   database, the experimental results demonstrated effectiveness and
   accuracy of the proposed method for 22 landmarks. Compared with other
   methods, the results in several points demonstrate state-of-the-art
   performance.}},
DOI = {{10.1142/S0219691318400076}},
Article-Number = {{1840007}},
ISSN = {{0219-6913}},
EISSN = {{1793-690X}},
Unique-ID = {{ISI:000428416300008}},
}

@article{ ISI:000436148300003,
Author = {Guo, Yingchun and Wei, Ruoyu and Liu, Yi},
Title = {{Weighted Gradient Feature Extraction Based on Multiscale Sub-Blocks for
   3D Facial Recognition in Bimodal Images}},
Journal = {{INFORMATION}},
Year = {{2018}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{In this paper, we propose a bimodal 3D facial recognition method aimed
   at increasing the recognition rate and reducing the effect of
   illumination, pose, expression, ages, and occlusion on facial
   recognition. There are two features extracted from the multiscale
   sub-blocks in both the 3D mode depth map and 2D mode intensity map,
   which are the local gradient pattern (LGP) feature and the weighted
   histogram of gradient orientation (WHGO) feature. LGP and WHGO features
   are cascaded to form the 3D facial feature vector LGP-WHGO, and are
   further trained and identified by the support vector machine (SVM).
   Experiments on the CASIA database, FRGC v2.0 database, and Bosphorus
   database show that, the proposed method can efficiently extract the
   structure information and texture information of the facial image, and
   have a robustness to illumination, expression, occlusion and pose.}},
DOI = {{10.3390/info9030048}},
Article-Number = {{48}},
ISSN = {{2078-2489}},
Unique-ID = {{ISI:000436148300003}},
}

@article{ ISI:000427009100019,
Author = {Ye, Cang and Qian, Xiangfei},
Title = {{3-D Object Recognition of a Robotic Navigation Aid for the Visually
   Impaired}},
Journal = {{IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING}},
Year = {{2018}},
Volume = {{26}},
Number = {{2}},
Pages = {{441-450}},
Month = {{FEB}},
Abstract = {{This paper presents a 3-D object recognition method and its
   implementation on a robotic navigation aid to allow real-time detection
   of indoor structural objects for the navigation of a blind person. The
   method segments a point cloud into numerous planar patches and extracts
   their inter-plane relationships (IPRs). Based on the existing IPRs of
   the object models, the method defines six high level features (HLFs) and
   determines the HLFs for each patch. A Gaussian-mixture-model-based plane
   classifier is then devised to classify each planar patch into one
   belonging to a particular object model. Finally, a recursive plane
   clustering procedure is used to cluster the classified planes into the
   model objects. As the proposed method uses geometric context to detect
   an object, it is robust to the object's visual appearance change. As a
   result, it is ideal for detecting structural objects (e.g., stairways,
   doorways, and so on). In addition, it has high scalability and
   parallelism. The method is also capable of detecting some indoor
   nonstructural objects. Experimental results demonstrate that the
   proposed method has a high success rate in object recognition.}},
DOI = {{10.1109/TNSRE.2017.2748419}},
ISSN = {{1534-4320}},
EISSN = {{1558-0210}},
Unique-ID = {{ISI:000427009100019}},
}

@article{ ISI:000418370200006,
Author = {Wang, Nannan and Gao, Xinbo and Tao, Dacheng and Yang, Heng and Li,
   Xuelong},
Title = {{Facial feature point detection: A comprehensive survey}},
Journal = {{NEUROCOMPUTING}},
Year = {{2018}},
Volume = {{275}},
Pages = {{50-65}},
Month = {{JAN 31}},
Abstract = {{This paper presents a comprehensive survey of facial feature point
   detection with the assistance of abundant manually labeled images.
   Facial feature point detection favors many applications such as face
   recognition, animation, tracking, hallucination, expression analysis and
   3D face modeling. Existing methods are categorized into two primary
   categories according to whether there is the need of a parametric shape
   model: parametric shape model-based methods and nonparametric shape
   model-based methods. Parametric shape model-based methods are further
   divided into two secondary classes according to their appearance models:
   local part model-based methods (e.g. constrained local model) and
   holistic model-based methods (e.g. active appearance model).
   Nonparametric shape model-based methods are divided into several groups
   according to their model construction process: exemplar-based methods,
   graphical model-based methods, cascaded regression-based methods, and
   deep learning based methods. Though significant progress has been made,
   facial feature point detection is still limited in its success by wild
   and real-world conditions: large variations across poses, expressions,
   illuminations, and occlusions. A comparative illustration and analysis
   of representative methods provides us a holistic understanding and deep
   insight into facial feature point detection, which also motivates us to
   further explore more promising future schemes. (c) 2017 Elsevier B.V.
   All rights reserved.}},
DOI = {{10.1016/j.neucom.2017.05.013}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
ORCID-Numbers = {{Li, Xuelong/0000-0002-0019-4197}},
Unique-ID = {{ISI:000418370200006}},
}

@article{ ISI:000418370200123,
Author = {Li, Ye and Wang, YingHui and Liu, Jing and Hao, Wen},
Title = {{Expression-insensitive 3D face recognition by the fusion of multiple
   subject-specific curves}},
Journal = {{NEUROCOMPUTING}},
Year = {{2018}},
Volume = {{275}},
Pages = {{1295-1307}},
Month = {{JAN 31}},
Abstract = {{This study proposes a 3D face recognition method using multiple
   subject-specific curves insensitive to intra-subject distortions caused
   by expression variations. Considering that most sharp variances in
   facial convex regions are closely related to the bone structure, the
   convex crest curves are first extracted as the most vital
   subject-specific facial curves based on the principal curvature extrema
   in convex local surfaces. Then, the central profile curve and the
   horizontal contour curve passing through the nose tip are detected by
   using the precise localization of the nose tip and symmetry plane. Based
   on their discriminative power and robustness to expression changes, the
   three types of curves are fused with appropriate weights at the
   feature-level and used for matching 3D faces with the iterative closest
   point algorithm. The combination of multiple expression-insensitive
   curves is complementary and provides sufficient and stable facial
   surface features for face recognition. In addition, for each convex
   crest curve, an expression-irrelevant factor is assigned as the adaptive
   weight to improve the face matching performance. The results of
   experiments using two public 3D databases, GavabDB and BU-3DFE,
   demonstrate the effectiveness of the proposed method, and its
   recognition rates on both databases reflect an encouraging performance.
   (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2017.09.070}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000418370200123}},
}

@article{ ISI:000414762100022,
Author = {Deng, Weihong and Fang, Yuke and Xu, Zhenqi and Hu, Jiani},
Title = {{Facial landmark localization by enhanced convolutional neural network}},
Journal = {{NEUROCOMPUTING}},
Year = {{2018}},
Volume = {{273}},
Pages = {{222-229}},
Month = {{JAN 17}},
Abstract = {{Facial landmark localization is important to many facial recognition and
   analysis tasks, such as face attributes analysis, head pose estimation,
   3D face modeling, and facial expression analysis. In this paper, we
   propose a new approach to localizing landmarks in facial image by deep
   convolutional neural network (DCNN). We make two enhancements on the CNN
   to adapt it to the feature localization task as follows. First, we
   replace the commonly used max pooling by depth-wise convolution to
   obtain better localization performance. Second, we define a response map
   for each facial points as a 2D probability map indicating the presence
   likelihood, and train our model with a KL divergence loss. To obtain
   robust localization results, our approach first takes the expectations
   of the response maps of enhanced CNN and then applies auto-encoder model
   to the global shape vector, which is effective to rectify the outlier
   points by the prior global landmark configurations. The proposed ECNN
   method achieves 5.32\% mean error on the experiments on the 300-W
   dataset, which is comparable to the state-of-the-art performance on this
   standard benchmark, showing the effectiveness of our methods. (C) 2017
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2017.07.052}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000414762100022}},
}

@inproceedings{ ISI:000460471100001,
Author = {Abbad, Abdelghafour and Abbad, Khalid and Tairi, Hamid},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{3D face recognition in the presence of facial expressions based on
   empirical mode decomposition}},
Booktitle = {{PROCEEDINGS OF THE 2ND MEDITERRANEAN CONFERENCE ON PATTERN RECOGNITION
   AND ARTIFICIAL INTELLIGENCE (MEDPRAI-2018)}},
Year = {{2018}},
Pages = {{1-6}},
Note = {{2nd Mediterranean Conference on Pattern Recognition and Artificial
   Intelligence (MedPRAI), Ibn Tofail Univ, Rabat, MOROCCO, MAR 27-28, 2018}},
Organization = {{Bahria Univ; Univ Larbi Tebessi Tebessa; OCP; ENSIAS; Int Assoc Pattern
   Recognit}},
Abstract = {{This paper presents an efficient 3D face recognition method to handle
   facial expression. The proposed method uses the Surfaces Empirical Mode
   Decomposition (SEMD), facial curves and local shape descriptor in a
   matching process to overcome the distortions caused by expressions in
   faces. The basic idea is that, the face is presented at different scales
   by SEMD. Then the isometric invariant features on each scale are
   extracted. After that, the geometric information is obtained on the 3D
   surface in terms of radial and level facial curves. Finally, the feature
   vectors on each scale are associated with their corresponding geometric
   information. The presented method is validated on GavabDB database
   resulting a rank 1 recognition rate (RR) of 98.9\% for all faces with
   neutral and non-neutral expressions. This result outperforms other 3D
   expression-invariant face recognition methods on the same database.}},
DOI = {{10.1145/3177148.3180087}},
ISBN = {{978-1-4503-5290-1}},
Unique-ID = {{ISI:000460471100001}},
}

@inproceedings{ ISI:000460476900026,
Author = {Kamanditya, Bharindra and Kuswara, Randy Pangestu and Nugroho, Muhammad
   Adi and Kusumoputro, Benyamin},
Book-Group-Author = {{IEEE}},
Title = {{Convolution Neural Network for Pose Estimation of Noisy
   Three-Dimensional Face Images}},
Booktitle = {{2018 5TH IEEE INTERNATIONAL CONFERENCE ON ENGINEERING TECHNOLOGIES AND
   APPLIED SCIENCES (IEEE ICETAS)}},
Year = {{2018}},
Note = {{5th IEEE International Conference on Engineering Technologies and
   Applied Sciences (IEEE ICETAS), Bangkok, THAILAND, NOV 22-23, 2018}},
Organization = {{IEEE; IEEE IIUM Student Branch; ETSS Management}},
Abstract = {{From limited two-dimensional recognition, facial recognition has now
   been developed to be able to recognize three-dimensional face images,
   which usually involves process of face pose estimation. As the
   conventional artificial neural networks has shown low recognition rate
   to this problem, Convolution Neural Network have been the most potential
   classifier to determine the pose estimation of a three-dimensional face
   images. Convolution operation is expected to minimize the effect of
   distortion and disorientation of the object, and able to efficiently
   reduce the required parameters. Results show that the CNN system could
   estimate the pose position of the 3D face images with high recognition
   rate, however, this recognition rate decline significantly for the noisy
   buried face images, showing the CNN still need improvement to deal with
   noisy environments.}},
ISBN = {{978-1-5386-7966-1}},
Unique-ID = {{ISI:000460476900026}},
}

@inproceedings{ ISI:000457881301017,
Author = {Akita, Tokihiko and Yamauchi, Yuji and Fujiyoshi, Hironobu},
Book-Group-Author = {{IEEE}},
Title = {{Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye
   Cameras}},
Booktitle = {{2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)}},
Series = {{IEEE International Conference on Intelligent Transportation Systems-ITSC}},
Year = {{2018}},
Pages = {{1103-1108}},
Note = {{21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC), Maui, HI, NOV 04-07, 2018}},
Organization = {{IEEE; IEEE Intelligent Transportat Syst Soc}},
Abstract = {{Recently, automated emergency brake systems for pedestrian have been
   commercialized. However, they cannot detect crossing pedestrians when
   turning at intersections because the field of view is not wide enough.
   Thus, we propose to utilize a surround view camera system becoming
   popular by making it into stereo vision which is robust for the
   pedestrian recognition. However, conventional stereo camera technologies
   cannot be applied due to fisheye cameras and uncalibrated camera poses.
   Thus we have created the new method to absorb difference of the
   pedestrian appearance between cameras by machine learning for the stereo
   vision. The method of stereo matching between image patches in each
   camera image was designed by combining D-Brief and NCC with SVM. Good
   generalization performance was achieved by it compared with individual
   conventional algorithms. Furthermore, feature amounts of the point cloud
   reconstructed by the stereo pairs are utilized with Random Forest to
   discriminate pedestrians. The algorithm was evaluated for the actual
   camera images of crossing pedestrians at various intersections, and
   96.0\% of pedestrian tracking rate with high position detection accuracy
   was achieved. They were compared with Faster R-CNN as the best pattern
   recognition technique, and our proposed method indicated better
   detection performance.}},
ISSN = {{2153-0009}},
ISBN = {{978-1-7281-0323-5}},
Unique-ID = {{ISI:000457881301017}},
}

@inproceedings{ ISI:000457843602003,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal},
Book-Group-Author = {{IEEE}},
Title = {{Learning from Millions of 3D Scans for Large-scale 3D Face Recognition}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{1896-1905}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{Deep networks trained on millions of facial images are believed to be
   closely approaching human-level performance in face recognition.
   However, open world face recognition still remains a challenge.
   Although, 3D face recognition has an inherent edge over its 2D
   counterpart, it has not benefited from the recent developments in deep
   learning due to the unavailability of large training as well as large
   test datasets. Recognition accuracies have already saturated on existing
   3D face datasets due to their small gallery sizes. Unlike 2D
   photographs, 3D facial scans cannot be sourced from the web causing a
   bottleneck in the development of deep 3D face recognition networks and
   datasets. In this backdrop, we propose a method for generating a large
   corpus of labeled 3D face identities and their multiple instances for
   training and a protocol for merging the most challenging existing 3D
   datasets for testing. We also propose the first deep CNN model designed
   specifically for 3D face recognition and trained on 3.1 Million 3D
   facial scans of 100K identities. Our test dataset comprises 1,853
   identities with a single 3D scan in the gallery and another 31K scans as
   probes, which is several orders of magnitude larger than existing ones.
   Without fine tuning on this dataset, our network already outperforms
   state of the art face recognition by over 10\%. We fine tune our network
   on the gallery set to perform end-to-end large scale 3D face recognition
   which further improves accuracy. Finally, we show the efficacy of our
   method for the open world face recognition problem.}},
DOI = {{10.1109/CVPR.2018.00203}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
ORCID-Numbers = {{Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000457843602003}},
}

@inproceedings{ ISI:000457843605028,
Author = {Cheng, Shiyang and Kotsia, Irene and Pantic, Maja and Zafeiriou,
   Stefanos},
Book-Group-Author = {{IEEE}},
Title = {{4DFAB: A Large Scale 4D Database for Facial Expression Analysis and
   Biometric Applications}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{5117-5126}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{The progress we are currently witnessing in many computer vision
   applications, including automatic face analysis, would not be made
   possible without tremendous efforts in collecting and annotating large
   scale visual databases. To this end, we propose 4DFAB, a new large scale
   database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes).
   4DFAB contains recordings of 180 subjects captured in four different
   sessions spanning over a five-year period. It contains 4D videos of
   subjects displaying both spontaneous and posed facial behaviours. The
   database can be used for both face and facial expression recognition, as
   well as behavioural biometrics. It can also be used to learn very
   powerful blendshapes for parametrising facial behaviour. In this paper,
   we conduct several experiments and demonstrate the usefulness of the
   database for various applications. The database will be made publicly
   available for research purposes.}},
DOI = {{10.1109/CVPR.2018.00537}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843605028}},
}

@inproceedings{ ISI:000457843605038,
Author = {Liu, Feng and Zhu, Ronghang and Zeng, Dan and Zhao, Qijun and Liu,
   Xiaoming},
Book-Group-Author = {{IEEE}},
Title = {{Disentangling Features in 3D Face Shapes for Joint Face Reconstruction
   and Recognition}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{5216-5225}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{This paper proposes an encoder-decoder network to disentangle shape
   features during 3D face reconstruction from single 2D images, such that
   the tasks of reconstructing accurate 3D face shapes and learning
   discriminative shape features for face recognition can be accomplished
   simultaneously. Unlike existing 3D face reconstruction methods, our
   proposed method directly regresses dense 3D face shapes from single 2D
   images, and tackles identity and residual (i.e., non-identity)
   components in 3D face shapes explicitly and separately based on a
   composite 3D face shape model with latent representations. We devise a
   training process for the proposed network with a joint loss measuring
   both face identification error and 3D face shape reconstruction error.
   To construct training data we develop a method for fitting 3D morphable
   model (3DMM) to multiple 2D images of a subject. Comprehensive
   experiments have been done on MICC, BU3DFE, LFW and YTF databases. The
   results show that our method expands the capacity of 3DMM for capturing
   discriminative shape features and facial detail, and thus outperforms
   existing methods both in 3D face reconstruction accuracy and in face
   recognition accuracy.}},
DOI = {{10.1109/CVPR.2018.00547}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
ResearcherID-Numbers = {{Zeng, Dan/M-4615-2019}},
ORCID-Numbers = {{Zeng, Dan/0000-0002-9036-7791}},
Unique-ID = {{ISI:000457843605038}},
}

@inproceedings{ ISI:000457843607026,
Author = {Deng, Jiankang and Cheng, Shiyang and Xue, Niannan and Zhou, Yuxiang and
   Zafeiriou, Stefanos},
Book-Group-Author = {{IEEE}},
Title = {{UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face
   Recognition}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{7093-7102}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{Recently proposed robust 3D face alignment methods establish either
   dense or sparse correspondence between a 3D face model and a 2D facial
   image. The use of these methods presents new challenges as well as
   opportunities for facial texture analysis. In particular by sampling the
   image using the fitted model, a facial UV can be created. Unfortunately,
   due to self-occlusion, such a UV map is always incomplete. In this
   paper, we propose a framework for training Deep Convolutional Neural
   Network (DCNN) to complete the facial UV map extracted from in-the-wild
   images. To this end, we first gather complete UV maps by fitting a 3D
   Morphable Model (3DMM) to various multiview image and video datasets, as
   well as leveraging on a new 3D dataset with over 3,000 identities.
   Second, we devise a meticulously designed architecture that combines
   local and global adversarial DCNNs to learn an identity-preserving
   facial UV completion model. We demonstrate that by attaching the
   completed UV to the fitted mesh and generating instances of arbitrary
   poses, we can increase pose variations for training deep face
   recognition/verification models, and minimise pose discrepancy during
   testing, which lead to better performance. Experiments on both
   controlled and in-the-wild UV datasets prove the effectiveness of our
   adversarial UV completion model. We achieve state-of-the-art
   verification accuracy, 94.05\%, under the CFP frontal-profile protocol
   only by combining pose augmentation during training and pose discrepancy
   reduction during testing. We will release the first in-the-wild UV
   dataset (we refer as WildUV) that comprises of complete facial UV maps
   from 1,892 identities for research purposes.}},
DOI = {{10.1109/CVPR.2018.00741}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843607026}},
}

@inproceedings{ ISI:000457843608057,
Author = {Genova, Kyle and Cole, Forrester and Maschinot, Aaron and Sarna, Aaron
   and Vlasic, Daniel and Freeman, William T.},
Book-Group-Author = {{IEEE}},
Title = {{Unsupervised Training for 3D Morphable Model Regression}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{8377-8386}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{We present a method for training a regression network from image pixels
   to 3D morphable model coordinates using only unlabeled photographs. The
   training loss is based on features from a facial recognition network,
   computed on the -fly by rendering the predicted faces with a
   differentiable renderer To make training from features feasible and
   avoid network fooling effects, we introduce three objectives: a batch
   distribution loss that encourages the output distribution to match the
   distribution of the morphable model, a loop back loss that ensures the
   network can correctly reinterpret its own output, and a multi-view
   identity loss that compares the features of the predicted 3D face and
   the input photograph from multiple viewing angles. We train a regression
   network using these objectives, a set of unlabeled photographs, and the
   morphable model itself and demonstrate state-of-the-art results.}},
DOI = {{10.1109/CVPR.2018.00874}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843608057}},
}

@inproceedings{ ISI:000457843609059,
Author = {Li, Jiaxin and Chen, Ben M. and Lee, Gim Hee},
Book-Group-Author = {{IEEE}},
Title = {{SO-Net: Self-Organizing Network for Point Cloud Analysis}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{9397-9406}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{This paper presents SO-Net, a permutation invariant architecture for
   deep learning with orderless point clouds. The SO-Net models the spatial
   distribution of point cloud by building a Self-Organizing Map (SOM).
   Based on the SOM, SO-Net performs hierarchical feature extraction on
   individual points and SOM nodes, and ultimately represents the input
   point cloud by a single feature vector. The receptive field of the
   network can be systematically adjusted by conducting point-to-node k
   nearest neighbor search. In recognition tasks such as point cloud
   reconstruction, classification, object part segmentation and shape
   retrieval, our proposed network demonstrates performance that is similar
   with or better than state-of-the-art approaches. In addition, the
   training speed is significantly faster than existing point cloud
   recognition networks because of the parallelizability and simplicity of
   the proposed architecture. Our code is available at the project
   website.(1)}},
DOI = {{10.1109/CVPR.2018.00979}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Unique-ID = {{ISI:000457843609059}},
}

@inproceedings{ ISI:000457913100097,
Author = {Liu, Chuanhe and Tang, Tianhao and Lv, Kui and Wang, Minghao},
Book-Group-Author = {{ACM}},
Title = {{Multi-Feature Based Emotion Recognition for Video Clips}},
Booktitle = {{ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON
   MULTIMODAL INTERACTION}},
Year = {{2018}},
Pages = {{630-634}},
Note = {{20th ACM International Conference on Multimodal Interaction (ICMI),
   Boulder, CO, OCT 16-20, 2018}},
Organization = {{Assoc Comp Machinery; Assoc Comp Machinery SIGCHI; Openstream;
   Microsoft; Univ Colorado Boulder, Inst Cognit Sci; audEERING}},
Abstract = {{In this paper, we present our latest progress in Emotion Recognition
   techniques, which combines acoustic features and facial features in both
   non-temporal and temporal mode. This paper presents the details of our
   techniques used in the Audio-Video Emotion Recognition subtask in the
   2018 Emotion Recognition in the Wild (EmotiW) Challenge. After the
   multimodal results fusion, our final accuracy in Acted Facial Expression
   in Wild (AFEW) test dataset achieves 61.87\%, which is 1.53\% higher
   than the best results last year. Such improvements prove the
   effectiveness of our methods.}},
DOI = {{10.1145/3242969.3264989}},
ISBN = {{978-1-4503-5692-3}},
Unique-ID = {{ISI:000457913100097}},
}

@inproceedings{ ISI:000457719100010,
Author = {Meyer, Gregory P. and Do, Minh N.},
Book-Group-Author = {{IEEE}},
Title = {{Real-time 3D Face Verification with a Consumer Depth Camera}},
Booktitle = {{2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV)}},
Year = {{2018}},
Pages = {{71-79}},
Note = {{15th Conference on Computer and Robot Vision (CRV), Toronto, CANADA, MAY
   08-11, 2018}},
Organization = {{Canadian Image Proc \& Pattern Recognit Soc; Assoc Canadienne Traitement
   Images Reconnaissance Formes; MDA; Modiface; NextAI; SPORTLOGiQ;
   StradigiAI; York Univ Vis Sci Applicat Program; York Univ Ctr Vis Res;
   ElementAI; EPSON; Miovision; Trans Plan}},
Abstract = {{We present a system for accurate real-time 3D face verification using a
   low-quality consumer depth camera. To verify the identity of a subject,
   we built a high-quality reference model offline by fitting a 3D
   morphable model to a sequence of low-quality depth images. At runtime,
   we compare the similarity between the reference model and a single depth
   image by aligning the model to the image and measuring differences
   between every point on the two facial surfaces. The model and the image
   will not match exactly due to sensor noise, occlusions, as well as
   changes in expression, hairstyle, and eye-wear; therefore, we leverage a
   data driven approach to determine whether or not the model and the image
   match. We train a random decision forest to verify the identity of a
   subject where the point-to-point distances between the reference model
   and the depth image are used as input features to the classifier. Our
   approach runs in real-time and is designed to continuously authenticate
   a user as he/she uses his/her device. In addition, our proposed method
   outperforms existing 2D and 3D face verification methods on a benchmark
   data set.}},
DOI = {{10.1109/CRV.2018.00020}},
ISBN = {{978-1-5386-6481-0}},
Unique-ID = {{ISI:000457719100010}},
}

@inproceedings{ ISI:000455228100040,
Author = {Xue, Mingliang and Duan, Xiaodong and Liu, Wanquan and Wang, Yuehai},
Editor = {{Zhou, J and Wang, Y and Sun, Z and Jia, Z and Feng, J and Shan, S and Ubul, K and Guo, Z}},
Title = {{An ICA-Based Other-Race Effect Elimination for Facial Expression
   Recognition}},
Booktitle = {{BIOMETRIC RECOGNITION, CCBR 2018}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2018}},
Volume = {{10996}},
Pages = {{367-376}},
Note = {{13th Chinese Conference on Biometric Recognition (CCBR), Urumqi, PEOPLES
   R CHINA, AUG 11-12, 2018}},
Organization = {{Chinese Assoc Artificial Intelligence; Chinese Acad Sci, Inst Automat;
   Springer; Xinjiang Univ}},
Abstract = {{Other-race effect affects the performance of multi-race facial
   expression recognition significantly. Though this phenomenon has been
   noticed by psychologists and computer vision researchers for decades,
   few work has been done to eliminate this influence caused by other-race
   effect. This work proposes an ICA-based other-race effect elimination
   method for 3D facial expression recognition. Firstly, the local depth
   features are extracted from 3D face point clouds, and then independent
   component analysis is used to project the features into a subspace in
   which the feature components are mutually independent. Second, a mutual
   information based feature selection method is adopted to determine
   race-sensitive features. Finally, the features after race-sensitive
   information elimination are utilized to conduct facial expression
   recognition. The proposed method is evaluated on BU-3DFE database, and
   the results reveal that the proposed method is effective to other-race
   effect elimination and could improve the multi-race facial expression
   recognition performance.}},
DOI = {{10.1007/978-3-319-97909-0\_40}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-97909-0; 978-3-319-97908-3}},
ORCID-Numbers = {{liu, wanquan/0000-0003-4910-353X}},
Unique-ID = {{ISI:000455228100040}},
}

@inproceedings{ ISI:000455228100045,
Author = {Feng, Ziqing and Zhao, Qijun},
Editor = {{Zhou, J and Wang, Y and Sun, Z and Jia, Z and Feng, J and Shan, S and Ubul, K and Guo, Z}},
Title = {{Robust Face Recognition with Deeply Normalized Depth Images}},
Booktitle = {{BIOMETRIC RECOGNITION, CCBR 2018}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2018}},
Volume = {{10996}},
Pages = {{418-427}},
Note = {{13th Chinese Conference on Biometric Recognition (CCBR), Urumqi, PEOPLES
   R CHINA, AUG 11-12, 2018}},
Organization = {{Chinese Assoc Artificial Intelligence; Chinese Acad Sci, Inst Automat;
   Springer; Xinjiang Univ}},
Abstract = {{Depth information has been proven useful for face recognition. However,
   existing depth-image-based face recognition methods still suffer from
   noisy depth values and varying poses and expressions. In this paper, we
   propose a novel method for normalizing facial depth images to frontal
   pose and neutral expression and extracting robust features from the
   normalized depth images. The method is implemented via two deep
   convolutional neural networks (DCNN), normalization network (Net(N)) and
   feature extraction network (Net(F)). Given a facial depth image, Net(N)
   first converts it to an HHA image, from which the 3D face is
   reconstructed via a DCNN. Net(N) then generates a pose-and-expression
   normalized (PEN) depth image from the reconstructed 3D face. The PEN
   depth image is finally passed to Net(F), which extracts a robust feature
   representation via another DCNN for face recognition. Our preliminary
   evaluation results demonstrate the superiority of the proposed method in
   recognizing faces of arbitrary poses and expressions with depth images.}},
DOI = {{10.1007/978-3-319-97909-0\_45}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-97909-0; 978-3-319-97908-3}},
Unique-ID = {{ISI:000455228100045}},
}

@inproceedings{ ISI:000454996700029,
Author = {Zhang, Gang and Han, Hu and Shan, Shiguang and Song, Xingguang and Chen,
   Xilin},
Book-Group-Author = {{IEEE}},
Title = {{Face Alignment across Large Pose via MT-CNN based 3D Shape
   Reconstruction}},
Booktitle = {{PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE \&
   GESTURE RECOGNITION (FG 2018)}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2018}},
Pages = {{210-217}},
Note = {{13th IEEE International Conference on Automatic Face \& Gesture
   Recognition (FG), Xi an, PEOPLES R CHINA, MAY 15-19, 2018}},
Organization = {{IEEE Comp Soc; IEEE Biometr Council}},
Abstract = {{Face alignment plays an important role for robust face recognition and
   analysis applications in the wild. While a number of face alignment
   methods are available, large-pose face alignment remains a very
   challenging problem due to the ambiguity of facial keypoints in 2D face
   images. Recent attempts to solve this problem via 3D model fitting show
   more robustness against large poses and 2D ambiguity, but their accuracy
   and speed are still limited. We propose a 3D reconstruction based method
   to quickly and accurately detect 2D facial landmarks and estimate their
   visibility. By designing a cascaded multi-task CNN model, we can
   efficiently reconstruct the 3D face shape, together with pose estimation
   as an auxiliary task. Finally, the landmarks on 3D shape are projected
   to the 2D face image to get the 2D landmarks and their visibility.
   Experimental results on the challenging 300W-LP, AFLW2000-3D, and AFLW
   databases show that the proposed approach can be comparable with the
   state-of-the-art methods and is able to run in real time (32ms per
   image) on 3.4 GHz CPU.}},
DOI = {{10.1109/FG.2018.00039}},
ISSN = {{2326-5396}},
ISBN = {{978-1-5386-2335-0}},
Unique-ID = {{ISI:000454996700029}},
}

@inproceedings{ ISI:000454996700065,
Author = {Jan, Asim and Ding, Huaxiong and Meng, Hongying and Chen, Liming and Li,
   Huibin},
Book-Group-Author = {{IEEE}},
Title = {{Accurate Facial Parts Localization and Deep Learning for 3D Facial
   Expression Recognition}},
Booktitle = {{PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE \&
   GESTURE RECOGNITION (FG 2018)}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2018}},
Pages = {{466-472}},
Note = {{13th IEEE International Conference on Automatic Face \& Gesture
   Recognition (FG), Xi an, PEOPLES R CHINA, MAY 15-19, 2018}},
Organization = {{IEEE Comp Soc; IEEE Biometr Council}},
Abstract = {{Meaningful facial parts can convey key cues for both facial action unit
   detection and expression prediction. Textured 3D face scan can provide
   both detailed 3D geometric shape and 2D texture appearance cues of the
   face which are beneficial for Facial Expression Recognition (FER).
   However, accurate facial parts extraction as well as their fusion are
   challenging tasks. In this paper, a novel system for 3D FER is designed
   based on accurate facial parts extraction and deep feature fusion of
   facial parts. Experiments are conducted on the BU-3DFE database,
   demonstrating the effectiveness of combing different facial parts,
   texture and depth cues and reporting the state-of-the-art results in
   comparison with all existing methods under the same setting.}},
DOI = {{10.1109/FG.2018.00075}},
ISSN = {{2326-5396}},
ISBN = {{978-1-5386-2335-0}},
ORCID-Numbers = {{Meng, Hongying/0000-0002-8836-1382}},
Unique-ID = {{ISI:000454996700065}},
}

@inproceedings{ ISI:000455343100004,
Author = {Desai, Kevin and Prabhakaran, Balakrishnan and Raghuraman, Suraj},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{Combining Skeletal Poses for 3D Human Model Generation using Multiple
   Kinects}},
Booktitle = {{PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18)}},
Year = {{2018}},
Pages = {{40-51}},
Note = {{9th ACM Multimedia Systems Conference (MMSys), Centrum Wiskunde \&
   Informatica Amsterdam, Amsterdam, NETHERLANDS, JUN 12-15, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGMM; ACM SIGOPS; ACM SIGMOBILE; ACM SIGCOMM}},
Abstract = {{RGB-D cameras, such as the Microsoft Kinect, provide us with the 3D
   information, color and depth, associated with the scene. Interactive 3D
   Tele-Immersion (i3DTI) systems use such RGB-D cameras to capture the
   person present in the scene in order to collaborate with other remote
   users and interact with the virtual objects present in the environment.
   Using a single camera, it becomes difficult to estimate an accurate
   skeletal pose and complete 3D model of the person, especially when the
   person is not in the complete view of the camera. With multiple cameras,
   even with partial views, it is possible to get a more accurate estimate
   of the skeleton of the person leading to a better and complete 3D model.
   In this paper, we present a real-time skeletal pose identification
   approach that leverages on the inaccurate skeletons of the individual
   Kinects, and provides a combined optimized skeleton. We estimate the
   Probability of an Accurate Joint (PAJ) for each joint from all of the
   Kinect skeletons. We determine the correct direction of the person and
   assign the correct joint sides for each skeleton. We then use a greedy
   consensus approach to combine the highly probable and accurate joints to
   estimate the combined skeleton. Using the individual skeletons, we
   segment the point clouds from all the cameras. We use the already
   computed PAJ values to obtain the Probability of an Accurate Bone (PAB).
   The individual point clouds are then combined one segment after another
   using the calculated PAB values. The generated combined point cloud is a
   complete and accurate 3D representation of the person present in the
   scene. We validate our estimated skeleton against two well-known methods
   by computing the error distance between the best view Kinect skeleton
   and the estimated skeleton. An exhaustive analysis is performed by using
   around 500000 skeletal frames in total, captured using 7 users and 7
   cameras. Visual analysis is performed by checking whether the estimated
   skeleton is completely present within the human model. We also develop a
   3D Holo-Bubble game to showcase the real-time performance of the
   combined skeleton and point cloud. Our results show that our method
   performs better than the state-of-the-art approaches that use multiple
   Kinects, in terms of objective error, visual quality and real-time user
   performance.}},
DOI = {{10.1145/3204949.3204958}},
ISBN = {{978-1-4503-5192-8}},
ORCID-Numbers = {{Desai, Kevin/0000-0002-2964-8981}},
Unique-ID = {{ISI:000455343100004}},
}

@inproceedings{ ISI:000454996700113,
Author = {Feng, Zhen-Hua and Huber, Patrik and Kittler, Josef and Hancock, Peter
   and Wu, Xiao-Jun and Zhao, Qijun and Koppen, Paul and Raetsch, Matthias},
Book-Group-Author = {{IEEE}},
Title = {{Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild}},
Booktitle = {{PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE \&
   GESTURE RECOGNITION (FG 2018)}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2018}},
Pages = {{780-786}},
Note = {{13th IEEE International Conference on Automatic Face \& Gesture
   Recognition (FG), Xi an, PEOPLES R CHINA, MAY 15-19, 2018}},
Organization = {{IEEE Comp Soc; IEEE Biometr Council}},
Abstract = {{This paper investigates the evaluation of dense 3D face reconstruction
   from a single 2D image in the wild. To this end, we organise a
   competition that provides a new benchmark dataset that contains 2000 2D
   facial images of 135 subjects as well as their 3D ground truth face
   scans. In contrast to previous competitions or challenges, the aim of
   this new benchmark dataset is to evaluate the accuracy of a 3D dense
   face reconstruction algorithm using real, accurate and high-resolution
   3D ground truth face scans. In addition to the dataset, we provide a
   standard protocol as well as a Python script for the evaluation. Last,
   we report the results obtained by three state-of-the-art 3D face
   reconstruction systems on the new benchmark dataset. The competition is
   organised along with the 2018 13th IEEE Conference on Automatic Face \&
   Gesture Recognition.}},
DOI = {{10.1109/FG.2018.00123}},
ISSN = {{2326-5396}},
ISBN = {{978-1-5386-2335-0}},
ResearcherID-Numbers = {{Hancock, Peter J.B./A-4633-2009}},
ORCID-Numbers = {{Hancock, Peter J.B./0000-0001-6025-7068}},
Unique-ID = {{ISI:000454996700113}},
}

@inproceedings{ ISI:000454832200014,
Author = {Dutta, Koushik and Bhattacharjee, Debotosh and Nasipuri, Mita},
Editor = {{Mukherjee, A and Goswami, RT}},
Title = {{TR-LBP: A modified Local Binary Pattern-based technique for 3D face
   recognition}},
Booktitle = {{PROCEEDINGS OF 2018 FIFTH INTERNATIONAL CONFERENCE ON EMERGING
   APPLICATIONS OF INFORMATION TECHNOLOGY (EAIT)}},
Series = {{Proceedings International Conference on Emerging Applications of
   Information Technology (EAIT)}},
Year = {{2018}},
Note = {{5th International Conference on Emerging Applications of Information
   Technology (EAIT), Kolkata, INDIA, JAN 12-13, 2018}},
Organization = {{Comp Soc India; IEEE Kolkata Sect; IEEE Young Professionals; SAP
   Partner; Dygitech Business Analytics Acad; TEQIP; Balani Infotech Pvt.
   Ltd.}},
Abstract = {{In this paper, a novel technique has been introduced for 3D face
   recognition based on the modified local binary pattern extracted from a
   3D range image. The new LBP technique is applied to shape index of 3D
   facial surface data. The novelty of this technique illustrates that a
   modified local binary pattern termed as Triangular Local Binary Pattern
   (TR-LBP), which gives new texture representation of 3D facial surface
   for improvement of facial texture classification performance compared to
   other variants of LBP. In this paper, authors have also described the
   TR-LBP technique by extending it on 2D intensity image of same subjects.
   Entropy-based feature extraction is used for feature vector creation.
   Further, KNN is used for calculating classification accuracy on two
   popular 3D face databases: Frav3D and Bosphorous. Here the
   classifications results are compared with other two existing LBP
   techniques applied to range images and shape index (SI) form of range
   image respectively.}},
ISSN = {{2165-0209}},
ISBN = {{978-1-5386-3719-7}},
Unique-ID = {{ISI:000454832200014}},
}

@inproceedings{ ISI:000454677900019,
Author = {Khan, Muhammad Sajid and Jehanzeb, Muhammad and Babar, Muhammad Imran
   and Faisal, Shah and Ullah, Zabeeh and Amin, Siti Zulaikha Binti Mohamad},
Editor = {{Miraz, MH and Excell, P and Ware, A and Soomro, S and Ali, M}},
Title = {{Face Recognition Analysis Using 3D Model}},
Booktitle = {{EMERGING TECHNOLOGIES IN COMPUTING, ICETIC 2018}},
Series = {{Lecture Notes of the Institute for Computer Sciences Social Informatics
   and Telecommunications Engineering}},
Year = {{2018}},
Volume = {{200}},
Pages = {{220-236}},
Note = {{1st International Conference on Emerging Technologies in Computing
   (ICETIC), London Metropolitan Univ, London, ENGLAND, AUG 23-24, 2018}},
Organization = {{Int Assoc Educators \& Researchers; IEEE ComSoc Bahrain Chapter; British
   Comp Soc, N Wales Branch; EAI}},
Abstract = {{Facial Recognition is a commonly used technology in security-related
   applications. It has been thoroughly studied and scrutinized for its
   number of practical real-world applications. On the road ahead of
   understanding this technology, there remain several obstacles. In this
   paper, methods of 3D face recognition are examined by measuring
   quantifiable applications and results. In facial recognition, three
   Dimensional Morphable Model (3DMM) techniques have attracted more and
   more attention as effectiveness in use increases over time. 3DMM
   provides automation and more accurate image rendering when compared to
   other traditional techniques. The accuracy in image rendering comes at a
   cost; as 3DMM requires more focus on texture estimation,
   shape-controlling limits, and extrinsic variations, accurately matching
   fitting models, feature tracking and precision identification. We have
   underlined different issues in comparison based on these methods.}},
DOI = {{10.1007/978-3-319-95450-9\_19}},
ISSN = {{1867-8211}},
ISBN = {{978-3-319-95450-9; 978-3-319-95449-3}},
Unique-ID = {{ISI:000454677900019}},
}

@inproceedings{ ISI:000451039807048,
Author = {Li, Shihua and Su, Lian and Liu, Yuhan and He, Ze},
Book-Group-Author = {{IEEE}},
Title = {{SEGMENTATION OF INDIVIDUAL TREES BASED ON A POINT CLOUD CLUSTERING
   METHOD USING AIRBORNE LIDAR DATA}},
Booktitle = {{IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING
   SYMPOSIUM}},
Series = {{IEEE International Symposium on Geoscience and Remote Sensing IGARSS}},
Year = {{2018}},
Pages = {{7520-7523}},
Note = {{38th IEEE International Geoscience and Remote Sensing Symposium
   (IGARSS), Valencia, SPAIN, JUL 22-27, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Geoscience
   \& Remote Sensing Soc; European Space Agcy}},
Abstract = {{The objective of this paper was to develop a new algorithm to segment
   individual trees directly by using the three-dimensional space
   characteristic of airborne light detection and ranging point cloud data.
   The local maximum method was used in the initial segmentation and the
   error identification tree exclusion. On the basis of the point cloud
   spatial distribution of individual trees and the adjacent relationship
   with the other trees, a point cloud clustering method was developed to
   decide the points belonging to the individual trees. This algorithm was
   tested by 6 forest plots in the Genhe forestry reserve. The results
   showed that this algorithm could segment individual trees quickly and
   accurately, and the overall accuracy of this algorithm was 96.3\%.}},
ISSN = {{2153-6996}},
ISBN = {{978-1-5386-7150-4}},
Unique-ID = {{ISI:000451039807048}},
}

@inproceedings{ ISI:000453087200079,
Author = {Wang, Zhong-min and Yin, Gui-lan and Wang, Rui-lai},
Book-Group-Author = {{DESTECH PUBLICAT INC}},
Title = {{Research on Mouth Adaptation and Facial Expression Synthesis of
   Three-dimensional Human Face}},
Booktitle = {{2018 INTERNATIONAL CONFERENCE ON ELECTRICAL, CONTROL, AUTOMATION AND
   ROBOTICS (ECAR 2018)}},
Series = {{DEStech Transactions on Engineering and Technology Research}},
Year = {{2018}},
Volume = {{307}},
Pages = {{448-453}},
Note = {{International Conference on Electrical, Control, Automation and Robotics
   (ECAR), Xiamen, PEOPLES R CHINA, SEP 16-17, 2018}},
Abstract = {{Based on the MPEG-4 standard, this paper uses emotion recognition
   technology to achieve a realistic facial animation with
   three-dimensional human faces driven by text. In the aspect of realizing
   facial animation with rich emotional changes, a Chinese syllable mouth
   type parameter database and 6 basic expression parameter libraries are
   established. Finally, weighted fusion of various action elements is
   performed on face animation parameters to output the animation sequence
   of human face.}},
ISSN = {{2475-885X}},
ISBN = {{978-1-6059-5579-7}},
Unique-ID = {{ISI:000453087200079}},
}

@inproceedings{ ISI:000450073500030,
Author = {Lim, Seong-Jae and Hwang, Bon-Woo and Yoon, Seung-Uk and Choi, Jin Sung
   and Park, Chang-Joon},
Editor = {{Mohanty, SP and Corcoran, P and Li, H and Sengupta, A and Lee, JH}},
Title = {{Automatic 3D Face Component Analysis Technique}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE)}},
Series = {{International Conference on Consumer Electronics}},
Year = {{2018}},
Note = {{IEEE International Conference on Consumer Electronics (ICCE), Las Vegas,
   NV, JAN 12-14, 2018}},
Organization = {{IEEE}},
Abstract = {{This paper proposes a technique to perform segmentation for the
   meaningful regions that part of the face captured by 3D scanners or 3D
   sensors, automatically. Each part recognition of the scanned face is
   vital for the 3D applications such as modeling, animation and 3D
   printing. We transfer the template model labeled with the meaningful
   part to the scanned face model to find the corresponding part of each
   meaningful part of the template model. This technique can be used to the
   several applications such as 3D face modeling, facial animation, virtual
   facial surgery and 3D printing.}},
ISBN = {{978-1-5386-3025-9}},
Unique-ID = {{ISI:000450073500030}},
}

@inproceedings{ ISI:000450395900012,
Author = {Tu, Min},
Editor = {{Wu, A}},
Title = {{Research on Face Recognition Technology Based on Computer Neural Network}},
Booktitle = {{2018 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL \& ELECTRONICS
   ENGINEERING AND COMPUTER SCIENCE (ICEEECS 2018)}},
Year = {{2018}},
Pages = {{49-52}},
Note = {{5th International Conference on Electrical \& Electronics Engineering
   and Computer Science (ICEEECS), Beijing, PEOPLES R CHINA, JUN 29-30,
   2018}},
Abstract = {{Face recognition undergoes three stages of full reliance on artificial,
   human-computer interaction, and machine automatic recognition, free from
   the initial recognition of a single frontal grayscale image to the
   development of three-dimensional face recognition on the basis of
   research on multi-pose face recognition with the realization of dynamic
   face recognition as a carrier, and achieves certain results. In China,
   although face recognition technology started late, it has developed
   rapidly and received strong support from the country. At present, many
   universities and research institutes in China have very good research
   foundations in the field of image processing and pattern recognition,
   and have actively carried out basic research on human biometric
   recognition technology including face recognition, and have achieved
   good results. Research results show that in the near future, China's
   research in this field will enter the world's advanced ranks. Face
   recognition is one of the most classical problems in the field of
   identification. At present, there are many solutions and good
   experimental results obtained. However, a general-purpose face
   recognition system that can be practically applied and used for
   arbitrary backgrounds and arbitrary gestures has not yet appeared.
   Therefore, face recognition goes into the application field to improve
   the effectiveness of the extracted facial features and classifier
   optimization, and etc.}},
ISBN = {{978-1-912407-04-0}},
Unique-ID = {{ISI:000450395900012}},
}

@inproceedings{ ISI:000450395900040,
Author = {Zhao Weizheng and Tang Weiwei},
Editor = {{Wu, A}},
Title = {{Improvement Measures of 3D Face Recognition Algorithm Based on Features
   of Facial Contour Curve}},
Booktitle = {{2018 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL \& ELECTRONICS
   ENGINEERING AND COMPUTER SCIENCE (ICEEECS 2018)}},
Year = {{2018}},
Pages = {{181-186}},
Note = {{5th International Conference on Electrical \& Electronics Engineering
   and Computer Science (ICEEECS), Beijing, PEOPLES R CHINA, JUN 29-30,
   2018}},
Abstract = {{An improved algorithm based on the feature of facial contour curve is
   proposed for 3D face recognition. The paper first analyzes the
   physiological structure of human face, extracts the feature points in
   the 3D face, and then extracts the face contour line, and combines them
   to form the feature model to realize the 3D face recognition. The
   experimental results show that this method can effectively improve the
   three-dimensional face recognition rate with strong anti-interference
   ability.}},
ISBN = {{978-1-912407-04-0}},
Unique-ID = {{ISI:000450395900040}},
}

@article{ ISI:000449193300001,
Author = {Lv, Chenlei and Zhao, Junli},
Title = {{3D Face Recognition based on Local Conformal Parameterization and
   Iso-Geodesic Stripes Analysis}},
Journal = {{MATHEMATICAL PROBLEMS IN ENGINEERING}},
Year = {{2018}},
Abstract = {{3D face recognition is an important topic in the field of pattern
   recognition and computer graphic. We propose a novel approach for 3D
   face recognition using local conformal parameterization and iso-geodesic
   stripes. In our framework, the 3D facial surface is considered as a
   Riemannian 2-manifold. The surface is mapped into the 2D circle
   parameter domain using local conformal parameterization. In the
   parameter domain, the geometric features are extracted from the
   iso-geodesic stripes. Combining the relative position measure, Chain 2D
   Weighted Walkthroughs (C2DWW), the 3D face matching results can be
   obtained. The geometric features from iso-geodesic stripes in parameter
   domain are robust in terms of head poses, facial expressions, and some
   occlusions. In the experiments, our method achieves a high recognition
   accuracy of 3D facial data from the Texas3D and Bosphorus3D face
   database.}},
DOI = {{10.1155/2018/4707954}},
Article-Number = {{4707954}},
ISSN = {{1024-123X}},
EISSN = {{1563-5147}},
Unique-ID = {{ISI:000449193300001}},
}

@inproceedings{ ISI:000446394502083,
Author = {Mitash, Chaitanya and Boularias, Abdeslam and Bekris, Kostas E.},
Book-Group-Author = {{IEEE}},
Title = {{Improving 6D Pose Estimation of Objects in Clutter via Physics-aware
   Monte Carlo Tree Search}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)}},
Series = {{IEEE International Conference on Robotics and Automation ICRA}},
Year = {{2018}},
Pages = {{3331-3338}},
Note = {{IEEE International Conference on Robotics and Automation (ICRA),
   Brisbane, AUSTRALIA, MAY 21-25, 2018}},
Organization = {{IEEE; CSIRO; Australian Govt, Dept Def Sci \& Technol; DJI; Queensland
   Univ Technol; Woodside; Baidu; Bosch; Houston Mechatron; Kinova Robot;
   KUKA; Hit Robot Grp; Honda Res Inst; iRobot; Mathworks; NuTonomy;
   Ouster; Uber}},
Abstract = {{This work proposes a process for efficiently searching over combinations
   of individual object 6D pose hypotheses in cluttered scenes, especially
   in cases involving occlusions and objects resting on each other. The
   initial set of candidate object poses is generated from state-of-the-art
   object detection and global point cloud registration techniques. The
   best scored pose per object by using these techniques may not be
   accurate due to overlaps and occlusions. Nevertheless, experimental
   indications provided in this work show that object poses with lower
   ranks may be closer to the real poses than ones with high ranks
   according to registration techniques. This motivates a global
   optimization process for improving these poses by taking into account
   scene-level physical interactions between objects. It also implies that
   the Cartesian product of candidate poses for interacting objects must be
   searched so as to identify the best scene-level hypothesis. To perform
   the search efficiently, the candidate poses for each object are
   clustered so as to reduce their number but still keep a sufficient
   diversity. Then, searching over the combinations of candidate object
   poses is performed through a Monte Carlo Tree Search (MCTS) process that
   uses the similarity between the observed depth image of the scene and a
   rendering of the scene given the hypothesized pose as a score that
   guides the search procedure. MCTS handles in a principled way the
   tradeoff between fine-tuning the most promising poses and exploring new
   ones, by using the Upper Confidence Bound (UCB) technique. Experimental
   results indicate that this process is able to quickly identify in
   cluttered scenes physically-consistent object poses that are
   significantly closer to ground truth compared to poses found by point
   cloud registration methods.}},
ISSN = {{1050-4729}},
ISBN = {{978-1-5386-3081-5}},
Unique-ID = {{ISI:000446394502083}},
}

@inproceedings{ ISI:000434349200016,
Author = {Kowalski, Marek and Nasarzewski, Zbigniew and Galinski, Grzegorz and
   Garbat, Piotr},
Book-Group-Author = {{IEEE}},
Title = {{HoloFace: Augmenting Human-to-Human Interactions on HoloLens}},
Booktitle = {{2018 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV
   2018)}},
Series = {{IEEE Winter Conference on Applications of Computer Vision}},
Year = {{2018}},
Pages = {{141-149}},
Note = {{18th IEEE Winter Conference on Applications of Computer Vision (WACV),
   NV, MAR 12-15, 2018}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometr Council; Cognex; Google; Honeywell;
   Kitware; Netflix; SAP; Amazon; Percept Automata; Verisk Analyt}},
Abstract = {{We present HoloFace, an open-source framework for face alignment, head
   pose estimation and facial attribute retrieval for Microsoft HoloLens.
   HoloFace implements two state-of-the-art face alignment methods which
   can be used interchangeably: one running locally and one running on a
   remote backend. Head pose estimation is accomplished by fitting a
   deformable 3D model to the landmarks localized using face alignment. The
   head pose provides both the rotation of the head and a position in the
   world space. The parameters of the fitted 3D face model provide
   estimates of facial attributes such as mouth opening or smile. Together
   the above information can be used to augment the faces of people seen by
   the HoloLens user, and thus their interaction. Potential usage scenarios
   include facial recognition, emotion recognition, eye gaze tracking and
   many others. We demonstrate the capabilities of our framework by
   augmenting the faces of people seen through the HoloLens with various
   objects and animations.}},
DOI = {{10.1109/WACV.2018.00022}},
ISSN = {{2472-6737}},
ISBN = {{978-1-5386-4886-5}},
ORCID-Numbers = {{Galinski, Grzegorz/0000-0003-0223-3265}},
Unique-ID = {{ISI:000434349200016}},
}

@inproceedings{ ISI:000434996800035,
Author = {Hada, Momoko and Yamada, Ryoko and Akamatsu, Shigeru},
Book-Group-Author = {{IEEE}},
Title = {{How does the transformation of an avatar face giving a favorable
   impression affect human recognition of the face?}},
Booktitle = {{2018 INTERNATIONAL WORKSHOP ON ADVANCED IMAGE TECHNOLOGY (IWAIT)}},
Series = {{Proceedings of International Workshop on Advanced Image Technology}},
Year = {{2018}},
Note = {{International Workshop on Advanced Image Technology (IWAIT), Chiang Mai,
   THAILAND, JAN 07-09, 2018}},
Abstract = {{We investigated how different appearances in the favorable impressions
   of 3D avatar faces affect face-recognition performances by humans. We
   conducted an encoding and testing experiment using synthesized facial
   images and artificially manipulated the strength of the perceived
   impressions in three different dimensions. We also subjectively assessed
   the favorability of the synthesized faces that were used as visual
   stimuli in face-recognition tests and found that facial transformation,
   which decreased the favorability impressions, generally deteriorates
   human face-recognition performance.}},
ISSN = {{2306-2274}},
ISBN = {{978-1-5386-2615-3}},
Unique-ID = {{ISI:000434996800035}},
}

@article{ ISI:000433431400011,
Author = {Ullah, Faizan and Shah, Sabir and Shah, Dilawar and Abdusalam and Ali,
   Shujaat},
Title = {{Protocol for Systematic Literature Review of Face Recognition in
   Uncontrolled Environment}},
Journal = {{EAI ENDORSED TRANSACTIONS ON SCALABLE INFORMATION SYSTEMS}},
Year = {{2018}},
Volume = {{4}},
Number = {{16}},
Abstract = {{One of the major challenges encountered by current face recognition
   techniques lies in the difficulties of handling varying poses, i.e.,
   recognition of faces in arbitrary in-depth rotations. The face image
   differences caused by rotations are often larger than the inter-person
   differences used in distinguishing identities. Face recognition across
   pose, on the other hand, has great potentials in many applications
   dealing with uncooperative subjects, in which the full power of face
   recognition being a passive biometric technique can be implemented and
   utilized. Extensive efforts have been put into the research toward
   pose-invariant face recognition in recent years and many prominent
   approaches have been proposed. However, several issues in face
   recognition across pose still remain open, such as lack of understanding
   about subspaces of pose variant images, problem intractability in 3D
   face modelling, complex face surface reflection mechanism, etc. This
   paper provides a critical survey of researches on image-based face
   recognition across pose. The existing techniques are comprehensively
   reviewed and discussed. They are classified into different categories
   according to their methodologies in handling pose variations. Their
   strategies, advantages/disadvantages and performances are elaborated. By
   generalizing different tactics in handling pose variations and
   evaluating their performances, several promising.}},
DOI = {{10.4108/eai.13-4-2018.154477}},
Article-Number = {{11}},
ISSN = {{2032-9407}},
Unique-ID = {{ISI:000433431400011}},
}

@article{ ISI:000428322500017,
Author = {Mohanraj, V. and Chakkaravarthy, S. Sibi and Gogul, I. and Kumar, V.
   Sathiesh and Kumar, Ranajit and Vaidehi, V.},
Title = {{Hybrid feature descriptors to detect face spoof attacks}},
Journal = {{JOURNAL OF INTELLIGENT \& FUZZY SYSTEMS}},
Year = {{2018}},
Volume = {{34}},
Number = {{3}},
Pages = {{1411-1419}},
Note = {{3rd International Symposium on Intelligent Systems Technologies and
   Applications (ISTA), Manipal Univ, Manipal, INDIA, SEP 13-16, 2017}},
Abstract = {{Face Recognition is widely used applications such as of mobile phone
   unlocking, credit card authentication and person authentication in
   airports. The face biometric authentication system can be easily spoofed
   by printed photograph, replay video of the legitimate user and 3D face
   mask. This paper proposes hybrid feature descriptors to detect the face
   spoofing attack (printed photograph and replay video attacks). The
   proposed method extracts three different feature descriptors such as
   Color moment, Haralick texture and Color Local Binary Pattern (CLBP)
   feature descriptors. The extracted features are concatenated and
   classified by Logistic Regression. The performance of the proposed
   method is evaluated on the Michigan State University Mobile Face
   Spoofing Database (MSU-MFSD) dataset and found to achieve better results
   than state-of-the-art methods.}},
DOI = {{10.3233/JIFS-169436}},
ISSN = {{1064-1246}},
EISSN = {{1875-8967}},
Unique-ID = {{ISI:000428322500017}},
}

@article{ ISI:000424092300038,
Author = {Zhou, Tan and Popescu, Sorin C. and Lawing, A. Michelle and Eriksson,
   Marian and Strimbu, Bogdan M. and Buerkner, Paul C.},
Title = {{Bayesian and Classical Machine Learning Methods: A Comparison for Tree
   Species Classification with LiDAR Waveform Signatures}},
Journal = {{REMOTE SENSING}},
Year = {{2018}},
Volume = {{10}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{A plethora of information contained in full-waveform (FW) Light
   Detection and Ranging (LiDAR) data offers prospects for characterizing
   vegetation structures. This study aims to investigate the capacity of FW
   LiDAR data alone for tree species identification through the integration
   of waveform metrics with machine learning methods and Bayesian
   inference. Specifically, we first conducted automatic tree segmentation
   based on the waveform-based canopy height model (CHM) using three
   approaches including TreeVaW, watershed algorithms and the combination
   of TreeVaW and watershed (TW) algorithms. Subsequently, the Random
   forests (RF) and Conditional inference forests (CF) models were employed
   to identify important tree-level waveform metrics derived from three
   distinct sources, such as raw waveforms, composite waveforms, the
   waveform-based point cloud and the combined variables from these three
   sources. Further, we discriminated tree (gray pine, blue oak, interior
   live oak) and shrub species through the RF, CF and Bayesian multinomial
   logistic regression (BMLR) using important waveform metrics identified
   in this study. Results of the tree segmentation demonstrated that the TW
   algorithms outperformed other algorithms for delineating individual tree
   crowns. The CF model overcomes waveform metrics selection bias caused by
   the RF model which favors correlated metrics and enhances the accuracy
   of subsequent classification. We also found that composite waveforms are
   more informative than raw waveforms and waveform-based point cloud for
   characterizing tree species in our study area. Both classical machine
   learning methods (the RF and CF) and the BMLR generated satisfactory
   average overall accuracy (74\% for the RF, 77\% for the CF and 81\% for
   the BMLR) and the BMLR slightly outperformed the other two methods.
   However, these three methods suffered from low individual classification
   accuracy for the blue oak which is prone to being misclassified as the
   interior live oak due to the similar characteristics of blue oak and
   interior live oak. Uncertainty estimates from the BMLR method compensate
   for this downside by providing classification results in a probabilistic
   sense and rendering users with more confidence in interpreting and
   applying classification results to real-world tasks such as forest
   inventory. Overall, this study recommends the CF method for feature
   selection and suggests that BMLR could be a superior alternative to
   classical machining learning methods.}},
DOI = {{10.3390/rs10010039}},
Article-Number = {{39}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Lawing, Michelle/F-7453-2019
   Popescu, Sorin C/D-5981-2015}},
ORCID-Numbers = {{Lawing, Michelle/0000-0003-4041-6177
   Popescu, Sorin C/0000-0002-8155-8801}},
Unique-ID = {{ISI:000424092300038}},
}

@article{ ISI:000423587100013,
Author = {Zhao, Jun-Li and Wu, Zhong-Ke and Pan, Zhen-Kuan and Duan, Fu-Qing and
   Li, Jin-Hua and Lv, Zhi-Han and Wang, Kang and Chen, Yu-Cong},
Title = {{3D Face Similarity Measure by Fr,chet Distances of Geodesics}},
Journal = {{JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY}},
Year = {{2018}},
Volume = {{33}},
Number = {{1}},
Pages = {{207-222}},
Month = {{JAN}},
Abstract = {{3D face similarity is a critical issue in computer vision, computer
   graphics and face recognition and so on. Since Fr,chet distance is an
   effective metric for measuring curve similarity, a novel 3D face
   similarity measure method based on Fr,chet distances of geodesics is
   proposed in this paper. In our method, the surface similarity between
   two 3D faces is measured by the similarity between two sets of 3D curves
   on them. Due to the intrinsic property of geodesics, we select geodesics
   as the comparison curves. Firstly, the geodesics on each 3D facial model
   emanating from the nose tip point are extracted in the same initial
   direction with equal angular increment. Secondly, the Fr,chet distances
   between the two sets of geodesics on the two compared facial models are
   computed. At last, the similarity between the two facial models is
   computed based on the Fr,chet distances of the geodesics obtained in the
   second step. We verify our method both theoretically and practically. In
   theory, we prove that the similarity of our method satisfies three
   properties: reflexivity, symmetry, and triangle inequality. And in
   practice, experiments are conducted on the open 3D face database GavaDB,
   Texas 3D Face Recognition database, and our 3D face database. After the
   comparison with iso-geodesic and Hausdorff distance method, the results
   illustrate that our method has good discrimination ability and can not
   only identify the facial models of the same person, but also distinguish
   the facial models of any two different persons.}},
DOI = {{10.1007/s11390-018-1814-7}},
ISSN = {{1000-9000}},
EISSN = {{1860-4749}},
Unique-ID = {{ISI:000423587100013}},
}

@article{ ISI:000422943700008,
Author = {Benedek, Csaba and Galai, Bence and Nagy, Balazs and Janko, Zsolt},
Title = {{Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance
   System}},
Journal = {{IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY}},
Year = {{2018}},
Volume = {{28}},
Number = {{1}},
Pages = {{101-113}},
Month = {{JAN}},
Abstract = {{This paper presents new approaches for gait and activity analysis based
   on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed
   algorithms are embedded into an integrated 4D vision and visualization
   system, which is able to analyze and interactively display real
   scenarios in natural outdoor environments with walking pedestrians. The
   main focus of the investigations is gait-based person reidentification
   during tracking and recognition of specific activity patterns, such as
   bending, waving, making phone calls, and checking the time looking at
   wristwatches. The descriptors for training and recognition are observed
   and extracted from realistic outdoor surveillance scenarios, where
   multiple pedestrians are walking in the field of interest following
   possibly intersecting trajectories; thus, the observations might often
   be affected by occlusions or background noise. Since there is no public
   database available for such scenarios, we created and published a new
   Lidar-based outdoor gait and activity data set on our website that
   contains point cloud sequences of 28 different persons extracted and
   aggregated from 35-min-long measurements. The presented results confirm
   that both efficient gait-based identification and activity recognition
   are achievable in the sparse point clouds of a single RMB Lidar sensor.
   After extracting the people trajectories, we synthesized a
   free-viewpoint video, in which moving avatar models follow the
   trajectories of the observed pedestrians in real time, ensuring that the
   leg movements of the animated avatars are synchronized with the real
   gait cycles observed in the Lidar stream.}},
DOI = {{10.1109/TCSVT.2016.2595331}},
ISSN = {{1051-8215}},
EISSN = {{1558-2205}},
ORCID-Numbers = {{Benedek, Csaba/0000-0003-3203-0741}},
Unique-ID = {{ISI:000422943700008}},
}

@article{ ISI:000418513500004,
Author = {Sghaier, Souhir and Farhat, Wajdi and Souani, Chokri},
Title = {{Novel Technique for 3D Face Recognition Using Anthropometric Methodology}},
Journal = {{INTERNATIONAL JOURNAL OF AMBIENT COMPUTING AND INTELLIGENCE}},
Year = {{2018}},
Volume = {{9}},
Number = {{1}},
Pages = {{60-77}},
Month = {{JAN-MAR}},
Abstract = {{This manuscript presents an improved system research that can detect and
   recognize the person in 3D space automatically and without the
   interaction of the people's faces. This system is based not only on a
   quantum computation and measurements to extract the vector features in
   the phase of characterization but also on learning algorithm (using SVM)
   to classify and recognize the person. This research presents an improved
   technique for automatic 3D face recognition using anthropometric
   proportions and measurement to detect and extract the area of interest
   which is unaffected by facial expression. This approach is able to treat
   incomplete and noisy images and reject the non-facial areas
   automatically. Moreover, it can deal with the presence of holes in the
   meshed and textured 3D image. It is also stable against small
   translation and rotation of the face. All the experimental tests have
   been done with two 3D face datasets FRAV 3D and GAVAB. Therefore, the
   test's results of the proposed approach are promising because they
   showed that it is competitive comparable to similar approaches in terms
   of accuracy, robustness, and flexibility. It achieves a high recognition
   performance rate of 95.35\% for faces with neutral and non-neutral
   expressions for the identification and 98.36\% for the authentification
   with GAVAB and 100\% with some gallery of FRAV 3D datasets.}},
DOI = {{10.4018/IJACI.2018010104}},
ISSN = {{1941-6237}},
EISSN = {{1941-6245}},
ResearcherID-Numbers = {{Farhat, Wajdi/N-5341-2015
   Souani, Chokri/B-1853-2015}},
ORCID-Numbers = {{Farhat, Wajdi/0000-0003-3647-8316
   Souani, Chokri/0000-0002-8987-3582}},
Unique-ID = {{ISI:000418513500004}},
}

@article{ ISI:000416263200003,
Author = {Ferrari, Claudio and Lisanti, Giuseppe and Berretti, Stefano and Del
   Bimbo, Alberto},
Title = {{A Dictionary Learning-Based 3D Morphable Shape Model}},
Journal = {{IEEE TRANSACTIONS ON MULTIMEDIA}},
Year = {{2017}},
Volume = {{19}},
Number = {{12}},
Pages = {{2666-2679}},
Month = {{DEC}},
Abstract = {{Face analysis from 2D images and videos is a central task in many
   multimedia applications. Methods developed to this end perform either
   face recognition or facial expression recognition, and in both cases
   results are negatively influenced by variations in pose, illumination,
   and resolution of the face. Such variations have a lower impact on 3D
   face data, which has given the way to the idea of using a 3D morphable
   model as an intermediate tool to enhance face analysis on 2D data. In
   this paper, we propose a new approach for constructing a 3D morphable
   shape model (called DL-3DMM) and show our solution can reach the
   accuracy of deformation required in applications where fine details of
   the face are concerned. For constructing the model, we start from a set
   of 3D face scans with large variability in terms of ethnicity and
   expressions. Across these training scans, we compute a point-to-point
   dense alignment, which is accurate also in the presence of topological
   variations of the face. The DL-3DMM is constructed by learning a
   dictionary of basis components on the aligned scans. The model is then
   fitted to 2D target faces using an efficient regularized
   ridge-regression guided by 2D/3D facial landmark correspondences in
   order to generate pose-normalized face images. Comparison between the
   DL-3DMM and the standard PCA-based 3DMM demonstrates that in general a
   lower reconstruction error can be obtained with our solution.
   Application to action unit detection and emotion recognition from 2D
   images and videos shows competitive results with state of the art
   methods on two benchmark datasets.}},
DOI = {{10.1109/TMM.2017.2707341}},
ISSN = {{1520-9210}},
EISSN = {{1941-0077}},
ORCID-Numbers = {{Berretti, Stefano/0000-0003-1219-4386
   Lisanti, Giuseppe/0000-0002-0785-9972}},
Unique-ID = {{ISI:000416263200003}},
}

@article{ ISI:000416263200014,
Author = {Li, Huibin and Sun, Jian and Xu, Zongben and Chen, Liming},
Title = {{Multimodal 2D+3D Facial Expression Recognition With Deep Fusion
   Convolutional Neural Network}},
Journal = {{IEEE TRANSACTIONS ON MULTIMEDIA}},
Year = {{2017}},
Volume = {{19}},
Number = {{12}},
Pages = {{2816-2831}},
Month = {{DEC}},
Abstract = {{This paper presents a novel and efficient deep fusion convolutional
   neural network (DF-CNN) for multimodal 2D+3D facial expression
   recognition (FER). DF-CNN comprises a feature extraction subnet, a
   feature fusion subnet, and a softmax layer. In particular, each textured
   three-dimensional (3D) face scan is represented as six types of 2D
   facial attribute maps (i.e., geometry map, three normal maps, curvature
   map, and texture map), all of which are jointly fed into DF-CNN for
   feature learning and fusion learning, resulting in a highly concentrated
   facial representation (32-dimensional). Expression prediction is
   performed by two ways: 1) learning linear support vector machine
   classifiers using the 32-dimensional fused deep features, or 2) directly
   performing softmax prediction using the six-dimensional expression
   probability vectors. Different from existing 3D FER methods, DF-CNN
   combines feature learning and fusion learning into a single end-to-end
   training framework. To demonstrate the effectiveness of DF-CNN, we
   conducted comprehensive experiments to compare the performance of DF-CNN
   with handcrafted features, pre-trained deep features, fine-tuned deep
   features, and state-of-the-art methods on three 3D face datasets (i.e.,
   BU-3DFE Subset I, BU-3DFE Subset II, and Bosphorus Subset). In all
   cases, DF-CNN consistently achieved the best results. To the best of our
   knowledge, this is the first work of introducing deep CNN to 3D FER and
   deep learning-based feature level fusion for multimodal 2D+3D FER.}},
DOI = {{10.1109/TMM.2017.2713408}},
ISSN = {{1520-9210}},
EISSN = {{1941-0077}},
ORCID-Numbers = {{Sun, Jian/0000-0001-7206-0641}},
Unique-ID = {{ISI:000416263200014}},
}

@article{ ISI:000411545400029,
Author = {Soltanpour, Sima and Boufama, Boubakeur and Wu, Q. M. Jonathan},
Title = {{A survey of local feature methods for 3D face recognition}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2017}},
Volume = {{72}},
Pages = {{391-406}},
Month = {{DEC}},
Abstract = {{One of the main modules in a face recognition system is feature
   extraction, which has a significant effect on the whole system
   performance. In the past decades, various types of feature extractors
   and descriptors have been proposed for 3D face recognition. Although
   several literature reviews have been carried out on 3D face recognition
   algorithms, only a few studies have been performed on feature extraction
   methods. The latter have a vital role to overcome degradation
   conditions, such as face expression variations and occlusions. Depending
   on the types of features used in 3D face recognition, these methods can
   be divided into two categories: global and local feature-based methods.
   Local feature-based methods have been effectively applied in the
   literature, as they are more robust to occlusions and missing data. This
   survey presents a state-of-the-art for 3D face recognition using local
   features, with the main focus being the extraction of these features.
   (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2017.08.003}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000411545400029}},
}

@article{ ISI:000419961800018,
Author = {Liu, Zexi and Cohen, Fernand},
Title = {{Synthesis and identification of three-dimensional faces from image(s)
   and three-dimensional generic models}},
Journal = {{JOURNAL OF ELECTRONIC IMAGING}},
Year = {{2017}},
Volume = {{26}},
Number = {{6}},
Month = {{NOV}},
Abstract = {{We describe an approach for synthesizing a three-dimensional (3-D) face
   structure from an image or images of a human face taken at a priori
   unknown poses using gender and ethnicity specific 3-D generic models.
   The synthesis process starts with a generic model, which is personalized
   as images of the person become available using preselected landmark
   points that are tessellated to form a high-resolution triangular mesh.
   From a single image, two of the three coordinates of the model are
   reconstructed in accordance with the given image of the person, while
   the third coordinate is sampled from the generic model, and the
   appearance is made in accordance with the image. With multiple images,
   all coordinates and appearance are reconstructed in accordance with the
   observed images. This method allows for accurate pose estimation as well
   as face identification in 3-D rendering of a difficult two-dimensional
   (2-D) face recognition problem into a much simpler 3-D surface matching
   problem. The estimation of the unknown pose is achieved using the
   Levenberg-Marquardt optimization process. Encouraging experimental
   results are obtained in a controlled environment with high-resolution
   images under a good illumination condition, as well as for images taken
   in an uncontrolled environment under arbitrary illumination with
   low-resolution cameras. (C) 2017 SPIE and IS\&T}},
DOI = {{10.1117/1.JEI.26.6.063005}},
Article-Number = {{063005}},
ISSN = {{1017-9909}},
EISSN = {{1560-229X}},
Unique-ID = {{ISI:000419961800018}},
}

@article{ ISI:000414883800006,
Author = {Desrosiers, Paul Audain and Bennis, Yasmine and Daoudi, Mohamed and Ben
   Amor, Boulbaba and Guerreschi, Pierre},
Title = {{Analyzing of facial paralysis by shape analysis of 3D face sequences}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{67}},
Pages = {{67-88}},
Month = {{NOV}},
Abstract = {{In this paper, we address the problem of quantifying the facial
   asymmetry from 3D face sequence (4D). We investigate the role of 4D data
   to reveal the amount of both static and dynamic asymmetry in the
   clinical case of facial paralysis. The goal is to provide tools to
   clinicians to evaluate quantitatively facial paralysis treatment based
   on Botulinum Toxin (BT), which can provide qualitative and quantitative
   evaluations. To this end, Dense Scalar Fields (DSFs), based on
   Riemannian analysis of 3D facial shape, is proposed to quantify facial
   deformations. To assess this approach, a new 3D facial sequences of 16
   patients data set is collected, before and after injecting the BT. For
   each patient, we have collected 8 facial expressions before and after
   injecting BT. Experimental results obtained on this data set show that
   the proposed approach allows clinicians to evaluate more accurately the
   facial asymmetry before and after the treatment. (C) 2017 Elsevier B.V.
   All rights reserved.}},
DOI = {{10.1016/j.imavis.2017.08.006}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018
   Daoudi, Mohammed/H-5935-2013}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305
   guerreschi, pierre/0000-0002-2955-5291
   Daoudi, Mohammed/0000-0003-4219-7860}},
Unique-ID = {{ISI:000414883800006}},
}

@article{ ISI:000413880800004,
Author = {Shui, Wuyang and Zhou, Mingquan and Maddock, Steve and He, Taiping and
   Wang, Xingce and Deng, Qingqiong},
Title = {{A PCA-Based method for determining craniofacial relationship and sexual
   dimorphism of facial shapes}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2017}},
Volume = {{90}},
Pages = {{33-49}},
Month = {{NOV 1}},
Abstract = {{Previous studies have used principal component analysis (PCA) to
   investigate the craniofacial relationship, as well as sex determination
   using facial factors. However, few studies have investigated the extent
   to which the choice of principal components (PCs) affects the analysis
   of craniofacial relationship and sexual dimorphism. In this paper, we
   propose a PCA-based method for visual and quantitative analysis, using
   140 samples of 3D heads (70 male and 70 female), produced from computed
   tomography (CT) images. There are two parts to the method. First, skull
   and facial landmarks are manually marked to guide the model's
   registration so that dense corresponding vertices occupy the same
   relative position in every sample. Statistical shape spaces of the skull
   and face in dense corresponding vertices are constructed using PCA.
   Variations in these vertices, captured in every principal component
   (PC), are visualized to observe shape variability. The correlations of
   skull- and face-based PC scores are analysed, and linear regression is
   used to fit the craniofacial relationship. We compute the PC
   coefficients of a face based on this craniofacial relationship and the
   PC scores of a skull, and apply the coefficients to estimate a 3D face
   for the skull. To evaluate the accuracy of the computed craniofacial
   relationship, the mean and standard deviation of every vertex between
   the two models are computed, where these models are reconstructed using
   real PC scores and coefficients. Second, each PC in facial space is
   analysed for sex determination, for which support vector machines (SVMs)
   are used. We examined the correlation between PCs and sex, and explored
   the extent to which the choice of PCs affects the expression of sexual
   dimorphism. Our results suggest that skull- and face-based PCs can be
   used to describe the craniofacial relationship and that the accuracy of
   the method can be improved by using an increased number of face-based
   PCs. The results show that the accuracy of the sex classification is
   related to the choice of PCs. The highest sex classification rate is
   91.43\% using our method.}},
DOI = {{10.1016/j.compbiomed.2017.08.023}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
ResearcherID-Numbers = {{Maddock, Steve/J-1849-2016}},
ORCID-Numbers = {{Maddock, Steve/0000-0003-3179-0263}},
Unique-ID = {{ISI:000413880800004}},
}

@article{ ISI:000410197200003,
Author = {Tang, Yinhang and Li, Huibin and Sun, Xiang and Morvan, Jean-Marie and
   Chen, Liming},
Title = {{Principal Curvature Measures Estimation and Application to 3D Face
   Recognition}},
Journal = {{JOURNAL OF MATHEMATICAL IMAGING AND VISION}},
Year = {{2017}},
Volume = {{59}},
Number = {{2}},
Pages = {{211-233}},
Month = {{OCT}},
Abstract = {{This paper presents an effective 3D face keypoint detection, description
   and matching framework based on three principle curvature measures.
   These measures give a unified definition of principle curvatures for
   both smooth and discrete surfaces. They can be reasonably computed based
   on the normal cycle theory and the geometric measure theory. The strong
   theoretical basis of these measures provides us a solid discrete
   estimation method on real 3D face scans represented as triangle meshes.
   Based on these estimated measures, the proposed method can automatically
   detect a set of sparse and discriminating 3D facial feature points. The
   local facial shape around each 3D feature point is comprehensively
   described by histograms of these principal curvature measures. To
   guarantee the pose invariance of these descriptors, three principle
   curvature vectors of these principle curvature measures are employed to
   assign the canonical directions. Similarity comparison between faces is
   accomplished by matching all these curvature-based local shape
   descriptors using the sparse representation-based reconstruction method.
   The proposed method was evaluated on three public databases, i.e. FRGC
   v2.0, Bosphorus, and Gavab. Experimental results demonstrated that the
   three principle curvature measures contain strong complementarity for 3D
   facial shape description, and their fusion can largely improve the
   recognition performance. Our approach achieves rank-one recognition
   rates of 99.6, 95.7, and 97.9\% on the neutral subset, expression
   subset, and the whole FRGC v2.0 databases, respectively. This indicates
   that our method is robust to moderate facial expression variations.
   Moreover, it also achieves very competitive performance on the pose
   subset (over 98.6\% except Yaw 90A degrees) and the occlusion subset
   (98.4\%) of the Bosphorus database. Even in the case of extreme pose
   variations like profiles, it also significantly outperforms the
   state-of-the-art approaches with a recognition rate of 57.1\%. The
   experiments carried out on the Gavab databases further demonstrate the
   robustness of our method to varies head pose variations.}},
DOI = {{10.1007/s10851-017-0728-2}},
ISSN = {{0924-9907}},
EISSN = {{1573-7683}},
Unique-ID = {{ISI:000410197200003}},
}

@article{ ISI:000409180500015,
Author = {Sui, Dan and Hou, Deheng and Duan, Xinyu},
Title = {{An interpolation algorithm fitted for dynamic 3D face reconstruction}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2017}},
Volume = {{76}},
Number = {{19}},
Pages = {{19575-19589}},
Month = {{OCT}},
Abstract = {{In order to solve the problem of low recognition accuracy in later
   period which is caused by the too few extracted parameters in the 3D
   face recognition, and the incapable formation of completed point cloud
   structure. An automatic iterative interpolation algorithm is proposed.
   The new and more accurate 3D face data points are obtained by automatic
   iteration. This algorithm can be used to restore the data point cloud
   information of 3D facial feature in 2D images by means of facial
   three-legged structure formed by 3D face and automatic interpolation.
   Thus, it can realize to shape the 3D facial dynamic model which can be
   recognized and has high saturability. Experimental results show that the
   interpolation algorithm can achieve the complete the construction of
   facial feature based on the facial feature after 3D dynamic
   reconstruction, and the validity is higher.}},
DOI = {{10.1007/s11042-015-3233-x}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000409180500015}},
}

@article{ ISI:000408249500016,
Author = {Deng, Xing and Da, Feipeng and Shao, Haijian},
Title = {{Adaptive feature selection based on reconstruction residual and
   accurately located landmarks for expression-robust 3D face recognition}},
Journal = {{SIGNAL IMAGE AND VIDEO PROCESSING}},
Year = {{2017}},
Volume = {{11}},
Number = {{7}},
Pages = {{1305-1312}},
Month = {{OCT}},
Abstract = {{A novel adaptive feature selection based on reconstruction residual and
   accurately located landmarks for expression-robust 3D face recognition
   is proposed in this paper. Firstly, the novel facial coarse-to-fine
   landmarks localization method based on Active Shape Model and Gabor
   wavelets transformation is proposed to exactly and automatically locate
   facial landmarks in range image. Secondly, the multi-scale fusion of the
   pyramid local binary patterns (F-PLBP) based on the irregular
   segmentation associated with the located landmarks is proposed to
   extract the discriminative feature. Thirdly, a sparse
   representation-based classifier based on the adaptive feature selection
   (A-SRC) using the distribution of the reconstruction residual is
   presented to select the expression-robust feature and identify the
   faces. Finally, the experimental evaluation based on FRGC v2.0 indicates
   that the adaptive feature selection method using F-PLBP combined with
   the A-SRC can obtain the high recognition accuracy by performing the
   higher discriminative power to overcome the influence from the facial
   expression variations.}},
DOI = {{10.1007/s11760-017-1087-6}},
ISSN = {{1863-1703}},
EISSN = {{1863-1711}},
Unique-ID = {{ISI:000408249500016}},
}

@article{ ISI:000412965200011,
Author = {Savran, Arman and Sankur, Bulent},
Title = {{Non-rigid registration based model-free 3D facial expression recognition}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2017}},
Volume = {{162}},
Pages = {{146-165}},
Month = {{SEP}},
Abstract = {{We propose a novel feature extraction approach for 3D facial expression
   recognition by incorporating non-rigid registration in face-model-free
   analysis, which in turn makes feasible data-driven, i.e., feature
   model-free recognition of expressions. The resulting simplicity of
   feature representation is due to the fact that facial information is
   adapted to the input faces via shape model-free dense registration, and
   this provides a dynamic feature extraction mechanism. This approach
   eliminates the necessity of complex feature representations as required
   in the case of static feature extraction methods, where the complexity
   arises from the necessity to model the local context; higher degree of
   complexity persists in deep feature hierarchies enabled by end-to-end
   learning on large-scale datasets. Face-model-free recognition implies
   independence from limitations and biases due to committed face models,
   bypassing complications of model fitting, and avoiding the burden of
   manual model construction. We show via information gain maps that
   non-rigid registration enables extraction of highly informative
   features, as it provides invariance to local shifts due to physiognomy
   (subject invariance) and residual pose misalignments; in addition, it
   allows estimation of local correspondences of expressions. To maximize
   the recognition rate, we use the strategy of employing a rich but
   computationally manageable set of local correspondence structures, and
   to this effect we propose a framework to optimally select multiple
   registration references. Our features are re-sampled surface curvature
   values at individual coordinates which are chosen per expression-class
   and per reference pair. We show the superior performance of our novel
   dynamic feature extraction approach on three distinct recognition
   problems, namely, action unit detection, basic expression recognition,
   and emotion dimension recognition. (C) 2017 Elsevier Inc. All rights
   reserved.}},
DOI = {{10.1016/j.cviu.2017.07.005}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
Unique-ID = {{ISI:000412965200011}},
}

@article{ ISI:000412378800003,
Author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Benouareth, Abdallah
   and Declercq, David},
Title = {{3D facial expression recognition using kernel methods on Riemannian
   manifold}},
Journal = {{ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE}},
Year = {{2017}},
Volume = {{64}},
Pages = {{25-32}},
Month = {{SEP}},
Abstract = {{Automatic human Facial Expressions Recognition (FER) is becoming of
   increased interest. FER finds its applications in many emerging areas
   such as affective computing and intelligent human computer interaction.
   Most of the existing work on FER has been done using 2D data which
   suffers from inherent problems of illumination changes and pose
   variations. With the development of 3D image capturing technologies, the
   acquisition of 3D data is becoming a more feasible task. The 3D data
   brings a more effective solution in addressing the issues raised by its
   2D counterpart. State-of-the-art 3D FER methods are often based on a
   single descriptor which may fail to handle the large inter-class and
   intra-class variability of the human facial expressions. In this work,
   we explore, for the first time, the usage of covariance matrices of
   descriptors, instead of the descriptors themselves, in 3D FER. Since
   covariance matrices are elements of the non-linear manifold of Symmetric
   Positive Definite (SPD) matrices, we particularly look at the
   application of manifold-based classification to the problem of 3D FER.
   We evaluate the performance of the proposed framework on the BU-3DFE and
   the Bosphorus datasets, and demonstrate its superiority compared to the
   state-of-the-art methods. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.engappai.2017.05.009}},
ISSN = {{0952-1976}},
EISSN = {{1873-6769}},
Unique-ID = {{ISI:000412378800003}},
}

@article{ ISI:000412265100009,
Author = {Liang, Yan and Zhang, Yun and Zeng, Xian-Xian},
Title = {{Pose-invariant 3D face recognition using half face}},
Journal = {{SIGNAL PROCESSING-IMAGE COMMUNICATION}},
Year = {{2017}},
Volume = {{57}},
Pages = {{84-90}},
Month = {{SEP}},
Abstract = {{Pose variations are still challenging problems in 3D face recognition
   because large pose variations will cause self-occlusion and result in
   missing data. In this paper, a new method for pose-invariant 3D face
   recognition is proposed to handle significant pose variations. For pose
   estimation and registration, a coarse-to-fine strategy is proposed to
   detect landmarks under large yaw variations. At the coarse search step,
   candidate landmarks are detected using HK curvature analysis and
   subdivided according to a facial geometrical structure-based
   classification strategy. At the fine search step, candidate landmarks
   are identified and labeled by comparing with a Facial Landmark Model. By
   using the half face matching, we perform the matching step with respect
   to frontal scans and side scans. Experiments carried out on the
   Bosphorus and UND/FRGC v2.0 databases show that our method has high
   accuracy and robustness to pose variations. (C) 2017 Elsevier B.V. All
   rights reserved.}},
DOI = {{10.1016/j.image.2017.05.004}},
ISSN = {{0923-5965}},
EISSN = {{1879-2677}},
Unique-ID = {{ISI:000412265100009}},
}

@article{ ISI:000410465400003,
Author = {Wang, Xue-Qiao and Yuan, Jia-Zheng and Li, Qing},
Title = {{3D Face Recognition Using Spherical Vector Norms Map}},
Journal = {{JOURNAL OF INFORMATION SCIENCE AND ENGINEERING}},
Year = {{2017}},
Volume = {{33}},
Number = {{5}},
Pages = {{1141-1161}},
Month = {{SEP}},
Abstract = {{In this paper, we introduce a novel, automatic method for 3D face
   recognition. A new feature called a spherical vector norms map of a 3D
   face is created using the normal vector of each point. This feature
   contains more detailed information than the original depth image in
   regions such as the eyes and nose. For certain flat areas of 3D face,
   such as the forehead and cheeks, this map could increase the
   distinguishability of different points. In addition, this feature is
   robust to facial expression due to an adjustment that is made in the
   mouth region. Then, the facial representations, which are based on
   Histograms of Oriented Gradients, are extracted from the spherical
   vector norms map and the original depth image. A new partitioning
   strategy is proposed to produce the histogram of eight patches of a
   given image, in which all of the pixels are binned based on the
   magnitude and direction of their gradients. In this study, SVNs map and
   depth image are represented compactly with two histograms of oriented
   gradients; this approach is completed by Linear Discriminant Analysis
   and a Nearest Neighbor classifier.}},
DOI = {{10.6688/JISE.2017.33.5.3}},
ISSN = {{1016-2364}},
Unique-ID = {{ISI:000410465400003}},
}

@article{ ISI:000408398200010,
Author = {Eng, Z. H. D. and Yick, Y. Y. and Guo, Y. and Xu, H. and Reiner, M. and
   Cham, T. J. and Chen, S. H. A.},
Title = {{3D faces are recognized more accurately and faster than 2D faces, but
   with similar inversion effects}},
Journal = {{VISION RESEARCH}},
Year = {{2017}},
Volume = {{138}},
Pages = {{78-85}},
Month = {{SEP}},
Abstract = {{Recognition of faces typically occurs via holistic processing where
   individual features are combined to provide an overall facial
   representation. However, when faces are inverted, there is greater
   reliance on featural processing where faces are recognized based on
   their individual features. These findings are based on a substantial
   number of studies using 2-dimensional (2D) faces and it is unknown
   whether these results can be extended to 3-dimensional (3D) faces, which
   have more depth information that is absent in the typical 2D stimuli
   used in face recognition literature. The current study used the face
   inversion paradigm as a means to investigate how holistic and featural
   processing are differentially influenced by 2D and 3D faces. Twenty-five
   participants completed a delayed face-matching task consisting of
   upright and inverted faces that were presented as both 2D and 3D
   stereoscopic images. Recognition accuracy was significantly higher for
   3D upright faces compared to 2D upright faces, providing support that
   the enriched visual information in 3D stereoscopic images facilitates
   holistic processing that is essential for the recognition of upright
   faces. Typical face inversion effects were also obtained, regardless of
   whether the faces were presented in 2D or 3D. Moreover, recognition
   performances for 2D inverted and 3D inverted faces did not differ. Taken
   together, these results demonstrated that 3D stereoscopic effects
   influence face recognition during holistic processing but not during
   featural processing. Our findings therefore provide a novel perspective
   that furthers our understanding of face recognition mechanisms, shedding
   light on how the integration of stereoscopic information in 3D faces
   influences face recognition processes. (c) 2017 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.visres.2017.06.004}},
ISSN = {{0042-6989}},
EISSN = {{1878-5646}},
ResearcherID-Numbers = {{Chen, SH Annabel/F-3742-2011
   }},
ORCID-Numbers = {{Chen, SH Annabel/0000-0002-1540-5516
   Xu, Hong/0000-0003-1389-5408}},
Unique-ID = {{ISI:000408398200010}},
}

@article{ ISI:000403135200018,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal and Eastwood, Peter},
Title = {{Deep, dense and accurate 3D face correspondence for generating
   population specific deformable models}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2017}},
Volume = {{69}},
Pages = {{238-250}},
Month = {{SEP}},
Abstract = {{We present a multilinear algorithm to automatically establish dense
   point-to-point correspondence over an arbitrarily large number of
   population specific 3D faces across identities, facial expressions and
   poses. The algorithm is initialized with a subset of anthropometric
   landmarks detected by our proposed Deep Landmark Identification Network
   which is trained on synthetic images. The landmarks are used to segment
   the 3D face into Voronoi regions by evolving geodesic level set curves.
   Exploiting the intrinsic features of these regions, we extract
   discriminative keypoints on the facial manifold to elastically match the
   regions across faces for establishing dense correspondence. Finally, we
   generate a Region based 3D Deformable Model which is fitted to unseen
   faces to transfer the correspondences. We evaluate our algorithm on the
   tasks of facial landmark detection and recognition using two benchmark
   datasets. Comparison with thirteen state-of-the-art techniques shows the
   efficacy of our algorithm. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2017.04.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Eastwood, Peter/0000-0002-4490-4138
   Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000403135200018}},
}

@article{ ISI:000406751100005,
Author = {Jourabloo, Amin and Liu, Xiaoming},
Title = {{Pose-Invariant Face Alignment via CNN-Based Dense 3D Model Fitting}},
Journal = {{INTERNATIONAL JOURNAL OF COMPUTER VISION}},
Year = {{2017}},
Volume = {{124}},
Number = {{2}},
Pages = {{187-203}},
Month = {{SEP}},
Abstract = {{Pose-invariant face alignment is a very challenging problem in computer
   vision, which is used as a prerequisite for many facial analysis tasks,
   e.g., face recognition, expression recognition, and 3D face
   reconstruction. Recently, there have been a few attempts to tackle this
   problem, but still more research is needed to achieve higher accuracy.
   In this paper, we propose a face alignment method that aligns an image
   with arbitrary poses, by combining the powerful cascaded CNN regressors,
   3D Morphable Model (3DMM), and mirrorability constraint. The core of our
   proposed method is a novel 3DMM fitting algorithm, where the camera
   projection matrix parameters and 3D shape parameters are estimated by a
   cascade of CNN-based regressors. Furthermore, we impose the
   mirrorability constraint during the CNN learning by employing a novel
   loss function inside the siamese network. The dense 3D shape enables us
   to design pose-invariant appearance features for effective CNN learning.
   Extensive experiments are conducted on the challenging large-pose face
   databases (AFLW and AFW), with comparison to the state of the art.}},
DOI = {{10.1007/s11263-017-1012-z}},
ISSN = {{0920-5691}},
EISSN = {{1573-1405}},
Unique-ID = {{ISI:000406751100005}},
}

@article{ ISI:000413881700008,
Author = {Deng, Xing and Da, Feipeng and Shao, Haijian},
Title = {{Efficient 3D face recognition using local covariance descriptor and
   Riemannian kernel sparse coding}},
Journal = {{COMPUTERS \& ELECTRICAL ENGINEERING}},
Year = {{2017}},
Volume = {{62}},
Pages = {{81-91}},
Month = {{AUG}},
Abstract = {{This paper proposes a novel 3D face recognition method using the local
   covariance descriptor and Riemannian kernel sparse coding in order to
   accurately evaluate the intrinsic correlation of the extracted features
   and further improve the 3D face recognition accuracy. Firstly, the
   keypoints are detected by the farthest point sampling method, and the
   corresponding keypoint neighborhood is extracted by the specified radius
   associated with geodesic distance. Then, different types of the
   efficient features are selected to construct the local covariance
   descriptor with inherent property. Finally, the appropriate Riemannian
   kernel sparse coding is used to identify the faces in probe.
   Experimental evaluation has been performed on two challenging 3D face
   datasets, FRGC v2.0 and Bosphorus, which indicates that the proposed
   approach can significantly improve the identification accuracy comparing
   with other state-of-the-art methods. (C) 2017 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.compeleceng.2017.01.028}},
ISSN = {{0045-7906}},
EISSN = {{1879-0755}},
Unique-ID = {{ISI:000413881700008}},
}

@article{ ISI:000410870200001,
Author = {Jo, Jaeik and Kim, Hyunjun and Kim, Jaihie},
Title = {{3D facial shape reconstruction using macro- and micro-level features
   from high resolution facial images}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{64}},
Pages = {{1-9}},
Month = {{AUG}},
Abstract = {{Three-dimensional (3D) facial modeling and stereo matching-based methods
   are widely used for 3D facial reconstruction from 2D single-view and
   multiple-view images. However, these methods cannot realistically
   reconstruct 3D faces because they use insufficient numbers of
   macro-level Facial Feature Points (FFPs). This paper proposes an
   accurate and person-specific 3D facial reconstruction method that uses
   ample numbers of macro and micro-level FFPs to enable coverage of all
   facial regions of high resolution facial images. Comparisons of 3D
   facial images reconstructed using the proposed method for ground-truth
   3D facial images from the Bosphorus 3D database show that the method is
   superior to a conventional Active Appearance Model-Structure from Motion
   (AAM + SfM)-based method in terms of average 3D root mean square error
   between the reconstructed and ground-truth 3D faces. Further, the
   proposed method achieved outstanding accuracy in local facial regions
   such as the cheek areas where extraction of FFPs is difficult for
   existing methods. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/jimavis.2017.05.001}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000410870200001}},
}

@article{ ISI:000410870200008,
Author = {Xia, Baiqiang and Ben Amor, Boulbaba and Daoudi, Mohamed},
Title = {{= Joint gender, ethnicity and age estimation from 3D faces An
   experimental illustration of their correlations}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{64}},
Pages = {{90-102}},
Month = {{AUG}},
Abstract = {{Humans present clear demographic traits which allow their peers to
   recognize their gender and ethnic groups as well as estimate their age.
   Abundant literature has investigated the problem of automated gender,
   ethnicity and age recognition from facial images. However, despite the
   co-existence of these traits, most of the studies have addressed them
   separately, very little attention has been given to their correlations.
   In this work, we address the problem of joint demographic estimation and
   investigate the correlation through the morphological differences in 3D
   facial shapes. To this end, a set of facial features are extracted to
   capture the 3D shape differences among the demographic groups. Then, a
   correlation-based feature selection is applied to highlight salient
   features and remove redundancy. These features are later fed to Random
   Forest for gender and ethnicity classification, and age estimation.
   Extensive experiments conducted on FRGCv2 dataset, under
   Expression-Dependent and Expression-Independent settings, demonstrate
   the effectiveness of the proposed approaches for the three traits, and
   also show the accuracy improvement when considering their correlations.
   To the best of our knowledge, this is the first study exploring the
   correlations of these facial soft-biometric traits using 3D faces. This
   is also the first work which studies the problem of age estimation from
   3D Faces.(1) (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2017.06.004}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305}},
Unique-ID = {{ISI:000410870200008}},
}

@article{ ISI:000408370500007,
Author = {Storey, Gary and Jiang, Richard and Bouridane, Ahmed},
Title = {{Role for 2D image generated 3D face models in the rehabilitation of
   facial palsy}},
Journal = {{HEALTHCARE TECHNOLOGY LETTERS}},
Year = {{2017}},
Volume = {{4}},
Number = {{4}},
Pages = {{145-148}},
Month = {{AUG}},
Abstract = {{The outcome for patients diagnosed with facial palsy has been shown to
   be linked to rehabilitation. Dense 3D morphable models have been shown
   within the computer vision to create accurate representations of human
   faces even from single 2D images. This has the potential to provide
   feedback to both the patient and medical expert dealing with the
   rehabilitation plan. It is proposed that a framework for the creation
   and measuring of patient facial movement consisting of a hybrid 2D
   facial landmark fitting technique which shows better accuracy in testing
   than current methods and 3D model fitting.}},
DOI = {{10.1049/htl.2017.0023}},
ISSN = {{2053-3713}},
ORCID-Numbers = {{Jiang, Richard/0000-0003-1721-9474
   Storey, Gary/0000-0001-5492-0433}},
Unique-ID = {{ISI:000408370500007}},
}

@article{ ISI:000406111700008,
Author = {Suttie, Michael and Wetherill, Leah and Jacobson, Sandra W. and
   Jacobson, Joseph L. and Hoyme, H. Eugene and Sowell, Elizabeth R. and
   Coles, Claire and Wozniak, Jeffrey R. and Riley, Edward P. and Jones,
   Kenneth L. and Foroud, Tatiana and Hammond, Peter and CIFASD},
Title = {{Facial Curvature Detects and Explicates Ethnic Differences in Effects of
   Prenatal Alcohol Exposure}},
Journal = {{ALCOHOLISM-CLINICAL AND EXPERIMENTAL RESEARCH}},
Year = {{2017}},
Volume = {{41}},
Number = {{8}},
Pages = {{1471-1483}},
Month = {{AUG}},
Abstract = {{BackgroundOur objective is to help clinicians detect the facial effects
   of prenatal alcohol exposure by developing computer-based tools for
   screening facial form.
   MethodsAll 415 individuals considered were evaluated by expert
   dysmorphologists and categorized as (i) healthy control (HC), (ii) fetal
   alcohol syndrome (FAS), or (iii) heavily prenatally alcohol exposed (HE)
   but not clinically diagnosable as FAS; 3D facial photographs were used
   to build models of facial form to support discrimination studies.
   Surface curvature-based delineations of facial form were introduced.
   Results(i) Facial growth in FAS, HE, and control subgroups is similar in
   both cohorts. (ii) Cohort consistency of agreement between clinical
   diagnosis and HC-FAS facial form classification is lower for midline
   facial regions and higher for nonmidline regions. (iii) Specific HC-FAS
   differences within and between the cohorts include: for HC, a smoother
   philtrum in Cape Coloured individuals; for FAS, a smoother philtrum in
   Caucasians; for control-FAS philtrum difference, greater homogeneity in
   Caucasians; for control-FAS face difference, greater homogeneity in Cape
   Coloured individuals. (iv) Curvature changes in facial profile induced
   by prenatal alcohol exposure are more homogeneous and greater in Cape
   Coloureds than in Caucasians. (v) The Caucasian HE subset divides into
   clusters with control-like and FAS-like facial dysmorphism. The Cape
   Coloured HE subset is similarly divided for nonmidline facial regions
   but not clearly for midline structures. (vi) The Cape Coloured HE subset
   with control-like facial dysmorphism shows orbital hypertelorism.
   ConclusionsFacial curvature assists the recognition of the effects of
   prenatal alcohol exposure and helps explain why different facial regions
   result in inconsistent control-FAS discrimination rates in disparate
   ethnic groups. Heavy prenatal alcohol exposure can give rise to orbital
   hypertelorism, supporting a long-standing suggestion that prenatal
   alcohol exposure at a particular time causes increased separation of the
   brain hemispheres with a concomitant increase in orbital separation.}},
DOI = {{10.1111/acer.13429}},
ISSN = {{0145-6008}},
EISSN = {{1530-0277}},
ORCID-Numbers = {{Wozniak, Jeffrey/0000-0002-7132-8519}},
Unique-ID = {{ISI:000406111700008}},
}

@article{ ISI:000401381100021,
Author = {Hu, Jian-Fang and Zheng, Wei-Shi and Xie, Xiaohua and Lai, Jianhuang},
Title = {{Sparse transfer for facial shape-from-shading}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2017}},
Volume = {{68}},
Pages = {{272-285}},
Month = {{AUG}},
Abstract = {{We present an image-based 3D face shape reconstruction method which
   transfers shape cues inferred from source face images to guide the
   reconstruction of the target face. Specifically, a sparse face shape
   adaption mechanism is used to generate a target-specific reference shape
   by adaptively and selectively combining source face shapes. This
   reference shape can also facilitate the reconstruction optimization for
   the target shape. As an off-line process, each source shape has been
   derived from a set of given sufficient source images (more than 9) based
   on a non-Lambertian reflectance model. Such a process allows for the
   existence of cast shadow and specularity, and more accurately infers the
   source shape. Guided by the target-specific reference shape, the shape
   of a target face can be estimated using a small number of images (even
   only one). The proposed reconstruction method refers to a lighting
   estimation and an albedo estimation for the target face. No standard 3D
   shape (such as the high-precision scanned 3D face) is required in the
   reconstruction process. Compared to the state-of-the-arts including the
   Photometric Stereo, Tensor Spline, the single reference based method,
   and the GEM algorithm, the proposed sparse transfer model can produce
   visually better facial details and obtain smaller reconstruction errors.
   (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2017.03.029}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000401381100021}},
}

@article{ ISI:000409284700029,
Author = {Boukamcha, Hamdi and Hallek, Mohamed and Smach, Fethi and Atri, Mohamed},
Title = {{Automatic landmark detection and 3D Face data extraction}},
Journal = {{JOURNAL OF COMPUTATIONAL SCIENCE}},
Year = {{2017}},
Volume = {{21}},
Pages = {{340-348}},
Month = {{JUL}},
Abstract = {{This paper contributes to 3D facial synthesis by presenting a novel
   method for parameterization using Landmark Point detection. The approach
   presented aims at improving facial recognition even in varying facial
   expressions, and missing data in 3D facial models. As such, the prime
   objective was to develop an automatically embedded process that can
   detect any frontal face in 3D face recognition systems, with face
   segmentation and surface curvature information. Using the hybrid
   interpolation method, experiments on facial landmarks were performed on
   4950 images from Face Recognition Grand Challenge database (FRGC).
   Distinctive facial landmarks from the nose-tips, Limits mouth and two
   eye corners formed the statistical inputs for Iterative Closest Point
   (ICP) in the Point Distribution Model (PDM). Performance or landmark
   localization is reported by using percentage deviation from the mean 3D
   profile. Localization results and estimated data on landmark locations
   demonstrate that the method confirms its effectiveness for proposed
   application. (C) 2016 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.jocs.2016.11.015}},
ISSN = {{1877-7503}},
ResearcherID-Numbers = {{atri, mohamed/C-4069-2014}},
ORCID-Numbers = {{atri, mohamed/0000-0001-8528-5647}},
Unique-ID = {{ISI:000409284700029}},
}

@article{ ISI:000404061800006,
Author = {Henriquez, Pedro and Matuszewski, Bogdan J. and Andreu-Cabedo, Yasmina
   and Bastiani, Luca and Colantonio, Sara and Coppini, Giuseppe and
   D'Acunto, Mario and Favilla, Riccardo and Germanese, Danila and Giorgi,
   Daniela and Marraccini, Paolo and Martinelli, Massimo and Morales,
   Maria-Aurora and Pascali, Maria Antonietta and Righi, Marco and
   Salvetti, Ovidio and Larsson, Marcus and Stromberg, Tomas and Randeberg,
   Lise and Bjorgan, Asgeir and Giannakakis, Giorgos and Pediaditis,
   Matthew and Chiarugi, Franco and Christinaki, Eirini and Marias, Kostas
   and Tsiknakis, Manolis},
Title = {{Mirror Mirror on the Wall ... An Unobtrusive Intelligent Multisensory
   Mirror for Well-Being Status Self-Assessment and Visualization}},
Journal = {{IEEE TRANSACTIONS ON MULTIMEDIA}},
Year = {{2017}},
Volume = {{19}},
Number = {{7}},
Pages = {{1467-1481}},
Month = {{JUL}},
Abstract = {{A person's well-being status is reflected by their face through a
   combination of facial expressions and physical signs. The SEMEOTICONS
   project translates the semeiotic code of the human face into
   measurements and computational descriptors that are automatically
   extracted from images, videos, and three-dimensional scans of the face.
   SEMEOTICONS developed a multisensory platform in the form of a smart
   mirror to identify signs related to cardio-metabolic risk. The aim was
   to enable users to self-monitor their well-being status over time and
   guide them to improve their lifestyle. Significant scientific and
   technological challenges have been addressed to build the multisensory
   mirror, from touchless data acquisition, to real-time processing and
   integration of multimodal data.}},
DOI = {{10.1109/TMM.2017.2666545}},
ISSN = {{1520-9210}},
EISSN = {{1941-0077}},
ResearcherID-Numbers = {{Randeberg, Lise Lyngsnes/G-8664-2019
   Righi, Marco/B-1595-2016
   }},
ORCID-Numbers = {{Randeberg, Lise Lyngsnes/0000-0003-2608-3759
   Righi, Marco/0000-0002-1448-0960
   Giannakakis, Giorgos/0000-0002-0958-5346
   Larsson, Marcus/0000-0001-6385-6760}},
Unique-ID = {{ISI:000404061800006}},
}

@article{ ISI:000399520700020,
Author = {Jiang, Richard and Ho, Anthony T. S. and Cheheb, Ismahane and
   Al-Maadeed, Noor and Al-Maadeed, Somaya and Bouridane, Ahmed},
Title = {{Emotion recognition from scrambled facial images via many graph
   embedding}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2017}},
Volume = {{67}},
Pages = {{245-251}},
Month = {{JUL}},
Abstract = {{Facial expression verification has been extensively exploited due to its
   wide application in affective computing, robotic vision, man-machine
   interaction and medical diagnosis. With the recent development of
   Internet-of-Things (loT), there is a need of mobile-targeted facial
   expression verification, where face scrambling has been proposed for
   privacy protection during image/video distribution over public network.
   Consequently, facial expression verification needs to be carried out in
   a scrambled domain, bringing out new challenges in facial expression
   recognition. An immediate impact from face scrambling is that
   conventional semantic facial components become not identifiable, and 3D
   face models cannot be clearly fitted to a scrambled image. Hence, the
   classical facial action coding system cannot be applied to facial
   expression recognition in the scrambled domain. To cope with chaotic
   signals from face scrambling, this paper proposes an new approach - Many
   Graph Embedding (MGE) to discover discriminative patterns from the
   subspaces of chaotic patterns, where the facial expression recognition
   is carried out as a fuzzy combination from many graph embedding. In our
   experiments, the proposed MGE was evaluated on three scrambled facial
   expression datasets: JAFFE, MUG and CK++. The benchmark results
   demonstrated that the proposed method is able to improve the recognition
   accuracy, making our method a promising candidate for the scrambled
   facial expression recognition in the emerging privacy-protected loT
   applications. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2017.02.003}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Jiang, Richard/0000-0003-1721-9474}},
Unique-ID = {{ISI:000399520700020}},
}

@article{ ISI:000406306000017,
Author = {Echeagaray-Patron, B. A. and Kober, V. I. and Karnaukhov, V. N. and
   Kuznetsov, V. V.},
Title = {{A Method of Face Recognition Using 3D Facial Surfaces}},
Journal = {{JOURNAL OF COMMUNICATIONS TECHNOLOGY AND ELECTRONICS}},
Year = {{2017}},
Volume = {{62}},
Number = {{6}},
Pages = {{648-652}},
Month = {{JUN}},
Abstract = {{Face recognition is one of the most rapidly developing areas of image
   processing and computer vision. In this work, a new method for face
   recognition and identification using 3D facial surfaces is proposed. The
   method is invariant to facial expression and pose variations in the
   scene. The method uses 3D shape data without color or texture
   information. The method is based on conformal mapping of original facial
   surfaces onto a Riemannian manifold, followed by comparison of conformal
   and isometric invariants computed in this manifold. Computer results are
   presented using known 3D face databases that contain significant amount
   of expression and pose variations.}},
DOI = {{10.1134/S1064226917060067}},
ISSN = {{1064-2269}},
EISSN = {{1555-6557}},
ResearcherID-Numbers = {{Karnaukhov, Victor/F-1880-2014}},
Unique-ID = {{ISI:000406306000017}},
}

@article{ ISI:000402732800006,
Author = {Marcolin, Federica and Vezzetti, Enrico},
Title = {{Novel descriptors for geometrical 3D face analysis}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2017}},
Volume = {{76}},
Number = {{12}},
Pages = {{13805-13834}},
Month = {{JUN}},
Abstract = {{3D face was recently investigated for various applications, including
   biometrics and diagnosis. Describing facial surface, i.e. how it bends
   and which kinds of patches is composed by, is the aim of studies of Face
   Analysis, whose ultimate goal is to identify which features could be
   extracted from three-dimensional faces depending on the application. In
   this study, we propose 105 novel geometrical descriptors for Face
   Analysis. They are generated by composing primary geometrical
   descriptors such as mean, Gaussian, principal curvatures, shape index,
   curvedness, and the coefficients of the fundamental forms, and by
   applying standard functions such as sine, cosine, and logarithm to them.
   The new descriptors were mapped on 217 facial depth maps and analysed in
   terms of descriptiveness of facial shape and exploitability for
   localizing landmark points. Automatic landmark extraction stands as the
   final aim of this analysis. Results showed that some newly generated
   descriptors were sounder than the primary ones, meaning that their local
   behaviours in correspondence to a landmark position is thoroughly
   specific and can be registered with high similarity on every face of our
   dataset.}},
DOI = {{10.1007/s11042-016-3741-3}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
ORCID-Numbers = {{Marcolin, Federica/0000-0002-4360-6905}},
Unique-ID = {{ISI:000402732800006}},
}

@article{ ISI:000401888600006,
Author = {Coomes, David A. and Dalponte, Michele and Jucker, Tommaso and Asner,
   Gregory P. and Banin, Lindsay F. and Burslem, David F. R. P. and Lewis,
   Simon L. and Nilus, Reuben and Phillips, Oliver L. and Phua, Mui-How and
   Qie, Lan},
Title = {{Area-based vs tree-centric approaches to mapping forest carbon in
   Southeast Asian forests from airborne laser scanning data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2017}},
Volume = {{194}},
Pages = {{77-88}},
Month = {{JUN 1}},
Abstract = {{Tropical forests are a key component of the global carbon cycle, and
   mapping their carbon density is essential for understanding human
   influences on climate and for ecosystem-service-based payments for
   forest protection. Discrete-return airborne laser scanning (ALS) is
   increasingly recognised as a high-quality technology for mapping
   tropical forest carbon, because it generates 3D point clouds of forest
   structure from which aboveground carbon density (ACD) can be estimated.
   Area-based models are state of the art when it comes to estimating ACD
   from ALS data, but discard tree-level information contained within the
   ALS point cloud. This paper compares area based and tree-centric models
   for estimating ACD in lowland old-growth forests in Sabah, Malaysia.
   These forests are challenging to map because of their immense height. We
   compare the performance of (a) an area-based model developed by Asner
   and Mascaro (2014), and used primarily in the neotropics hitherto, with
   (b) a tree-centric approach that uses a new algorithm (itcSegment) to
   locate trees within the ALS canopy height model, measures their heights
   and crown widths, and calculates biomass from these dimensions. We find
   that Asner and Mascaro's model needed regional calibration, reflecting
   the distinctive structure of Southeast Asian forests. We also discover
   that forest basal area is closely related to canopy gap fraction
   measured by ALS, and use this finding to refine Asner and Mascaro's
   model. Finally, we show that our tree-centric approach is less accurate
   at estimating ACD than the best-performing area-based model (RMSE 18\%
   vs 13\%). Tree-centric modelling is appealing because it is based on
   summing the biomass of individual trees, but until algorithms can detect
   understory trees reliably and estimate biomass from crown dimensions
   precisely, areas-based modelling will remain the method of choice. (C)
   2017 The Authors. Published by Elsevier Inc.}},
DOI = {{10.1016/j.rse.2017.03.017}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Burslem, David FRP/F-1204-2019
   Jucker, Tommaso/S-4724-2017
   Phillips, Oliver L/A-1523-2011
   Dalponte, Michele/E-5117-2011}},
ORCID-Numbers = {{Burslem, David FRP/0000-0001-6033-0990
   Jucker, Tommaso/0000-0002-0751-6312
   Phillips, Oliver L/0000-0002-8993-6168
   Dalponte, Michele/0000-0001-9850-8985}},
Unique-ID = {{ISI:000401888600006}},
}

@article{ ISI:000400868800002,
Author = {Schonborn, Sandro and Egger, Bernhard and Morel-Forster, Andreas and
   Vetter, Thomas},
Title = {{Markov Chain Monte Carlo for Automated Face Image Analysis}},
Journal = {{INTERNATIONAL JOURNAL OF COMPUTER VISION}},
Year = {{2017}},
Volume = {{123}},
Number = {{2}},
Pages = {{160-183}},
Month = {{JUN}},
Abstract = {{We present a novel fully probabilistic method to interpret a single face
   image with the 3D Morphable Model. The new method is based on Bayesian
   inference and makes use of unreliable image-based information. Rather
   than searching a single optimal solution, we infer the posterior
   distribution of the model parameters given the target image. The method
   is a stochastic sampling algorithm with a propose-and-verify
   architecture based on the Metropolis-Hastings algorithm. The stochastic
   method can robustly integrate unreliable information and therefore does
   not rely on feed-forward initialization. The integrative concept is
   based on two ideas, a separation of proposal moves and their
   verification with the model (Data-Driven Markov Chain Monte Carlo), and
   filtering with the Metropolis acceptance rule. It does not need
   gradients and is less prone to local optima than standard fitters. We
   also introduce a new collective likelihood which models the average
   difference between the model and the target image rather than individual
   pixel differences. The average value shows a natural tendency towards a
   normal distribution, even when the individual pixel-wise difference is
   not Gaussian. We employ the new fitting method to calculate posterior
   models of 3D face reconstructions from single real-world images. A
   direct application of the algorithm with the 3D Morphable Model leads us
   to a fully automatic face recognition system with competitive
   performance on the Multi-PIE database without any database adaptation.}},
DOI = {{10.1007/s11263-016-0967-5}},
ISSN = {{0920-5691}},
EISSN = {{1573-1405}},
Unique-ID = {{ISI:000400868800002}},
}

@article{ ISI:000397371800015,
Author = {Ding, Changxing and Tao, Dacheng},
Title = {{Pose-invariant face recognition with homography-based normalization}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2017}},
Volume = {{66}},
Number = {{SI}},
Pages = {{144-152}},
Month = {{JUN}},
Abstract = {{Pose-invariant face recognition (PIFR) refers to the ability that
   recognizes face images with arbitrary pose variations. Among existing
   PIFR algorithms, pose normalization has been proved to be an effective
   approach which preserves texture fidelity, but usually depends on
   precise 3D face models or at high computational cost. In this paper, we
   propose an highly efficient PIFR algorithm that effectively handles the
   main challenges caused by pose variation. First, a dense grid of 3D
   facial landmarks are projected to each 2D face image, which enables
   feature extraction in an pose adaptive manner. Second, for the local
   patch around each landmark, an optimal warp is estimated based on
   homography to correct texture deformation caused by pose variations. The
   reconstructed frontal-view patches are then utilized for face
   recognition with traditional face descriptors. The homography-based
   normalization is highly efficient and the synthesized frontal face
   images are of high quality. Finally, we propose an effective approach
   for occlusion detection, which enables face recognition with visible
   patches only. Therefore, the proposed algorithm effectively handles the
   main challenges in PIFR. Experimental results on four popular face
   databases demonstrate that the proposed approach performs well on both
   constrained and unconstrained environments.}},
DOI = {{10.1016/j.patcog.2016.11.024}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{Ding, Changxing/L-7075-2019}},
Unique-ID = {{ISI:000397371800015}},
}

@article{ ISI:000399250000012,
Author = {Emambakhsh, Mehryar and Evans, Adrian},
Title = {{Nasal Patches and Curves for Expression-Robust 3D Face Recognition}},
Journal = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}},
Year = {{2017}},
Volume = {{39}},
Number = {{5}},
Pages = {{995-1007}},
Month = {{MAY}},
Abstract = {{The potential of the nasal region for expression robust 3D face
   recognition is thoroughly investigated by a novel five-step algorithm.
   First, the nose tip location is coarsely detected and the face is
   segmented, aligned and the nasal region cropped. Then, a very accurate
   and consistent nasal landmarking algorithm detects seven keypoints on
   the nasal region. In the third step, a feature extraction algorithm
   based on the surface normals of Gabor-wavelet filtered depth maps is
   utilised and, then, a set of spherical patches and curves are localised
   over the nasal region to provide the feature descriptors. The last step
   applies a genetic algorithm-based feature selector to detect the most
   stable patches and curves over different facial expressions. The
   algorithm provides the highest reported nasal region-based recognition
   ranks on the FRGC, Bosphorus and BU-3DFE datasets. The results are
   comparable with, and in many cases better than, many state-of-the-art 3D
   face recognition algorithms, which use the whole facial domain. The
   proposed method does not rely on sophisticated alignment or denoising
   steps, is very robust when only one sample per subject is used in the
   gallery, and does not require a training step for the landmarking
   algorithm.}},
DOI = {{10.1109/TPAMI.2016.2565473}},
ISSN = {{0162-8828}},
EISSN = {{1939-3539}},
ORCID-Numbers = {{Emambakhsh, Mehryar/0000-0001-6416-7668
   Evans, Adrian/0000-0001-8586-8295}},
Unique-ID = {{ISI:000399250000012}},
}

@article{ ISI:000390884300006,
Author = {Jones, Scott P. and Dwyer, Dominic M. and Lewis, Michael B.},
Title = {{The utility of multiple synthesized views in the recognition of
   unfamiliar faces}},
Journal = {{QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY}},
Year = {{2017}},
Volume = {{70}},
Number = {{5}},
Pages = {{906-918}},
Month = {{MAY}},
Abstract = {{The ability to recognize an unfamiliar individual on the basis of prior
   exposure to a photograph is notoriously poor and prone to errors, but
   recognition accuracy is improved when multiple photographs are
   available. In applied situations, when only limited real images are
   available (e.g., from a mugshot or CCTV image), the generation of new
   images might provide a technological prosthesis for otherwise fallible
   human recognition. We report two experiments examining the effects of
   providing computer-generated additional views of a target face. In
   Experiment 1, provision of computer-generated views supported better
   target face recognition than exposure to the target image alone and
   equivalent performance to that for exposure of multiple photograph
   views. Experiment 2 replicated the advantage of providing generated
   views, but also indicated an advantage for multiple viewings of the
   single target photograph. These results strengthen the claim that
   identifying a target face can be improved by providing multiple
   synthesized views based on a single target image. In addition, our
   results suggest that the degree of advantage provided by synthesized
   views may be affected by the quality of synthesized material.}},
DOI = {{10.1080/17470218.2016.1158302}},
ISSN = {{1747-0218}},
EISSN = {{1747-0226}},
ResearcherID-Numbers = {{Dwyer, Dominic Michael/D-1498-2009
   }},
ORCID-Numbers = {{Dwyer, Dominic Michael/0000-0001-8069-5508
   Lewis, Michael/0000-0002-5735-5318
   Jones, Scott/0000-0001-5516-4385}},
Unique-ID = {{ISI:000390884300006}},
}

@article{ ISI:000395219700006,
Author = {Hu, Xiao and Peng, Shaohu and Wang, Li and Yang, Zhao and Li, Zhaowen},
Title = {{Surveillance video face recognition with single sample per person based
   on 3D modeling and blurring}},
Journal = {{NEUROCOMPUTING}},
Year = {{2017}},
Volume = {{235}},
Pages = {{46-58}},
Month = {{APR 26}},
Abstract = {{Video surveillance has attracted more and more interests in the last
   decade, video-based Face Recognition (FR) therefore became an important
   task. However, the surveillance videos include many vague non-frontal
   faces especially the view of faces looking down and up. As a result,
   most FR algorithms would perform worse when they were applied in
   surveillance videos. On the other hand, it was common at video
   monitoring field that only Single training Sample Per Person (SSPP) is
   available from their identification card. In order to effectively
   improve FR for both the SSPP problem and the low-quality problem, this
   paper proposed an approach to synthesis face images-based on 3D face
   modeling and blurring. In the proposed algorithm, firstly a 2D frontal
   face with high-resolution was used to build a 3D face model, then
   several virtual faces with different poses were synthesized from the 3D
   model, and finally some degraded face images were constructed from the
   original and the virtual faces through blurring process. At last
   multiple face images could be chosen from frontal, virtual and degraded
   faces to build a training set Both SCface and LFW databases were
   employed to evaluate the proposed algorithm by using PCA, FLDA, scale
   invariant feature transform, compressive sensing and deep learning. The
   results on both datasets showed that the performance of these methods
   could be improved when virtual faces were generated to train the
   classifiers. Furthermore, in SCface database the average recognition
   rates increased up to 10\%, 16.62\%, 13.03\%, 19.44\% and 23.28\%
   respectively for the above-mentioned methods when virtual view and
   blurred faces were taken to train their classifiers. Experimental
   results indicated that the proposed method for generating more train
   samples was effective and could be considered to be applied in
   intelligent video monitoring system.}},
DOI = {{10.1016/j.neucom.2016.12.059}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000395219700006}},
}

@article{ ISI:000425868600001,
Author = {Wang, Lamei and Chen, Wenfeng and Li, Hong},
Title = {{Use of 3D faces facilitates facial expression recognition in children}},
Journal = {{SCIENTIFIC REPORTS}},
Year = {{2017}},
Volume = {{7}},
Month = {{APR 3}},
Abstract = {{This study assessed whether presenting 3D face stimuli could facilitate
   children's facial expression recognition. Seventy-one children aged
   between 3 and 6 participated in the study. Their task was to judge
   whether a face presented in each trial showed a happy or fearful
   expression. Half of the face stimuli were shown with 3D representations,
   whereas the other half of the images were shown as 2D pictures. We
   compared expression recognition under these conditions. The results
   showed that the use of 3D faces improved the speed of facial expression
   recognition in both boys and girls. Moreover, 3D faces improved boys'
   recognition accuracy for fearful expressions. Since fear is the most
   difficult facial expression for children to recognize, the facilitation
   effect of 3D faces has important practical implications for children
   with difficulties in facial expression recognition. The potential
   benefits of 3D representation for other expressions also have
   implications for developing more realistic assessments of children's
   expression recognition.}},
DOI = {{10.1038/srep45464}},
Article-Number = {{45464}},
ISSN = {{2045-2322}},
ResearcherID-Numbers = {{Chen, Wenfeng/H-2424-2012
   }},
ORCID-Numbers = {{Chen, Wenfeng/0000-0002-4271-8366
   wang, la mei/0000-0002-9203-5539}},
Unique-ID = {{ISI:000425868600001}},
}

@article{ ISI:000399118600008,
Author = {Ramachandra, Raghavendra and Busch, Christoph},
Title = {{Presentation Attack Detection Methods for Face Recognition Systems: A
   Comprehensive Survey}},
Journal = {{ACM COMPUTING SURVEYS}},
Year = {{2017}},
Volume = {{50}},
Number = {{1}},
Month = {{APR}},
Abstract = {{The vulnerability of face recognition systems to presentation attacks
   (also known as direct attacks or spoof attacks) has received a great
   deal of interest from the biometric community. The rapid evolution of
   face recognition systems into real-time applications has raised new
   concerns about their ability to resist presentation attacks,
   particularly in unattended application scenarios such as automated
   border control. The goal of a presentation attack is to subvert the face
   recognition system by presenting a facial biometric artifact. Popular
   face biometric artifacts include a printed photo, the electronic display
   of a facial photo, replaying video using an electronic display, and 3D
   face masks. These have demonstrated a high security risk for
   state-of-the-art face recognition systems. However, several presentation
   attack detection (PAD) algorithms (also known as countermeasures or
   antispoofing methods) have been proposed that can automatically detect
   and mitigate such targeted attacks. The goal of this survey is to
   present a systematic overview of the existing work on face presentation
   attack detection that has been carried out. This paper describes the
   various aspects of face presentation attacks, including different types
   of face artifacts, state-of-the-art PAD algorithms and an overview of
   the respective research labs working in this domain, vulnerability
   assessments and performance evaluation metrics, the outcomes of
   competitions, the availability of public databases for benchmarking new
   PAD algorithms in a reproducible manner, and finally a summary of the
   relevant international standardization in this field. Furthermore, we
   discuss the open challenges and future work that need to be addressed in
   this evolving field of biometrics.}},
DOI = {{10.1145/3038924}},
Article-Number = {{8}},
ISSN = {{0360-0300}},
EISSN = {{1557-7341}},
Unique-ID = {{ISI:000399118600008}},
}

@article{ ISI:000395116200010,
Author = {Hiremath, Manjunatha and Hiremath, P. S.},
Title = {{3D Face Recognition Based on Symbolic FDA Using SVM Classifier with
   Similarity and Dissimilarity Distance Measure}},
Journal = {{INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE}},
Year = {{2017}},
Volume = {{31}},
Number = {{4}},
Month = {{APR}},
Abstract = {{Human face images are the basis not only for person recognition, but for
   also identifying other attributes like gender, age, ethnicity, and
   emotional states of a person. Therefore, face is an important biometric
   identifier in the law enforcement and human-computer interaction (HCI)
   systems. The 3D human face recognition is emerging as a significant
   biometric technology. Research interest into 3D face recognition has
   increased during recent years due to availability of improved 3D
   acquisition devices and processing algorithms. A 3D face image is
   represented by 3D meshes or range images which contain depth
   information. In this paper, the objective is to propose a new 3D face
   recognition method based on radon transform and symbolic factorial
   discriminant analysis using KNN and SVM classifier with similarity and
   dissimilarity measures, which are applied on 3D facial range images. The
   experimentation is done using three publicly available databases,
   namely, Bhosphorus, Texas and CASIA 3D face database. The experimental
   results demonstrate the effectiveness of the proposed method.}},
DOI = {{10.1142/S0218001417560067}},
Article-Number = {{1756006}},
ISSN = {{0218-0014}},
EISSN = {{1793-6381}},
Unique-ID = {{ISI:000395116200010}},
}

@article{ ISI:000398720100091,
Author = {Weinmann, Martin and Weinmann, Michael and Mallet, Clement and Bredif,
   Mathieu},
Title = {{A Classification-Segmentation Framework for the Detection of Individual
   Trees in Dense MMS Point Cloud Data Acquired in Urban Areas}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{In this paper, we present a novel framework for detecting individual
   trees in densely sampled 3D point cloud data acquired in urban areas.
   Given a 3D point cloud, the objective is to assign point-wise labels
   that are both class-aware and instance-aware, a task that is known as
   instance-level segmentation. To achieve this, our framework addresses
   two successive steps. The first step of our framework is given by the
   use of geometric features for a binary point-wise semantic
   classification with the objective of assigning semantic class labels to
   irregularly distributed 3D points, whereby the labels are defined as
   ``tree points{''} and ``other points{''}. The second step of our
   framework is given by a semantic segmentation with the objective of
   separating individual trees within the ``tree points{''}. This is
   achieved by applying an efficient adaptation of the mean shift algorithm
   and a subsequent segment-based shape analysis relying on semantic rules
   to only retain plausible tree segments. We demonstrate the performance
   of our framework on a publicly available benchmark dataset, which has
   been acquired with a mobile mapping system in the city of Delft in the
   Netherlands. This dataset contains 10.13 M labeled 3D points among which
   17.6\% are labeled as ``tree points{''}. The derived results clearly
   reveal a semantic classification of high accuracy (up to 90.77\%) and an
   instance-level segmentation of high plausibility, while the simplicity,
   applicability and efficiency of the involved methods even allow applying
   the complete framework on a standard laptop computer with a reasonable
   processing time (less than 2.5 h).}},
DOI = {{10.3390/rs9030277}},
Article-Number = {{277}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Bredif, Mathieu/T-3029-2018
   }},
ORCID-Numbers = {{Bredif, Mathieu/0000-0003-0228-1232
   Mallet, Clement/0000-0002-2675-165X
   Weinmann, Martin/0000-0002-8654-7546}},
Unique-ID = {{ISI:000398720100091}},
}

@article{ ISI:000398720100002,
Author = {Nevalainen, Olli and Honkavaara, Eija and Tuominen, Sakari and Viljanen,
   Niko and Hakala, Teemu and Yu, Xiaowei and Hyyppa, Juha and Saari,
   Heikki and Polonen, Ilkka and Imai, Nilton N. and Tommaselli, Antonio M.
   G.},
Title = {{Individual Tree Detection and Classification with UAV-Based
   Photogrammetric Point Clouds and Hyperspectral Imaging}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{Small unmanned aerial vehicle (UAV) based remote sensing is a rapidly
   evolving technology. Novel sensors and methods are entering the market,
   offering completely new possibilities to carry out remote sensing tasks.
   Three-dimensional (3D) hyperspectral remote sensing is a novel and
   powerful technology that has recently become available to small UAVs.
   This study investigated the performance of UAV-based photogrammetry and
   hyperspectral imaging in individual tree detection and tree species
   classification in boreal forests. Eleven test sites with 4151 reference
   trees representing various tree species and developmental stages were
   collected in June 2014 using a UAV remote sensing system equipped with a
   frame format hyperspectral camera and an RGB camera in highly variable
   weather conditions. Dense point clouds were measured photogrammetrically
   by automatic image matching using high resolution RGB images with a 5 cm
   point interval. Spectral features were obtained from the hyperspectral
   image blocks, the large radiometric variation of which was compensated
   for by using a novel approach based on radiometric block adjustment with
   the support of in-flight irradiance observations. Spectral and 3D point
   cloud features were used in the classification experiment with various
   classifiers. The best results were obtained with Random Forest and
   Multilayer Perceptron (MLP) which both gave 95\% overall accuracies and
   an F-score of 0.93. Accuracy of individual tree identification from the
   photogrammetric point clouds varied between 40\% and 95\%, depending on
   the characteristics of the area. Challenges in reference measurements
   might also have reduced these numbers. Results were promising,
   indicating that hyperspectral 3D remote sensing was operational from a
   UAV platform even in very difficult conditions. These novel methods are
   expected to provide a powerful tool for automating various environmental
   close-range remote sensing tasks in the very near future.}},
DOI = {{10.3390/rs9030185}},
Article-Number = {{185}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Imai, Nilton/O-8909-2018}},
ORCID-Numbers = {{Nevalainen, Olli/0000-0002-4826-2929
   Honkavaara, Eija/0000-0002-7236-2145
   Imai, Nilton/0000-0003-0516-0567}},
Unique-ID = {{ISI:000398720100002}},
}

@article{ ISI:000395521200012,
Author = {Chen, Jingdao and Fang, Yihai and Cho, Yong K. and Kim, Changwan},
Title = {{Principal Axes Descriptor for Automated Construction-Equipment
   Classification from Point Clouds}},
Journal = {{JOURNAL OF COMPUTING IN CIVIL ENGINEERING}},
Year = {{2017}},
Volume = {{31}},
Number = {{2}},
Month = {{MAR}},
Abstract = {{Recognizing construction assets (e.g.,materials, equipment, labor) from
   point cloud data of construction environments provides essential
   information for engineering and management applications including
   progress monitoring, safety management, supply-chain management, and
   quality control. This study introduces a novel principal axes descriptor
   (PAD) for construction-equipment classification from point cloud data.
   Scattered as-is point clouds are first processed with downsampling,
   segmentation, and clustering steps to obtain individual instances of
   construction equipment. A geometric descriptor consisting of dimensional
   variation, occupancy distribution, shape profile, and plane counting
   features is then calculated to encode three-dimensional (3D)
   characteristics of each equipment category. Using the derived features,
   machine learning methods such as k-nearest neighbors and support vector
   machine are employed to determine class membership among major
   construction-equipment categories such as backhoe loader, bulldozer,
   dump truck, excavator, and front loader. Construction-equipment
   classification with the proposed PAD was validated using computer-aided
   design (CAD)-generated point clouds as training data and laser-scanned
   point clouds from an equipment yard as testing data. The recognition
   performance was further evaluated using point clouds from a construction
   site as well as a pose variation data set. PAD was shown to achieve a
   higher recall rate and lower computation time compared to competing 3D
   descriptors. The results indicate that the proposed descriptor is a
   viable solution for construction-equipment classification from point
   cloud data. (C) 2016 American Society of Civil Engineers.}},
DOI = {{10.1061/(ASCE)CP.1943-5487.0000628}},
Article-Number = {{04016058}},
ISSN = {{0887-3801}},
EISSN = {{1943-5487}},
ORCID-Numbers = {{Fang, Yihai/0000-0002-9451-4947}},
Unique-ID = {{ISI:000395521200012}},
}

@article{ ISI:000400896700002,
Author = {Rhee, Seon-Min and Yoo, ByungIn and Han, Jae-Joon and Hwang, Wonjun},
Title = {{Deep neural network using color and synthesized three-dimensional shape
   for face recognition}},
Journal = {{JOURNAL OF ELECTRONIC IMAGING}},
Year = {{2017}},
Volume = {{26}},
Number = {{2}},
Month = {{MAR}},
Abstract = {{We present an approach for face recognition using synthesized
   three-dimensional (3-D) shape information together with two-dimensional
   (2-D) color in a deep convolutional neural network (DCNN). As 3-D facial
   shape is hardly affected by the extrinsic 2-D texture changes caused by
   illumination, make-up, and occlusions, it could provide more reliable
   complementary features in harmony with the 2-D color feature in face
   recognition. Unlike other approaches that use 3-D shape information with
   the help of an additional depth sensor, our approach generates a
   personalized 3-D face model by using only face landmarks in the 2-D
   input image. Using the personalized 3-D face model, we generate a
   frontalized 2-D color facial image as well as 3-D facial images (e.g., a
   depth image and a normal image). In our DCNN, we first feed 2-D and 3-D
   facial images into independent convolutional layers, where the low-level
   kernels are successfully learned according to their own characteristics.
   Then, we merge them and feed into higher-level layers under a single
   deep neural network. Our proposed approach is evaluated with labeled
   faces in the wild dataset and the results show that the error rate of
   the verification rate at false acceptance rate 1\% is improved by up to
   32.1\% compared with the baseline where only a 2-D color image is used.
   (C) 2017 SPIE and IS\&T}},
DOI = {{10.1117/1.JEI.26.2.020502}},
Article-Number = {{020502}},
ISSN = {{1017-9909}},
EISSN = {{1560-229X}},
Unique-ID = {{ISI:000400896700002}},
}

@article{ ISI:000397013700050,
Author = {Hu, Xingbo and Chen, Wei and Xu, Weiyang},
Title = {{Adaptive Mean Shift-Based Identification of Individual Trees Using
   Airborne LiDAR Data}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Identifying individual trees and delineating their canopy structures
   from the forest point cloud data acquired by an airborne LiDAR (Light
   Detection And Ranging) has significant implications in forestry
   inventory. Once accurately identified, tree structural attributes such
   as tree height, crown diameter, canopy based height and diameter at
   breast height can be derived. This paper focuses on a novel
   computationally efficient method to adaptively calibrate the kernel
   bandwidth of a computational scheme based on mean shift-a non-parametric
   probability density-based clustering technique-to segment the 3D
   (three-dimensional) forest point clouds and identify individual tree
   crowns. The basic concept of this method is to partition the 3D space
   over each test plot into small vertical units (irregular columns
   containing 3D spatial features from one or more trees) first, by using a
   fixed bandwidth mean shift procedure and a small square grouping
   technique, and then rough estimation of crown sizes for distinct trees
   within a unit, based on an original 2D (two-dimensional) incremental
   grid projection technique, is applied to provide a basis for dynamical
   calibration of the kernel bandwidth for an adaptive mean shift procedure
   performed in each partition. The adaptive mean shift-based scheme, which
   incorporates our proposed bandwidth calibration method, is validated on
   10 test plots of a dense, multi-layered evergreen broad-leaved forest
   located in South China. Experimental results reveal that this approach
   can work effectively and when compared to the conventional point-based
   approaches (e.g., region growing, k-means clustering, fixed bandwidth or
   multi-scale mean shift), its accuracies are relatively high: it detects
   86 percent of the trees ({''}recall{''}) and 92 percent of the
   identified trees are correct ({''}precision{''}), showing good potential
   for use in the area of forest inventory.}},
DOI = {{10.3390/rs9020148}},
Article-Number = {{148}},
ISSN = {{2072-4292}},
ORCID-Numbers = {{xu, weiyang/0000-0002-8980-6005}},
Unique-ID = {{ISI:000397013700050}},
}

@article{ ISI:000397013700010,
Author = {Yu, Xiaowei and Hyyppa, Juha and Litkey, Paula and Kaartinen, Harri and
   Vastaranta, Mikko and Holopainen, Markus},
Title = {{Single-Sensor Solution to Tree Species Classification Using
   Multispectral Airborne Laser Scanning}},
Journal = {{REMOTE SENSING}},
Year = {{2017}},
Volume = {{9}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{This paper investigated the potential of multispectral airborne laser
   scanning (ALS) data for individual tree detection and tree species
   classification. The aim was to develop a single-sensor solution for
   forest mapping that is capable of providing species-specific
   information, required for forest management and planning purposes.
   Experiments were conducted using 1903 ground measured trees from 22
   sample plots and multispectral ALS data, acquired with an Optech Titan
   scanner over a boreal forest, mainly consisting of Scots pine (Pinus
   Sylvestris), Norway spruce (Picea Abies), and birch (Betula sp.), in
   southern Finland. ALS-features used as predictors for tree species were
   extracted from segmented tree objects and used in random forest
   classification. Different combinations of features, including point
   cloud features, and intensity features of single and multiple channels,
   were tested. Among the field-measured trees, 61.3\% were correctly
   detected. The best overall accuracy (OA) of tree species classification
   achieved for correctly-detected trees was 85.9\% (Kappa = 0.75), using a
   point cloud and single-channel intensity features combination, which was
   not significantly different from the ones that were obtained either
   using all features (OA = 85.6\%, Kappa = 0.75), or single-channel
   intensity features alone (OA = 85.4\%, Kappa = 0.75). Point cloud
   features alone achieved the lowest accuracy, with an OA of 76.0\%.
   Field-measured trees were also divided into four categories. An
   examination of the classification accuracy for four categories of trees
   showed that isolated and dominant trees can be detected with a detection
   rate of 91.9\%, and classified with a high overall accuracy of 90.5\%.
   The corresponding detection rate and accuracy were 81.5\% and 89.8\% for
   a group of trees, 26.4\% and 79.1\% for trees next to a larger tree, and
   7.2\% and 53.9\% for trees situated under a larger tree, respectively.
   The results suggest that Channel 2 (1064 nm) contains more information
   for separating pine, spruce, and birch, followed by channel 1 (1550 nm)
   and channel 3 (532 nm) with an overall accuracy of 81.9\%, 78.3\%, and
   69.1\%, respectively. Our results indicate that the use of multispectral
   ALS data has great potential to lead to a single-sensor solution for
   forest mapping.}},
DOI = {{10.3390/rs9020108}},
ISSN = {{2072-4292}},
ResearcherID-Numbers = {{Kaartinen, Harri/B-1474-2015
   Vastaranta, Mikko/K-9656-2018}},
ORCID-Numbers = {{Vastaranta, Mikko/0000-0001-6552-9122}},
Unique-ID = {{ISI:000397013700010}},
}

@article{ ISI:000395844700002,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Title = {{Statistical non-rigid ICP algorithm and its application to 3D face
   alignment}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{58}},
Pages = {{3-12}},
Month = {{FEB}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means Of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region. (C) 2016 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.imavis.2016.10.007}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000395844700002}},
}

@article{ ISI:000395844700018,
Author = {Zeng, Dan and Zhao, Qijun and Long, Shuqin and Li, Jing},
Title = {{Examplar coherent 3D face reconstruction from forensic mugshot database}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{58}},
Pages = {{193-203}},
Month = {{FEB}},
Abstract = {{Reconstructing 3D face models from 2D face images is usually done by
   using a single reference 3D face model or some gender/ethnicity specific
   3D face models. However, different persons, even those of the same
   gender or ethnicity, usually have significantly different faces in terms
   of their overall appearance, which forms the base of person recognition
   via faces. Consequently, existing 3D reference model based methods have
   limited capability of reconstructing precise 3D face models for a large
   variety of persons. In this paper, we propose to explore a reservoir of
   diverse reference models for 3D face reconstruction from forensic
   mugshot face images, where facial examplars coherent with the input
   determine the final shape estimation. Specifically, our 3D face
   reconstruction is formulated as an energy minimization problem with: 1)
   shading constraint from multiple input face images, 2) distortion and
   self-occlusion based color consistency between different views, and 3)
   depth uncertainty based smoothness constraint on adjacent pixels. The
   proposed energy is minimized in a coarse to fine way, where the shape
   refinement step is done by using a multi label segmentation algorithm.
   Experimental results on challenging datasets demonstrate that the
   proposed algorithm is capable of recovering high quality 3D face models.
   We also show that our reconstructed models successfully boost face
   recognition accuracy. (C) 2016 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2016.03.001}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
ResearcherID-Numbers = {{Zeng, Dan/M-4615-2019}},
ORCID-Numbers = {{Zeng, Dan/0000-0002-9036-7791}},
Unique-ID = {{ISI:000395844700018}},
}

@article{ ISI:000397220000009,
Author = {Akhavein, Hassan and Farivar, Reza},
Title = {{Gaze behavior during 3-D face identification is depth cue invariant}},
Journal = {{JOURNAL OF VISION}},
Year = {{2017}},
Volume = {{17}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Gaze behavior during scene and object recognition can highlight the
   relevant information for a task. For example, salience maps-highlighting
   regions that have heightened luminance, contrast, color, etc. in a
   scenecan be used to predict gaze targets. Certain tasks, such as face
   recognition, result in a typical pattern of fixations on high salience
   features. While local salience of a 2-D feature may contribute to gaze
   behavior and object recognition, we are perfectly capable of recognizing
   objects from 3-D depth cues devoid of meaningful 2-D features. Faces can
   be recognized from pure texture, binocular disparity, or
   structure-from-motion displays (Dehmoobadsharifabadi \& Farivar, 2016;
   Farivar, Blanke, \& Chaudhuri, 2009; Liu, Collin, Farivar, \& Chaudhuri,
   2005), and yet these displays are devoid of local salient 2-D features.
   We therefore sought to determine whether gaze behavior is driven by an
   underlying 3-D representation that is depth-cue invariant or depth-cue
   specific. By using a face identification task comprising morphs of 3-D
   facial surfaces, we were able to measure identification thresholds and
   thereby equate for task difficulty across different depth cues. We found
   that gaze behavior for faces defined by shading and texture cues was
   highly comparable, but we observed some deviations for faces defined by
   binocular disparity. Interestingly, we found no effect of task
   difficulty on gaze behavior. The results are discussed in the context of
   depth-cue invariant representations for facial surfaces, with gaze
   behavior being constrained by low-level limits of depth extraction from
   specific cues such as binocular disparity.}},
DOI = {{10.1167/17.2.9}},
Article-Number = {{9}},
ISSN = {{1534-7362}},
Unique-ID = {{ISI:000397220000009}},
}

@article{ ISI:000391965900001,
Author = {Dinh-Cuong Hoang and Liang-Chia Chen and Thanh-Hung Nguyen},
Title = {{Sub-OBB based object recognition and localization algorithm using range
   images}},
Journal = {{MEASUREMENT SCIENCE AND TECHNOLOGY}},
Year = {{2017}},
Volume = {{28}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{This paper presents a novel approach to recognize and estimate pose of
   the 3D objects in cluttered range images. The key technical breakthrough
   of the developed approach can enable robust object recognition and
   localization under undesirable condition such as environmental
   illumination variation as well as optical occlusion to viewing the
   object partially. First, the acquired point clouds are segmented into
   individual object point clouds based on the developed 3D object
   segmentation for randomly stacked objects. Second, an efficient
   shape-matching algorithm called Sub-OBB based object recognition by
   using the proposed oriented bounding box (OBB) regional area-based
   descriptor is performed to reliably recognize the object. Then, the 3D
   position and orientation of the object can be roughly estimated by
   aligning the OBB of segmented object point cloud with OBB of matched
   point cloud in a database generated from CAD model and 3D virtual
   camera. To detect accurate pose of the object, the iterative closest
   point (ICP) algorithm is used to match the object model with the
   segmented point clouds. From the feasibility test of several scenarios,
   the developed approach is verified to be feasible for object pose
   recognition and localization.}},
DOI = {{10.1088/1361-6501/aa513a}},
Article-Number = {{025401}},
ISSN = {{0957-0233}},
EISSN = {{1361-6501}},
Unique-ID = {{ISI:000391965900001}},
}

@inproceedings{ ISI:000428410700165,
Author = {Hong, Sungeun and Im, Woobin and Ryu, Jongbin and Yang, Hyun S.},
Book-Group-Author = {{IEEE}},
Title = {{SSPP-DAN: DEEP DOMAIN ADAPTATION NETWORK FOR FACE RECOGNITION WITH
   SINGLE SAMPLE PER PERSON}},
Booktitle = {{2017 24TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2017}},
Pages = {{825-829}},
Note = {{24th IEEE International Conference on Image Processing (ICIP), Beijing,
   PEOPLES R CHINA, SEP 17-20, 2017}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc}},
Abstract = {{Real-world face recognition using a single sample per person (SSPP) is a
   challenging task. The problem is exacerbated if the conditions under
   which the gallery image and the probe set are captured are completely
   different. To address these issues from the perspective of domain
   adaptation, we introduce an SSPP domain adaptation network (SSPP-DAN).
   In the proposed approach, domain adaptation, feature extraction, and
   classification are performed jointly using a deep architecture with
   domain-adversarial training. However, the SSPP characteristic of one
   training sample per class is insufficient to train the deep
   architecture. To overcome this shortage, we generate synthetic images
   with varying poses using a 3D face model. Experimental evaluations using
   a realistic SSPP dataset show that deep domain adaptation and image
   synthesis complement each other and dramatically improve accuracy.
   Experiments on a benchmark dataset using the proposed approach show
   state-of-the-art performance.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-5090-2175-8}},
Unique-ID = {{ISI:000428410700165}},
}

@inproceedings{ ISI:000425498402048,
Author = {Liu, Liu and Li, Hongdong and Dai, Yuchao},
Book-Group-Author = {{IEEE}},
Title = {{Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale
   3D Map}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)}},
Series = {{IEEE International Conference on Computer Vision}},
Year = {{2017}},
Pages = {{2391-2400}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{Given an image of a street scene in a city, this paper develops a new
   method that can quickly and precisely pinpoint at which location (as
   well as viewing direction) the image was taken, against a pre-stored
   large-scale 3D point-cloud map of the city. We adopt the recently
   developed 2D-3D direct feature matching framework for this task
   {[}23,31,32,42-44]. This is a challenging task especially for
   large-scale problems. As the map size grows bigger, many 3D points in
   the wider geographical area can be visually very similar-or even
   identical-causing severe ambiguities in 2D-3D feature matching. The key
   is to quickly and unambiguously find the correct matches between a query
   image and the large 3D map. Existing methods solve this problem mainly
   via comparing individual features' visual similarities in a local and
   per feature manner, thus only local solutions can be found, inadequate
   for large-scale applications.
   In this paper, we introduce a global method which harnesses global
   contextual information exhibited both within the query image and among
   all the 3D points in the map. This is achieved by a novel global ranking
   algorithm, applied to a Markov network built upon the 3D map, which
   takes account of not only visual similarities between individual 2D-3D
   matches, but also their global compatibilities (as measured by
   co-visibility) among all matching pairs found in the scene. Tests on
   standard benchmark datasets show that our method achieved both higher
   precision and comparable recall, compared with the state-of-the-art.}},
DOI = {{10.1109/ICCV.2017.260}},
ISSN = {{1550-5499}},
ISBN = {{978-1-5386-1032-9}},
ORCID-Numbers = {{Liu, Liu/0000-0002-5880-5974}},
Unique-ID = {{ISI:000425498402048}},
}

@article{ ISI:000397995100002,
Author = {Ahmed, Oumer S. and Shemrock, Adam and Chabot, Dominique and Dillon,
   Chris and Williams, Griffin and Wasson, Rachel and Franklin, Steven E.},
Title = {{Hierarchical land cover and vegetation classification using
   multispectral data acquired from an unmanned aerial vehicle}},
Journal = {{INTERNATIONAL JOURNAL OF REMOTE SENSING}},
Year = {{2017}},
Volume = {{38}},
Number = {{8-10}},
Pages = {{2037-2052}},
Note = {{Conference on Small Unmanned Aerial Systems (sUAS) for Environmental
   Research, Univ Worcester, Worcester, ENGLAND, JUN, 2016}},
Abstract = {{The use of multispectral cameras deployed on unmanned aerial vehicles
   (UAVs) in land cover and vegetation mapping applications continues to
   improve and receive increasing recognition and adoption by resource
   management and forest survey practitioners. Comparisons of different
   camera data and platform performance characteristics are an important
   contribution in understanding the role and operational capability of
   this technology. In this article, object-based classification accuracies
   for different cover types and vegetation species of interest in central
   Ontario were examined using data from three UAV-based multispectral
   cameras. Five land-cover classes (forest, shrub, herbaceous, bare soil,
   and built-up) were determined to be up to 95\% correct overall with
   calibrated multispectral Parrot Sequoia digital camera data compared to
   independent field observations. The levels of classification accuracy
   decreased approximately 10-15\% when spectrally less capable
   consumer-grade RGB sensors were used. Multispectral Parrot Sequoia
   classification accuracy was approximately 89\% when more detailed
   vegetation classes, including individual deciduous tree species, shrub
   communities and agricultural crops, were analysed. Additional work is
   suggested in the use of such UAV multispectral and point cloud data in
   ash tree discrimination to support emerald ash borer infestation
   detection and management, and in analysis of functional and structural
   vegetation characteristics (e.g. leaf area index).}},
DOI = {{10.1080/01431161.2017.1294781}},
ISSN = {{0143-1161}},
EISSN = {{1366-5901}},
Unique-ID = {{ISI:000397995100002}},
}

@inproceedings{ ISI:000426330000025,
Author = {Hu, Huiying and Shah, Syed Afaq Ali and Bennamoun, Mohammed and Molton,
   Michael},
Book-Group-Author = {{IEEE}},
Title = {{2D and 3D Face Recognition Using Convolutional Neural Network}},
Booktitle = {{TENCON 2017 - 2017 IEEE REGION 10 CONFERENCE}},
Series = {{TENCON IEEE Region 10 Conference Proceedings}},
Year = {{2017}},
Pages = {{133-138}},
Note = {{IEEE Region 10 Conference (TENCON), MALAYSIA, NOV 05-08, 2017}},
Organization = {{IEEE; IEEE Region 10}},
Abstract = {{Face recognition remains a challenge today as recognition performance is
   strongly affected by variability such as illumination, expressions and
   poses. In this work we apply Convolutional Neural Networks (CNNs) on the
   challenging task of both 2D and 3D face recognition. We constructed two
   CNN models, namely CNN-1 (two convolutional layers) and CNN-2 (one
   convolutional layer) for testing on 2D and 3D dataset. A comprehensive
   parametric study of two CNN models on face recognition is represented in
   which different combinations of activation function, learning rate and
   filter size are investigated. We find that CNN-2 has a better accuracy
   performance on both 2D and 3D face recognition. Our experimental results
   show that an accuracy of 85.15\% was accomplished using CNN-2 on depth
   images with FRGCv2.0 dataset (4950 images with 557 objectives). An
   accuracy of 95\% was achieved using CNN-2 on 2D raw image with the AT\&T
   dataset (400 images with 40 objectives). The results indicate that the
   proposed CNN model is capable to handle complex information from facial
   images in different dimensions. These results provide valuable insights
   into further application of CNN on 3D face recognition.}},
ISSN = {{2159-3442}},
ISBN = {{978-1-5090-1134-6}},
ORCID-Numbers = {{Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000426330000025}},
}

@inproceedings{ ISI:000446968900004,
Author = {Tang, Yinhang and Chen, Liming},
Editor = {{BenAmor, B and Chaieb, F and Ghorbel, F}},
Title = {{Shape Analysis Based Anti-spoofing 3D Face Recognition with Mask Attacks}},
Booktitle = {{REPRESENTATIONS, ANALYSIS AND RECOGNITION OF SHAPE AND MOTION FROM
   IMAGING DATA}},
Series = {{Communications in Computer and Information Science}},
Year = {{2017}},
Volume = {{684}},
Pages = {{41-55}},
Note = {{6th International Workshop on Representations, Analysis and Recognition
   of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
   OCT 27-29, 2016}},
Abstract = {{With the growth of face recognition, the spoofing mask attacks attract
   more attention in biometrics research area. In recent years, the
   countermeasures based on the texture and depth image against spoofing
   mask attacks have been reported, but the research based on 3D meshed
   sample has not been studied yet. In this paper, we propose to apply 3D
   shape analysis based on principal curvature measures to describe the
   meshed facial surface. Meanwhile, a verification protocol based on this
   feature descriptor is designed to verify person identity and to evaluate
   the anti-spoofing performance on Morpho database. Furthermore, for
   simulating a real-life testing scenario, FRGCv2 database is enrolled as
   an extension of face scans to augment the ratio of genuine face samples
   to fraud mask samples. The experimental results show that our system can
   guarantee a high verification rate for genuine faces and the
   satisfactory anti-spoofing performance against spoofing mask attacks in
   parallel.}},
DOI = {{10.1007/978-3-319-60654-5\_4}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-319-60654-5; 978-3-319-60653-8}},
Unique-ID = {{ISI:000446968900004}},
}

@inproceedings{ ISI:000446968900007,
Author = {Xia, Baiqiang},
Editor = {{BenAmor, B and Chaieb, F and Ghorbel, F}},
Title = {{3D Nasal Shape: A New Basis for Soft-Biometrics Recognition}},
Booktitle = {{REPRESENTATIONS, ANALYSIS AND RECOGNITION OF SHAPE AND MOTION FROM
   IMAGING DATA}},
Series = {{Communications in Computer and Information Science}},
Year = {{2017}},
Volume = {{684}},
Pages = {{75-83}},
Note = {{6th International Workshop on Representations, Analysis and Recognition
   of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
   OCT 27-29, 2016}},
Abstract = {{In the past 10 years, Soft-Biometrics recognition using 3D face has
   become prevailing, with many successful research works developed. In
   contrast, the usage of facial parts for Soft-Biometrics recognition
   remains less investigated. In particular, the nasal shape contains rich
   information for demographic perception. They are usually free from
   hair/glasses occlusions, and stay robust to facial expressions, which
   are challenging issues 3D face analysis. In this work, we propose the
   idea of 3D nasal Soft-Biometrics recognition. To this end, the simple 3D
   coordinates features are derived from the radial curves representation
   of the 3D nasal shape. With the 466 earliest scans of FRGCv2 dataset
   (mainly neutral), we achieved 91\% gender (Male/Female) and 94\%
   ethnicity (Asian/Non-asian) classification rates in 10-fold
   cross-validation. It demonstrates the richness of the nasal shape in
   presenting the two Soft-Biometrics, and the effectiveness of the
   proposed recognition scheme. The performances are further confirmed by
   more rigorous cross-dataset experiments, which also demonstrates the
   generalization ability of propose approach. When experimenting on the
   whole FRGCv2 dataset (40\% are expressive), comparable recognition
   performances are achieved, which confirms the general knowledge that the
   nasal shape stays robust during facial expressions.}},
DOI = {{10.1007/978-3-319-60654-5\_7}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-319-60654-5; 978-3-319-60653-8}},
Unique-ID = {{ISI:000446968900007}},
}

@inproceedings{ ISI:000446968900008,
Author = {Werghi, Naoufel and Drira, Hassen},
Editor = {{BenAmor, B and Chaieb, F and Ghorbel, F}},
Title = {{Towards a Methodology for Retrieving Suspects Using 3D Facial
   Descriptors}},
Booktitle = {{REPRESENTATIONS, ANALYSIS AND RECOGNITION OF SHAPE AND MOTION FROM
   IMAGING DATA}},
Series = {{Communications in Computer and Information Science}},
Year = {{2017}},
Volume = {{684}},
Pages = {{84-94}},
Note = {{6th International Workshop on Representations, Analysis and Recognition
   of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
   OCT 27-29, 2016}},
Abstract = {{We propose a first investigation towards a methodology for exploiting 3D
   descriptors in suspect retrieval in the context of crime investigation.
   In this field, the standard method is to construct a facial composite,
   based on witness description, by an artist of via software, then search
   a match for it in legal databases. An alternative or complementary
   scheme would be to define a system of 3D facial attributes that can fit
   human verbal face description and use them to annotate face databases.
   Such framework allows a more efficient search of legal face database and
   more effective suspect shortlisting. In this paper, we describe some
   first steps towards that goal, whereby we define some novel 3D face
   attributes, we analyze their capacity for face categorization though a
   hieratical clustering analysis. Then we present some experiments, using
   a cohort of 107 subjects, assessing the extent to which some faces
   partition based on some of these attributes meets its human-based
   counterpart. Both the clustering analysis and the experiments results
   reveal encouraging indicators for this novel proposed scheme.}},
DOI = {{10.1007/978-3-319-60654-5\_8}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-319-60654-5; 978-3-319-60653-8}},
ResearcherID-Numbers = {{Werghi, Naoufel/D-2398-2018}},
ORCID-Numbers = {{Werghi, Naoufel/0000-0002-5542-448X}},
Unique-ID = {{ISI:000446968900008}},
}

@inproceedings{ ISI:000444905600019,
Author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Declercq, David and
   Benouareth, Abdallah},
Editor = {{Imai, F and Tremeau, A and Braz, J}},
Title = {{Geometrical and Visual Feature Quantization for 3D Face Recognition}},
Booktitle = {{PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER
   VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP
   2017), VOL 5}},
Year = {{2017}},
Pages = {{187-193}},
Note = {{12th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP), Porto, PORTUGAL,
   FEB 27-MAR 01, 2017}},
Organization = {{Inst Syst \& Technologies Informat, Control \& Commun; ACM SIGGRAPH;
   AFIG; Eurographics}},
Abstract = {{In this paper, we present an efficient method for 3D face recognition
   based on vector quantization of both geometrical and visual proprieties
   of the face. The method starts by describing each 3D face using a set of
   orderless features, and use then the Bag-of-Features paradigm to
   construct the face signature. We analyze the performance of three
   well-known classifiers: the Naive Bayes, the Multilayer perceptron and
   the Random forests. The results reported on the FRGCv2 dataset show the
   effectiveness of our approach and prove that the method is robust to
   facial expression.}},
DOI = {{10.5220/0006101701870193}},
ISBN = {{978-989-758-226-4}},
Unique-ID = {{ISI:000444905600019}},
}

@inproceedings{ ISI:000440475200002,
Author = {Gaonkar, A. A. and Gad, M. D. and Vetrekar, N. T. and Tilve, Vithal Shet
   and Gad, R. S.},
Editor = {{Mukherjee, S and Mukherjee, S and Mukherjee, DP and Sivaswamy, J and Awate, S and Setlur, S and Namboodiri, AM and Chaudhury, S}},
Title = {{Experimental Evaluation of 3D Kinect Face Database}},
Booktitle = {{COMPUTER VISION, GRAPHICS, AND IMAGE PROCESSING, ICVGIP 2016}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10481}},
Pages = {{15-26}},
Note = {{10th Indian Conference on Vision, Graphics and Image Processing
   (ICVGIP), IIT Guwahati, Guwahati, INDIA, DEC 19, 2016}},
Abstract = {{3D face recognition has gain a paramount importance over 2D due to its
   potential to address the limitations of 2D face recognition against the
   variation in facial poses, angles, occlusions etc. Research in 3D face
   recognition has accelerated in recent years due to the development of
   low cost 3D Kinect camera sensor. This has leads to the development of
   few RGB-D database across the world. Here in this paper we introduce the
   base results of our 3D facial database (GU-RGBD database) comprising
   variation in pose (0 degrees, 45 degrees, 90 degrees, -45 degrees, -90
   degrees), expression (smile, eyes closed), occlusion (half face covered
   with paper) and illumination variation using Kinect. We present a
   proposed noise removal non-linear interpolation filter for the patches
   present in the depth images. The results were obtained on three face
   recognition algorithms and fusion at matching score level for
   recognition and verification rate. The obtained results indicated that
   the performance with our proposed filter shows improvement over pose
   with score level fusion using sum rule.}},
DOI = {{10.1007/978-3-319-68124-5\_2}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-68124-5; 978-3-319-68123-8}},
Unique-ID = {{ISI:000440475200002}},
}

@inproceedings{ ISI:000435270800012,
Author = {Ahdid, Rachid and Taifi, Khaddouj and Said, Said and Fakir, Mohamed and
   Manaut, Bouzid},
Editor = {{Banissi, E and Sarfraz, M and Zeroual, A}},
Book-Author = {{Fakir, M}},
Title = {{Automatic Face Recognition System using Iso-Geodesic Curves in Riemanian
   Manifold}},
Booktitle = {{2017 14TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS, IMAGING AND
   VISUALIZATION (CGIV 2017)}},
Series = {{International Conference on Computer Graphics Imaging and Visualization}},
Year = {{2017}},
Pages = {{73-78}},
Note = {{14th International Conference on Computer Graphics, Imaging and
   Visualization (CGiV), Marrakesh, MOROCCO, MAY 23-25, 2017}},
Abstract = {{In this paper, we present an automatic 3D face recognition system. This
   system is based on the representation of human faces surfaces as
   collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching
   algorithm. To compare two facial surfaces, we compute a geodesic
   distance between a pair of facial curves using a Riemannian geometry. In
   the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor
   (KNN) and Support Vector Machines (SVM). To test this method and
   evaluate its performance, a simulation series of experiments were
   performed on 3D Shape REtrieval Contest 2008 database (SHREC2008).}},
DOI = {{10.1109/CGiV.2017.25}},
ISBN = {{978-1-5386-0852-4}},
Unique-ID = {{ISI:000435270800012}},
}

@inproceedings{ ISI:000433001500012,
Author = {Villarini, Barbara and Gkelias, Athanasios and Argyriou, Vasilios},
Editor = {{Schwenker, F and Scherer, S}},
Title = {{Photometric Stereo for 3D Face Reconstruction Using Non Linear
   Illumination Models}},
Booktitle = {{MULTIMODAL PATTERN RECOGNITION OF SOCIAL SIGNALS IN
   HUMAN-COMPUTER-INTERACTION, MPRSS 2016}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2017}},
Volume = {{10183}},
Pages = {{140-152}},
Note = {{4th IAPR TC 9 Workshop on Pattern Recognition of Social Signals in
   Human-Computer-Interaction (MPRSS), Cancun, MEXICO, DEC 04, 2016}},
Organization = {{Int Assoc Pattern Recognit, TC 9 Pattern Recognit Human Comp Interact;
   Ulm Univ; Univ So Calif; Transreg Collaborat Res Ctr SFB TRR 62 Compan
   Technol Cognit Tech Syst}},
Abstract = {{Face recognition in presence of illumination changes, variant pose and
   different facial expressions is a challenging problem. In this paper, a
   method for 3D face reconstruction using photometric stereo and without
   knowing the illumination directions and facial expression is proposed in
   order to achieve improvement in face recognition. A dimensionality
   reduction method was introduced to represent the face deformations due
   to illumination variations and self shadows in a lower space. The
   obtained mapping function was used to determine the illumination
   direction of each input image and that direction was used to apply
   photometric stereo. Experiments with faces were performed in order to
   evaluate the performance of the proposed scheme. From the experiments it
   was shown that the proposed approach results very accurate 3D surfaces
   without knowing the light directions and with a very small differences
   compared to the case of known directions. As a result the proposed
   approach is more general and requires less restrictions enabling 3D face
   recognition methods to operate with less data.}},
DOI = {{10.1007/978-3-319-59259-6\_12}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-59259-6; 978-3-319-59258-9}},
Unique-ID = {{ISI:000433001500012}},
}

@inproceedings{ ISI:000432372100099,
Author = {Torkhani, Ghada and Ladgham, Anis and Sakly, Anis},
Book-Group-Author = {{IEEE}},
Title = {{3D Gabor-Edge Filters Applied to Face Depth Images}},
Booktitle = {{2017 18TH INTERNATIONAL CONFERENCE ON SCIENCES AND TECHNIQUES OF
   AUTOMATIC CONTROL AND COMPUTER ENGINEERING (STA)}},
Series = {{International Conference on Sciences and Techniques of Automatic Control
   and Computer Engineering}},
Year = {{2017}},
Pages = {{578-582}},
Note = {{18th International Conference on Sciences and Techniques of Automatic
   Control and Computer Engineering (STA), Monastir, TUNISIA, DEC 21-23,
   2017}},
Organization = {{IEEE Tunisia Sect; Tunisian Assoc Numer Tech \& Automat; Univ Sfax, Natl
   Engn Sch Sfax, Lab Sci \& Tech Automat Control \& Comp Engn; Tunisia
   Sect Control Syst Soc Chapter; Tunisia Sect Robot \& Automat Soc
   Chapter; Tunisia Sect Signal Proc Soc Chapter; Tunisia Sect Circuits \&
   Syst Soc Chapter; Tunisia Sect Solid State Circuits Soc Chapter}},
Abstract = {{This manuscript introduces a novel 3D face authentication system
   inspired from the advantageous capacities of Gabor-Edge filters. The
   approach studies 3D face difficulties such as expression variety,
   different rotations and exposure to illuminations. The proposed systems
   starts by preprocessing the 3D face images to resolve acquisition
   problems. Then, a filtering process is performed by implanting our 3D
   Gabor-Edge technique extended based on the classic 3D Gabor masks. The
   next step is to achieve the classification of facial features from the
   edge saliency by the artificial Neural Network Classifier (NNC). The
   evaluation of the adopted system is achieved by exporting common
   datasets from GavabDB database. Experimental results are reported to
   prove the high accuracy rates of our method compared to the recent
   researches in the same biometric field.}},
ISSN = {{2378-7163}},
ISBN = {{978-1-5386-1084-8}},
Unique-ID = {{ISI:000432372100099}},
}

@inproceedings{ ISI:000428410702188,
Author = {Soltanpour, Sima and Wu, Q. M. Jonathan},
Book-Group-Author = {{IEEE}},
Title = {{HIGH-ORDER LOCAL NORMAL DERIVATIVE PATTERN (LNDP) FOR 3D FACE
   RECOGNITION}},
Booktitle = {{2017 24TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2017}},
Pages = {{2811-2815}},
Note = {{24th IEEE International Conference on Image Processing (ICIP), Beijing,
   PEOPLES R CHINA, SEP 17-20, 2017}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc}},
Abstract = {{This paper proposes a novel descriptor based on the local derivative
   pattern (LDP) for 3D face recognition. Compared to the local binary
   pattern (LBP), LDP can capture more detailed information by encoding
   directional pattern features. It is based on the local derivative
   variations that extract high-order local information. We propose a novel
   discriminative facial shape descriptor, local normal derivative pattern
   (LNDP) that extracts LDP from the surface normal. Using surface normal,
   the orientation of a surface at each point is determined as a
   first-order surface differential. Three normal component images are
   extracted by estimating three components of normal vectors in x, y, and
   z channels. Each normal component is divided into several patches and
   encoded using LDP. The final descriptor is created by concatenating
   histograms of the LNDP on each patch. Experimental results on two famous
   3D face databases, FRGC v2.0 and Bosphorus illustrate the effectiveness
   of the proposed descriptor.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-5090-2175-8}},
Unique-ID = {{ISI:000428410702188}},
}

@inproceedings{ ISI:000427752100101,
Author = {Cheng, Zijun and Shi, Tianwei and Cui, Wenhua and Dong, Yunqi and Fang,
   Xuehan},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Recognition Based on Kinect Depth Data}},
Booktitle = {{2017 4TH INTERNATIONAL CONFERENCE ON SYSTEMS AND INFORMATICS (ICSAI)}},
Series = {{International Conference on Systems and Informatics}},
Year = {{2017}},
Pages = {{555-559}},
Note = {{4th International Conference on Systems and Informatics (ICSAI),
   Hangzhou, PEOPLES R CHINA, NOV 11-13, 2017}},
Organization = {{IEEE Syst Man \& Cybernet Soc; Shanghai Dianji Univ; Shanghai Dianji
   Univ, Sch Elect \& Informat}},
Abstract = {{In this paper, a contour map human facial recognition algorithm is
   proposed to implement the three-dimensional (3D) face recognition with
   the Kinect Xbox One. Since the scale of 3D depth data collected from
   Kinect is tremendous, the face recognition process cannot be handled in
   real time. To improve the speed and accuracy of the recognition process,
   the proposed algorithm turns the 3D depth data to the two-dimensional
   (2D) contour map. Furthermore, due to the 3D depth data obtained by
   Kinect, there is no need of expensive, ponderous and slow 3D scanners.
   Ten male and female subjects were involved in the validation experiment
   and the results verify that the proposed algorithm was feasible for face
   recognition. In addition, compared with other methods, Eigenface, Local
   Binary Patterns (LBP) and Linear Discriminant Analysis (LDA), the
   proposed algorithm has the better security and reliability.}},
ISSN = {{2474-0217}},
ISBN = {{978-1-5386-1107-4}},
Unique-ID = {{ISI:000427752100101}},
}

@inproceedings{ ISI:000427598700098,
Author = {Soltanpour, Sima and Wu, Q. M. Jonathan},
Book-Group-Author = {{IEEE}},
Title = {{Multiscale Depth Local Derivative Pattern for Sparse Representation
   Based 3D Face Recognition}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)}},
Series = {{IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings}},
Year = {{2017}},
Pages = {{560-565}},
Note = {{IEEE International Conference on Systems, Man, and Cybernetics (SMC),
   Banff, CANADA, OCT 05-08, 2017}},
Organization = {{IEEE}},
Abstract = {{3D face recognition is a popular research area due to its vast
   application in biometrics and security. Local feature-based methods gain
   importance in the recent years due to their robustness under degradation
   conditions. In this paper, a novel high-order local pattern descriptor
   in combination with sparse representation based classifier (SRC) is
   proposed for expression robust 3D face recognition. 3D point clouds are
   converted to depth maps after preprocessing. Multi-directional
   derivatives are applied in spatial space to encode the depth maps based
   on the local derivative pattern (LDP) scheme. Directional pattern
   features are calculated according to local derivative variations. Since
   LDP computes spatial relationship of neighbors in a local region, it
   extracts distinct information from the depth map. Multiscale depth-LDP
   is presented as a novel descriptor for 3D face recognition. The
   descriptor is employed along with the SRC to increase the range data
   distinctiveness. A histogram on the derivative pattern creates a spatial
   feature descriptor that represents the distinctive micro-patterns from
   3D data. We evaluate the proposed algorithm on two famous 3D face
   databases, FRGC v2.0 and Bosphorus. The experimental results demonstrate
   that the proposed approach achieves acceptable performance under facial
   expression.}},
ISSN = {{1062-922X}},
ISBN = {{978-1-5386-1645-1}},
Unique-ID = {{ISI:000427598700098}},
}

@inproceedings{ ISI:000427598702135,
Author = {Mohsin, Nasreen and Payandeh, Shahram},
Book-Group-Author = {{IEEE}},
Title = {{Localization and Identification of Body Extremities Based on Data from
   Multiple Depth Sensors}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)}},
Series = {{IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings}},
Year = {{2017}},
Pages = {{2736-2741}},
Note = {{IEEE International Conference on Systems, Man, and Cybernetics (SMC),
   Banff, CANADA, OCT 05-08, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper explores the novel use of multiple depth sensors to overcome
   occlusions and improve localization and tracking of body extremities.
   The usage of data from only depth sensors not only overcomes visual
   challenges associated with RGB sensors under low illumination, but also
   protects the identity of surveyed person with high confidentiality. For
   integrating depth information from multiple sources, the paper presents
   first an overview of a novel calibration method for multiple depth
   sensors. In case of occlusion of any fiducial point in the primary
   sensor's depth image, co-ordinates of the point can be obtained from the
   frame of other sensors using the calibration parameters. To localize
   salient body parts such as hands, head and feet, a surface triangular
   mesh is applied on generated 3D point cloud from the primary sensor. The
   geodesic extrema from the mesh coincide with body extremities. The body
   extremities can be identified based on those relative geodesic distances
   between the extremities. Once the body parts are labelled, a portion of
   body can be targeted and evaluated for specific gait analysis and
   visualization. For the performance evaluation, our calibration method
   has fared well in comparison to other available techniques. Also, our
   proposed localization of salient body parts is able to successfully tag
   the specific body part i.e. the head region.}},
ISSN = {{1062-922X}},
ISBN = {{978-1-5386-1645-1}},
Unique-ID = {{ISI:000427598702135}},
}

@inproceedings{ ISI:000427083300058,
Author = {Jazouli, Maha and Majda, Aicha and Zarghili, Arsalane},
Editor = {{Nfaoui, E and Boumhidi, J and Sabri, A and Yahyaouy, A and Bouchaffra, D}},
Title = {{A \$P Recognizer for Automatic Facial Emotion Recognition using Kinect
   Sensor}},
Booktitle = {{2017 INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV)}},
Year = {{2017}},
Note = {{Intelligent Systems and Computer Vision (ISCV), Fez, MOROCCO, APR 17-19,
   2017}},
Organization = {{IEEE; Univ Sidi Mohamed ben Abdellah; IEEE Comp Soc; IEEE Morocco Sect}},
Abstract = {{Autism is a developmental disorder involving qualitative impairments in
   social interaction. One source of those impairments are difficulties
   with facial expressions of emotion. Autistic people often have
   difficulty to recognize or to understand other people's emotions and
   feelings, or expressing their own. This work proposes a method to
   automatically recognize seven basic emotions among autistic children in
   real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and
   Neutral. The method uses the Microsoft Kinect sensor to track and
   identify points of interest from the 3D face model and it is based on
   the \$P point-cloud recognizer to identify multi-stroke emotions as
   point-clouds. The experimental results show that our system can achieve
   above 94.28\% recognition rate. Our study provides a novel clinical tool
   to help children with autism to assisting doctors in operating rooms.}},
ISBN = {{978-1-5090-4062-9}},
Unique-ID = {{ISI:000427083300058}},
}

@inproceedings{ ISI:000427293600008,
Author = {Frikha, Tarek and Chaabane, Faten and Said, Boukhchim and Drira, Hassen
   and Abid, Mohamed and Ben Amar, Chokri},
Editor = {{ElHassouni, M and Karim, M and BenHamida, A and BenSlima, A and Solaiman, B}},
Title = {{Embedded approach for a Riemannian-based framework of analyzing 3D faces}},
Booktitle = {{2017 3RD INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)}},
Year = {{2017}},
Pages = {{43-47}},
Note = {{3rd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP), Fez, MOROCCO, MAY 22-24, 2017}},
Organization = {{Univ Sidi Mohamed Ben Abdellah; Fac Sci; Fac Med \& Pharm; CNRST; TICSM;
   IEEE Morocco Sect; IEEE Signal Proc Soc Morocco Chapter}},
Abstract = {{Developing multimedia embedded applications continues to flourish. In
   fact, a biometric facial recognition system can be used not only on PCs
   abut also in embedded systems, it is a potential enhancer to meet
   security and surveillance needs. The analysis of facial recognition
   consists offoursteps: face analysis, face expressions' recognition,
   missing data completion and full face recognition.
   This paper proposes a hardware architecture based on an adaptation
   approach foran algorithm which has proven good face detection and
   recognition in 3D space. The proposed application was tested using a co
   design technique based on a mixed Hardware Software architecture: the
   FPGA platform.}},
ISBN = {{978-1-5386-0551-6}},
Unique-ID = {{ISI:000427293600008}},
}

@inproceedings{ ISI:000427154700002,
Author = {Zavan, Flavio H. de B. and Gasparin, Nathaly and Batista, Julio C. and
   Silva, Luan P. E. and Albiero, Vitor and Bellon, Olga R. P. and Silva,
   Luciano},
Book-Group-Author = {{IEEE}},
Title = {{Face Analysis in the Wild}},
Booktitle = {{2017 30TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES TUTORIALS
   (SIBGRAPI-T)}},
Series = {{SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing}},
Year = {{2017}},
Pages = {{9-16}},
Note = {{30th SIBGRAPI Conference on Graphics, Patterns and Images Tutorials
   (SIBGRAPI-T), Niteroi, BRAZIL, OCT 17-20, 2017}},
Organization = {{Brazilian Comp Soc; UFRJ; PUC Rio; NVIDIA; IBM; Univ Fed Fluminense,
   Inst Computacao; Univ Fed Rio Janeiro, Programa Engn Sistemas
   Computacao; Pontificia Univ Catolica Rio aneiro, Dept Informatica; Univ
   Fed Rio Janeiro; ACM SIGGRAPH; CAPES; CNPq; SIBGRAPI}},
Abstract = {{With the global demand for extra security systems, and the growing of
   human-machine interaction, facial analysis in unconstrained environments
   (in the wild) became a hot-topic in recent computer vision research.
   Unconstrained environments include surveillance footage, social media
   photos and live broadcasts. This type of images and videos include no
   control over illumination, position, size, occlusion, and facial
   expressions. Successful facial processing methods for controlled
   scenarios are unable to pledge with challenging circumstances.
   Consequently, methods tailored for handling those situations are
   indispensable for the face analysis research progress. This work
   presents a comprehensive review of state-of-the-art methods, drawing
   attention to the complications derived from in the wild scenarios and
   the behavior differences when applied to the controlled images. The main
   topics to be covered are: (1) face detection; (2) facial image quality;
   (3) head pose estimation; (4) face alignment; (5) 3D face
   reconstruction; (6) gender and age estimation; (7) facial expressions
   and emotions; and (8) face recognition. Finally, available code and
   applications for in the wild face analysis are presented, followed by a
   discussion on future directions.}},
DOI = {{10.1109/SIBGRAPI-T.2017.11}},
ISSN = {{1530-1834}},
ISBN = {{978-1-5386-0619-3}},
Unique-ID = {{ISI:000427154700002}},
}

@inproceedings{ ISI:000426973200017,
Author = {Kim, Donghyun and Hernandez, Matthias and Choi, Jongmoo and Medioni,
   Gerard},
Book-Group-Author = {{IEEE}},
Title = {{Deep 3D Face Identification}},
Booktitle = {{2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)}},
Year = {{2017}},
Pages = {{133-142}},
Note = {{IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
   OCT 01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{We propose a novel 3D face recognition algorithm using a deep
   convolutional neural network (DCNN) and a 3D face expression
   augmentation technique. The performance of 2D face recognition
   algorithms has significantly increased by leveraging the
   representationalp ower of deep neural networks and the use of
   large-scale labeled training data. In this paper, we show that transfer
   learning from a CNN trained on 2D face images can effectively work for
   3D face recognition by fine-tuning the CNN with an extremely small
   number of 3D facial scans. We also propose a 3D face expression
   augmentation technique which synthesizes a number of different facial
   expressions from a single 3D face scan. Our proposed method shows
   excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets
   without using hand-crafted features. The 3D face identification using
   our deep features also scales well for large databases.}},
ISBN = {{978-1-5386-1124-1}},
Unique-ID = {{ISI:000426973200017}},
}

@inproceedings{ ISI:000426973200029,
Author = {Li, Huibin and Sun, Jian and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Location-Sensitive Sparse Representation of Deep Normal Patterns for
   Expression-robust 3D Face Recognition}},
Booktitle = {{2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)}},
Year = {{2017}},
Pages = {{234-242}},
Note = {{IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
   OCT 01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{This paper presents a straight-forward yet efficient, and
   expression-robust 3D face recognition approach by exploring location
   sensitive sparse representation of deep normal patterns (DNP). In
   particular given raw 3D facial surfaces, we first run 3D face
   pre-processing pipeline, including nose tip detection, face region
   cropping, and pose normalization. The 3D coordinates of each normalized
   3D facial surface are then projected into 2D plane to generate geometry
   images, from which three images of facial surface normal components are
   estimated. Each normal image is then fed into a pre-trained deep face
   net to generate deep representations of facial surface normals, i.e.,
   deep normal patterns. Considering the importance of different facial
   locations, we propose a location sensitive sparse representation
   classifier (LS-SRC) for similarity measure among deep normal patterns
   associated with different 3D faces. Finally, simple score-level fusion
   of different normal components are used for the final decision. The
   proposed approach achieves significantly high performance, and reporting
   rank-one scores of 98.01\%, 97.60\%, and 96.13\% on the FRGC v2.0,
   Bosphorus, and BU-3DFE databases when only one sample per subject is
   used in the gallery. These experimental results reveals that the
   performance of 3D face recognition would be constantly improved with the
   aid of training deep models from massive 2D face images, which opens the
   door for future directions of 3D face recognition.}},
ISBN = {{978-1-5386-1124-1}},
Unique-ID = {{ISI:000426973200029}},
}

@inproceedings{ ISI:000426973200059,
Author = {Dou, Pengfei and Kakadiaris, Ioannis A.},
Book-Group-Author = {{IEEE}},
Title = {{Multi-View 3D Face Reconstruction with Deep Recurrent Neural Networks}},
Booktitle = {{2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)}},
Year = {{2017}},
Pages = {{483-492}},
Note = {{IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
   OCT 01-04, 2017}},
Organization = {{IEEE}},
Abstract = {{Inage-based 3D face reconstruction has great potential in different
   areas, such as facial recognition, facial analysis, and facial
   animation. Due to the variations in image quality, single-image-based 3D
   face reconstruction might not be sufficient to accurately reconstruct a
   3D face. To overcome this limitation, multi-view 3D face reconstruction
   uses multiple images of the same subject and aggregates complementary
   information for better accuracy. Though theoretically appealing, there
   are multiple challenges in practice. Amnong these challenges, the most
   significant is that it is difficult to establish coherent and accurate
   correspondence among a set of images, especially when these images are
   captured in different conditions. In this paper, we propose a method,
   Deep Recurrent 3D FAce Reconstruction (DRFAR), to solve the task of
   multi-view 3D face reconstruction using a subspace representation of the
   3D facial shape and a deep recurrent neural network that consists of
   both a deep convolutional neural network (DCNN) and a recurrent neural
   network (RNN). The DCNN disentangles the facial identity and the facial
   expression components for each single image independently, while the RNN
   fuses identity-related features from the DCNN and aggregates the
   identity specific contextual information, or the identity signal, from
   the whole set of images to predict the facial identity parameter, which
   is robust to variations in image quality and is consistent over the
   whole set of images. Through extensive experiments, we evaluate our
   proposed method and demonstrate its superiority over existing methods.}},
ISBN = {{978-1-5386-1124-1}},
Unique-ID = {{ISI:000426973200059}},
}

@inproceedings{ ISI:000426951900017,
Author = {Bentaieb, Samia and Ouamri, Abdelaziz and Keche, Mokhtar and Nait-Ali,
   Amine},
Editor = {{Naitali, A}},
Title = {{Gender Classification from 3D Face Images using Multi-Task Sparse
   Representation over Reduced Dictionary}},
Booktitle = {{2017 2ND INTERNATIONAL CONFERENCE ON BIO-ENGINEERING FOR SMART
   TECHNOLOGIES (BIOSMART)}},
Year = {{2017}},
Note = {{2nd International Conference on Bio-engineering for Smart Technologies
   (BioSMART), Paris, FRANCE, AUG 30-SEP 01, 2017}},
Organization = {{IEEE; IEEE France Sect; IEEE France Sect EMBS Chapterie; Univ Paris
   Creteil Val Marne; Lab Images Signaux Systeme Intelligents; ESME; Inst
   Mines Telecom, Telecom SudParis}},
Abstract = {{In this paper, we address the problem of gender classification based on
   facial images. The Speeded Up Robust Feature (SURF) algorithm
   descriptors are used as features to built dictionaries and a multi-task
   Sparse Representation Classification (SRC) is used as classifier to
   determine the gender of an individual face. Our approach uses smaller
   and compact dictionaries by removing the redundant atoms from the
   constructed ones.
   The feasibility of using the SURF on the shape index map for gender
   classification is demonstrated through experimental investigation
   conducted on FRGCv2 dataset. The proposed approach achieves 91.04 +/-
   1.19\% of correct gender classification rate using only 5\% of the size
   of the dictionary and 97.83 +/- 0.76 \% is obtained using 23\% of the
   size of the dictionary.}},
ISBN = {{978-1-5386-0706-0}},
Unique-ID = {{ISI:000426951900017}},
}

@inproceedings{ ISI:000426951900005,
Author = {Majid Zadeh Heravi, Farnaz and Nait-Ali, Amine},
Editor = {{Naitali, A}},
Title = {{A 3D Dynamic Shape Model to Simulate Rejuvenation \& Ageing Trajectory
   of 3D Face Images}},
Booktitle = {{2017 2ND INTERNATIONAL CONFERENCE ON BIO-ENGINEERING FOR SMART
   TECHNOLOGIES (BIOSMART)}},
Year = {{2017}},
Note = {{2nd International Conference on Bio-engineering for Smart Technologies
   (BioSMART), Paris, FRANCE, AUG 30-SEP 01, 2017}},
Organization = {{IEEE; IEEE France Sect; IEEE France Sect EMBS Chapterie; Univ Paris
   Creteil Val Marne; Lab Images Signaux Systeme Intelligents; ESME; Inst
   Mines Telecom, Telecom SudParis}},
Abstract = {{It is not only interesting to predict how an individual of a relatively
   young age will look in the future but also to reconstruct the facial
   appearance in the past during childhood. It can be even more desirable
   when different circumstances, behavior and lifestyle and their impacts
   on the facial shape appearance as a consequence are taken into account.
   Such may be applicable for many practical reasons in healthcare,
   forensics psychology, missing people and children, etc. This paper
   presents the 3D Face Time Machine Matrix (FT2M), a 3D Dynamic Shape
   Model which is a fusion of two models of ageing and rejuvenation with
   facial shape variations due to lifestyle and behavioral factors. This
   dynamic model is learned from a database of three dimensional facial
   images which is built by ten individual age groups between 3 to 75 years
   old. 3D facial aging modeling is a complex process since it affects both
   the shape and texture of the face. We propose a Dynamic face model to
   transform the given input face to his youthful or adulthood appearance
   by taking into account his lifestyle and behavioral traits and the
   probable changes may occur in perceptible appearance by altering its
   shape and texture simultaneously.}},
ISBN = {{978-1-5386-0706-0}},
Unique-ID = {{ISI:000426951900005}},
}

@inproceedings{ ISI:000425239601072,
Author = {Chang, Feng-Ju and Anh Tuan Tran and Hassner, Tal and Masi, Iacopo and
   Nevatia, Ram and Medioni, Gerard},
Book-Group-Author = {{IEEE}},
Title = {{FacePoseNet: Making a Case for Landmark-Free Face Alignment}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW
   2017)}},
Series = {{IEEE International Conference on Computer Vision Workshops}},
Year = {{2017}},
Pages = {{1599-1608}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{We show how a simple convolutional neural network (CNN) can be trained
   to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head
   pose, directly from image intensities. We further explain how this
   FacePoseNet (FPN) can be used to align faces in 2D and 3D as an
   alternative to explicit facial landmark detection for these tasks. We
   claim that in many cases the standard means of measuring landmark
   detector accuracy can be misleading when comparing different face
   alignments. Instead, we compare our FPN with existing methods by
   evaluating how they affect face recognition accuracy on the IJB-A and
   IJB-B benchmarks: using the same recognition pipeline, but varying the
   face alignment method. Our results show that (a) better landmark
   detection accuracy measured on the 300W benchmark does not necessarily
   imply better face recognition accuracy. (b) Our FPN provides superior 2D
   and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces
   at a small fraction of the computational cost of comparably accurate
   landmark detectors. For many purposes, FPN is thus a far faster and far
   more accurate face alignment method than using facial landmark
   detectors.}},
DOI = {{10.1109/ICCVW.2017.188}},
ISSN = {{2473-9936}},
ISBN = {{978-1-5386-1034-3}},
Unique-ID = {{ISI:000425239601072}},
}

@inproceedings{ ISI:000425239602072,
Author = {Le, Ha A. and Kakadiaris, Ioannis A.},
Book-Group-Author = {{IEEE}},
Title = {{UHDB31: A Dataset for Better Understanding Face Recognition across Pose
   and Illumination Variation}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW
   2017)}},
Series = {{IEEE International Conference on Computer Vision Workshops}},
Year = {{2017}},
Pages = {{2555-2563}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{Face datasets are a fundamental tool to analyze the performance of face
   recognition algorithms. However, the accuracy achieved on current
   benchmark datasets is saturated. Although multiple face datasets have
   been published recently, they only focus on the number of samples and
   lack diversity on facial appearance factors, such as pose and
   illumination. In addition, while 3D data have been demonstrated improved
   face recognition accuracy by a significant margin, only a few 3D face
   datasets provide high quality 2D and 3D data. In this paper, we
   introduce a new and challenging dataset, called UHDB31, which not only
   allows direct measurement of the influence of pose, illumination, and
   resolution on face recognition but also facilitates different
   experimental configurations with both 2D and 3D data. We conduct a
   series of experiments with various face recognition algorithms and point
   out how far they are from solving the face recognition problem under
   pose, illumination, and resolution variation. The dataset is publicly
   available and free for research use(1).}},
DOI = {{10.1109/ICCVW.2017.300}},
ISSN = {{2473-9936}},
ISBN = {{978-1-5386-1034-3}},
Unique-ID = {{ISI:000425239602072}},
}

@inproceedings{ ISI:000425236400020,
Author = {Li, Huifang and Li, Yidong and Liu, Wenhua and Dong, Hairong},
Editor = {{Demazeau, Y and Gao, J and Xu, G and Kozlak, J and Muller, K and Razzak, I and Chen, H and Gu, Y}},
Title = {{Coarse-to-fine Facial Landmarks Localization based on Convolutional
   Feature}},
Booktitle = {{PROCEEDINGS OF 4TH INTERNATIONAL CONFERENCE ON BEHAVIORAL, ECONOMIC
   ADVANCE IN BEHAVIORAL, ECONOMIC, SOCIOCULTURAL COMPUTING (BESC)}},
Year = {{2017}},
Note = {{4th International Conference on Behavioral, Economic Advance in
   Behavioral, Economic, Socio-Cultural Computing (BESC), Krakow, POLAND,
   OCT 16-18, 2017}},
Abstract = {{Accurate facial landmarks localization (FLL) plays an important role in
   face recognition, face tracking and 3D face reconstruction. It can be
   formulated as a regression problem, which outputs facial landmarks
   positions from the detected face image. Deep constitutional neural
   network (CNN) has achieved great success in vision tasks, but it is
   insignificant to use it directly. In this paper, instead of adopting CNN
   model straightforwardly, we combine different convolutional features
   with extreme machine learning (ELM) in a cascade framework to achieve
   accurate FLL. Specifically, we extract globally and spatially
   convolutional feature in the first stage for containing better
   localization property by training deep CNN, which takes the whole face
   region as input and concatenates lower layers with higher layers. Then,
   we extract locally and correlatedly convolutional feature in the
   following stages for preserving shape constraint by building
   multi-objective CNN, which inputs local patches centered at the current
   landmarks and concatenates independent subnetwork of each landmark
   together. Moreover, the regressor embedded in CNN is replaced by the
   robust ELM for accurate shape regression. Extensive experiments
   demonstrate that our method performs better in challenging datasets.}},
ISBN = {{978-1-5386-2365-7}},
Unique-ID = {{ISI:000425236400020}},
}

@inproceedings{ ISI:000423869700004,
Author = {Haufel, Gisela and Bulatov, Dimitri and Solbrig, Peter},
Editor = {{Michel, U and Schulz, K and Nikolakopoulos, KG and Civco, D}},
Title = {{Sensor Data Fusion for Textured Reconstruction and Virtual
   Representation of Alpine Scenes}},
Booktitle = {{EARTH RESOURCES AND ENVIRONMENTAL REMOTE SENSING/GIS APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10428}},
Note = {{17th SPIE Conference on Earth Resources and Environmental Remote
   Sensing/GIS Applications, Warsaw, POLAND, SEP 12-14, 2017}},
Organization = {{SPIE}},
Abstract = {{The concept of remote sensing is to provide information about a
   wide-range area without making physical contact with this area. If,
   additionally to satellite imagery, images and videos taken by drones
   provide a more up-to-date data at a higher resolution, or accurate
   vector data is downloadable from the Internet, one speaks of sensor data
   fusion. The concept of sensor data fusion is relevant for many
   applications, such as virtual tourism, automatic navigation, hazard
   assessment, etc. In this work, we describe sensor data fusion aiming to
   create a semantic 3D model of an extremely interesting yet challenging
   dataset: An alpine region in Southern Germany. A particular challenge of
   this work is that rock faces including overhangs are present in the
   input airborne laser point cloud. The proposed procedure for
   identification and reconstruction of overhangs from point clouds
   comprises four steps: Point cloud preparation, filtering out vegetation,
   mesh generation and texturing. Further object types are extracted in
   several interesting subsections of the dataset: Building models with
   textures from UAV (Unmanned Aerial Vehicle) videos, hills reconstructed
   as generic surfaces and textured by the orthophoto, individual trees
   detected by the watershed algorithm, as well as the vector data for
   roads retrieved from openly available shape files and GPS-device tracks.
   We pursue geo-specific reconstruction by assigning texture and width to
   roads of several pre-determined types and modeling isolated trees and
   rocks using commercial software. For visualization and simulation of the
   area, we have chosen the simulation system Virtual Battlespace 3 (VBS3).
   It becomes clear that the proposed concept of sensor data fusion allows
   a coarse reconstruction of a large scene and, at the same time, an
   accurate and up-to-date representation of its relevant subsections, in
   which simulation can take place.}},
DOI = {{10.1117/12.2278237}},
Article-Number = {{UNSP 1042805}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1321-8; 978-1-5106-1320-1}},
ORCID-Numbers = {{Bulatov, Dimitri/0000-0002-0560-2591}},
Unique-ID = {{ISI:000423869700004}},
}

@inproceedings{ ISI:000418371401059,
Author = {Dou, Pengfei and Shah, Shishir K. and Kakadiaris, Ioannis A.},
Book-Group-Author = {{IEEE}},
Title = {{End-to-end 3D face reconstruction with deep neural networks}},
Booktitle = {{30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR
   2017)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{1503-1512}},
Note = {{30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Honolulu, HI, JUL 21-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{Monocular 3D facial shape reconstruction from a single 2D facial image
   has been an active research area due to its wide applications. Inspired
   by the success of deep neural networks (DNN), we propose a DNN-based
   approach for End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single
   2D image. Different from recent works that reconstruct and refine the 3D
   face in an iterative manner using both an RGB image and an initial 3D
   facial shape rendering, our DNN model is end-to-end, and thus the
   complicated 3D rendering process can be avoided. Moreover, we integrate
   in the DNN architecture two components, namely a multi-task loss
   function and a fusion convolutional neural network (CNN) to improve
   facial expression reconstruction. With the multi-task loss function, 3D
   face reconstruction is divided into neutral 3D facial shape
   reconstruction and expressive 3D facial shape reconstruction. The
   neutral 3D facial shape is class-specific. Therefore, higher layer
   features are useful. In comparison, the expressive 3D facial shape
   favors lower or intermediate layer features. With the fusion-CNN,
   features from different intermediate layers are fused and transformed
   for predicting the 3D expressive facial shape. Through extensive
   experiments, we demonstrate the superiority of our end-to-end framework
   in improving the accuracy of 3D face reconstruction.}},
DOI = {{10.1109/CVPR.2017.164}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-0457-1}},
Unique-ID = {{ISI:000418371401059}},
}

@inproceedings{ ISI:000418371405064,
Author = {Peng, Weilong and Feng, Zhiyong and Xu, Chao and Su, Yong},
Book-Group-Author = {{IEEE}},
Title = {{Parametric T-spline Face Morphable Model for Detailed Fitting in Shape
   Subspace}},
Booktitle = {{30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR
   2017)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{5515-5523}},
Note = {{30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Honolulu, HI, JUL 21-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration
   for the synthesis of 3D faces by assuming that faces are in a linear
   class. However, the human face is in a nonlinear manifold, and a new
   test are always not in the pre-learnt subspace accurately because of the
   disparity brought by ethnicity, age, gender, etc. In the paper, we
   propose a parametric T-spline morphable model (T-splineMM) for 3D face
   representation, which has great advantages of fitting data from an
   unknown source accurately. In the model, we describe a face by C-2
   T-spline surface, and divide the face surface into several shape units
   (SUs), according to facial action coding system (FACS), on T-mesh
   instead of on the surface directly. A fitting algorithm is proposed to
   optimize coefficients of T-spline control point components along
   pre-learnt identity and expression subspaces, as well as to optimize the
   details in refinement progress. As any pre-learnt subspace is not
   complete to handle the variety and details of faces and expressions, it
   covers a limited span of morphing. SUs division and detail refinement
   make the model fitting the facial muscle deformation in a larger span of
   morphing subspace. We conduct experiments on face scan data, kinect data
   as well as the space-time data to test the performance of detail
   fitting, robustness to missing data and noise, and to demonstrate the
   effectiveness of our model. Convincing results are illustrated to
   demonstrate the effectiveness of our model compared with the popular
   methods.}},
DOI = {{10.1109/CVPR.2017.585}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-0457-1}},
Unique-ID = {{ISI:000418371405064}},
}

@inproceedings{ ISI:000418791700005,
Author = {Yu, Jun},
Editor = {{Amsaleg, L and Gudmundsson, GP and Gurrin, C and Jonsson, BP and Satoh, S}},
Title = {{A Unified Framework for Monocular Video-Based Facial Motion Tracking and
   Expression Recognition}},
Booktitle = {{MULTIMEDIA MODELING, MMM 2017, PT II}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10133}},
Pages = {{50-62}},
Note = {{23rd International Conference on MultiMedia Modeling (MMM), Reykjavik
   Univ, Reykjavik, ICELAND, JAN 04-06, 2017}},
Abstract = {{This paper proposes a unified facial motion tracking and expression
   recognition framework for monocular video. For retrieving facial motion,
   an online weight adaptive statistical appearance method is embedded into
   the particle filtering strategy by using a deformable facial mesh model
   served as an intermediate to bring input images into correspondence by
   means of registration and deformation. For recognizing facial
   expression, facial animation and facial expression are estimated
   sequentially for fast and efficient applications, in which facial
   expression is recognized by static anatomical facial expression
   knowledge. In addition, facial animation and facial expression are
   simultaneously estimated for robust and precise applications, in which
   facial expression is recognized by fusing static and dynamic facial
   expression knowledge. Experiments demonstrate the high tracking
   robustness and accuracy as well as the high facial expression
   recognition score of the proposed framework.}},
DOI = {{10.1007/978-3-319-51814-5\_5}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-51814-5; 978-3-319-51813-8}},
Unique-ID = {{ISI:000418791700005}},
}

@inproceedings{ ISI:000417429000016,
Author = {Li Fangmin and Chen Ke and Liu Xinhua},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Reconstruction Based on Convolutional Neural Network}},
Booktitle = {{2017 10TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTATION TECHNOLOGY
   AND AUTOMATION (ICICTA 2017)}},
Series = {{International Conference on Intelligent Computation Technology and
   Automation}},
Year = {{2017}},
Pages = {{71-74}},
Note = {{10th International Conference on Intelligent Computation Technology and
   Automation (ICICTA), Changsha, PEOPLES R CHINA, OCT 09-10, 2017}},
Organization = {{Changsha Univ Sci \& Technol, Commun Res Inst; Cent S Univ, Shenzhen Res
   Inst; Hunan City Coll, Dept Urban Management}},
Abstract = {{Fast and robust 3D reconstruction of facial geometric structure from a
   single image is a challenging task with numerous applications, but there
   exist two problems when applied ``in the wild{''}: the 3D estimates are
   unstable for different photos of the same subject; the 3D estimates are
   over-regularized and generic. In response, a robust method for
   regressing discriminative 3D morphable face models(3DMM) is described to
   support face recognition and 3D mask printing. Combining the local data
   sets with the public data sets, improving the exiting 3DMM fitting
   method and then using a convolutional neural network(CNN) to improve
   reconstruction effect. The ground truth 3D faces of the CNN are the
   pooled 3DMM parameters extracted from the photos of the same subject.
   Using CNN to regress 3DMM shape and texture parameters directly from an
   input photo and offering a method for generating huge numbers of labeled
   examples. There are two key points of the paper: one is the training
   data generation for the model training; the other is the training of 3D
   reconstruction model. Experimental results and analysis show that this
   method costs much less time than traditional methods of 3D face
   modeling, and it is improved for different races on photos with any
   angles than the existing methods based on deep learning, and the system
   has better robustness.}},
DOI = {{10.1109/ICICTA.2017.23}},
ISSN = {{1949-1263}},
ISBN = {{978-1-5386-1230-9}},
Unique-ID = {{ISI:000417429000016}},
}

@article{ ISI:000417481500002,
Author = {Marcolin, Federica},
Title = {{Miscellaneous expertise of 3D facial landmarks in recent literature}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMETRICS}},
Year = {{2017}},
Volume = {{9}},
Number = {{4}},
Pages = {{279-304}},
Abstract = {{As the interest in human face grows, facial landmarks become more and
   more important for a large variety of fields and applications.
   Multipurpose medical is evidently leading in this sense, but others such
   as skull study for crime scenes, sex estimation, and attractiveness
   quantification, morphological and cephalometric analyses are present. A
   cluster analysis of the examined papers is performed depending on scope,
   landmarking method, and facial database features. The purpose is to face
   these topics by providing the reader with a comprehensive view of what
   3D facial landmarks are and what ``they have been up to{''} in 2014 and
   2015. The aim is to offer to users the very up-to-date scenario, the
   best outcomes, i.e., the latest frontier of landmarks' talents and
   skills. The third dimension allowed to select the most prominent
   contributions, especially in terms of scientific advance innovativeness.}},
DOI = {{10.1504/IJBM.2017.10009329}},
ISSN = {{1755-8301}},
EISSN = {{1755-831X}},
ORCID-Numbers = {{Marcolin, Federica/0000-0002-4360-6905}},
Unique-ID = {{ISI:000417481500002}},
}

@inproceedings{ ISI:000413813100460,
Author = {Soylemez, Omer Famk and Ergen, Burhan and Soylemez, Nesrin Hark},
Book-Group-Author = {{IEEE}},
Title = {{A 3D Facial Expression Recognition System Based On SVM Classifier Using
   Distance Based Features}},
Booktitle = {{2017 25TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE
   (SIU)}},
Series = {{Signal Processing and Communications Applications Conference}},
Year = {{2017}},
Note = {{25th Signal Processing and Communications Applications Conference (SIU),
   Antalya, TURKEY, MAY 15-18, 2017}},
Organization = {{Turk Telekom; Arcelik A S; Aselsan; ARGENIT; HAVELSAN; NETAS;
   Adresgezgini; IEEE Turkey Sect; AVCR Informat Technologies; Cisco; i2i
   Syst; Integrated Syst \& Syst Design; ENOVAS; FiGES Engn; MS Spektral;
   Istanbul Teknik Univ}},
Abstract = {{In this study, an SVM-based system is proposed for the classification of
   facial expressions that are represented in 3D. Distance based features
   are used as a feature vector, which are determined by the distances
   between the different key points on the image. Study was conducted on a
   subset (Happy, sadness, surprise) of Bosphorus 3D Face Database. 9
   different fiducial points arc used to calculate a total of 5 distance
   features. SVM classification was performed with K-fold cross validation
   thus mean classification performance of different training and test
   clusters were determined. \%85 success rate has achieved as a result of
   the expression analysis performed on the 3D facial scans.}},
ISSN = {{2165-0608}},
ISBN = {{978-1-5090-6494-6}},
Unique-ID = {{ISI:000413813100460}},
}

@inproceedings{ ISI:000414287400034,
Author = {Wang, Lezi and Yu, Xiang and Metaxas, Dimitris N.},
Book-Group-Author = {{IEEE}},
Title = {{A Coupled Encoder-Decoder Network for Joint Face Detection and Landmark
   Localization}},
Booktitle = {{2017 12TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE
   RECOGNITION (FG 2017)}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2017}},
Pages = {{251-257}},
Note = {{12th IEEE International Conference on Automatic Face and Gesture
   Recognition (FG), Washington, DC, MAY 30-JUN 03, 2017}},
Organization = {{IEEE; Baidu; Mitsubishi Elect Res Labs Inc; 3dMD; DI4D; Syst \& Technol
   Res; ObjectVideo Labs; MUKH Technologies; IEEE Comp Soc}},
Abstract = {{Face detection and landmark localization have been extensively
   investigated and are the prerequisite for many face applications, such
   as face recognition and 3D face reconstruction. Most existing methods
   achieve success on only one of the two problems. In this paper, we
   propose a coupled encoder-decoder network to jointly detect faces and
   localize facial key points. The encoder and decoder generate response
   maps for facial landmark localization. Moreover, we observe that the
   intermediate feature maps from the encoder and decoder have strong power
   in describing facial regions, which motivates us to build a unified
   framework by coupling the feature maps for multi-scale cascaded face
   detection. Experiments on face detection show strongly competitive
   results against the existing methods on two public benchmarks. The
   landmark localization further shows consistently better accuracy than
   state-of-the-arts on three face-in-the-wild databases.}},
DOI = {{10.1109/FG.2017.40}},
ISSN = {{2326-5396}},
ISBN = {{978-1-5090-4023-0}},
Unique-ID = {{ISI:000414287400034}},
}

@inproceedings{ ISI:000414287400080,
Author = {Tang, Yinhang and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{3D Facial Geometric Attributes based Anti-spoofing Approach against Mask
   Attacks}},
Booktitle = {{2017 12TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE
   RECOGNITION (FG 2017)}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2017}},
Pages = {{589-595}},
Note = {{12th IEEE International Conference on Automatic Face and Gesture
   Recognition (FG), Washington, DC, MAY 30-JUN 03, 2017}},
Organization = {{IEEE; Baidu; Mitsubishi Elect Res Labs Inc; 3dMD; DI4D; Syst \& Technol
   Res; ObjectVideo Labs; MUKH Technologies; IEEE Comp Soc}},
Abstract = {{3D scanning and 3D printing techniques, as the technical impetus of 3D
   face recognition, also boost unconsciously the security threat against
   it from the spoofing attacks via manufactured mask. In order to improve
   the robustness of 3D face recognition system, several countermeasures
   against mask attacks based on photometric features have been reported in
   recent years. However, the anti-spoofing approach involving 3D meshed
   face scan and the related 3D facial features have not been studied yet.
   For filling this gap, in this paper, we propose to exploit the
   anti-spoofing performance of geometric attributes based 3D facial
   description. It synthesises the advantages of the selected geometric
   attributes, named principal curvature measures, and the meshSIFT-based
   feature descriptor. Specifically, the estimation of geometric attributes
   is coherent to the property of discrete surface, and the feature related
   to them can accurately describe the shape of facial surface. These
   characteristics are beneficial to discovering the geometry-based
   dissimilarity between genuine face and fraud mask. In the experiment
   part, the baselines of verification and anti-spoofing performance are
   evaluated on the Morpho database. Furthermore, for simulating a
   real-world scenario and testing the corresponding anti-spoofing
   performance, the size of genuine face set is massively extended by
   uniting the Morpho database and the FRGC v2.0 database to increase the
   ratio of genuine faces to fraud masks. The evaluation results prove that
   the proposed 3D face verification system can guarantee competitive
   verification accuracy for genuine faces and promising anti-spoofing
   performance against mask attacks.}},
DOI = {{10.1109/FG.2017.74}},
ISSN = {{2326-5396}},
ISBN = {{978-1-5090-4023-0}},
Unique-ID = {{ISI:000414287400080}},
}

@inproceedings{ ISI:000414298700135,
Author = {Luo, Min and Luo, Yadong and Li, Hui and Zhang, Xia and Yang, Yongkui},
Book-Group-Author = {{IEEE}},
Title = {{Design and Implementation of High Resolution Face Image Acquisition
   System under Low Illumination Based on the Open Source Computer Vision
   Library}},
Booktitle = {{2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC
   2017)}},
Year = {{2017}},
Pages = {{680-683}},
Note = {{2nd International Conference on Image, Vision and Computing (ICIVC),
   Chengdu, PEOPLES R CHINA, JUN 02-04, 2017}},
Organization = {{IEEE; Sichuan Prov Comp Sci; Singapore Inst Elect; Chengdu Univ Informat
   Technol; Chinese Acad Sci Co Ltd, Chengdu Informat Technol}},
Abstract = {{Super-resolution face image acquisition system is indispensable in
   people's life. Under the condition of low illumination, the illumination
   environment difference is too big or the light is insufficient, which
   leads to the traditional image acquisition system can not collect the
   high quality face image, and the limitation is poor. Based on open
   source computer vision library (OpenCV) in the C++ environment
   configuration, the use of Three Dimension (3D) face recognition
   technology algorithm, design a set of low illumination conditions of the
   super resolution face image acquisition system. Experiments show that
   the design scheme with real-time focusing speed), fast (single
   acquisition 0.05 seconds), accurate (facial recognition rate of 99.3\%)
   etc. characteristics, be able to fully meet the needs of low
   illumination conditions for super-resolution of face image acquisition.}},
ISBN = {{978-1-5090-6238-6}},
Unique-ID = {{ISI:000414298700135}},
}

@inproceedings{ ISI:000413240500077,
Author = {Zhang, Tingting and Mu, Zhichun and Li, Yihang and Liu, Qing and Zhang,
   Yi},
Editor = {{DeMarsico, M and DiBaja, GS and Fred, A}},
Title = {{3D Face and Ear Recognition based on Partial MARS Map}},
Booktitle = {{ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN
   RECOGNITION APPLICATIONS AND METHODS}},
Year = {{2017}},
Pages = {{633-637}},
Note = {{6th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM), Porto, PORTUGAL, FEB 24-26, 2017}},
Abstract = {{This paper proposes a 3D face recognition approach based on facial pose
   estimation, which is robust to large pose variations in the
   unconstrained scene. Deep learning method is used to facial pose
   estimation, and the generation of partial MARS (Multimodal fAce and eaR
   Spherical) map reduces the probability of feature points appearing in
   the deformed region. Then we extract the features from the depth and
   texture maps. Finally, the matching scores from two types of maps should
   be calculated by Bayes decision to generate the final result. In the
   large pose variations, the recognition rate of the method in this paper
   is 94.6\%. The experimental results show that our approach has superior
   performance than the existing methods used on the MARS map, and has
   potential to deal with 3D face recognition in unconstrained scene.}},
DOI = {{10.5220/0006244206330637}},
ISBN = {{978-989-758-222-6}},
Unique-ID = {{ISI:000413240500077}},
}

@inproceedings{ ISI:000413068300005,
Author = {Prathusha, Sai S. and Suja, P. and Tripathi, Shikha and Louis, R.},
Editor = {{Basu, A and Das, S and Horain, P and Bhattacharya, S}},
Title = {{Emotion Recognition from Facial Expressions of 4D Videos Using Curves
   and Surface Normals}},
Booktitle = {{INTELLIGENT HUMAN COMPUTER INTERACTION, IHCI 2016}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10127}},
Pages = {{51-64}},
Note = {{8th International Conference on Intelligent Human Computer Interaction
   (IHCI), Pilani, INDIA, DEC 12-13, 2016}},
Organization = {{Council Sci \& Ind Res, Cent Elect Engn Res Inst Pilani; Birla Inst
   Technol \& Sci Pilani; Indian Inst Informat Technol}},
Abstract = {{In this paper, we propose and compare three methods for recognizing
   emotions from facial expressions using 4D videos. In the first two
   methods, the 3D faces are re-sampled by using curves to extract the
   feature information. Two different methods are presented to resample the
   faces in an intelligent way using parallel curves and radial curves. The
   movement of the face is measured through these curves using two frames:
   neutral and peak frame. The deformation matrix is formed by computing
   the distance point to point on the corresponding curves of the neutral
   frame and peak frame. This matrix is used to create the feature vector
   that will be used for classification using Support Vector Machine (SVM).
   The third method proposed is to extract the feature information from the
   face by using surface normals. At every point on the frame, surface
   normals are extracted. The deformation matrix is formed by computing the
   Euclidean distances between the corresponding normals at a point on
   neutral and peak frames. This matrix is used to create the feature
   vector that will be used for classification of emotions using SVM. The
   proposed methods are analyzed and they showed improvement over existing
   literature.}},
DOI = {{10.1007/978-3-319-52503-7\_5}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-52503-7; 978-3-319-52502-0}},
Unique-ID = {{ISI:000413068300005}},
}

@inproceedings{ ISI:000406996500085,
Author = {Carraro, Marco and Munaro, Matteo and Roitberg, Alina and Menegatti,
   Emanuele},
Editor = {{Chen, W and Hosoda, K and Menegatti, E and Shimizu, M and Wang, H}},
Title = {{Improved Skeleton Estimation by Means of Depth Data Fusion from Multiple
   Depth Cameras}},
Booktitle = {{INTELLIGENT AUTONOMOUS SYSTEMS 14}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{531}},
Pages = {{1155-1167}},
Note = {{14th International Conference on Intelligent Autonomous Systems (IAS),
   Shanghai Jiao Tong Univ, Shanghai, PEOPLES R CHINA, JUL 03-07, 2016}},
Abstract = {{In this work, we address the problem of human skeleton estimation when
   multiple depth cameras are available. We propose a system that takes
   advantage of the knowledge of the camera poses to create a collaborative
   virtual depth image of the person in the scene which consists of points
   from all the cameras and that represents the person in a frontal pose.
   This depth image is fed as input to the open-source body part detector
   in the Point Cloud Library. A further contribution of this work is the
   improvement of this detector obtained by introducing two new components:
   as a pre-processing, a people detector is applied to remove the
   background from the depth map before estimating the skeleton, while an
   alpha-beta tracking is added as a post-processing step for filtering the
   obtained joint positions over time. The overall system has been proven
   to effectively improve the skeleton estimation on two sequences of
   people in different poses acquired from two first-generation Microsoft
   Kinect.}},
DOI = {{10.1007/978-3-319-48036-7\_85}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-48036-7; 978-3-319-48035-0}},
Unique-ID = {{ISI:000406996500085}},
}

@inproceedings{ ISI:000405560700088,
Author = {Manit, Jirapong and Bremer, Christina and Schweikard, Achim and Ernst,
   Floris},
Editor = {{Webster, RJ and Fei, B}},
Title = {{Patient identification using a near-infrared lasers canner}},
Booktitle = {{MEDICAL IMAGING 2017: IMAGE-GUIDED PROCEDURES, ROBOTIC INTERVENTIONS,
   AND MODELING}},
Series = {{Proceedings of SPIE}},
Year = {{2017}},
Volume = {{10135}},
Note = {{Conference on Medical Imaging - Image-Guided Procedures, Robotic
   Interventions, and Modeling, Orlando, FL, FEB 14-16, 2017}},
Organization = {{SPIE; Alpin Med Syst; Siemens Healthineers; No Digital Inc}},
Abstract = {{We propose a new biometric approach where the tissue thickness of a
   person's forehead is used as a biometric feature. Given that the spatial
   registration of two 3D laser scans of the same human face usually
   produces a low error value, the principle of point cloud registration
   and its error metric can be applied to human classification techniques.
   However, by only considering the spatial error, it is not possible to
   reliably verify a person's identity. We propose to use a novel
   near-infrared laser-based head tracking system to determine an
   additional feature, the tissue thickness, and include this in the error
   metric. Using MRI as a ground truth, data from the foreheads of 30
   subjects was collected from which a 4D reference point cloud was created
   for each subject. The measurements from the near-infrared system were
   registered with all reference point clouds using the ICP algorithm.
   Afterwards, the spatial and tissue thickness errors were extracted,
   forming a 2D feature space. For all subjects, the lowest feature
   distance resulted from the registration of a measurement and the
   reference point cloud of the same person.
   The combined registration error features yielded two clusters in the
   feature space, one from the same subject and another from the other
   subjects. When only the tissue thickness error was considered, these
   clusters were less distinct but still present. These findings could help
   to raise safety standards for head and neck cancer patients and lays the
   foundation for a future human identification technique.}},
DOI = {{10.1117/12.2254963}},
Article-Number = {{UNSP 101352L}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-0715-6; 978-1-5106-0716-3}},
Unique-ID = {{ISI:000405560700088}},
}

@inproceedings{ ISI:000404959000120,
Author = {Tarnowski, Pawel and Kolodziej, Marcin and Majkowski, Andrzej and Rak,
   Remigiusz J.},
Editor = {{Koumoutsakos, P and Lees, M and Krzhizhanovskaya, V and Dongarra, J and Sloot, P}},
Title = {{Emotion recognition using facial expressions}},
Booktitle = {{INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE (ICCS 2017)}},
Series = {{Procedia Computer Science}},
Year = {{2017}},
Volume = {{108}},
Pages = {{1175-1184}},
Note = {{International Conference on Computational Science (ICCS), Zurich,
   SWITZERLAND, JUN 12-14, 2017}},
Abstract = {{In the article there are presented the results of recognition of seven
   emotional states (neutral, joy, sadness, surprise, anger, fear, disgust)
   based on facial expressions. Coefficients describing elements of facial
   expressions, registered for six subjects, were used as features. The
   features have been calculated for three-dimensional face model. The
   classification of features were performed using k-NN classifier and MLP
   neural network. (C) 2017 The Authors. Published by Elsevier B.V.}},
DOI = {{10.1016/j.procs.2017.05.025}},
ISSN = {{1877-0509}},
ResearcherID-Numbers = {{Tarnowski, Pawel/N-4957-2016
   Kolodziej, Marcin/N-3139-2014
   }},
ORCID-Numbers = {{Tarnowski, Pawel/0000-0003-0392-4084
   Kolodziej, Marcin/0000-0003-2856-7298
   Majkowski, Andrzej/0000-0002-6557-836X}},
Unique-ID = {{ISI:000404959000120}},
}

@inproceedings{ ISI:000402657200006,
Author = {Bobulski, Janusz},
Editor = {{Choras, RS}},
Title = {{Face Recognition with 3D Face Asymmetry}},
Booktitle = {{IMAGE PROCESSING AND COMMUNICATIONS CHALLENGES 8}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{525}},
Pages = {{53-60}},
Note = {{8th International Conference on Image Processing and Communications
   (IP\&C), UTP Univ Technol \& Sci, Inst Telecommunicat \& Comp Sci,
   Bydgoszcz, POLAND, SEP 07-09, 2016}},
Organization = {{UTP Univ Technol \& Sci}},
Abstract = {{Using of 3D images for the identification was in a field of the interest
   of many researchers which developed a few methods offering good results.
   However, there are few techniques exploiting the 3D asymmetry amongst
   these methods. We propose fast algorithm for rough extraction face
   asymmetry that is used to 3D face recognition with hidden Markov models.
   This paper presents conception of fast method for determine 3D face
   asymmetry. The research results indicate that face recognition with 3D
   face asymmetry may be used in biometrics systems.}},
DOI = {{10.1007/978-3-319-47274-4\_6}},
ISSN = {{2194-5357}},
EISSN = {{2194-5365}},
ISBN = {{978-3-319-47274-4; 978-3-319-47273-7}},
ResearcherID-Numbers = {{Bobulski, Janusz/C-4998-2014}},
ORCID-Numbers = {{Bobulski, Janusz/0000-0003-3345-604X}},
Unique-ID = {{ISI:000402657200006}},
}

@inproceedings{ ISI:000399004300032,
Author = {Keshwani, Latasha and Pete, Dnyandeo},
Editor = {{Satapathy, SC and Bhateja, V and Joshi, A}},
Title = {{Comparative Analysis of Frontal Face Recognition Using Radial Curves and
   Back Propagation Neural Network}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON DATA ENGINEERING AND
   COMMUNICATION TECHNOLOGY, ICDECT 2016, VOL 2}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2017}},
Volume = {{469}},
Pages = {{333-344}},
Note = {{1st International Conference on Data Engineering and Communication
   Technology (ICDECT), Christ Inst Management, Lavasa, INDIA, MAR 10-11,
   2016}},
Organization = {{Aspire Res Fdn}},
Abstract = {{Person identification using face as a cue is one of the most prominent
   and robust technique. This paper presents 3D face recognition system
   using Radial curves and Back Propagation Neural Networks (BPNN). The
   face images used for experimentation are under various challenges like
   illumination, pose variation, expression and occlusions. The features of
   images are extracted using Eigen vectors. These features are compared
   using radial curves on the face starting from center of the face to the
   end of the face. Each corresponding curve is matched using Euclidean
   Distance classifier. The BPNN is used to train the features for face
   matching. The proposed algorithms are tested on ORL and DMCE database.
   The performance analysis is based on recognition rate accuracy of the
   system. The proposed radial curve system yields recognition rate
   accuracy of 100 \% for images from the ORL database and 98 \% for the
   images from DMCE database.}},
DOI = {{10.1007/978-981-10-1678-3\_32}},
ISSN = {{2194-5357}},
ISBN = {{978-981-10-1678-3; 978-981-10-1677-6}},
Unique-ID = {{ISI:000399004300032}},
}

@article{ ISI:000397373000001,
Author = {Jin, Hai and Wang, Xun and Zhong, Zichun and Hua, Jing},
Title = {{Robust 3D face modeling and reconstruction from frontal and side images}},
Journal = {{COMPUTER AIDED GEOMETRIC DESIGN}},
Year = {{2017}},
Volume = {{50}},
Pages = {{1-13}},
Month = {{JAN}},
Abstract = {{Robust and effective capture and reconstruction of 3D face models
   directly by smartphone users enables many applications. This paper
   presents a novel 3D face modeling and reconstruction solution that
   robustly and accurately acquire 3D face models from a couple of images
   captured by a single smartphone camera. Two selfie photos of a subject
   taken from the front and side are first used to guide our Non-Negative
   Matrix Factorization (NMF) induced part-based face model to iteratively
   reconstruct an initial 3D face of the subject. Then, an iterative detail
   updating method is applied to the initial generated 3D face to
   reconstruct facial details through optimizing lighting parameters and
   local depths. Our iterative 3D face reconstruction method permits fully
   automatic registration of a part based face representation to the
   acquired face data and the detailed 2D/3D features to build a
   high-quality 3D face model. The NMF part-based face representation
   learned from a 3D face database facilitates effective global and
   adaptive local detail data fitting alternatively. Our system is flexible
   and it allows users to conduct the capture in any uncontrolled
   environment. We demonstrate the capability of our method by allowing
   users to capture and reconstruct their 3D faces by themselves. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.cagd.2016.11.001}},
ISSN = {{0167-8396}},
EISSN = {{1879-2332}},
ResearcherID-Numbers = {{Smith, Nash/R-3104-2017}},
ORCID-Numbers = {{Smith, Nash/0000-0002-4406-0043}},
Unique-ID = {{ISI:000397373000001}},
}

@article{ ISI:000393265800001,
Author = {Moeini, Ali and Faez, Karim and Moeini, Hossein and Safai, Armon Matthew},
Title = {{Open-set face recognition across look-alike faces in real-world
   scenarios}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2017}},
Volume = {{57}},
Pages = {{1-14}},
Month = {{JAN}},
Abstract = {{The open-set problem is among the problems that have significantly
   changed the performance of face recognition algorithms in real-world
   scenarios. Open-set operates under the supposition that not all the
   probes have a pair in the gallery. Most face recognition systems in
   real-world scenarios focus on handling pose, expression and illumination
   problems on face recognition. In addition to these challenges, when the
   number of subjects is increased for face recognition, these problems are
   intensified by look-alike faces for which there are two subjects with
   lower intra-class variations. In such challenges, the inter-cldss
   similarity is higher than the intra-class variation for these two
   subjects. In fact, these look-alike faces can be created as intrinsic,
   situation-based and also by facial plastic surgery. This work introduces
   three real-world open-set face recognition methods across facial plastic
   surgery changes and a look-alike face by 3D face reconstruction and
   sparse representation. Since some real world databases for face
   recognition do not have multiple images per person in the gallery, With
   just one image per subject in the gallery, this paper proposes a novel
   idea to overcome this challenge by 3D modeling from gallery images and
   synthesizing them for generating several images. Accordingly, a 3D model
   is initially reconstructed from frontal face images in a real-world
   gallery. Then, each 3D reconstructed face in the gallery is synthesized
   to several possible views and a sparse dictionary is generated based on
   the synthesized face image for each person. Also, a likeness dictionary
   is defined and its optimization problem is solved by the proposed
   method. Finally, the face recognition is performed for open-set face
   recognition using three proposed representation classifications.
   Promising results are achieved for face recognition across plastic
   surgery and look-alike faces on three databases including the plastic
   surgery face, look-alike face and LFW databases compared to several
   state-of-the-art methods. Also, several real-world and open-set
   scenarios are performed to evaluate the proposed method on these
   databases in real-world scenarios. (C) 2016 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.imavis.2016.11.002}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
ResearcherID-Numbers = {{faez, karim/K-5117-2019}},
ORCID-Numbers = {{faez, karim/0000-0002-1159-4866}},
Unique-ID = {{ISI:000393265800001}},
}

@inproceedings{ ISI:000392417700075,
Author = {Marcolin, Federica and Violante, Maria Grazia and Moos, Sandro and
   Vezzetti, Enrico and Tornincasa, Stefano and Dagnes, Nicole and
   Speranza, Domenico},
Editor = {{Eynard, B and Nigrelli, V and Oliveri, SM and PerisFajarnes, G and Rizzuti, S}},
Title = {{Three-dimensional face analysis via new geometrical descriptors}},
Booktitle = {{ADVANCES ON MECHANICS, DESIGN ENGINEERING AND MANUFACTURING}},
Series = {{Lecture Notes in Mechanical Engineering}},
Year = {{2017}},
Pages = {{747-756}},
Note = {{International Joint Conference on Mechanics, Design Engineering and
   Advanced Manufacturing (JCM), Catania, ITALY, SEP 14-16, 2016}},
Organization = {{Ateliers Interetablissements Productique Poles Resources Informatiques
   MECAnique; Design \& Methods Ind Engn Soc; Asociac Espanola Ingn
   Grafica; Univ Catania, Rapid Prototyping \& Geometr Modelling Lab; Univ
   Catania, Dept Elect, Elect \& Informat Engn}},
Abstract = {{3D face was recently investigated for various applications, including
   biometrics and diagnosis. Describing facial surface, i.e. how it bends
   and which kinds of patches is composed by, is the aim of studies in Face
   Analysis, whose ultimate goal is to identify which features could be
   extracted from three-dimensional faces depending on the application. In
   this study, we propose 54 novel geometrical descriptors for Face
   Analysis. They are generated by composing primary-geometrical
   descriptors such as mean. Gaussian. principal curvatures, shape index,
   curvedness, and the coefficients of the fundamental forms. The new
   descriptors were mapped on 217 facial depth maps and analysed in terms
   of descriptiveness of facial shape and exploitability for localizing
   landmark points. Automatic landmark extraction stands as the final aim
   of this analysis. Results showed that the newly generated descriptors
   are suitable to 3D face description and to support landmark localization
   procedures.}},
DOI = {{10.1007/978-3-319-45781-9\_75}},
ISSN = {{2195-4356}},
ISBN = {{978-3-319-45781-9; 978-3-319-45780-2}},
ResearcherID-Numbers = {{speranza, domenico massimiliano/G-5513-2018
   }},
ORCID-Numbers = {{speranza, domenico massimiliano/0000-0002-5881-1688
   Dagnes, Nicole/0000-0003-4690-7567
   Marcolin, Federica/0000-0002-4360-6905}},
Unique-ID = {{ISI:000392417700075}},
}

@inproceedings{ ISI:000392265900097,
Author = {Nebaba, S. G. and Zakharova, A. A. and Sidorenko, T. V. and Viitman, V.
   R.},
Editor = {{Casati, F and Barysheva, GA and Krieger, W}},
Title = {{Methods of Automatic Face Angle Recognition for Life Support and Safety
   Systems}},
Booktitle = {{III INTERNATIONAL SCIENTIFIC SYMPOSIUM ON LIFELONG WELLBEING IN THE
   WORLD (WELLSO 2016)}},
Series = {{European Proceedings of Social and Behavioural Sciences}},
Year = {{2017}},
Volume = {{19}},
Pages = {{735-744}},
Note = {{3rd International Scientific Symposium on Lifelong Wellbeing in the
   World (WELLSO), Tomsk Polytechn Univ, Tomsk, RUSSIA, SEP 11-16, 2016}},
Abstract = {{The vision of the surrounding and people that are within eyeshot
   influences the human well-being and safety. The rationale of system
   development that allows recognizing faces from difficult perspectives
   online and informing timely about approaching people is undisputed.
   The manuscript describes the methods of automatic detection of
   equilibrium face points in the bitmap image and methods of forming 3D
   face model. The optimal search algorithm for equilibrium points has been
   chosen. The method of forming 3D face model basing on a single bitmap
   image and building up the face image rotated to the preset angle has
   been proposed. The algorithm for estimating the angle and algorithm of
   the face image rotation have been implemented. The manuscript also
   reviews the existing methods of forming 3D face model. The algorithm for
   the formation of 3D face model from a single bitmap image and a set of
   individual 3D models have been proposed as well as the algorithm for
   forming different face angles with the calculated 3D face model aimed to
   create biometric vectors cluster. Operation results of the algorithm for
   face images formation from different angles have been presented. (C)
   2017 Published by Future Academy www.FutureAcademy.org.uk}},
DOI = {{10.15405/epsbs.2017.01.97}},
ISSN = {{2357-1330}},
ResearcherID-Numbers = {{Nebaba, Stepan/R-7357-2016}},
Unique-ID = {{ISI:000392265900097}},
}

@article{ ISI:000392292000002,
Author = {Deng, Xing and Da, Feipeng and Shao, Haijian},
Title = {{Expression-robust 3D face recognition based on feature-level fusion and
   feature-region fusion}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2017}},
Volume = {{76}},
Number = {{1}},
Pages = {{13-31}},
Month = {{JAN}},
Abstract = {{3D face shape is essentially a non-rigid free-form surface, which will
   produce non-rigid deformation under expression variations. In terms of
   that problem, a promising solution named Coherent Point Drift (CPD)
   non-rigid registration for the non-rigid region is applied to eliminate
   the influence from the facial expression while guarantees 3D surface
   topology. In order to take full advantage of the extracted
   discriminative feature of the whole face under facial expression
   variations, the novel expression-robust 3D face recognition method using
   feature-level fusion and feature-region fusion is proposed. Furthermore,
   the Principal Component Analysis and Linear Discriminant Analysis in
   combination with Rotated Sparse Regression (PL-RSR) dimensionality
   reduction method is presented to promote the computational efficiency
   and provide a solution to the curse of dimensionality problem, which
   benefit the performance optimization. The experimental evaluation
   indicates that the proposed strategy has achieved the rank-1 recognition
   rate of 97.91 \% and 96.71 \% based on Face Recognition Grand Challenge
   (FRGC) v2.0 and Bosphorus respectively, which means the proposed
   approach outperforms state-of-the-art approach.}},
DOI = {{10.1007/s11042-015-3012-8}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000392292000002}},
}

@article{ ISI:000390977800011,
Author = {Kakadiaris, Ioannis A. and Toderici, George and Evangelopoulos, Georgios
   and Passalis, Georgios and Chu, Dat and Zhao, Xi and Shah, Shishir K.
   and Theoharis, Theoharis},
Title = {{3D-2D face recognition with pose and illumination normalization}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2017}},
Volume = {{154}},
Pages = {{137-151}},
Month = {{JAN}},
Abstract = {{In this paper, we propose a 3D-2D framework for face recognition that is
   more practical than 3D-3D, yet more accurate than 2D-2D. For 3D-2D face
   recognition, the gallery data comprises of 3D shape and 2D texture data
   and the probes are arbitrary 2D images. A 3D-2D system (UR2D) is
   presented that is based on a 3D deformable face model that allows
   registration of 3D and 2D data, face alignment, and normalization of
   pose and illumination. During enrollment, subject-specific 3D models are
   constructed using 3D+2D data. For recognition, 2D images are represented
   in a normalized image space using the gallery 3D models and
   landmark-based 3D-2D projection estimation. A method for bidirectional
   relighting is applied for non-linear, local illumination normalization
   between probe and gallery textures, and a global orientation-based
   correlation metric is used for pairwise similarity scoring. The
   generated, personalized, pose- and light- normalized signatures can be
   used for one-to-one verification or one-to-many identification. Results
   for 3D-2D face recognition on the UHDB11 3D-2D database with 2D images
   under large illumination and pose variations support our hypothesis
   that, in challenging datasets, 3D-2D outperforms 2D-2D and decreases the
   performance gap against 3D-3D face recognition. Evaluations on FRGC v2.0
   3D-2D data with frontal facial images, demonstrate that the method can
   generalize to databases with different and diverse illumination
   conditions. (C) 2016 Published by Elsevier Inc.}},
DOI = {{10.1016/j.cviu.2016.04.012}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
ORCID-Numbers = {{Evangelopoulos, Georgios/0000-0003-2240-1801}},
Unique-ID = {{ISI:000390977800011}},
}

@inproceedings{ ISI:000454739700033,
Author = {ElSayed, Ahmed and Mahmood, Ausif and Sobh, Tarek},
Book-Group-Author = {{IEEE}},
Title = {{Effect of Super Resolution on High Dimensional Features for Unsupervised
   Face Recognition in the Wild}},
Booktitle = {{2017 IEEE APPLIED IMAGERY PATTERN RECOGNITION WORKSHOP (AIPR)}},
Series = {{IEEE Applied Imagery Pattern Recognition Workshop}},
Year = {{2017}},
Note = {{IEEE Applied Imagery Pattern Recognition Workshop (AIPR), Washington,
   DC, OCT 10-12, 2017}},
Abstract = {{Majority of the face recognition algorithms use query faces captured
   from uncontrolled, in the wild, environment. Because of cameras' limited
   capabilities, it is common for these captured facial images to be
   blurred or low resolution. Super resolution algorithms are therefore
   crucial in improving the resolution of such images especially when the
   image size is small and enlargement is required. This paper aims to
   demonstrate the effect of one of the state-of-the-art algorithms in the
   field of image super resolution. To demonstrate the functionality of the
   algorithm, various before and after 3D face alignment cases are provided
   using the images from the Labeled Faces in the Wild (lfw) dataset.
   Resulting images are subject to test on a closed set recognition
   protocol using unsupervised algorithms with high dimensional extracted
   features. The inclusion of super resolution algorithm resulted in
   significant improvement in recognition rate over recently reported
   results obtained from unsupervised algorithms on the same dataset.}},
ISSN = {{1550-5219}},
ISBN = {{978-1-5386-1235-4}},
Unique-ID = {{ISI:000454739700033}},
}

@inproceedings{ ISI:000456303900024,
Author = {Reji, R. and SojanLal, P.},
Editor = {{Krishnan, N and Karthikeyan, M}},
Title = {{Region Based 3D Face Recognition}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND
   COMPUTING RESEARCH (ICCIC)}},
Series = {{IEEE International Conference on Computational Intelligence and
   Computing Research}},
Year = {{2017}},
Pages = {{144-149}},
Note = {{8th IEEE International Conference on Computational Intelligence and
   Computing Research (IEEE ICCIC), Tamilnadu Coll Engn, Coimbatore, INDIA,
   DEC 14-16, 2017}},
Organization = {{IEEE; IEEE PODHIGAI; IEEE SIPCICOM}},
Abstract = {{This paper focuses on a region based methodology for expression in
   sensitive 3D face recognition process. Considering facial regions that
   are comparatively unchanging during expressions, results shows that
   using fifteen sub regions on the face can attain high 3D face
   recognition. We use a modified face recognition algorithm along with
   hierarchical contour based image registration for finding the similarity
   score. Our method operates in two modes: verification mode and
   confirmation mode. Crop 100 mm of frontal face region, apply
   preprocessing and automatically detect nose tip, translate the face
   image to origin and crop fifteen sub regions. The cropped sub regions
   are defined by cuboids which occupy more volumetric data, Nose Tip is
   the most projecting point of the face with the highest value along
   Z-axis so consider it as origin. The modified face recognition algorithm
   reduces the effects caused by facial expressions and artifacts. Finally
   a Hierarchical contour based image registration technique is applied
   which yields better results. The approach is applied on Bosphorus 3D
   datasets and achieved a verification rate of 95.3\% at 0.1\% false
   acceptance rate. In the identification scenario 99.3\% rank one
   recognition is achieved.}},
ISSN = {{2471-7851}},
ISBN = {{978-1-5090-6621-6}},
Unique-ID = {{ISI:000456303900024}},
}

@inproceedings{ ISI:000463335100004,
Author = {Bejaoui, Hela and Ghazouani, Haythem and Barhoumi, Walid},
Editor = {{BlancTalon, J and Penne, R and Philips, W and Popescu, D and Scheunders, P}},
Title = {{Fully Automated Facial Expression Recognition Using 3D Morphable Model
   and Mesh-Local Binary Pattern}},
Booktitle = {{ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS (ACIVS 2017)}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10617}},
Pages = {{39-50}},
Note = {{18th International Conference on Advanced Concepts for Intelligent
   Vision Systems (ACIVS), Antwerp, BELGIUM, SEP 18-21, 2017}},
Organization = {{Antwerp Univ; Commonwealth Sci \& Ind Res Org; Ghent Univ}},
Abstract = {{With recent advances in artificial intelligence and pattern recognition,
   automatic facial expression recognition draws a great deal of interest.
   In this area, most of works involved 2D imagery. However, they present
   some challenges related to pose, illumination variation and
   self-occlusion. To deal with these problems, we propose to reconstruct
   the face in 3D space, from only one 2D image, using the 3D Morphable
   Model (3DMM). Thus, thanks to its robustness against pose and
   illumination variations, 3DMM offers high-resolution model and fast
   fitting functionality. Then, given the reconstructed 3D face, we extract
   a set of features, which are effective to describe shape changes and
   expression-related facial appearance, using Mesh-Local Binary Pattern
   (mesh-LBP). Obtained results proved the effectiveness of combining 3DMM
   and mesh-LBP for automatic facial expression recognition from 2D single
   image. In fact, to evaluate the proposed method against state-of-the-art
   methods, a comparative study shows that the method outperforms existing
   ones.}},
DOI = {{10.1007/978-3-319-70353-4\_4}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-70353-4; 978-3-319-70352-7}},
ResearcherID-Numbers = {{Barhoumi, Walid/C-6576-2014
   }},
ORCID-Numbers = {{Barhoumi, Walid/0000-0003-2123-4992
   Ghazouani, Haythem/0000-0002-6521-5024}},
Unique-ID = {{ISI:000463335100004}},
}

@article{ ISI:000388777400069,
Author = {Lv, Jiang-Jing and Huang, Jia-Shui and Zhou, Xiang-Dong and Zhou, Xi and
   Feng, Yong},
Title = {{Latent face model for across-media face recognition}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{216}},
Pages = {{735-745}},
Month = {{DEC 5}},
Abstract = {{Across-media face recognition refers to recognizing face images from
   different sources (e.g., face sketch, 3D face model, and low resolution
   image). In spite of promising processes achieved in face recognition
   recent years, across-media face recognition is still a challenging
   problem due to the difficulty of feature matching between different
   modalities. In this paper, we propose a latent face model that creates
   mappings from a hidden space to different media space. Images from
   different media of the same person share the same latent vector in
   hidden space. A coupled Joint Bayesian model is used to calculate the
   joint probability of two faces from different media. To verify the
   effectiveness of our proposed method, extensive experiments conducted on
   various databases: self-collected low-resolution vs. high-resolution
   database, sketches vs. photos databases, 3D face model vs. photos on LFW
   database. Experimental results show that our method boosts the
   performance of face recognition with images from different sources. (C)
   2016 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2016.08.036}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000388777400069}},
}

@article{ ISI:000401423700003,
Author = {Kramer, Heather A. and Collins, Brandon M. and Gallagher, Claire V. and
   Keane, John J. and Stephens, Scott L. and Kelly, Maggi},
Title = {{Accessible light detection and ranging: estimating large tree density
   for habitat identification}},
Journal = {{ECOSPHERE}},
Year = {{2016}},
Volume = {{7}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Large trees are important to a wide variety of wildlife, including many
   species of conservation concern, such as the California spotted owl
   (Strix occidentalis occidentalis). Light detection and ranging (LiDAR)
   has been successfully utilized to identify the density of large-diameter
   trees, either by segmenting the LiDAR point cloud into individual trees,
   or by building regression models between variables extracted from the
   LiDAR point cloud and field data. Neither of these methods is easily
   accessible for most land managers due to the reliance on specialized
   software, and much available LiDAR data are being underutilized due to
   the steep learning curve required for advanced processing using these
   programs. This study derived a simple, yet effective method for
   estimating the density of large-stemmed trees from the LiDAR canopy
   height model, a standard raster product derived from the LiDAR point
   cloud that is often delivered with the LiDAR and is easy to process by
   personnel trained in geographic information systems (GIS). Ground plots
   needed to be large (1 ha) to build a robust model, but the spatial
   accuracy of plot center was less crucial to model accuracy. We also
   showed that predicted large tree density is positively linked to
   California spotted owl nest sites.}},
DOI = {{10.1002/ecs2.1593}},
Article-Number = {{e01593}},
ISSN = {{2150-8925}},
Unique-ID = {{ISI:000401423700003}},
}

@article{ ISI:000396382500043,
Author = {Magnard, Christophe and Morsdorf, Felix and Small, David and Stilla, Uwe
   and Schaepman, Michael E. and Meier, Erich},
Title = {{Single tree identification using airborne multibaseline SAR
   interferometry data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2016}},
Volume = {{186}},
Pages = {{567-580}},
Month = {{DEC 1}},
Abstract = {{Remote sensing data allow large scale observation of forested
   ecosystems. Forest assessment benefits from information about individual
   trees. Multibaseline SAR interferometry (InSAR) is able to generate
   dense point clouds of forest canopies, similar to airborne laser
   scanning (ALS). This type of point cloud was generated using data from
   the Ka-band MEMPHIS system, acquired over a mainly coniferous forest
   near Vordemwald in the Swiss Midlands. This point cloud was segmented
   using an advanced clustering technique to detect individual trees and
   derive their positions, heights, and crown diameters. To evaluate the
   InSAR point cloud properties and limitations, it was compared to
   products derived from ALS and stereo-photogrammetry. All point clouds
   showed similar geolocation accuracies with 02-0.3 m relative shifts.
   Both InSAR and photogrammetry techniques yielded points predominantly
   located in the upper levels of the forest vegetation, while ALS provided
   points from the top of the canopy down to the understory and forest
   floor. The canopy height models agreed very well with each other, with
   R-2 values between 0.84 and 0.89. The detected trees and their estimated
   physical and structural parameters were validated by comparing them to
   reference forestry data. A detection rate of similar to 90\% was
   achieved for larger trees, corresponding to half of the reference trees.
   The smaller trees were detected with a success rate of similar to 50\%.
   The tree height was slightly underestimated, with a R-2 value of 0.63.
   The estimated crown diameter agreed on an average sense, however with a
   relatively low R-2 value of 0.19. Very high success rates (>90\%) were
   obtained when matching the trees detected from the InSAR-data with those
   detected from the ALS- and photogrammetry-data. There, InSAR tree
   heights were in the mean 1-1.5 m lower, with high R-2 values ranging
   between 0.8 and 0.9. Our results demonstrate the use of millimeter wave
   SAR interferometry data as an alternative to ALS- and
   photogrammetry-based data for forest monitoring. (C) 2016 Elsevier Inc.
   All rights reserved.}},
DOI = {{10.1016/j.rse.2016.09.018}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Schaepman, Michael/B-9213-2009
   }},
ORCID-Numbers = {{Schaepman, Michael/0000-0002-9627-9565
   Magnard, Christophe/0000-0002-1473-8650}},
Unique-ID = {{ISI:000396382500043}},
}

@article{ ISI:000386225000016,
Author = {Bondi, Enrico and Pala, Pietro and Berretti, Stefano and Del Bimbo,
   Alberto},
Title = {{Reconstructing High-Resolution Face Models From Kinect Depth Sequences}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2016}},
Volume = {{11}},
Number = {{12}},
Pages = {{2843-2853}},
Month = {{DEC}},
Abstract = {{Performing face recognition across 3D scans with different resolution is
   now attracting an increasing interest thanks to the introduction of a
   new generation of depth cameras, capable of acquiring color/depth images
   over time. In fact, these devices acquire and provide depth data with
   much lower resolution compared with the 3D high-resolution scanners
   typically used for face recognition applications. If data are acquired
   without user cooperation, the problem is even more challenging, and the
   gap of resolution between probe and gallery scans can yield to a severe
   loss in terms of recognition accuracy. Based on these premises, we
   propose a method to build a higher resolution 3D face model from 3D data
   acquired by a low-resolution scanner. This face model is built using
   data acquired when a person passes in front of the scanner, without
   assuming any particular cooperation. The 3D data are registered and
   filtered by combining a model of the expected distribution of the
   acquisition error with a variant of the lowess method to remove outliers
   and build the final face model. The proposed approach is evaluated in
   terms of accuracy of face reconstruction and face recognition.}},
DOI = {{10.1109/TIFS.2016.2601059}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ORCID-Numbers = {{Berretti, Stefano/0000-0003-1219-4386
   PALA, PIETRO/0000-0001-5670-3774}},
Unique-ID = {{ISI:000386225000016}},
}

@article{ ISI:000386741300011,
Author = {Li, Billy Y. L. and Xue, Mingliang and Mian, Ajmal and Liu, Wanquan and
   Krishna, Aneesh},
Title = {{Robust RGB-D face recognition using Kinect sensor}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{214}},
Pages = {{93-108}},
Month = {{NOV 19}},
Abstract = {{In this paper we propose a robust face recognition algorithm for low
   resolution RGB-D Kinect data. Many techniques are proposed for image
   preprocessing due to the noisy depth data. First, facial symmetry is
   exploited based on the 3D point cloud to obtain a canonical frontal view
   image irrespective of the initial pose and then depth data is converted
   to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are
   then used to project RGB to DCS (Discriminant Color Space) and normal
   maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust
   Sparse Coding method is proposed that codes the multiple channels (DCS
   or DNM) of a test image as a sparse combination of training samples with
   different pixel weighting. Weights are calculated dynamically in an
   iterative process to achieve robustness against variations in pose,
   illumination, facial expressions and disguise. In contrast to existing
   techniques, our multi-channel approach is more robust to variations.
   Reconstruction errors of the test image (DCS and DNM) are normalized and
   fused to decide its identity. The proposed algorithm is evaluated on
   four public databases. It achieves 98.4\% identification rate on
   CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects.
   Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2
   databases, the proposed algorithm achieves 97.6\%, 95.6\% and 95.2\%
   identification rates respectively. To the best of our knowledge, these
   are the highest identification rates reported so far for the first three
   databases. (C) 2016 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2016.06.012}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
ORCID-Numbers = {{Krishna, Aneesh/0000-0001-8637-5732
   liu, wanquan/0000-0003-4910-353X}},
Unique-ID = {{ISI:000386741300011}},
}

@article{ ISI:000389801200007,
Author = {Li, Billy Y. L. and Mian, Ajmal S. and Liu, Wanquan and Krishna, Aneesh},
Title = {{Face recognition based on Kinect}},
Journal = {{PATTERN ANALYSIS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{19}},
Number = {{4}},
Pages = {{977-987}},
Month = {{NOV}},
Abstract = {{In this paper, we present a new algorithm that utilizes low-quality red,
   green, blue and depth (RGB-D) data from the Kinect sensor for face
   recognition under challenging conditions. This algorithm extracts
   multiple features and fuses them at the feature level. A Finer Feature
   Fusion technique is developed that removes redundant information and
   retains only the meaningful features for possible maximum class
   separability. We also introduce a new 3D face database acquired with the
   Kinect sensor which has released to the research community. This
   database contains over 5,000 facial images (RGB-D) of 52 individuals
   under varying pose, expression, illumination and occlusions. Under the
   first three variations and using only the noisy depth data, the proposed
   algorithm can achieve 72.5 \% recognition rate which is significantly
   higher than the 41.9 \% achieved by the baseline LDA method. Combined
   with the texture information, 91.3 \% recognition rate has achieved
   under illumination, pose and expression variations. These results
   suggest the feasibility of low-cost 3D sensors for real-time face
   recognition.}},
DOI = {{10.1007/s10044-015-0456-4}},
ISSN = {{1433-7541}},
EISSN = {{1433-755X}},
ORCID-Numbers = {{Krishna, Aneesh/0000-0001-8637-5732
   liu, wanquan/0000-0003-4910-353X}},
Unique-ID = {{ISI:000389801200007}},
}

@article{ ISI:000385945000006,
Author = {Yu, Xiang and Huang, Junzhou and Zhang, Shaoting and Metaxas, Dimitris
   N.},
Title = {{Face Landmark Fitting via Optimized Part Mixtures and Cascaded
   Deformable Model}},
Journal = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}},
Year = {{2016}},
Volume = {{38}},
Number = {{11}},
Pages = {{2212-2226}},
Month = {{NOV}},
Abstract = {{This paper addresses the problem of facial landmark localization and
   tracking from a single camera. We present a two-stage cascaded
   deformable shape model to effectively and efficiently localize facial
   landmarks with large head pose variations. In initialization stage, we
   propose a group sparse optimized mixture model to automatically select
   the most salient facial landmarks. By introducing 3D face shape model,
   we apply procrustes analysis to provide pose-aware landmark
   initialization. In landmark localization stage, the first step uses
   mean-shift local search with constrained local model to rapidly approach
   the global optimum. The second step uses component-wise active contours
   to discriminatively refine the subtle shape variation. Our framework
   simultaneously handles face detection, pose-robust landmark localization
   and tracking in real time. Extensive experiments are conducted on both
   laboratory environmental databases and face-in-the-wild databases. The
   results reveal that our approach consistently outperforms
   state-of-the-art methods for face alignment and tracking.}},
DOI = {{10.1109/TPAMI.2015.2509999}},
ISSN = {{0162-8828}},
EISSN = {{1939-3539}},
Unique-ID = {{ISI:000385945000006}},
}

@article{ ISI:000386874900020,
Author = {Guo, Yulan and Lei, Yinjie and Liu, Li and Wang, Yan and Bennamoun,
   Mohammed and Sohelf, Ferdous},
Title = {{EI3D: Expression-invariant 3D face recognition based on feature and
   shape matching}},
Journal = {{PATTERN RECOGNITION LETTERS}},
Year = {{2016}},
Volume = {{83}},
Number = {{3}},
Pages = {{403-412}},
Month = {{NOV 1}},
Abstract = {{This paper presents a local feature based shape matching algorithm for
   expression-invariant 3D face recognition. Each 3D face is first
   automatically detected from a raw 3D data and normalized to achieve pose
   invariance. The 3D face is then represented by a set of keypoints and
   their associated local feature descriptors to achieve robustness to
   expression variations. During face recognition, a probe face is compared
   against each gallery face using both local feature matching and 3D point
   cloud registration. The number of feature matches, the average distance
   of matched features, and the number of closest point pairs after
   registration are used to measure the similarity between two 3D faces.
   These similarity metrics are then fused to obtain the final results. The
   proposed algorithm has been tested on the FRGC v2 benchmark and a high
   recognition performance has been achieved. It obtained the
   state-of-the-art results by achieving an overall rank- 1 identification
   rate of 97.0\% and an average verification rate of 99.01\% at 0.001
   false acceptance rate for all faces with neutral and non-neutral
   expressions. Further, the robustness of our algorithm under different
   occlusions has been demonstrated on the Bosphorus dataset. (C) 2016
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.patrec.2016.04.003}},
ISSN = {{0167-8655}},
EISSN = {{1872-7344}},
ResearcherID-Numbers = {{Sohel, Ferdous/C-2428-2013
   }},
ORCID-Numbers = {{Sohel, Ferdous/0000-0003-1557-4907
   Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000386874900020}},
}

@article{ ISI:000386396800002,
Author = {Ma, Mingyang and Peng, Silong and Hu, Xiyuan},
Title = {{A lighting robust fitting approach of 3D morphable model for face
   reconstruction}},
Journal = {{VISUAL COMPUTER}},
Year = {{2016}},
Volume = {{32}},
Number = {{10}},
Pages = {{1223-1238}},
Month = {{OCT}},
Abstract = {{Three-dimensional morphable model (3DMM) is a powerful tool for
   recovering 3D shape and texture from a single facial image. The success
   of 3DMM relies on two things: an effective optimization strategy and a
   realistic approach to synthesizing face images. However, most previous
   methods have focused on developing an optimization strategy under
   Phong's synthesis approach. In this paper, we adopt a more realistic
   synthesis technique that fully considers illumination and reflectance in
   the 3DMM fitting process. Using the sphere harmonic illumination model
   (SHIM), our new synthesis approach can account for more lighting factors
   than Phong's model. Spatially varying specular reflectance is also
   introduced into the synthesis process. Under SHIM, the cost function is
   nearly linear for all parameters, which simplifies the optimization. We
   apply our new optimization algorithm to determine the shape and texture
   parameters simultaneously. The accuracy of the recovered shape and
   texture can be improved significantly by considering the spatially
   varying specular reflectance. Hence, our algorithm produces an enhanced
   shape and texture compared with previous SHIM-based methods that recover
   shape from feature points. Although we use just a single input image in
   a profile pose, our approach gives plausible results. Experiments on a
   well-known image database show that, compared to state-of-the-art
   methods based on Phong's model, the proposed approach enhances the
   robustness of the 3DMM fitting results under extreme lighting and
   profile pose.}},
DOI = {{10.1007/s00371-015-1158-z}},
ISSN = {{0178-2789}},
EISSN = {{1432-2315}},
ORCID-Numbers = {{Hu, Xiyuan/0000-0002-7095-6986}},
Unique-ID = {{ISI:000386396800002}},
}

@article{ ISI:000389208600010,
Author = {Ammar, Chouchane and Mebarka, Belahcene and Abdelmalik, Ouamane and
   Salah, Bourennane},
Title = {{Evaluation of Histograms Local Features and Dimensionality Reduction for
   3D Face Verification}},
Journal = {{JOURNAL OF INFORMATION PROCESSING SYSTEMS}},
Year = {{2016}},
Volume = {{12}},
Number = {{3}},
Pages = {{468-488}},
Month = {{SEP}},
Abstract = {{The paper proposes a novel framework for 3D face verification using
   dimensionality reduction based on highly distinctive local features in
   the presence of illumination and expression variations. The histograms
   of efficient local descriptors are used to represent distinctively the
   facial images. For this purpose, different local descriptors are
   evaluated, Local Binary Patterns (LBP), Three-Patch Local Binary
   Patterns (TPLBP), Four-Patch Local Binary Patterns (FPLBP), Binarized
   Statistical Image Features (BSIF) and Local Phase Quantization (LPQ).
   Furthermore, experiments on the combinations of the four local
   descriptors at feature level using simply histograms concatenation are
   provided. The performance of the proposed approach is evaluated with
   different dimensionality reduction algorithms: Principal Component
   Analysis (PCA), Orthogonal Locality Preserving Projection (OLPP) and the
   combined PCA+EFM (Enhanced Fisher linear discriminate Model). Finally,
   multi-class Support Vector Machine (SVM) is used as a classifier to
   carry out the verification between imposters and customers. The proposed
   method has been tested on CASIA-3D face database and the experimental
   results show that our method achieves a high verification performance.}},
DOI = {{10.3745/JIPS.02.0037}},
ISSN = {{1976-913X}},
EISSN = {{2092-805X}},
Unique-ID = {{ISI:000389208600010}},
}

@article{ ISI:000385213200015,
Author = {Issa, Haitham and Issa, Sali and Issa, Mohammad},
Title = {{New Prototype of Hybrid 3D-Biometric Facial Recognition System}},
Journal = {{INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY}},
Year = {{2016}},
Volume = {{13}},
Number = {{5}},
Pages = {{590-594}},
Month = {{SEP}},
Abstract = {{In the last decades, a lot of 3D face recognition techniques have been
   proposed. They can be divided into three parts, holistic matching
   techniques, feature-based techniques and hybrid techniques. In this
   paper, a hybrid technique is used, where, a prototype of a new hybrid
   face recognition technique depends on 3D face scan images are designed,
   simulated and implemented. Some geometric rules are used for analyzing
   and mapping the face. Image processing is used to get the
   two-dimensional values of predetermined and specific facial points,
   software programming is used to perform a three-dimensional coordinates
   of the predetermined points and to calculate several geometric parameter
   ratios and relations. Neural network technique is used for processing
   the calculated geometric parameters and then performing facial
   recognition. The new design is not affected by variant pose,
   illumination and expression and has high accurate level compared with
   the 2D analysis. Moreover, the proposed algorithm is of higher
   performance than latest's published biometric recognition algorithms in
   terms of cost, confidentiality of results, and availability of design
   tools.}},
ISSN = {{1683-3198}},
Unique-ID = {{ISI:000385213200015}},
}

@article{ ISI:000382679900012,
Author = {Bagchi, Parama and Bhattacharjee, Debotosh and Nasipuri, Mita},
Title = {{A robust analysis, detection and recognition of facial features in 2.5D
   images}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{75}},
Number = {{18}},
Pages = {{11059-11096}},
Month = {{SEP}},
Abstract = {{A robust technique for recognition of 3D faces which performs well with
   face images with various poses, expressions and occlusions. In this
   method, the face images represented in 3D mesh format are smoothed using
   trilinear interpolation and then converted to 2.5D image or range
   images. Nose-tip which is the most prominent feature on human face is
   detected first on the corner points selected by 3D Harris corner and
   curvedness at those corner points. K-Means clustering is applied to
   group those corner points in 2 groups. The cluster of points with larger
   curvedness values represents the possible locations of nose-tip.
   Nose-tip is finally localized using Mean-Gaussian curvature values of
   the prospective corner points in that cluster. Using the nose-tip
   location, other facial landmarks namely corners of the eyes and mouth
   are located and a facial graph is generated. The dimensionality of 2.5D
   feature space is that, depth values are stored at each (x, y) grid of
   the 2.5D image, so a 3D face image uses some function to map the depth
   value at any pixel position to the intensity with which that pixel will
   be displayed. Here finally extracted features for each subject is of
   dimensionality {[}1x21], taking into account the Euclidean distances in
   three dimensional form between each feature points detected
   automatically. Taking Euclidean distances between all pairs of landmark
   points as features, face images are classified using Multilayer
   Perceptron (MLP), as well as Support Vector Machines (SVM). Maximum
   recognition rates of 75 and 87.5 \% have been obtained in case of
   Bosphorus Databases, 62.5 and 87.5 \% in case of GavabDB databases, 75
   and 87.5 \% in case of Frav3D Databases by Multilayer Perceptron and
   Support Vector Machines respectively.}},
DOI = {{10.1007/s11042-015-2835-7}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000382679900012}},
}

@article{ ISI:000376708000002,
Author = {Alashkar, Taleb and Ben Amor, Boulbaba and Daoudi, Mohamed and Berretti,
   Stefano},
Title = {{A Grassmann framework for 4D facial shape analysis}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2016}},
Volume = {{57}},
Pages = {{21-30}},
Month = {{SEP}},
Abstract = {{In this paper, we investigate the contribution of dynamic evolution of
   3D faces to identity recognition. To this end, we adopt a subspace
   representation of the flow of curvature-maps computed on 3D facial
   frames of a sequence, after normalizing their pose. Such representation
   allows us to embody the shape as well as its temporal evolution within
   the same subspace representation. Dictionary learning and sparse coding
   over the space of fixed-dimensional subspaces, called Grassmann
   manifold, have been used to perform face recognition. We have conducted
   extensive experiments on the BU-4DFE dataset. The obtained results of
   the proposed approach provide promising results. (C) 2016 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.patcog.2016.03.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018
   Daoudi, Mohammed/H-5935-2013}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305
   Berretti, Stefano/0000-0003-1219-4386
   Daoudi, Mohammed/0000-0003-4219-7860}},
Unique-ID = {{ISI:000376708000002}},
}

@article{ ISI:000384279700004,
Author = {Samad, Manar D. and Iftekharuddin, Khan M.},
Title = {{Frenet Frame-Based Generalized Space Curve Representation for
   Pose-Invariant Classification and Recognition of 3-D Face}},
Journal = {{IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS}},
Year = {{2016}},
Volume = {{46}},
Number = {{4}},
Pages = {{522-533}},
Month = {{AUG}},
Abstract = {{The state-of-the-art methods in classifying 3-D representation of the
   face involve challenges in extracting representative features directly
   from the large volume of facial data. These methods mostly ignore the
   effect of pose distortions on 3-D facial data and entail heavy
   computations as well as manual processing steps. This work proposes a
   novel Frenet frame-based generalized space curve representation method
   for 3-D pose-invariant face and facial expression recognition and
   classification. Three-dimensional facial curves are extracted from
   either frontal or synthetically posed 3-D facial data to derive the
   proposed Frenet frame-based features. A mathematical framework shows the
   proof of pose invariance property for the features. The effectiveness of
   the proposed method is evaluated in two recognition tasks: 3-D face
   recognition (3D-FR) and 3-D facial expression recognition (3D-FER) using
   benchmarked 3-D datasets. The proposed framework yields 96\% rank-I
   recognition rate for 3D-FR and 91.4\% area under ROC curves for six
   basic 3D-FER. The performance evaluation also shows that the proposed
   mathematical framework yields pose-invariant 3D-FR and 3D-FER for a wide
   range of pose angles. This pose invariance property of the Frenet
   frame-based features alleviates the need for an expensive 3-D face
   registration in the preprocessing step, which, in turn, enables a faster
   processing time. The evaluation results further suggest that the
   proposed method is not only computationally efficient and versatile, but
   also offers competitive performance when compared with the existing
   state-of-the-art methods reported for either 3D-FR or 3D-FER.}},
DOI = {{10.1109/THMS.2016.2515602}},
ISSN = {{2168-2291}},
EISSN = {{2168-2305}},
Unique-ID = {{ISI:000384279700004}},
}

@article{ ISI:000381123900010,
Author = {Fan, Ke and Mian, Ajmal and Liu, Wanquan and Li, Ling},
Title = {{Unsupervised manifold alignment using soft-assign technique}},
Journal = {{MACHINE VISION AND APPLICATIONS}},
Year = {{2016}},
Volume = {{27}},
Number = {{6}},
Pages = {{929-942}},
Month = {{AUG}},
Abstract = {{In this paper, we propose a robust unsupervised algorithm for automatic
   alignment of two manifolds in different datasets with possibly different
   dimensionalities. The significant contribution is that the proposed
   alignment algorithm is performed automatically without any assumptions
   on the correspondences between the two manifolds. For such purpose, we
   first automatically extract local feature histograms at each point of
   the manifolds and establish an initial similarity between the two
   datasets by matching their histogram-based features. Based on such
   similarity, an embedding space is estimated where the distance between
   the two manifolds is minimized while maximally retaining the original
   structure of the manifolds. The elegance of this idea is that such
   complicated problem is formulated as a generalized eigenvalue problem,
   which can be easily solved. The alignment process is achieved by
   iteratively increasing the sparsity of correspondence matrix until the
   two manifolds are correctly aligned and consequently one can reveal
   their joint structure. We demonstrate the effectiveness of our algorithm
   on different datasets by aligning protein structures, 3D face models and
   facial images of different subjects under pose and lighting variations.
   Finally, we also compare with a state-of-the-art algorithm and the
   results show the superiority of the proposed manifold alignment in terms
   of vision effect and numerical accuracy.}},
DOI = {{10.1007/s00138-016-0772-8}},
ISSN = {{0932-8092}},
EISSN = {{1432-1769}},
ResearcherID-Numbers = {{Smith, Nash/R-3104-2017
   }},
ORCID-Numbers = {{Smith, Nash/0000-0002-4406-0043
   liu, wanquan/0000-0003-4910-353X
   Li, Ling/0000-0001-9722-9503}},
Unique-ID = {{ISI:000381123900010}},
}

@article{ ISI:000379266300013,
Author = {Quan, Wei and Matuszewski, Bogdan J. and Shark, Lik-Kwan},
Title = {{Statistical shape modelling for expression-invariant face analysis and
   recognition}},
Journal = {{PATTERN ANALYSIS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{19}},
Number = {{3}},
Pages = {{765-781}},
Month = {{AUG}},
Abstract = {{Paper introduces a 3-D shape representation scheme for automatic face
   analysis and identification, and demonstrates its invariance to facial
   expression. The core of this scheme lies on the combination of
   statistical shape modelling and non-rigid deformation matching. While
   the former matches 3-D faces with facial expression, the latter provides
   a low-dimensional feature vector that controls the deformation of model
   for matching the shape of new input, thereby enabling robust
   identification of 3-D faces. The proposed scheme is also able to handle
   the pose variation without large part of missing data. To assist the
   establishment of dense point correspondences, a modified
   free-form-deformation based on B-spline warping is applied with the help
   of extracted landmarks. The hybrid iterative closest point method is
   introduced for matching the models and new data. The feasibility and
   effectiveness of the proposed method was investigated using standard
   publicly available Gavab and BU-3DFE datasets, which contain faces with
   expression and pose changes. The performance of the system was compared
   with that of nine benchmark approaches. The experimental results
   demonstrate that the proposed scheme provides a competitive solution for
   face recognition.}},
DOI = {{10.1007/s10044-014-0439-x}},
ISSN = {{1433-7541}},
EISSN = {{1433-755X}},
Unique-ID = {{ISI:000379266300013}},
}

@article{ ISI:000379752600018,
Author = {Zhen, Qingkai and Huang, Di and Wang, Yunhong and Chen, Liming},
Title = {{Muscular Movement Model-Based Automatic 3D/4D Facial Expression
   Recognition}},
Journal = {{IEEE TRANSACTIONS ON MULTIMEDIA}},
Year = {{2016}},
Volume = {{18}},
Number = {{7}},
Pages = {{1438-1450}},
Month = {{JUL}},
Abstract = {{Facial expression is an important channel for human nonverbal
   communication. This paper presents a novel and effective approach to
   automatic 3D/4D facial expression recognition based on the muscular
   movement model (MMM). In contrast to most of existing methods, the MMM
   deals with such an issue in the viewpoint of anatomy. It first
   automatically segments the input 3D face (frame) by localizing the
   corresponding points within each muscular region of the reference using
   iterative closest normal point. A set of features with multiple
   differential quantities, including coordinate, normal, and shape index
   values, are then extracted to describe the geometry deformation of each
   segmented region. Meanwhile, we analyze the importance of these muscular
   areas, and a score level fusion strategy is exploited to optimize their
   weights by the genetic algorithm in the learning step. The support
   vector machine and the hidden Markov model are finally used to predict
   the expression label in 3D and 4D, respectively. The experiments are
   conducted on the BU-3DFE and BU-4DFE databases, and the results achieved
   clearly demonstrate the effectiveness of the proposed method.}},
DOI = {{10.1109/TMM.2016.2557063}},
ISSN = {{1520-9210}},
EISSN = {{1941-0077}},
Unique-ID = {{ISI:000379752600018}},
}

@article{ ISI:000375611700005,
Author = {Hernandez-Rodriguez, Felipe and Castelan, Mario},
Title = {{A photometric sampling method for facial shape recovery}},
Journal = {{MACHINE VISION AND APPLICATIONS}},
Year = {{2016}},
Volume = {{27}},
Number = {{4}},
Pages = {{483-497}},
Month = {{MAY}},
Abstract = {{The authors propose a photometric method to recover facial shape that is
   consistent with expected facial proportions. The method borrows ideas
   from photometric sampling, a technique that estimates shape from
   continuous variations of a light source around a single circular path.
   This approach aims at enriching photometric information by including
   variations of the light source along its zenith angle. To this end, a
   luminance matrix describing lighting response along both azimuth and
   zenith angles of the light source is built for each pixel. A method
   based on fitting sine functions onto the singular vectors of the
   collected luminance matrices is proposed for estimating a surface normal
   map. The estimated surface normals are later refined to maximize a
   facial proportion criterion and finally be integrated. Experiments
   demonstrate that our approach successfully approximates 3D face shape
   while preserving facial proportions within the limits of expected depth.}},
DOI = {{10.1007/s00138-016-0755-9}},
ISSN = {{0932-8092}},
EISSN = {{1432-1769}},
Unique-ID = {{ISI:000375611700005}},
}

@article{ ISI:000372355200007,
Author = {Werghi, Naoufel and Tortorici, Claudio and Berretti, Stefano and Del
   Bimbo, Alberto},
Title = {{Boosting 3D LBP-Based Face Recognition by Fusing Shape and Texture
   Descriptors on the Mesh}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2016}},
Volume = {{11}},
Number = {{5}},
Pages = {{964-979}},
Month = {{MAY}},
Abstract = {{In this paper, we present a novel approach for fusing shape and texture
   local binary patterns (LBPs) on a mesh for 3D face recognition. Using a
   recently proposed framework, we compute LBP directly on the face mesh
   surface, then we construct a grid of the regions on the facial surface
   that can accommodate global and partial descriptions. Compared with its
   depth-image counterpart, our approach is distinguished by the following
   features: 1) inherits the intrinsic advantages of mesh surface (e.g.,
   preservation of the full geometry); 2) does not require normalization;
   and 3) can accommodate partial matching. In addition, it allows early
   level fusion of texture and shape modalities. Through experiments
   conducted on the BU-3DFE and Bosphorus databases, we assess different
   variants of our approach with regard to facial expressions and missing
   data, also in comparison to the state-of-the-art solutions.}},
DOI = {{10.1109/TIFS.2016.2515505}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
ResearcherID-Numbers = {{Werghi, Naoufel/D-2398-2018
   }},
ORCID-Numbers = {{Werghi, Naoufel/0000-0002-5542-448X
   Berretti, Stefano/0000-0003-1219-4386}},
Unique-ID = {{ISI:000372355200007}},
}

@article{ ISI:000374364300013,
Author = {Ming, Yue and Hong, Xiaopeng},
Title = {{A unified 3D face authentication framework based on robust local mesh
   SIFT feature}},
Journal = {{NEUROCOMPUTING}},
Year = {{2016}},
Volume = {{184}},
Number = {{SI}},
Pages = {{117-130}},
Month = {{APR 5}},
Abstract = {{In this paper, we design a unified 3D face authentication system for
   practical use. First, we propose a facial depth recovery method to
   construct a facial depth map from stereoscopic videos. It.effectively
   utilize prior facial information and incorporate the visibility term to
   classify static and dynamic pixels for robust depth estimation.
   Secondly, in order to make 3D face authentication more accurate and
   consistent, we present an intrinsic scale feature detection for
   interesting points on 3D facial mesh regions. Then, a novel feature
   descriptor is proposed, called Local Mesh Scale -Invariant Feature
   Transform (LMSIFT) to reflect the different face recognition abilities
   in different facial regions. Finally, the sparse optimization problem of
   visual codebook is used to 3D face learning. We evaluate our approach on
   publicly available 3D face databases and self-collected realistic scene
   databases. We also develop an interactive education system to
   investigate its performance in practice, which demonstrates the high
   performance of the proposed approach for accurate 3D face
   authentication. Compared with previous popular approaches, our system
   has consistently better performance in terms of effectiveness,
   robustness and universality. (C) 2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2015.07.127}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000374364300013}},
}

@article{ ISI:000373410000015,
Author = {Dornaika, F. and Chahla, C. and Khattar, F. and Abdallah, F. and
   Snoussi, H.},
Title = {{Discriminant sparse label-sensitive embedding: Application to
   image-based face pose estimation}},
Journal = {{ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE}},
Year = {{2016}},
Volume = {{50}},
Pages = {{168-176}},
Month = {{APR}},
Abstract = {{In this letter, the authors propose a new embedding scheme for
   image-based continuous face pose estimation. The main contributions are
   as follows. First, it is shown that the concept of label-sensitive
   Locality Preserving Projections, proposed for age estimation, can be
   used for model-less face pose estimation. Second, the authors propose a
   linear embedding by exploiting the connections between facial features
   and pose labels via a sparse coding scheme. The resulting technique is
   called Sparse Label sensitive Locality Preserving Projections
   (Sp-LsLPP). Third, for enhancing the discrimination between poses, the
   projections obtained by Sp-LsLPP are fed to a Discriminant Embedding
   that exploits the continuous labels. The resulting framework has less
   parameters compared to related works. It has been applied to the problem
   of model-less face yaw angle estimation (person independent 3D face pose
   estimation). It was tested on three databases: FacePix, Taiwan, and
   Columbia. It was conveniently compared with other linear and non-linear
   techniques. The experimental results confirm that the proposed framework
   can outperform, in general, the existing ones. (C) 2016 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.engappai.2016.01.035}},
ISSN = {{0952-1976}},
EISSN = {{1873-6769}},
Unique-ID = {{ISI:000373410000015}},
}

@article{ ISI:000372791200003,
Author = {Liang, Haoran and Liang, Ronghua and Song, Mingli and He, Xiaofei},
Title = {{Coupled Dictionary Learning for the Detail-Enhanced Synthesis of 3-D
   Facial Expressions}},
Journal = {{IEEE TRANSACTIONS ON CYBERNETICS}},
Year = {{2016}},
Volume = {{46}},
Number = {{4}},
Pages = {{890-901}},
Month = {{APR}},
Abstract = {{The desire to reconstruct 3-D face models with expressions from 2-D face
   images fosters increasing interest in addressing the problem of face
   modeling. This task is important and challenging in the field of
   computer animation. Facial contours and wrinkles are essential to
   generate a face with a certain expression; however, these details are
   generally ignored or are not seriously considered in previous studies on
   face model reconstruction. Thus, we employ coupled radius basis function
   networks to derive an intermediate 3-D face model from a single 2-D face
   image. To optimize the 3-D face model further through landmarks, a
   coupled dictionary that is related to 3-D face models and their
   corresponding 3-D landmarks is learned from the given training set
   through local coordinate coding. Another coupled dictionary is then
   constructed to bridge the 2-D and 3-D landmarks for the transfer of
   vertices on the face model. As a result, the final 3-D face can be
   generated with the appropriate expression. In the testing phase, the 2-D
   input faces are converted into 3-D models that display different
   expressions. Experimental results indicate that the proposed approach to
   facial expression synthesis can obtain model details more effectively
   than previous methods can.}},
DOI = {{10.1109/TCYB.2015.2417211}},
ISSN = {{2168-2267}},
EISSN = {{2168-2275}},
Unique-ID = {{ISI:000372791200003}},
}

@article{ ISI:000368744300013,
Author = {Danelakis, Antonios and Theoharis, Theoharis and Pratikakis, Ioannis and
   Perakis, Panagiotis},
Title = {{An effective methodology for dynamic 3D facial expression retrieval}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2016}},
Volume = {{52}},
Pages = {{174-185}},
Month = {{APR}},
Abstract = {{The problem of facial expression recognition in dynamic sequences of 3D
   face scans has received a significant amount of attention in the recent
   past whereas the problem of retrieval in this type of data has not. A
   novel retrieval methodology for such data is introduced in this paper.
   The proposed methodology automatically detects specific facial landmarks
   and uses them to create a descriptor. This descriptor is the
   concatenation of three sub-descriptors which capture topological as well
   as geometric information of the 3D face scans. The motivation behind the
   proposed hybrid facial expression descriptor is the fact that some
   facial expressions, like happiness and surprise, are characterized by
   obvious changes in the mouth topology while others, like anger, fear and
   sadness, produce geometric but no significant topological changes. The
   proposed retrieval scheme exploits the Dynamic Time Warping technique in
   order to compare descriptors corresponding to different 3D facial
   sequences. A detailed evaluation of the introduced retrieval scheme is
   presented showing that it outperforms previous state-of-the-art
   retrieval schemes. Experiments have been conducted using the six
   prototypical expressions of the standard dataset BU-4DFE and the eight
   prototypical expressions of the recently available dataset BP4D-
   Spontaneous. Finally, a majority voting scheme based on the retrieval
   results is used to achieve unsupervised dynamic 3D facial expression
   recognition. The achieved classification accuracy is comparable to the
   state-of-the-art supervised dynamic 3D facial expression recognition
   techniques. (C) 2015 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2015.10.012}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Perakis, Panagiotis/0000-0002-5053-484X}},
Unique-ID = {{ISI:000368744300013}},
}

@article{ ISI:000368744300016,
Author = {Lei, Yinjie and Guo, Yulan and Hayat, Munawar and Bennamoun, Mohammed
   and Zhou, Xinzhi},
Title = {{A Two-Phase Weighted Collaborative Representation for 3D partial face
   recognition with single sample}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2016}},
Volume = {{52}},
Pages = {{218-237}},
Month = {{APR}},
Abstract = {{3D face recognition with the availability of only partial data (missing
   parts, occlusions and data corruptions) and single training sample is a
   highly challenging task. This paper presents an efficient 3D face
   recognition approach to address this challenge. We represent a facial
   scan with a set of local Keypoint-based Multiple Triangle Statistics
   (KMTS), which is robust to partial facial data, large facial expressions
   and pose variations. To address the single sample problem, we then
   propose a Two-Phase Weighted Collaborative Representation Classification
   (TPWCRC) framework. A class-based probability estimation is first
   calculated based on the extracted local descriptors as a prior
   knowledge. The resulting class-based probability estimation is then
   incorporated into the proposed classification framework as a locality
   constraint to further enhance its discriminating power. Experimental
   results on six challenging 3D facial datasets show that the proposed
   KMTS-TPWCRC framework achieves promising results for human face
   recognition with missing parts, occlusions, data corruptions,
   expressions and pose variations. (C) 2015 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.patcog.2015.09.035}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Hayat, Munawar/0000-0002-2706-5985
   Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000368744300016}},
}

@article{ ISI:000375932300027,
Author = {Tian, Lei and Fan, Chunxiao and Ming, Yue},
Title = {{Multiple scales combined principle component analysis deep learning
   network for face recognition}},
Journal = {{JOURNAL OF ELECTRONIC IMAGING}},
Year = {{2016}},
Volume = {{25}},
Number = {{2}},
Month = {{MAR}},
Abstract = {{It is well known that higher level features can represent the abstract
   semantics of original data. We propose a multiple scales combined deep
   learning network to learn a set of high-level feature representations
   through each stage of convolutional neural network for face recognition,
   which is named as multiscaled principle component analysis (PCA) Network
   (MS-PCANet). There are two main differences between our model and the
   traditional deep learning network. On the one hand, we get the prefixed
   filter kernels by learning the principal component of images' patches
   using PCA, nonlinearly process the convolutional results by using simple
   binary hashing, and pool them using spatial pyramid pooling method. On
   the other hand, in our model, the output features of several stages are
   fed to the classifier. The purpose of combining feature representations
   from multiple stages is to provide multiscaled features to the
   classifier, since the features in the latter stage are more global and
   invariant than those in the early stage. Therefore, our MS-PCANet
   feature compactly encodes both holistic abstract information and local
   specific information. Extensive experimental results show our MS-PCANet
   model can efficiently extract high-level feature presentations and
   outperform state-of-the-art face/expression recognition methods on
   multiple modalities benchmark face-related datasets. (C) 2016 SPIE and
   IS\&T}},
DOI = {{10.1117/1.JEI.25.2.023025}},
Article-Number = {{023025}},
ISSN = {{1017-9909}},
EISSN = {{1560-229X}},
Unique-ID = {{ISI:000375932300027}},
}

@article{ ISI:000371781700044,
Author = {Zhang, Wenhao and Smith, Melvyn L. and Smith, Lyndon N. and Farooq,
   Abdul},
Title = {{Gender recognition from facial images: two or three dimensions?}},
Journal = {{JOURNAL OF THE OPTICAL SOCIETY OF AMERICA A-OPTICS IMAGE SCIENCE AND
   VISION}},
Year = {{2016}},
Volume = {{33}},
Number = {{3}},
Pages = {{333-344}},
Month = {{MAR 1}},
Abstract = {{This paper seeks to compare encoded features from both two-dimensional
   (2D) and three-dimensional (3D) face images in order to achieve
   automatic gender recognition with high accuracy and robustness. The
   Fisher vector encoding method is employed to produce 2D, 3D, and fused
   features with escalated discriminative power. For 3D face analysis, a
   two-source photometric stereo (PS) method is introduced that enables 3D
   surface reconstructions with accurate details as well as desirable
   efficiency. Moreover, a 2D + 3D imaging device, taking the two-source PS
   method as its core, has been developed that can simultaneously gather
   color images for 2D evaluations and PS images for 3D analysis. This
   system inherits the superior reconstruction accuracy from the standard
   (three or more light) PS method but simplifies the reconstruction
   algorithm as well as the hardware design by only requiring two light
   sources. It also offers great potential for facilitating human computer
   interaction by being accurate, cheap, efficient, and nonintrusive. Ten
   types of low-level 2D and 3D features have been experimented with and
   encoded for Fisher vector gender recognition. Evaluations of the Fisher
   vector encoding method have been performed on the FERET database, Color
   FERET database, LFW database, and FRGCv2 database, yielding 97.7\%,
   98.0\%, 92.5\%, and 96.7\% accuracy, respectively. In addition, the
   comparison of 2D and 3D features has been drawn from a self-collected
   dataset, which is constructed with the aid of the 2D + 3D imaging device
   in a series of data capture experiments. With a variety of experiments
   and evaluations, it can be proved that the Fisher vector encoding method
   outperforms most state-of-the-art gender recognition methods. It has
   also been observed that 3D features reconstructed by the two-source PS
   method are able to further boost the Fisher vector gender recognition
   performance, i.e., up to a 6\% increase on the self-collected database.
   (C) 2016 Optical Society of America}},
DOI = {{10.1364/JOSAA.33.000333}},
ISSN = {{1084-7529}},
EISSN = {{1520-8532}},
ORCID-Numbers = {{Smith, Lyndon/0000-0001-5821-0586}},
Unique-ID = {{ISI:000371781700044}},
}

@article{ ISI:000371787800087,
Author = {Alletto, Stefano and Abati, Davide and Serra, Giuseppe and Cucchiara,
   Rita},
Title = {{Exploring Architectural Details Through a Wearable Egocentric Vision
   Device}},
Journal = {{SENSORS}},
Year = {{2016}},
Volume = {{16}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Augmented user experiences in the cultural heritage domain are in
   increasing demand by the new digital native tourists of 21st century. In
   this paper, we propose a novel solution that aims at assisting the
   visitor during an outdoor tour of a cultural site using the unique first
   person perspective of wearable cameras. In particular, the approach
   exploits computer vision techniques to retrieve the details by proposing
   a robust descriptor based on the covariance of local features. Using a
   lightweight wearable board, the solution can localize the user with
   respect to the 3D point cloud of the historical landmark and provide him
   with information about the details at which he is currently looking.
   Experimental results validate the method both in terms of accuracy and
   computational effort. Furthermore, user evaluation based on real-world
   experiments shows that the proposal is deemed effective in enriching a
   cultural experience.}},
DOI = {{10.3390/s16020237}},
ISSN = {{1424-8220}},
Unique-ID = {{ISI:000371787800087}},
}

@article{ ISI:000371667200009,
Author = {Danelakis, Antonios and Theoharis, Theoharis and Pratikakis, Ioannis},
Title = {{A robust spatio-temporal scheme for dynamic 3D facial expression
   retrieval}},
Journal = {{VISUAL COMPUTER}},
Year = {{2016}},
Volume = {{32}},
Number = {{2}},
Pages = {{257-269}},
Month = {{FEB}},
Note = {{3DOR Workshop, Strasbourg, FRANCE, APR 06, 2014}},
Abstract = {{The problem of facial expression recognition in dynamic sequences of 3D
   face scans has received a significant amount of attention in the recent
   past whereas the problem of retrieval in this type of data has not. A
   novel retrieval scheme for such data is introduced in this paper. It is
   the first spatio-temporal retrieval scheme ever used for retrieval in
   dynamic sequences of 3D face scans. The proposed scheme automatically
   detects specific facial landmarks and uses them to create a
   spatio-temporal descriptor. At first, geometric as well as topological
   information of the 3D face scans is captured by using the detected
   landmarks. In the sequel, the aforementioned spatial information is
   filtered by using wavelet transformation, resulting to our final
   spatio-temporal descriptor. Our descriptor is invariant to the number of
   the 3D face scans of a facial expression sequence. The proposed
   retrieval scheme exploits the Square of Euclidean distance in order to
   compare descriptors corresponding to different 3D facial sequences. A
   detailed evaluation of the introduced retrieval scheme is presented
   showing that it outperforms previous state-of-the-art retrieval schemes.
   Experiments have been conducted using the six prototypical expressions
   of the standard data set BU - 4DFE. Finally, a majority voting
   methodology based on the retrieval results is used to achieve
   unsupervised dynamic 3D facial expression recognition. The achieved
   classification accuracy outperforms the state-of-the-art supervised
   dynamic 3D facial expression recognition techniques.}},
DOI = {{10.1007/s00371-015-1142-7}},
ISSN = {{0178-2789}},
EISSN = {{1432-2315}},
Unique-ID = {{ISI:000371667200009}},
}

@article{ ISI:000370350100005,
Author = {Ma, Lixia and Zheng, Guang and Eitel, Jan U. H. and Moskal, L. Monika
   and He, Wei and Huang, Huabing},
Title = {{Improved Salient Feature-Based Approach for Automatically Separating
   Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar
   Point Cloud Data of Forest Canopies}},
Journal = {{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING}},
Year = {{2016}},
Volume = {{54}},
Number = {{2}},
Pages = {{679-696}},
Month = {{FEB}},
Abstract = {{Accurate separation of photosynthetic and nonphotosynthetic components
   in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a
   challenging but of key importance to understand the spatial distribution
   of the radiation regime, photosynthetic processes, and carbon and water
   exchanges of the forest canopy. The objective of this paper was to
   improve current methods for separating photosynthetic and
   nonphotosynthetic components in TLS data of forest canopies by adding
   two additional filters only based on its geometric information. By
   comparing the proposed approach with the eigenvalues plus color
   information-based method, we found that the proposed approach could
   effectively improve the overall producer's accuracy from 62.12\% to
   95.45\%, and the overall classification producer's accuracy would
   increase from 84.28\% to 97.80\% as the forest leaf area index (LAI)
   decreases from 4.15 to 3.13. In addition, variations in tree species had
   negligible effects on the final classification accuracy, as shown by the
   overall producer's accuracy for coniferous (93.09\%) and broadleaf
   (94.96\%) trees. To remove quantitatively the effects of the woody
   materials in a forest canopy for improving TLS-based LAI estimates, we
   also computed the ``woody-to-total area ratio{''} based on the
   classified linear class points from an individual tree. Automatic
   classification of the forest point cloud data set will facilitate the
   application of TLS on retrieving 3-D forest canopy structural
   parameters, including LAI and leaf and woody area ratios.}},
DOI = {{10.1109/TGRS.2015.2459716}},
ISSN = {{0196-2892}},
EISSN = {{1558-0644}},
ResearcherID-Numbers = {{Moskal, L. Monika/F-8715-2010
   }},
ORCID-Numbers = {{Moskal, L. Monika/0000-0003-1563-6506
   He, Wei/0000-0003-0779-2496}},
Unique-ID = {{ISI:000370350100005}},
}

@article{ ISI:000369879600001,
Author = {Moeini, Ali and Faez, Karim and Sadeghi, Hamid and Moeini, Hossein},
Title = {{2D facial expression recognition via 3D reconstruction and feature
   fusion}},
Journal = {{JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION}},
Year = {{2016}},
Volume = {{35}},
Pages = {{1-14}},
Month = {{FEB}},
Abstract = {{In this paper, a novel feature extraction method is proposed for facial
   expression recognition by extracting the feature from facial depth and
   3D mesh alongside texture. Accordingly, the 3D Facial Expression Generic
   Elastic Model (3D FE-GEM) method is used to reconstruct an
   expression-invariant 3D model from the human face. Then, the texture,
   depth and mesh are extracted from the reconstructed face model.
   Afterwards, the Local Binary Pattern (LBP), proposed 3D High-Low Local
   Binary Pattern (3DH-LLBP) and Local Normal Binary Patterns (LNBPs) are
   applied to texture, depth and mesh of the face, respectively, to extract
   the feature from 2D images. Finally, the final feature vectors are
   generated through feature fusion and are classified by the Support
   Vector Machine (SVM). Convincing results are acquired for facial
   expression recognition on the CK+, CK, JAFFE and Bosphorus image
   databases compared to several stateof-the-art methods. (C) 2015 Elsevier
   Inc. All rights reserved.}},
DOI = {{10.1016/j.jvcir.2015.11.006}},
ISSN = {{1047-3203}},
EISSN = {{1095-9076}},
ResearcherID-Numbers = {{faez, karim/K-5117-2019}},
ORCID-Numbers = {{faez, karim/0000-0002-1159-4866}},
Unique-ID = {{ISI:000369879600001}},
}

@inproceedings{ ISI:000406771300100,
Author = {Pang, Guan and Neumann, Ulrich},
Book-Group-Author = {{IEEE}},
Title = {{3D Point Cloud Object Detection with Multi-View Convolutional Neural
   Network}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{585-590}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Efficient detection of three dimensional (3D) objects in point clouds is
   a challenging problem. Performing 3D descriptor matching or 3D
   scanning-window search with detector are both time-consuming due to the
   3-dimensional complexity. One solution is to project 3D point cloud into
   2D images and thus transform the 3D detection problem into 2D space, but
   projection at multiple viewpoints and rotations produce a large amount
   of 2D detection tasks, which limit the performance and complexity of the
   2D detection algorithm choice. We propose to use convolutional neural
   network (CNN) for the 2D detection task, because it can handle all
   viewpoints and rotations for the same class of object together, as well
   as predicting multiple classes of objects with the same network, without
   the need for individual detector for each object class. We further
   improve the detection efficiency by concatenating two extra levels of
   early rejection networks with binary outputs before the multi-class
   detection network. Experiments show that our method has competitive
   overall performance with at least one-order of magnitude speedup
   comparing with latest 3D point cloud detection methods.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771300100}},
}

@inproceedings{ ISI:000406771301004,
Author = {Tian, Guifen and Mori, Tatusya and Okuda, Yuji},
Book-Group-Author = {{IEEE}},
Title = {{Spoofing Detection for Embedded Face Recognition System using A Low Cost
   Stereo Camera}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{1017-1022}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Spoofing detection is essential for practical face recognition system.
   Based on the fact that genuine face has special geometric curvatures
   across surface, this paper brings forward an ultra-fast yet accurate
   spoofing detection approach using a low-cost stereo camera. To obtain
   curvatures, the three dimensional shapes of selected facial landmarks
   are analyzed, by fitting point cloud around each landmark to a specific
   partial face surface. Spoofing detection is then performed by evaluating
   curvatures of each landmark and integrating them together. Experiments
   verify that the approach is able to detect spoofed faces in printed
   photographs without or with various bending at FAR equal to 0.00\%.
   Meanwhile, genuine faces have a trivial opportunity to be falsely
   rejected: FRR is 0.59\% for near frontal faces and less than 5\% for
   faces with large varying poses. Detection time is 51 milliseconds when
   executed on a single processor {[}1] running at a clock frequency of
   266M Hz, this makes the detection very suitable for embedded face
   recognition system.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771301004}},
}

@inproceedings{ ISI:000406771302041,
Author = {Zhen, Qingkai and Huang, Di and Wang, Yunhong and Drira, Hassen and Ben
   Amor, Boulbaba and Daoudi, Mohamed},
Book-Group-Author = {{IEEE}},
Title = {{Magnifying Subtle Facial Motions for 4D Expression Recognition}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{2252-2257}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{In this paper, we propose an effective approach for automatic 4D Facial
   Expression Recognition (FER). The flow of 3D facial scans is first
   modeled to capture spatial deformations based on the recently-developed
   Riemannian approach, namely Dense Scalar Fields (DSF), where
   registration and comparison of neighboring 3D face frames are jointly
   led. The deformations are then fed into a temporal filtering based
   magnification step to amplify the slight facial actions over time. The
   proposed method allows revealing subtle (hidden) deformations which
   enhances the performance in classification. We evaluate our approach on
   the BU-4DFE dataset, and the state-of-art accuracy up to 94.18\% is
   achieved, which is superior to the top one so far reported, clearly
   demonstrating its effectiveness.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018
   }},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305
   Drira, Hassen/0000-0003-1052-4353}},
Unique-ID = {{ISI:000406771302041}},
}

@inproceedings{ ISI:000406771302059,
Author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni,
   Gerard},
Book-Group-Author = {{IEEE}},
Title = {{Expression Invariant 3D Face Modeling from an RGB-D Video}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{2362-2367}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{We aim to reconstruct an accurate neutral 3D face model from an RGB-D
   video in the presence of extreme expression changes. Since each depth
   frame, taken by a low-cost sensor, is noisy, point clouds from multiple
   frames can be registered and aggregated to build an accurate 3D model.
   However, direct aggregation of multiple data produces erroneous results
   in natural interaction (e.g., talking and showing expressions). We
   propose to analyze facial expression from an RGB frame and neutralize
   the corresponding 3D point cloud if needed. We first estimate the
   person's expression by fitting blend-shape coefficients using 2D facial
   landmarks for each frame and calculate an expression deformity
   (expression score). With the estimated expression score, we determine
   whether an input face is neutral or non-neutral. If the face is
   non-neutral, we proceed to neutralize the expression of the 3D point
   cloud in that frame. To neutralize the 3D point cloud of a face, we
   deform our generic 3D face model by applying the estimated blendshape
   coefficients, find displacement vectors from the deformed generic face
   to a neutral generic face, and apply the displacement vectors to the
   input 3D point cloud. After preprocessing frames in a video, we rank
   frames based on the expression scores and register the ranked frames
   into a single 3D model. Our system produces a neutral 3D face model in
   the presence of extreme expression changes even when neutral faces do
   not exist in the video.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Unique-ID = {{ISI:000406771302059}},
}

@inproceedings{ ISI:000406538600016,
Author = {Shao, Xiaohu and Cheng, Cheng and Liu, Yanfei and Zhou, Xiangdong},
Editor = {{Tan, T and Li, X and Chen, X and Zhou, J and Yang, J and Cheng, H}},
Title = {{Pose-Invariant Face Recognition Based on a Flexible Camera Calibration}},
Booktitle = {{PATTERN RECOGNITION (CCPR 2016), PT I}},
Series = {{Communications in Computer and Information Science}},
Year = {{2016}},
Volume = {{662}},
Pages = {{191-200}},
Note = {{7th Chinese Conference on Pattern Recognition (CCPR), Chengdu, PEOPLES R
   CHINA, NOV 05-07, 2016}},
Abstract = {{In this paper, we present a flexible camera calibration for pose
   normalization to accomplish a pose-invariant face recognition. The
   accuracy of calibration can be easily influenced by errors of landmark
   detection or various shapes of different faces and expressions. By
   jointly using RANSAC and facial unique characters, we explore a flexible
   calibration method to achieve a more accurate camera calibration and
   pose normalization for face images. Our proposed method is able to
   eliminate noisy facial landmarks and retain the ones which best match
   the undeformable 3D face model. The experimental results show that our
   method improves the accuracy of pose-invariant face recognition,
   especially for the faces with unsatisfied landmark detection, variant
   shapes, and exaggerated expressions.}},
DOI = {{10.1007/978-981-10-3002-4\_16}},
ISSN = {{1865-0929}},
ISBN = {{978-981-10-3002-4; 978-981-10-3001-7}},
Unique-ID = {{ISI:000406538600016}},
}

@inproceedings{ ISI:000406056300054,
Author = {An, Shu and Ruan, Qiuqi},
Editor = {{Baozong, Y and Qiuqi, R and Yao, Z and Gaoyun AN}},
Title = {{3D Facial Expression Recognition Algorithm using Local Threshold Binary
   Pattern and Histogram of Oriented Gradient}},
Booktitle = {{PROCEEDINGS OF 2016 IEEE 13TH INTERNATIONAL CONFERENCE ON SIGNAL
   PROCESSING (ICSP 2016)}},
Series = {{International Conference on Signal Processing}},
Year = {{2016}},
Pages = {{265-270}},
Note = {{13th IEEE International Conference on Signal Processing (ICSP), Chengdu,
   PEOPLES R CHINA, NOV 06-10, 2016}},
Organization = {{IEEE; Inst Engn \& Technol; Union Radio Sci Int; Chinese Inst Elect;
   Beijing Jiaotong Univ; Int Conf Signal Proc; IEEE Beijing Sect; IET
   Beijing Local Network; Natl Nat Sci Fdn China; CIE Signal Proc Soc; IEEE
   Signal Proc Soc Beijing Chapter; IEEE Comp Soc Beijing Chapter; Japan
   China Sci \& Technol Exchange Assoc; Shenzhen Univ, Intelligent Informat
   Inst; CIC Commun \& Signal Proc Soc; Univ Minist Educ China, Program
   Innovat Res Team}},
Abstract = {{Facial expression which carries rich information of body behavior is the
   leading carrier of human affective and the symbol of intelligence. The
   main purpose of this paper is to recognize 3D human facial expression.
   The research in this paper includes the expression feature extraction
   algorithm and fusion with different kinds of feature. To contain more
   local texture feature information, we proposed a new feature of 3D
   facial expression named Local Threshold Binary Pattern (LTBP) which
   based on Local Binary Pattern (LBP). We calculate the difference of gray
   value standard between neighboring pixels and the center pixel as a
   threshold to binary instead of the traditional LBP operation which only
   comparison of size between neighboring pixels and the center pixel.
   After we get the LTBP feature, we fuse the LTBP and HOG (Histogram of
   Oriented Gradient) features to get multi-feature fusion for 3D facial
   expression recognition. Our algorithm of 3D facial expression
   recognition comprises three steps: (1) extracting two sets of feature
   vectors and establishing the correlation criterion function between the
   two sets of feature vectors; (2) solving the two sets canonical
   projective vectors and extracting their canonical correlation features
   by the framework of canonical correlation analysis algorithm; (3) doing
   feature fusion for classification by using proposed strategy. We have
   performed comprehensive experiments on the BU-3DFE database which is
   presently the largest available 3D face database. We have achieved
   verification rates of more than 90\% for the 3D facial expression
   recognition.}},
ISSN = {{2164-5221}},
ISBN = {{978-1-5090-1345-6}},
Unique-ID = {{ISI:000406056300054}},
}

@inproceedings{ ISI:000401716400022,
Author = {Naveen, S. and Rugmini, K. P. and Moni, R. S.},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Reconstruction by Pose Correction, Patch Cloning and Texture
   Wrapping}},
Booktitle = {{PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION
   SYSTEMS AND NETWORKS (COMNET)}},
Series = {{International Conference on Communication Systems and Networks}},
Year = {{2016}},
Pages = {{112-116}},
Note = {{International Conference on Communication Systems and Networks (ComNet),
   Trivandrum, INDIA, JUL 21-23, 2016}},
Organization = {{IEEE}},
Abstract = {{Face is being considered as one of the most commonly used biometric
   modality. The inaccuracy in two dimensional face recognition systems is
   mainly due to pose variations, occlusions, illumination etc. Among this,
   changes in illumination condition do not affect 3D face recognition
   systems. But pose variation drastically changes the appearance of face
   images. To solve the problems with depth map and texture images
   corrupted by head pose variations and the occlusions generated due to
   these pose variations, a reconstruction method is proposed which consist
   of three stages. In the first stage, the pose correction is done by
   Iterative Closest Point (ICP) algorithm and in the second stage the
   occluded region of the face is reconstructed by a resurfacing method
   called patch cloning. It is followed by the wrapping of reconstructed
   depth map by its texture to generate a 3D model. The statistical error
   between the original face and the reconstructed face is also evaluated.
   In this work, facial symmetry is used as a prior knowledge. Experiments
   are done with the FRAV3D database.}},
ISSN = {{2155-2487}},
ISBN = {{978-1-5090-3349-2}},
Unique-ID = {{ISI:000401716400022}},
}

@inproceedings{ ISI:000401510000148,
Author = {Amin, Rafiul and Shams, A. Farhan and Rahman, S. M. Mahbubur and
   Hatzinakos, Dimitrios},
Book-Group-Author = {{IEEE}},
Title = {{Evaluation of Discrimination Power of Facial Parts from 3D Point Cloud
   Data}},
Booktitle = {{2016 9TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING
   (ICECE)}},
Series = {{International Conference on Computer and Electrical Engineering ICCEE}},
Year = {{2016}},
Pages = {{602-605}},
Note = {{9th International Conference on Electrical and Computer Engineering
   (ICECE), Dhaka, BANGLADESH, DEC 20-22, 2016}},
Organization = {{Bangladesh Univ Engn \& Technol, Dept Elect \& Elect Engn; Inst Elect \&
   Elect Engineers; Energypac; PARADISE Cables ltd; BTCL; Summit Communicat
   Ltd; Dhaka Power Distribut Co Ltd}},
Abstract = {{Feature selection from facial regions is a well-known approach to
   increase the performance of 2D image-based face recognition systems. In
   case of 3D modality, the approach of region-based feature selection for
   face recognition is relatively new. In this context, this paper presents
   an approach to evaluate the discrimination power of different regions of
   a 3D facial surface for its potential use in face recognition systems.
   We propose the use of weighted average of unit normal vector on the
   facial surface as the feature for region-based face recognition from 3D
   point cloud data (PCD). The iterative closest point algorithm is
   employed for the registration of segmented regions of facial point
   clouds. A metric based on angular distance between normals is introduced
   to indicate the similarity between two surfaces of same facial region.
   Finally, the intra class correlation based discrimination score is
   formulated to find out the key facial regions such as the eyes, nose,
   and mouth that are significant while recognizing a person with facial
   surface PCD.}},
ISBN = {{978-1-5090-2963-1}},
ResearcherID-Numbers = {{Amin, Rafiul/L-8633-2019}},
Unique-ID = {{ISI:000401510000148}},
}

@inproceedings{ ISI:000400012304105,
Author = {Bolkart, Timo and Wuhrer, Stefanie},
Book-Group-Author = {{IEEE}},
Title = {{A Robust Multilinear Model Learning Framework for 3D Faces}},
Booktitle = {{2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2016}},
Pages = {{4911-4919}},
Note = {{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Seattle, WA, JUN 27-30, 2016}},
Organization = {{IEEE Comp Soc; Comp Vis Fdn}},
Abstract = {{Multilinear models are widely used to represent the statistical
   variations of 3D human faces as they decouple shape changes due to
   identity and expression. Existing methods to learn a multilinear face
   model degrade if not every person is captured in every expression, if
   face scans are noisy or partially occluded, if expressions are
   erroneously labeled, or if the vertex correspondence is inaccurate.
   These limitations impose requirements on the training data that
   disqualify large amounts of available 3D face data from being usable to
   learn a multilinear model. To overcome this, we introduce the first
   framework to robustly learn a multilinear model from 3D face databases
   with missing data, corrupt data, wrong semantic correspondence, and
   inaccurate vertex correspondence. To achieve this robustness to
   erroneous training data, our framework jointly learns a multilinear
   model and fixes the data. We evaluate our framework on two publicly
   available 3D face databases, and show that our framework achieves a data
   completion accuracy that is comparable to state-of-the-art tensor
   completion methods. Our method reconstructs corrupt data more accurately
   than state-of-the-art methods, and improves the quality of the learned
   model significantly for erroneously labeled expressions.}},
DOI = {{10.1109/CVPR.2016.531}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-8851-1}},
Unique-ID = {{ISI:000400012304105}},
}

@inproceedings{ ISI:000400688200019,
Author = {Starczewski, Janusz T. and Pabiasz, Sebastian and Vladymyrska, Natalia
   and Marvuglia, Antonino and Napoli, Christian and Wozniak, Marcin},
Editor = {{Rutkowski, L and Korytkowski, M and Scherer, R and Tadeusiewicz, R and Zadeh, LA and Zurada, JM}},
Title = {{Self Organizing Maps for 3D Face Understanding}},
Booktitle = {{ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING, (ICAISC 2016), PT II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9693}},
Pages = {{210-217}},
Note = {{15th International Conference on Artificial Intelligence and Soft
   Computing (ICAISC), Zakopane, POLAND, JUN 12-16, 2016}},
Organization = {{Polish Neural Network Soc; Univ Social Sci; Czestochowa Univ Technol,
   Inst Computat Intelligence}},
Abstract = {{Landmarks are unique points that can be located on every face. Facial
   landmarks typically recognized by people are correlated with
   anthropomorphic points. Our purpose is to employ in 3D face recognition
   such landmarks that are easy to interpret. Face understanding is
   construed as identification of face characteristic points with automatic
   labeling of them. In this paper, we apply methods based on Self
   Organizing Maps to understand 3D faces.}},
DOI = {{10.1007/978-3-319-39384-1\_19}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-39384-1}},
ResearcherID-Numbers = {{Wozniak, Marcin/L-6640-2013
   Marvuglia, Antonino/J-2595-2019
   }},
ORCID-Numbers = {{Wozniak, Marcin/0000-0002-9073-5347
   Napoli, Christian/0000-0002-3336-5853
   Starczewski, Janusz/0000-0003-4694-7868}},
Unique-ID = {{ISI:000400688200019}},
}

@inproceedings{ ISI:000400617600065,
Author = {Yin, Jing and Yang, XiaoFang},
Book-Group-Author = {{IEEE}},
Title = {{3D FACIAL RECONSTRUCTION OF BASED ON OPENCV AND DIRECTX}},
Booktitle = {{PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND
   IMAGE PROCESSING (ICALIP)}},
Year = {{2016}},
Pages = {{341-344}},
Note = {{5th International Conference on Audio, Language and Image Processing
   (ICALIP), Shanghai, PEOPLES R CHINA, JUL 11-12, 2016}},
Organization = {{IEEE; IET; IEEE CIS Shanghai Chapter; IET Shanghai Local Network;
   Shanghai Univ; Natl Sci \& Technol Near Surface Detect Lab; Shanghai
   Univ Engn Sci; Tongji Univ; Fudan Univ; Shanghai Jiao Tong Univ}},
Abstract = {{Face as a biometric identification in computer vision is an important
   medium, in areas such as video surveillance, animation games, security
   anti-terrorist has a very wide range of applications, creating vivid,
   strong visibility of 3d face model, now has become a challenging in the
   field of computer vision is one of the important topics. At first, this
   paper used the zhongxing-micro ZC301P cameras to build a binocular
   stereo vision system for recording images. After the camera calibration
   and binocular calibration, the three-dimensional data of facial images
   were extracted using the functions of OpenCV computer vision library,
   and then 3d face model were reconstructed preliminary by DirectX.
   According the reconstruction process, the human face three-dimensional
   reconstruction software was designed and developed. The paper laid the
   foundation for the next step work that is to obtain more clear and
   strong visibility of 3d face.}},
ISBN = {{978-1-5090-0654-0}},
Unique-ID = {{ISI:000400617600065}},
}

@inproceedings{ ISI:000395499500098,
Author = {Bobkowska, Katarzyna and Janowski, Artur and Przyborski, Marek and
   Szulwic, Jakub},
Book-Group-Author = {{SGEM}},
Title = {{A NEW METHOD OF PERSONS IDENTIFICATION BASED ON COMPARATIVE ANALYSIS OF
   3D FACE MODELS}},
Booktitle = {{INFORMATICS, GEOINFORMATICS AND REMOTE SENSING CONFERENCE PROCEEDINGS,
   SGEM 2016, VOL II}},
Series = {{International Multidisciplinary Scientific GeoConference-SGEM}},
Year = {{2016}},
Pages = {{767+}},
Note = {{16th International Multidisciplinary Scientific Geoconference (SGEM
   2016), Albena, BULGARIA, JUN 30-JUL 06, 2016}},
Organization = {{Bulgarian Acad Sci; Acad Sci Czech Republ; Latvian Acad Sci; Polish Acad
   Sci; Russian Acad Sci; Serbian Acad Sci \& Arts; Slovak Acad Sci; Natl
   Acad Sci Ukraine; Inst Water Problem \& Hydropower NAS KR; Natl Acad Sci
   Armenia; Sci Council Japan; World Acad Sci; European Acad Sci Arts \&
   Lett; Acad Sci Moldova; Montenegrin Acad Sci \& Arts; Croatian Acad Sci
   \& Arts; Georgian Natl Acad Sci; Acad Fine Arts \& Design Bratislava;
   Turkish Acad Sci; Bulgarian Ind Assoc; Bulgarian Minist Environm \&
   Water}},
Abstract = {{The article presents the use of modern close range photogrammetry for
   possessing highly accurate 3D models of the human face (including the
   ears). Modern methods used to obtain precise data describing the
   construction of a human face, and even the whole human body, should
   allow to get finished measurement material in a very short time. Those
   features belong to the optical scanning technology. Comparative analysis
   of models of the human face has been made (created from the cloud of
   points obtained from optical scanner) for the same person as well as for
   two different persons. Among other things, the parameters describing the
   similarity of a human face (in particular, the similarity of the human
   ear) based primarily on the analysis of the differences between the
   points of the model (comparison of several hundred thousand points on
   the model) were determined. Ultimately, these parameters can be used to
   identify persons. Considering the great opportunities presented methods,
   other potential applications have been presented.}},
ISSN = {{1314-2704}},
ISBN = {{978-619-7105-59-9}},
ResearcherID-Numbers = {{Szulwic, Jakub/B-7032-2014
   Przyborski, Marek/R-3314-2017
   }},
ORCID-Numbers = {{Szulwic, Jakub/0000-0003-2573-4492
   Przyborski, Marek/0000-0001-5354-1407
   Bobkowska, Katarzyna/0000-0003-4968-3407}},
Unique-ID = {{ISI:000395499500098}},
}

@inproceedings{ ISI:000390287300035,
Author = {Pan, Zhenglin and Polceanu, Mihai and Lisetti, Christine},
Editor = {{Traum, D and Swartout, W and Khooshabeh, P and Kopp, S and Scherer, S and Leuski, A}},
Title = {{On Constrained Local Model Feature Normalization for Facial Expression
   Recognition}},
Booktitle = {{INTELLIGENT VIRTUAL AGENTS, IVA 2016}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{10011}},
Pages = {{369-372}},
Note = {{16th International Conference on Intelligent Virtual Agents (IVA), Los
   Angeles, CA, SEP 20-23, 2016}},
Organization = {{Alelo; Springer; Univ So Calif, Inst Creat Technologies}},
Abstract = {{Real time user independent facial expression recognition is important
   for virtual agents but challenging. However, since in real time
   recognition users are not necessarily presenting all the emotions, some
   proposed methods are not applicable. In this paper, we present a new
   approach that instead of using the traditional base face normalization
   on whole face shapes, performs normalization on the point cloud of each
   landmark. The result shows that our method outperforms the other two
   when the user input does not contain all six universal emotions.}},
DOI = {{10.1007/978-3-319-47665-0\_35}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-47665-0; 978-3-319-47664-3}},
Unique-ID = {{ISI:000390287300035}},
}

@inproceedings{ ISI:000392743800047,
Author = {Boehm, J. and Bredif, M. and Gierlinger, T. and Kraemer, M. and
   Lindenbergh, R. and Liu, K. and Michel, F. and Sirmacek, B.},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND
   IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{301-307}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Current 3D data capturing as implemented on for example airborne or
   mobile laser scanning systems is able to efficiently sample the surface
   of a city by billions of unselective points during one working day. What
   is still difficult is to extract and visualize meaningful information
   hidden in these point clouds with the same efficiency. This is where the
   FP7 IQmulus project enters the scene. IQmulus is an interactive facility
   for processing and visualizing big spatial data. In this study the
   potential of IQmulus is demonstrated on a laser mobile mapping point
   cloud of 1 billion points sampling similar to 10 km of street
   environment in Toulouse, France. After the data is uploaded to the
   IQmulus Hadoop Distributed File System, a workflow is defined by the
   user consisting of retiling the data followed by a PCA driven local
   dimensionality analysis, which runs efficiently on the IQmulus cloud
   facility using a Spark implementation. Points scattering in 3 directions
   are clustered in the tree class, and are separated next into individual
   trees. Five hours of processing at the 12 node computing cluster results
   in the automatic identification of 4000+ urban trees. Visualization of
   the results in the IQmulus fat client helps users to appreciate the
   results, and developers to identify remaining flaws in the processing
   workflow.}},
DOI = {{10.5194/isprsarchives-XLI-B3-301-2016}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Bredif, Mathieu/T-3029-2018
   Boehm, Jan/K-2336-2012
   }},
ORCID-Numbers = {{Bredif, Mathieu/0000-0003-0228-1232
   Boehm, Jan/0000-0003-2190-0449
   Lindenbergh, Roderik/0000-0001-8655-5266}},
Unique-ID = {{ISI:000392743800047}},
}

@inproceedings{ ISI:000392743800092,
Author = {Kadamen, Jayren and Sithole, George},
Editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
Title = {{AUTOMATICALLY DETERMINING SCALE WITHIN UNSTRUCTU RED POINT CLOUDS}},
Booktitle = {{XXIII ISPRS CONGRESS, COMMISSION III}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B3}},
Pages = {{617-624}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Three dimensional models obtained from imagery have an arbitrary scale
   and therefore have to be scaled. Automatically scaling these models
   requires the detection of objects in these models which can be
   computationally intensive. Real-time object detection may pose problems
   for applications such as indoor navigation. This investigation poses the
   idea that relational cues, specifically height ratios, within indoor
   environments may offer an easier means to obtain scales for models
   created using imagery. The investigation aimed to show two things, (a)
   that the size of objects, especially the height off ground is consistent
   within an environment, and (b) that based on this consistency, objects
   can be identified and their general size used to scale a model. To test
   the idea a hypothesis is first tested on a terrestrial lidar scan of an
   indoor environment. Later as a proof of concept the same test is applied
   to a model created using imagery. The most notable finding was that the
   detection of objects can be more readily done by studying the ratio
   between the dimensions of objects that have their dimensions defined by
   human physiology. For example the dimensions of desks and chairs are
   related to the height of an average person. In the test, the difference
   between generalised and actual dimensions of objects were assessed. A
   maximum difference of 3.96\% (2.93cm) was observed from automated
   scaling. By analysing the ratio between the heights (distance from the
   floor) of the tops of objects in a room, identification was also
   achieved.}},
DOI = {{10.5194/isprsarchives-XLI-B3-617-2016}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000392743800092}},
}

@inproceedings{ ISI:000392739800107,
Author = {Zhou, K. and Gorte, B. and Zlatanova, S.},
Editor = {{Halounova, L and Safar, V and Remondino, F and Hodac, J and Pavelka, K and Shortis, M and Rinaudo, F and Scaioni, M and Boehm, J and RiekeZapp, D}},
Title = {{EXPLORING REGULARITIES FOR IMPROVING FACADE RECONSTRUCTION FROM POINT
   CLOUDS}},
Booktitle = {{XXIII ISPRS Congress, Commission V}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2016}},
Volume = {{41}},
Number = {{B5}},
Pages = {{749-755}},
Note = {{23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
   Prague, CZECH REPUBLIC, JUL 12-19, 2016}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{(Semi)-automatic facade reconstruction from terrestrial LiDAR point
   clouds is often affected by both quality of point cloud itself and
   imperfectness of object recognition algorithms. In this paper, we employ
   regularities, which exist on facades, to mitigate these problems. For
   example, doors, windows and balconies often have orthogonal and parallel
   boundaries. Many windows are constructed with the same shape. They may
   be arranged at the same lines and distance intervals, so do different
   windows. By identifying regularities among objects with relatively poor
   quality, these can be applied to calibrate the objects and improve their
   quality. The paper focuses on the regularities among the windows, which
   is the majority of objects on the wall. Regularities are classified into
   three categories: within an individual window, among similar windows and
   among different windows. Nine cases are specified as a reference for
   exploration. A hierarchical clustering method is employed to identify
   and apply regularities in a feature space, where regularities can be
   identified from clusters. To find the corresponding features in the nine
   cases of regularities, two phases are distinguished for similar and
   different windows. In the first phase, ICP (iterative closest points) is
   used to identify groups of similar windows. The registered points and a
   number of transformation matrices are used to identify and apply
   regularities among similar windows. In the second phase, features are
   extracted from the boundaries of the different windows. When applying
   regularities by relocating windows, the connections, called chains,
   established among the similar windows in the first phase are preserved.
   To test the performance of the algorithms, two datasets from terrestrial
   LiDAR point clouds are used. Both show good effects on the reconstructed
   model, while still matching with original point cloud, preventing over
   or under-regularization.}},
DOI = {{10.5194/isprsarchives-XLI-B5-749-2016}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Gorte, Bernardus/0000-0001-8953-6394}},
Unique-ID = {{ISI:000392739800107}},
}

@inproceedings{ ISI:000391534900063,
Author = {Yu, Xun and Gao, Yongsheng and Zhou, Jun},
Editor = {{Liew, AWC and Lovell, B and Fookes, C and Zhou, J and Gao, Y and Blumenstein, M and Wang, Z}},
Title = {{Boosting Radial Strings for 3D Face Recognition with Expressions and
   Occlusions}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND
   APPLICATIONS (DICTA)}},
Year = {{2016}},
Pages = {{436-441}},
Note = {{International Conference on Digital Image Computing - Techniques and
   Applications (DICTA), Gold Coast, AUSTRALIA, NOV 30-DEC 02, 2016}},
Organization = {{Australian Govt, Dept Defence, Defence Sci \& Technol Grp; IAPR; Canon
   Informat Syst Res Australia; IEEE; Griffith Univ; APRS}},
Abstract = {{In this paper, we present a new radial string representation and
   matching approach for 3D face recognition under expression variations
   and partial occlusions. The radial strings are an indexed collection of
   strings emanating from the nose tip of a face scan. The matching between
   two radial strings is conducted through a dynamic programming process,
   in which a partial matching mechanism is established to find those
   unoccluded substrings effectively. Moreover, the most discriminative and
   stable radial strings are selected optimally by the well-known AdaBoost
   algorithm to achieve a composite classifier for 3D face recognition
   under facial expression changes. Experimental results on the GavabDB and
   the Bosphorus databases show that the proposed approach achieves
   promising results for human face recognition with expressions and
   occlusions.}},
ISBN = {{978-1-5090-2896-2}},
Unique-ID = {{ISI:000391534900063}},
}

@inproceedings{ ISI:000391534900098,
Author = {Gilani, Syed Zulqarnain and Mian, Ajmal},
Editor = {{Liew, AWC and Lovell, B and Fookes, C and Zhou, J and Gao, Y and Blumenstein, M and Wang, Z}},
Title = {{Towards Large-scale 3D Face Recognition}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND
   APPLICATIONS (DICTA)}},
Year = {{2016}},
Pages = {{682-689}},
Note = {{International Conference on Digital Image Computing - Techniques and
   Applications (DICTA), Gold Coast, AUSTRALIA, NOV 30-DEC 02, 2016}},
Organization = {{Australian Govt, Dept Defence, Defence Sci \& Technol Grp; IAPR; Canon
   Informat Syst Res Australia; IEEE; Griffith Univ; APRS}},
Abstract = {{3D face recognition holds great promise in achieving robustness to pose,
   expressions and occlusions. However, 3D face recognition algorithms are
   still far behind their 2D counterparts due to the lack of large-scale
   datasets. We present a model based algorithm for 3D face recognition and
   test its performance by combining two large public datasets of 3D faces.
   We propose a Fully Convolutional Deep Network (FCDN) to initialize our
   algorithm. Reliable seed points are then extracted from each 3D face by
   evolving level set curves with a single curvature dependent adaptive
   speed function. We then establish dense correspondence between the faces
   in the training set by matching the surface around the seed points on a
   template face to the ones on the target faces. A morphable model is then
   fitted to probe faces and face recognition is performed by matching the
   parameters of the probe and gallery faces. Our algorithm achieves state
   of the art landmark localization results. Face recognition results on
   the combined FRGCv2 and Bosphorus datasets show that our method is
   affective in recognizing query faces with real world variations in pose
   and expression, and with occlusion and missing data despite a huge
   gallery. Comparing results of individual and combined datasets show that
   the recognition accuracy drops when the size of the gallery increases.}},
ISBN = {{978-1-5090-2896-2}},
Unique-ID = {{ISI:000391534900098}},
}

@inproceedings{ ISI:000390841700083,
Author = {Torkhani, Ghada and Ladgham, Anis and Mansouri, Mohamed Nejib and Sakly,
   Anis},
Book-Group-Author = {{IEEE}},
Title = {{Gabor-SVM Applied to 3D-2D Deformed Mesh Model}},
Booktitle = {{2016 2ND INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)}},
Year = {{2016}},
Pages = {{447-452}},
Note = {{2nd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP), Monastir, TUNISIA, MAR 21-23, 2016}},
Organization = {{IEEE; IEEE Tunisia Sect; ATMS Lab; ATSI; IEEE Explore; EMB; ENIS Sch;
   Telecom Paris; Supelec; CESBIO; Telecom SudParis; ENIT; Univ Paris Sud;
   IEEE Signal Proc Soc Tunisia Chapter; ISAAM Inst; Minist Higher Educ
   Res; IEEE EMP Tunisia Chapter; Novartis Company}},
Abstract = {{We propose a robust method for 3D face recognition using 3D to 2D
   modeling and facial curvatures detection. The 3D2D algorithm permits to
   transform 3D images into 3D triangular mesh, then the mesh model is
   deformed and fitted to the 2D space in order to obtain a 2D smoother
   mesh. Then, we apply Gabor wavelets to the deformed model in order to
   exploit surface curves in the detection of salient face features. The
   classification of the final Gabor facial model is performed using the
   support vector machines (SVM). To demonstrate the quality of our
   technique, we give some experiments using the 3D AJMAL faces database.
   The experimental results prove that the proposed method is able to give
   a good recognition quality and a high accuracy rate.}},
ISBN = {{978-1-4673-8526-8}},
Unique-ID = {{ISI:000390841700083}},
}

@inproceedings{ ISI:000390782003007,
Author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni,
   Gerard},
Book-Group-Author = {{IEEE}},
Title = {{ACCUTE 3D FACE MODELING AND RECOGNITION FROM RGB-D STREAM IN THE
   PRESENCE OF LARGE POSE CNGES}},
Booktitle = {{2016 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2016}},
Pages = {{3011-3015}},
Note = {{23rd IEEE International Conference on Image Processing (ICIP), Phoenix,
   AZ, SEP 25-28, 2016}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers, Signal
   Proc Soc}},
Abstract = {{We propose a 3D face modeling and recognition system using an RGB-D
   stream in the presence of large pose changes. In the previous work, all
   facial data points are registered with a reference to improve the
   accuracy of 3D face model from a low-resolution depth sequence. This
   registration often fails when applied to non-frontal faces. It causes
   inaccurate 3D face models and poor performance of matching. We address
   this problem by pre-aligning each input face ('frontalization') before
   the registration, which avoids registration failures. For each frame,
   our method estimates the 3D face pose, assesses the quality of data,
   segments the facial region, frontalizes it. and performs an accurate
   registration with the previous 3D model. The 3D 3D recognition system
   using accurate 3D models from our method outperforms other face
   recognition systems and shows 100\% rank 1 recognition accuracy on a
   dataset with 30 subjects.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4673-9961-6}},
Unique-ID = {{ISI:000390782003007}},
}

@inproceedings{ ISI:000390782003008,
Author = {Yu, Xun and Gao, Yongsheng and Zhou, Jun},
Book-Group-Author = {{IEEE}},
Title = {{3D FACE RECOGNITION UNDER PARTIAL OCCLUSIONS USING RADIAL STRINGS}},
Booktitle = {{2016 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2016}},
Pages = {{3016-3020}},
Note = {{23rd IEEE International Conference on Image Processing (ICIP), Phoenix,
   AZ, SEP 25-28, 2016}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers, Signal
   Proc Soc}},
Abstract = {{3D face recognition with partial occlusions is a highly challenging
   problem. In this paper, we propose a novel radial string representation
   and matching approach to recognize 3D facial scans in the presence of
   partial occlusions. Here we encode 3D facial surfaces into an indexed
   collection of radial strings emanating from the nosetips and Dynamic
   Programming (DP) is then used to measure the similarity between two
   radial strings. In order to address the recognition problems with
   partial occlusions, a partial matching mechanism is established in our
   approach that effectively eliminates those occluded parts and finds the
   most discriminative parts during the matching process. Experimental
   results on the Bosphorus database demonstrate that the proposed approach
   yields superior performance on partially occluded data.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4673-9961-6}},
ORCID-Numbers = {{Zhou, Jun/0000-0001-5822-8233
   Gao, Yongsheng/0000-0002-5382-5351}},
Unique-ID = {{ISI:000390782003008}},
}

@inproceedings{ ISI:000390841200018,
Author = {Zhang, Jinjin and Huang, Di and Wang, Yunhong and Sun, Jia},
Editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun, J}},
Title = {{Lock3DFace: A Large-Scale Database of Low-Cost Kinect 3D Faces}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2016}},
Note = {{9th International Conference on Biometrics (ICB), Halmstad Univ,
   Halmstad, SWEDEN, JUN 13-16, 2016}},
Organization = {{IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB;
   Safran Ident \& Secur; EU Horizon 2020 Project IDENT; Speed Ident AB;
   Cognitec}},
Abstract = {{In this paper, we present a large-scale database consisting of low cost
   Kinect 3D face videos, namely Lock3DFace, for 3D face analysis,
   particularly for 3D Face Recognition (FR). To the best of our knowledge,
   Lock3DFace is currently the largest low cost 3D face database for public
   academic use. The 3D samples are highly noisy and contain a diversity of
   variations in expression, pose, occlusion, time lapse, and their
   corresponding texture and near infrared channels have changes in
   lighting condition and radiation intensity, allowing for evaluating FR
   methods in complex situations. Furthermore, based on Lock3DFace, we
   design the standard experimental protocol for low-cost 3D FR, and give
   the baseline performance of individual subsets belonging to different
   scenarios for fair comparison in the future.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-5090-1869-7}},
Unique-ID = {{ISI:000390841200018}},
}

@inproceedings{ ISI:000388373401159,
Author = {Hu, Xiaoping and Wang, Ying and Zhu, Feiyun and Pan, Chunhong},
Book-Group-Author = {{IEEE}},
Title = {{LEARNING-BASED FULLY 3D FACE RECONSTRUCTION FROM A SINGLE IMAGE}},
Booktitle = {{2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS}},
Series = {{International Conference on Acoustics Speech and Signal Processing
   ICASSP}},
Year = {{2016}},
Pages = {{1651-1655}},
Note = {{IEEE International Conference on Acoustics, Speech, and Signal
   Processing, Shanghai, PEOPLES R CHINA, MAR 20-25, 2016}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc}},
Abstract = {{This paper presents an algorithm for fully reconstructing a 3D face from
   a single image. This task is still highly challenging as most current
   methods only care about the frontal face, ignoring side face, such as
   the neck, ears etc. In our algorithm, to get the more detailed texture,
   we deal with the shape reconstruction and texture recovery respectively.
   For shape, we estimate the deformation of the 3D model by a set of
   feature points. For texture, due to the similar facial structure, we
   divide the full texture into patches and show how sparse learning model
   can be used to fully recover the texture of the 3D face. Extensive
   experiment results on the CMU-PIE database and images downloaded from
   the Internet demonstrate that our method outperforms the
   state-of-the-art methods.}},
ISSN = {{1520-6149}},
ISBN = {{978-1-4799-9988-0}},
Unique-ID = {{ISI:000388373401159}},
}

@inproceedings{ ISI:000389640300019,
Author = {Kittler, Josef and Huber, Patrik and Feng, Zhen-Hua and Hu, Guosheng and
   Christmas, William},
Editor = {{Perales, FJ and Kittler, J}},
Title = {{3D Morphable Face Models and Their Applications}},
Booktitle = {{ARTICULATED MOTION AND DEFORMABLE OBJECTS}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2016}},
Volume = {{9756}},
Pages = {{185-206}},
Note = {{9th International Conference on Articulated Motion and Deformable
   Objects (AMDO), Palma de Mallorca, SPAIN, JUL 13-15, 2016}},
Abstract = {{3D Morphable Face Models (3DMM) have been used in face recognition for
   some time now. They can be applied in their own right as a basis for 3D
   face recognition and analysis involving 3D face data. However their
   prevalent use over the last decade has been as a versatile tool in 2D
   face recognition to normalise pose, illumination and expression of 2D
   face images. A 3DMM has the generative capacity to augment the training
   and test databases for various 2D face processing related tasks. It can
   be used to expand the gallery set for pose-invariant face matching. For
   any 2D face image it can furnish complementary information, in terms of
   its 3D face shape and texture. It can also aid multiple frame fusion by
   providing the means of registering a set of 2D images. A key enabling
   technology for this versatility is 3D face model to 2D face image
   fitting. In this paper recent developments in 3D face modelling and
   model fitting will be overviewed, and their merits in the context of
   diverse applications illustrated on several examples, including pose and
   illumination invariant face recognition, and 3D face reconstruction from
   video.}},
DOI = {{10.1007/978-3-319-41778-3\_19}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-41778-3; 978-3-319-41777-6}},
ORCID-Numbers = {{Huber, Patrik/0000-0002-1474-1040
   Feng, Zhenhua/0000-0002-4485-4249}},
Unique-ID = {{ISI:000389640300019}},
}

@inproceedings{ ISI:000389586800019,
Author = {Hong, Yu-Jin and Nam, Gi Pyo and Choi, Heeseung and Cho, Junghyun and
   Kim, Ig-Jae},
Editor = {{Streitz, N and Markopoulos, P}},
Title = {{3-Dimensional Face from a Single Face Image with Various Expressions}},
Booktitle = {{DISTRIBUTED, AMBIENT AND PERVASIVE INTERACTIONS, (DAPI 2016)}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2016}},
Volume = {{9749}},
Pages = {{202-209}},
Note = {{4th International Conference on Distributed, Ambient and Pervasive
   Interactions (DAPI) held as part of 18th International Conference on
   Human-Computer Interaction (HCI International), Toronto, CANADA, JUL
   17-22, 2016}},
Abstract = {{Generating a user-specific 3D face model is useful for a variety of
   applications, such as facial animation, games or movie industries.
   Recently, there have been spectacular developments in 3D sensors,
   however, accurately recovering the 3D shape model from a single image is
   a major challenge of computer vision and graphics. In this paper, we
   present a method that can not only acquire a 3D shape from only a single
   face image but also reconstruct facial expression. To accomplish this, a
   3D face database with a variety of identities and facial expressions was
   restructured as a data array which was decomposed for the acquisition of
   bilinear models. With this model, we represent facial variances as two
   kinds of elements: expressions and identities. Then, target face image
   is fitted to 3D model while estimating its expression and shape
   parameters. As application example, we transferred expressions to
   reconstructed 3D models and naturally applied new facial expressions to
   show the efficiency of the proposed method.}},
DOI = {{10.1007/978-3-319-39862-4\_19}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-39862-4; 978-3-319-39861-7}},
Unique-ID = {{ISI:000389586800019}},
}

@inproceedings{ ISI:000388117502023,
Author = {Bagga, Manpreet and Singh, Baijit},
Editor = {{Hoda, MN}},
Title = {{Spoofing Detection In Face Recognition: A Review}},
Booktitle = {{PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON
   COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT}},
Year = {{2016}},
Pages = {{2037-2042}},
Note = {{3rd International Conference on Computing for Sustainable Global
   Development (INDIACom), New Delhi, INDIA, MAR 16-18, 2016}},
Organization = {{GGSIP Univ; Govt India, Minist Sci \& Technol, Dept Sci \& Technol;
   Council Sci \& Ind Res; All India Council Tech Educ; Inst Elect \&
   Telecommunicat Engineers, Delhi Ctr; Inst Engn \& Technol, Delhi Local
   Networks; Jagdishprasad Jhabarmal Tibrewala Univ; Bharati Vidyapeeths
   Inst Comp Applicat \& Management; ISTE, Delhi Sect}},
Abstract = {{In the recent days, the facial biometric system is widely used for the
   mobile payments and other surveillance systems. Its popularity is going
   to be increased because of its easiness to use and also it is user
   friendly. But the main problem in this system is its vulnerability to
   the spoof attacks made by 2D or 3D face masks or printed photographs. In
   order to guard against face spoofing, the anti-spoofing methods have
   been developed to do liveliness detection. In this paper, the different
   type of face spoofing attacks and the different techniques used for
   anti-spoofing arc analyzed.}},
ISBN = {{978-9-3805-4419-9}},
Unique-ID = {{ISI:000388117502023}},
}

@inproceedings{ ISI:000389494900012,
Author = {Vezzetti, Enrico and Marcolin, Federica and Tornincasa, Stefano and
   Moos, Sandro and Violante, Maria Grazia and Dagnes, Nicole and Monno,
   Giuseppe and Uva, Antonio Emmanuele and Fiorentino, Michele},
Editor = {{DePaolis, LT and Mongelli, A}},
Title = {{Facial Landmarks for Forensic Skull-Based 3D Face Reconstruction: A
   Literature Review}},
Booktitle = {{Augmented Reality, Virtual Reality, and Computer Graphics, Pt I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2016}},
Volume = {{9768}},
Pages = {{172-180}},
Note = {{3rd International Conference on Augmented Reality, Virtual Reality and
   Computer Graphics (SALENTO AVR), Otranto, ITALY, JUN 15-18, 2016}},
Organization = {{Univ Salento, Dept Engn Innovat}},
Abstract = {{Recent Face Analysis advances have focused the attention on studying and
   formalizing 3D facial shape. Landmarks, i.e. typical points of the face,
   are perfectly suited to the purpose, as their position on visage shape
   allows to build up a map of each human being's appearance. This turns to
   be extremely useful for a large variety of fields and related
   applications. In particular, the forensic context is taken into
   consideration in this study. This work is intended as a survey of
   current research advances in forensic science involving 3D facial
   landmarks. In particular, by selecting recent scientific contributions
   in this field, a literature review is proposed for in-depth analyzing
   which landmarks are adopted, and how, in this discipline. The main
   outcome concerns the identification of a leading research branch, which
   is landmark-based facial reconstruction from skull. The choice of
   selecting 3D contributions is driven by the idea that the most
   innovative Face Analysis research trends work on three-dimensional data,
   such as depth maps and meshes, with three-dimensional software and
   tools. The third dimension improves the accurateness and is robust to
   colour and lightning variations.}},
DOI = {{10.1007/978-3-319-40621-3\_12}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-40621-3; 978-3-319-40620-6}},
ORCID-Numbers = {{Dagnes, Nicole/0000-0003-4690-7567
   Marcolin, Federica/0000-0002-4360-6905}},
Unique-ID = {{ISI:000389494900012}},
}

@inproceedings{ ISI:000389499000053,
Author = {Xue, Mingliang and Duan, Xiaodong and Zhou, Juxiang and Wang, Cunrui and
   Wang, Yuangang and Li, Zedong and Liu, Wanquan},
Editor = {{You, Z and Wang, Y and Sun, Z and Shan, S and Zheng, W and Feng, J and Zhao, Q}},
Book-Author = {{Zhou, J}},
Title = {{A Computational Other-Race-Effect Analysis for 3D Facial Expression
   Recognition}},
Booktitle = {{Biometric Recognition}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2016}},
Volume = {{9967}},
Pages = {{483-493}},
Note = {{11th Chinese Conference on Biometric Recognition (CCBR), Chengdu,
   PEOPLES R CHINA, OCT 14-16, 2016}},
Organization = {{Chinese Assoc Artificial Intelligence; Chinese Acad Sci, Inst Automat;
   Springer; Sichuan Univ; Wisesoft Co Ltd}},
Abstract = {{This paper investigates the other-race-effects in automatic 3D facial
   expression recognition, giving the computational analysis of the
   recognition performance obtained from two races, namely white and east
   Asian. The 3D face information is represented by local depth feature,
   and then a feature learning process is used to obtain race-sensitive
   features to simulate the other-race-effect. The learned features from
   own race and other race are then used to do facial expression
   recognition. The proposed analysis is conducted on BU-3DFE database, and
   the results show that the learned features from one race achieve better
   recognition performance on the own-race faces. It reveals that the
   other-race-effect are significant in facial expression recognition
   problem, which confirms the results of psychological experiment results.}},
DOI = {{10.1007/978-3-319-46654-5\_53}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-46654-5; 978-3-319-46653-8}},
ResearcherID-Numbers = {{Wang, Yuangang/L-7956-2019
   }},
ORCID-Numbers = {{liu, wanquan/0000-0003-4910-353X}},
Unique-ID = {{ISI:000389499000053}},
}

@inproceedings{ ISI:000389381200037,
Author = {Trung Truong and Ngoc Ly},
Editor = {{Nguyen, NT and Trawinski, B and Fujita, H and Hong, TP}},
Title = {{Building the Facial Expressions Recognition System Based on RGB-D Images
   in High Performance}},
Booktitle = {{Intelligent Information and Database Systems, ACIIDS 2016, Pt II}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9622}},
Pages = {{377-387}},
Note = {{8th Asian Conference on Intelligent Information and Database Systems
   (ACIIDS), Da Nang, VIETNAM, MAR 14-16, 2016}},
Organization = {{Vietnam Korea Friendship Informat Technol Coll; Wroclaw Univ Technol;
   IEEE SMC Tech Comm Computat Collect Intelligence; Bina Nusantara Univ;
   Ton Duc Thang Univ; Quang Binh Univ}},
Abstract = {{In this paper, we propose a novel idea for automatic facial expression
   analysis with the aim of resolving the existing challenges in 2D images.
   The subtle combination of the geometry-based method with the
   appearance-based features in depth and color images contributes to
   increasing in distinguishable features among various facial expressions.
   Particular functions are utilised to calculate the correlation between
   expressions in order to determine the exact facial expression. Our
   approach consists of a sequence of steps including estimating the normal
   vector of facial surface, then extracting the geometric features such as
   the orientation of normal vector in the point cloud. The useful color
   information is known as LBP. According to the result of the experiment,
   we demonstrate that the effective fusion scheme of texture and shape
   feature on color and depth images. In comparison with the non fusion
   scheme, our fusion scheme has resulted in the increase of recognition
   under low and high illuminated light, about 19.84\% and 1.59\%,
   respectively.}},
DOI = {{10.1007/978-3-662-49390-8\_37}},
ISSN = {{0302-9743}},
ISBN = {{978-3-662-49390-8; 978-3-662-49389-2}},
ORCID-Numbers = {{Truong, Quang Trung/0000-0002-6242-2191}},
Unique-ID = {{ISI:000389381200037}},
}

@inproceedings{ ISI:000387649600042,
Author = {Amolik, Akshay and Ahamad, Syed Tahir and Dey, Sourav and Manjula, R.},
Book-Group-Author = {{IEEE}},
Title = {{3D Face View Generation from Human Drawn Sketch: A review}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON COMPUTATION OF POWER, ENERGY
   INFORMATION AND COMMUNICATION (ICCPEIC)}},
Series = {{International Conference on Computation of Power, Energy Information and
   Communication}},
Year = {{2016}},
Pages = {{237-243}},
Note = {{5th IEEE International Conference on Computation of Power, Energy,
   Information and Communication (ICCPEIC), Adhiparasakthi Engn Coll,
   Melmaruvathur, INDIA, APR 20-21, 2016}},
Organization = {{IEEE; Dept Elect \& Elect Engn}},
Abstract = {{From last few decades, generating a 3D face model from an human drawn
   sketch has caught the interest of many researchers in the area of image
   processing and face recognition. It has various applications in 3D
   cartoon modelling, police investigation and verification, and in Image
   Processing. Many techniques are there to generate 3D models from a
   sketch. 3D landmark estimation, 2D landmark detection, and synthesis of
   texture and surface with respect to 3-D morphable model are the steps,
   respectively, to generate the 3D face model. 3D face modelling using
   these steps has a higher rate of accuracy of identification of a person
   from her sketch and no proper photograph. In this piece of literature,
   we present a review on efficient technique that can be used to generate
   3D face from sketch drawn by human.}},
ISSN = {{2472-4033}},
ISBN = {{978-1-5090-0900-8}},
Unique-ID = {{ISI:000387649600042}},
}

@inproceedings{ ISI:000386658300011,
Author = {Sopiak, Dominik and Oravec, Milos and Pavlovicova, Jarmila and
   Bukovcikova, Zuzana and Dittingerova, Monika and Bil'anska, Alexandra
   and Novotna, Maria and Gontkovic, Jozef},
Editor = {{Li, MZ and Xiong, N and Tong, Z and Du, J and Liu, C and Li, KL and Wang, L}},
Title = {{Generating Face Images Based on 3D Morphable Model}},
Booktitle = {{2016 12TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS
   AND KNOWLEDGE DISCOVERY (ICNC-FSKD)}},
Year = {{2016}},
Pages = {{58-62}},
Note = {{12th International Conference on Natural Computation, Fuzzy Systems and
   Knowledge Discovery (ICNC-FSKD), Changsha, PEOPLES R CHINA, AUG 13-15,
   2016}},
Organization = {{IEEE; IEEE Circuits \& Syst Soc; Nanyang Technol Univ; Hunan Univ; State
   Univ New York; Hunan Normal Univ; Natl Univ Defense Technol}},
Abstract = {{In modern days the demand for biometrics increases rapidly. The world
   still needs to solve many problems and answer to lot of questions
   regarding to biometrics for creating better solutions for recognition
   and verification of objects. Biometrics has become really important
   topic of our security. Number of input samples per person affects
   recognition in modern algorithms used for face recognition. We can say
   that single-sample problem is one of the most challenging problems in
   face recognition. Most of face recognition algorithms requires at least
   two samples per person but it is very important to create a system where
   only one sample per person would achieve a good performance. This topic
   is especially related to passport and id photos because they include
   only one picture of person. In this paper, we explain process of
   reconstruction and generating 3D face model created from id or passport
   photo. We also present our results that shows influence of generated new
   samples on face recognition.}},
ISBN = {{978-1-5090-4093-3}},
Unique-ID = {{ISI:000386658300011}},
}

@inproceedings{ ISI:000386931400187,
Author = {Lei, Yinjie and Feng, Siyu and Zhou, Xinzhi and Guo, Yulan},
Book-Group-Author = {{IEEE}},
Title = {{An efficient 3D partial face recognition approach with single sample}},
Booktitle = {{PROCEEDINGS OF THE 2016 IEEE 11TH CONFERENCE ON INDUSTRIAL ELECTRONICS
   AND APPLICATIONS (ICIEA)}},
Series = {{IEEE Conference on Industrial Electronics and Applications}},
Year = {{2016}},
Pages = {{994-999}},
Note = {{IEEE 11th Conference on Industrial Electronics and Applications (ICIEA),
   Hefei, PEOPLES R CHINA, JUN 05-07, 2016}},
Organization = {{IEEE; IEEE Ind Elect Soc; IEEE Ind Elect Chapter; IEEE Singapore Sect;
   Anhui Univ}},
Abstract = {{3D partial face recognition under missing parts, occlusions and data
   corruptions is a major challenge for the practical application of the
   techniques of 3D face recognition. Moreover, one individual can only
   provide one sample for training in most practical scenarios, and thus
   the face recognition with single sample problem is another highly
   challenging task. We propose an efficient framework for 3D partial face
   recognition with single sample addressing both of the two problems.
   First, we represent a facial scan with a set of keypoint based local
   geometrical descriptors, which gains sufficient robustness to partial
   facial data along with expression/pose variations. Then, a two-step
   modified collaborative representation classification scheme is proposed
   to address the single sample recognition problem. A class-based
   probability estimation is given during the first classification step,
   and the obtained result is then incorporated into the modified
   collaborative representation classification as a locality constraint to
   improve its classification performance. Extensive experiments on the
   Bosphorus and FRGC v2.0 datasets demonstrate the efficiency of the
   proposed approach when addressing the problem of 3D partial face
   recognition with single sample.}},
ISSN = {{2156-2318}},
ISBN = {{978-1-4673-8644-9}},
Unique-ID = {{ISI:000386931400187}},
}

@inproceedings{ ISI:000386567400066,
Author = {Yadav, Prem Chand and Singh, H. V. and Patel, Ankit Kumar and Singh,
   Anurag},
Editor = {{Pandey, AS}},
Title = {{A Comparative Analysis of Different Facial Action Tracking Models and
   Techniques}},
Booktitle = {{2016 INTERNATIONAL CONFERENCE ON EMERGING TRENDS IN ELECTRICAL
   ELECTRONICS \& SUSTAINABLE ENERGY SYSTEMS (ICETEESES)}},
Year = {{2016}},
Pages = {{347-349}},
Note = {{International Conference on Emerging Trends in Electrical Electronics \&
   Sustainable Energy Systems (ICETEESES), Sultanpur, INDIA, MAR 11-12,
   2016}},
Organization = {{IEEE UP Sect; Kamla Nehru Inst Technol, Dept Elect Engn; Kamla Nehru
   Inst Technol, Dept Elect Engn}},
Abstract = {{The tracking of facial activities from video is an important and
   challenging problem. Now a day, many computer vision techniques have
   been proposed to characterize the facial activities in the three levels
   (from local to global). First level is the bottom level, in which the
   facial feature tracking focuses on detecting and tracking of the
   prominent local landmarks surrounding facial components (e.g. mouth,
   eyebrow, etc), in second level the facial action units (AUs)
   characterize the specific behaviors of these local facial components
   (e.g. mouth open, eyebrow raiser, etc) and the third level is facial
   expression level, which represents subjects emotions (e.g. Surprise,
   Happy, Anger, etc.) and controls the global muscular movement of the
   whole face. Most of the existing methods focus on one or two levels of
   facial activities, and track (or recognize) them separately. In this
   paper, various facial action tracking models and techniques are compared
   in different conditions such as the performance of Active Facial
   Tracking for Fatigue Detection, Real Time 3D Face Pose Tracking from an
   Uncalibrated Camera, Simultaneous facial action tracking and expression
   recognition using a particle filter and Simultaneous Tracking and Facial
   Expression Recognition using Multiperson and Multiclass Autoregressive
   Models.}},
ISBN = {{978-1-5090-2118-5}},
Unique-ID = {{ISI:000386567400066}},
}

@article{ ISI:000386591800006,
Author = {Krotewicz, Pawel},
Title = {{Novel ear-assisted 3D face recognition under expression variations}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMETRICS}},
Year = {{2016}},
Volume = {{8}},
Number = {{1, SI}},
Pages = {{65-81}},
Abstract = {{This paper concerns the novel region-based ear-assisted 3D face
   recognition based on iterative closest point (ICP) algorithm in face
   expression changing scenario. The proposed algorithm for 3D face
   biometric recognition was prepared and tested. As a first contribution,
   current state-of-the-art in the field of 3D face recognition is
   presented and the main approaches to the problem are briefly described.
   Furthermore, all the data processing steps: preprocessing, segmentation,
   feature extraction and feature comparison are described in detail. As a
   second contribution, the algorithm behaviour is scrutinised on the
   DMCSv1 database and the results in the form of DET curves are
   highlighted in this paper. Also, the comparison with results obtained by
   means of the algorithm neglecting ear regions is provided. It occurs
   that ear geometry information added to face as an auxiliary input to
   iterative closest point can greatly improve recognition results
   especially in the case of very strong facial expressions. Equal error
   rate does not exceed 6.25\% on arbitrary data subset. In the last
   section, conclusions are formulated and plans for future work are
   presented.}},
DOI = {{10.1504/IJBM.2016.077148}},
ISSN = {{1755-8301}},
EISSN = {{1755-831X}},
Unique-ID = {{ISI:000386591800006}},
}

@inproceedings{ ISI:000385794300011,
Author = {Magruder, Lori A. and Leigh, Holly W. and Soderlund, Alexander and
   Clymer, Bradley and Baer, Jessica and Neuenschwander, Amy L.},
Editor = {{Turner, MD and Kamerman, GW}},
Title = {{Automated feature extraction for 3-dimensional point clouds}},
Booktitle = {{LASER RADAR TECHNOLOGY AND APPLICATIONS XXI}},
Series = {{Proceedings of SPIE}},
Year = {{2016}},
Volume = {{9832}},
Note = {{Conference on Laser Radar Technology and Applications XXI, Baltimore,
   MD, APR 19-20, 2016}},
Organization = {{SPIE}},
Abstract = {{Light detection and ranging (LIDAR) technology offers the capability to
   rapidly capture high-resolution, 3-dimensional surface data with
   centimeter-level accuracy for a large variety of applications. Due to
   the foliage-penetrating properties of LIDAR systems, these geospatial
   data sets can detect ground surfaces beneath trees, enabling the
   production of high-fidelity bare earth elevation models. Precise
   characterization of the ground surface allows for identification of
   terrain and non-terrain points within the point cloud, and facilitates
   further discernment between natural and man-made objects based solely on
   structural aspects and relative neighboring parameterizations. A
   framework is presented here for automated extraction of natural and
   man-made features that does not rely on coincident ortho-imagery or
   point RGB attributes. The TEXAS (Terrain EXtraction And Segmentation)
   algorithm is used first to generate a bare earth surface from a lidar
   survey, which is then used to classify points as terrain or non-terrain.
   Further classifications are assigned at the point level by leveraging
   local spatial information. Similarly classed points are then clustered
   together into regions to identify individual features. Descriptions of
   the spatial attributes of each region are generated, resulting in the
   identification of individual tree locations, forest extents, building
   footprints, and 3-dimensional building shapes, among others. Results of
   the fully-automated feature extraction algorithm are then compared to
   ground truth to assess completeness and accuracy of the methodology.}},
DOI = {{10.1117/12.2223845}},
Article-Number = {{UNSP 98320F}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0073-7}},
Unique-ID = {{ISI:000385794300011}},
}

@inproceedings{ ISI:000385263000030,
Author = {Xu, Yi and Price, True and Frahm, Jan-Michael and Monrose, Fabian},
Book-Group-Author = {{USENIX Assoc}},
Title = {{Virtual U: Defeating Face Liveness Detection by Building Virtual Models
   From Your Public Photos}},
Booktitle = {{PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM}},
Year = {{2016}},
Pages = {{497-512}},
Note = {{25th USENIX Security Symposium, Austin, TX, AUG 10-12, 2016}},
Organization = {{USENIX; Facebook; NSF; Cisco; Google; Microsoft; Neustar; IBM Res;
   Symantec; ACM Queue; ADMIN; CRC Press; Linux Pro Magazine; NetApp;
   VMWare; LXer; UserFriendly Org; OReilly Media; No Starch Press; Virus
   Bulletin}},
Abstract = {{In this paper, we introduce a novel approach to bypass modern face
   authentication systems. More specifically, by leveraging a handful of
   pictures of the target user taken from social media, we show how to
   create realistic, textured, 3D facial models that undermine the security
   of widely used face authentication solutions. Our framework makes use of
   virtual reality (VR) systems, incorporating along the way the ability to
   perform animations (e.g., raising an eyebrow or smiling) of the facial
   model, in order to trick liveness detectors into believing that the 3D
   model is a real human face. The synthetic face of the user is displayed
   on the screen of the VR device, and as the device rotates and translates
   in the real world, the 3D face moves accordingly. To an observing face
   authentication system, the depth and motion cues of the display match
   what would be expected for a human face.
   We argue that such VR-based spoofing attacks constitute a fundamentally
   new class of attacks that point to a serious weaknesses in camera-based
   authentication systems: Unless they incorporate other sources of
   verifiable data, systems relying on color image data and camera motion
   are prone to attacks via virtual realism. To demonstrate the practical
   nature of this threat, we conduct thorough experiments using an
   end-to-end implementation of our approach and show how it undermines the
   security of several face authentication solutions that include both
   motion-based and liveness detectors.}},
ISBN = {{978-1-931971-32-4}},
ResearcherID-Numbers = {{Smith, Nash/R-3104-2017}},
ORCID-Numbers = {{Smith, Nash/0000-0002-4406-0043}},
Unique-ID = {{ISI:000385263000030}},
}

@inproceedings{ ISI:000383222400036,
Author = {Zribi, Soumaya and Khadhraoui, Taher and Benzarti, Faouzi and Amiri,
   Hamid},
Editor = {{Song, YT}},
Title = {{Automatic 3D Face Preprocessing}},
Booktitle = {{2016 IEEE/ACIS 14TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING
   RESEARCH, MANAGEMENT AND APPLICATIONS (SERA)}},
Year = {{2016}},
Pages = {{281-285}},
Note = {{IEEE/ACIS 14th International Conference on Software Engineering
   Research, Management and Application (SERA), Towson Univ, Baltimore, MD,
   JUN 08-10, 2016}},
Organization = {{IEEE Comp Soc; IEEE; Int Assoc Comp \& Informat Sci; Shanghai Univ;
   Shanghai Key Lab Comp Software Testing \& Evaluating}},
Abstract = {{An efficient and above all cheap solutions, biometrics provide extensive
   information in access control applications. 3D mode provides great new
   opportunities in this sector in recent years. Firstly the paper
   discusses the formal work done in this domain discussing approach based
   on curvature calculation, alignment of surfaces, feature selection and
   facial curve including different techniques for 3D facial recognition
   data. The second part is about the steps required for the preprocessing
   phase of the 3D face data. Various experiments conducted and results
   obtained.}},
ISBN = {{978-1-5090-0809-4}},
Unique-ID = {{ISI:000383222400036}},
}

@inproceedings{ ISI:000380803300154,
Author = {Jasek, Roman and Talandova, Hana and Adamek, Milan},
Editor = {{Simos, T and Tsitouras, C}},
Title = {{Methodology of the Determination of the Uncertainties by Using the
   Biometric Device the Broadway 3D}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON NUMERICAL ANALYSIS AND
   APPLIED MATHEMATICS 2015 (ICNAAM-2015)}},
Series = {{AIP Conference Proceedings}},
Year = {{2016}},
Volume = {{1738}},
Note = {{International Conference on Numerical Analysis and Applied Mathematics
   (ICNAAM), Rhodes, GREECE, SEP 23-29, 2015}},
Abstract = {{The biometric identification by face is among one of the most widely
   used methods of biometric identification. Due to it provides a faster
   and more accurate identification; it was implemented into area of
   security 3D face reader by Broadway manufacturer was used to measure. It
   is equipped with the 3D camera system, which uses the method of
   structured light scanning and saves the template into the 3D model of
   face. The obtained data were evaluated by software Turnstile Enrolment
   Application (TEA). The measurements were used 3D face reader the
   Broadway 3D. First, the person was scanned and stored in the database.
   Thereafter person has already been compared with the stored template in
   the database for each method. Finally, a measure of reliability was
   evaluated for the Broadway 3D face reader.}},
DOI = {{10.1063/1.4951908}},
Article-Number = {{120025}},
ISSN = {{0094-243X}},
ISBN = {{978-0-7354-1392-4}},
ResearcherID-Numbers = {{Jasek, Roman/E-3492-2018}},
ORCID-Numbers = {{Jasek, Roman/0000-0002-9831-9372}},
Unique-ID = {{ISI:000380803300154}},
}

@inproceedings{ ISI:000381427400036,
Author = {Naveen, S. and Moni, R. S.},
Editor = {{Berretti, S and Thampi, SM and Srivastava, PR}},
Title = {{Contourlet and Fourier Transform Features Based 3D Face Recognition
   System}},
Booktitle = {{INTELLIGENT SYSTEMS TECHNOLOGIES AND APPLICATIONS, VOL 1}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2016}},
Volume = {{384}},
Pages = {{411-425}},
Note = {{International Symposium on Intelligent Systems Technologies and
   Applications (ISTA), SCMS Grp Inst, SCMS Sch Engn \& Technol, Kochi,
   INDIA, AUG 10-13, 2015}},
Abstract = {{Human face recognition based on geometrical structure has been an area
   of interest among researchers for the past few decades especially in
   pattern recognition. 3D Face recognition systems are of interest in this
   context. The main advantage of 3D Face recognition is the availability
   of geometrical information of the face structure which is more or less
   unique for a subject. This paper focuses on the problems of person
   identification using 3D Face data. Use of unregistered 3D Face data for
   feature extraction significantly increases the operational speed of the
   system with huge database enrollment. In this work, unregistered Face
   data, i.e. both texture and depth is fed to a classifier in spectral
   representations of the same data. 2-D Discrete Contourlet Transform and
   2-D Discrete Fourier Transform is used here for the spectral
   representation which forms the feature matrix. Fusion of texture and
   depth statistical information of face is proposed in this paper since
   the individual schemes are of lower performance. Application of
   statistical method seems to degrade the performance of the system when
   applied to texture data and was effective in the case of depth data.
   Fusion of the matching scores proves that the recognition accuracy can
   be improved significantly by fusion of scores of multiple
   representations. FRAV3D database is used for testing the algorithm.}},
DOI = {{10.1007/978-3-319-23036-8\_36}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-23036-8; 978-3-319-23035-1}},
Unique-ID = {{ISI:000381427400036}},
}

@inproceedings{ ISI:000373009100042,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Editor = {{Satapathy, SC and Mandal, JK and Udgata, SK and Bhateja, V}},
Title = {{Issues and Approaches to Design of a Range Image Face Database}},
Booktitle = {{INFORMATION SYSTEMS DESIGN AND INTELLIGENT APPLICATIONS, VOL 2, INDIA
   2016}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2016}},
Volume = {{434}},
Pages = {{425-436}},
Note = {{3rd International Conference on Information System Design and
   Intelligent Applications (INDIA), ANITS Campus, Visakhapatnam, INDIA,
   JAN 08-09, 2016}},
Organization = {{Anil Neerukonda Inst Technol \& Sci, Det CSE; ANITS CSI Student Branch}},
Abstract = {{Development of new databases contributing much among researchers for
   solving many challenging tasks that might have an important role during
   the implementation of efficient algorithms to handle all difficulties
   for an automatic system. In this paper, authors have introduced the
   issues and approaches that have been considered during image acquisition
   procedure during designing of own face database. This database consists
   of almost all the challenges in the domain of computer vision especially
   face recognition. Acquisition of database's images are done in our own
   institute's laboratory with variations of facial actions (i.e. movement
   of facial units, expression), illumination, occlusion, as well as a
   pose. Along with the 3D face images, corresponding 2D face images have
   also been captured using Structured Light Scanner (SLS). Particularly,
   this image acquisition technique is not harmful as laser scanner does.
   Moreover, authors have made the visualization of practical
   representation of laboratory setup within this article that would again
   be helpful to the researchers for better understanding the image
   acquisition procedure in detail. In this databases, authors have
   accomplished the X, Y planes along with range face image and
   corresponding 2D image of human face.}},
DOI = {{10.1007/978-81-322-2752-6\_42}},
ISSN = {{2194-5357}},
ISBN = {{978-81-322-2752-6; 978-81-322-2750-2}},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000373009100042}},
}

@article{ ISI:000371563600001,
Author = {Alsmadi, Mutasem},
Title = {{Facial Recognition under Expression Variations}},
Journal = {{INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY}},
Year = {{2016}},
Volume = {{13}},
Number = {{1A, SI}},
Pages = {{133-141}},
Abstract = {{Researchers in different fields such as image processing, neural
   sciences, computer programs and psychophysics have investigated number
   of problems related to facial recognition by machines and humans since
   1975. Automatic recognition of the human emotions using facial
   expression is an important, but difficult problem. This study introduces
   a novel and automatic approach to analyze and recognize human facial
   expressions and emotions using a Metaheuristic Algorithm (MA), which
   hybridizes iterated local search and Genetic Algorithms with
   Back-Propagation algorithm (ILSGA-BP). Back Propagation algorithm (BP)
   was used to train and test the extracted features from the extracted
   right eye, left eye and mouth using radial curves and Cubic Bezier
   curves, M4 was used to enhance and optimize the initial weights of the
   traditional BP. FEEDTUM facial expression database was used in this
   study for training and testing processes with seven different emotions
   namely; surprise, happiness, disgust, neutral, fear, sadness and anger.
   A comparison of the results obtained using the extracted features from
   the radial curves, Cubic Bezier curves and the combination of them were
   conducted. The comparison shows the superiority of the combination of
   the radial curves and the Cubic Bezier curves with percentage ranges
   between 87\% and 97\% over the radial curves alone with a percentage
   ranges between 80\% and 97\% and over the Cubic Bezier curves with a
   percentage ranges between 83\% and 97\%. Moreover, based on the
   extracted features using the radial curves, Cubic Bezier curves and the
   combination of them, the experimental results show that the proposed
   ILSGA-BP algorithm outperformed the BP algorithm with overall accuracy
   88\%, 89\% and 93.4\% respectively, compared to 83\%, 82\% and 85\%
   respectively using BP algorithm.}},
ISSN = {{1683-3198}},
Unique-ID = {{ISI:000371563600001}},
}

@article{ ISI:000370290900001,
Author = {Song, Dan and Luo, Jing and Zi, Chunyuan and Tian, Huixin},
Title = {{3D Face Recognition Using Anthropometric and Curvelet Features Fusion}},
Journal = {{JOURNAL OF SENSORS}},
Year = {{2016}},
Abstract = {{Curvelet transform can describe the signal by multiple scales, and
   multiple directions. In order to improve the performance of 3D face
   recognition algorithm, we proposed an Anthropometric and Curvelet
   features fusion-based algorithm for 3D face recognition (Anthropometric
   Curvelet Fusion Face Recognition, ACFFR). First, the eyes, nose, and
   mouth feature regions are extracted by the Anthropometric
   characteristics and curvature features of the human face. Second,
   Curvelet energy features of the facial feature regions at different
   scales and different directions are extracted by Curvelet transform. At
   last, Euclidean distance is used as the similarity between template and
   objectives. To verify the performance, the proposed algorithm is
   compared with Anthroface3D and Curveletface3D on the Texas 3D FR
   database. The experimental results have shown that the proposed
   algorithm performs well, with equal error rate of 1.75\% and accuracy of
   97.0\%. The algorithm we proposed in this paper has better robustness to
   expression and light changes than Anthroface3D and Curveletface3D.}},
DOI = {{10.1155/2016/6859364}},
Article-Number = {{6859364}},
ISSN = {{1687-725X}},
EISSN = {{1687-7268}},
Unique-ID = {{ISI:000370290900001}},
}

@inproceedings{ ISI:000369718100001,
Author = {Boggaram, Achyut Sarma and Mallampalli, Pujitha Raj and Muthyala,
   Chandrasekhar Reddy and Manjusha, R.},
Editor = {{Behera, HS and Mohapatra, DP}},
Title = {{A Novel Approach for Biometric Authentication System Using Ear, 2D Face
   and 3D Face Modalities}},
Booktitle = {{COMPUTATIONAL INTELLIGENCE IN DATA MINING, VOL 1, CIDM 2015}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2016}},
Volume = {{410}},
Pages = {{1-11}},
Note = {{2nd International Conference on Computational Intelligence in Data
   Mining (ICCIDM), Bhubaneswar, INDIA, DEC 05-06, 2015}},
Organization = {{Roland Inst Technol}},
Abstract = {{Biometric system using face recognition is the frontier of the security
   across various applications in the fields of multimedia, medicine,
   civilian surveillance, robotics, etc. Differences in illumination
   levels, pose variations, eye-wear, facial hair, aging and disguise are
   some of the current challenges in face recognition. The ear, which is
   turning out to be a promising biometric identifier having some desirable
   properties such as universality, uniqueness, permanence, can also be
   used along with face for better performance of the system. A multi-modal
   biometric system combining 2D face, 3D face (depth image) and ear
   modalities using Microsoft Kinect and Webcam is proposed to address
   these challenges to some extent. Also avoiding redundancy in the
   extracted features for better processing speed is another challenge in
   designing the system. After careful survey of the existing algorithms
   applied to 2D face, 3D face and ear data, we focus on the well-known PCA
   (Principal Component Analysis) based Eigen Faces algorithm for ear and
   face recognition to obtain a better performance with minimal
   computational requirements. The resulting proposed system turns out
   insensitive to lighting conditions, pose variations, aging and can
   completely replace the current recognition systems economically and
   provide a better security. A total of 109 subjects participated in
   diversified data acquisition sessions involving multiple poses,
   illuminations, eyewear and persons from different age groups. The
   dataset is also a first attempt on the stated combination of biometrics
   and is a contribution to the field of Biometrics by itself for future
   experiments. The results are obtained separately against each biometric
   and final decision is obtained using all the individual results for
   higher accuracy. The proposed system performed at 98.165 \% verification
   rate which is greater than either of the dual combinations or each of
   the stated modality in a statistical and significant manner.}},
DOI = {{10.1007/978-81-322-2734-2\_1}},
ISSN = {{2194-5357}},
ISBN = {{978-81-322-2734-2; 978-81-322-2732-8}},
ORCID-Numbers = {{Ganesan, Manjusha/0000-0002-6984-2957}},
Unique-ID = {{ISI:000369718100001}},
}

@article{ ISI:000369518500015,
Author = {Ouamane, A. and Belahcene, M. and Benakcha, A. and Bourennane, S. and
   Taleb-Ahmed, A.},
Title = {{Robust multimodal 2D and 3D face authentication using local feature
   fusion}},
Journal = {{SIGNAL IMAGE AND VIDEO PROCESSING}},
Year = {{2016}},
Volume = {{10}},
Number = {{1}},
Pages = {{129-137}},
Month = {{JAN}},
Abstract = {{In this work, we present a robust face authentication approach merging
   multiple descriptors and exploiting both 3D and 2D information. First,
   we correct the heads rotation in 3D by iterative closest point
   algorithm, followed by an efficient preprocessing phase. Then, we
   extract different features namely: multi-scale local binary patterns
   (MSLBP), novel statistical local features (SLF), Gabor wavelets, and
   scale invariant feature transform (SIFT). The principal component
   analysis followed by enhanced fisher linear discriminant model is used
   for dimensionality reduction and classification. Finally, fusion at the
   score level is carried out using two-class support vector machines.
   Extensive experiments are conducted on the CASIA 3D faces database. The
   evaluation of individual descriptors clearly showed the superiority of
   the proposed SLF features. In addition, applying the (3D + 2D)
   multimodal score level fusion, the best result is obtained by combining
   the SLF with the MSLBP + SIFT descriptor yielding in an equal error rate
   of 0.98\% and a recognition rate of RR = 97.22 \%.}},
DOI = {{10.1007/s11760-014-0712-x}},
ISSN = {{1863-1703}},
EISSN = {{1863-1711}},
Unique-ID = {{ISI:000369518500015}},
}

@article{ ISI:000367856500018,
Author = {Bellil, Wajdi and Brahim, Hajer and Ben Amar, Chokri},
Title = {{Gappy wavelet neural network for 3D occluded faces: detection and
   recognition}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2016}},
Volume = {{75}},
Number = {{1}},
Pages = {{365-380}},
Month = {{JAN}},
Abstract = {{The first handicap in 3D faces recognizing under unconstrained problem
   is the largest variability of the visual aspect when we use various
   sources. This great variability complicates the task of identifying
   persons from their 3D facial scans and it is the most reason that bring
   to face detection and recognition of the major problems in pattern
   recognition fields, biometrics and computer vision. We propose a new 3D
   face identification and recognition method based on Gappy Wavelet Neural
   Network (GWNN) that is able to provide better accuracy in the presence
   of facial occlusions. The proposed approach consists of three steps: the
   first step is face detection. The second step is to identify and remove
   occlusions. Occluded regions detection is done by considering that
   occlusions can be defined as local face deformations. These deformations
   are detected by a comparison between the input facial test wavelet
   coefficients and wavelet coefficients of generic face model formed by
   the mean data base faces. They are beneficial for neighborhood
   relationships between pixels rotation, dilation and translation
   invariant. Then, occluded regions are refined by removing wavelet
   coefficient above a certain threshold. Finally, the last stage of
   processing and retrieving is made based on wavelet neural network to
   recognize and to restore 3D occluded regions that gathers the most. The
   experimental results on this challenging database demonstrate that the
   proposed approach improves recognition rate performance from 93.57 to
   99.45 \% which represents a competitive result compared to the state of
   the art.}},
DOI = {{10.1007/s11042-014-2294-6}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000367856500018}},
}

@article{ ISI:000365181400013,
Author = {Moeini, Ali and Faez, Karim and Moeini, Hossein},
Title = {{Unconstrained pose-invariant face recognition by a triplet collaborative
   dictionary matrix}},
Journal = {{PATTERN RECOGNITION LETTERS}},
Year = {{2015}},
Volume = {{68}},
Number = {{1}},
Pages = {{83-89}},
Month = {{DEC 15}},
Abstract = {{In this paper, a novel method is proposed for unconstrained pose
   invariant face recognition from only an image in a gallery. A 3D face is
   initially reconstructed using only a 2D frontal image. Then, for each
   person in the gallery, a Triplet Collaborative Dictionary Matrix (TCDM)
   is created from all face poses by rotating the 3D reconstructed models
   and extracting features in rotated face. Each TCDM is subsequently
   rendered based On triplet angles of face poses. Finally, the
   classification is performed by Collaborative Representation
   Classification (CRC) with Regularized Least Square (RLS). Promising
   results were acquired to handle pose changes on the FERET, LFW and video
   face databases compared to state-of-the-art methods in pose-invariant
   face recognition. (C) 2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.patrec.2015.08.012}},
ISSN = {{0167-8655}},
EISSN = {{1872-7344}},
ResearcherID-Numbers = {{faez, karim/K-5117-2019}},
ORCID-Numbers = {{faez, karim/0000-0002-1159-4866}},
Unique-ID = {{ISI:000365181400013}},
}

@article{ ISI:000361685100009,
Author = {Juefei-Xu, Felix and Luu, Khoa and Savvides, Marios},
Title = {{Spartans: Single-Sample Periocular-Based Alignment-Robust Recognition
   Technique Applied to Non-Frontal Scenarios}},
Journal = {{IEEE TRANSACTIONS ON IMAGE PROCESSING}},
Year = {{2015}},
Volume = {{24}},
Number = {{12}},
Pages = {{4780-4795}},
Month = {{DEC}},
Abstract = {{In this paper, we investigate a single-sample periocular-based
   alignment-robust face recognition technique that is pose-tolerant under
   unconstrained face matching scenarios. Our Spartans framework starts by
   utilizing one single sample per subject class, and generate new face
   images under a wide range of 3D rotations using the 3D generic elastic
   model which is both accurate and computationally economic. Then, we
   focus on the periocular region where the most stable and discriminant
   features on human faces are retained, and marginalize out the regions
   beyond the periocular region since they are more susceptible to
   expression variations and occlusions. A novel facial descriptor,
   high-dimensional Walsh local binary patterns, is uniformly sampled on
   facial images with robustness toward alignment. During the learning
   stage, subject-dependent advanced correlation filters are learned for
   pose-tolerant non-linear subspace modeling in kernel feature space
   followed by a coupled max-pooling mechanism which further improve the
   performance. Given any unconstrained unseen face image, the Spartans can
   produce a highly discriminative matching score, thus achieving high
   verification rate. We have evaluated our method on the challenging
   Labeled Faces in the Wild database and solidly outperformed the
   state-of-the-art algorithms under four evaluation protocols with a high
   accuracy of 89.69\%, a top score among image-restricted and unsupervised
   protocols. The advancement of Spartans is also proven in the Face
   Recognition Grand Challenge and Multi-PIE databases. In addition, our
   learning method based on advanced correlation filters is much more
   effective, in terms of learning subject-dependent pose-tolerant
   subspaces, compared with many well-established subspace methods in both
   linear and non-linear cases.}},
DOI = {{10.1109/TIP.2015.2468173}},
ISSN = {{1057-7149}},
EISSN = {{1941-0042}},
Unique-ID = {{ISI:000361685100009}},
}

@article{ ISI:000361934300008,
Author = {Li, Huibin and Ding, Huaxiong and Huang, Di and Wang, Yunhong and Zhao,
   Xi and Morvan, Jean-Marie and Chen, Liming},
Title = {{An efficient multimodal 2D+3D feature-based approach to automatic facial
   expression recognition}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2015}},
Volume = {{140}},
Pages = {{83-92}},
Month = {{NOV}},
Abstract = {{We present a fully automatic multimodal 2D + 3D feature-based facial
   expression recognition approach and demonstrate its performance on the
   BU-3DFE database. Our approach combines multi-order gradient-based local
   texture and shape descriptors in order to achieve efficiency and
   robustness. First, a large set of fiducial facial landmarks of 20 face
   images along with their 3D face scans are localized using a novel
   algorithm namely incremental Parallel Cascade of Linear Regression
   (iPar-CLR). Then, a novel Histogram of Second Order Gradients (HSOG)
   based local image descriptor in conjunction with the widely used
   first-order gradient based SIFT descriptor are used to describe the
   local texture around each 2D landmark. Similarly, the local geometry
   around each 3D landmark is described by two novel local shape
   descriptors constructed using the first-order and the second-order
   surface differential geometry quantities, i.e., Histogram of mesh
   Gradients (meshHOG) and Histogram of mesh Shape index (curvature
   quantization, meshHOS). Finally, the Support Vector Machine (SVM) based
   recognition results of all 2D and 3D descriptors are fused at both
   feature-level and score-level to further improve the accuracy.
   Comprehensive experimental results demonstrate that there exist
   impressive complementary characteristics between the 2D and 3D
   descriptors. We use the BU-3DFE benchmark to compare our approach to the
   state-of-the-art ones. Our multimodal feature-based approach outperforms
   the others by achieving an average recognition accuracy of 86.32\%.
   Moreover, a good generalization ability is shown on the Bosphorus
   database. (C) 2015 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.cviu.2015.07.005}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
Unique-ID = {{ISI:000361934300008}},
}

@article{ ISI:000359029900025,
Author = {Liang, Ronghua and Shen, Wenjia and Li, Xiao-Xin and Wang, Haixia},
Title = {{Bayesian multi-distribution-based discriminative feature extraction for
   3D face recognition}},
Journal = {{INFORMATION SCIENCES}},
Year = {{2015}},
Volume = {{320}},
Pages = {{406-417}},
Month = {{NOV 1}},
Abstract = {{Due to the difficulties associated with the collection of 3D samples, 3D
   face recognition technologies often have to work with smaller than
   desirable sample sizes. With the aim of enlarging the training number
   for each subject, we divide each training image into several patches.
   However, this immediately introduces two further problems for 3D models:
   high computational cost and dispersive features caused by the divided 3D
   image patches. We therefore first map 3D face images into 2D depth
   images, which greatly reduces the dimension of the samples. Though the
   depth images retain most of the robust features of 3D images, such as
   pose and illumination invariance, they lose many discriminative features
   of the original 3D samples. In this study, we propose a Bayesian
   learning framework to extract the discriminative features from the depth
   images. Specifically, we concentrate the features of the intra-class
   patches to a mean feature by maximizing the multivariate Gaussian
   likelihood function, and, simultaneously, enlarge the distances between
   the inter-class mean features by maximizing the exponential priori
   distribution of the mean features. For classification, we use the
   nearest neighbor classifier combined with the Mahalanobis distance to
   calculate the distance between the features of the test image and items
   in the training set. Experiments on two widely-used 3D face databases
   demonstrate the efficiency and accuracy of our proposed method compared
   to relevant state-of-the-art methods. Published by Elsevier Inc.}},
DOI = {{10.1016/j.ins.2015.03.063}},
ISSN = {{0020-0255}},
EISSN = {{1872-6291}},
Unique-ID = {{ISI:000359029900025}},
}

@article{ ISI:000363075300013,
Author = {Kankare, Ville and Liang, Xinlian and Vastaranta, Mikko and Yu, Xiaowei
   and Holopainen, Markus and Hyyppa, Juha},
Title = {{Diameter distribution estimation with laser scanning based multisource
   single tree inventory}},
Journal = {{ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING}},
Year = {{2015}},
Volume = {{108}},
Pages = {{161-171}},
Month = {{OCT}},
Abstract = {{Tree detection and tree species recognition are bottlenecks of the
   airborne remote sensing-based single tree inventories. The effect of
   these factors in forest attribute estimation can be reduced if airborne
   measurements are aided with tree mapping information that is collected
   from the ground. The main objective here was to demonstrate the use of
   terrestrial laser scanning-derived (TLS) tree maps in aiding airborne
   laser scanning-based (ALS) single tree inventory (multisource single
   tree inventory, MS-STI) and its capability in predicting diameter
   distribution in various forest conditions. Automatic measurement of TLS
   point clouds provided the tree maps and the required reference
   information from the tree attributes. The study area was located in Evo,
   Finland, and the reference data was acquired from 27 different sample
   plots with varying forest conditions. The workflow of MS-STI included:
   (1) creation of automatic tree map from TLS point clouds, (2) automatic
   diameter at breast height (DBH) measurement from TLS point clouds, (3)
   individual tree detection (ITD) based on ALS, (4) matching the ITD
   segments to the field-measured reference, (5) ALS point cloud metric
   extraction from the single tree segments and (6) DBH estimation based on
   the derived metrics. MS-STI proved to be accurate and efficient method
   for DBH estimation and predicting diameter distribution. The overall
   accuracy (root mean squared error, RMSE) of the DBH was 36.9 mm. Results
   showed that the DBH accuracy decreased if the tree density (trees/ha)
   increased. The highest accuracies were found in old-growth forests (tree
   densities less than 500 stems/ha). MS-STI resulted in the best
   accuracies regarding Norway spruce (Picea abies (L.) H.
   Karst.)-dominated forests (RMSE of 29.9 mm). Diameter distributions were
   predicted with low error indices, thereby resulting in a good fit
   compared to the reference. Based on the results, diameter distribution
   estimation with MS-STI is highly dependent on the forest structure and
   the accuracy of the tree maps that are used. The most important
   development step in the future for the MS-STI and automatic measurements
   of the TLS point cloud is to develop tree species recognition methods
   and further develop tree detection techniques. The possibility of using
   MLS or harvester data as a basis for the required tree maps should also
   be assessed in the future. (C) 2015 International Society for
   Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.isprsjprs.2015.07.007}},
ISSN = {{0924-2716}},
EISSN = {{1872-8235}},
ResearcherID-Numbers = {{Vastaranta, Mikko/K-9656-2018}},
ORCID-Numbers = {{Vastaranta, Mikko/0000-0001-6552-9122}},
Unique-ID = {{ISI:000363075300013}},
}

@article{ ISI:000361492600008,
Author = {Lee, Yong-Hwan and Kim, Cheong Ghil and Kim, Youngseop and Whangbo, Taeg
   Keun},
Title = {{Facial landmarks detection using improved active shape model on android
   platform}},
Journal = {{MULTIMEDIA TOOLS AND APPLICATIONS}},
Year = {{2015}},
Volume = {{74}},
Number = {{20}},
Pages = {{8821-8830}},
Month = {{OCT}},
Abstract = {{Detection of facial feature is fundamental for applications such as
   security, biometrics, 3D face modeling and personal authentication.
   Active Shape Model (ASM) is one of the most popular local texture models
   for face detection. This paper presents an issue related to face
   detection based on ASM, and proposes an efficient extraction algorithm
   for facial landmarks suitable for use on mobile devices. We modifies the
   original ASM to improve its performance with three changes; (1)
   Improving the initialization model using the center of the eyes by using
   a feature map of color information, (2) Constructing modified model
   definition and fitting more landmarks than the classical ASM, and (3)
   Extending and building a 2-D profile model for detecting faces in input
   image. The proposed method is evaluated on dataset containing over 700
   images of faces, and experimental results reveal that the proposed
   algorithm exhibited a significant improvement of over 10.2 \% in average
   success ratio, compared to the classic ASM, clearly outperforming on
   success rate and computing time.}},
DOI = {{10.1007/s11042-013-1565-y}},
ISSN = {{1380-7501}},
EISSN = {{1573-7721}},
Unique-ID = {{ISI:000361492600008}},
}

@article{ ISI:000360999400005,
Author = {Patil, Hemprasad and Kothari, Ashwin and Bhurchandi, Kishor},
Title = {{3-D face recognition: features, databases, algorithms and challenges}},
Journal = {{ARTIFICIAL INTELLIGENCE REVIEW}},
Year = {{2015}},
Volume = {{44}},
Number = {{3}},
Pages = {{393-441}},
Month = {{OCT}},
Abstract = {{Face recognition is being widely accepted as a biometric technique
   because of its non-intrusive nature. Despite extensive research on 2-D
   face recognition, it suffers from poor recognition rate due to pose,
   illumination, expression, ageing, makeup variations and occlusions. In
   recent years, the research focus has shifted toward face recognition
   using 3-D facial surface and shape which represent more discriminating
   features by the virtue of increased dimensionality. This paper presents
   an extensive survey of recent 3-D face recognition techniques in terms
   of feature detection, classifiers as well as published algorithms that
   address expression and occlusion variation challenges followed by our
   critical comments on the published work. It also summarizes remarkable
   3-D face databases and their features used for performance evaluation.
   Finally we suggest vital steps of a robust 3-D face recognition system
   based on the surveyed work and identify a few possible directions for
   research in this area.}},
DOI = {{10.1007/s10462-015-9431-0}},
ISSN = {{0269-2821}},
EISSN = {{1573-7462}},
Unique-ID = {{ISI:000360999400005}},
}

@article{ ISI:000357910700010,
Author = {Li, Xiaoli and Ruan, Qiuqi and An, Gaoyun and Jin, Yi and Zhao, Ruizhen},
Title = {{Multiple strategies to enhance automatic 3D facial expression
   recognition}},
Journal = {{NEUROCOMPUTING}},
Year = {{2015}},
Volume = {{161}},
Pages = {{89-98}},
Month = {{AUG 5}},
Abstract = {{The research on 3D facial expression recognition has attracted numbers
   of interests due to its superiority to 2D data and it has been greatly
   promoted in recent years. However, its performance needs to be further
   improved and its data structure needs to be further analyzed to keep its
   automation well as the mesh structure of 3D face models cannot be
   applied directly to algebraic operations. This paper addresses these
   problems with multiple strategies, so that 3D facial expression
   recognition can be automatically implemented and its performance is
   subsequently enhanced. Firstly, an image-like-structure is proposed to
   represent the 3D face models, so that algebraic operations can be
   directly applied to analyze 3D data. Based on this image-like-structure,
   the strategies of irregular division schemes and the entropy weighted
   blocks are employed to improve the recognition accuracy. The former aims
   to keep the integrity of local structure; the latter is employed to
   emphasize the contribution of different facial regions. Both of them can
   be separately or jointly, utilized to facial feature descriptors. With
   the remarkable experimental results based on LBP and LIP, we can
   conclude that these strategies are available to promote the performance
   of automatic 3D facial expression recognition, which draws a promising
   direction for automatic 3D facial expression recognition. (C) 2015
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2015.02.063}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000357910700010}},
}

@article{ ISI:000367122800020,
Author = {Ratyal, Naeem Iqbal and Taj, Imtiaz Ahmad and Bajwa, Usama Ijaz and
   Sajid, Muhammad},
Title = {{3D face recognition based on pose and expression invariant alignment}},
Journal = {{COMPUTERS \& ELECTRICAL ENGINEERING}},
Year = {{2015}},
Volume = {{46}},
Pages = {{241-255}},
Month = {{AUG}},
Abstract = {{In this paper we present a novel pose and expression invariant approach
   for 3D face registration based on intrinsic coordinate system
   characterized by nose tip, horizontal nose plane and vertical symmetry
   plane of the face. It is observed that distance of nose tip from 3D
   scanner is reduced after pose correction which is presented as a
   quantifying heuristic for proposed registration scheme. In addition,
   motivated by the fact that a single classifier cannot be generally
   efficient against all face regions, a two tier ensemble classifier based
   3D face recognition approach is presented which employs Principal
   Component Analysis (PCA) for feature extraction and Mahalanobis Cosine
   (MahCos) matching score for classification of facial regions with
   weighted Borda Count (WBC) based combination and a re-ranking stage. The
   performance of proposed approach is corroborated by extensive
   experiments performed on two databases: GavabDB and FRGC v2.0,
   confirming effectiveness of fusion strategies to improve performance.
   (C) 2015 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.compeleceng.2015.06.007}},
ISSN = {{0045-7906}},
EISSN = {{1879-0755}},
Unique-ID = {{ISI:000367122800020}},
}

@article{ ISI:000358417700010,
Author = {Moeini, Ali and Faez, Karim and Moeini, Hossein},
Title = {{Real-world gender classification via local Gabor binary pattern and
   three-dimensional face reconstruction by generic elastic model}},
Journal = {{IET IMAGE PROCESSING}},
Year = {{2015}},
Volume = {{9}},
Number = {{8}},
Pages = {{690-698}},
Month = {{AUG}},
Abstract = {{In this study, a novel method is proposed for gender classification by
   adding facial depth features to texture features. Accordingly, the
   three-dimensional (3D) generic elastic model is used to reconstruct the
   3D model from human face using only a single 2D frontal image. Then, the
   texture and depth are extracted from the reconstructed face model.
   Afterwards, the local Gabor binary pattern (LGBP) is applied to both
   facial texture and reconstructed depth to extract the feature vectors
   from both texture and reconstructed depth images. Finally, by combining
   2D and 3D feature vectors, the final LGBP histogram bins are generated
   and classified by the support vector machine. Favourable outcomes are
   acquired for gender classification on the labelled faces in the wild and
   FERET databases based on the proposed method compared to several
   state-of-the-arts in gender classification.}},
DOI = {{10.1049/iet-ipr.2014.0733}},
ISSN = {{1751-9659}},
EISSN = {{1751-9667}},
ResearcherID-Numbers = {{faez, karim/K-5117-2019}},
ORCID-Numbers = {{faez, karim/0000-0002-1159-4866}},
Unique-ID = {{ISI:000358417700010}},
}

@article{ ISI:000357545400014,
Author = {Schmitt, Michael and Shahzad, Muhammad and Zhu, Xiao Xiang},
Title = {{Reconstruction of individual trees from multi-aspect TomoSAR data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2015}},
Volume = {{165}},
Pages = {{175-185}},
Month = {{AUG}},
Abstract = {{The localization and reconstruction of individual trees as well as the
   extraction of their geometrical parameters is an important field of
   research in both forestry and remote sensing. While the current
   state-of-the-art mostly focuses on the exploitation of optical imagery
   and airborne LiDAR data, modern SAR sensors have not yet met the
   interest of the research community in that regard. This paper presents a
   prototypical processing chain for the reconstruction of individual
   deciduous trees: First, single-pass multi-baseline InSAR data acquired
   from multiple aspect angles are used for the generation of a layover-
   and shadow-free 3D point cloud by tomographic SAR processing. The
   resulting point cloud is then segmented by unsupervised mean shift
   clustering, before ellipsoid models are fitted to the points of each
   cluster. From these 3D ellipsoids the relevant geometrical tree
   parameters are extracted. Evaluation with respect to a manually derived
   reference dataset prove that almost 74\% of all trees are successfully
   segmented and reconstructed, thus providing a promising perspective for
   further research toward individual tree recognition from SAR data. (C)
   2015 The Authors. Published by Elsevier Inc This is an open access
   article under the CC BY-NC-ND license.}},
DOI = {{10.1016/j.rse.2015.05.012}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000357545400014}},
}

@article{ ISI:000356107200001,
Author = {Wang, Chao and Cho, Yong K. and Kim, Changwan},
Title = {{Automatic BIM component extraction from point clouds of existing
   buildings for sustainability applications}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2015}},
Volume = {{56}},
Pages = {{1-13}},
Month = {{AUG}},
Abstract = {{Building information models (BIMs) are increasingly being applied
   throughout a building's lifecycle for various applications, such as
   progressive construction monitoring and defect detection, building
   renovation, energy simulation, and building system analysis in the
   Architectural, Engineering, Construction, and Facility Management
   (AEC/FM) domains. In conventional approaches, as-is BIM is primarily
   manually created from point clouds, which is labor-intensive, costly,
   and time consuming. This paper proposes a method for automatically
   extracting building geometries from unorganized point clouds. The
   collected raw data undergo data downsizing, boundary detection, and
   building component categorization, resulting in the building components
   being recognized as individual objects and their visualization as
   polygons. The results of tests conducted on three collected as-is
   building data to validate the technical feasibility and evaluate the
   performance of the proposed method indicate that it can simplify and
   accelerate the as-is building model from the point cloud creation
   process. (C) 2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.autcon.2015.04.001}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
ResearcherID-Numbers = {{Jeong, Yongwook/N-7413-2016}},
Unique-ID = {{ISI:000356107200001}},
}

@article{ ISI:000362485300007,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Title = {{Register-My-Face: a tool to register three-dimensional face images}},
Journal = {{JOURNAL OF ELECTRONIC IMAGING}},
Year = {{2015}},
Volume = {{24}},
Number = {{4}},
Month = {{JUL}},
Abstract = {{Efficient registration across pose is the most challenging research area
   for accurate recognition of human face images. Authors have discussed a
   tool that has been developed for registration of face images across
   poses using the nose tip of the face images. The nose tip has been
   considered here because of its stability in situations such as
   variations in pose, expression, etc. The aim of this investigation is to
   develop a face registration tool called ``Register-My-Face{''} with a
   working methodology in all the three directions, namely yaw, pitch, and
   roll. This tool has been developed for three-dimensional (3-D) face
   registration, which is inspired by analyzing the ``depth values{''} of
   face range images. The registration of the face is done using a
   geometrical technique which is based on computing the corresponding
   rotation in three orthogonal directions. The advantages of the designed
   tool are that it does not need any training phase for accurate detection
   of the nose tip, and this method can handle large pose variations,
   including 90 deg pose variations about the Y-axis in both the positive
   and negative directions. The method that has been followed to develop
   this tool is also independent of facial expression, occlusion and
   illumination variations. Moreover, it quickly detects the nose tip
   because it does not need to process the entire face surface, but only
   requires the isolated nose region. The tool has been integrated with
   three different databases; GavabDB, Bosphorus, and Frav3D, and the
   investigation highlights the robustness of the tool. Additionally, for
   exploring the performance of the tool, a SIMULINK model for hardware
   interface is also developed with a discrete solver and is tested on two
   different configuration setups and executed in two different execution
   modes with two simulation stop timings 10.0 and 1.0. This model can
   proceed according to the algorithm with a minimum of 6.640 s to register
   an unregistered raw 3-D face scan input image from the Frav3D database.
   Accuracies of the nose region of 98.87\%, 94.44\%, and 98.08\% for the
   Frav3D, GavabDB, and Bosphorus databases, respectively, are observed.
   For nose tip detection, the success rates are 98.91\% for the Frav3D
   database, 98.74\% for the GavabDB database, and 96.03\% for the
   Bosphorus database. Based on the success rate of nose tip detection, the
   registration process is implemented on three databases. Registration
   accuracy, computed between a neutral and the registered range face image
   for the Frav3D database is 87.5\% for GavabDB and 89.87\% for Bosphorus
   database, and the rate of success is 70.23\%. (C) 2015 SPIE and IS\&T}},
DOI = {{10.1117/1.JEI.24.4.043007}},
Article-Number = {{043007}},
ISSN = {{1017-9909}},
EISSN = {{1560-229X}},
ResearcherID-Numbers = {{Bhattacharjee, Debotosh/L-8521-2015
   }},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-4483-706X
   Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000362485300007}},
}

@article{ ISI:000354988900026,
Author = {Zhang Cheng and Gu Yu-zhang and Hu Ke-li and Wang Ying-guan},
Title = {{Face recognition using SIFT features under 3D meshes}},
Journal = {{JOURNAL OF CENTRAL SOUTH UNIVERSITY}},
Year = {{2015}},
Volume = {{22}},
Number = {{5}},
Pages = {{1817-1825}},
Month = {{MAY}},
Abstract = {{Expression, occlusion, and pose variations are three main challenges for
   3D face recognition. A novel method is presented to address 3D face
   recognition using scale-invariant feature transform (SIFT) features on
   3D meshes. After preprocessing, shape index extrema on the 3D facial
   surface are selected as keypoints in the difference scale space and the
   unstable keypoints are removed after two screening steps. Then, a local
   coordinate system for each keypoint is established by principal
   component analysis (PCA). Next, two local geometric features are
   extracted around each keypoint through the local coordinate system.
   Additionally, the features are augmented by the symmetrization according
   to the approximate left-right symmetry in human face. The proposed
   method is evaluated on the Bosphorus, BU-3DFE, and Gavab databases,
   respectively. Good results are achieved on these three datasets. As a
   result, the proposed method proves robust to facial expression
   variations, partial external occlusions and large pose changes.}},
DOI = {{10.1007/s11771-015-2700-x}},
ISSN = {{2095-2899}},
EISSN = {{1993-0666}},
ORCID-Numbers = {{Hu, Keli/0000-0002-5628-7640}},
Unique-ID = {{ISI:000354988900026}},
}

@article{ ISI:000353891500010,
Author = {Kashani, Alireza G. and Crawford, Patrick S. and Biswas, Sufal K. and
   Graettinger, Andrew J. and Grau, David},
Title = {{Automated Tornado Damage Assessment and Wind Speed Estimation Based on
   Terrestrial Laser Scanning}},
Journal = {{JOURNAL OF COMPUTING IN CIVIL ENGINEERING}},
Year = {{2015}},
Volume = {{29}},
Number = {{3}},
Month = {{MAY}},
Abstract = {{There are more than 1,000 tornadoes in the United States each year, yet
   engineers do not typically design for tornadoes because of insufficient
   information about wind loads. Collecting building-level damage data in
   the aftermath of tornadoes can improve the understanding of tornado
   winds, but these data are difficult to collect because of safety, time,
   and access constraints. This study presents and tests an automated
   geographic information system (GIS) method using postevent point cloud
   data collected by terrestrial scanners and preevent aerial images to
   calculate the percentage of roof and wall damage and estimate wind
   speeds at an individual building scale. Simulations determined that for
   typical point cloud density (>25points/m2), a GIS raster cell size of
   40-50cm resulted in less than 10\% error in damaged roof and wall
   detection. Data collected after recent tornadoes were used to correlate
   wind speed estimates and the percent of detected damage. The developed
   method estimated wind speeds from damage data collected after the 2011
   Tuscaloosa, AL tornado at finer scales than the typical large-scale
   assessments done by reconnaissance engineers.}},
DOI = {{10.1061/(ASCE)CP.1943-5487.0000389}},
Article-Number = {{04014051}},
ISSN = {{0887-3801}},
EISSN = {{1943-5487}},
Unique-ID = {{ISI:000353891500010}},
}

@article{ ISI:000353152700010,
Author = {Yu, Jun and Wang, Zeng-Fu},
Title = {{A Video, Text, and Speech-Driven Realistic 3-D Virtual Head for
   Human-Machine Interface}},
Journal = {{IEEE TRANSACTIONS ON CYBERNETICS}},
Year = {{2015}},
Volume = {{45}},
Number = {{5}},
Pages = {{977-988}},
Month = {{MAY}},
Abstract = {{A multiple inputs-driven realistic facial animation system based on 3-D
   virtual head for human-machine interface is proposed. The system can be
   driven independently by video, text, and speech, thus can interact with
   humans through diverse interfaces. The combination of parameterized
   model and muscular model is used to obtain a tradeoff between
   computational efficiency and high realism of 3-D facial animation. The
   online appearance model is used to track 3-D facial motion from video in
   the framework of particle filtering, and multiple measurements, i.e.,
   pixel color value of input image and Gabor wavelet coefficient of
   illumination ratio image, are infused to reduce the influence of
   lighting and person dependence for the construction of online appearance
   model. The tri-phone model is used to reduce the computational
   consumption of visual co-articulation in speech synchronized viseme
   synthesis without sacrificing any performance. The objective and
   subjective experiments show that the system is suitable for
   human-machine interaction.}},
DOI = {{10.1109/TCYB.2014.2341737}},
ISSN = {{2168-2267}},
EISSN = {{2168-2275}},
Unique-ID = {{ISI:000353152700010}},
}

@article{ ISI:000352441700012,
Author = {Urbanova, Petra and Hejna, Petr and Jurda, Mikolas},
Title = {{Testing photogrammetry-based techniques for three-dimensional surface
   documentation in forensic pathology}},
Journal = {{FORENSIC SCIENCE INTERNATIONAL}},
Year = {{2015}},
Volume = {{250}},
Pages = {{77-86}},
Month = {{MAY}},
Abstract = {{Three-dimensional surface technologies particularly close range
   photogrammetry and optical surface scanning have recently advanced into
   affordable, flexible and accurate techniques. Forensic postmortem
   investigation as performed on a daily basis, however, has not yet fully
   benefited from their potentials. In the present paper, we tested two
   approaches to 3D external body documentation - digital camera-based
   photogrammetry combined with commercial Agisoft PhotoScan (R) software
   and stereophotogrammetry-based Vectra H1 (R), a portable handheld
   surface scanner. In order to conduct the study three human subjects were
   selected, a living person, a 25-year-old female, and two forensic cases
   admitted for postmortem examination at the Department of Forensic
   Medicine, Hradec Kralove, Czech Republic (both 63-year-old males), one
   dead to traumatic, self-inflicted, injuries (suicide by hanging), the
   other diagnosed with the heart failure.
   All three cases were photographed in 3608 manner with a Nikon 7000
   digital camera and simultaneously documented with the handheld scanner.
   In addition to having recorded the pre-autopsy phase of the forensic
   cases, both techniques were employed in various stages of autopsy. The
   sets of collected digital images (approximately 100 per case) were
   further processed to generate point clouds and 3D meshes. Final 3D
   models (a pair per individual) were counted for numbers of points and
   polygons, then assessed visually and compared quantitatively using ICP
   alignment algorithm and a cloud point comparison technique based on
   closest point to point distances.
   Both techniques were proven to be easy to handle and equally laborious.
   While collecting the images at autopsy took around 20 min, the
   post-processing was much more time-demanding and required up to 10 h of
   computation time. Moreover, for the full-body scanning the
   post-processing of the handheld scanner required rather time-consuming
   manual image alignment. In all instances the applied approaches produced
   high-resolution photorealistic, real sized or easy to calibrate 3D
   surface models. Both methods equally failed when the scanned body
   surface was covered with body hair or reflective moist areas. Still, it
   can be concluded that single camera close range photogrammetry and
   optical surface scanning using Vectra H1 scanner represent relatively
   low-cost solutions which were shown to be beneficial for postmortem body
   documentation in forensic pathology. (C) 2015 Elsevier Ireland Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.forsciint.2015.03.005}},
ISSN = {{0379-0738}},
EISSN = {{1872-6283}},
ORCID-Numbers = {{Urbanova, Petra/0000-0001-9321-3360}},
Unique-ID = {{ISI:000352441700012}},
}

@article{ ISI:000352534300007,
Author = {Moeini, Ali and Moeini, Hossein},
Title = {{Real-World and Rapid Face Recognition Toward Pose and Expression
   Variations via Feature Library Matrix}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY}},
Year = {{2015}},
Volume = {{10}},
Number = {{5}},
Pages = {{969-984}},
Month = {{MAY}},
Abstract = {{In this paper, a novel method for face recognition under pose and
   expression variations is proposed from only a single image in the
   gallery. A 3D probabilistic facial expression recognition generic
   elastic model is proposed to reconstruct a 3D model from real-world
   human face using only a single 2D frontal image with/without facial
   expressions. Then, a feature library matrix (FLM) is generated for each
   subject in the gallery from all face poses by rotating the 3D
   reconstructed models and extracting features in the rotated face pose.
   Therefore, each FLM is subsequently rendered for each subject in the
   gallery based on triplet angles of face poses. In addition, before
   matching the FLM, an initial estimate of triplet angles is obtained from
   the face pose in probe images using an automatic head pose estimation
   approach. Then, an array of the FLM is selected for each subject based
   on the estimated triplet angles. Finally, the selected arrays from FLMs
   are compared with extracted features from the probe image by iterative
   scoring classification using the support vector machine. Convincing
   results are acquired to handle pose and expression changes on the
   Bosphorus, Face Recognition Technology (FERET), Carnegie Mellon
   University-Pose, Illumination, and Expression (CMU-PIE), and Labeled
   Faces in the Wild (LFW) face databases compared with several
   state-of-the-art methods in pose-invariant face recognition. The
   proposed method not only demonstrates an excellent performance by
   obtaining high accuracy on all four databases but also outperforms other
   approaches realistically.}},
DOI = {{10.1109/TIFS.2015.2393553}},
ISSN = {{1556-6013}},
EISSN = {{1556-6021}},
Unique-ID = {{ISI:000352534300007}},
}

@article{ ISI:000353332000012,
Author = {Yi, Jizheng and Mao, Xia and Chen, Lijiang and Xue, Yuli and Rovetta,
   Alberto and Caleanu, Catalin-Daniel},
Title = {{Illumination Normalization of Face Image Based on Illuminant Direction
   Estimation and Improved Retinex}},
Journal = {{PLOS ONE}},
Year = {{2015}},
Volume = {{10}},
Number = {{4}},
Month = {{APR 23}},
Abstract = {{Illumination normalization of face image for face recognition and facial
   expression recognition is one of the most frequent and difficult
   problems in image processing. In order to obtain a face image with
   normal illumination, our method firstly divides the input face image
   into sixteen local regions and calculates the edge level percentage in
   each of them. Secondly, three local regions, which meet the requirements
   of lower complexity and larger average gray value, are selected to
   calculate the final illuminant direction according to the error function
   between the measured intensity and the calculated intensity, and the
   constraint function for an infinite light source model. After knowing
   the final illuminant direction of the input face image, the Retinex
   algorithm is improved from two aspects: (1) we optimize the surround
   function; (2) we intercept the values in both ends of histogram of face
   image, determine the range of gray levels, and stretch the range of gray
   levels into the dynamic range of display device. Finally, we achieve
   illumination normalization and get the final face image. Unlike previous
   illumination normalization approaches, the method proposed in this paper
   does not require any training step or any knowledge of 3D face and
   reflective surface model. The experimental results using extended Yale
   face database B and CMU-PIE show that our method achieves better
   normalization effect comparing with the existing techniques.}},
DOI = {{10.1371/journal.pone.0122200}},
Article-Number = {{e0122200}},
ISSN = {{1932-6203}},
Unique-ID = {{ISI:000353332000012}},
}

@article{ ISI:000349271500019,
Author = {Azazi, Amal and Lotfi, Syaheerah Lebai and Venkat, Ibrahim and
   Fernandez-Martinez, Fernando},
Title = {{Towards a robust affect recognition: Automatic facial expression
   recognition in 3D faces}},
Journal = {{EXPERT SYSTEMS WITH APPLICATIONS}},
Year = {{2015}},
Volume = {{42}},
Number = {{6}},
Pages = {{3056-3066}},
Month = {{APR 15}},
Abstract = {{Facial expressions are a powerful tool that communicates a person's
   emotional state and subsequently his/her intentions. Compared to 2D face
   images, 3D face images offer more granular cues that are not available
   in the 2D images. However, one major setback of 3D faces is that they
   impose a higher dimensionality than 2D faces. In this paper, we attempt
   to address this problem by proposing a fully automatic 3D facial
   expression recognition model that tackles the high dimensionality
   problem in a twofold solution. First, we transform the 3D faces into the
   2D plane using conformal mapping. Second, we propose a Differential
   Evolution (DE) based optimization algorithm to select the optimal facial
   feature set and the classifier parameters simultaneously. The optimal
   features are selected from a pool of Speed Up Robust Features (SURF)
   descriptors of all the prospective facial points. The proposed model
   yielded an average recognition accuracy of 79\% using the Bosphorus
   database and 79.36\% using the BU-3DFE database. In addition, we exploit
   the facial muscular movements to enhance the probability estimation (PE)
   of Support Vector Machine (SVM). Joint application of feature selection
   with the proposed enhanced PE (EPE) yielded an average recognition
   accuracy of 84\% using the Bosphorus database and 85.81\% using the
   BU-3DFE database, which is statistically significantly better (at p <
   0.01 and p < 0.001, respectively) if compared to the individual exploit
   of the optimal features only. (C) 2014 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.eswa.2014.10.042}},
ISSN = {{0957-4174}},
EISSN = {{1873-6793}},
ResearcherID-Numbers = {{Fernandez-Martinez, Fernando/M-2935-2014
   }},
ORCID-Numbers = {{Fernandez-Martinez, Fernando/0000-0003-3877-0089
   Lebai Lutfi, Syaheerah/0000-0001-7349-0061}},
Unique-ID = {{ISI:000349271500019}},
}

@article{ ISI:000218884500001,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Title = {{Illumination, Pose and Occlusion Invariant Face Recognition from Range
   Images Using ERFI Model}},
Journal = {{INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS}},
Year = {{2015}},
Volume = {{4}},
Number = {{2}},
Pages = {{1-20}},
Month = {{APR-JUN}},
Abstract = {{In this paper the pivotal contribution of the authors is to recognize
   the 3D face images from range images in the unconstrained environment
   i.e. under varying illumination, pose as well as occlusion that are
   considered to be the most challenging task in the domain of face
   recognition. During this investigation, face images have been normalized
   in terms of pose registration as well as occlusion restoration using
   ERFI (Energy Range Face Image) model. 3D face images are inherently
   illumination invariant due its point-based representation of data along
   three axes. Here, other than quantitative analysis, a subjective
   analysis is also carried out. However, synthesized datasets have been
   accomplished to investigate the performance of recognition rate from
   Frav3D and Bosphorus databases using SIFT and SURF like features.
   Moreover, weighted fusion of these individual feature sets is also done.
   Later these feature sets have been classified by K-NN and Sequence
   Matching Technique and achieved maximum recognition rates of 99.17\% and
   98.81\% for Frav3D and GavabDB databases respectively.}},
DOI = {{10.4018/ijsda.2015040101}},
ISSN = {{2160-9772}},
EISSN = {{2160-9799}},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-1163-6413
   Bhattacharjee, Debotosh/0000-0002-4483-706X}},
Unique-ID = {{ISI:000218884500001}},
}

@article{ ISI:000348880300019,
Author = {Elaiwat, S. and Bennamoun, M. and Boussaid, F. and El-Sallam, A.},
Title = {{A Curve let-based approach for textured 3D face recognition}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{48}},
Number = {{4}},
Pages = {{1235-1246}},
Month = {{APR}},
Abstract = {{In this paper, we present a fully automated multimodal Curvelet-based
   approach for textured 3D face recognition. The proposed approach relies
   on a novel multimodal keypoint detector capable of repeatably
   identifying keypoints on textured 3D face surfaces. Unique local surface
   descriptors are then constructed around each detected keypoint by
   integrating Curvelet elements of different orientations, resulting in
   highly descriptive rotation invariant features. Unlike previously
   reported Curvelet-based face recognition algorithms which extract global
   features from textured faces only, our algorithm extracts both texture
   and 3D local features. In addition, this is achieved across a number of
   frequency bands to achieve robust and accurate recognition under varying
   illumination conditions and facial expressions. The proposed algorithm
   was evaluated using three well-known and challenging datasets, namely
   FRGC v2, BU-3DFE and Bosphorus datasets. Reported results show superior
   performance compared to prior art, with 99.2\%, 95.1\% and 91\%
   verification rates at 0.001 FAR for FRGC v2, BU-3DFE and Bosphorus
   datasets, respectively. (C) 2014 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2014.10.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ORCID-Numbers = {{Bennamoun, Mohammed/0000-0002-6603-3257}},
Unique-ID = {{ISI:000348880300019}},
}

@article{ ISI:000351796000002,
Author = {Ming, Yue},
Title = {{Robust regional bounding spherical descriptor for 3D face recognition
   and emotion analysis}},
Journal = {{IMAGE AND VISION COMPUTING}},
Year = {{2015}},
Volume = {{35}},
Pages = {{14-22}},
Month = {{MAR}},
Abstract = {{3D face recognition and emotion analysis play important roles in many
   fields of communication and edutainment An effective facial descriptor,
   with higher discriminating capability for face recognition and higher
   descriptiveness for facial emotion analysis, is a challenging issue.
   However, in the practical applications, the descriptiveness and
   discrimination are independent and contradictory to each other. 3D
   facial data provide a promising way to balance these two aspects. In
   this paper, a robust regional bounding spherical descriptor (RBSR) is
   proposed to facilitate 3D face recognition and emotion analysis. In our
   framework, we first segment a group of regions on each 3D facial point
   cloud by shape index and spherical bands on the human face. Then the
   corresponding facial areas are projected to regional bounding spheres to
   obtain our regional descriptor. Finally, a regional and global
   regression mapping (RGRM) technique is employed to the weighted regional
   descriptor for boosting the classification accuracy. Three largest
   available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the
   performance comparison and the experimental results show a consistently
   better performance for 3D face recognition and emotion analysis. (C)
   2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.imavis.2014.12.003}},
ISSN = {{0262-8856}},
EISSN = {{1872-8138}},
Unique-ID = {{ISI:000351796000002}},
}

@article{ ISI:000351134600001,
Author = {Mengesha, Taye and Hawkins, Michael and Nieuwenhuis, Maarten},
Title = {{Validation of terrestrial laser scanning data using conventional forest
   inventory methods}},
Journal = {{EUROPEAN JOURNAL OF FOREST RESEARCH}},
Year = {{2015}},
Volume = {{134}},
Number = {{2}},
Pages = {{211-222}},
Month = {{MAR}},
Abstract = {{The application of terrestrial laser scanning (TLS) in capturing forest
   inventory parameters such as diameter at breast height, height and
   diameters along stem profiles, and in monitoring forest growth, was
   investigated and validated by comparison with conventionally measured
   individual tree parameters and plot-level forest growth in a stand of
   Sitka spruce (Picea sitchensis (Bong.) Carr.) in Ireland. The data
   acquisition for all the plots with different tree sizes and different
   slopes was carried out using a terrestrial laser scanner (FARO LS 800
   HE80) in November 2007 and November 2009, using the same plot centres
   and measurement procedures. The point cloud data were processed with
   Autostem (TM) software. The results showed that TLS enables the
   acquisition of forest stand parameters with an acceptable accuracy.
   Pruning of the lower branches did not improve tree recognition and the
   number of (partly) occluded trees stayed the same. Over the 2-year
   period, the average difference between the volume increment of the trees
   visible to the scanner derived using the conventional method and
   Autostem (TM) was 4.77 m(3) ha(-1) and resulted in scanner-derived
   estimates that were lower than the estimates obtained by conventional
   method by 6.1 \%. Using a simple correction factor to account for
   occlusion in the laser scanner data, the difference between these
   estimates for all trees in the stand became an over-estimation by 6.96
   m(3) ha(-1) (8.1 \%). At heights up along the stems > 15 m, the errors
   in stem diameter estimates started to escalate.}},
DOI = {{10.1007/s10342-014-0844-0}},
ISSN = {{1612-4669}},
EISSN = {{1612-4677}},
Unique-ID = {{ISI:000351134600001}},
}

@article{ ISI:000347747000011,
Author = {Li, Dong and Lam, Kin-Man},
Title = {{Design and learn distinctive features from pore-scale facial keypoints}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{48}},
Number = {{3}},
Pages = {{732-745}},
Month = {{MAR}},
Abstract = {{Establishing correct correspondences between two faces with different
   viewpoints has played an important role in 3D face reconstruction and
   other computer-vision applications. Usually, face images are considered
   to lack sufficient distinctive features to establish a large number of
   correspondences on uncalibrated images. In this paper, we investigate
   pore-scale facial features, which are formed from pores, fine wrinkles,
   and hair. These features have many characteristics that make them
   suitable for matching facial images under different variations. Using
   both biological observation and computervision consideration, a new
   framework is devised for pore-scale facial-feature extraction and
   matching. The matching difficulty under various skin appearances of
   different subjects and imaging distortion is also analyzed. For further
   improving the matching performance and tackling distortions such as
   varying illuminations and unfocused blurring, a pore-to-pore
   correspondences dataset is established for training a more distinctive
   and compact descriptor. Experiments are conducted on a face database
   containing 105 subjects, and the results prove that the pore-scale
   features are highly distinctive; face images with a minimum resolution
   of 600 x 700 (0.4 mega) pixels contain sufficient details to perform a
   reliable matching in different poses. Generally, our algorithm can
   establish between 500 and 2000 correct correspondences on a pair of
   uncalibrated face images of the same person. Furthermore, the proposed
   methods can be applied to face recognition, 3D reconstruction, etc. (C)
   2014 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2014.09.026}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000347747000011}},
}

@article{ ISI:000347747000012,
Author = {Xia, Baigiang and Ben Amor, Boulbaba and Drira, Hassen and Daoudi,
   Mohamed and Ballihi, Lahoucine},
Title = {{Combining face averageness and symmetry for 3D-based gender
   classification}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{48}},
Number = {{3}},
Pages = {{746-758}},
Month = {{MAR}},
Abstract = {{Although human face averageness and symmetry are valuable clues in
   social perception (such as attractiveness, masculinity/femininity, and
   healthy/ sick), in the literature of facial attribute recognition,
   little consideration has been given to them. In this work, we propose to
   study the morphological differences between male and female faces by
   analyzing the averageness and symmetry of their 3D shapes. In
   particular, we address the following questions: (i) is there any
   relationship between gender and face averageness/symmetry? and (ii) if
   this relationship exists, which specific areas on the face are involved?
   To this end, we propose first to capture densely both the face shape
   averageness (AVE) and symmetry (SYM) using our Dense Scalar Field (DSF),
   which denotes the shooting directions of geodesics between facial
   shapes. Then, we explore such representations by using classical machine
   learning techniques, the Feature Selection (FS) methods and Random
   Forest (RF) classification algorithm. Experiments conducted on the
   FRGCv2 dataset show that a significant relationship exists between
   gender and facial averageness/symmetry when achieving a classification
   rate of 93.7\% on the 466 earliest scans of subjects (mainly neutral)
   and 92.4\% on the whole FRGCv2 dataset (including facial expressions).
   (C) 2014 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2014.09.021}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
ResearcherID-Numbers = {{Amor, Boulbaba Ben/K-7066-2018
   Daoudi, Mohammed/H-5935-2013}},
ORCID-Numbers = {{Amor, Boulbaba Ben/0000-0002-4176-9305
   Drira, Hassen/0000-0003-1052-4353
   Daoudi, Mohammed/0000-0003-4219-7860}},
Unique-ID = {{ISI:000347747000012}},
}

@article{ ISI:000346542300028,
Author = {Li, Xiaoli and Ruan, Qiuqi and Jin, Yi and An, Gaoyun and Zhao, Ruizhen},
Title = {{Fully automatic 3D facial expression recognition using polytypic
   multi-block local binary patterns}},
Journal = {{SIGNAL PROCESSING}},
Year = {{2015}},
Volume = {{108}},
Pages = {{297-308}},
Month = {{MAR}},
Abstract = {{3D facial expression recognition has been greatly promoted for
   overcoming the inherent drawbacks of 2D facial expression recognition
   and has achieved superior recognition accuracy to the 2D. In this paper,
   a novel holistic, full-automatic approach for 3D facial expression
   recognition is proposed. First, 3D face models are represented in
   2D-image-like structure which makes it possible to take advantage of the
   wealth of 2D methods to analyze 3D models. Then an enhanced facial
   representation, namely polytypic multi-block local binary patterns
   (P-MLBP), is proposed. The P-MLBP involves both the feature-based
   irregular divisions to depict the facial expressions accurately and the
   fusion of depth and texture information of 3D models to enhance the
   facial feature. Based on the BU-3DFE database, three kinds of
   classifiers are employed to conduct 3D facial expression recognition for
   evaluation. Their experimental results outperform the state of the art
   and show the effectiveness of P-MLBP for 3D facial expression
   recognition. Therefore, the proposed strategy is validated for 3D facial
   expression recognition; and its simplicity opens a promising direction
   for fully automatic 3D facial expression recognition. (C) 2014 Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.sigpro.2014.09.033}},
ISSN = {{0165-1684}},
EISSN = {{1879-2677}},
Unique-ID = {{ISI:000346542300028}},
}

@article{ ISI:000356105100042,
Author = {Zhang, Jian and Tao, Dapeng and Bian, Xiangjuan and Zhan, Xiaosi},
Title = {{Monocular face reconstruction with global and local shape constraints}},
Journal = {{NEUROCOMPUTING}},
Year = {{2015}},
Volume = {{149}},
Number = {{C}},
Pages = {{1535-1543}},
Month = {{FEB 3}},
Abstract = {{To reconstruct 3D face from single monocular image, this paper proposes
   an approach which comprises three steps. First, a set of 3D facial
   features is recovered from 2D features extracted from the image. The
   features are recovered by solving equations derived from a regularized
   scaled orthogonal projection. The regularization is achieved by a global
   shape constraint exploiting a prior reference 3D facial shape. Second,
   we warp a high-resolution reference 3D face, using both recovered 3D
   features and local shape constraint at each model points. Last,
   realistic 3D face is obtained through texture synthesis. Compared with
   existing approach, the proposed feature recovery method has higher
   accuracy, and it is robust to facial pose variation appeared on the
   given image. Moreover, the model warping method based on local shape
   constraints can warp a high-resolution reference 3D face using few 3D
   features more reasonably and accurately. The proposed approach generates
   realistic 3D face with impressive visual effect. (C) 2014 Elsevier B.V.
   All rights reserved.}},
DOI = {{10.1016/j.neucom.2014.08.039}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000356105100042}},
}

@article{ ISI:000349588900008,
Author = {Bolkart, Timo and Wuhrer, Stefanie},
Title = {{3D faces in motion: Fully automatic registration and statistical
   analysis}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2015}},
Volume = {{131}},
Pages = {{100-115}},
Month = {{FEB}},
Abstract = {{This paper presents a representation of 3D facial motion sequences that
   allows performing statistical analysis of 3D face shapes in motion. The
   resulting statistical analysis is applied to automatically generate
   realistic facial animations and to recognize dynamic facial expressions.
   To perform statistical analysis of 3D facial shapes in motion over
   different subjects and different motion sequences, a large database of
   motion sequences needs to be brought in full correspondence. Existing
   algorithms that compute correspondences between 3D facial motion
   sequences either require manual input or suffer from instabilities
   caused by drift. For large databases, algorithms that require manual
   interaction are not practical. We propose an approach to robustly
   compute correspondences between a large set of facial motion sequences
   in a fully automatic way using a multilinear model as statistical prior.
   In order to register the motion sequences, a good initialization is
   needed. We obtain this initialization by introducing a landmark
   prediction method for 3D motion sequences based on Markov Random Fields.
   Using this motion sequence registration, we find a compact
   representation of each motion sequence consisting of one vector of
   coefficients for identity and a high dimensional curve for expression.
   Based on this representation, we synthesize new motion sequences and
   perform expression recognition. We show experimentally that the obtained
   registration is of high quality, where 56\% of all vertices are at
   distance at most 1 mm from the input data, and that our synthesized
   motion sequences look realistic. (C) 2014 Elsevier Inc. All rights
   reserved.}},
DOI = {{10.1016/j.cviu.2014.06.013}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
Unique-ID = {{ISI:000349588900008}},
}

@article{ ISI:000349308600007,
Author = {Li, Ye and Wang, YingHui and Wang, BingBo and Sui, LianSheng},
Title = {{Nose tip detection on three-dimensional faces using pose-invariant
   differential surface features}},
Journal = {{IET COMPUTER VISION}},
Year = {{2015}},
Volume = {{9}},
Number = {{1}},
Pages = {{75-84}},
Month = {{FEB}},
Abstract = {{Three-dimensional (3D) facial data offer the potential to overcome the
   difficulties caused by the variation of head pose and illumination in 2D
   face recognition. In 3D face recognition, localisation of nose tip is
   essential to face normalisation, face registration and pose correction
   etc. Most of the existing methods of nose tip detection on 3D face deal
   mainly with frontal or near-frontal poses or are rotation sensitive.
   Many of them are training-based or model-based. In this study, a novel
   method of nose tip detection is proposed. Using pose-invariant
   differential surface features - high-order and low-order curvatures, it
   can detect nose tip on 3D faces under various poses automatically and
   accurately. Moreover, it does not require training and does not depend
   on any particular model. Experimental results on GavabDB verify the
   robustness and accuracy of the proposed method.}},
DOI = {{10.1049/iet-cvi.2014.0070}},
ISSN = {{1751-9632}},
EISSN = {{1751-9640}},
Unique-ID = {{ISI:000349308600007}},
}

@inproceedings{ ISI:000387959204037,
Author = {Hassner, Tal and Harel, Shai and Paz, Eran and Enbar, Roee},
Book-Group-Author = {{IEEE}},
Title = {{Effective Face Frontalization in Unconstrained Images}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2015}},
Pages = {{4295-4304}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{``Frontalization{''} is the process of synthesizing frontal facing views
   of faces appearing in single unconstrained photos. Recent reports have
   suggested that this process may substantially boost the performance of
   face recognition systems. This, by transforming the challenging problem
   of recognizing faces viewed from unconstrained viewpoints to the easier
   problem of recognizing faces in constrained, forward facing poses.
   Previous frontalization methods did this by attempting to approximate 3D
   facial shapes for each query image. We observe that 3D face shape
   estimation from unconstrained photos may be a harder problem than
   frontalization and can potentially introduce facial misalignments.
   Instead, we explore the simpler approach of using a single, unmodified,
   3D surface as an approximation to the shape of all input faces. We show
   that this leads to a straightforward, efficient and easy to implement
   method for frontalization. More importantly, it produces aesthetic new
   frontal views and is surprisingly effective when used for face
   recognition and gender estimation.}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-6964-0}},
Unique-ID = {{ISI:000387959204037}},
}

@inproceedings{ ISI:000380500900562,
Author = {Yurtkan, Kamil and Soyel, Hamit and Demirel, Hasan},
Book-Group-Author = {{IEEE}},
Title = {{ENTROPY DRIVEN FEATURE SELECTION FOR FACIAL EXPRESSION RECOGNITION BASED
   ON 3-D FACIAL FEATURE DISTANCES}},
Booktitle = {{2015 23RD SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE
   (SIU)}},
Series = {{Signal Processing and Communications Applications Conference}},
Year = {{2015}},
Pages = {{2322-2325}},
Note = {{23nd Signal Processing and Communications Applications Conference (SIU),
   Inonu Univ, Malatya, TURKEY, MAY 16-19, 2015}},
Organization = {{Dept Comp Engn \& Elect \& Elect Engn; Elect \& Elect Engn; Bilkent Univ}},
Abstract = {{Facial expressions contain a lot of information about the feelings of a
   human. It plays an important role in human-computer interaction. In this
   paper, entropy based feature selection method applied to 3D facial
   feature distances is presented for a facial expression recognition
   system classifying the expressions into 6 basic classes based on
   3-Dimensional (3D) face geometry. Our previous work on entropy based
   feature selection has been improved by employing 3D feature distances
   between the 83 points on the face as facial features. 3D distances are
   more robust to rotations of the face and involve more accurate
   information than 3D feature positions that are used in our previous
   work. Entropy is applied in order to rank the feature distances for
   feature selection. The system is tested on BU-3DFE database in person
   independent manner and provides encouraging recognition rates.}},
ISSN = {{2165-0608}},
ISBN = {{978-1-4673-7386-9}},
ResearcherID-Numbers = {{Demirel, Hasan/H-5769-2013}},
Unique-ID = {{ISI:000380500900562}},
}

@inproceedings{ ISI:000380584600001,
Author = {Boukamcha, Hamdi and Atri, Mohamed and Elhallek, Mohamed and Smach,
   Fethi},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Landmark Auto Detection}},
Booktitle = {{2015 WORLD SYMPOSIUM ON COMPUTER NETWORKS AND INFORMATION SECURITY
   (WSCNIS)}},
Year = {{2015}},
Note = {{World Symposium Computer Networks Information Security (WSCNIS 2015),
   Hammamet, TUNISIA, SEP 19-21, 2015}},
Abstract = {{This paper presents our methodology for Landmark Point detection to
   improve 3D face recognition in a presence of variant facial expression.
   The objective was to develop an automatic process for distinguishing and
   segmenting to be embedded in a 3D face recognition system using only 3D
   Point Distribution Model (PDM) as input. The approach used hydride
   method to extract this features from the surface curvature information.
   Landmark Localization is done on the segmented face via finding the
   change that decreases the deviation of the model from the mean profile.
   Face registering is achieved using previous anthropometric information
   and the localized landmarks. The results confirm that the method used is
   accurate and robust for the proposed application.}},
ISBN = {{978-1-4799-9907-1}},
ResearcherID-Numbers = {{atri, mohamed/C-4069-2014}},
ORCID-Numbers = {{atri, mohamed/0000-0001-8528-5647}},
Unique-ID = {{ISI:000380584600001}},
}

@inproceedings{ ISI:000382327100024,
Author = {Wang, Yuanyuan and Zhu, Xiao Xiang},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane, G}},
Title = {{SEMANTIC INTERPRETATION OF INSAR ESTIMATES USING OPTICAL IMAGES WITH
   APPLICATION TO URBAN INFRASTRUCTURE MONITORING}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W3}},
Pages = {{153-160}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{Synthetic aperture radar interferometry (InSAR) has been an established
   method for long term large area monitoring. Since the launch of
   meter-resolution spaceborne SAR sensors, the InSAR community has shown
   that even individual buildings can be monitored in high level of detail.
   However, the current deformation analysis still remains at a primitive
   stage of pixel-wise motion parameter inversion and manual identification
   of the regions of interest. We are aiming at developing an automatic
   urban infrastructure monitoring approach by combining InSAR and the
   semantics derived from optical images, so that the deformation analysis
   can be done systematically in the semantic/object level. This paper
   explains how we transfer the semantic meaning derived from optical image
   to the InSAR point clouds, and hence different semantic classes in the
   InSAR point cloud can be automatically extracted and monitored. Examples
   on bridges and railway monitoring are demonstrated.}},
DOI = {{10.5194/isprsarchives-XL-3-W3-153-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000382327100024}},
}

@inproceedings{ ISI:000382326300052,
Author = {Shahzad, M. and Zhu, X. X.},
Editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Rottensteiner, F and Yang, M and Christophe, S and Coltekin, A and Bredif, M}},
Title = {{RECONSTRUCTION OF BUILDING FOOTPRINTS USING SPACEBORNE TOMOSAR POINT
   CLOUDS}},
Booktitle = {{ISPRS GEOSPATIAL WEEK 2015}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{II-3}},
Number = {{W5}},
Pages = {{385-392}},
Note = {{ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015}},
Organization = {{WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging \& Sensors;
   WG VII 7 Synergy Radar; ISPRS}},
Abstract = {{This paper presents an approach that automatically (but parametrically)
   reconstructs 2-D/3-D building footprints using 3-D synthetic aperture
   radar (SAR) tomography (TomoSAR) point clouds. These point clouds are
   generated by processing SAR image stacks via SAR tomographic inversion.
   The proposed approach reconstructs the building outline by exploiting
   both the roof and facade points. Initial building footprints are derived
   by applying the alpha shapes method on pre-segmented point clusters of
   individual buildings. A recursive angular deviation based refinement is
   then carried out to obtain refined/smoothed 2-D polygonal boundaries. A
   robust fusion framework then fuses the information pertaining to
   building facades to the smoothed polygons. Afterwards, a rectilinear
   building identification procedure is adopted and constraints are added
   to yield geometrically correct and visually aesthetic building shapes.
   The proposed approach is illustrated and validated using TomoSAR point
   clouds generated from a stack of TerraSAR-X high-resolution spotlight
   images from ascending orbit covering approximately 1.5 km(2) area in the
   city of Berlin, Germany.}},
DOI = {{10.5194/isprsannals-II-3-W5-385-2015}},
ISSN = {{2194-9034}},
Unique-ID = {{ISI:000382326300052}},
}

@inproceedings{ ISI:000380496400016,
Author = {Marvadi, Dhara and Joshi, Maulin and Paunwala, Chirag and Vora, Aarohi},
Book-Group-Author = {{IEEE}},
Title = {{Comparative Analysis of 3D Face Recognition Using 2D-PCA and 2D-LDA
   Approaches}},
Booktitle = {{2015 5TH NIRMA UNIVERSITY INTERNATIONAL CONFERENCE ON ENGINEERING
   (NUICONE)}},
Series = {{Nirma University International Conference on Engineering}},
Year = {{2015}},
Note = {{5th Nirma University International Conference on Engineering (NUiCONE),
   Ahmedabad, INDIA, NOV 26-28, 2015}},
Organization = {{Nirma Univ, Inst Technol}},
Abstract = {{Even if, most of 2D face recognition approaches reached recognition rate
   more than 90\% in controlled environment, current days face recognition
   systems degrade their performance in case of uncontrolled environment
   which includes pose variations, illumination variations, expression
   variations and ageing effect etc. Inclusion of 3D face analysis gives an
   age over 2D face recognition as they give vital informations such as 3D
   shape, texture and depth which improve discrimination power of an
   algorithm. In this paper, we have investigated different 3D face
   recognition approaches that are robust to changes in facial expressions
   and illumination variations. 2D-PCA and 2D-LDA approaches have been
   extended to 3D face recognition because they can directly work on 2D
   depth image matrices rather than 1D vectors without need for
   transformations before feature extraction. In turn, this reduces storage
   space and time required for computations. 2D depth image is extracted
   from 3D face model and nose region from depth mapped image has been
   detected as a reference point for cropping stage to convert model into a
   standard size. Two Dimensional Principal Component Analysis (2D-PCA) and
   Two Dimensional Linear Discriminant analysis (2D-LDA) are employed to
   obtain feature vectors globally compared to feature vectors obtained
   locally using PCA or LDA. Finally, euclidean distance classifier is
   applied for comparison of extracted features. A set of experiments on
   GavabDB 3D face database, which has 61 individuals in total,
   demonstrated that 3D face recognition using 2D-LDA method has achieved
   recognition accuracy of 93.3\% and EER of 8.96\% over database, which is
   higher compared to 2D-PCA. So, more optimized performance has been
   achieved using 2D-LDA for 3D face recognition analysis.}},
ISSN = {{2375-1282}},
ISBN = {{978-1-4799-9991-0}},
Unique-ID = {{ISI:000380496400016}},
}

@inproceedings{ ISI:000380516600012,
Author = {Li, Jing and Long, Shuqin and Zeng, Dan and Zhao, Qijun},
Book-Group-Author = {{IEEE}},
Title = {{Example-Based 3D Face Reconstruction from Uncalibrated Frontal and
   Profile Images}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2015}},
Pages = {{193-200}},
Note = {{International Conference on Biometrics (ICB), Phuket, THAILAND, MAY
   19-22, 2015}},
Organization = {{SIEW-SNGIEM AWARD; Kasetsart Univ; CHANWANICH; ST Elect; SAFRAN Morpho}},
Abstract = {{Reconstructing 3D face models from multiple uncalibrated 2D face images
   is usually done by using a single reference 3D face model or some
   gender/ethnicity-specific 3D face models. However, different persons,
   even those of the same gender or ethnicity, usually have significantly
   different faces in terms of their overall appearance, which forms the
   base of person recognition using faces. Consequently, existing 3D
   reference model based methods have limited capability of reconstructing
   3D face models for a large variety of persons. In this paper, we propose
   to explore a reservoir of diverse reference models to improve the 3D
   face reconstruction performance. Specifically, we convert the face
   reconstruction problem into a multi-label segmentation problem. Its
   energy function is formulated from different cues, including 1)
   similarity between the desired output and the initial model, 2) color
   consistency between different views, 3) smoothness constraint on
   adjacent pixels, and 4) model consistency within local neighborhood.
   Experimental results on challenging datasets demonstrate that the
   proposed algorithm is capable of recovering high quality face models in
   both qualitative and quantitative evaluations.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-4799-7824-3}},
Unique-ID = {{ISI:000380516600012}},
}

@inproceedings{ ISI:000380516600070,
Author = {Svoboda, Jan and Bronstein, Michael M. and Drahansky, Martin},
Book-Group-Author = {{IEEE}},
Title = {{Contactless biometric hand geometry recognition using a low-cost 3D
   camera}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2015}},
Pages = {{452-457}},
Note = {{International Conference on Biometrics (ICB), Phuket, THAILAND, MAY
   19-22, 2015}},
Organization = {{SIEW-SNGIEM AWARD; Kasetsart Univ; CHANWANICH; ST Elect; SAFRAN Morpho}},
Abstract = {{In the past decade, the interest in using 3D data for biometric person
   authentication has increased significantly, propelled by the
   availability of affordable 3D sensors. The adoption of 3D features has
   been especially successful in face recognition applications, leading to
   several commercial 3D face recognition products. In other biometric
   modalities such as hand recognition, several studies have shown the
   potential advantage of using 3D geometric information, however, no
   commercial-grade systems are currently available. In this paper, we
   present a contactless 3D hand recognition system based on the novel
   Intel RealSense camera, the first mass-produced embeddable 3D sensor.
   The small form factor and low cost make this sensor especially appealing
   for commercial biometric applications, however, they come at the price
   of lower resolution compared to more expensive 3D scanners used in
   previous research. We analyze the robustness of several existing 2D and
   3D features that can be extracted from the images captured by the
   RealSense camera and study the use of metric learning for their fusion.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-4799-7824-3}},
Unique-ID = {{ISI:000380516600070}},
}

@inproceedings{ ISI:000380516600072,
Author = {Tang, Yinhang and Sun, Xiang and Huang, Di and Morvan, Jean-Marie and
   Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Recognition with Asymptotic Cones based Principal Curvatures}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)}},
Series = {{International Conference on Biometrics}},
Year = {{2015}},
Pages = {{466-472}},
Note = {{International Conference on Biometrics (ICB), Phuket, THAILAND, MAY
   19-22, 2015}},
Organization = {{SIEW-SNGIEM AWARD; Kasetsart Univ; CHANWANICH; ST Elect; SAFRAN Morpho}},
Abstract = {{The classical curvatures of smooth surfaces (Gaussian, mean and
   principal curvatures) have been widely used in 3D face recognition (FR).
   However, facial surfaces resulting from 3D sensors are discrete meshes.
   In this paper, we present a general framework and define three principal
   curvatures on discrete surfaces for the purpose of 3D FR. These
   principal curvatures are derived from the construction of asymptotic
   cones associated to any Borel subset of the discrete surface. They
   describe the local geometry of the underlying mesh. First two of them
   correspond to the classical principal curvatures in the smooth case. We
   isolate the third principal curvature that carries out meaningful
   geometric shape information. The three principal curvatures in different
   Borel subsets scales give multi-scale local facial surface descriptors.
   We combine the proposed principal curvatures with the LNP-based facial
   descriptor and SRC for recognition. The identification and verification
   experiments demonstrate the practicability and accuracy of the third
   principal curvature and the fusion of multi-scale Borel subset
   descriptors on 3D face from FRGC v2.0.}},
ISSN = {{2376-4201}},
ISBN = {{978-1-4799-7824-3}},
Unique-ID = {{ISI:000380516600072}},
}

@inproceedings{ ISI:000380606700172,
Author = {Talandova, Hana and Kralik, Lukas and Adamek, Milan},
Editor = {{Zhang, Z and Zhang, R and Fernandez, V and Liu, S}},
Title = {{Determination of the Physiological Similarities of Family Members by
   Using a Broadway 3D Biometric Device}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON LOGISTICS, INFORMATICS AND SERVICE
   SCIENCES (LISS)}},
Year = {{2015}},
Note = {{International Conference on Logistics, Informatics and Service Sciences
   (LISS), Beijing Jiaotong Univ, Int Ctr Informat Res, Barcelona, SPAIN,
   JUL 27-29, 2015}},
Organization = {{Beijing Jiaotong Univ, China Ctr Ind Secur Res; Beijing Jiaotong Univ,
   Sch Econ \& Management; IEEE SMC; Natl Nat Sci Fdn China; K C Wong Educ
   Fdn; Sino EU Doctoral Sch Sustainabil Engn; EU FP7; Beijing Logist
   Informat Res Base; Key Lab Logist Management \& Technol Beijing}},
Abstract = {{The biometric identification by the face is one of the oldest biometric
   identification. With increasing progress in the area of identification
   by the face this technique was implemented into area of security, where
   it provides a faster and more accurate identification. The 3D face
   reader uses for the identification of the person: eyes, mouth, nose, and
   in contrast to 2D readers also chin and cheeks. 3D face reader by
   Broadway manufacturer was used to measure the physiological similarities
   of family members. It is equipped with the 3D camera system, which uses
   the method of structured light scanning and saves the template into the
   3D model of face. The obtained data were evaluated by software Turnstile
   Enrolment Application (TEA). The participants of the measurement were
   members of three different families. Each person was compared with the
   previously saved templates of other family members. Using this method
   the similarity of family members was evalua00ted.}},
ISBN = {{978-1-4799-1891-1}},
Unique-ID = {{ISI:000380606700172}},
}

@inproceedings{ ISI:000380558900020,
Author = {Pang, Guan and Qiu, Rongqi and Huang, Jing and You, Suya and Neumann,
   Ulrich},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Industrial Point Cloud Modeling and Recognition}},
Booktitle = {{2015 14th IAPR International Conference on Machine Vision Applications
   (MVA)}},
Year = {{2015}},
Pages = {{22-25}},
Note = {{14th IAPR International Conference on Machine Vision Applications (MVA),
   Tokyo, JAPAN, MAY 18-22, 2015}},
Organization = {{MVA Organisation; IAPR TC-8; National Institute of Advanced Industrial
   Science and Technology (AIST); The Telecommunications Advancement
   Foundation; KDDI Foundation}},
Abstract = {{3D modeling of point clouds is an important but time-consuming process,
   inspiring extensive research in automatic methods. Prior efforts focus
   on primitive geometry, street structures or indoor objects, but
   industrial data has rarely been pursued. Our work presents a method for
   automatic modeling and recognition of 3D industrial site point clouds,
   dividing the task into 3 separate sub-problems: pipe modeling, plane
   classification, and object recognition. The results are integrated to
   obtain the complete model, revealing some issues during the integration,
   solved by utilizing information gained from each individual process.
   Experiments show that the presented method automatically models large
   and complex industrial scenes with a quality that outperforms leading
   commercial modeling software and is comparable to professional hand-made
   models.}},
ISBN = {{978-4-9011-2214-6}},
Unique-ID = {{ISI:000380558900020}},
}

@inproceedings{ ISI:000380792100051,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Book-Group-Author = {{IEEE}},
Title = {{3D Face Recognition from Complement Component Range Face Images}},
Booktitle = {{2015 IEEE International Conference on Computer Graphics, Vision and
   Information Security (CGVIS)}},
Year = {{2015}},
Pages = {{275-278}},
Note = {{IEEE International Conference on Computer Graphics, Vision and
   Information Security (CGVIS), KIIT Univ, Bhubaneswar, INDIA, NOV 02-03,
   2015}},
Organization = {{IEEE Kolkata Sect; IEEE KIIT Student Branch; IEEE; Webprit Solut}},
Abstract = {{Face and facial attributes represent meaningful definition about a
   variety of information to discriminate an individual from others and for
   developing a computational model for automatic face recognition purpose.
   However, in this work, selection of relevant features from newly created
   face space is the pivotal contribution of the authors. Here, authors
   have demonstrated a new face space `Complement Component' that have been
   used to extract the four basic components along X, and Y axes in four
   directions. Later, authors have experimented the discriminative
   attributes from these face spaces for recognition purpose. Here,
   comparison of the proposed method has been reported by examining its
   success on two well accepted 3D face databases, namely: Frav3D and
   Texas3D. In case of 2D face images, it does not contain depth like
   information i.e. Z-values in X-Y plane through intensity values.
   Therefore, it has not been undertaken during this investigation.}},
ISBN = {{978-1-4673-7437-8}},
Unique-ID = {{ISI:000380792100051}},
}

@inproceedings{ ISI:000380586200014,
Author = {Gong, Xun and Fu, Zehua and Li, Xinxin and Feng, Lin},
Book-Group-Author = {{IEEE}},
Title = {{A Two-Stage Estimation Method for Depth Estimation of Facial Landmarks}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IDENTITY, SECURITY AND BEHAVIOR
   ANALYSIS (ISBA)}},
Year = {{2015}},
Note = {{IEEE International Conference on Identity, Security and Behavior
   Analysis (ISBA), Hong Kong, PEOPLES R CHINA, MAR 23-25, 2015}},
Organization = {{IEEE}},
Abstract = {{To address the problem of 3D face modeling based on a set of landmarks
   on images, the traditional feature-based morphable model, using face
   class-specific information, makes direct use of these 2D points to infer
   a dense 3D face surface. However, the unknown depth of landmarks
   degrades accuracy considerably. A promising solution is to predict the
   depth of landmarks at first. Bases on this idea, a two-stage estimation
   method is proposed to compute the depth value of landmarks from two
   images. And then, the estimated 3D landmarks are applied to a
   deformation algorithm to make a precise 3D dense facial shape. Test
   results on synthesized images with known ground-truth show that the
   proposed two-stage estimation method can obtain landmarks' depth both
   effectively and efficiently, and further that the reconstructed accuracy
   is greatly enhanced with the estimated 3D landmarks. Reconstruction
   results of real-world photos are rather realistic.}},
ISBN = {{978-1-4799-1974-1}},
Unique-ID = {{ISI:000380586200014}},
}

@inproceedings{ ISI:000380486500091,
Author = {Andreu-Cabedo, Yasmina and Castellano, Pedro and Colantonio, Sara and
   Coppini, Giuseppe and Favilla, Riccardo and Germanese, Danila and
   Giannakakis, Giorgos and Giorgi, Daniela and Larsson, Marcus and
   Marraccini, Paolo and Martinelli, Massimo and Matuszewski, Bogdan and
   Milanic, Matijia and Pascali, Mariantonietta and Pediaditis, Mattew and
   Raccichini, Giovanni and Randeberg, Lise and Salvetti, Ovidio and
   Stromberg, Tomas},
Book-Group-Author = {{IEEE}},
Title = {{MIRROR MIRROR ON THE WALL... AN INTELLIGENT MULTISENSORY MIRROR FOR
   WELL-BEING SELF-ASSESSMENT}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA \& EXPO (ICME)}},
Series = {{IEEE International Conference on Multimedia and Expo}},
Year = {{2015}},
Note = {{IEEE International Conference on Multimedia \& Expo (ICME), Turin,
   ITALY, JUN 29-JUL 03, 2015}},
Abstract = {{The face reveals the healthy status of an individual, through a
   combination of physical signs and facial expressions. The project
   SEMEOTICONS is translating the semeiotic code of the human face into
   computational descriptors and measures, automatically extracted from
   videos, images, and 3D scans of the face. SEMEOTICONS is developing a
   multisensory platform, in the form of a smart mirror, looking for signs
   related to cardio-metabolic risk. The goal is to enable users to
   self-monitor their well-being status over time and improve their
   life-style via tailored user guidance. Building the multisensory mirror
   requires addressing significant scientific and technological challenges,
   from touch-less data acquisition, to real-time processing and
   integration of multimodal data.}},
ISSN = {{1945-7871}},
ISBN = {{978-1-4799-7082-7}},
ResearcherID-Numbers = {{Randeberg, Lise Lyngsnes/G-8664-2019
   Milanic, Matija/S-4990-2018}},
ORCID-Numbers = {{Randeberg, Lise Lyngsnes/0000-0003-2608-3759
   Milanic, Matija/0000-0002-4417-0293}},
Unique-ID = {{ISI:000380486500091}},
}

@inproceedings{ ISI:000380447200087,
Author = {Goodarzi, Farhad and Saripan, M. Iqbal},
Book-Group-Author = {{IEEE}},
Title = {{Real time face pose estimation using geometrical features}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING
   APPLICATIONS (ICSIPA)}},
Year = {{2015}},
Pages = {{482-487}},
Note = {{IEEE 2015 International Conference on Signal and Image Processing
   Applications (ICSIPA), Kuala Lumpur, MALAYSIA, OCT 19-21, 2015}},
Organization = {{IEEE; ICSIPA; IEEE SIGNAL PROC SOC}},
Abstract = {{3D Head pose estimation using a hybrid pose estimation method is
   discussed in this paper. The only equipment used is simple webcam that
   is available on most computers and laptops today. The hybrid method
   consists of tracking facial landmarks of face and using geometrical face
   pose estimation to compute distances to estimate 3D position of head.
   The pose estimation system works real time as it is ultimately will be
   used for 2D-3D face recognition system. Actual data show reasonable
   error for rotation along each axis (Yaw, Pitch and Roll) by using only
   few facial landmarks.}},
ISBN = {{978-1-4799-8996-6}},
Unique-ID = {{ISI:000380447200087}},
}

@inproceedings{ ISI:000380445000302,
Author = {Shirke, Vikas D. and Gawande, Ujwalla},
Book-Group-Author = {{IEEE}},
Title = {{Generation of 3D Face Views from Artist Drawn Sketch: A Review}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INSTRUMENTATION AND CONTROL
   (ICIC)}},
Year = {{2015}},
Pages = {{1582-1586}},
Note = {{International Conference on Industrial Instrumentation and Control, Coll
   Engn Pune, Pune, INDIA, MAY 28-30, 2015}},
Organization = {{IEEE Pune Sect}},
Abstract = {{Approach to construct 3D face model from an artist drawn sketch is an
   area of interest in image processing from last few decades. It has
   various application like police investigation, 3D cartoon modeling. From
   an individual's sketch it is possible to construct 3D face model using
   various techniques. To construct 3D face views, from the individuals
   sketch steps required are 2D landmark detection, 3D landmark estimation,
   surface and texture synthesis with reference to 3D morphable model. This
   system is beneficial for the purpose of increasing the identification
   accuracy of the persons whose photographs are not available. In this
   paper, we tend to surveyed different techniques to construct 3D face
   views from artist drawn sketch.}},
ISBN = {{978-1-4799-7165-7}},
Unique-ID = {{ISI:000380445000302}},
}

@inproceedings{ ISI:000380407300092,
Author = {Pawar, Asmita A. and Patil, Nitin N.},
Book-Group-Author = {{IEEE}},
Title = {{Recognition of 3-D Faces with Missing Parts and Line Scratch Removal
   using New Technique}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)}},
Year = {{2015}},
Note = {{International Conference on Pervasive Computing (ICPC), Pune, INDIA, JAN
   08-10, 2015}},
Organization = {{IEEE Pune Sect; IEEE Comp Soc; Savitribai Phule Pune Univ; IEEE Commun
   Soc Pune Chapter; Sinhgad Inst; Sakal Times}},
Abstract = {{This paper presents an implementation of face recognition, which is a
   very important task of human face identification. Line scratch detection
   in images is a highly challenging situation because of various
   characteristics of this defect. Few characteristics are considered with
   the different texture and geometry of images. We propose a useful
   algorithm for frame-by-frame line scratch detection in face image which
   deals with 3D approach and a filtering of detection. The temporary
   filtering algorithm can be used to remove false detection due to thin
   vertical structures by detecting the scratches on an image. Experimental
   evaluation can be detecting the lines and scratches on a face image and
   they used to solve this difficult approach. Our method is used with
   missing parts in an image. Three-dimensional face recognition is an
   extended method of facial recognition is considered according with the
   geometry and texture of a face. It has been elaborated that 3D face
   recognition methods can provide high accuracy as well as high detection
   with a comparison of 2D recognition. 3D avoids such mismatch effect of
   2D face recognition algorithms. Additionally, most 3D scanners achieve
   both a 3D mesh and the texture of a face image. This allows combining
   the output of pure 3D matches with the more traditional algorithms of 2D
   face recognition, thus producing better performance.}},
ISBN = {{978-1-4799-6272-3}},
Unique-ID = {{ISI:000380407300092}},
}

@inproceedings{ ISI:000380433000180,
Author = {Bentaieb, Samia and Ouamri, Abdelaziz and Keche, Mokhtar},
Book-Group-Author = {{IEEE}},
Title = {{Nose Tip Localization on a Three Dimensional Face across Pose,
   Expressions and Occlusions Variations in a Riemannian Context}},
Booktitle = {{3RD INTERNATIONAL CONFERENCE ON CONTROL, ENGINEERING \& INFORMATION
   TECHNOLOGY (CEIT 2015)}},
Year = {{2015}},
Note = {{INTERNATIONAL CONFERENCE ON CONTROL ENGINEERING \& INFORMATION
   TECHNOLOGY (CEIT), Tlemcen, ALGERIA, MAY 25-27, 2015}},
Abstract = {{Nose tip localization is an important step for registration,
   preprocessing and recognition of 3D face data. In this paper, we propose
   a new approach for the nose tip detection that is robust to pose and
   expression variations and in presence of occlusions. From a rotated 3D
   face, we extract facial curves that are matched to a profile curve
   model. An optimal matching using the Riemannian geometry, based on the
   Elastic Shape Analysis is performed to obtain the accurate nose tip. The
   proposed method requires no training and can locate the nose tip in less
   than 6 seconds. Experiments are performed on the Bosphorus database.
   Quantitative analysis and comparison with the ground truth locations are
   provided. The results confirm that our approach achieves 97.68\% with
   error no larger than 12 mm and 98.19\% within 20 mm.}},
ISBN = {{978-1-4799-8213-4}},
Unique-ID = {{ISI:000380433000180}},
}

@inproceedings{ ISI:000380489200045,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Book-Group-Author = {{IEEE}},
Title = {{Decremental Depth Bunch Based 3D Face Recognition from Range Image}},
Booktitle = {{TENCON 2015 - 2015 IEEE REGION 10 CONFERENCE}},
Series = {{TENCON IEEE Region 10 Conference Proceedings}},
Year = {{2015}},
Note = {{IEEE Region 10 Conference, Macao, PEOPLES R CHINA, NOV 01-04, 2015}},
Organization = {{IEEE; IPIM; CIUR; IEEE SINGAPORE SEC; CPSS; manetic; Melco Crown
   Entertainment; MACAO WATER; cadence ACADEMIC NETWORK; MuleSoft; cem;
   Wanlida; MANGO MESSAGING TECH; SEE; FDCT}},
Abstract = {{In this paper, a new technique, i.e. decremental depth bunches have been
   presented where two facial discriminating mechanisms have also been
   implemented for recognizing the individuals. Notably, based on
   variations of the depth values, different bunches of face regions (i.e.
   the small components) are extracted by differentiating the depth
   information that eventually describes detailed facial surface
   information. Now, from each bunches, statistical attributes as well as
   Hough peaks are encountered to initiate two feature vectors for
   feature-based as well as a holistic mechanism for classification by K-NN
   and Cosine distance respectively. The proposed mechanism is explicitly
   dependent on facial depth information that have been accomplished in
   range face images. Therefore, authors have considered two databases,
   namely: Frav3D and Bosphorus, that contains laser as well as structured
   light 3D scanner based procured 3D face images respectively.}},
ISSN = {{2159-3442}},
ISBN = {{978-1-4799-8641-5}},
Unique-ID = {{ISI:000380489200045}},
}

@inproceedings{ ISI:000380388000083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 4}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380388000083}},
}

@inproceedings{ ISI:000380388000012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 4}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380388000012}},
}

@inproceedings{ ISI:000380427900009,
Author = {Chen, Lulu and Ferryman, James},
Book-Group-Author = {{IEEE}},
Title = {{Combining 3D and 2D for less constrained periocular recognition}},
Booktitle = {{2015 IEEE 7TH INTERNATIONAL CONFERENCE ON BIOMETRICS THEORY,
   APPLICATIONS AND SYSTEMS (BTAS 2015)}},
Series = {{International Conference on Biometrics Theory Applications and Systems}},
Year = {{2015}},
Note = {{IEEE 7th International Conference on Biometrics Theory, Applications and
   Systems (BTAS), Arlington, VA, SEP 08-11, 2015}},
Organization = {{IEEE}},
Abstract = {{Periocular recognition has recently become an active topic in
   biometrics. Typically it uses 2D image data of the periocular region.
   This paper is the first description of combining 3D shape structure with
   2D texture. A simple and effective technique using iterative closest
   point (ICP) was applied for 3D periocular region matching. It proved its
   strength for relatively unconstrained eye region capture, and does not
   require any training. Local binary patterns (LBP) were applied for 2D
   image based periocular matching. The two modalities were combined at the
   score-level. This approach was evaluated using the Bosphorus 3D face
   database, which contains large variations in facial expressions, head
   poses and occlusions. The rank-1 accuracy achieved from the 3D data
   (80\%) was better than that for 2D (58\%), and the best accuracy (83\%)
   was achieved by fusing the two types of data. This suggests that
   significant improvements to periocular recognition systems could be
   achieved using the 3D structure information that is now available from
   small and inexpensive sensors.}},
ISSN = {{2474-9680}},
ISBN = {{978-1-4799-8777-1}},
Unique-ID = {{ISI:000380427900009}},
}

@inproceedings{ ISI:000380427900044,
Author = {Dou, Pengfei and Zhang, Lingfeng and Wu, Yuhang and Shah, Shishir K. and
   Kakadiaris, Ioannis A.},
Book-Group-Author = {{IEEE}},
Title = {{Pose-Robust Face Signature for Multi-View Face Recognition}},
Booktitle = {{2015 IEEE 7TH INTERNATIONAL CONFERENCE ON BIOMETRICS THEORY,
   APPLICATIONS AND SYSTEMS (BTAS 2015)}},
Series = {{International Conference on Biometrics Theory Applications and Systems}},
Year = {{2015}},
Note = {{IEEE 7th International Conference on Biometrics Theory, Applications and
   Systems (BTAS), Arlington, VA, SEP 08-11, 2015}},
Organization = {{IEEE}},
Abstract = {{Despite the great progress achieved in unconstrained face recognition,
   pose variations still remain a challenging and unsolved practical issue.
   We propose a novel framework for multi-view face recognition based on
   extracting and matching pose-robust face signatures from 2D images.
   Specifically, we propose an efficient method for monocular 3D face
   reconstruction, which is used to lift the 2D facial appearance to a
   canonical texture space and estimate the self-occlusion. On the lifted
   facial texture we then extract various local features, which are further
   enhanced by the occlusion encodings computed on the self-occlusion mask,
   resulting in a pose-robust face signature, a novel feature
   representation of the original 2D facial image. Extensive experiments on
   two public datasets demonstrate that our method not only simplifies the
   matching of multi-view 2D facial images by circumventing the requirement
   for pose-adaptive classifiers, but also achieves superior performance.}},
ISSN = {{2474-9680}},
ISBN = {{978-1-4799-8777-1}},
Unique-ID = {{ISI:000380427900044}},
}

@inproceedings{ ISI:000380427900055,
Author = {Wu, Yuhang and Xu, Xiang and Shah, Shishir K. and Kakadiaris, Ioannis A.},
Book-Group-Author = {{IEEE}},
Title = {{Towards fitting a 3D dense facial model to a 2D image: A landmark-free
   approach}},
Booktitle = {{2015 IEEE 7TH INTERNATIONAL CONFERENCE ON BIOMETRICS THEORY,
   APPLICATIONS AND SYSTEMS (BTAS 2015)}},
Series = {{International Conference on Biometrics Theory Applications and Systems}},
Year = {{2015}},
Note = {{IEEE 7th International Conference on Biometrics Theory, Applications and
   Systems (BTAS), Arlington, VA, SEP 08-11, 2015}},
Organization = {{IEEE}},
Abstract = {{Head pose estimation helps to align a 3D face model to a 2D image, which
   is critical to research requiring dense 2D-to-2D or 3D-to-2D
   correspondence. Traditional pose estimation relies strongly on the
   accuracy of landmarks, so it is sensitive to missing or incorrect
   landmarks. In this paper, we propose a landmark-free approach to
   estimate the pose projection matrix. The method can be used to estimate
   this matrix in unconstrained scenarios and we demonstrate its
   effectiveness through multiple head pose estimation experiments.}},
ISSN = {{2474-9680}},
ISBN = {{978-1-4799-8777-1}},
Unique-ID = {{ISI:000380427900055}},
}

@inproceedings{ ISI:000380434700004,
Author = {Tulyakov, Sergey and Vieriu, Radu-Laurentiu and Sangineto, Enver and
   Sebe, Nicu},
Book-Group-Author = {{IEEE}},
Title = {{FaceCept3D: Real Time 3D Face Tracking and Analysis}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW)}},
Year = {{2015}},
Pages = {{28-33}},
Note = {{IEEE International Conference on Computer Vision Workshops, santigo,
   CHILE, DEC 11-18, 2015}},
Organization = {{CPS; IEEE Comp Soc; amazon; Microsoft; SENSETIME; Baidu; intel;
   facebook; Adobe; Panasonic; Google; OMRON; blippar; iRobot; HISCENE;
   nVIDIA; Viscqvery; AiCUre; M/Tec}},
Abstract = {{We present an open source cross platform technology for 3D face tracking
   and analysis. It contains a full stack of components for complete face
   understanding: detection, head pose tracking, facial expression and
   action units recognition. Given a depth sensor, one can combine
   FaceCept3D modules to fulfill a specific application scenario. Key
   advantages of the technology include real time processing speed and
   ability to handle extreme head pose variations. Possible application
   areas of the technology range from human computer interaction to active
   aging, where precise and real-time analysis is required. The technology
   is available to community.}},
DOI = {{10.1109/ICCVW.2015.13}},
ISBN = {{978-0-7695-5720-5}},
ORCID-Numbers = {{Sebe, Niculae/0000-0002-6597-7248}},
Unique-ID = {{ISI:000380434700004}},
}

@inproceedings{ ISI:000380379900083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 1}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380379900083}},
}

@inproceedings{ ISI:000380379900012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 1}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380379900012}},
}

@inproceedings{ ISI:000380393900083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 2}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380393900083}},
}

@inproceedings{ ISI:000380393900012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 2}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380393900012}},
}

@inproceedings{ ISI:000380377400083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 3}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380377400083}},
}

@inproceedings{ ISI:000380377400012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 3}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380377400012}},
}

@inproceedings{ ISI:000380390600083,
Author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic,
   Maja},
Book-Group-Author = {{IEEE}},
Title = {{Active Nonrigid ICP Algorithm}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 5}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{The problem of fitting a 3D facial model to a 3D mesh has received a lot
   of attention the past 15-20 years. The majority of the techniques fit a
   general model consisting of a simple parameterisable surface or a mean
   3D facial shape. The drawback of this approach is that is rather
   difficult to describe the non-rigid aspect of the face using just a
   single facial model. One way to capture the 3D facial deformations is by
   means of a statistical 3D model of the face or its parts. This is
   particularly evident when we want to capture the deformations of the
   mouth region. Even though statistical models of face are generally
   applied for modelling facial intensity, there are few approaches that
   fit a statistical model of 3D faces. In this paper, in order to capture
   and describe the non-rigid nature of facial surfaces we build a
   part-based statistical model of the 3D facial surface and we combine it
   with non-rigid iterative closest point algorithms. We show that the
   proposed algorithm largely outperforms state-of-the-art algorithms for
   3D face fitting and alignment especially when it comes to the
   description of the mouth region.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380390600083}},
}

@inproceedings{ ISI:000380390600012,
Author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
Book-Group-Author = {{IEEE}},
Title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering
   Representation}},
Booktitle = {{2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE
   AND GESTURE RECOGNITION (FG), VOL. 5}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2015}},
Note = {{IEEE 11th International Conference and Workshops on Automatic Face and
   Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Biometric Council}},
Abstract = {{Facial Expression Recognition (FER) is one of the most active topics in
   the domain of computer vision and pattern recognition, and it has
   received increasing attention for its wide application potentials as
   well as attractive scientific challenges. In this paper, we present a
   novel method to automatic 3D FER based on geometric scattering
   representation. A set of maps of shape features in terms of multiple
   order differential quantities, i.e. the Normal Maps (NOM) and the Shape
   Index Maps (SIM), are first jointly adopted to comprehensively describe
   geometry attributes of the facial surface. The scattering operator is
   then introduced to further highlight expression related cues on these
   maps, thereby constructing geometric scattering representations of 3D
   faces for classification. The scattering descriptor not only encodes
   distinct local shape changes of various expressions as by several
   milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
   information hidden in high frequencies, which is quite crucial to better
   distinguish expressions that are easily confused. We evaluate the
   proposed approach on the BU-3DFE database, and the performance is up to
   84.8\% and 82.7\% with two commonly used protocols respectively which is
   superior to the state of the art ones.}},
ISSN = {{2326-5396}},
ISBN = {{978-1-4799-6026-2}},
Unique-ID = {{ISI:000380390600012}},
}

@inproceedings{ ISI:000380483800005,
Author = {Liu, Jian and Zhang, Quan and Zhang, Chen and Tang, Chaojing},
Book-Group-Author = {{IEEE}},
Title = {{ROBUST NOSE TIP DETECTION FOR FACE RANGE IMAGES BASED ON LOCAL FEATURES
   IN SCALE- SPACE}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON 3D IMAGING (IC3D)}},
Series = {{International Conference on 3D Imaging}},
Year = {{2015}},
Note = {{International Conference on 3D Imaging (IC3D), Liege, BELGIUM, DEC
   14-15, 2015}},
Organization = {{The Inst of Elect and Elect Engn; Signal Proc Soc; 3D Stereo Media}},
Abstract = {{Being the most distinct feature point in 3D facial landmarks, nose tip
   plays a significant role in 3D facial studies such as face detection,
   face recognition, facial features extraction, face alignment, etc.
   Successful detection of nose tip can facilitate many tasks of 3D facial
   studies. In this paper, we propose a novel method to detect nose tip
   robustly. The method is robust to noise, needs not training, can handle
   large rotations and occlusions. To reduce computational cost, we first
   remove small isolated regions from the input range image, then establish
   scale-space by robust smoothing the preprocessed range image. In each
   scale of the scale-space, the Multi-angle Energy (ME) of each point is
   computed and sorted in descending order. Then the first. points in the
   descending order list are obtained and hierarchical clustering method is
   used to cluster these points. In the first h largest clusters, we can
   find one point with the largest ME. For all scales of the scale-space,
   we get a series of such points which are treated as nose tip candidates.
   For these candidates, we apply hierarchical clustering again. In the
   obtained largest cluster, we compute the mean value of ME. The ME of
   nose tip will be closest to the mean value. We evaluate our method in
   two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The
   experimental results verify the robustness of our method with a high
   nose tip detection rate.}},
ISSN = {{2379-1772}},
ISBN = {{978-1-5090-1265-7}},
Unique-ID = {{ISI:000380483800005}},
}

@inproceedings{ ISI:000380405100160,
Author = {Adriana Echeagaray-Patron, Beatriz and Miramontes-Jaramillo, Daniel and
   Kober, Vitaly},
Editor = {{Arabnia, HR and Deligiannidis, L and Tran, QN}},
Title = {{Conformal parameterization and curvature analysis for 3D facial
   recognition}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND COMPUTATIONAL
   INTELLIGENCE (CSCI)}},
Year = {{2015}},
Pages = {{843-844}},
Note = {{International Conference on Computational Science and Computational
   Intelligence (CSCI), Las Vegas, NV, DEC 07-09, 2015}},
Organization = {{Amer Council on Sci \& Educ}},
Abstract = {{This work proposes a new algorithm for 3D face recognition. The
   algorithm uses 3D shape data without color or texture information and
   exploits local curvature information which is a measure with high
   discriminant capability and robust to deformations such as rotation and
   scaling. In order to reduce high dimensionality of typical face surfaces
   our approach uses a conformal parameterization, preserving angles of
   original faces and simplifies the correspondence problem. Experimental
   results are presented and discussed using CASIA and Gavab databases.}},
DOI = {{10.1109/CSCI.2015.133}},
ISBN = {{978-1-4673-9795-7}},
ORCID-Numbers = {{Kober, Vitaly/0000-0002-9374-9883}},
Unique-ID = {{ISI:000380405100160}},
}

@inproceedings{ ISI:000380429100020,
Author = {Lagorio, A. and Cadoni, M. and Grosso, E. and Tistarelli, M.},
Book-Group-Author = {{IEEE}},
Title = {{A 3D ALGORITHM FOR UNSUPERVISED FACE IDENTIFICATION}},
Booktitle = {{2015 INTERNATIONAL WORKSHOP ON BIOMETRICS AND FORENSICS (IWBF)}},
Year = {{2015}},
Note = {{2015 3rd International Workshop on Biometrics and Forensics (IWBF),
   Gjovik, NORWAY, MAR 03-04, 2015}},
Organization = {{European Cooperat Sci \& Technol; IEEE; COST Act IC1106; Inst Engn \&
   Technol; European Assoc Signal Proc; European Assoc Biometr; HOGSKOLEN}},
Abstract = {{With the increasing availability of low-cost 3D data acquisition
   devices, the use of 3D face data for the recognition of individuals is
   becoming more appealing and computationally feasible. This paper
   proposes a completely automatic algorithm for face registration and
   matching. The algorithm is based on the extraction of stable 3D facial
   features characterizing the face and the subsequent construction of a
   signature manifold. The facial features are extracted by performing a
   continuous-to-discrete scale-space analysis. Registration is driven from
   the matching of triplets of feature points and the registration error is
   computed as shape matching score. Conversely to most techniques in the
   literature, a major advantage of the proposed method is that no data
   pre-processing is required. Therefore all presented results have been
   obtained exclusively from the raw data available from the 3D acquisition
   device.
   The method has been tested on the Bosphorus 3D face database and the
   performances compared to the ICP baseline algorithm. Even in presence of
   noise in the data, the algorithm proved to be very robust and reported
   identification performances which are aligned to the current state of
   the art, but without requiring any pre-processing of the raw data.}},
ISBN = {{978-1-4799-8105-2}},
Unique-ID = {{ISI:000380429100020}},
}

@inproceedings{ ISI:000377335600061,
Author = {Samad, Manar D. and Bobzien, Jonna L. and Harrington, John W. and
   Iftekharuddin, Khan M.},
Editor = {{Huan, J and Miyano, S and Shehu, A and Hu, X and Ma, B and Rajasekaran, S and Gombar, VK and Schapranow, IM and Yoo, IH and Zhou, JY and Chen, B and Pai, V and Pierce, B}},
Title = {{Analysis of Facial Muscle Activation in Children with Autism using 3D
   Imaging}},
Booktitle = {{PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIOINFORMATICS AND
   BIOMEDICINE}},
Series = {{IEEE International Conference on Bioinformatics and Biomedicine-BIBM}},
Year = {{2015}},
Pages = {{337-342}},
Note = {{IEEE International Conference on Bioinformatics and Biomedicine,
   Washington, DC, NOV 09-12, 2015}},
Organization = {{IEEE; IEEE Comp Soc; Natl Sci Fdn}},
Abstract = {{Autism Spectrum Disorder (ASD) impairs an individual's non-verbal skills
   including natural and contextual facial expressions. Such impairments
   may manifest as odd facial expressions (facial oddity) based on
   subjective evaluations of facial images. A few studies conducted on
   individuals with ASD have focused on the physiology of facial muscle
   usage by employing eletrophysiological sensors in response to visual
   stimuli. The sensors are placed directly on the face and may inhibit or
   limit the spontaneous facial response which may be too subtle for
   subjective human evaluations. This study uses a non-intrusive 3D facial
   imaging sensor that captures detailed geometric information of the face
   to facilitate quantification and detection of subtle changes in facial
   expression based on the physiology of facial muscle. A novel computer
   vision and data mining approach is developed from curve-based geometric
   feature of 3D facial data to discern the changes in the facial muscle
   actions. A pilot study is conducted with sixteen subjects (8 subjects
   with ASD and 8 typically-developing controls) where 3D facial images
   have been captured in response to visual stimuli involving 3D facial
   expressions. Statistical analyses reveal a significantly asymmetric
   facial muscle action in subjects with ASD compared to the
   typically-developing controls. This study demonstrates feasibility of
   using non-intrusive facial imaging sensor data in evaluating possible
   physiology-based impairments.}},
ISSN = {{2156-1125}},
ISBN = {{978-1-4673-6798-1}},
Unique-ID = {{ISI:000377335600061}},
}

@article{ ISI:000378042100004,
Author = {Abate, Andrea F. and Narducci, Fabio and Ricciardi, Stefano},
Title = {{BIOMETRICS EMPOWERED AMBIENT INTELLIGENCE ENVIRONMENT}},
Journal = {{ATTI ACCADEMIA PELORITANA DEI PERICOLANTI-CLASSE DI SCIENZE FISICHE
   MATEMATICHE E NATURALI}},
Year = {{2015}},
Volume = {{93}},
Number = {{2}},
Abstract = {{The paper presents a face recognition system based on 3D features with
   the aim of verifying the identity of subjects accessing a controlled
   Ambient Intelligence Environment and customizing all the services
   accordingly. The proposed approach relies on stereoscopic face
   acquisition and 3D mesh reconstruction avoiding non-automated and
   expensive 3D scanners, unsuitable for real time applications in general.
   A bidimensional feature descriptor is extracted from each 3D mesh. It
   consists in a color image transferring face's 3D features in a 2D space.
   An automatic weighting mask of each authorized person improves the
   robustness of recognition in presence of diverse facial expressions and
   beard. The experiments conducted show high average recognition rate and
   a measurable effectiveness of both flesh mask and expression weighting
   mask.}},
DOI = {{10.1478/AAPP.932A4}},
Article-Number = {{A4}},
ISSN = {{0365-0359}},
EISSN = {{1825-1242}},
ResearcherID-Numbers = {{Narducci, Fabio/R-5833-2017
   }},
ORCID-Numbers = {{Narducci, Fabio/0000-0003-4879-7138
   Ricciardi, Stefano/0000-0003-2123-452X}},
Unique-ID = {{ISI:000378042100004}},
}

@inproceedings{ ISI:000374793400004,
Author = {Wang, Hanchao and Mu, Zhichun and Zeng, Hui and Huang, Mingming},
Editor = {{Yang, J and Yang, J and Sun, Z and Shan, S and Zheng, W and Feng, J}},
Title = {{3D Face Recognition Using Local Features Matching on Sphere Depth
   Representation}},
Booktitle = {{BIOMETRIC RECOGNITION, CCBR 2015}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9428}},
Pages = {{27-34}},
Note = {{10th Chinese Conference on Biometric Recognition (CCBR), Tianjin,
   PEOPLES R CHINA, NOV 13-15, 2015}},
Organization = {{Chinese Assoc Artificial Intelligence; Springer; Civil Aviat Univ China;
   Tianjin Univ Sci \& Technol; CASIA, Inst Intelligent Recognit}},
Abstract = {{This paper proposes a 3D face recognition approach using sphere depth
   image, which is robust to pose variations in unconstrained environments.
   The input 3D face point clouds is first transformed into sphere depth
   images, and then represented as a 3DLBP image to enhance the
   distinctiveness of smooth and similar facial depth images. An improved
   SIFT algorithm is applied in the following matching process. The
   improved SIFT algorithm employs the learning to rank approach to select
   the keypoints with higher stability and repeatability instead of
   manually rule-based method used by the original SIFT algorithm. The
   proposed face recognition method is evaluated on CASIA 3D face database.
   And the experimental results show our approach has superior performance
   than many existing methods for 3D face recognition and handles pose
   variations quite well.}},
DOI = {{10.1007/978-3-319-25417-3\_4}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-25417-3; 978-3-319-25416-6}},
Unique-ID = {{ISI:000374793400004}},
}

@inproceedings{ ISI:000374793400019,
Author = {Liu, Shuai and Mu, Zhichun and Huang, Hongbo},
Editor = {{Yang, J and Yang, J and Sun, Z and Shan, S and Zheng, W and Feng, J}},
Title = {{3D Face Recognition Fusing Spherical Depth Map and Spherical Texture Map}},
Booktitle = {{BIOMETRIC RECOGNITION, CCBR 2015}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9428}},
Pages = {{151-159}},
Note = {{10th Chinese Conference on Biometric Recognition (CCBR), Tianjin,
   PEOPLES R CHINA, NOV 13-15, 2015}},
Organization = {{Chinese Assoc Artificial Intelligence; Springer; Civil Aviat Univ China;
   Tianjin Univ Sci \& Technol; CASIA, Inst Intelligent Recognit}},
Abstract = {{Face recognition in unconstrained environments is often influenced by
   pose variations. And the problem is basically the identification that
   uses partial data. In this paper, a method fusing structure and texture
   information is proposed to solve the problem. In the register phase, the
   approximate 180 degree information of face is acquired, and the data
   used to identify individual is obtained from a random single view. Pure
   face is extracted from 3D data first, then convert the original data to
   the form of spherical depth map (SDM) and spherical texture map (STM),
   which are invariant to out-plane rotation, subsequently facilitating the
   successive alignment-free identification that is robust to pose
   variations. We make identification through sparse representation for its
   well performance with the two maps. Experiments show that our proposed
   method gets a high recognition rate with pose and expression variations.}},
DOI = {{10.1007/978-3-319-25417-3\_19}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-25417-3; 978-3-319-25416-6}},
Unique-ID = {{ISI:000374793400019}},
}

@inproceedings{ ISI:000374793800022,
Author = {Matias Di Martino, J. and Fernandez, Alicia and Ferrari, Jose},
Editor = {{Pardo, A and Kittler, J}},
Title = {{One-Shot 3D-Gradient Method Applied to Face Recognition}},
Booktitle = {{PROGRESS IN PATTERN RECOGNITION, IMAGE ANALYSIS, COMPUTER VISION, AND
   APPLICATIONS, CIARP 2015}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9423}},
Pages = {{176-183}},
Note = {{20th Iberoamerican Congress on Pattern Recognition (CIARP), Montevideo,
   URUGUAY, NOV 09-12, 2015}},
Organization = {{IAPR; Uruguayan IAPR Chapter; Argentine Soc Pattern Recognit; Special
   Interest Grp Brazilian Comp Soc; Chilean Assoc Pattern Recognit; Cuban
   Assoc Pattern Recognit; Mexican Assoc Comp Vis, Neural Comp \& Robot;
   Spanish Assoc Pattern Recognit \& Image Anal; Portuguese Assoc Pattern
   Recognit}},
Abstract = {{In this work we describe a novel one-shot face recognition setup.
   Instead of using a 3D scanner to reconstruct the face, we acquire a
   single photo of the face of a person while a rectangular pattern is been
   projected over it. Using this unique image, it is possible to extract 3D
   low-level geometrical features without the explicit 3D reconstruction.
   To handle expression variations and occlusions that may occur (e.g.
   wearing a scarf or a bonnet), we extract information just from the
   eyes-forehead and nose regions which tend to be less influenced by
   facial expressions. Once features are extracted, SVM hyper-planes are
   obtained from each subject on the database (one vs all approach), then
   new instances can be classified according to its distance to each of
   those hyper-planes. The advantage of our method with respect to other
   ones published in the literature, is that we do not need and explicit 3D
   reconstruction. Experiments with the Texas 3D Database and with new
   acquired data are presented, which shows the potential of the presented
   framework to handle different illumination conditions, pose and facial
   expressions.}},
DOI = {{10.1007/978-3-319-25751-8\_22}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-25751-8; 978-3-319-25750-1}},
Unique-ID = {{ISI:000374793800022}},
}

@inproceedings{ ISI:000374793800033,
Author = {Matias Di Martino, J. and Fernandez, Alicia and Ferrari, Jose},
Editor = {{Pardo, A and Kittler, J}},
Title = {{Automatic Eyes and Nose Detection Using Curvature Analysis}},
Booktitle = {{PROGRESS IN PATTERN RECOGNITION, IMAGE ANALYSIS, COMPUTER VISION, AND
   APPLICATIONS, CIARP 2015}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9423}},
Pages = {{271-278}},
Note = {{20th Iberoamerican Congress on Pattern Recognition (CIARP), Montevideo,
   URUGUAY, NOV 09-12, 2015}},
Organization = {{IAPR; Uruguayan IAPR Chapter; Argentine Soc Pattern Recognit; Special
   Interest Grp Brazilian Comp Soc; Chilean Assoc Pattern Recognit; Cuban
   Assoc Pattern Recognit; Mexican Assoc Comp Vis, Neural Comp \& Robot;
   Spanish Assoc Pattern Recognit \& Image Anal; Portuguese Assoc Pattern
   Recognit}},
Abstract = {{In the present work we propose a method for detecting the nose and eyes
   position when we observe a scene that contains a face. The main goal of
   the proposed technique is that it capable of bypassing the 3D explicit
   mapping of the face and instead take advantage of the information
   available in the Depth gradient map of the face. To this end we will
   introduce a simple false positive rejection approach restricting the
   distance between the eyes, and between the eyes and the nose. The main
   idea is to use nose candidates to estimate those regions where is
   expected to find the eyes, and vice versa. Experiments with Texas
   database are presented and the proposed approach is testes when data
   presents different power of noise and when faces are in different
   positions with respect to the camera.}},
DOI = {{10.1007/978-3-319-25751-8\_33}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-25751-8; 978-3-319-25750-1}},
Unique-ID = {{ISI:000374793800033}},
}

@inproceedings{ ISI:000374476000057,
Author = {Liu, Shu and Fan, Yangyu and Guo, Zhe and Samal, Ashok},
Editor = {{He, X and Gao, X and Zhang, Y and Zhou, ZH and Liu, ZY and Fu, B and Hu, F and Zhang, Z}},
Title = {{2.5D Facial Attractiveness Computation Based on Data-Driven Geometric
   Ratios}},
Booktitle = {{INTELLIGENCE SCIENCE AND BIG DATA ENGINEERING: IMAGE AND VIDEO DATA
   ENGINEERING, ISCIDE 2015, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9242}},
Pages = {{564-573}},
Note = {{5th International Conference on Intelligence Science and Big Data
   Engineering (IScIDE), Suzhou, PEOPLES R CHINA, JUN 14-16, 2015}},
Organization = {{Chinese Golden Triangle Informat Sci \& Intelligence Sci Forum}},
Abstract = {{Computational approaches to investigating face attractiveness have
   become an emerging topic in facial analysis research. Integrating
   techniques from image analysis, pattern recognition and machine
   learning, this subarea aims to explore the nature, components and
   impacts of facial attractiveness and to develop computational algorithms
   to analyze the attractiveness of a face. In this paper we develop an
   attractiveness computation model for both frontal and profile images
   (2.5D). We focus on the role of geometric ratios in the determination of
   facial attractivenss. Stepwise regression is used as the feature
   selection method to select the discriminatory variables from a huge set
   of data-driven ratios. Decision tree is then used to generate an
   automated classifier for both frontal and profile computation models.
   The BJUT-3D Face Database is pre-processed and tested as our
   experimental dataset. The low statistic errors and high correlation
   indicate the accuracy of our computation models.}},
DOI = {{10.1007/978-3-319-23989-7\_57}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-23989-7; 978-3-319-23987-3}},
Unique-ID = {{ISI:000374476000057}},
}

@inproceedings{ ISI:000371977803080,
Author = {Batabyal, Tamal and Vaccari, Andrea and Acton, Scott T.},
Book-Group-Author = {{IEEE}},
Title = {{UGraSP: A UNIFIED FRAMEWORK FOR ACTIVITY RECOGNITION AND PERSON
   IDENTIFICATION USING GRAPH SIGNAL PROCESSING}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{3270-3274}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{With the growing availability and wide distribution of low-cost,
   high-performance 3D imaging sensors, the image analysis community has
   witnessed an increased demand for solutions to the challenges of
   activity recognition and person identification. We propose an integrated
   framework, based on graph signal processing, that simultaneously
   performs both tasks using a single set of features. The novelty of our
   approach is based on the fact that the set of features used for activity
   recognition accommodates person identification without additional
   computation. The analysis is based on the extracted structure-invariant
   graph (skeleton). The Laplacian of the skeleton is used both to identify
   the person and recognize the performed activity. While person
   identification is achieved directly from the analysis of the Laplacian,
   activity recognition is obtained after transformation, into the graph
   spectral domain, of the vectorized form of the skeletal joints 3D
   coordinates. Feature vectors for activity recognition are then derived,
   in this domain, from the covariance matrices evaluated over fixed-length
   sequential video segments. Both classification tasks are implemented
   using linear support vector machines (SVM). When applied to real
   activity datasets, our approach shows an improved performance over the
   existing state-of-the-art.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977803080}},
}

@inproceedings{ ISI:000371977803154,
Author = {Lv, Shiwen and Da, Feipeng and Deng, Xing},
Book-Group-Author = {{IEEE}},
Title = {{A 3D Face Recognition Method Using Region-Based Extended Local Binary
   Pattern}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{3635-3639}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{A 3D face recognition method using region-based extended local binary
   pattern (eLBP) is proposed. First, the depth image converted from the
   preprocessed 3D pointclouds is normalized. Then, different regions
   according to their distortions under facial expressions are extracted by
   binary masks and represented by the uniform pattern of extended LBP.
   Finally, sparse representation classifier (SRC) is adopted for
   classification on the single region. Feature-level and score-level
   fusion with weight-sparse representation classifier (W-SRC) are also
   tested and compared, and the latter has better performance. The
   experiments on FRGC v2.0 database demonstrate that the proposed method
   is robust and efficient.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977803154}},
}

@inproceedings{ ISI:000371977804130,
Author = {Arteaga, Reynaldo J. and Ruuth, Steven J.},
Book-Group-Author = {{IEEE}},
Title = {{LAPLACE-BELTRAMI SPECTRA FOR SHAPE COMPARISON OF SURFACES IN 3D USING
   THE CLOSEST POINT METHOD}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{4511-4515}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{The need to compare separate objects arises in a wide range of
   applications. In one approach for comparing objects, `ShapeDNA' is
   constructed to give a numerical fingerprint representing an individual
   object. Shape-DNA is a cropped set of eigenvalues of the
   Laplace-Beltrami operator for the surface of the object. In this paper,
   we compute the Shape-DNA of surfaces using the closest point method. Our
   approach may be applied to a variety of surface representations
   including triangulations, point clouds and certain analytical shapes. A
   2D multidimensional scaling plot illustrates that similar objects form
   groups based on the Shape-DNAs. Our method has the benefit that it may
   be applied to surfaces defined by dense point clouds without requiring
   the construction of point connectivity.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
Unique-ID = {{ISI:000371977804130}},
}

@inproceedings{ ISI:000352725200030,
Author = {Shahzad, M. and Schmitt, M. and Zhu, X. X.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{SEGMENTATION AND CROWN PARAMETER EXTRACTION OF INDIVIDUAL TREES IN AN
   AIRBORNE TOMOSAR POINT CLOUD}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W2}},
Pages = {{205-209}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{The analysis of individual trees is an important field of research in
   the forest remote sensing community. While the current state-of-the-art
   mostly focuses on the exploitation of optical imagery and airborne LiDAR
   data, modern SAR sensors have not yet met the interest of the research
   community in that regard. This paper describes how several critical
   parameters of individual deciduous trees can be extraced from airborne
   multi-aspect TomoSAR point clouds: First, the point cloud is segmented
   by unsupervised mean shift clustering. Then ellipsoid models are fitted
   to the points of each cluster. Finally, from these 3D ellipsoids the
   geometrical tree parameters location, height and crown radius are
   extracted. Evaluation with respect to a manually derived reference
   dataset prove that almost 86\% of all trees are localized, thus
   providing a promising perspective for further research towards
   individual tree recognition from SAR data.}},
DOI = {{10.5194/isprsarchives-XL-3-W2-205-2015}},
ISSN = {{2194-9034}},
ORCID-Numbers = {{Zhu, Xiaoxiang/0000-0001-5530-3613}},
Unique-ID = {{ISI:000352725200030}},
}

@inproceedings{ ISI:000352725200038,
Author = {Vetrivel, A. and Gerke, M. and Kerle, N. and Vosselman, G.},
Editor = {{Stilla, U and Heipke, C}},
Title = {{Segmentation of UAV-based images incorporating 3D point cloud
   information}},
Booktitle = {{PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2015}},
Volume = {{40-3}},
Number = {{W2}},
Pages = {{261-268}},
Note = {{Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
   Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
   Univ Munchen, Munich, GERMANY, MAR 25-27, 2015}},
Organization = {{ISPRS}},
Abstract = {{Numerous applications related to urban scene analysis demand automatic
   recognition of buildings and distinct sub-elements. For example, if
   LiDAR data is available, only 3D information could be leveraged for the
   segmentation. However, this poses several risks, for instance, the
   in-plane objects cannot be distinguished from their surroundings. On the
   other hand, if only image based segmentation is performed, the geometric
   features (e.g., normal orientation, planarity) are not readily
   available. This renders the task of detecting the distinct sub-elements
   of the building with similar radiometric characteristic infeasible. In
   this paper the individual sub-elements of buildings are recognized
   through sub-segmentation of the building using geometric and radiometric
   characteristics jointly. 3D points generated from Unmanned Aerial
   Vehicle (UAV) images are used for inferring the geometric
   characteristics of roofs and facades of the building. However, the
   image-based 3D points are noisy, error prone and often contain gaps.
   Hence the segmentation in 3D space is not appropriate. Therefore, we
   propose to perform segmentation in image space using geometric features
   from the 3D point cloud along with the radiometric features. The initial
   detection of buildings in 3D point cloud is followed by the segmentation
   in image space using the region growing approach by utilizing various
   radiometric and 3D point cloud features. The developed method was tested
   using two data sets obtained with UAV images with a ground resolution of
   around 1-2 cm. The developed method accurately segmented most of the
   building elements when compared to the plane-based segmentation using 3D
   point cloud alone.}},
DOI = {{10.5194/isprsarchives-XL-3-W2-261-2015}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{Vosselman, George/D-3985-2009
   Gerke, Markus/A-8791-2012
   Kerle, Norman/A-5508-2010}},
ORCID-Numbers = {{Vosselman, George/0000-0001-8813-8028
   Gerke, Markus/0000-0002-2221-6182
   }},
Unique-ID = {{ISI:000352725200038}},
}

@inproceedings{ ISI:000370974903006,
Author = {Linder, Timm and Wehner, Sven and Arras, Kai O.},
Book-Group-Author = {{IEEE}},
Title = {{Real-Time Full-Body Human Gender Recognition in (RGB)-D Data}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)}},
Series = {{IEEE International Conference on Robotics and Automation ICRA}},
Year = {{2015}},
Pages = {{3039-3045}},
Note = {{IEEE International Conference on Robotics and Automation (ICRA),
   Seattle, WA, MAY 26-30, 2015}},
Organization = {{IEEE}},
Abstract = {{Understanding social context is an important skill for robots that share
   a space with humans. In this paper, we address the problem of
   recognizing gender, a key piece of information when interacting with
   people and understanding human social relations and rules. Unlike
   previous work which typically considered faces or frontal body views in
   image data, we address the problem of recognizing gender in RGB-D data
   from side and back views as well. We present a large, gender-balanced,
   annotated, multi-perspective RGB-D dataset with full-body views of over
   a hundred different persons captured with both the Kinect v1 and Kinect
   v2 sensor. We then learn and compare several classifiers on the Kinect
   v2 data using a HOG baseline, two state-of-the-art deep-learning
   methods, and a recent tessellation-based learning approach. Originally
   developed for person detection in 3D data, the latter is able to learn
   the best selection, location and scale of a set of simple point cloud
   features. We show that for gender recognition, it outperforms the other
   approaches for both standing and walking people while being very
   efficient to compute with classification rates up to 150 Hz.}},
ISSN = {{1050-4729}},
ISBN = {{978-1-4799-6923-4}},
Unique-ID = {{ISI:000370974903006}},
}

@article{ ISI:000369689500019,
Author = {Wahab, Wahidin and Ridwan, M. and Kusumoputro, Benyamin},
Title = {{DESIGN AND IMPLEMENTATION OF AN AUTOMATIC FACE-IMAGE DATA ACQUISITION
   SYSTEM USING IP BASED MULTI CAMERA}},
Journal = {{INTERNATIONAL JOURNAL OF TECHNOLOGY}},
Year = {{2015}},
Volume = {{6}},
Number = {{6}},
Pages = {{1042-1049}},
Abstract = {{Current research trends in 3D Face recognition system requires a special
   hardware for fast capturing face image data from multi angle view. To
   support this research, we had designed and implemented an automatic
   image data acquisition system using multi-camera for capturing facial
   images from 5 degrees different angle views, which spanned horizontally
   from 180 degrees from left to right, and vertically from horizontal up
   to 70 degrees above the face. The system was designed using 30 IP
   cameras that were mounted on two rigid steel arms that had the form of
   three quarter of a circle, the two steel arms formed the angle of 90
   degrees to each other. At each arm, 15 IP cameras were mounted with 5
   degrees spacing vertically to each others. This arm was driven by a DC
   motor which was controlled by a microcontroller and supervised directly
   by a laptop computer along with the data acquisition activities. The
   software for capturing images was designed using C\# GUI programming
   language. The system had been working in good condition and image-data
   were saved in JPEG format. Time duration of capturing images data for
   one object face expression with 30 times capturing for the whole angle
   views, was only 3 minutes 44.5 seconds with total number of 16,650
   images collected. The delay time between two cameras capturing was less
   than 1 sec. This project is aimed to support the 3D face recognition
   research in the department}},
ISSN = {{2086-9614}},
EISSN = {{2087-2100}},
ORCID-Numbers = {{Ridwan, Muhammad/0000-0002-7415-0920}},
Unique-ID = {{ISI:000369689500019}},
}

@inproceedings{ ISI:000368591300013,
Author = {Said, Salwa and Jemai, Olfa and Zaied, Mourad and Ben Amar, Chokri},
Editor = {{Verikas, A and Radeva, P and Nikolaev, D}},
Title = {{3D Fast Wavelet Network Model-Assisted 3D Face Recognition}},
Booktitle = {{EIGHTH INTERNATIONAL CONFERENCE ON MACHINE VISION (ICMV 2015)}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9875}},
Note = {{8th International Conference on Machine Vision (ICMV), Barcelona, SPAIN,
   NOV 19-21, 2015}},
Abstract = {{In last years, the emergence of 3D shape in face recognition is due to
   its robustness to pose and illumination changes. These attractive
   benefits are not all the challenges to achieve satisfactory recognition
   rate. Other challenges such as facial expressions and computing time of
   matching algorithms remain to be explored. In this context, we propose
   our 3D face recognition approach using 3D wavelet networks. Our approach
   contains two stages: learning stage and recognition stage. For the
   training we propose a novel algorithm based on 3D fast wavelet
   transform. From 3D coordinates of the face (x,y,z), we proceed to
   voxelization to get a 3D volume which will be decomposed by 3D fast
   wavelet transform and modeled after that with a wavelet network, then
   their associated weights are considered as vector features to represent
   each training face. For the recognition stage, an unknown identity face
   is projected on all the training WN to obtain a new vector features
   after every projection. A similarity score is computed between the old
   and the obtained vector features. To show the efficiency of our
   approach, experimental results were performed on all the FRGC v.2
   benchmark.}},
DOI = {{10.1117/12.2228368}},
Article-Number = {{98750E}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0116-1}},
Unique-ID = {{ISI:000368591300013}},
}

@inproceedings{ ISI:000367310300024,
Author = {Guo, Zhe and Liu, Shu and Wang, Yi and Lei, Tao},
Editor = {{Wang, Y and Jiang, X}},
Title = {{Learning Deformation Model for Expression-Robust 3D Face Recognition}},
Booktitle = {{SEVENTH INTERNATIONAL CONFERENCE ON GRAPHIC AND IMAGE PROCESSING (ICGIP
   2015)}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9817}},
Note = {{7th International Conference on Graphic and Image Processing (ICGIP),
   Singapore, SINGAPORE, OCT 23-25, 2015}},
Organization = {{Wuhan Univ; Int Assoc Comp Sci \& Informat Technol}},
Abstract = {{Expression change is the major cause of local plastic deformation of the
   facial surface. The intra-class differences with large expression change
   somehow are larger than the inter-class differences as it's difficult to
   distinguish the same individual with facial expression change. In this
   paper, an expression-robust 3D face recognition method is proposed by
   learning expression deformation model. The expression of the individuals
   on the training set is modeled by principal component analysis, the main
   components are retained to construct the facial deformation model. For
   the test 3D face, the shape difference between the test and the neutral
   face in training set is used for reconstructing the expression change by
   the constructed deformation model. The reconstruction residual error is
   used for face recognition. The average recognition rate on GavabDB and
   self-built database reaches 85.1\% and 83\%, respectively, which shows
   strong robustness for expression changes.}},
DOI = {{10.1117/12.2228002}},
Article-Number = {{98170O}},
ISSN = {{0277-786X}},
ISBN = {{978-1-5106-0058-4}},
Unique-ID = {{ISI:000367310300024}},
}

@inproceedings{ ISI:000366513400025,
Author = {Adriana Echeagaray-Patron, Beatriz and Kober, Vitaly},
Editor = {{Awwal, AAS and Iftekharuddin, KM and Matin, MA and Vazquez, MG and Marquez, A}},
Title = {{3D face recognition based on matching of facial surfaces}},
Booktitle = {{OPTICS AND PHOTONICS FOR INFORMATION PROCESSING IX}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9598}},
Note = {{9th Conference of Optics and Photonics for Information Processing, San
   Diego, CA, AUG 10-12, 2015}},
Organization = {{SPIE}},
Abstract = {{Face recognition is an important task in pattern recognition and
   computer vision. In this work a method for 3D face recognition in the
   presence of facial expression and poses variations is proposed. The
   method uses 3D shape data without color or texture information. A new
   matching algorithm based on conformal mapping of original facial
   surfaces onto a Riemannian manifold followed by comparison of conformal
   and isometric invariants computed in the manifold is suggested.
   Experimental results are presented using common 3D face databases that
   contain significant amount of expression and pose variations.}},
DOI = {{10.1117/12.2186695}},
Article-Number = {{95980V}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-764-7}},
ORCID-Numbers = {{Kober, Vitaly/0000-0002-9374-9883}},
Unique-ID = {{ISI:000366513400025}},
}

@inproceedings{ ISI:000364991200038,
Author = {De Giorgis, Nikolas and Rocca, Luigi and Puppo, Enrico},
Editor = {{Murino, V and Puppo, E}},
Title = {{Scale-Space Techniques for Fiducial Points Extraction from 3D Faces}},
Booktitle = {{IMAGE ANALYSIS AND PROCESSING - ICIAP 2015, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{9279}},
Pages = {{421-431}},
Note = {{18th International Conference on Image Analysis and Processing (ICIAP),
   Genoa, ITALY, SEP 07-11, 2015}},
Organization = {{Datalogic; Google Inc; Centro Studi Gruppo Orizzonti Holding; Ansaldo
   Energia; EBIT Esaote; Softeco; eVS embedded Vis Syst S r l; 3DFlow S r
   l; Camelot Biomed Syst S r l; Ist Italiano Tecnologia, Pattern Anal \&
   Comp Vis Dept; Univ Genova; Univ Verona; Camera Commercio Genova; Comune
   Genova}},
Abstract = {{We propose a method for extracting fiducial points from human faces that
   uses 3D information only and is based on two key steps: multi-scale
   curvature analysis, and the reliable tracking of features in a
   scale-space based on curvature. Our scale-space analysis, coupled to
   careful use of prior information based on variability boundaries of
   anthropometric facial proportions, does not require a training step,
   because it makes direct use of morphological characteristics of the
   analyzed surface. The proposed method precisely identifies important
   fiducial points and is able to extract new fiducial points that were
   previously unrecognized, thus paving the way to more effective
   recognition algorithms.}},
DOI = {{10.1007/978-3-319-23231-7\_38}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-23231-7; 978-3-319-23230-0}},
ORCID-Numbers = {{Puppo, Enrico/0000-0001-9780-5283}},
Unique-ID = {{ISI:000364991200038}},
}

@inproceedings{ ISI:000364714000046,
Author = {Tian, Lei and Fan, Chunxiao and Ming, Yue and Shi, Jiakun},
Editor = {{Liu, H and Kubota, N and Zhu, X and Dillmann, R and Zhou, D}},
Title = {{SRDANet: An Efficient Deep Learning Algorithm for Face Analysis}},
Booktitle = {{INTELLIGENT ROBOTICS AND APPLICATIONS, ICIRA 2015, PT I}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2015}},
Volume = {{9244}},
Pages = {{499-510}},
Note = {{8th International Conference on Intelligent Robotics and Applications
   (ICIRA), Portsmouth, ENGLAND, AUG 24-27, 2015}},
Abstract = {{In this work, we take advantage of the superiority of Spectral Graph
   Theory in classification application and propose a novel deep learning
   framework for face analysis which is called Spectral Regression
   Discriminant Analysis Network (SRDANet). Our SRDANet model shares the
   same basic architecture of Convolutional Neural Network (CNN), which
   comprises three basic components: convolutional filter layer, nonlinear
   processing layer and feature pooling layer. While it is different from
   traditional deep learning network that in our convolutional layer, we
   extract the leading eigenvectors from patches in facial image which are
   used as filter kernels instead of randomly initializing kernels and
   update them by stochastic gradient descent (SGD). And the output of all
   cascaded convolutional filter layers is used as the input of nonlinear
   processing layer. In the following nonlinear processing layer, we use
   hashing method for nonlinear processing. In feature pooling layer, the
   block-based histograms are employed to pooling output features instead
   of max-pooling technique. At last, the output of feature pooling layer
   is considered as one final feature output of our model. Different from
   the previous single-task research for face analysis, our proposed
   approach demonstrates an excellent performance in face recognition and
   expression recognition with 2D/3D facial images simultaneously.
   Extensive experiments conducted on many different face analysis
   databases demonstrate the efficiency of our proposed SRDANet model.
   Databases such as Extended Yale B, PIE, ORL are used for 2D face
   recognition, FRGC v2 is used for 3D face recognition and BU-3DFE is used
   for 3D expression recognition.}},
DOI = {{10.1007/978-3-319-22879-2\_46}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-22879-2; 978-3-319-22878-5}},
Unique-ID = {{ISI:000364714000046}},
}

@inproceedings{ ISI:000365181700036,
Author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
Editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal, JK}},
Title = {{Range Face Image Registration Using ERFI from 3D Images}},
Booktitle = {{PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF
   INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2}},
Series = {{Advances in Intelligent Systems and Computing}},
Year = {{2015}},
Volume = {{328}},
Pages = {{323-333}},
Note = {{3rd International Conference on Frontiers in Intelligent Computing -
   Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014}},
Organization = {{Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol \& Sci, CSI Student
   Branch}},
Abstract = {{In this paper, we present a novel and robust approach for 3D faces
   registration based on Energy Range Face Image (ERFI). ERFI is the
   frontal face model for the individual people from the database. It can
   be considered as a mean frontal range face image for each person. Thus,
   the total energy of the frontal range face images has been preserved by
   ERFI. For registration purpose, an interesting point or a land mark,
   which is the nose tip (or `pronasal') from face surface is extracted.
   Then, this landmark is exploited to correct the oriented faces by
   applying the 3D geometrical rotation technique with respect to the ERFI
   model for registration purpose. During the error calculation phase,
   Manhattan distance metric between the localized `pronasal' landmark on
   face image and that of ERFI model is determined on Euclidian space. The
   accuracy is quantified with selection of cut-points `T' on measured
   Manhattan distances along yaw, pitch and roll. The proposed method has
   been tested on Frav3D database and achieved 82.5\% accurate pose
   registration.}},
DOI = {{10.1007/978-3-319-12012-6\_36}},
ISSN = {{2194-5357}},
ISBN = {{978-3-319-12012-6; 978-3-319-12011-9}},
ResearcherID-Numbers = {{Bhattacharjee, Debotosh/L-8521-2015
   }},
ORCID-Numbers = {{Bhattacharjee, Debotosh/0000-0002-4483-706X
   Bhattacharjee, Debotosh/0000-0002-1163-6413}},
Unique-ID = {{ISI:000365181700036}},
}

@inproceedings{ ISI:000363756900031,
Author = {Ming, Yue and Jin, Yi},
Editor = {{Liu, H and Kubota, N and Zhu, X and Dillmann, R and Zhou, D}},
Title = {{Robust 3D Local SIFT Features for 3D Face Recognition}},
Booktitle = {{INTELLIGENT ROBOTICS AND APPLICATIONS (ICIRA 2015), PT III}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2015}},
Volume = {{9246}},
Pages = {{352-359}},
Note = {{8th International Conference on Intelligent Robotics and Applications
   (ICIRA), Portsmouth, ENGLAND, AUG 24-27, 2015}},
Abstract = {{In this paper, a robust 3D local SIFT feature is proposed for 3D face
   recognition. For preprocessing the original 3D face data, facial
   regional segmentation is first employed by fusing curvature
   characteristics and shape band mechanism. Then, we design a new local
   descriptor for the extracted regions, called 3D local Scale-Invariant
   Feature Transform (3D LSIFT). The key point detection based on 3D LSIFT
   can effectively reflect the geometric characteristic of 3D facial
   surface by encoding the gray and depth information captured by 3D face
   data. Then, 3D LSIFT descriptor extends to describe the discrimination
   on 3D faces. Experimental results based on the common international 3D
   face databases demonstrate the higher-qualified performance of our
   proposed algorithm with effectiveness, robustness, and universality.}},
DOI = {{10.1007/978-3-319-22873-0\_31}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-22873-0; 978-3-319-22872-3}},
Unique-ID = {{ISI:000363756900031}},
}

@inproceedings{ ISI:000361841100052,
Author = {Lin, Yizhou and Hua, Gang and Mordohai, Philippos},
Editor = {{Agapito, L and Bronstein, MM and Rother, C}},
Title = {{Egocentric Object Recognition Leveraging the 3D Shape of the Grasping
   Hand}},
Booktitle = {{COMPUTER VISION - ECCV 2014 WORKSHOPS, PT III}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{8927}},
Pages = {{746-762}},
Note = {{13th European Conference on Computer Vision (ECCV), Zurich, SWITZERLAND,
   SEP 06-12, 2014}},
Abstract = {{We present a systematic study on the relationship between the 3D shape
   of a hand that is about to grasp an object and recognition of the object
   to be grasped. In this paper, we investigate the direction from the
   shape of the hand to object recognition for unimpaired users. Our work
   shows that the 3D shape of a grasping hand from an egocentric point of
   view can help improve recognition of the objects being grasped. Previous
   work has attempted to exploit hand interactions or gaze information in
   the egocentric setting to guide object segmentation. However, all such
   analyses are conducted in 2D. We hypothesize that the 3D shape of a
   grasping hand is highly correlated to the physical attributes of the
   object being grasped. Hence, it can provide very beneficial visual
   information for object recognition. We validate this hypothesis by first
   building a 3D, egocentric vision pipeline to segment and reconstruct
   dense 3D point clouds of the grasping hands. Then, visual descriptors
   are extracted from the point cloud and subsequently fed into an object
   recognition system to recognize the object being grasped. Our
   experiments demonstrate that the 3D hand shape can indeed greatly help
   improve the visual recognition accuracy, when compared with the baseline
   where only 2D image features are utilized.}},
DOI = {{10.1007/978-3-319-16199-0\_52}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-16199-0; 978-3-319-16198-3}},
Unique-ID = {{ISI:000361841100052}},
}

@inproceedings{ ISI:000361841100053,
Author = {Li, Wei and Li, Xudong and Goldberg, Martin and Zhu, Zhigang},
Editor = {{Agapito, L and Bronstein, MM and Rother, C}},
Title = {{Face Recognition by 3D Registration for the Visually Impaired Using a
   RGB-D Sensor}},
Booktitle = {{COMPUTER VISION - ECCV 2014 WORKSHOPS, PT III}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{8927}},
Pages = {{763-777}},
Note = {{13th European Conference on Computer Vision (ECCV), Zurich, SWITZERLAND,
   SEP 06-12, 2014}},
Abstract = {{To help visually impaired people recognize people in their daily life, a
   3D face feature registration approach is proposed with a RGB-D sensor.
   Compared to 2D face recognition methods, 3D data based approaches are
   more robust to the influence of face orientations and illumination
   changes. Different from most 3D data based methods, we employ a one-step
   ICP registration approach that is much less time consuming. The error
   tolerance of the 3D registration approach is analyzed with various error
   levels in 3D measurements. The method is tested with a Kinect sensor, by
   analyzing both the angular and distance errors to recognition
   performance. A number of other potential benefits in using 3D face data
   are also discussed, such as RGB image rectification, multiple-view face
   integration, and facial expression modeling, all useful for social
   interactions of visually impaired people with others.}},
DOI = {{10.1007/978-3-319-16199-0\_53}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-16199-0; 978-3-319-16198-3}},
Unique-ID = {{ISI:000361841100053}},
}

@inproceedings{ ISI:000361702900005,
Author = {Moeini, Ali and Moeini, Hossein},
Editor = {{Ji, Q and Moeslund, TB and Hua, G and Nasrollahi, K}},
Title = {{Multimodal Facial Expression Recognition Based on 3D Face Reconstruction
   from 2D Images}},
Booktitle = {{FACE AND FACIAL EXPRESSION RECOGNITION FROM REAL WORLD VIDEOS}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2015}},
Volume = {{8912}},
Pages = {{46-57}},
Note = {{International Workshop on Face and Facial Expression Recognition from
   Real World Videos (FFER), Stockholm, SWEDEN, AUG 24, 2014}},
Abstract = {{In this paper, a novel feature extraction method was proposed for facial
   expression recognition. A 3D Facial Expression Generic Elastic Model (3D
   FE-GEM) was proposed to reconstruct an expression-invariant 3D model of
   each human face in the present database using only a single 2D frontal
   image with/without facial expressions. Then, the texture and depth of
   the face were extracted from the reconstructed model. Afterwards, the
   Gabor filter bank was applied to both texture and reconstructed depth of
   the face to extract the feature vectors from both texture and
   reconstructed depth images. Finally, by combining 2D and 3D feature
   vectors, the final feature vectors are generated and classified by the
   Support Vector Machine (SVM). Favorable outcomes were acquired for
   facial expression recognition on the available image database based on
   the proposed method compared to several state-of-the-arts in facial
   expression recognition.}},
DOI = {{10.1007/978-3-319-13737-7\_5}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-13737-7; 978-3-319-13736-0}},
Unique-ID = {{ISI:000361702900005}},
}

@inproceedings{ ISI:000360175900188,
Author = {Naveen, S. and Moni, R. S.},
Editor = {{Samuel, P}},
Title = {{Multimodal Face Recognition System using Spectral Transformation of 2D
   Texture feature and Statistical processing of Face Range Images}},
Booktitle = {{PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION AND
   COMMUNICATION TECHNOLOGIES, ICICT 2014}},
Series = {{Procedia Computer Science}},
Year = {{2015}},
Volume = {{46}},
Pages = {{1537-1545}},
Note = {{International Conference on Information and Communication Technologies
   (ICICT), Kochi, INDIA, DEC 03-05, 2014}},
Organization = {{Cochin Uni Sci \& Technol, Sch Engn; TEQIP Phase II}},
Abstract = {{3D Face recognition has been an area of interest for the past few
   decades in pattern recognition. This paper focuses on problems of person
   identification using 3D Face data. Here unregistered Face data, i.e.
   both texture and depth is fed to classifier in spectral representations
   of data. 2D Discrete Fourier Transform (DFT) is used for spectral
   representation. Fusion of scores improves the recognition accuracy
   significantly since use of depth information alone in spectral
   representation was not sufficient to increase accuracy. Statistical
   method seems to degrade performance of system when applied to texture
   data and was effective for depth data. (C) 2015 The Authors. Published
   by Elsevier B.V.}},
DOI = {{10.1016/j.procs.2015.02.078}},
ISSN = {{1877-0509}},
Unique-ID = {{ISI:000360175900188}},
}

@inproceedings{ ISI:000359471600007,
Author = {Balaban, Stephen},
Editor = {{Kakadiaris, IA and Kumar, A and Scheirer, WJ}},
Title = {{Deep learning and face recognition: the state of the art}},
Booktitle = {{BIOMETRIC AND SURVEILLANCE TECHNOLOGY FOR HUMAN AND ACTIVITY
   IDENTIFICATION XII}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9457}},
Note = {{Conference on Biometric and Surveillance Technology for Human and
   Activity Identification XII, Baltimore, MD, APR 22, 2015}},
Organization = {{SPIE}},
Abstract = {{Deep Neural Networks (DNNs) have established themselves as a dominant
   technique in machine learning. DNNs have been top performers on a wide
   variety of tasks including image classification, speech recognition, and
   face recognition.(1-3) Convolutional neural networks (CNNs) have been
   used in nearly all of the top performing methods on the Labeled Faces in
   the Wild (LFW) dataset.(3-6) In this talk and accompanying paper, I
   attempt to provide a review and summary of the deep learning techniques
   used in the state-of-the-art. In addition, I highlight the need for both
   larger and more challenging public datasets to benchmark these systems.
   Despite the ability of DNNs and autoencoders to perform unsupervised
   feature learning, modern facial recognition pipelines still require
   domain specific engineering in the form of re-alignment. For example, in
   Facebook's recent Deep Face paper, a 3D ``frontalization{''} step lies
   at the beginning of the pipeline. This step creates a 3D face model for
   the incoming image and then uses a series of affine transformations of
   the fiducial points to ``frontalize{''} the image. This step enables the
   Deep Face system to use a neural network architecture with locally
   connected layers without weight sharing as opposed to standard
   convolutional layers.(6) Deep learning techniques combined with large
   datasets have allowed research groups to surpass human level performance
   on the LFW dataset.(3,5)
   The high accuracy (99.63\% for Face Net at the time of publishing) and
   utilization of outside data (hundreds of millions of images in the case
   of Google's Face Net) suggest that current face verification benchmarks
   such as LFW may not be challenging enough, nor provide enough data, for
   current techniques.(3,5) There exist a variety of organizations with
   mobile photo sharing applications that would be capable of releasing a
   very large scale and highly diverse dataset of facial images captured on
   mobile devices. Such an ``Image Net for Face Recognition{''} would
   likely receive a warm welcome from researchers and practitioners alike.}},
DOI = {{10.1117/12.2181526}},
Article-Number = {{94570B}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-573-5}},
Unique-ID = {{ISI:000359471600007}},
}

@inproceedings{ ISI:000354613300002,
Author = {Luo, Jing and Geng, Shu Ze and Xiao, Zhao Xia and Xiu, Chun Bo},
Editor = {{Wang, Y and Jiang, X and Zhang, D}},
Title = {{A Review of recent advances in 3D Face Recognition}},
Booktitle = {{SIXTH INTERNATIONAL CONFERENCE ON GRAPHIC AND IMAGE PROCESSING (ICGIP
   2014)}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9443}},
Note = {{6th International Conference on Graphic and Image Processing (ICGIP),
   Beijing, PEOPLES R CHINA, OCT 24-26, 2014}},
Organization = {{Int Assoc Comp Sci \& Informat Technol; Wuhan Univ}},
Abstract = {{Face recognition based on machine vision has achieved great advances and
   been widely used in the various fields. However, there are some
   challenges on the face recognition, such as facial pose, variations in
   illumination, and facial expression. So, this paper gives the recent
   advances in 3D face recognition. 3D face recognition approaches are
   categorized into four groups: minutiae approach, space transform
   approach, geometric features approach, model approach. Several typical
   approaches are compared in detail, including feature extraction,
   recognition algorithm, and the performance of the algorithm. Finally,
   this paper summarized the challenge existing in 3D face recognition and
   the future trend. This paper aims to help the researches majoring on
   face recognition.}},
DOI = {{10.1117/12.2178750}},
Article-Number = {{944303}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-558-2}},
Unique-ID = {{ISI:000354613300002}},
}

@inproceedings{ ISI:000353328200021,
Author = {Qu, Chengchao and Monari, Eduardo and Schuchert, Tobias and Beyerer,
   Juergen},
Editor = {{Lam, EY and Niel, KS}},
Title = {{Realistic Texture Extraction for 3D Face Models Robust to Self-Occlusion}},
Booktitle = {{IMAGE PROCESSING: MACHINE VISION APPLICATIONS VIII}},
Series = {{Proceedings of SPIE}},
Year = {{2015}},
Volume = {{9405}},
Note = {{Conference on Image Processing - Machine Vision Applications VIII, San
   Francisco, CA, FEB 10-11, 2015}},
Organization = {{Soc Imaging Sci \& Technol; SPIE}},
Abstract = {{In the context of face modeling, probably the most well-known approach
   to represent 3D faces is the 3D Morphable Model (3DMM). When 3DMM is
   fitted to a 2D image, the shape as well as the texture and illumination
   parameters are simultaneously estimated. However, if real facial texture
   is needed, texture extraction from the 2D image is necessary. This paper
   addresses the possible problems in texture extraction of a single image
   caused by self-occlusion. Unlike common approaches that leverage the
   symmetric property of the face by mirroring the visible facial part,
   which is sensitive to inhomogeneous illumination, this work first
   generates a virtual texture map for the skin area iteratively by
   averaging the color of neighbored vertices. Although this step creates
   unrealistic, overly smoothed texture, illumination stays constant
   between the real and virtual texture. In the second pass, the mirrored
   texture is gradually blended with the real or generated texture
   according to the visibility. This scheme ensures a gentle handling of
   illumination and yet yields realistic texture. Because the blending area
   only relates to non-informative area, main facial features still have
   unique appearance in different face halves. Evaluation results reveal
   realistic rendering in novel poses robust to challenging illumination
   conditions and small registration errors.}},
Article-Number = {{94050P}},
ISSN = {{0277-786X}},
ISBN = {{978-1-62841-495-0}},
Unique-ID = {{ISI:000353328200021}},
}

@inproceedings{ ISI:000380617000020,
Author = {Hu, Xiao and Liao, Qixin and Peng, Shaohu},
Editor = {{Xiao, Z and Tong, Z and Li, K and Wang, X and Li, K}},
Title = {{Video Surveillance Face Recognition by More Virtual Training Samples
   Based on 3D Modeling}},
Booktitle = {{2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)}},
Year = {{2015}},
Pages = {{113-117}},
Note = {{11th International Conference on Natural Computation (ICNC) / 12th
   International Conference on Fuzzy Systems and Knowledge Discovery
   (FSKD), Zhangjiajie, PEOPLES R CHINA, AUG 15-17, 2015}},
Organization = {{IEEE; CAS IEEE Circuits \& Syst Soc; Hunan Univ; Jishou Univ}},
Abstract = {{Video surveillance has been applied in more and more fields for security
   in last decade years, video-based face recognition therefore became an
   important task of an intelligent monitoring system. However, among these
   captured video faces there are many non-frontal faces. As a result, the
   state-of-art face algorithms would become worse when they were employed
   to recognize video faces. On the other hand, it was a common phenomenon
   especially at video monitoring field that only one training sample per
   person is gained from their identification card. The single sample per
   person (SSPP) results in effecting even not taking advantage of some
   fine algorithms such LDA. In order to effectively improve the correct
   recognition rate of multi-pose face recognition with a single frontal
   training sample, this paper proposed a face recognition algorithm based
   on 3D modeling. In the proposed algorithm, firstly a 2D frontal face
   with high-resolution was taken to build a 3D face model, and then
   several virtual faces with different poses were produced from the 3D
   face model. At last, both the original frontal face image and virtual
   face images were put into a gallery set. The algorithm was evaluated on
   SCface database using traditional PCA and LDA methods. The result showed
   that the proposed approach could effectively improve video face
   recognition rate and the correct recognition rate went up about 13\% by
   LDA compared with traditional PCA. Therefore, the method that was
   proposed to create virtual looking down training samples was an
   effective algorithm and could be considered to apply in intelligent
   video monitoring system.}},
ISBN = {{978-1-4673-7679-2}},
Unique-ID = {{ISI:000380617000020}},
}

@inproceedings{ ISI:000377348700061,
Author = {Liu, Jian and Zhang, Quan and Tang, Chaojing},
Book-Author = {{Xu, B}},
Title = {{CoMES: A Novel Method for Robust Nose Tip Detection in Face Range Images}},
Booktitle = {{2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION
   CONTROL CONFERENCE (IAEAC)}},
Year = {{2015}},
Pages = {{309-315}},
Note = {{IEEE Advanced Information Technology, Electronic and Automation Control
   Conference (IAEAC), Chongqing, PEOPLES R CHINA, DEC 19-20, 2015}},
Organization = {{IEEE; IEEE Beijing Sect; Global Union Acad Sci \& Technol; Chongqing
   Global Union Acad Sci \& Technol}},
Abstract = {{As the most distinct feature point in facial landmarks, nose tip plays a
   significant role in 3D facial studies. Successful detection of nose tip
   can facilitate many 3D facial studies tasks. In this paper, we propose a
   novel method to detect nose tip robustly. The method is robust to noise,
   need not training, can handle large rotations and occlusions. We first
   remove small isolated connected regions and noise from the input range
   image, then establish scale-space by robust smoothing the preprocessed
   range image. In each scale of the scale-space, we compute multi-angle
   energy of each point, then we use hierarchical clustering method to
   cluster the points whose multi-angle energies are larger than a
   threshold value. In the largest cluster, we can find one point with the
   largest multi-angle energy. For all scales of the scale-space, we get a
   series of such points and apply hierarchical clustering again for these
   points, nose tip will have the largest multi-angle energy in the largest
   cluster. We evaluate our method in FRGC v2.0 3D face database and
   BOSPHORUS 3D face database. The experimental results verify the
   robustness of our method with a high nose tip detection rate.}},
ISBN = {{978-1-4799-1980-2}},
Unique-ID = {{ISI:000377348700061}},
}

@inproceedings{ ISI:000387959204074,
Author = {Gilani, Syed Zulqarnain and Shafait, Faisal and Mian, Ajmal},
Book-Group-Author = {{IEEE}},
Title = {{Shape-based Automatic Detection of a Large Number of 3D Facial Landmarks}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2015}},
Pages = {{4639-4648}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{We present an algorithm for automatic detection of a large number of
   anthropometric landmarks on 3D faces. Our approach does not use texture
   and is completely shape based in order to detect landmarks that are
   morphologically significant. The proposed algorithm evolves level set
   curves with adaptive geometric speed functions to automatically extract
   effective seed points for dense correspondence. Correspondences are
   established by minimizing the bending energy between patches around seed
   points of given faces to those of a reference face. Given its
   hierarchical structure, our algorithm is capable of establishing
   thousands of correspondences between a large number of faces. Finally, a
   morphable model based on the dense corresponding points is fitted to an
   unseen query face for transfer of correspondences and hence automatic
   detection of landmarks. The proposed algorithm can detect any number of
   pre-defined landmarks including subtle landmarks that are even difficult
   to detect manually. Extensive experimental comparison on two benchmark
   databases containing 6, 507 scans shows that our algorithm outperforms
   six state of the art algorithms.}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-6964-0}},
ORCID-Numbers = {{Gilani, Syed Zulqarnain/0000-0002-7448-2327}},
Unique-ID = {{ISI:000387959204074}},
}

@inproceedings{ ISI:000371977802159,
Author = {Tortorici, Claudio and Werghi, Naoufel and Berretti, Stefano},
Book-Group-Author = {{IEEE}},
Title = {{BOOSTING 3D LBP-BASED FACE RECOGNITION BY FUSING SHAPE AND TEXTURE
   DESCRIPTORS ON THE MESH}},
Booktitle = {{2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)}},
Series = {{IEEE International Conference on Image Processing ICIP}},
Year = {{2015}},
Pages = {{2670-2674}},
Note = {{IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015}},
Organization = {{Inst Elect \& Elect Engineers; IEEE Signal Proc Soc}},
Abstract = {{In this paper, we present a novel approach for fusing shape and texture
   local binary patterns (LBP) for 3D face recognition. Using the framework
   proposed in {[}1], we compute LBP directly on the face mesh surface,
   then we construct a grid of the regions on the facial surface that can
   accommodate global and partial descriptions. Compared to its depth-image
   counterpart, our approach is distinguished by the following features: a)
   inherits the intrinsic advantages of mesh surface; b) does not require
   normalization; c) can accommodate partial matching. In addition, it
   allows early-level fusion of texture and shape modalities. Through
   experiments conducted on the BU-3DFE and Bosphorus databases, we assess
   different variants of our approach with regard to facial expressions and
   missing data.}},
ISSN = {{1522-4880}},
ISBN = {{978-1-4799-8339-1}},
ResearcherID-Numbers = {{Werghi, Naoufel/D-2398-2018
   }},
ORCID-Numbers = {{Werghi, Naoufel/0000-0002-5542-448X
   Berretti, Stefano/0000-0003-1219-4386}},
Unique-ID = {{ISI:000371977802159}},
}

@inproceedings{ ISI:000399463800019,
Author = {Shah, Syed Afaq Ali and Bennamoun, Mohammed and Boussaid, Farid},
Book-Group-Author = {{IEEE}},
Title = {{AUTOMATIC 3D FACE LANDMARK LOCALIZATION BASED ON 3D VECTOR FIELD
   ANALYSIS}},
Booktitle = {{2015 INTERNATIONAL CONFERENCE ON IMAGE AND VISION COMPUTING NEW ZEALAND
   (IVCNZ)}},
Series = {{International Conference on Image and Vision Computing New Zealand}},
Year = {{2015}},
Note = {{International Conference on Image and Vision Computing New Zealand
   (IVCNZ), Auckland, NEW ZEALAND, NOV 23-24, 2015}},
Abstract = {{In applications such as 3D face synthesis and animation, a prominent
   face landmark is required to enable 3D face normalization, pose
   correction, 3D face recognition and reconstruction. Due to variations in
   facial expressions, automatic 3D face landmark localization remains a
   challenge. Nose tip is one of the salient landmarks in a human face. In
   this paper, a novel nose tip localization technique is proposed. In the
   proposed approach, the rotation of the 3D vector field is analyzed for
   robust and efficient nose tip localization. The proposed technique has
   the following three characteristics: (1) it does not require any
   training; (2) it does not rely on any particular model; (3) it is very
   efficient, requiring an average time of only 1.9s for nose tip
   detection. We tested the proposed technique on BU3DFE and Shrec'10
   datasets. Experimental results show that the proposed technique is
   robust to variations in facial expressions, achieving a 100\% detection
   rate on these publicly available 3D face datasets.}},
ISSN = {{2151-2191}},
ISBN = {{978-1-5090-0357-0}},
Unique-ID = {{ISI:000399463800019}},
}

@article{ ISI:000215156100010,
Author = {Fernandez-Cervantes, Victor and Garcia, Arturo and Antonio Ramos, Marco
   and Mendez, Andres},
Title = {{Facial Geometry Identification through Fuzzy Patterns with RGBD Sensor}},
Journal = {{COMPUTACION Y SISTEMAS}},
Year = {{2015}},
Volume = {{19}},
Number = {{3}},
Pages = {{529-546}},
Abstract = {{Automatic human facial recognition is an important and complicated task;
   it is necessary to design algorithms capable of recognizing the constant
   patterns in the face and to use computing resources efficiently. In this
   paper we present a novel algorithm to recognize the human face in real
   time; the system's input is the depth and color data from the Microsoft
   KinectTM device. The algorithm recognizes patterns/shapes on the point
   cloud topography. The template of the face is based in facial geometry;
   the forensic theory classifies the human face with respect to constant
   patterns: cephalometric points, lines, and areas of the face. The
   topography, relative position, and symmetry are directly related to the
   craniometric points. The similarity between a point cloud cluster and a
   pattern description is measured by a fuzzy pattern theory algorithm. The
   face identification is composed by two phases: the first phase
   calculates the face pattern hypothesis of the facial points, configures
   each point shape, the related location in the areas, and lines of the
   face. Then, in the second phase, the algorithm performs a search on
   these face point configurations.}},
DOI = {{10.13053/CyS-19-3-2015}},
ISSN = {{1405-5546}},
EISSN = {{2007-9737}},
ORCID-Numbers = {{Ramos Corchado, Marco Antonio/0000-0003-3982-6988}},
Unique-ID = {{ISI:000215156100010}},
}

@article{ ISI:000344204000007,
Author = {Jo, Jaeik and Choi, Heeseung and Kim, Ig-Jae and Kim, Jaihie},
Title = {{Single-view-based 3D facial reconstruction method robust against pose
   variations}},
Journal = {{PATTERN RECOGNITION}},
Year = {{2015}},
Volume = {{48}},
Number = {{1}},
Pages = {{73-85}},
Month = {{JAN}},
Abstract = {{The 3D Morphable Model (3DMM) and the Structure from Motion (SfM)
   methods are widely used for 3D facial reconstruction from 2D single-view
   or multiple-view images. However, model-based methods suffer from
   disadvantages such as high computational costs and vulnerability to
   local minima and head pose variations. The SfM-based methods require
   multiple facial images in various poses. To overcome these
   disadvantages, we propose a single-view-based 3D facial reconstruction
   method that is person-specific and robust to pose variations. Our
   proposed method combines the simplified 3DMM and the SfM methods. First,
   2D initial frontal Facial Feature Points (FFPs) are estimated from a
   preliminary 3D facial image that is reconstructed by the simplified
   3DMM. Second, a bilateral symmetric facial image and its corresponding
   FFPs are obtained from the original side-view image and corresponding
   FFPs by using the mirroring technique. Finally, a more accurate the 3D
   facial shape is reconstructed by the SfM using the frontal, original,
   and bilateral symmetric FFPs. We evaluated the proposed method using
   facial images in 35 different poses. The reconstructed facial images and
   the ground-truth 3D facial shapes obtained from the scanner were
   compared. The proposed method proved more robust to pose variations than
   3DMM. The average 3D Root Mean Square Error (RMSE) between the
   reconstructed and ground-truth 3D faces was less than 2.6 mm when 2D
   FFPs were manually annotated, and less than 3.5 mm when automatically
   annotated. (C) 2014 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.patcog.2014.07.013}},
ISSN = {{0031-3203}},
EISSN = {{1873-5142}},
Unique-ID = {{ISI:000344204000007}},
}
