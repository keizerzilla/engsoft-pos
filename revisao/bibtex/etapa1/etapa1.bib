@inproceedings{8075548,
abstract = {Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform.},
annote = {24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Frikha, Tarek and Chaabane, Faten and Said, Boukhchim and Drira, Hassen and Abid, Mohamed and {Ben Amar}, Chokri and Lille, Lifl},
booktitle = {2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)},
doi = {10.1109/ATSIP.2017.8075548},
file = {:home/tutu/artigos{\_}revisao/08075548.pdf:pdf},
isbn = {978-1-5386-0551-6},
keywords = {biometrics (access control),embedded systems,etapa1,face,id341,ieeexplore,izaias,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui2},
mendeley-tags = {etapa1,id341,ieeexplore,izaias,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui2},
month = {may},
pages = {1--5},
publisher = {IEEE},
title = {{Embedded approach for a Riemannian-based framework of analyzing 3D faces}},
url = {http://ieeexplore.ieee.org/document/8075548/},
year = {2017}
}
@article{Zhang20153357,
abstract = {Expression and pose variations are two major challenges for 3D face recognition. This paper presents a method to cope with these two challenges by fusing the matching results of adaptive multiple regions on the 3D face. First, one approach is proposed for pose correction of 3D face based on three landmark points: nose tip, nasion, and subnasale. Then multiple regions are adaptively chosen from the facial surface, which include nose, left and right eye-forehead regions, left and right cheeks, and mouth-chin region. Next, a least trimmed square Hausdorff distance method is applied for region matching. Moreover, to obtain a better overall performance, several score-level and rank-level fusion schemes are used to fuse the contribution of each region. The proposed approach is evaluated on the Bosphorus and the BU-3DFE databases, and yields good results. The study shows that the proposed algorithm is robust to expression and pose changes. {\textcopyright}, 2015, Binary Information Press. All right reserved.},
annote = {cited By 0},
author = {Zhang, C and Gu, Y and Wang, Y and Li, F and Zhan, Y and Pi, J and Qu, L},
doi = {10.12733/jcis14297},
journal = {Journal of Computational Information Systems},
keywords = {revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
number = {9},
pages = {3357--3369},
title = {{Adaptive multiple regions matching for 3D face recognition under expression and pose variations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938320850{\&}doi=10.12733{\%}2Fjcis14297{\&}partnerID=40{\&}md5=af76d7b96bb6eca2db8bc3e1babc780a},
volume = {11},
year = {2015}
}
@article{ISI:000392292000002,
abstract = {3D face shape is essentially a non-rigid free-form surface, which will produce non-rigid deformation under expression variations. In terms of that problem, a promising solution named Coherent Point Drift (CPD) non-rigid registration for the non-rigid region is applied to eliminate the influence from the facial expression while guarantees 3D surface topology. In order to take full advantage of the extracted discriminative feature of the whole face under facial expression variations, the novel expression-robust 3D face recognition method using feature-level fusion and feature-region fusion is proposed. Furthermore, the Principal Component Analysis and Linear Discriminant Analysis in combination with Rotated Sparse Regression (PL-RSR) dimensionality reduction method is presented to promote the computational efficiency and provide a solution to the curse of dimensionality problem, which benefit the p erformance optimization. The experimental evaluation indicates that the proposed strategy has achieved the rank-1 recognition rate of 97.91 {\%} and 96.71 {\%} based on Face Recognition Grand Challenge (FRGC) v2.0 and Bosphorus respectively, which means the proposed approach outperforms state-of-the-art approach.},
author = {Deng, Xing and Da, Feipeng and Shao, Haijian},
doi = {10.1007/s11042-015-3012-8},
file = {:home/tutu/artigos{\_}revisao/Deng2017{\_}Article{\_}Expression-robust3DFaceRecogni.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {3D face recognition,Dimensionality reduction,Feature-level fusion,Feature-region fusion,Non-rigid point set registration,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {jan},
number = {1},
pages = {13--31},
title = {{Expression-robust 3D face recognition based on feature-level fusion and feature-region fusion}},
volume = {76},
year = {2017}
}
@article{Belghini2015317,
abstract = {{\textcopyright} 2014 IEEE. In this paper, we propose a fuzzy similarity based classification approach for 3D face recognition. In the feature extraction method, we exploit curve concept to represent the 3D facial data, two types of curves was considered: depth-level and depth-radial curves. As the dimension of the obtained features is high, the problem 'curse of dimensionality' appears. To solve this problem, the Random Projection (RP) method was used. The proposed classifier performs Fuzzification operation using triangular membership functions for input data and ordered weighted averaging operators to measure similarity. Experiment was conducted using vrml files from 3D Database considering only one training sample per person. The obtained results are very promising for depth-level and depth-radial curves, besides the recognition rates are higher than 98{\%}.},
annote = {From Duplicate 1 (3D face recognition using facial curves, sparse random projection and fuzzy similarity measure - Belghini, N; Ezghari, S; Zahi, A)

cited By 1},
author = {Belghini, Naouar and Ezghari, Soufiane and Zahi, Azeddine},
doi = {10.1109/CIST.2014.7016639},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Belghini, Ezghari, Zahi - 2015 - 3D face recognition using facial curves, sparse random projection and fuzzy similarity measure.pdf:pdf},
isbn = {9781479959792},
issn = {23271884},
journal = {Colloquium in Information Science and Technology, CIST},
keywords = {3D face recognition,OWA operator,facial curves,fuzzy logic,revisao{\_}V1,revisao{\_}scopus,similarity measure,sparse random projection,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
number = {January},
pages = {317--322},
title = {{3D face recognition using facial curves, sparse random projection and fuzzy similarity measure}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938075627{\&}doi=10.1109{\%}2FCIST.2014.7016639{\&}partnerID=40{\&}md5=bca04cb8b40640821dbbc277a0cf6e71},
volume = {2015-Janua},
year = {2015}
}
@inproceedings{ISI:000374793400004,
abstract = {This paper proposes a 3D face recognition approach using sphere depth
image, which is robust to pose variations in unconstrained environments.
The input 3D face point clouds is first transformed into sphere depth
images, and then represented as a 3DLBP image to enhance the
distinctiveness of smooth and similar facial depth images. An improved
SIFT algorithm is applied in the following matching process. The
improved SIFT algorithm employs the learning to rank approach to select
the keypoints with higher stability and repeatability instead of
manually rule-based method used by the original SIFT algorithm. The
proposed face recognition method is evaluated on CASIA 3D face database.
And the experimental results show our approach has superior performance
than many existing methods for 3D face recognition and handles pose
variations quite well.},
annote = {10th Chinese Conference on Biometric Recognition (CCBR), Tianjin,
PEOPLES R CHINA, NOV 13-15, 2015},
author = {Wang, Hanchao and Mu, Zhichun and Zeng, Hui and Huang, Mingming},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-25417-3_4},
editor = {{Yang, J and Yang, J and Sun, Z and Shan, S and Zheng, W and Feng}, J},
file = {:home/tutu/artigos{\_}revisao/1aea7e6aea20a283b83accdd43ef6537-wang2015.pdf:pdf},
isbn = {9783319254166},
issn = {16113349},
keywords = {3D face recognition,Learning to rank,Local binary patterns,Sphere depth image,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
organization = {Chinese Assoc Artificial Intelligence; Springer; Civil Aviat Univ China; Tianjin Univ Sci {\&} Technol; CASIA, Inst Intelligent Recognit},
pages = {27--34},
series = {Lecture Notes in Computer Science},
title = {{3D face recognition using local features matching on sphere depth representation}},
volume = {9428},
year = {2015}
}
@article{ISI:000376708000002,
abstract = {In this paper, we investigate the contribution of dynamic evolution of 3D faces to identity recognition. To this end, we adopt a subspace representation of the flow of curvature-maps computed on 3D facial frames of a sequence, after normalizing their pose. Such representation allows us to embody the shape as well as its temporal evolution within the same subspace representation. Dictionary learning and sparse coding over the space of fixed-dimensional subspaces, called Grassmann manifold, have been used to perform face recognition. We have conducted extensive experiments on the BU-4DFE dataset. The obtained results of the proposed approach provide promising results.},
author = {Alashkar, Taleb and {Ben Amor}, Boulbaba and Daoudi, Mohamed and Berretti, Stefano},
doi = {10.1016/j.patcog.2016.03.013},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alashkar et al. - 2016 - A Grassmann framework for 4D facial shape analysis.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {4D face recognition,Curvature-maps,Dictionary learning,Grassmann manifold,Sparse coding,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux10},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux10},
pages = {21--30},
title = {{A Grassmann framework for 4D facial shape analysis}},
volume = {57},
year = {2016}
}
@inproceedings{ISI:000391534900063,
abstract = {{\textcopyright} 2016 IEEE. In this paper, we present a new radial string representation and matching approach for 3D face recognition under expression variations and partial occlusions. The radial strings are an indexed collection of strings emanating from the nose tip of a face scan. The matching between two radial strings is conducted through a dynamic programming process, in which a partial matching mechanism is established to effectively find those un-occluded substrings. Moreover, the most discriminative and stable radial strings are selected optimally by the well-known AdaBoost algorithm to achieve a composite classifier for 3D face recognition under facial expression changes. Experimental results on the GavabDB and the Bosphorus databases show that the proposed approach achieves promising results for human face recognition with expressions and occlusions.},
annote = {International Conference on Digital Image Computing - Techniques and
Applications (DICTA), Gold Coast, AUSTRALIA, NOV 30-DEC 02, 2016},
author = {Yu, Xun and Gao, Yongsheng and Zhou, Jun},
booktitle = {2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
doi = {10.1109/DICTA.2016.7797014},
editor = {{Liew, AWC and Lovell, B and Fookes, C and Zhou, J and Gao, Y and Blumenstein, M and Wang}, Z},
file = {:home/tutu/artigos{\_}revisao/07797014.pdf:pdf},
isbn = {9781509028962},
keywords = {face recognition,facial curves,feature selection,machine learning,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,string matching,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {nov},
organization = {Australian Govt, Dept Defence, Defence Sci {\&} Technol Grp; IAPR; Canon Informat Syst Res Australia; IEEE; Griffith Univ; APRS},
pages = {1--6},
publisher = {IEEE},
title = {{Boosting Radial Strings for 3D Face Recognition with Expressions and Occlusions}},
url = {http://ieeexplore.ieee.org/document/7797014/},
year = {2016}
}
@article{Guo2016403,
abstract = {This paper presents a local feature based shape matching algorithm for expression-invariant 3D face recognition. Each 3D face is first automatically detected from a raw 3D data and normalized to achieve pose invariance. The 3D face is then represented by a set of keypoints and their associated local feature descriptors to achieve robustness to expression variations. During face recognition, a probe face is compared against each gallery face using both local feature matching and 3D point cloud registration. The number of feature matches, the average distance of matched features, and the number of closest point pairs after registration are used to measure the similarity between two 3D faces. These similarity metrics are then fused to obtain the final results. The proposed algorithm has been tested on the FRGC v2 benchmark and a high recognition performance has been achieved. It obtained the state-of-the-art results by achieving an overall rank-1 identification rate of 97.0{\%} and an average verification rate of 99.01{\%} at 0.001 false acceptance rate for all faces with neutral and non-neutral expressions. Further, the robustness of our algorithm under different occlusions has been demonstrated on the Bosphorus dataset.},
annote = {cited By 7
28/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
28/04 Exclu{\'{i}}do (etapa 1)},
author = {Guo, Yulan and Lei, Yinjie and Liu, Li and Wang, Yan and Bennamoun, Mohammed and Sohel, Ferdous},
doi = {10.1016/j.patrec.2016.04.003},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2016 - EI3D Expression-invariant 3D face recognition based on feature and shape matching.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {3D face recognition,Face identification,Facial expression,Keypoint detection,Local feature,Shape matching,etapa1,id384,isi,poly,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,scopus,tutui1},
mendeley-tags = {etapa1,id384,isi,poly,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,scopus,tutui1},
pages = {403--412},
title = {{EI3D: Expression-invariant 3D face recognition based on feature and shape matching}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966659227{\&}doi=10.1016{\%}2Fj.patrec.2016.04.003{\&}partnerID=40{\&}md5=b09b19b7a5436b53278c02d001e93910},
volume = {83},
year = {2016}
}
@inproceedings{ISI:000390782003008,
abstract = {3D face recognition with partial occlusions is a highly challenging
problem. In this paper, we propose a novel radial string representation
and matching approach to recognize 3D facial scans in the presence of
partial occlusions. Here we encode 3D facial surfaces into an indexed
collection of radial strings emanating from the nosetips and Dynamic
Programming (DP) is then used to measure the similarity between two
radial strings. In order to address the recognition problems with
partial occlusions, a partial matching mechanism is established in our
approach that effectively eliminates those occluded parts and finds the
most discriminative parts during the matching process. Experimental
results on the Bosphorus database demonstrate that the proposed approach
yields superior performance on partially occluded data.},
annote = {From Duplicate 1 (3D face recognition under partial occlusions using radial strings - Yu, Xun; Gao, Yongsheng; Zhou, Jun)

From Duplicate 1 (3D face recognition under partial occlusions using radial strings - Yu, Xun; Gao, Yongsheng; Zhou, Jun)

23rd IEEE International Conference on Image Processing (ICIP), Phoenix,
AZ, SEP 25-28, 2016

From Duplicate 2 (3D face recognition under partial occlusions using radial strings - Yu, Xun; Gao, Yongsheng; Zhou, Jun)

23rd IEEE International Conference on Image Processing (ICIP), Phoenix,
AZ, SEP 25-28, 2016},
author = {Yu, Xun and Gao, Yongsheng and Zhou, Jun},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2016.7532913},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Gao, Zhou - 2016 - 3D face recognition under partial occlusions using radial strings.pdf:pdf},
isbn = {9781467399616},
issn = {15224880},
keywords = {3D face recognition,Partial occlusions,Radial string matching,Structural recognition,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {sep},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers, Signal Proc Soc},
pages = {3016--3020},
publisher = {IEEE},
series = {IEEE International Conference on Image Processing ICIP},
title = {{3D face recognition under partial occlusions using radial strings}},
url = {http://ieeexplore.ieee.org/document/7532913/},
volume = {2016-Augus},
year = {2016}
}
@inproceedings{7797090,
abstract = {3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases.},
annote = {23/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
23/04/2018 Exclu{\'{i}}do (etapa 1)
04/05/2018 Revisado (etapa 1)},
author = {Gilani, S Z and Mian, A},
booktitle = {2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
doi = {10.1109/DICTA.2016.7797090},
file = {:home/tutu/artigos{\_}revisao/07797090.pdf:pdf},
keywords = {convolution,estela,etapa1,face recognition,feature extraction,id181,ieeexplore,im,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {estela,etapa1,id181,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {nov},
pages = {1--8},
title = {{Towards Large-Scale 3D Face Recognition}},
year = {2016}
}
@article{Liang:2015:BMD:2805325.2805648,
abstract = {Due to the difficulties associated with the collection of 3D samples, 3D face recognition technologies often have to work with smaller than desirable sample sizes. With the aim of enlarging the training number for each subject, we divide each training image into several patches. However, this immediately introduces two further problems for 3D models: high computational cost and dispersive features caused by the divided 3D image patches. We therefore first map 3D face images into 2D depth images, which greatly reduces the dimension of the samples. Though the depth images retain most of the robust features of 3D images, such as pose and illumination invariance, they lose many discriminative features of the original 3D samples. In this study, we propose a Bayesian learning framework to extract the discriminative features from the depth images. Specifically, we concentrate the features of the intra-class patches to a mean feature by maximizing the multivariate Gaussian likelihood function, and, simultaneously, enlarge the distances between the inter-class mean features by maximizing the exponential priori distribution of the mean features. For classification, we use the nearest neighbor classifier combined with the Mahalanobis distance to calculate the distance between the features of the test image and items in the training set. Experiments on two widely-used 3D face databases demonstrate the efficiency and accuracy of our proposed method compared to relevant state-of-the-art methods.},
address = {New York, NY, USA},
annote = {26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Liang, Ronghua and Shen, Wenjia and Li, Xiao Xin and Wang, Haixia},
doi = {10.1016/j.ins.2015.03.063},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2015 - Bayesian multi-distribution-based discriminative feature extraction for 3D face recognition.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {3D face recognition,Bayesian learning,Depth image,Single training sample per person,acm,estela,etapa1,id283,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {acm,estela,etapa1,id283,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {nov},
number = {C},
pages = {406--417},
publisher = {Elsevier Science Inc.},
title = {{Bayesian multi-distribution-based discriminative feature extraction for 3D face recognition}},
url = {http://dx.doi.org/10.1016/j.ins.2015.03.063 http://linkinghub.elsevier.com/retrieve/pii/S0020025515002364},
volume = {320},
year = {2015}
}
@inproceedings{ISI:000386931400187,
abstract = {{\textcopyright} 2016 IEEE. 3D partial face recognition under missing parts, occlusions and data corruptions is a major challenge for the practical application of the techniques of 3D face recognition. Moreover, one individual can only provide one sample for training in most practical scenarios, and thus the face recognition with single sample problem is another highly challenging task. We propose an efficient framework for 3D partial face recognition with single sample addressing both of the two problems. First, we represent a facial scan with a set of keypoint based local geometrical descriptors, which gains sufficient robustness to partial facial data along with expression/pose variations. Then, a two-step modified collaborative representation classification scheme is proposed to address the single sample recognition problem. A class-based probability estimation is given during the first classification step, and the obtained result is then incorporated into the modified collaborative representation classification as a locality constraint to improve its classification performance. Extensive experiments on the Bosphorus and FRGC v2.0 datasets demonstrate the efficiency of the proposed approach when addressing the problem of 3D partial face recognition with single sample.},
annote = {IEEE 11th Conference on Industrial Electronics and Applications (ICIEA),
Hefei, PEOPLES R CHINA, JUN 05-07, 2016},
author = {Lei, Yinjie and Feng, Siyu and Zhou, Xinzhi and Guo, Yulan},
booktitle = {Proceedings of the 2016 IEEE 11th Conference on Industrial Electronics and Applications, ICIEA 2016},
doi = {10.1109/ICIEA.2016.7603727},
file = {:home/tutu/artigos{\_}revisao/07603727.pdf:pdf},
isbn = {9781509026050},
issn = {2156-2318},
keywords = {3D facial representation,3D partial face recognition,collaborative representation,locality constraint,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,single sample problem,tutux6},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux6},
month = {jun},
organization = {IEEE; IEEE Ind Elect Soc; IEEE Ind Elect Chapter; IEEE Singapore Sect; Anhui Univ},
pages = {994--999},
publisher = {IEEE},
series = {IEEE Conference on Industrial Electronics and Applications},
title = {{An efficient 3D partial face recognition approach with single sample}},
url = {http://ieeexplore.ieee.org/document/7603727/},
year = {2016}
}
@article{Sghaier:2018:NTF:3193702.3193706,
abstract = {This manuscript presents an improved system research that can detect and recognize the person in 3D space automatically and without the interaction of the people's faces. This system is based not only on a quantum computation and measurements to extract the vector features in the phase of characterization but also on learning algorithm (using SVM) to classify and recognize the person. This research presents an improved technique for automatic 3D face recognition using anthropometric proportions and measurement to detect and extract the area of interest which is unaffected by facial expression. This approach is able to treat incomplete and noisy images and reject the non-facial areas automatically. Moreover, it can deal with the presence of holes in the meshed and textured 3D image. It is also stable against small translation and rotation of the face. All the experimental tests have been done with two 3D face datasets FRAV 3D and GAVAB. Therefore, the test's results of the proposed approach are promising because they showed that it is competitive comparable to similar approaches in terms of accuracy, robustness, and flexibility. It achieves a high recognition performance rate of 95.35{\%} for faces with neutral and non-neutral expressions for the identification and 98.36{\%} for the authentification with GAVAB and 100{\%} with some gallery of FRAV 3D datasets.},
address = {Hershey, PA, USA},
annote = {02/05/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
02/05/2018 Exclu{\'{i}}do (etapa 1)

janeiro de 2018},
author = {Sghaier, Souhir and Farhat, Wajdi and Souani, Chokri},
doi = {10.4018/ijaci.2018010104},
issn = {1941-6237},
journal = {International Journal of Ambient Computing and Intelligence},
keywords = {3D Face,Anthropometric,Euclidean Distance,Eye Corners,Feature Extraction,Learning,Measurements,Nose Tip,acm,etapa1,gil,id431,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {acm,etapa1,gil,id431,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {jan},
number = {1},
pages = {60--77},
publisher = {IGI Global},
title = {{Novel Technique for 3D Face Recognition Using Anthropometric Methodology}},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJACI.2018010104},
volume = {9},
year = {2017}
}
@article{Deng20171305,
abstract = {A novel adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition is proposed in this paper. Firstly, the novel facial coarse-to-fine landmarks localization method based on Active Shape Model and Gabor wavelets transformation is proposed to exactly and automatically locate facial landmarks in range image. Secondly, the multi-scale fusion of the pyramid local binary patterns (F-PLBP) based on the irregular segmentation associated with the located landmarks is proposed to extract the discriminative feature. Thirdly, a sparse representation-based classifier based on the adaptive feature selection (A-SRC) using the distribution of the reconstruction residual is presented to select the expression-robust feature and identify the faces. Finally, the experimental evaluation based on FRGC v2.0 indicates that the adaptive feature selection method using F-PLBP combined with the A-SRC can obtain the high recognition accuracy by performing the higher discriminative power to overcome the influence from the facial expression variations.},
annote = {cited By 0
01/05/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
01/05/2018 Exclu{\'{i}}do (etapa 1)},
author = {Deng, Xing and Da, Feipeng and Shao, Haijian},
doi = {10.1007/s11760-017-1087-6},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng, Da, Shao - 2017 - Adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robu.pdf:pdf},
issn = {18631711},
journal = {Signal, Image and Video Processing},
keywords = {3D face recognition,Adaptive feature selection,Facial landmark localization,Multi-scale fusion,estela,etapa1,id422,isi,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,scopus,tutui1},
mendeley-tags = {estela,etapa1,id422,isi,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,scopus,tutui1},
number = {7},
pages = {1305--1312},
title = {{Adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017095710{\&}doi=10.1007{\%}2Fs11760-017-1087-6{\&}partnerID=40{\&}md5=fc06247cc3ca7870221563a085266269},
volume = {11},
year = {2017}
}
@article{Peter201977,
abstract = {Face recognition is commonly used for biometric security purposes in video surveillance and user authentications. The nature of face exhibits non-linear shapes due to appearance deformations, and face variations presented by facial expressions. Recognizing faces reliably across changes in facial expression has proved to be a more difficult problem leading to low recognition rates in many face recognition experiments. This is mainly due to the tens degree-of-freedom in a non-linear space. Recently, non-linear PCA has been revived as it posed a significant advantage for data representation in high dimensionality space. In this paper, we experimented the use of non-linear kernel approach in 3D face recognition and the results of the recognition rates have shown that the kernel method outperformed the standard PCA. {\textcopyright} Springer Nature Singapore Pte Ltd. 2019.},
annote = {cited By 0},
author = {Peter, Marcella and Minoi, Jacey Lynn and Hipiny, Irwandi Hipni Mohamad},
doi = {10.1007/978-981-13-2622-6_8},
file = {:home/tutu/artigos{\_}revisao/55e48f9de70a93e32edc3f1ed2f2bb70-peter2018.pdf:pdf},
isbn = {9789811326219},
issn = {18761119},
journal = {Lecture Notes in Electrical Engineering},
keywords = {3D face,Facial recognition,Kernel PCA,revisao{\_}V2,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V2,revisao{\_}scopus,tutui1},
pages = {77--86},
title = {{3D face recognition using kernel-based PCA approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053242189{\&}doi=10.1007{\%}2F978-981-13-2622-6{\_}8{\&}partnerID=40{\&}md5=c4b90626b4453034107d227cbd63ef3d},
volume = {481},
year = {2019}
}
@inproceedings{ISI:000390782003007,
abstract = {We propose a 3D face modeling and recognition system using an RGB-D stream in the presence of large pose changes. In the previous work, all facial data points are registered with a reference to improve the accuracy of 3D face model from a low-resolution depth sequence. This registration often fails when applied to non-frontal faces. It causes inaccurate 3D face models and poor performance of matching. We address this problem by pre-aligning each input face ('frontalization') before the registration, which avoids registration failures. For each frame, our method estimates the 3D face pose, assesses the quality of data, segments the facial region, frontalizes it. and performs an accurate registration with the previous 3D model. The 3D 3D recognition system using accurate 3D models from our method outperforms other face recognition systems and shows 100{\%} rank 1 recognition accuracy on a dataset with 30 subjects.},
annote = {23rd IEEE International Conference on Image Processing (ICIP), Phoenix,
AZ, SEP 25-28, 2016},
author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni, Gerard},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2016.7532912},
file = {:home/tutu/artigos{\_}revisao/07532912.pdf:pdf},
isbn = {9781467399616},
issn = {15224880},
keywords = {3D Face Modeling,3D Face Recognition,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux10},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux10},
month = {sep},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers, Signal Proc Soc},
pages = {3011--3015},
publisher = {IEEE},
series = {IEEE International Conference on Image Processing ICIP},
title = {{Accurate 3D face modeling and recognition from RGB-D stream in the presence of large pose changes}},
url = {http://ieeexplore.ieee.org/document/7532912/},
volume = {2016-Augus},
year = {2016}
}
@article{Zhao2018207,
abstract = {{\textcopyright} 2018, Springer Science+Business Media, LLC, part of Springer Nature. 3D face similarity is a critical issue in computer vision, computer graphics and face recognition and so on. Since Fr{\'{e}}chet distance is an effective metric for measuring curve similarity, a novel 3D face similarity measure method based on Fr{\'{e}}chet distances of geodesics is proposed in this paper. In our method, the surface similarity between two 3D faces is measured by the similarity between two sets of 3D curves on them. Due to the intrinsic property of geodesics, we select geodesics as the comparison curves. Firstly, the geodesics on each 3D facial model emanating from the nose tip point are extracted in the same initial direction with equal angular increment. Secondly, the Fr{\'{e}}chet distances between the two sets of geodesics on the two compared facial models are computed. At last, the similarity between the two facial models is computed based on the Fr{\'{e}}chet distances of the geodesics obtained in the second step. We verify our method both theoretically and practically. In theory, we prove that the similarity of our method satisfies three properties: reflexivity, symmetry, and triangle inequality. And in practice, experiments are conducted on the open 3D face database GavaDB, Texas 3D Face Recognition database, and our 3D face database. After the comparison with iso-geodesic and Hausdorff distance method, the results illustrate that our method has good discrimination ability and can not only identify the facial models of the same person, but also distinguish the facial models of any two different persons.},
annote = {cited By 0},
author = {Zhao, Jun Li and Wu, Zhong Ke and Pan, Zhen Kuan and Duan, Fu Qing and Li, Jin Hua and Lv, Zhi Han and Wang, Kang and Chen, Yu Cong},
doi = {10.1007/s11390-018-1814-7},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2018 - 3D Face Similarity Measure by Fr{\'{e}}chet Distances of Geodesics.pdf:pdf},
issn = {18604749},
journal = {Journal of Computer Science and Technology},
keywords = {3D face,Fr{\'{e}}chet distance,geodesic,revisao{\_}V1,revisao{\_}scopus,similarity measure,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
number = {1},
pages = {207--222},
title = {{3D Face Similarity Measure by Fr{\'{e}}chet Distances of Geodesics}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041342736{\&}doi=10.1007{\%}2Fs11390-018-1814-7{\&}partnerID=40{\&}md5=8478bdecdc0eeeb6cb4dda3bb1ecda52},
volume = {33},
year = {2018}
}
@inproceedings{ISI:000400688200019,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2016. Landmarks are unique points that can be located on every face. Facial landmarks typically recognized by people are correlated with anthropomorphic points. Our purpose is to employ in 3D face recognition such landmarks that are easy to interpret. Face understanding is construed as identification of face characteristic points with automatic labeling of them. In this paper, we apply methods based on Self Organizing Maps to understand 3D faces.},
annote = {15th International Conference on Artificial Intelligence and Soft
Computing (ICAISC), Zakopane, POLAND, JUN 12-16, 2016},
author = {Starczewski, Janusz T. and Pabiasz, Sebastian and Vladymyrska, Natalia and Marvuglia, Antonino and Napoli, Christian and W{\'{o}}zniak, Marcin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-39384-1_19},
editor = {{Rutkowski, L and Korytkowski, M and Scherer, R and Tadeusiewicz, R and Zadeh, LA and Zurada}, JM},
isbn = {9783319393834},
issn = {16113349},
keywords = {3D face recognition,Self organizing maps,Understanding of images,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
organization = {Polish Neural Network Soc; Univ Social Sci; Czestochowa Univ Technol, Inst Computat Intelligence},
pages = {210--217},
series = {Lecture Notes in Artificial Intelligence},
title = {{Self organizing maps for 3D face understanding}},
volume = {9693},
year = {2016}
}
@article{Dutta2019175,
abstract = {In this paper, a 3D face recognition system has been developed based on the volumetric representation of 3D range image. The main approach to build this system is to calculate volume on some distinct region of 3D range face data. The system has mainly three steps. In the very first step, seven significant facial landmarks are identified on the face. Secondly, six distinct triangular regions A to F are created on the face using any three individual landmarks where nose tip is common to all regions. Further 3D volumes of all the respective triangular regions have been calculated based on plane fitting on the input range images. Finally, KNN and SVM classifiers are considered for classification. Initially, the classification and recognition are carried out on the different volumetric region, and a further combination of all the regions is considered. The proposed approach is tested on three useful challenging databases, namely Frav3D, Bosphorous, and GavabDB.},
annote = {cited By 1},
author = {Dutta, Koushik and Bhattacharjee, Debotosh and Nasipuri, Mita and Poddar, Anik},
doi = {10.1007/978-981-13-3702-4_11},
file = {:home/tutu/artigos{\_}revisao/d1ccc7669cac94b04182f44edc5e57cd-dutta2019.pdf:pdf},
isbn = {9789811337017},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {3D range image,Classification,Facial landmark,Plane fitting,Volumetric representation,revisao{\_}V2,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V2,revisao{\_}scopus,tutui1},
pages = {175--189},
title = {{3D face recognition based on volumetric representation of range image}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061157145{\&}doi=10.1007{\%}2F978-981-13-3702-4{\_}11{\&}partnerID=40{\&}md5=2543d63ceec9074d39578ea3320cdfbd},
volume = {883},
year = {2019}
}
@incollection{Quan2015199,
abstract = {We propose an unsupervised online learning method based on the “growing neural gas” algorithm (GNG), for a data-stream configuration where each incoming data is visited only once and used to incrementally update the learned model as soon as it is available. The method maintains a model as a dynamically evolving graph topology of data-representatives that we call neurons. Unlike usual incremental learning methods, it avoids the sensitivity to initialization parameters by using an adaptive parameter-free distance threshold to produce new neurons. More-over, the proposed method performs a merging process which uses a distance-based probabilistic criterion to eventually merge neurons. This allows the algorithm to preserve a good computational efficiency over infinite time. Experiments on different real datasets, show that the proposed method is competitive with existing algorithms of the same family, while being independent of sensitive parameters and being able to maintain fewer neurons, which makes it convenient for learning from infinite data-streams.},
annote = {cited By 0},
author = {Quan, Wei and Matuszewski, Bogdan J. and Shark, Lik Kwan},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-27677-9_13},
isbn = {9783319276762},
issn = {16113349},
keywords = {3-D face recognition,Geodesic-map representation,Non-rigid deformation,Shape modeling,revisao{\_}V2,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V2,revisao{\_}scopus,tutui1},
pages = {199--212},
title = {{3-D Face Recognition Using Geodesic-Map Representation and Statistical Shape Modelling}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955314982{\&}doi=10.1007{\%}2F978-3-319-27677-9{\_}13{\&}partnerID=40{\&}md5=e85737d15347f5ea197969c838869be9 http://link.springer.com/10.1007/978-3-319-27677-9{\_}13},
volume = {9493},
year = {2015}
}
@inproceedings{ISI:000401510000148,
abstract = {{\textcopyright} 2016 IEEE. Feature selection from facial regions is a well-known approach to increase the performance of 2D image-based face recognition systems. In case of 3D modality, the approach of region-based feature selection for face recognition is relatively new. In this context, this paper presents an approach to evaluate the discrimination power of different regions of a 3D facial surface for its potential use in face recognition systems. We propose the use of weighted average of unit normal vector on the facial surface as the feature for region-based face recognition from 3D point cloud data (PCD). The iterative closest point algorithm is employed for the registration of segmented regions of facial point clouds. A metric based on angular distance between normals is introduced to indicate the similarity between two surfaces of same facial region. Finally, the intra class correlation based discrimination score is formulated to find out the key facial regions such as the eyes, nose, and mouth that are significant while recognizing a person with facial surface PCD.},
annote = {From Duplicate 1 (Evaluation of Discrimination power of facial parts from 3D point cloud data - Amin, Rafiul; Shams, A. Farhan; Rahman, S. M.Mahbubur; Hatzinakos, Dimitrios)

9th International Conference on Electrical and Computer Engineering
(ICECE), Dhaka, BANGLADESH, DEC 20-22, 2016},
author = {Amin, Rafiul and Shams, A. Farhan and Rahman, S. M.Mahbubur and Hatzinakos, Dimitrios},
booktitle = {Proceedings of 9th International Conference on Electrical and Computer Engineering, ICECE 2016},
doi = {10.1109/ICECE.2016.7853992},
file = {:home/tutu/artigos{\_}revisao/07853992.pdf:pdf},
isbn = {9781509029631},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
month = {dec},
organization = {Bangladesh Univ Engn {\&} Technol, Dept Elect {\&} Elect Engn; Inst Elect {\&} Elect Engineers; Energypac; PARADISE Cables ltd; BTCL; Summit Communicat Ltd; Dhaka Power Distribut Co Ltd},
pages = {602--605},
publisher = {IEEE},
series = {International Conference on Computer and Electrical Engineering ICCEE},
title = {{Evaluation of Discrimination power of facial parts from 3D point cloud data}},
url = {http://ieeexplore.ieee.org/document/7853992/},
year = {2017}
}
@article{Gaonkar201615,
abstract = {3D face recognition has gain a paramount importance over 2D due to its potential to address the limitations of 2D face recognition against the variation in facial poses, angles, occlusions etc. Research in 3D face recognition has accelerated in recent years due to the development of low cost 3D Kinect camera sensor. This has leads to the development of few RGB-D database across the world. Here in this paper we introduce the base results of our 3D facial database (GU-RGBD database) comprising variation in pose (0°, 45°, 90°, −45°, −90°), expression (smile, eyes closed), occlusion (half face covered with paper) and illumination variation using Kinect. We present a proposed noise removal non-linear interpolation filter for the patches present in the depth images. The results were obtained on three face recognition algorithms and fusion at matching score level for recognition and verification rate. The obtained results indicated that the performance with our proposed filter shows improvement over pose with score level fusion using sum rule. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 0},
author = {Gaonkar, A. A. and Gad, M. D. and Vetrekar, N. T. and Tilve, Vithal Shet and Gad, R. S.},
doi = {10.1007/978-3-319-68124-5_2},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {lerdepois,revisao{\_}V2,revisao{\_}scopus,tutui2},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}scopus,tutui2},
pages = {15--26},
title = {{Experimental evaluation of 3D kinect face database}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057824711{\&}doi=10.1007{\%}2F978-3-319-68124-5{\_}2{\&}partnerID=40{\&}md5=3a104665aebdb39c053c8b83ce35c8fb},
volume = {10481 LNCS},
year = {2016}
}
@inproceedings{7387550,
abstract = {Face recognition research mainly focuses on traditional 2D color images, which is extremely susceptible to be affected by external factors such as various viewpoints and has limited recognition accuracy. In order to achieve improved recognition performance, as well as the 3D face holds more abundant information than 2D, we present a 3D human face recognition algorithm using the Microsoft's Kinect. The proposed approach integrates the depth data with the RGB data to generate 3D face raw data and then extracts feature points, identifies the target via a two-level cascade classifier. Also, we build a 3D-face database including 16 individuals captured exclusively using Kinect. The experimental results indicate that the introduced algorithm can not only achieve better recognition accuracy in comparison to existing 2D and 3D face recognition algorithms when the probe face is exactly in front of Kinect sensor, but also can increase 9.3{\%} of recognition accuracy compared to the PCA-3D algorithm when it is not confronting the camera.},
annote = {09/05/2018 Em processo de sele{\c{c}}{\~{a}}o (Etapa 1)
09/05/2018 Exclu{\'{i}}do (Etapa 1)},
author = {Zhou, Wei and Chen, Jian Xin and Wang, Lei},
booktitle = {Proceedings of 2015 IEEE International Conference on Computer and Communications, ICCC 2015},
doi = {10.1109/CompComm.2015.7387550},
file = {:home/tutu/artigos{\_}revisao/07387550.pdf:pdf},
isbn = {9781467381253},
keywords = {3D face recognition,Kinect,RGB-D images,XML file,artur,classifier,etapa1,id459,ieeexplore,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {artur,etapa1,id459,ieeexplore,revisao{\_}V1,revisao{\_}scopus,tutui1},
month = {oct},
pages = {109--114},
publisher = {IEEE},
title = {{A RGB-D face recognition approach without confronting the camera}},
url = {http://ieeexplore.ieee.org/document/7387550/},
year = {2016}
}
@article{ISI:000370290900001,
abstract = {Curvelet transform can describe the signal by multiple scales, and multiple directions. In order to improve the performance of 3D face recognition algorithm, we proposed an Anthropometric and Curvelet features fusion-based algorithm for 3D face recognition (Anthropometric Curvelet Fusion Face Recognition, ACFFR). First, the eyes, nose, and mouth feature regions are extracted by the Anthropometric characteristics and curvature features of the human face. Second, Curvelet energy features of the facial feature regions at different scales and different directions are extracted by Curvelet transform. At last, Euclidean distance is used as the similarity between template and objectives. To verify the performance, the proposed algorithm is compared with Anthroface3D and Curveletface3D on the Texas 3D FR database. The experimental results have shown that the proposed algorithm performs well, with equal error rate of 1.75{\%} and accuracy of 97.0{\%}. The algorithm we proposed in this paper has better robustness to expression and light changes than Anthroface3D and Curveletface3D.},
author = {Song, Dan and Luo, Jing and Zi, Chunyuan and Tian, Huixin},
doi = {10.1155/2016/6859364},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song et al. - 2016 - 3D Face Recognition Using Anthropometric and Curvelet Features Fusion.pdf:pdf},
issn = {16877268},
journal = {Journal of Sensors},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
title = {{3D Face Recognition Using Anthropometric and Curvelet Features Fusion}},
volume = {2016},
year = {2016}
}
@article{ISI:000448833400016,
abstract = {In this study, a fully automatic pose and expression invariant 3D face
alignment algorithm is proposed to handle frontal and profile face
images which is based on a two pass course to fine alignment strategy.
The first pass of the algorithm coarsely aligns the face images to an
intrinsic coordinate system (ICS) through a single 3D rotation and the
second pass aligns them at fine level using a minimum nose tip-scanner
distance (MNSD) approach. For facial recognition, multi-view faces are
synthesized to exploit real 3D information and test the efficacy of the
proposed system. Due to optimal separating hyper plane (OSH), Support
Vector Machine (SVM) is employed in multi-view face verification (FV)
task. In addition, a multi stage unified classifier based face
identification (FI) algorithm is employed which combines results from
seven base classifiers, two parallel face recognition algorithms and an
exponential rank combiner, all in a hierarchical manner. The performance
figures of the proposed methodology are corroborated by extensive
experiments performed on four benchmark datasets: GavabDB, Bosphorus,
UMB-DB and FRGC v2.0. Results show mark improvement in alignment
accuracy and recognition rates. Moreover, a computational complexity
analysis has been carried out for the proposed algorithm which reveals
its superiority in terms of computational efficiency as well.},
author = {Ratyal, Naeem and Taj, Imtiaz and Bajwa, Usama and Sajid, Muhammad},
doi = {10.3837/tiis.2018.10.016},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ratyal et al. - 2018 - Pose and Expression Invariant Alignment based Multi-View 3D Face Recognition.pdf:pdf},
issn = {22881468},
journal = {KSII Transactions on Internet and Information Systems},
keywords = {3D FR,3D alignment,Profile face,SVM,Unified classifier,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
number = {10},
pages = {4903--4929},
title = {{Pose and expression invariant alignment based multi-view 3d face recognition}},
volume = {12},
year = {2018}
}
@article{ISI:000408398200010,
abstract = {Recognition of faces typically occurs via holistic processing where individual features are combined to provide an overall facial representation. However, when faces are inverted, there is greater reliance on featural processing where faces are recognized based on their individual features. These findings are based on a substantial number of studies using 2-dimensional (2D) faces and it is unknown whether these results can be extended to 3-dimensional (3D) faces, which have more depth information that is absent in the typical 2D stimuli used in face recognition literature. The current study used the face inversion paradigm as a means to investigate how holistic and featural processing are differentially influenced by 2D and 3D faces. Twenty-five participants completed a delayed face-matching task consisting of upright and inverted faces that were presented as both 2D and 3D stereoscopic images. Recognition accuracy was significantly higher for 3D upright faces compared to 2D upright faces, providing support that the enriched visual information in 3D stereoscopic images facilitates holistic processing that is essential for the recognition of upright faces. Typical face inversion effects were also obtained, regardless of whether the faces were presented in 2D or 3D. Moreover, recognition performances for 2D inverted and 3D inverted faces did not differ. Taken together, these results demonstrated that 3D stereoscopic effects influence face recognition during holistic processing but not during featural processing. Our findings therefore provide a novel perspective that furthers our understanding of face recognition mechanisms, shedding light on how the integration of stereoscopic information in 3D faces influences face recognition processes.},
author = {Eng, Z. H.D. and Yick, Y. Y. and Guo, Y. and Xu, H. and Reiner, M. and Cham, T. J. and Chen, S. H.A.},
doi = {10.1016/j.visres.2017.06.004},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eng et al. - 2017 - 3D faces are recognized more accurately and faster than 2D faces, but with similar inversion effects.pdf:pdf},
issn = {18785646},
journal = {Vision Research},
keywords = {2D,3D,Face inversion effect,Face recognition,Featural processing,Holistic processing,Stereoscopic images,revisao{\_}V1,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutui1},
pages = {78--85},
title = {{3D faces are recognized more accurately and faster than 2D faces, but with similar inversion effects}},
volume = {138},
year = {2017}
}
@article{Deng20155509,
abstract = {Copyright {\textcopyright} 2015 Binary Information Press. In order to eliminate the impact of facial expressions and improve the efficiency of calculation, this paper proposes a novel expression-robust 3D face recognition algorithm using region-based feature fusion technique based on multiscale wavelet transformations. The discrete wavelet transformation is applied to extract frequency component features of geometric image based on the semi-rigid face region as well as the non-rigid face region in order to reduce the influence from the facial expression using the Coherent Point Drift non-rigid point set registration. The dimensionality reduction methods are utilized to promote the computational efficiency, and the experimental results show that our algorithm outperforms state-of-the-art methods based on FRGC v2.0.},
annote = {cited By 0},
author = {Deng, X and Da, F and Shao, H},
doi = {10.12733/jcis14953},
journal = {Journal of Computational Information Systems},
keywords = {fatima,lerdepois,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {fatima,lerdepois,revisao{\_}V1,revisao{\_}scopus,tutui1},
number = {15},
pages = {5509--5517},
title = {{Expression-robust 3D face recognition using region-based multiscale wavelet feature fusion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950271521{\&}doi=10.12733{\%}2Fjcis14953{\&}partnerID=40{\&}md5=da8d93edfc030808fb309b097e409e94},
volume = {11},
year = {2015}
}
@article{Chouchane2015,
abstract = {{\textcopyright} 2014 IEEE. Face recognition in an uncontrolled condition such as illumination and expression variations is a challenging task. Local descriptor is one of the most efficient methods used to deal with these problems. In this paper, we present an automatic 3D face recognition approach based on three local descriptors, local phase quantization (LPQ), Three-Patch Local Binary Patterns (TPLBP) and Four-Patch Local Binary Patterns (TPLBP). Facial images are passing through one of the three descriptors and divided into sub-regions or rectangular blocks. The histogram of each sub-region is extracted and concatenated into a single feature vector. PCA (Principal Component Analysis) and EFM (Enhanced Fisher linear discriminant Model) are used to reduce the dimensionality of the resulting feature vectors. Finally, these vectors are sent to the classification step, when we use two methods; SVM (Support Victor Machine) and similarity measures. CASIA 3D face database is introduced to experimental evaluation. The experimental results illustrate a high recognition performance of the proposed approach.},
annote = {From Duplicate 2 (3D face recognition based on histograms of local descriptors - Chouchane, A; Belahcene, M; Ouamane, A; Bourennane, S)

cited By 4},
author = {Chouchane, A. and Belahcene, M. and Ouamane, A. and Bourennane, S.},
doi = {10.1109/IPTA.2014.7001925},
file = {:home/tutu/artigos{\_}revisao/07001925.pdf:pdf},
isbn = {9781479964611},
journal = {2014 4th International Conference on Image Processing Theory, Tools and Applications, IPTA 2014},
keywords = {3D face recognition,FPLBP,Local phase quantization,Locale descriptors,Support vector machines,TPLBP,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
title = {{3D face recognition based on histograms of local descriptors}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921721830{\&}doi=10.1109{\%}2FIPTA.2014.7001925{\&}partnerID=40{\&}md5=4bbf52130cd2484d05cce4611c376687},
year = {2015}
}
@inproceedings{8122665,
abstract = {3D face recognition is a popular research area due to its vast application in biometrics and security. Local featurebased methods gain importance in the recent years due to their robustness under degradation conditions. In this paper, a novel high-order local pattern descriptor in combination with sparse representation based classifier (SRC) is proposed for expression robust 3D face recognition. 3D point clouds are converted to depth maps after preprocessing. Multi-directional derivatives are applied in spatial space to encode the depth maps based on the local derivative pattern (LDP) scheme. Directional pattern features are calculated according to local derivative variations. Since LDP computes spatial relationship of neighbors in a local region, it extracts distinct information from the depth map. Multiscale depth-LDP is presented as a novel descriptor for 3D face recognition. The descriptor is employed along with the SRC to increase the range data distinctiveness. A histogram on the derivative pattern creates a spatial feature descriptor that represents the distinctive micro-patterns from 3D data. We evaluate the proposed algorithm on two famous 3D face databases, FRGC v2.0 and Bosphorus. The experimental results demonstrate that the proposed approach achieves acceptable performance under facial expression.},
annote = {15/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
15/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Soltanpour, Sima and Wu, Q. M.Jonathan},
booktitle = {2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017},
doi = {10.1109/SMC.2017.8122665},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soltanpour, Wu - 2017 - Multiscale depth local derivative pattern for sparse representation based 3D face recognition.pdf:pdf},
isbn = {9781538616451},
keywords = {emotion recognition,etapa1,face recognition,feature extra,gil,id54,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {etapa1,gil,id54,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {oct},
pages = {560--565},
publisher = {IEEE},
title = {{Multiscale depth local derivative pattern for sparse representation based 3D face recognition}},
url = {http://ieeexplore.ieee.org/document/8122665/},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{ISI:000374793400019,
abstract = {Face recognition in unconstrained environments is often influenced by pose variations. And the problem is basically the identification that uses partial data. In this paper, a method fusing structure and texture information is proposed to solve the problem. In the register phase, the approximate 180 degree information of face is acquired, and the data used to identify individual is obtained from a random single view. Pure face is extracted from 3D data first, then convert the original data to the form of spherical depth map (SDM) and spherical texture map (STM), which are invariant to out-plane rotation, subsequently facilitating the successive alignment-free identification that is robust to pose variations. We make identification through sparse representation for its well performance with the two maps. Experiments show that our proposed method gets a high recognition rate with pose and expression variations.},
annote = {- interessante a parte 3D, mas ele usa informa{\c{c}}{\~{a}}o de textura},
author = {Liu, Shuai and Mu, Zhichun and Huang, Hongbo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-25417-3_19},
editor = {{Yang, J and Yang, J and Sun, Z and Shan, S and Zheng, W and Feng}, J},
file = {:home/tutu/artigos{\_}revisao/96d0de48ff0343e2beba214a7471fd14-liu2015.pdf:pdf},
isbn = {9783319254166},
issn = {16113349},
keywords = {Face recognition,Sparse representation,Spherical Depth Map,Spherical Texture Map,lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutuix5},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutuix5},
organization = {Chinese Assoc Artificial Intelligence; Springer; Civil Aviat Univ China; Tianjin Univ Sci {\&} Technol; CASIA, Inst Intelligent Recognit},
pages = {151--159},
series = {Lecture Notes in Computer Science},
title = {{3D face recognition fusing spherical depth map and spherical texture map}},
volume = {9428},
year = {2015}
}
@article{Li:2018:EFR:3198485.3198687,
abstract = {This study proposes a 3D face recognition method using multiple subject-specific curves insensitive to intra-subject distortions caused by expression variations. Considering that most sharp variances in facial convex regions are closely related to the bone structure, the convex crest curves are first extracted as the most vital subject-specific facial curves based on the principal curvature extrema in convex local surfaces. Then, the central profile curve and the horizontal contour curve passing through the nose tip are detected by using the precise localization of the nose tip and symmetry plane. Based on their discriminative power and robustness to expression changes, the three types of curves are fused with appropriate weights at the feature-level and used for matching 3D faces with the iterative closest point algorithm. The combination of multiple expression-insensitive curves is complementary and provides sufficient and stable facial surface features for face recognition. In addition, for each convex crest curve, an expression-irrelevant factor is assigned as the adaptive weight to improve the face matching performance. The results of experiments using two public 3D databases, GavabDB and BU-3DFE, demonstrate the effectiveness of the proposed method, and its recognition rates on both databases reflect an encouraging performance.},
address = {Amsterdam, The Netherlands, The Netherlands},
annote = {29/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
29/04 Exclu{\'{i}}do (etapa 1)

Janeiro de 2018},
author = {Li, Ye and Wang, Ying Hui and Liu, Jing and Hao, Wen},
doi = {10.1016/j.neucom.2017.09.070},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2018 - Expression-insensitive 3D face recognition by the fusion of multiple subject-specific curves.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {3D face recognition,Expression-insensitive,Feature-level,Fusion,Subject-specific curve,acm,etapa1,id395,import{\_}poly,poly,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {acm,etapa1,id395,import{\_}poly,poly,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
number = {C},
pages = {1295--1307},
publisher = {Elsevier Science Publishers B. V.},
title = {{Expression-insensitive 3D face recognition by the fusion of multiple subject-specific curves}},
url = {https://doi.org/10.1016/j.neucom.2017.09.070},
volume = {275},
year = {2018}
}
@inproceedings{Li2018234,
abstract = {This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by explor- ing location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normaliza- tion. The 3D coordinates of each normalized 3D facial sur- face are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep represen- tations of facial surface normals, i.e., deep normal pattern- s. Considering the importance of different facial locations, we propose a location sensitive sparse representation clas- sifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, sim- ple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one s- cores of 98.01{\%}, 97.60{\%}, and 96.13{\%} on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sam- ple per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition.},
annote = {From Duplicate 1 (Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition - Li, Huibin; Sun, Jian; Chen, Liming)

From Duplicate 1 (Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition - Li, Huibin; Sun, Jian; Chen, Liming)

From Duplicate 1 (Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition - Li, H; Sun, J; Chen, L)

cited By 1

From Duplicate 2 (Location-Sensitive Sparse Representation of Deep Normal Patterns for Expression-robust 3D Face Recognition - Li, Huibin; Sun, Jian; Chen, Liming)

IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
OCT 01-04, 2017

From Duplicate 2 (Location-Sensitive Sparse Representation of Deep Normal Patterns for Expression-robust 3D Face Recognition - Li, Huibin; Sun, Jian; Chen, Liming)

IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
OCT 01-04, 2017

From Duplicate 2 (Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition - Li, H; Sun, J; Chen, L)

cited By 1

From Duplicate 3 (Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition - Li, Huibin; Sun, Jian; Chen, Liming)

IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
OCT 01-04, 2017

From Duplicate 4 (Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition - Li, Huibin; Sun, Jian; Chen, Liming)

From Duplicate 1 (Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition - Li, H; Sun, J; Chen, L)

cited By 1

From Duplicate 2 (Location-Sensitive Sparse Representation of Deep Normal Patterns for Expression-robust 3D Face Recognition - Li, Huibin; Sun, Jian; Chen, Liming)

IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
OCT 01-04, 2017},
author = {Li, Huibin and Sun, Jian and Chen, Liming},
booktitle = {IEEE International Joint Conference on Biometrics, IJCB 2017},
doi = {10.1109/BTAS.2017.8272703},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Sun, Chen - 2017 - Location-Sensitive Sparse Representation of Deep Normal Patterns for Expression-robust 3D Face Recognition.pdf:pdf},
isbn = {9781538611241},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
organization = {IEEE},
pages = {234--242},
title = {{Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046291057{\&}doi=10.1109{\%}2FBTAS.2017.8272703{\&}partnerID=40{\&}md5=e5c4065442f8bc899597efdc443ed01c},
volume = {2018-Janua},
year = {2018}
}
@article{Bellil:2016:GWN:2877705.2877756,
abstract = {{\textcopyright} 2014, Springer Science+Business Media New York. The first handicap in 3D faces recognizing under unconstrained problem is the largest variability of the visual aspect when we use various sources. This great variability complicates the task of identifying persons from their 3D facial scans and it is the most reason that bring to face detection and recognition of the major problems in pattern recognition fields, biometrics and computer vision. We propose a new 3D face identification and recognition method based on Gappy Wavelet Neural Network (GWNN) that is able to provide better accuracy in the presence of facial occlusions. The proposed approach consists of three steps: the first step is face detection. The second step is to identify and remove occlusions. Occluded regions detection is done by considering that occlusions can be defined as local face deformations. These deformations are detected by a comparison between the input facial test wavelet coefficients and wavelet coefficients of generic face model formed by the mean data base faces. They are beneficial for neighborhood relationships between pixels rotation, dilation and translation invariant. Then, occluded regions are refined by removing wavelet coefficient above a certain threshold. Finally, the last stage of processing and retrieving is made based on wavelet neural network to recognize and to restore 3D occluded regions that gathers the most. The experimental results on this challenging database demonstrate that the proposed approach improves recognition rate performance from 93.57 to 99.45 {\%} which represents a competitive result compared to the state of the art.},
address = {Hingham, MA, USA},
annote = {22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Bellil, Wajdi and Brahim, Hajer and {Ben Amar}, Chokri},
doi = {10.1007/s11042-014-2294-6},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellil, Brahim, Ben Amar - 2016 - Gappy wavelet neural network for 3D occluded faces detection and recognition.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {3D face recognition; Wavelets,Gappy data,Occlusion detection,Wavelet neural network,acm,etapa1,gil,id125,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {acm,etapa1,gil,id125,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {jan},
number = {1},
pages = {365--380},
publisher = {Kluwer Academic Publishers},
title = {{Gappy wavelet neural network for 3D occluded faces: detection and recognition}},
url = {http://dx.doi.org/10.1007/s11042-014-2294-6 http://link.springer.com/10.1007/s11042-014-2294-6},
volume = {75},
year = {2016}
}
@inproceedings{Galbally:2016:BSI:2982636.2982657,
abstract = {Biometric systems typically suffer a significant loss of performance when the acquisition sensor is changed between enrolment and authentication. Such a problem, commonly known as sensor interoperability, poses a serious challenge to the accuracy of matching algorithms. The present work addresses for the first time the sensor interoperability issue in 3D face recognition systems, analysing the performance of two popular and well known techniques for 3D facial authentication. For this purpose, a new gender-balanced database comprising 3D data of 26 subjects has been acquired using two devices belonging to the new generation of low-cost 3D sensors. The results show the high sensor-dependency of the tested systems and the need to develop matching algorithms robust to the variation in the sensor resolution.},
address = {Portugal},
annote = {13/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
13/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Galbally, Javier and Satta, Riccardo},
booktitle = {Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods},
doi = {10.5220/0005682501990204},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Galbally, Satta - 2016 - Biometric Sensor Interoperability A Case Study in 3D Face Recognition.pdf:pdf},
isbn = {978-989-758-173-1},
keywords = {3D Face Database.,3D Face Recognition,Interoperability,acm,etapa1,gil,id51,revisao{\_}V1,revisao{\_}scopus,tutui2},
mendeley-tags = {acm,etapa1,gil,id51,revisao{\_}V1,revisao{\_}scopus,tutui2},
pages = {199--204},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
series = {ICPRAM 2016},
title = {{Biometric Sensor Interoperability: A Case Study in 3D Face Recognition}},
url = {http://dx.doi.org/10.5220/0005682501990204},
year = {2016}
}
@article{Wei201766,
abstract = {In this paper, we propose a general 3D face recognition framework by combining the idea of surface harmonic mapping and deep learning. In particular, given a 3D face scan, we first run the pre-processing pipeline and detect three main facial landmarks (i.e., nose tip and two inner eye corners). Then, harmonic mapping is employed to map the 3D coordinates and differential geometry quantities (e.g., normal vectors, curvatures) of each 3D face scan to a 2D unit disc domain, generating a group of 2D harmonic shape images (HSI). The 2D rotation of the harmonic shape images are removed by using the three detected landmarks. All these pose normalized harmonic shape images are fed into a pre-trained deep convolutional neural network (DCNN) to generate their deep representations. Finally, sparse representation classifier with score-level fusion is used for face similarity measurement and the final decision. The advantage of our method is twofold: (i) it is a general framework and can be easily extended to other surface mapping and deep learning algorithms. (ii) it is registration-free and only needs three landmarks. The effectiveness of the proposed framework was demonstrated on the BU-3DFE database, and reporting a rank-one recognition rate of 89.38{\%} on the whole database. {\textcopyright} 2017, Springer International Publishing AG.},
annote = {cited By 0},
author = {Wei, Xiaofan and Li, Huibin and Gu, Xianfeng David},
doi = {10.1007/978-3-319-69923-3_8},
isbn = {9783319699226},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D face recognition,Deep learning,Surface harmonic mapping,revisao{\_}V2,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V2,revisao{\_}scopus,tutui1},
pages = {66--76},
title = {{Three Dimensional Face Recognition via Surface Harmonic Mapping and Deep Learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032656852{\&}doi=10.1007{\%}2F978-3-319-69923-3{\_}8{\&}partnerID=40{\&}md5=598d2aabb96d1b2d7f9963542a2fcddf},
volume = {10568 LNCS},
year = {2017}
}
@article{Emambakhsh2017,
abstract = {The potential of the nasal region for expression robust 3D face recognition is thoroughly investigated by a novel five-step algorithm. First, the nose tip location is coarsely detected and the face is segmented, aligned and the nasal region cropped. Then, a very accurate and consistent nasal landmarking algorithm detects seven keypoints on the nasal region. In the third step, a feature extraction algorithm based on the surface normals of Gabor-wavelet filtered depth maps is utilised and, then, a set of spherical patches and curves are localised over the nasal region to provide the feature descriptors. The last step applies a genetic algorithm-based feature selector to detect the most stable patches and curves over different facial expressions. The algorithm provides the highest reported nasal region-based recognition ranks on the FRGC, Bosphorus and BU-3DFE datasets. The results are comparable with, and in many cases better than, many state-of-the-art 3D face recognition algorithms, which use the whole facial domain. The proposed method does not rely on sophisticated alignment or denoising steps, is very robust when only one sample per subject is used in the gallery, and does not require a training step for the landmarking algorithm. https://github.com/mehryaragha/NoseBiometrics},
author = {Emambakhsh, Mehryar and Evans, Adrian},
doi = {10.1109/TPAMI.2016.2565473},
file = {:home/tutu/artigos{\_}revisao/07467565.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Face recognition,Gabor wavelets,facial landmarking,feature selection,nose region,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,surface normals,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {may},
number = {5},
pages = {995--1007},
title = {{Nasal Patches and Curves for Expression-Robust 3D Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7467565/},
volume = {39},
year = {2017}
}
@inproceedings{Soltanpour20182811,
abstract = {{\textcopyright} 2017 IEEE. This paper proposes a novel descriptor based on the local derivative pattern (LDP) for 3D face recognition. Compared to the local binary pattern (LBP), LDP can capture more detailed information by encoding directional pattern features. It is based on the local derivative variations that extract high-order local information. We propose a novel discriminative facial shape descriptor, local normal derivative pattern (LNDP) that extracts LDP from the surface normal. Using surface normal, the orientation of a surface at each point is determined as a first-order surface differential. Three normal component images are extracted by estimating three components of normal vectors in x, y, and z channels. Each normal component is divided into several patches and encoded using LDP. The final descriptor is created by concatenating histograms of the LNDP on each patch. Experimental results on two famous 3D face databases, FRGC v2.0 and Bosphorus illustrate the effectiveness of the proposed descriptor.},
annote = {cited By 1},
author = {Soltanpour, Sima and Wu, Q. M.Jonathan},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2017.8296795},
isbn = {9781509021758},
issn = {15224880},
keywords = {3D face,High-order local pattern,Local derivative pattern,Surface normal,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
pages = {2811--2815},
title = {{High-order local normal derivative pattern (LNDP) for 3D face recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045292850{\&}doi=10.1109{\%}2FICIP.2017.8296795{\&}partnerID=40{\&}md5=12c33512c432220af4c4c49f5116433e},
volume = {2017-Septe},
year = {2018}
}
@article{ISI:000410465400003,
abstract = {In this paper, we introduce a novel, automatic method for 3D face
recognition. A new feature called a spherical vector norms map of a 3D
face is created using the normal vector of each point. This feature
contains more detailed information than the original depth image in
regions such as the eyes and nose. For certain flat areas of 3D face,
such as the forehead and cheeks, this map could increase the
distinguishability of different points. In addition, this feature is
robust to facial expression due to an adjustment that is made in the
mouth region. Then, the facial representations, which are based on
Histograms of Oriented Gradients, are extracted from the spherical
vector norms map and the original depth image. A new partitioning
strategy is proposed to produce the histogram of eight patches of a
given image, in which all of the pixels are binned based on the
magnitude and direction of their gradients. In this study, SVNs map and
depth image are represented compactly with two histograms of oriented
gradients; this approach is completed by Linear Discriminant Analysis
and a Nearest Neighbor classifier.},
author = {Wang, Xue Qiao and Yuan, Jia Zheng and Li, Qing},
doi = {10.6688/JISE.2017.33.5.3},
issn = {10162364},
journal = {Journal of Information Science and Engineering},
keywords = {3D face recognition,Face recognition grand challenge database,Histograms of oriented gradients,Linear discriminant analysis,Spherical vector norms map,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
number = {5},
pages = {1141--1161},
title = {{3D face recognition using spherical vector norms map}},
volume = {33},
year = {2017}
}
@inproceedings{8227850,
abstract = {Face recognition remains a challenge today as recognition performance is strongly affected by variability such as illumination, expressions and poses. In this work we apply Convolutional Neural Networks (CNNs) on the challenging task of both 2D and 3D face recognition. We constructed two CNN models, namely CNN-1 (two convolutional layers) and CNN-2 (one convolutional layer) for testing on 2D and 3D dataset. A comprehensive parametric study of two CNN models on face recognition is represented in which different combinations of activation function, learning rate and filter size are investigated. We find that CNN-2 has a better accuracy performance on both 2D and 3D face recognition. Our experimental results show that an accuracy of 85.15{\%} was accomplished using CNN-2 on depth images with FRGCv2.0 dataset (4950 images with 557 objectives). An accuracy of 95{\%} was achieved using CNN-2 on 2D raw image with the AT{\&}T dataset (400 images with 40 objectives). The results indicate that the proposed CNN model is capable to handle complex information from facial images in different dimensions. These results provide valuable insights into further application of CNN on 3D face recognition.},
annote = {26/04 em processo
26/04 exclus{\~{a}}o},
author = {Hu, Huiying and Shah, Syed Afaq Ali and Bennamoun, Mohammed and Molton, Michael},
booktitle = {IEEE Region 10 Annual International Conference, Proceedings/TENCON},
doi = {10.1109/TENCON.2017.8227850},
file = {:home/tutu/artigos{\_}revisao/08227850.pdf:pdf},
isbn = {9781509011339},
issn = {21593450},
keywords = {Convolutional Neural Networks,Depth Image,Face Recognition,etapa1,id314,ieeexplore,poly,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {etapa1,id314,ieeexplore,poly,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {nov},
pages = {133--138},
title = {{2D and 3D face recognition using convolutional neural network}},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{ISI:000367310300024,
abstract = {Expression change is the major cause of local plastic deformation of the facial surface. The intra-class differences with large expression change somehow are larger than the inter-class differences as it's difficult to distinguish the same individual with facial expression change. In this paper, an expression-robust 3D face recognition method is proposed by learning expression deformation model. The expression of the individuals on the training set is modeled by principal component analysis, the main components are retained to construct the facial deformation model. For the test 3D face, the shape difference between the test and the neutral face in training set is used for reconstructing the expression change by the constructed deformation model. The reconstruction residual error is used for face recognition. The average recognition rate on GavabDB and self-built database reaches 85.1{\%} and 83{\%}, respectively, which shows strong robustness for expression changes.},
annote = {7th International Conference on Graphic and Image Processing (ICGIP),
Singapore, SINGAPORE, OCT 23-25, 2015},
author = {Guo, Zhe and Liu, Shu and Wang, Yi and Lei, Tao},
booktitle = {Seventh International Conference on Graphic and Image Processing (ICGIP 2015)},
doi = {10.1117/12.2228002},
editor = {{Wang, Y and Jiang}, X},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2015 - Learning deformation model for expression-robust 3D face recognition.pdf:pdf},
isbn = {978-1-5106-0058-4},
issn = {0277-786X},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
organization = {Wuhan Univ; Int Assoc Comp Sci {\&} Informat Technol},
pages = {98170O},
series = {Proceedings of SPIE},
title = {{Learning deformation model for expression-robust 3D face recognition}},
volume = {9817},
year = {2015}
}
@article{Gilani:2017:DDA:3103259.3103504,
abstract = {We present a multilinear algorithm to automatically establish dense point-to-point correspondence over an arbitrarily large number of population specific 3D faces across identities, facial expressions and poses. The algorithm is initialized with a subset of anthropometric landmarks detected by our proposed Deep Landmark Identification Network which is trained on synthetic images. The landmarks are used to segment the 3D face into Voronoi regions by evolving geodesic level set curves. Exploiting the intrinsic features of these regions, we extract discriminative keypoints on the facial manifold to elastically match the regions across faces for establishing dense correspondence. Finally, we generate a Region based 3D Deformable Model which is fitted to unseen faces to transfer the correspondences. We evaluate our algorithm on the tasks of facial landmark detection and recognition using two benchmark datasets. Comparison with thirteen state-of-the-art techniques shows the efficacy of our algorithm.},
address = {New York, NY, USA},
annote = {18/05/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
18/05/2018 Exclu{\'{i}}do (etapa 1)},
author = {Gilani, Syed Zulqarnain and Mian, Ajmal and Eastwood, Peter},
doi = {10.1016/j.patcog.2017.04.013},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gilani, Mian, Eastwood - 2017 - Deep, dense and accurate 3D face correspondence for generating population specific deformable models.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {3D face morphing,Deep learning,Dense 3D face correspondence,Face recognition,Keypoint detection,Landmark identification,Shape descriptor,acm,estela,etapa1,id488,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {acm,estela,etapa1,id488,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {sep},
number = {C},
pages = {238--250},
publisher = {Elsevier Science Inc.},
title = {{Deep, dense and accurate 3D face correspondence for generating population specific deformable models}},
url = {https://doi.org/10.1016/j.patcog.2017.04.013 http://linkinghub.elsevier.com/retrieve/pii/S0031320317301644},
volume = {69},
year = {2017}
}
@inproceedings{ISI:000444905600019,
abstract = {In this paper, we present an efficient method for 3D face recognition based on vector quantization of both geometrical and visual proprieties of the face. The method starts by describing each 3D face using a set of orderless features, and use then the Bag-of-Features paradigm to construct the face signature. We analyze the performance of three well-known classifiers: the Naive Bayes, the Multilayer perceptron and the Random forests. The results reported on the FRGCv2 dataset show the effectiveness of our approach and prove that the method is robust to facial expression.},
annote = {12th International Joint Conference on Computer Vision, Imaging and
Computer Graphics Theory and Applications (VISIGRAPP), Porto, PORTUGAL,
FEB 27-MAR 01, 2017},
author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Declercq, David and Benouareth, Abdallah},
booktitle = {PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5},
doi = {10.5220/0006101701870193},
editor = {{Imai, F and Tremeau, A and Braz}, J},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hariri et al. - 2017 - Geometrical and Visual Feature Quantization for 3D Face Recognition.pdf:pdf},
isbn = {978-989-758-226-4},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
organization = {Inst Syst {\&} Technologies Informat, Control {\&} Commun; ACM SIGGRAPH; AFIG; Eurographics},
pages = {187--193},
title = {{Geometrical and Visual Feature Quantization for 3D Face Recognition}},
year = {2017}
}
@inproceedings{ISI:000371977803154,
abstract = {A 3D face recognition method using region-based extended local binary pattern (eLBP) is proposed. First, the depth image converted from the preprocessed 3D pointclouds is normalized. Then, different regions according to their distortions under facial expressions are extracted by binary masks and represented by the uniform pattern of extended LBP. Finally, sparse representation classifier (SRC) is adopted for classification on the single region. Feature-level and score-level fusion with weight-sparse representation classifier (W-SRC) are also tested and compared, and the latter has better performance. The experiments on FRGC v2.0 database demonstrate that the proposed method is robust and efficient.},
annote = {IEEE International Conference on Image Processing (ICIP), Quebec City,
CANADA, SEP 27-30, 2015},
author = {Lv, Shiwen and Da, Feipeng and Deng, Xing},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2015.7351482},
file = {:home/tutu/artigos{\_}revisao/07351482.pdf:pdf},
isbn = {9781479983391},
issn = {15224880},
keywords = {3D face recognition,binary mask,depth image,extended local binary pattern,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1,weight-sparse representation classifier},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {sep},
organization = {Inst Elect {\&} Elect Engineers; IEEE Signal Proc Soc},
pages = {3635--3639},
publisher = {IEEE},
series = {IEEE International Conference on Image Processing ICIP},
title = {{A 3D face recognition method using region-based extended local binary pattern}},
url = {http://ieeexplore.ieee.org/document/7351482/},
volume = {2015-Decem},
year = {2015}
}
@article{Hajati2019936,
abstract = {{\textcopyright} 2019, Springer Nature Switzerland AG. We propose Polar Topographic Derivatives (PTD) to fuse the shape and texture information of a facial surface for 3D face recognition. Polar Average Absolute Deviations (PAADs) of the Gabor topography maps are extracted as features. High-order polar derivative patterns are obtained by encoding texture variations in a polar neighborhood. By using the and Bosphorus 3D face database, our method shows that it is robust to expression and pose variations comparing to existing state-of-the-art benchmark approaches.},
annote = {cited By 0},
author = {Hajati, Farshid and Cheraghian, Ali and {Ameri Sianaki}, Omid and Zeinali, Behnam and Gheisari, Soheila},
doi = {10.1007/978-3-030-15035-8_92},
isbn = {9783030150341},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {revisao{\_}V2,revisao{\_}scopus,tutux5},
mendeley-tags = {revisao{\_}V2,revisao{\_}scopus,tutux5},
pages = {936--945},
title = {{Polar Topographic Derivatives for 3D Face Recognition: Application to Internet of Things Security}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064875340{\&}doi=10.1007{\%}2F978-3-030-15035-8{\_}92{\&}partnerID=40{\&}md5=a1997efd234042ace42c04b05607ab05},
volume = {927},
year = {2019}
}
@inproceedings{Xie:2016:IFR:3028842.3028853,
abstract = {Automatic face recognition techniques applied on particular group or mass database introduces error cases. Error prevention is crucial for the court. Reranking of recognition results based on anthropology analysis can significant improve the accuracy of automatic methods. Previous studies focused on manual facial comparison. This paper proposed a weighted facial similarity computing method based on morphological analysis of components characteristics. Search sequence of face recognition reranked according to similarity, while the interference terms can be removed. Within this research project, standardized photographs, surveillance videos, 3D face images, identity card photographs of 241 male subjects from China were acquired. Sequencing results were modified by modeling selected individual features from the DMV altas. The improved method raises the accuracy of face recognition through anthroposophic or morphologic theory.},
address = {New York, New York, USA},
author = {Xie, Lanchi and Xu, Lei and Zhang, Ning and Guo, Jingjing and Yan, Yuwen and Li, Zhihui and Li, Zhigang and Xu, Xiaojing},
booktitle = {Proceedings of the 2016 International Conference on Intelligent Information Processing - ICIIP '16},
doi = {10.1145/3028842.3028853},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie et al. - 2017 - Improved face recognition result reranking based on shape contexts.pdf:pdf},
isbn = {9781450347990},
keywords = {face recognition,reranking,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,shape contexts,shape matching,similarity calculation,tutux3,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux3,tutux5},
pages = {1--6},
publisher = {ACM Press},
series = {ICIIP '16},
title = {{Improved face recognition result reranking based on shape contexts}},
url = {http://doi.acm.org/10.1145/3028842.3028853 http://dl.acm.org/citation.cfm?doid=3028842.3028853},
year = {2017}
}
@inproceedings{ISI:000380429100020,
abstract = {? 2015 IEEE.With the increasing availability of low-cost 3D data acquisition devices, the use of 3D face data for the recognition of individuals is becoming more appealing and computationally feasible. This paper proposes a completely automatic algorithm for face registration and matching. The algorithm is based on the extraction of stable 3D facial features characterizing the face and the subsequent construction of a signature manifold. The facial features are extracted by performing a continuous-to-discrete scale-space analysis. Registration is driven from the matching of triplets of feature points and the registration error is computed as shape matching score. Conversely to most techniques in the literature, a major advantage of the proposed method is that no data pre-processing is required. Therefore all presented results have been obtained exclusively from the raw data available from the 3D acquisition device. The method has been tested on the Bosphorus 3D face database and the performances compared to the ICP baseline algorithm. Even in presence of noise in the data, the algorithm proved to be very robust and reported identification performances which are aligned to the current state of the art, but without requiring any pre-processing of the raw data.},
annote = {2015 3rd International Workshop on Biometrics and Forensics (IWBF),
Gjovik, NORWAY, MAR 03-04, 2015},
author = {Lagorio, A. and Cadoni, M. and Grosso, E. and Tistarelli, M.},
booktitle = {3rd International Workshop on Biometrics and Forensics, IWBF 2015},
doi = {10.1109/IWBF.2015.7110239},
file = {:home/tutu/artigos{\_}revisao/07110239.pdf:pdf},
isbn = {9781479981052},
keywords = {3D Face recognition,Face recognition,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {mar},
organization = {European Cooperat Sci {\&} Technol; IEEE; COST Act IC1106; Inst Engn {\&} Technol; European Assoc Signal Proc; European Assoc Biometr; HOGSKOLEN},
pages = {1--7},
publisher = {IEEE},
title = {{A 3D algorithm for unsupervised face identification}},
url = {http://ieeexplore.ieee.org/document/7110239/},
year = {2015}
}
@inproceedings{8269662,
abstract = {Facial recognition has most significant real-life requests like investigation and access control. It is associated through the issue of appropriately verifying face pictures and transmit them person in a database. In a past years face study has been emerging active topic. Most of the face detector techniques could be classified into feature based methods and image based also. Feature based techniques adds low-level analysis, feature analysis, etc. Facial recognition is a system capable of verifying / identifying a human after 3D images. By evaluating selected facial unique features from the image and face dataset. Design from transformation method given vector dimensional illustration of individual face in a prepared set of images, Principle component analysis inclines to search a dimensional sub-space whose normal vector features correspond to the maximum variance direction in the real image space. The PCA algorithm evaluates the feature extraction, data, i.e. Eigen Values and vectors of the scatter matrix. In literature survey, Face recognition is a design recognition mission performed exactly on faces. It can be described as categorizing a facial either “known” or “unknown”, after comparing it with deposits known individuals. It is also necessary to need a system that has the capability of knowledge to recognize indefinite faces. Computational representations of facial recognition must statement various difficult issues. After existing work, we study the SIFT structures for the gratitude method. The novel technique is compared with well settled facial recognition methods, name component analysis and eigenvalues and vector. This algorithm is called PCA and ICA (Independent Component Analysis). In research work, we implement the novel approach to detect the face in minimum time and evaluate the better accuracy based on Back Propagation Neural Networks. We design the framework in face recognition using MATLAB 2013a simulation tool. Evaluate the performance parameters, i.e. the FAR (false acceptance rate), FRR (False rejection Rate) and Accuracy and compare the existing performance parameters i.e. accuracy.},
author = {Kaur, Rajwant and Sharma, Dolly and Verma, Amit},
booktitle = {4th IEEE International Conference on Signal Processing, Computing and Control, ISPCC 2017},
doi = {10.1109/ISPCC.2017.8269662},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaur, Sharma, Verma - 2017 - An advance 2D face recognition by feature extraction (ICA) and optimize multilayer architecture.pdf:pdf},
isbn = {9781509058389},
keywords = {Eigen values and Vectors,Face recognition,Features of face,Neural Network,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
pages = {122--129},
title = {{An advance 2D face recognition by feature extraction (ICA) and optimize multilayer architecture}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Reji2018,
abstract = {This paper focuses on a region based methodology for expression in
sensitive 3D face recognition process. Considering facial regions that
are comparatively unchanging during expressions, results shows that
using fifteen sub regions on the face can attain high 3D face
recognition. We use a modified face recognition algorithm along with
hierarchical contour based image registration for finding the similarity
score. Our method operates in two modes: verification mode and
confirmation mode. Crop 100 mm of frontal face region, apply
preprocessing and automatically detect nose tip, translate the face
image to origin and crop fifteen sub regions. The cropped sub regions
are defined by cuboids which occupy more volumetric data, Nose Tip is
the most projecting point of the face with the highest value along
Z-axis so consider it as origin. The modified face recognition algorithm
reduces the effects caused by facial expressions and artifacts. Finally
a Hierarchical contour based image registration technique is applied
which yields better results. The approach is applied on Bosphorus 3D
datasets and achieved a verification rate of 95.3{\%} at 0.1{\%} false
acceptance rate. In the identification scenario 99.3{\%} rank one
recognition is achieved.},
annote = {cited By 0},
author = {Reji, R. and Sojanlal, P.},
booktitle = {2017 IEEE International Conference on Computational Intelligence and Computing Research, ICCIC 2017},
doi = {10.1109/ICCIC.2017.8524581},
file = {:home/tutu/artigos{\_}revisao/08524581.pdf:pdf},
isbn = {9781509066209},
keywords = {3D face recognition,Biometrics,Contour based image registration,MFRA,Rank based Score,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
title = {{Region Based 3D Face Recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057946157{\&}doi=10.1109{\%}2FICCIC.2017.8524581{\&}partnerID=40{\&}md5=92c092fa4e2838ab444fcc0356fcdb75},
year = {2018}
}
@inproceedings{7453907,
abstract = {In this paper, we introduce a new feature named spherical vector norms for 3D face recognition. The proposed feature is efficient, insensitive to facial expression and contains discriminatory information of 3D face. The feature extraction method is firstly finding a set of the points with the closest distance to the standard face, denoted as closest point coordinates, and then extracting the spherical vector norms of these points. This paper combines point coordinates and spherical vector norms for improving recognition. Finally this approach is finished by Linear Discriminant Analysis (LDA) and Nearest Neighbor classifier. We have performed different experiments on the Face Recognition Grand Challenge database. It achieves the verification rate of 97.11{\%} on All vs. All experiment at 0.1{\%} FAR and 96.64{\%} verification rate on Neutral vs. Expression experiment.},
annote = {28/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
28/04 Exclu{\'{i}}do (etapa 1)},
author = {{Gaoyun An} and {Yi Jin} and {Xueqiao Wang} and {Qiuqi Ruan}},
booktitle = {6th International Conference on Wireless, Mobile and Multi-Media (ICWMMN 2015)},
doi = {10.1049/cp.2015.0943},
isbn = {978-1-78561-046-2},
keywords = {etapa1,face recognition,feature extraction,id356,ieeexplore,image classifi,poly,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {etapa1,id356,ieeexplore,poly,revisao{\_}V1,revisao{\_}scopus,tutui1},
month = {nov},
pages = {5 .--5 .},
publisher = {Institution of Engineering and Technology},
title = {{3D face recognition using closest point coordinates and spherical vector norms}},
url = {https://digital-library.theiet.org/content/conferences/10.1049/cp.2015.0943},
year = {2016}
}
@article{ISI:000463462600049,
abstract = {Extracting efficient features from the large volume of 3D facial data directly is extremely difficult in 3D face recognition (3D-FR) with the latest methods, which mostly require heavy computations and manual processing steps. This paper presents a computationally efficient 3D-FR system based on a novel Frenet frame-based feature that is derived from the 3D facial iso-geodesic curves. In terms of the evaluation of the proposed method, we conducted a number of experiments on the CASIA 3D face database, and a superior recognition performance has been achieved. The performance evaluation suggests that the pose invariance attribute of the features relieves the need of an expensive 3D face registration in the face preprocessing procedure, where we take less time to process conversely. Our experiments further demonstrate that the proposed method not only achieves competitive recognition performance when compared with some existing techniques for 3D-FR, but also is computationally efficient.},
author = {Shi, Biao and Zang, Huaijuan and Zheng, Rongsheng and Zhan, Shu},
doi = {10.1016/j.jvcir.2019.02.002},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2019 - An efficient 3D face recognition approach using Frenet feature of iso-geodesic curves.pdf:pdf},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {3D face recognition,Facial curves,Frenet framework,Iso-geodesic,Pose invariant,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui2},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui2},
pages = {455--460},
title = {{An efficient 3D face recognition approach using Frenet feature of iso-geodesic curves}},
volume = {59},
year = {2019}
}
@article{ISI:000351796000002,
abstract = {3D face recognition and emotion analysis play important roles in many fields of communication and edutainment. An effective facial descriptor, with higher discriminating capability for face recognition and higher descriptiveness for facial emotion analysis, is a challenging issue. However, in the practical applications, the descriptiveness and discrimination are independent and contradictory to each other. 3D facial data provide a promising way to balance these two aspects. In this paper, a robust regional bounding spherical descriptor (RBSR) is proposed to facilitate 3D face recognition and emotion analysis. In our framework, we first segment a group of regions on each 3D facial point cloud by shape index and spherical bands on the human face. Then the corresponding facial areas are projected to regional bounding spheres to obtain our regional descriptor. Finally, a regional and global regression mapping (RGRM) technique is employed to the weighted regional descriptor for boosting the classification accuracy. Three largest available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the performance comparison and the experimental results show a consistently better performance for 3D face recognition and emotion analysis.},
author = {Ming, Yue},
doi = {10.1016/j.imavis.2014.12.003},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ming - 2015 - Robust regional bounding spherical descriptor for 3D face recognition and emotion analysis.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {3D face recognition,Emotion analysis,Kullback-Leiber divergence (KLD),Regional and global regression,Regional bounding spherical descriptor,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {mar},
pages = {14--22},
title = {{Robust regional bounding spherical descriptor for 3D face recognition and emotion analysis}},
volume = {35},
year = {2015}
}
@article{ISI:000395034500021,
abstract = {{\textcopyright} 2016 American Academy of Forensic SciencesTechniques of 2D–3D superimposition are widely used in cases of personal identification from video surveillance systems. However, the progressive improvement of 3D image acquisition technology will enable operators to perform also 3D–3D facial superimposition. This study aims at analyzing the possible applications of 3D–3D superimposition to personal identification, although from a theoretical point of view. Twenty subjects underwent a facial 3D scan by stereophotogrammetry twice at different time periods. Scans were superimposed two by two according to nine landmarks, and root-mean-square (RMS) value of point-to-point distances was calculated. When the two superimposed models belonged to the same individual, RMS value was 2.10 mm, while it was 4.47 mm in mismatches with a statistically significant difference (p {\textless} 0.0001). This experiment shows the potential of 3D–3D superimposition: Further studies are needed to ascertain technical limits which may occur in practice and to improve methods useful in the forensic practice.},
author = {Gibelli, Daniele and {De Angelis}, Danilo and Poppa, Pasquale and Sforza, Chiarella and Cattaneo, Cristina},
doi = {10.1111/1556-4029.13290},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gibelli et al. - 2017 - A View to the Future A Novel Approach for 3D–3D Superimposition and Quantification of Differences for Identifi.pdf:pdf},
issn = {15564029},
journal = {Journal of Forensic Sciences},
keywords = {forensic anatomy,forensic anthropology,forensic science,personal identification,revisao{\_}V2,revisao{\_}webofscience,stereophotogrammetry,tutux9,video surveillance system},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
number = {2},
pages = {457--461},
title = {{A View to the Future: A Novel Approach for 3D-3D Superimposition and Quantification of Differences for Identification from Next-Generation Video Surveillance Systems}},
volume = {62},
year = {2017}
}
@inproceedings{7424213,
abstract = {This work proposes a new algorithm for 3D face recognition. The algorithm uses 3D shape data without color or texture information and exploits local curvature information which is a measure with high discriminant capability and robust to deformations such as rotation and scaling. In order to reduce high dimensionality of typical face surfaces our approach uses a conformal parameterization, preserving angles of original faces and simplifies the correspondence problem. Experimental results are presented and discussed using CASIA and Gavab databases.},
annote = {From Duplicate 2 (Conformal parameterization and curvature analysis for 3D facial recognition - Echeagaray-Patron, B A; Miramontes-Jaramillo, D; Kober, V)

From Duplicate 1 (Conformal parameterization and curvature analysis for 3D facial recognition - Echeagaray-Patron, B A; Miramontes-Jaramillo, D; Kober, V)

cited By 20

From Duplicate 2 (Conformal parameterization and curvature analysis for 3D facial recognition - Echeagaray-Patron, B A; Miramontes-Jaramillo, D; Kober, V)

From Duplicate 1 (Conformal parameterization and curvature analysis for 3D facial recognition - Echeagaray-Patron, B A; Miramontes-Jaramillo, D; Kober, V)

cited By 20},
author = {Echeagaray-Patron, B A and Miramontes-Jaramillo, D and Kober, V},
booktitle = {2015 International Conference on Computational Science and Computational Intelligence (CSCI)},
doi = {10.1109/CSCI.2015.133},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Echeagaray-Patron, Miramontes-Jaramillo, Kober - 2015 - Conformal Parameterization and Curvature Analysis for 3D Facial Recognition.pdf:pdf},
keywords = {3D facial recogn,face recognition,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutui1,visual databases},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutui1},
pages = {843--844},
title = {{Conformal Parameterization and Curvature Analysis for 3D Facial Recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964412211{\&}doi=10.1109{\%}2FCSCI.2015.133{\&}partnerID=40{\&}md5=6a3fc9e7dac95a6f25aacc425aa40af5},
year = {2016}
}
@article{ISI:000457666900036,
abstract = {We propose a novel method for measuring the nasal similarity among 3D faces. Firstly, we construct a representation for the nose shape, which is composed of a set of geodesic curves, each crosses the bridge of the nose. Next, using these geodesic curves, we formulate a similarity measure to compare among noses in the curve shape space. Under the Riemannian framework, the shape space is a quotient space for which the scaling, translation and rotation are removed. Since the nose similarity measure is based on the shape comparison, the proposed method has the following advantages: (1) the similarity measure is robust to facial expressions since the nose is not affected by facial expressions; (2) the geometric features of the nose shape match well with the human perception; (3) the similarity measure is independent of the mesh grid because the chosen nose curves are not sensitive to the triangular mesh model. We construct a nasal hierarchical structure for noses organization which is based on nose similarity measure results. In our experiments, we evaluate the performance of the proposed method and compare it with competing methods on three public face databases namely, FRGC2.0, Texas3D and BosphorusDB. The results show superiority of the proposed method in terms of both the speed and the accuracy when the nasal measurements are processed in the nasal hierarchical structure and the nasal samples with low sampling rate (5{\%}-25{\%} of original point cloud).},
author = {Lv, Chenlei and Wu, Zhongke and Wang, Xingce and Zhou, Mingquan and Toh, Kar Ann},
doi = {10.1016/j.patcog.2018.12.006},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lv et al. - 2019 - Nasal similarity measure of 3D faces based on curve shape space.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Nose similarity measure,Riemannian manifold,Shape space,revisao{\_}V1,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutui1},
pages = {458--469},
title = {{Nasal similarity measure of 3D faces based on curve shape space}},
volume = {88},
year = {2019}
}
@inproceedings{7443802,
abstract = {Face recognition has broad excitement in the latest trend in image processing. Face recognition refers to identify a specific individual in digital image by analyzing and comparing patterns. It has numerous benefits which attract every sector but there are some issues such as more time consumption and lesser accuracy which degrade the user services. To solve this problem we proposed a highly accurate and fast method to reduce the execution time. The proposed method uses average half face approach because overall system's accuracy is better in it rather than using the original full face image. The proposed method can be used to recognize both 2D and 3D images. It mainly includes the average half face creation, feature detection, full face recognition through average half face using distance metrics and finally checking system's accuracy along with time consumption. The proposed method is based on eye, nose and mouth detection.},
author = {Arora, Sourabh and Chawla, Shikha},
booktitle = {12th IEEE International Conference Electronics, Energy, Environment, Communication, Computer, Control: (E3-C3), INDICON 2015},
doi = {10.1109/INDICON.2015.7443802},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Chawla - 2015 - An intensified approach to face recognition through average half face.pdf:pdf},
isbn = {9781467373999},
issn = {2325-9418},
keywords = {Accuracy,Average half face,Distance metrics,Face recognition,Image processing,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
month = {dec},
pages = {1--6},
title = {{An intensified approach to face recognition through average half face}},
year = {2016}
}
@inproceedings{ISI:000364714000046,
abstract = {In this work, we take advantage of the superiority of Spectral Graph
Theory in classification application and propose a novel deep learning
framework for face analysis which is called Spectral Regression
Discriminant Analysis Network (SRDANet). Our SRDANet model shares the
same basic architecture of Convolutional Neural Network (CNN), which
comprises three basic components: convolutional filter layer, nonlinear
processing layer and feature pooling layer. While it is different from
traditional deep learning network that in our convolutional layer, we
extract the leading eigenvectors from patches in facial image which are
used as filter kernels instead of randomly initializing kernels and
update them by stochastic gradient descent (SGD). And the output of all
cascaded convolutional filter layers is used as the input of nonlinear
processing layer. In the following nonlinear processing layer, we use
hashing method for nonlinear processing. In feature pooling layer, the
block-based histograms are employed to pooling output features instead
of max-pooling technique. At last, the output of feature pooling layer
is considered as one final feature output of our model. Different from
the previous single-task research for face analysis, our proposed
approach demonstrates an excellent performance in face recognition and
expression recognition with 2D/3D facial images simultaneously.
Extensive experiments conducted on many different face analysis
databases demonstrate the efficiency of our proposed SRDANet model.
Databases such as Extended Yale B, PIE, ORL are used for 2D face
recognition, FRGC v2 is used for 3D face recognition and BU-3DFE is used
for 3D expression recognition.},
annote = {8th International Conference on Intelligent Robotics and Applications
(ICIRA), Portsmouth, ENGLAND, AUG 24-27, 2015},
author = {Tian, Lei and Fan, Chunxiao and Ming, Yue and Shi, Jiakun},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-22879-2_46},
editor = {{Liu, H and Kubota, N and Zhu, X and Dillmann, R and Zhou}, D},
file = {:home/tutu/artigos{\_}revisao/2e5dfc3fcd722a16a71b8b2b2afa5cfe-tian2015.pdf:pdf},
isbn = {978-3-319-22879-2; 978-3-319-22878-5},
issn = {16113349},
keywords = {Deep learning,Expression recognition,Face recognition,SRDA network,Spectral regression discriminant analysis,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
pages = {499--510},
series = {Lecture Notes in Artificial Intelligence},
title = {{SRDANet: An efficient deep learning algorithm for face analysis}},
volume = {9244},
year = {2015}
}
@inproceedings{Echeagaray-Patrón2015,
abstract = {Face recognition is an important task in pattern recognition and computer vision. In this work a method for 3D face recognition in the presence of facial expression and poses variations is proposed. The method uses 3D shape data without color or texture information. A new matching algorithm based on conformal mapping of original facial surfaces onto a Riemannian manifold followed by comparison of conformal and isometric invariants computed in the manifold is suggested. Experimental results are presented using common 3D face databases that contain significant amount of expression and pose variations.},
annote = {From Duplicate 1 (3D face recognition based on matching of facial surfaces - Echeagaray-Patr{\'{o}}n, Beatriz A.; Kober, Vitaly)

From Duplicate 2 (3D face recognition based on matching of facial surfaces - Echeagaray-Patr{\'{o}}n, B A; Kober, V)

cited By 14

From Duplicate 2 (3D face recognition based on matching of facial surfaces - Echeagaray-Patr{\'{o}}n, Beatriz A.; Kober, Vitaly)

cited By 14},
author = {Echeagaray-Patr{\'{o}}n, Beatriz A. and Kober, Vitaly},
booktitle = {Optics and Photonics for Information Processing IX},
doi = {10.1117/12.2186695},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Echeagaray-Patr{\'{o}}n, Kober - 2015 - 3D face recognition based on matching of facial surfaces(2).pdf:pdf},
keywords = {3d face recognition,3d facial shape analysis,conformal mapping,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
number = {September 2015},
pages = {95980V},
title = {{3D face recognition based on matching of facial surfaces}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951325808{\&}doi=10.1117{\%}2F12.2186695{\&}partnerID=40{\&}md5=cb5a757e08c6a66683fc0706f09faf95},
volume = {9598},
year = {2015}
}
@inproceedings{ISI:000380516600072,
abstract = {The classical curvatures of smooth surfaces (Gaussian, mean and
principal curvatures) have been widely used in 3D face recognition (FR).
However, facial surfaces resulting from 3D sensors are discrete meshes.
In this paper, we present a general framework and define three principal
curvatures on discrete surfaces for the purpose of 3D FR. These
principal curvatures are derived from the construction of asymptotic
cones associated to any Borel subset of the discrete surface. They
describe the local geometry of the underlying mesh. First two of them
correspond to the classical principal curvatures in the smooth case. We
isolate the third principal curvature that carries out meaningful
geometric shape information. The three principal curvatures in different
Borel subsets scales give multi-scale local facial surface descriptors.
We combine the proposed principal curvatures with the LNP-based facial
descriptor and SRC for recognition. The identification and verification
experiments demonstrate the practicability and accuracy of the third
principal curvature and the fusion of multi-scale Borel subset
descriptors on 3D face from FRGC v2.0.},
author = {Tang, Yinhang and Sun, Xiang and Huang, Di and Morvan, Jean Marie and Wang, Yunhong and Chen, Liming},
booktitle = {Proceedings of 2015 International Conference on Biometrics, ICB 2015},
doi = {10.1109/ICB.2015.7139111},
file = {:home/tutu/artigos{\_}revisao/07139111.pdf:pdf},
isbn = {9781479978243},
issn = {2376-4201},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {may},
organization = {SIEW-SNGIEM AWARD; Kasetsart Univ; CHANWANICH; ST Elect; SAFRAN Morpho},
pages = {466--472},
publisher = {IEEE},
series = {International Conference on Biometrics},
title = {{3D face recognition with asymptotic cones based principal curvatures}},
url = {http://ieeexplore.ieee.org/document/7139111/},
year = {2015}
}
@incollection{Ming:2015:RLS:2940229.2940265,
abstract = {In this paper, a robust 3D local SIFT feature is proposed for 3D face recognition. For preprocessing the original 3D face data, facial regional segmentation is first employed by fusing curvature characteristics and shape band mechanism. Then, we design a new local descriptor for the extracted regions, called 3D local Scale-Invariant Feature Transform (3D LSIFT). The key point detection based on 3D LSIFT can effectively reflect the geometric characteristic of 3D facial surface by encoding the gray and depth information captured by 3D face data. Then, 3D LSIFT descriptor extends to describe the discrimination on 3D faces. Experimental results based on the common international 3D face databases demonstrate the higher-qualified performance of our proposed algorithm with effectiveness, robustness, and universality.},
address = {New York, NY, USA},
annote = {From Duplicate 1 (Robust 3D local SIFT features for 3D face recognition - Ming, Yue; Jin, Yi)

From Duplicate 2 (Robust 3D Local SIFT Features for 3D Face Recognition - Ming, Yue; Jin, Yi)

26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)

From Duplicate 2 (Robust 3D Local SIFT Features for 3D Face Recognition - Ming, Yue; Jin, Yi)

26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Ming, Yue and Jin, Yi},
booktitle = {Proceedings of the 8th International Conference on Intelligent Robotics and Applications - Volume 9246},
doi = {10.1007/978-3-319-22873-0_31},
file = {:home/tutu/artigos{\_}revisao/a7043849b25b35f527aaefae13f9ee2d-ming2015.pdf:pdf},
isbn = {978-3-319-22872-3},
issn = {16113349},
keywords = {3D Local Scale-Invariant Feature Transform,3D face recognition,3D local Scale-Invariant feature transform,Depth information,Facial region segmentation,acm,estela,etapa1,id278,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {acm,estela,etapa1,id278,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
pages = {352--359},
publisher = {Springer-Verlag New York, Inc.},
series = {ICIRA 2015},
title = {{Robust 3D Local SIFT Features for 3D Face Recognition}},
url = {http://dx.doi.org/10.1007/978-3-319-22873-0{\_}31 http://link.springer.com/10.1007/978-3-319-22873-0{\_}31},
volume = {9246},
year = {2015}
}
@inproceedings{ISI:000402657200006,
abstract = {{\textcopyright} Springer International Publishing AG 2017. Using of 3D images for the identification was in a field of the interest of many researchers which developed a few methods offering good results. However, there are few techniques exploiting the 3D asymmetry amongst these methods. We propose fast algorithm for rough extraction face asymmetry that is used to 3D face recognition with hidden Markov models. This paper presents conception of fast method for determine 3D face asymmetry. The research results indicate that face recognition with 3D face asymmetry may be used in biometrics systems.},
annote = {From Duplicate 1 (Face recognition with 3D face asymmetry - Bobulski, Janusz)

8th International Conference on Image Processing and Communications
(IP{\&}C), UTP Univ Technol {\&} Sci, Inst Telecommunicat {\&} Comp Sci,
Bydgoszcz, POLAND, SEP 07-09, 2016

From Duplicate 2 (Face recognition with 3D face asymmetry - Bobulski, Janusz)

From Duplicate 1 (Face Recognition with 3D Face Asymmetry - Bobulski, Janusz)

8th International Conference on Image Processing and Communications
(IP{\&}C), UTP Univ Technol {\&} Sci, Inst Telecommunicat {\&} Comp Sci,
Bydgoszcz, POLAND, SEP 07-09, 2016},
author = {Bobulski, Janusz},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-47274-4_6},
editor = {Choras, RS},
file = {:home/tutu/artigos{\_}revisao/99346aa27359933f6bf1fc817356e3ed-bobulski2016.pdf:pdf},
isbn = {9783319472737},
issn = {21945357},
keywords = {Face asymmetry,Face recognition,Hidden Markov models,Identity verification,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
organization = {UTP Univ Technol {\&} Sci},
pages = {53--60},
series = {Advances in Intelligent Systems and Computing},
title = {{Face recognition with 3D face asymmetry}},
volume = {525},
year = {2017}
}
@article{Ratyal2015241,
abstract = {In this paper we present a novel pose and expression invariant approach for 3D face registration based on intrinsic coordinate system characterized by nose tip, horizontal nose plane and vertical symmetry plane of the face. It is observed that distance of nose tip from 3D scanner is reduced after pose correction which is presented as a quantifying heuristic for proposed registration scheme. In addition, motivated by the fact that a single classifier cannot be generally efficient against all face regions, a two tier ensemble classifier based 3D face recognition approach is presented which employs Principal Component Analysis (PCA) for feature extraction and Mahalanobis Cosine (MahCos) matching score for classification of facial regions with weighted Borda Count (WBC) based combination and a re-ranking stage. The performance of proposed approach is corroborated by extensive experiments performed on two databases: GavabDB and FRGC v2.0, confirming effectiveness of fusion strategies to improve performance. {\^{A}}{\textcopyright} 2015 Elsevier Ltd. All rights reserved.},
annote = {cited By 5
24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (Etapa 1)
24/04/2018 Exclu{\'{i}}do (Etapa 1)},
author = {Ratyal, N I and Taj, I A and Bajwa, U I and Sajid, M},
doi = {10.1016/j.compeleceng.2015.06.007},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ratyal et al. - 2015 - 3D face recognition based on pose and expression invariant alignment.pdf:pdf},
journal = {Computers and Electrical Engineering},
keywords = {acm,artur,etapa1,id183,isi,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,robson,scopus,tutui1},
mendeley-tags = {acm,artur,etapa1,id183,isi,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,robson,scopus,tutui1},
pages = {241--255},
title = {{3D face recognition based on pose and expression invariant alignment}},
volume = {46},
year = {2015}
}
@inproceedings{7988861,
abstract = {This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.},
author = {Angonese, Alberto Torres and {Ferreira Rosa}, Paulo Fernando},
booktitle = {ICMT 2017 - 6th International Conference on Military Technologies},
doi = {10.1109/MILTECHS.2017.7988861},
file = {:home/tutu/artigos{\_}revisao/07988861.pdf:pdf},
isbn = {9781538619889},
keywords = {CNN,Deep Learning,HOG,People Detection,Simultaneous Localization and Mapping (SLAM),revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,revisao{\_}scopus,robot operating system ROS,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {779--786},
title = {{Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform}},
year = {2017}
}
@article{Lei:2016:TWC:2875518.2875660,
abstract = {3D face recognition with the availability of only partial data (missing parts, occlusions and data corruptions) and single training sample is a highly challenging task. This paper presents an efficient 3D face recognition approach to address this challenge. We represent a facial scan with a set of local Keypoint-based Multiple Triangle Statistics (KMTS), which is robust to partial facial data, large facial expressions and pose variations. To address the single sample problem, we then propose a Two-Phase Weighted Collaborative Representation Classification (TPWCRC) framework. A class-based probability estimation is first calculated based on the extracted local descriptors as a prior knowledge. The resulting class-based probability estimation is then incorporated into the proposed classification framework as a locality constraint to further enhance its discriminating power. Experimental results on six challenging 3D facial datasets show that the proposed KMTS-TPWCRC framework achieves promising results for human face recognition with missing parts, occlusions, data corruptions, expressions and pose variations.},
address = {New York, NY, USA},
annote = {21/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
21/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Lei, Yinjie and Guo, Yulan and Hayat, Munawar and Bennamoun, Mohammed and Zhou, Xinzhi},
doi = {10.1016/j.patcog.2015.09.035},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lei et al. - 2016 - A Two-Phase Weighted Collaborative Representation for 3D partial face recognition with single sample.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {3D face recognition,3D representation,Partial facial data,Single sample problem,Sparse representation,acm,etapa1,gil,id111,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {acm,etapa1,gil,id111,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {apr},
number = {C},
pages = {218--237},
publisher = {Elsevier Science Inc.},
title = {{A Two-Phase Weighted Collaborative Representation for 3D partial face recognition with single sample}},
url = {http://dx.doi.org/10.1016/j.patcog.2015.09.035 http://linkinghub.elsevier.com/retrieve/pii/S0031320315003660},
volume = {52},
year = {2016}
}
@article{ISI:000446151100037,
abstract = {Most human expression variations cause a non-rigid deformation of face scans, which is a challenge today. In this article, we present a novel framework for 3D face recognition that uses a geometry and local shape descriptor in a matching process to overcome the distortions caused by expressions in faces. This algorithm consists of four major components. First, the 3D face model is presented at different scales. Second, isometric-invariant features on each scale are extracted. Third, the geometric information is obtained on the 3D surface in terms of radial and level facial curves. Fourth, the feature vectors on each scale are concatenated with their corresponding geometric information. We conducted a number of experiments using two well-known and challenging datasets, namely, the GavabDB and Bosphorus datasets, and superior recognition performance has been achieved. The new system displays an overall rank-1 identification rate of 98.9{\%} for all faces with neutral and non-neutral expressions on the GavabDB database. (C) 2017 Elsevier Ltd. All rights reserved.},
author = {Abbad, Abdelghafour and Abbad, Khalid and Tairi, Hamid},
doi = {10.1016/j.compeleceng.2017.08.017},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abbad, Abbad, Tairi - 2018 - 3D face recognition Multi-scale strategy based on geometric and local descriptors.pdf:pdf},
issn = {0045-7906},
journal = {COMPUTERS {\&} ELECTRICAL ENGINEERING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
pages = {525--537},
title = {{3D face recognition: Multi-scale strategy based on geometric and local descriptors}},
volume = {70},
year = {2018}
}
@article{ISI:000412265100009,
abstract = {Pose variations are still challenging problems in 3D face recognition because large pose variations will cause self-occlusion and result in missing data. In this paper, a new method for pose-invariant 3D face recognition is proposed to handle significant pose variations. For pose estimation and registration, a coarse-to-fine strategy is proposed to detect landmarks under large yaw variations. At the coarse search step, candidate landmarks are detected using HK curvature analysis and subdivided according to a facial geometrical structure-based classification strategy. At the fine search step, candidate landmarks are identified and labeled by comparing with a Facial Landmark Model. By using the half face matching, we perform the matching step with respect to frontal scans and side scans. Experiments carried out on the Bosphorus and UND/FRGC v2.0 databases show that our method has high accuracy and robustness to pose variations. (C) 2017 Elsevier B.V. All rights reserved.},
author = {Liang, Yan and Zhang, Yun and Zeng, Xian-Xian},
doi = {10.1016/j.image.2017.05.004},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang, Zhang, Zeng - 2017 - Pose-invariant 3D face recognition using half face.pdf:pdf},
issn = {0923-5965},
journal = {SIGNAL PROCESSING-IMAGE COMMUNICATION},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
pages = {84--90},
title = {{Pose-invariant 3D face recognition using half face}},
volume = {57},
year = {2017}
}
@article{ISI:000368591300013,
abstract = {In last years, the emergence of 3D shape in face recognition is due to its robustness to pose and illumination changes. These attractive benefits are not all the challenges to achieve satisfactory recognition rate. Other challenges such as facial expressions and computing time of matching algorithms remain to be explored. In this context, we propose our 3D face recognition approach using 3D wavelet networks. Our approach contains two stages: learning stage and recognition stage. For the training we propose a novel algorithm based on 3D fast wavelet transform. From 3D coordinates of the face (x,y,z), we proceed to voxelization to get a 3D volume which will be decomposed by 3D fast wavelet transform and modeled after that with a wavelet network, then their associated weights are considered as vector features to represent each training face. For the recognition stage, an unknown identity face is projected on all the training WN to obtain a new vector features after every projection. A similarity score is computed between the old and the obtained vector features. To show the efficiency of our approach, experimental results were performed on all the FRGC v.2 benchmark.},
annote = {From Duplicate 1 (3D fast wavelet network model-assisted 3D face recognition - Said, Salwa; Jemai, Olfa; Zaied, Mourad; Ben Amar, Chokri)

- galera curte usar gallery/probe ao inves de train/test;
- identification me parece melhor que recognition (fica mais atrelado a natureza biometrica do problema);
- nao necessita de registro;
-

From Duplicate 2 (3D fast wavelet network model-assisted 3D face recognition - Said, Salwa; Jemai, Olfa; Zaied, Mourad; Ben Amar, Chokri)

From Duplicate 1 (3D fast wavelet network model-assisted 3D face recognition - Said, Salwa; Jemai, Olfa; Zaied, Mourad; Ben Amar, Chokri)

8th International Conference on Machine Vision (ICMV), Barcelona, SPAIN,
NOV 19-21, 2015

From Duplicate 2 (3D fast wavelet network model-assisted 3D face recognition - Said, Salwa; Jemai, Olfa; Zaied, Mourad; Ben Amar, Chokri)

From Duplicate 1 (3D Fast Wavelet Network Model-Assisted 3D Face Recognition - Said, Salwa; Jemai, Olfa; Zaied, Mourad; Ben Amar, Chokri)

8th International Conference on Machine Vision (ICMV), Barcelona, SPAIN,
NOV 19-21, 2015},
author = {Said, Salwa and Jemai, Olfa and Zaied, Mourad and {Ben Amar}, Chokri},
doi = {10.1117/12.2228368},
editor = {{Verikas, A and Radeva, P and Nikolaev}, D},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Said et al. - 2015 - 3D fast wavelet network model-assisted 3D face recognition.pdf:pdf},
isbn = {978-1-5106-0116-1},
issn = {0277-786X},
journal = {Eighth International Conference on Machine Vision (ICMV 2015)},
keywords = {3d face recognition,fast wavelet transform,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1,wavelet network},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
number = {December 2015},
pages = {98750E},
series = {Proceedings of SPIE},
title = {{3D fast wavelet network model-assisted 3D face recognition}},
volume = {9875},
year = {2015}
}
@inproceedings{8389776,
abstract = {Face Recognition is one of the biometric technique to vestige the given faces. We present a 3D face recognition method using Hadoop to recognize 3D faces under varying expressions, lighting and different poses to overcome the challenges of the 2D face recognition. In this paper, a threshold facial region of an image is detected and preprocessing is done based through the image excellence. If the selected face is frontal face with good lighting, extract the prerequisite features and do the necessary comparison steps to recognize the faces. In case, if the selected face is in bad lighting, then perform histogram equalization and normalization to increase the contrast. Different Poses and expressions are the very challenging zones which require surplus pre-processing to improve the performance of the face recognition system. Hence an enhanced normalization method called 3D Morphable Model are used as a pre- processing technique to create a frontal view from a non-frontal view and also merge images with different views in to a single frontal view. Next to diminish the number of features used for recognition process; we emulate the linear discriminant analysis method for further classification. Eventually, we used an open-source Hadoop Image Processing Interface (HIPI) to act as an interface for MapReduce technology for recognition. {\textcopyright} 2017 IEEE.},
annote = {From Duplicate 1 (3D face recognition using Hadoop - Geetha, G.; Safa, M.; Fancy, C.; Chittal, K.)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0

From Duplicate 3 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0

From Duplicate 2 (3D face recognition using Hadoop - Geetha, G.; Safa, M.; Fancy, C.; Chittal, K.)

From Duplicate 2 (3D face recognition using Hadoop - Geetha, G.; Safa, M.; Fancy, C.; Chittal, K.)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0

From Duplicate 3 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0

From Duplicate 3 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0

From Duplicate 3 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0

From Duplicate 2 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

From Duplicate 2 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0

From Duplicate 3 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0},
author = {Geetha, G. and Safa, M. and Fancy, C. and Chittal, K.},
booktitle = {2017 International Conference on Energy, Communication, Data Analytics and Soft Computing, ICECDS 2017},
doi = {10.1109/ICECDS.2017.8389776},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geetha et al. - 2017 - 3D face recognition using Hadoop.pdf:pdf},
isbn = {9781538618868},
keywords = {Hadoop,Image Processing,Linear Discriminant analysis,Map Reduce,biometrics (access control),face recognition,featu,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,tutui1},
month = {aug},
pages = {1882--1885},
publisher = {IEEE},
title = {{3D face recognition using Hadoop}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050073609{\&}doi=10.1109{\%}2FICECDS.2017.8389776{\&}partnerID=40{\&}md5=426f7e7f8b23f4019df744e279422d63 https://ieeexplore.ieee.org/document/8389776/},
year = {2017}
}
@inproceedings{ISI:000446968900004,
abstract = {With the growth of face recognition, the spoofing mask attacks attract more attention in biometrics research area. In recent years, the countermeasures based on the texture and depth image against spoofing mask attacks have been reported, but the research based on 3D meshed sample has not been studied yet. In this paper, we propose to apply 3D shape analysis based on principal curvature measures to describe the meshed facial surface. Meanwhile, a verification protocol based on this feature descriptor is designed to verify person identity and to evaluate the anti-spoofing performance on Morpho database. Furthermore, for simulating a real-life testing scenario, FRGCv2 database is enrolled as an extension of face scans to augment the ratio of genuine face samples to fraud mask samples. The experimental results show that our system can guarantee a high verification rate for genuine faces and the satisfactory anti-spoofing performance against spoofing mask attacks in parallel.},
annote = {From Duplicate 1 (Shape analysis based anti-spoofing 3D face recognition with mask attacks - Tang, Yinhang; Chen, Liming)

6th International Workshop on Representations, Analysis and Recognition
of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
OCT 27-29, 2016

From Duplicate 2 (Shape Analysis Based Anti-spoofing 3D Face Recognition with Mask Attacks - Tang, Yinhang; Chen, Liming; B, Yinhang Tang; Chen, Liming)

From Duplicate 2 (Shape Analysis Based Anti-spoofing 3D Face Recognition with Mask Attacks - Tang, Yinhang; Chen, Liming)

6th International Workshop on Representations, Analysis and Recognition
of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
OCT 27-29, 2016},
author = {Tang, Yinhang and Chen, Liming and B, Yinhang Tang and Chen, Liming},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-319-60654-5_4},
editor = {{BenAmor, B and Chaieb, F and Ghorbel}, F},
file = {:home/tutu/artigos{\_}revisao/0ffc4a681b88886e4d0518882f592e51-tang2017.pdf:pdf},
isbn = {9783319606538},
issn = {18650929},
keywords = {,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux11},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux11},
pages = {41--55},
series = {Communications in Computer and Information Science},
title = {{Shape analysis based anti-spoofing 3D face recognition with mask attacks}},
volume = {684},
year = {2017}
}
@inproceedings{ISI:000426973200029,
abstract = {This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by explor- ing location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normaliza- tion. The 3D coordinates of each normalized 3D facial sur- face are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep represen- tations of facial surface normals, i.e., deep normal pattern- s. Considering the importance of different facial locations, we propose a location sensitive sparse representation clas- sifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, sim- ple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one s- cores of 98.01{\%}, 97.60{\%}, and 96.13{\%} on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sam- ple per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition.},
annote = {IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
OCT 01-04, 2017},
author = {Li, Huibin and Sun, Jian and Chen, Liming},
booktitle = {IEEE International Joint Conference on Biometrics, IJCB 2017},
doi = {10.1109/BTAS.2017.8272703},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Sun, Chen - 2017 - Location-Sensitive Sparse Representation of Deep Normal Patterns for Expression-robust 3D Face Recognition.pdf:pdf},
isbn = {9781538611241},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
organization = {IEEE},
pages = {234--242},
title = {{Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition}},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{ISI:000380516600070,
abstract = {In the past decade, the interest in using 3D data for biometric person authentication has increased significantly, propelled by the availability of affordable 3D sensors. The adoption of 3D features has been especially successful in face recognition applications, leading to several commercial 3D face recognition products. In other biometric modalities such as hand recognition, several studies have shown the potential advantage of using 3D geometric information, however, no commercial-grade systems are currently available. In this paper, we present a contactless 3D hand recognition system based on the novel Intel RealSense camera, the first mass-produced embeddable 3D sensor. The small form factor and low cost make this sensor especially appealing for commercial biometric applications, however, they come at the price of lower resolution compared to more expensive 3D scanners used in previous research. We analyze the robustness of several existing 2D and 3D features that can be extracted from the images captured by the RealSense camera and study the use of metric learning for their fusion.},
annote = {International Conference on Biometrics (ICB), Phuket, THAILAND, MAY
19-22, 2015},
author = {Svoboda, Jan and Bronstein, Michael M and Drahansky, Martin},
booktitle = {2015 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)},
isbn = {978-1-4799-7824-3},
issn = {2376-4201},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {SIEW-SNGIEM AWARD; Kasetsart Univ; CHANWANICH; ST Elect; SAFRAN Morpho},
pages = {452--457},
series = {International Conference on Biometrics},
title = {{Contactless biometric hand geometry recognition using a low-cost 3D camera}},
year = {2015}
}
@inproceedings{Nozawa:2015:FRS:2787626.2792634,
abstract = {A reconstruction of a human face shape from a single image is an important theme for criminal investigation such as recognition of suspected people from surveillance cameras with only a few frames. It is, however, still difficult to recover a face shape from a non-frontal face image. Method using shading cues on a face depends on the lighting circumstance and cannot be adapted to images in which shadows occurs, for example [Kemelmacher et al. 2011]. On the other hand, [Blanz et al. 2004] reconstructed a shape by 3D Morphable Model (3DMM) only with facial feature points. This method, however, requires the pose-wise correspondences of vertices in the model to feature points of input image because a face contour cannot be seen when the facial direction is not the front. In this paper, we propose a method which can reconstruct a facial shape from a non-frontal face image only with a single general correspondence table. Our method searches for the correspondences of points on a facial contour in the iterative reconstruction process, and makes the reconstruction simple and stable.},
address = {New York, New York, USA},
author = {Nozawa, Naoki and Kuwahara, Daiki and Morishima, Shigeo},
booktitle = {ACM SIGGRAPH 2015 Posters on - SIGGRAPH '15},
doi = {10.1145/2787626.2792634},
isbn = {9781450336321},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
pages = {1--1},
publisher = {ACM Press},
series = {SIGGRAPH '15},
title = {{3D face reconstruction from a single non-frontal face image}},
url = {http://doi.acm.org/10.1145/2787626.2792634 http://dl.acm.org/citation.cfm?doid=2787626.2792634},
year = {2015}
}
@inproceedings{7301378,
abstract = {Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89{\%} with less than 100 training examples, which corresponds to 10{\%} of the pre-selected data pool.},
author = {Polewski, P and Yao, W and Heurich, M and Krzystek, P and Stilla, U},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2015.7301378},
issn = {2160-7516},
keywords = {environmental factors,feature extraction,forestry,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {10--18},
title = {{Active learning approach to detecting standing dead trees from ALS point clouds combined with aerial infrared imagery}},
year = {2015}
}
@inproceedings{8633125,
abstract = {The babies infected from Zika before they are born are at risk for problems with brain development and microcephaly. 3D head images of 43 Zika cases and 43 controls were collected aiming to extract shape characteristics for diagnosis purposes. Principal component analysis (PCA) has been applied on the vaults and faces of the collected 3D images and the scores on the second principal components of the vaults and faces showed significant differences between the control and Zika groups. The shape variations from -2$\sigma$ to 2$\sigma$ illustrated the typical characteristics of microcephaly of the Zika babies. Canonical correlation analysis (CCA) showed a significant correlation in the first CCA variates of face and vault which indicated the potential of 3D facial imaging for Zika surveillance. Further head circumferences and distances from ear to ear were measured from the 3D images and preliminary results showed the adding ear to ear distances for classifying control and Zika children strengthened the abilities of tested classification models.},
author = {Ju, X and {Garcia J{\'{u}}nior}, I R and {De Freitas Silva}, L and Mossey, P and Al-Rudainy, D and Ayoub, A and {De Mattos}, A M},
booktitle = {2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
doi = {10.1109/CISP-BMEI.2018.8633125},
keywords = {diseases,image classification,medical image proces,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {oct},
pages = {1--4},
title = {{3D Head Shape Analysis of Suspected Zika Infected Infants}},
year = {2018}
}
@article{ISI:000445394200001,
abstract = {Aim of study: In this study we applied 3D point clouds generated by images obtained from an Unmanned Aerial Vehicle (UAV) to evaluate the uniformity of young forest stands. Area of study: Two commercial forest stands were selected, with two plots each. The forest species studied were Eucalyptus spp. and Pinus taeda L. and the trees had an age of 1.5 years. Material and methods: The individual trees were detected based on watershed segmentation and local maxima, using the spectral values stored in the point cloud. After the tree detection, the heights were calculated using two approaches, in the first one using the Digital Surface Model (DSM) and a Digital Terrain Model, and in the second using only the DSM. We used the UAV-derived heights to estimate an uniformity index. Main results: The trees were detected with a maximum 6{\%} of error. However, the height was underestimated in all cases, in an average of 1 and 0.7 m for Pinus and Eucalyptus stands. We proposed to use the models built herein to estimate tree height, but the regression models did not explain the variably within the data satisfactorily. Therefore, the uniformity index calculated using the direct UAV-height values presented results close to the field inventory, reaching better results when using the second height approach (error ranging 2.8-7.8{\%}). Research highlights: The uniformity index using the UAV-derived height from the proposed methods was close to the values obtained in field. We noted the potential for using UAV imagery in forest monitoring.},
author = {Hentz, Angela M K and Silva, Carlos A and {Dalla Corte}, Ana P and Netto, Sylvio P and Stager, Michael P and Klauberg, Carine},
doi = {10.5424/fs/2018272-11713},
issn = {2171-5068},
journal = {FOREST SYSTEMS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {2},
title = {{Estimating forest uniformity in Eucalyptus spp. and Pinus taeda L. stands using field measurements and structure from motion point clouds generated from unmanned aerial vehicle (UAV) data collection}},
volume = {27},
year = {2018}
}
@article{ISI:000455069200012,
abstract = {Objective: To study the clinical anatomy of the epitympanum, the attic, and its medial wall, to try to discover a new clinical operation-related anatomical landmark, and to investigate the adjacent anatomical relationship with this landmark. Materials and Methods: Eight donor temporal bone specimens were dissected endoscopically. For 29 healthy persons (17 males and 12 females), CT images of the temporal bone (57 ears) were taken, 3-dimensional (3-D) reconstruction and multidimensional plane reconstruction were performed, and identification and assessment of 3-D spatial relationships between any 2 of these complex structures were done. Results: 3-D images of the temporal bone structures including the facial nerve, the cochlea, the semicircular canal, and the brain plate were reconstructed and shown in detail. We discovered a new clinical surgery-related anatomical landmark (the ``cog{\{}''{\}} tangent and the trailing edge of the cog). Based on the tangent and the trailing edge of the cog, we quantified the anatomical relationship between it and its neighboring important structures. Conclusion: Based on endoscopic anatomy and the temporal bone spiral CT 3-D structure reconstruction of the epitympanum, the attic, and the adjacent structures, we found an extension of the clinical significance the cog. Quantification of the adjacent anatomical relationship of this landmark is very important for otology microsurgical operation. (c) 2018 S. Karger AG, Basel},
author = {Wang, Kaishi and Jiang, Yi and Zhang, Zhifei and Lu, Yongtian and Ni, Yusu},
doi = {10.1159/000493012},
issn = {0301-1569},
journal = {ORL-JOURNAL FOR OTO-RHINO-LARYNGOLOGY HEAD AND NECK SURGERY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {5-6},
pages = {317--325},
title = {{Extension of the Clinical Significance of the ``Cog{\{}''{\}}}},
volume = {80},
year = {2018}
}
@article{NourbakhshKaashki201866,
abstract = {This research proposes a method for 3D face recognition in various conditions using 3D constrained local model (CLM-Z). In this method, a combination of 2D images (RGBs) and depth images (Ds) captured by Kinect has been used. After detecting the face and smoothing the depth image, CLM-Z model has been used to model and detect the important points of the face. These points are described using Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), and 3D Local Binary Patterns (3DLBP). Finally, each face is recognized by a Support Vector Machine (SVM). The challenging situations are changes of lighting, facial expression and head pose. The results on CurtinFaces and IIIT-D datasets demonstrate that the proposed method outperformed state-of-the-art methods under illumination, expression and pitch pose conditions and comparable results were obtained in other cases. Additionally, our proposed method is robust even when the training data has not been carefully collected. {\textcopyright} 2018 Elsevier Inc.},
annote = {cited By 0},
author = {{Nourbakhsh Kaashki}, N and Safabakhsh, R},
doi = {10.1016/j.jvcir.2018.02.003},
journal = {Journal of Visual Communication and Image Representation},
keywords = {revisao{\_}V1,revisao{\_}scopus,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutux5},
pages = {66--85},
title = {{RGB-D face recognition under various conditions via 3D constrained local model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042322207{\&}doi=10.1016{\%}2Fj.jvcir.2018.02.003{\&}partnerID=40{\&}md5=2b91afeff8a19c987036cf8a30b610dc},
volume = {52},
year = {2018}
}
@article{ISI:000460829200061,
abstract = {Mobile Laser Scanning (MLS) is a versatile remote sensing technology based on Light Detection and Ranging (lidar) technology that has been utilized for a wide range of applications. Several previous reviews focused on applications or characteristics of these systems exist in the literature, however, reviews of the many innovative data processing strategies described in the literature have not been conducted in sufficient depth. To this end, we review and summarize the state of the art for MLS data processing approaches, including feature extraction, segmentation, object recognition, and classification. In this review, we first discuss the impact of the scene type to the development of an MLS data processing method. Then, where appropriate, we describe relevant generalized algorithms for feature extraction and segmentation that are applicable to and implemented in many processing approaches. The methods for object recognition and point cloud classification are further reviewed including both the general concepts as well as technical details. In addition, available benchmark datasets for object recognition and classification are summarized. Further, the current limitations and challenges that a significant portion of point cloud processing techniques face are discussed. This review concludes with our future outlook of the trends and opportunities of MLS data processing algorithms and applications.},
author = {Che, Erzhuo and Jung, Jaehoon and Olsen, Michael J},
doi = {10.3390/s19040810},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {4},
title = {{Object Recognition, Segmentation, and Classification of Mobile Laser Scanning Point Clouds: A State of the Art Review}},
volume = {19},
year = {2019}
}
@article{ISI:000433517100003,
abstract = {We propose a deformation-based representation for analyzing expressions fromthree-dimensional (3D) faces. A point cloud of a 3D face is decomposed into an ordered deformable set of curves that start from a fixed point. Subsequently, a mapping function is defined to identify the set of curves with an element of a high-dimensional matrix Lie group, specifically the direct product of SE(3). Representing 3D faces as an element of a high-dimensional Lie group has two main advantages. First, using the group structure, facial expressions can be decoupled from a neutral face. Second, an underlying non-linear facial expression manifold can be captured with the Lie group and mapped to a linear space, Lie algebra of the group. This opens up the possibility of classifying facial expressions with linear models without compromising the underlying manifold. Alternatively, linear combinations of linearised facial expressions can be mapped back from the Lie algebra to the Lie group. The approach is tested on the Binghamton University 3D Facial Expression (BU-3DFE) and the Bosphorus datasets. The results show that the proposed approach performed comparably, on the BU-3DFE dataset, without using features or extensive landmark points.},
author = {Demisse, Girum G and Aouada, Djamila and Ottersten, Bjorn},
doi = {10.1145/3176649},
issn = {1551-6857},
journal = {ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS},
keywords = {revisao{\_}V1,revisao{\_}acm,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}webofscience,tutux7},
number = {1, S},
title = {{Deformation-Based 3D Facial Expression Representation}},
volume = {14},
year = {2018}
}
@inproceedings{7517246,
abstract = {This paper presents a virtual try-on system to correctly visualize 3D objects (e.g., glasses) in the face of a given user. By capturing the image and depth information of a user through a low-cost RGB-D camera, we apply a face tracking technique to detect specific landmarks in the facial image. These landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3D facial morphable model that fits as good as possible to the user's head and face. At the end, we deform the chosen 3D objects from its rest shape to a deformed shape matching the specific facial shape of the user. The last step projects and renders the 3D object into the original image, with enhanced precision and in proper scale, showing the selected object in the user's face. We validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. Our results demonstrate that, by fitting a facial model to the user's face, the rendered virtual 3D objects look more realistic.},
author = {Azevedo, P and {Dos Santos}, T O and {De Aguiar}, E},
booktitle = {2016 XVIII Symposium on Virtual and Augmented Reality (SVR)},
doi = {10.1109/SVR.2016.12},
keywords = {augmented reality,face recognition,i,image capture,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {1--9},
title = {{An Augmented Reality Virtual Glasses Try-On System}},
year = {2016}
}
@article{ISI:000425828200076,
abstract = {Terrestrial laser scanning (TLS) has been shown to enable an efficient, precise, and non-destructive inventory of vegetation structure at ranges up to hundreds of meters. We developed a method that leverages TLS collections with machine learning techniques to model and map canopy cover and biomass of several classes of short-stature vegetation across large plots. We collected high-definition TLS scans of 26 1-ha plots in desert grasslands and big sagebrush shrublands in southwest Idaho, USA. We used the Random Forests machine learning algorithm to develop decision tree models predicting the biomass and canopy cover of several vegetation classes from statistical descriptors of the aboveground heights of TLS points. Manual measurements of vegetation characteristics collected within each plot served as training and validation data. Models based on five or fewer TLS descriptors of vegetation heights were developed to predict the canopy cover fraction of shrubs (R-2 = 0.77, RMSE = 7{\%}), annual grasses (R-2 = 0.70, RMSE = 21{\%}), perennial grasses (R-2 = 0.36, RMSE = 12{\%}), forbs (R-2 = 0.52, RMSE = 6{\%}), bare earth or litter (R-2 = 0.49, RMSE = 19{\%}), and the biomass of shrubs (R-2 = 0.71, RMSE = 175 g) and herbaceous vegetation (R-2 = 0.61, RMSE = 99 g) (all values reported are out-of-bag). Our models explained much of the variability between predictions and manual measurements, and yet we expect that future applications could produce even better results by reducing some of the methodological sources of error that we encountered. Our work demonstrates how TLS can be used efficiently to extend manual measurement of vegetation characteristics from small to large plots in grasslands and shrublands, with potential application to other similarly structured ecosystems. Our method shows that vegetation structural characteristics can be modeled without classifying and delineating individual plants, a challenging and time-consuming step common in previous methods applying TLS to vegetation inventory. Improving application of TLS to studies of shrub steppe ecosystems will serve immediate management needs by enhancing vegetation inventories, environmental modeling studies, and the ability to train broader datasets collected from air and space.},
author = {Anderson, Kyle E and Glenn, Nancy F and Spaete, Lucas P and Shinneman, Douglas J and Pilliod, David S and Arkle, Robert S and McIlroy, Susan K and Derryberry, DeWayne R},
doi = {10.1016/j.ecolind.2017.09.034},
issn = {1470-160X},
journal = {ECOLOGICAL INDICATORS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jan},
pages = {793--802},
title = {{Estimating vegetation biomass and cover across large plots in shrub and grass dominated drylands using terrestrial lidar and machine learning}},
volume = {84},
year = {2018}
}
@article{ISI:000402350200003,
abstract = {An essential input to the musculoskeletal (MS) trunk models that estimate muscle and spine forces is kinematics of the thorax, pelvis, and lumbar vertebrae. While thorax and pelvis kinematics are usually measured via skin motion capture devices (with inherent errors on the proper identification of the underlying bony landmarks and the relative skin-sensor-bone movements), those of the intervening lumbar vertebrae are commonly approximated at fixed proportions based on the thorax-pelvis kinematics. This study proposes an image-based kinematics measurement approach to drive subject-specific (musculature, geometry, mass, and center of masses) MS models. Kinematics of the thorax, pelvis, and individual lumbar vertebrae as well as disc inclinations, gravity loading, and musculature were all measured via different imaging techniques. The model estimated muscle and lumbar forces in various upright and flexed postures in which kinematics were obtained using upright fluoroscopy via 2D/3D image registration. Predictions of this novel image-kinematics-driven model (Img-KD) were compared with those of the traditional kinematics-driven (T-KD) model in which individual lumbar vertebral rotations were assumed based on thorax-pelvis orientations. Results indicated that while differences between Img-KD and T-KD models remained small for the force in the global muscles (attached to the thoracic cage) ({\textless}15{\%}), L4-S1 compression ({\textless}15{\%}), and shear ({\textless}20{\%}) forces in average for all the simulated tasks, they were relatively larger for the force in the local muscles (attached to the lumbar vertebrae). Assuming that the skin-based measurements of thorax and pelvis kinematics are accurate enough, the T-KD model predictions of spinal forces remain reliable. (C) 2017 Elsevier Ltd. All rights reserved,},
author = {Eskandari, A H and Arjmand, N and Shirazi-Adl, A and Farahmand, F},
doi = {10.1016/j.jbiomech.2017.03.011},
issn = {0021-9290},
journal = {JOURNAL OF BIOMECHANICS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {may},
pages = {18--26},
title = {{Subject-specific 2D/3D image registration and kinematics-driven musculoskeletal model of the spine}},
volume = {57},
year = {2017}
}
@article{ISI:000355288200010,
abstract = {The aim of this study was to compare facial 3D analysis to DNA testing in twin zygosity determinations. Facial 3D images of 106 pairs of young adult Lithuanian twins were taken with a stereophotogrammetric device (3dMD, Atlanta, Georgia) and zygosity was determined according to similarity of facial form. Statistical pattern recognition methodology was used for classification. The results showed that in 75{\%} to 90{\%} of the cases, zygosity determinations were similar to DNA-based results. There were 81 different classification scenarios, including 3 groups, 3 features, 3 different scaling methods, and 3 threshold levels. It appeared that coincidence with 0.5 mm tolerance is the most suitable feature for classification. Also, leaving out scaling improves results in most cases. Scaling was expected to equalize the magnitude of differences and therefore lead to better recognition performance. Still, better classification features and a more effective scaling method or classification in different facial areas could further improve the results. In most of the cases, male pair zygosity recognition was at a higher level compared with females. Erroneously classified twin pairs appear to be obvious outliers in the sample. In particular, faces of young dizygotic (DZ) twins may be so similar that it is very hard to define a feature that would help classify the pair as DZ. Correspondingly, monozygotic (MZ) twins may have faces with quite different shapes. Such anomalous twin pairs are interesting exceptions, but they form a considerable portion in both zygosity groups.},
author = {Vuollo, Ville and Sidlauskas, Mantas and Sidlauskas, Antanas and Harila, Virpi and Salomskiene, Loreta and Zhurov, Alexei and Holmstrom, Lasse and Pirttiniemi, Pertti and Heikkinen, Tuomo},
doi = {10.1017/thg.2015.16},
issn = {1832-4274},
journal = {TWIN RESEARCH AND HUMAN GENETICS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jun},
number = {3},
pages = {306--313},
title = {{Comparing Facial 3D Analysis With DNA Testing to Determine Zygosities of Twins}},
volume = {18},
year = {2015}
}
@inproceedings{Balint-Benczedi:2015:KAR:2772879.2773515,
address = {Richland, SC},
author = {B{\'{a}}lint-Bencz{\'{e}}di, Ferenc and Wiedemeyer, Thiemo and Tenorth, Moritz and Be$\backslash$ssler, Daniel and Beetz, Michael},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
isbn = {978-1-4503-3413-6},
keywords = {knowledge based reasoning,personal robotics,revisao{\_}V2,revisao{\_}acm,robot perception,tutux1,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux1,tutux9},
pages = {1941--1942},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '15},
title = {{A Knowledge-Based Approach to Robotic Perception Using Unstructured Information Management}},
url = {http://dl.acm.org/citation.cfm?id=2772879.2773515},
year = {2015}
}
@inproceedings{7368276,
abstract = {This paper presents our methodology for Landmark Point detection to improve 3D face recognition in a presence of variant facial expression. The objective was to develop an automatic process for distinguishing and segmenting to be embedded in a 3D face recognition system using only 3D Point Distribution Model (PDM) as input. The approach used hydride method to extract this features from the surface curvature information. Landmark Localization is done on the segmented face via finding the change that decreases the deviation of the model from the mean profile. Face registering is achieved using previous anthropometric information and the localized landmarks. The results confirm that the method used is accurate and robust for the proposed application.},
annote = {29/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
29/04 Exclu{\'{i}}do (etapa 1)},
author = {Boukamcha, H and Elhallek, M and Atri, M and Smach, F},
booktitle = {2015 World Symposium on Computer Networks and Information Security (WSCNIS)},
doi = {10.1109/WSCNIS.2015.7368276},
keywords = {3D face landmar,computer graphics,etapa1,face recognition,id394,ieeexplore,lerdepois,poly,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {etapa1,id394,ieeexplore,lerdepois,poly,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {1--6},
title = {{3D face landmark auto detection}},
year = {2015}
}
@inproceedings{ISI:000390841200018,
abstract = {In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future.},
annote = {9th International Conference on Biometrics (ICB), Halmstad Univ,
Halmstad, SWEDEN, JUN 13-16, 2016},
author = {Zhang, Jinjin and Huang, Di and Wang, Yunhong and Sun, Jia},
booktitle = {2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)},
editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun}, J},
isbn = {978-1-5090-1869-7},
issn = {2376-4201},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB; Safran Ident {\&} Secur; EU Horizon 2020 Project IDENT; Speed Ident AB; Cognitec},
series = {International Conference on Biometrics},
title = {{Lock3DFace: A Large-Scale Database of Low-Cost Kinect 3D Faces}},
year = {2016}
}
@inproceedings{7869954,
abstract = {This paper proposes a novel 3D Constrained Local Models (CLM) approach applied for the detection of facial landmarks in 3D images. This approach capitalizes on the properties of Independent Component Analysis (ICA) to define appropriate priors of a face Point Distribution Model. Tailored to the mesh manifold modality, this approach address the limitations of the depth images which require pose normalization and suffer from the loss of the shape information caused by 2D projection. We validate this framework through a series of experiments conducted with the public Bosporus database, whereby it demonstrates a competitive performance compared to other state of the art methods.},
author = {Rai, M C E and Tortorici, C and Al-Muhairi, H and Werghi, N and Linguraru, M},
booktitle = {2016 IEEE 59th International Midwest Symposium on Circuits and Systems (MWSCAS)},
doi = {10.1109/MWSCAS.2016.7869954},
issn = {1558-3899},
keywords = {face recognition,independent component analysis,lerdepois,me,revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
month = {oct},
pages = {1--4},
title = {{Facial landmarks detection using 3D constrained local model on mesh manifold}},
year = {2016}
}
@article{ISI:000356741800007,
abstract = {We propose a new methodology for large-scale urban 3D scene analysis in terms of automatically assigning 3D points the respective semantic labels. The methodology focuses on simplicity and reproducibility of the involved components as well as performance in terms of accuracy and computational efficiency. Exploiting a variety of low-level 2D and 3D geometric features, we further improve their distinctiveness by involving individual neighborhoods of optimal size. Due to the use of individual neighborhoods, the methodology is not tailored to a specific dataset, but in principle designed to process point clouds with a few millions of 3D points. Consequently, an extension has to be introduced for analyzing huge 3D point clouds with possibly billions of points for a whole city. For this purpose, we propose an extension which is based on an appropriate partitioning of the scene and thus allows a successive processing in a reasonable time without affecting the quality of the classification results. We demonstrate the performance of our methodology on two labeled benchmark datasets with respect to robustness, efficiency, and scalability. (C) 2015 Elsevier Ltd. All rights reserved.},
author = {Weinmann, M and Urban, S and Hinz, S and Jutzi, B and Mallet, C},
doi = {10.1016/j.cag.2015.01.006},
issn = {0097-8493},
journal = {COMPUTERS {\&} GRAPHICS-UK},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jun},
pages = {47--57},
title = {{Distinctive 2D and 3D features for automated large-scale scene analysis in urban areas}},
volume = {49},
year = {2015}
}
@article{ISI:000352441700012,
abstract = {Three-dimensional surface technologies particularly close range photogrammetry and optical surface scanning have recently advanced into affordable, flexible and accurate techniques. Forensic postmortem investigation as performed on a daily basis, however, has not yet fully benefited from their potentials. In the present paper, we tested two approaches to 3D external body documentation - digital camera-based photogrammetry combined with commercial Agisoft PhotoScan (R) software and stereophotogrammetry-based Vectra H1 (R), a portable handheld surface scanner. In order to conduct the study three human subjects were selected, a living person, a 25-year-old female, and two forensic cases admitted for postmortem examination at the Department of Forensic Medicine, Hradec Kralove, Czech Republic (both 63-year-old males), one dead to traumatic, self-inflicted, injuries (suicide by hanging), the other diagnosed with the heart failure. All three cases were photographed in 3608 manner with a Nikon 7000 digital camera and simultaneously documented with the handheld scanner. In addition to having recorded the pre-autopsy phase of the forensic cases, both techniques were employed in various stages of autopsy. The sets of collected digital images (approximately 100 per case) were further processed to generate point clouds and 3D meshes. Final 3D models (a pair per individual) were counted for numbers of points and polygons, then assessed visually and compared quantitatively using ICP alignment algorithm and a cloud point comparison technique based on closest point to point distances. Both techniques were proven to be easy to handle and equally laborious. While collecting the images at autopsy took around 20 min, the post-processing was much more time-demanding and required up to 10 h of computation time. Moreover, for the full-body scanning the post-processing of the handheld scanner required rather time-consuming manual image alignment. In all instances the applied approaches produced high-resolution photorealistic, real sized or easy to calibrate 3D surface models. Both methods equally failed when the scanned body surface was covered with body hair or reflective moist areas. Still, it can be concluded that single camera close range photogrammetry and optical surface scanning using Vectra H1 scanner represent relatively low-cost solutions which were shown to be beneficial for postmortem body documentation in forensic pathology. (C) 2015 Elsevier Ireland Ltd. All rights reserved.},
author = {Urbanova, Petra and Hejna, Petr and Jurda, Mikolas},
doi = {10.1016/j.forsciint.2015.03.005},
issn = {0379-0738},
journal = {FORENSIC SCIENCE INTERNATIONAL},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {77--86},
title = {{Testing photogrammetry-based techniques for three-dimensional surface documentation in forensic pathology}},
volume = {250},
year = {2015}
}
@inproceedings{Kim2018133,
abstract = {We propose a novel 3D face recognition algorithm using a deep convolutional neural network (DCNN) and a 3D face expression augmentation technique. The performance of 2D face recognition algorithms has significantly increased by leveraging the representational power of deep neural networks and the use of large-scale labeled training data. In this paper, we show that transfer learning from a CNN trained on 2D face images can effectively work for 3D face recognition by fine-tuning the CNN with an extremely small number of 3D facial scans. We also propose a 3D face expression augmentation technique which synthesizes a number of different facial expressions from a single 3D face scan. Our proposed method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets without using hand-crafted features. The 3D face identification using our deep features also scales well for large databases. {\textcopyright} 2017 IEEE.},
annote = {From Duplicate 1 (Deep 3D face identification - Kim, Donghyun; Hernandez, Matthias; Choi, Jongmoo; Medioni, Gerard)

22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)

From Duplicate 2 (Deep 3D face identification - Kim, Donghyun; Hernandez, Matthias; Choi, Jongmoo; Medioni, Gerard)

From Duplicate 1 (Deep 3D face identification - Kim, Donghyun; Hernandez, Matthias; Choi, Jongmoo; Medioni, Gerard)

22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)

From Duplicate 2 (Deep 3D face identification - Kim, D; Hernandez, M; Choi, J; Medioni, G)

cited By 0

From Duplicate 3 (Deep 3D face identification - Kim, D; Hernandez, M; Choi, J; Medioni, G)

cited By 0},
author = {Kim, Donghyun and Hernandez, Matthias and Choi, Jongmoo and Medioni, Gerard},
booktitle = {IEEE International Joint Conference on Biometrics, IJCB 2017},
doi = {10.1109/BTAS.2017.8272691},
isbn = {978-1-5386-1124-1},
keywords = {convolution,etapa1,face recognition,feedforward neural ne,gil,id145,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {etapa1,gil,id145,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
month = {oct},
pages = {133--142},
publisher = {IEEE},
title = {{Deep 3D face identification}},
url = {http://ieeexplore.ieee.org/document/8272691/ https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046247920{\&}doi=10.1109{\%}2FBTAS.2017.8272691{\&}partnerID=40{\&}md5=9660021125b86249b9768912252c19ff},
volume = {2018-Janua},
year = {2018}
}
@article{ISI:000450379200003,
abstract = {The assessment of the health conditions of trees in forests is extremely important for biodiversity, forest management, global environment monitoring, and carbon dynamics. There is a vast amount of research using remote sensing (RS) techniques for the assessment of the current condition of a forest, but only a small number of these are concerned with detection and classification of dead trees. Among the available RS techniques, only the airborne laser scanner (ALS) enables dead tree detection at the single tree level with high accuracy. The main objective of the study was to identify spruce, pine and deciduous trees by alive or dead classifications. Three RS data sets including ALS (leaf-on and leaf-off) and color-infrared (CIR) imagery (leaf-on) were used for the study. We used intensity and structural variables from the ALS data and spectral information derived from aerial imagery for the classification procedure. Additionally, we tested the differences in the classification accuracy of all variants contained in the data integration. In the study, the random forest (RF) classifier was used. The study was carried out in the Polish part of the Bialowieia Forest (BF). In general, we can state that all classifications, with different combinations of ALS features and CIR, resulted in high overall accuracy (OA {\textgreater}= 90{\%}) and Kappa (kappa {\textgreater} 0.86). For the best variant (CIR{\_}ALS(WSn-FH)), the mean values of overall accuracy and Kappa were equal to 94.3{\%} and 0.93, respectively. The leaf -on point cloud features alone produced the lowest accuracies (OA = 75-81{\%} and x = 0.68-0.76). Improvements of 0-0.04 in the Kappa coefficient and 0-3.1{\%} in the overall classification accuracy were found after the point cloud normalization for all variants. Full -height point cloud features (F) produced lower accuracies than the results based on features calculated for half of the tree height point clouds (H) and combined FH. The importance of each of the predictors for different data sets for tree species classification provided by the RF algorithm was investigated. The lists of top features were the same, independent of intensity normalization. For the classification based on both of the point clouds (leaf on and leaf-off), three structural features (a proportion of first returns for both half -height and full -height variants and the canopy relief ratio of points) and two intensity features from first returns and half -height variant (the coefficient of variation and skewness) were rated as the most important. In the classification based on the point cloud with CIR features, two image features were among the most important (the NDVI and mean value of reflectance in the green band).},
author = {Kaminska, Agnieszka and Lisiewicz, Maciej and Sterenczak, Krzysztof and Kraszewski, Bartlomiej and Sadkowski, Rafal},
doi = {10.1016/j.rse.2018.10.005},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {31--43},
title = {{Species-related single dead tree detection using multi-temporal ALS data and CIR imagery}},
volume = {219},
year = {2018}
}
@inproceedings{ISI:000380584900016,
abstract = {In most Mobile Laser Scanning (MLS) applications, filtering is a necessary step. In this paper, a segmentation-based filtering method is proposed for MLS point cloud, where a segment rather than an individual point is the basic processing unit. Particularly, the MLS point cloud in some blocks are clustered into segments by a surface growing algorithm, then the object segments are detected and removed. A segment-based filtering method is employed to detect the ground segments. Two MLS point cloud datasets are used to evaluate the proposed method. Experiments indicate that, compared with the classic progressive TIN (Triangulated Irregular Network) densification algorithm, the proposed method is capable of reducing the omission error, the commission error and total error by 3.62{\%}, 7.87{\%} and 5.54{\%} on average, respectively.},
annote = {International Workshop on Image and Data Fusion (IWIDF), Kona, HI, JUL
21-23, 2015},
author = {Lin, Xiangguo and Zhang, Jixian},
booktitle = {IWIDF 2015},
doi = {10.5194/isprsarchives-XL-7-W4-99-2015},
editor = {{Zhang, J and Lu, Z and Zeng}, Y},
issn = {2194-9034},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {W4},
pages = {99--102},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{SEGMENTATION-BASED GROUND POINTS DETECTION FROM MOBILE LASER SCANNING POINT CLOUD}},
volume = {47},
year = {2015}
}
@article{ISI:000435048200005,
abstract = {A two dimensional (2D) laser scanner was mounted at the front part of a small 4-wheel autonomous robot with differential steering, at an angle of 30 degrees pointing downwards. The machine was able to drive between maize rows and collect concurrent time-stamped data. A robotic total station tracked the position of a prism mounted on the vehicle. The total station and laser scanner data were fused to generate a three dimensional (3D) point cloud. This 3D representation was used to detect individual plant positions, which are of particular interest for applications such as phenotyping, individual plant treatment and precision weeding. Two different methodologies were applied to the 3D point cloud to estimate the position of the individual plants. The first methodology used the Euclidian Clustering on the entire point cloud. The second methodology utilised the position of an initial plant and the fixed plant spacing to search iteratively for the best clusters. The two algorithms were applied at three different plant growth stages. For the first method, results indicated a detection rate up to 73.7{\%} with a root mean square error of 3.6 cm. The second method was able to detect all plants (100{\%} detection rate) with an accuracy of 2.7-3.0 cm, taking the plant spacing of 13 cm into account.},
author = {Reiser, David and Vazquez-Arellano, Manuel and Paraforos, Dimitris S and Garrido-Izard, Miguel and Griepentrog, Hans W},
doi = {10.1016/j.compind.2018.03.023},
issn = {0166-3615},
journal = {COMPUTERS IN INDUSTRY},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {42--52},
title = {{Iterative individual plant clustering in maize with assembled 2D LiDAR data}},
volume = {99},
year = {2018}
}
@inproceedings{8125644,
abstract = {3D reconstruction of human faces is of high importance in applications like forensics, facial features based human tracking and realistic reconstruction of human faces in virtual reality applications. The published algorithms so far require either a large number of views of a human face or a 3D facial reference model. This paper proposes a technique for 3D human face reconstruction using only five views without any reference model unlike most of the contemporary facial reconstruction techniques. Face localization is done followed by facial feature point extraction in the facial images. The features are tracked in the other images using the Kanade-Lucas-Tomasi (KLT) object tracking algorithm. 3D location of each feature is calculated in all the images using triangulation and a 3D point cloud is formed. The Point cloud is processed to remove outliers and holes. The 3D face is then formed by interpolating and meshing the point cloud. This computationally efficient and low cost technique outperforms commercial and online available software and published algorithms.},
author = {Kumar, A V and Prasad, V V R and Bhurchandi, K M and Satpute, V R and Pious, L and Kar, S},
booktitle = {2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)},
doi = {10.1109/CoDIT.2017.8125644},
keywords = {face recognition,feature extraction,image reconstr,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {1185--1190},
title = {{Dense reconstruction of 3D human face using 5 images and no reference model}},
year = {2017}
}
@article{ISI:000440122900009,
abstract = {The surface quality of three-dimensional (3-D) curved surfaces is one of the most important factors that can directly influence the performance of the final product. This paper presents a systematic approach for detection and monitoring of defects on 3-D curved surfaces based on high-density point cloud data. Firstly, an algorithm to remove outliers and a boundary recognition algorithm are proposed to divide the entire 3-D curved surface including millions of measured points into multiple sub-regions. Secondly, two new evaluation indexes based on wavelet packet entropy and normal vector are explored to represent the features of the multiple sub-regions to determine whether the sub-regions are out-of-limit (OOL) of specifications. Thirdly, three quality parameters representing quality characteristics of a curved surface are presented and their values are calculated based on the clusters of OOL sub-regions. Finally, three individual control charts are presented to monitor the three quality parameters. As long as any quality parameter is out of the control range, the manufacturing process of the curved surface is determined to be out-of-control (OOC). The results of a case study show that the proposed approach can effectively identify the OOC manufacturing process and detect defects on 3-D curved surfaces.},
author = {Huang, Delin and Du, Shichang and Li, Guilong and Zhao, Chen and Deng, Yafei},
doi = {10.1016/j.precisioneng.2018.03.001},
issn = {0141-6359},
journal = {PRECISION ENGINEERING-JOURNAL OF THE INTERNATIONAL SOCIETIES FOR PRECISION ENGINEERING AND NANOTECHNOLOGY},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
month = {jul},
pages = {79--95},
title = {{Detection and monitoring of defects on three-dimensional curved surfaces based on high-density point cloud data}},
volume = {53},
year = {2018}
}
@article{ISI:000385343000017,
abstract = {This research done is to solve the problems faced by digital forensic analysts in identifying a suspect captured on their CCTV. Identifying the suspect through the CCTV video footage is a very challenging task for them as it involves tedious rounds of processes to match the facial information in the video footage to a set of suspect's images. The biggest problem faced by digital forensic analysis is modeling 2D model extracted from CCTV video as the model does not provide enough information to carry out the identification process. Problems occur when a suspect in the video is not facing the camera, the image extracted is the side image of the suspect and it is difficult to make a matching with portrait image in the database. There are also many factors that contribute to the process of extracting facial information from a video to be difficult, such as low-quality video. Through 2D to 3D image model mapping, any partial face information that is incomplete can be matched more efficiently with 3D data by rotating it to matched position. The first methodology in this research is data collection; any data obtained through video recorder. Then, the video will be converted into an image. Images are used to develop the Active Appearance Model (the 2D face model is AAM) 2D and AAM 3D. AAM is used as an input for learning and testing process involving three classifiers, which are Random Forest, Support Vector Machine (SVM), and Neural Networks classifier. The experimental results show that the 3D model is more suitable for use in face recognition as the percentage of the recognition is higher compared with the 2D model.},
author = {{Abd Rahman}, Siti Zaharah and Abdullah, Siti Norul Huda Sheikh and Hao, Lim Eng and Abdulameer, Mohammed Hasan and Zamani, Nazri Ahmad and Darus, Mohammad Zaharudin A},
issn = {0127-9696},
journal = {JURNAL TEKNOLOGI},
keywords = {lerdepois,revisao{\_}V2,revisao{\_}webofscience,tutux5},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}webofscience,tutux5},
number = {2-2},
pages = {121--129},
title = {{MAPPING 2D TO 3D FORENSIC FACIAL RECOGNITION VIA BIO-INSPIRED ACTIVE APPEARANCE MODEL}},
volume = {78},
year = {2016}
}
@article{ISI:000457666900042,
abstract = {3D registration is a very active topic, spanning research areas such as computational geometry, computer graphics and pattern recognition. It aims to solve spatial transformation that aligns two point clouds. In this work we propose the use of a single direction sensor, such as an accelerometer or a magnetometer, commonly available on contemporary mobile platforms, such as tablets and smartphones. Both sensors have been heavily investigated earlier, but only for joint use with other sensors, such as gyroscopes and GPS. We show a time-efficient and accurate 3D registration method that takes advantage of only either an accelerometer or a magnetometer. We demonstrate a 3D reconstruction of individual point clouds and the proposed 3D registration method on a tablet equipped with an accelerometer or a magnetometer. However, we point out that the proposed method is not restricted to mobile platforms. Indeed, it can easily be applied in any 3D measurement system that is upgradable with some ubiquitous direction sensor, for example by adding a smartphone equipped with either an accelerometer or a magnetometer. We compare the proposed method against several state-of-the-art methods implemented in the open source Point Cloud Library (PCL). The proposed method outperforms the PCL methods tested, both in terms of processing time and accuracy. (C) 2018 Elsevier Ltd. All rights reserved.},
author = {Pribanic, Tomislav and Petkovic, Tomislav and Donlic, Matea},
doi = {10.1016/j.patcog.2018.12.008},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {532--546},
title = {{3D registration based on the direction sensor measurements}},
volume = {88},
year = {2019}
}
@article{Neto2019594,
abstract = {Nowadays, there is an increasing need for systems that can accurately and quickly identify a person. Traditional identification methods utilize something a person knows or something a person has. This kind of methods has several drawbacks, being the main one the fact that it is impossible to detect an imposter who uses genuine credentials to pass as a genuine person. One way to solve these kinds of problems is to utilize biometric identification. The face is one of the biometric features that best suits the covert identification. However, in general, biometric systems based on 2D face recognition perform very poorly in unconstrained environments, common in covert identification scenarios, since the input images present variations in pose, illumination, and facial expressions. One way to mitigate this problem is to use 3D face data, but the current 3D scanners are expensive and require a lot of cooperation from people being identified. Therefore, in this work, we propose an approach based on local descriptors for 3D Face Recognition based on 3D face models reconstructed from collections of 2D images. Initial results show 95{\%} in a subset of the LFW Face dataset. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 0},
author = {Neto, J B C and Marana, A N},
doi = {10.1007/978-3-030-13469-3_69},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {lerdepois,revisao{\_}V2,revisao{\_}scopus,tutux5},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}scopus,tutux5},
pages = {594--601},
title = {{3D face recognition with reconstructed faces from a collection of 2D images}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063066921{\&}doi=10.1007{\%}2F978-3-030-13469-3{\_}69{\&}partnerID=40{\&}md5=ae38ed3a54a47df05e22f1ec3db64720},
volume = {11401 LNCS},
year = {2019}
}
@inproceedings{8688162,
abstract = {In this paper, we present a novel panoramic image model for scattered point clouds, and apply it to the problem of place recognition. We project a point cloud onto a sphere, and then the sphere is divided into a set of individual grids by longitudes and latitudes. Each grid is regard as a pixel and its value is computed using the geometrical relationship among the points in the grid and its neighbors. For convenience, the sphere is transferred into a flat. Since point clouds are converted to 2D images, we use ORB features and bag of words technique to solve place recognition problem. Our experimental results show that our image model is a more universal one and achieve a good performance in place recognition in both accuracy and efficiency.},
author = {Cao, F and Yan, F and Gu, Y and Ding, C and Zhuang, Y and Wang, W},
booktitle = {2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)},
doi = {10.1109/CYBER.2018.8688162},
issn = {2379-7711},
keywords = {revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
pages = {79--83},
title = {{A Novel Image Model of Point Clouds and its Application in Place Recognition}},
year = {2018}
}
@article{ISI:000428508200022,
abstract = {The vast extent and inaccessibility of boreal forest ecosystems are barriers to routine monitoring of forest structure and composition. In this research, we bridge the scale gap between intensive but sparse plot measurements and extensive remote sensing studies by collecting forest inventory variables at the plot scale using an unmanned aerial vehicle (UAV) and a structure from motion (SfM) approach. At 20 Forest Inventory and Analysis (FIA) subplots in interior Alaska, we acquired overlapping imagery and generated dense, 3D, RGB (red, green, blue) point clouds. We used these data to model forest type at the individual crown scale as well as subplot-scale tree density (TD), basal area (BA), and aboveground biomass (AGB). We achieved 85{\%} cross-validation accuracy for five species at the crown level. Classification accuracy was maximized using three variables representing crown height, form, and color. Consistent with previous UAV-based studies, SfM point cloud data generated robust models of TD (r(2) = 0.91), BA (r(2) = 0.79), and AGB (r(2) = 0.92), using a mix of plot-and crown-scale information. Precise estimation of TD required either segment counts or species information to differentiate black spruce from mixed white spruce plots. The accuracy of species-specific estimates of TD, BA, and AGB at the plot scale was somewhat variable, ranging from accurate estimates of black spruce TD (+/1{\%}) and aspen BA (-2{\%}) to misallocation of aspen AGB (+118{\%}) and white spruce AGB (-50{\%}). These results convey the potential utility of SfM data for forest type discrimination in FIA plots and the remaining challenges to develop classification approaches for species-specific estimates at the plot scale that are more robust to segmentation error.},
author = {Alonzo, Michael and Andersen, Hans-Erik and Morton, Douglas C and Cook, Bruce D},
doi = {10.3390/f9030119},
issn = {1999-4907},
journal = {FORESTS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
number = {3},
title = {{Quantifying Boreal Forest Structure and Composition Using UAV Structure from Motion}},
volume = {9},
year = {2018}
}
@article{ISI:000425868600001,
abstract = {This study assessed whether presenting 3D face stimuli could facilitate children's facial expression recognition. Seventy-one children aged between 3 and 6 participated in the study. Their task was to judge whether a face presented in each trial showed a happy or fearful expression. Half of the face stimuli were shown with 3D representations, whereas the other half of the images were shown as 2D pictures. We compared expression recognition under these conditions. The results showed that the use of 3D faces improved the speed of facial expression recognition in both boys and girls. Moreover, 3D faces improved boys' recognition accuracy for fearful expressions. Since fear is the most difficult facial expression for children to recognize, the facilitation effect of 3D faces has important practical implications for children with difficulties in facial expression recognition. The potential benefits of 3D representation for other expressions also have implications for developing more realistic assessments of children's expression recognition.},
author = {Wang, Lamei and Chen, Wenfeng and Li, Hong},
doi = {10.1038/srep45464},
issn = {2045-2322},
journal = {SCIENTIFIC REPORTS},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
title = {{Use of 3D faces facilitates facial expression recognition in children}},
volume = {7},
year = {2017}
}
@article{Chouchane:2015:FRU:2836876.2836879,
abstract = {This paper presents an automatic face recognition system in the presence of illumination, expressions and pose variations based on depth and intensity information. At first, the registration of 3D faces is achieved using iterative closest point (ICP). Nose tip point must be located using Maximum Intensity Method. This point usually has the largest depth value; however there is a problem with some unnecessary data such as: shoulders, hair, neck and parts of clothes; to cope with this issue, we propose the integral projection curves (IPC)-based facial area segmentation to extract the facial area. After that, the combined method principal component analysis (PCA) with enhanced Fisher model (EFM) is used to obtain the feature matrix vectors. Finally, the classification is performed using distance measurement and support vector machine (SVM). The experiments are implemented on two face databases CASIA3D and GavabDB; our results show that the proposed method achieves a high recognition performance. Online publication date: Mon, 05-Oct-2015},
address = {Inderscience Publishers, Geneva, SWITZERLAND},
annote = {24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
24/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Chouchane, Ammar and Belahcene, Mebarka and Bourennane, Salah},
doi = {10.1504/IJISTA.2015.072219},
issn = {1740-8865},
journal = {International Journal of Intelligent Systems Technologies and Applications},
keywords = {acm,etapa1,gil,id234,revisao{\_}V1,revisao{\_}scopus,tutux5},
mendeley-tags = {acm,etapa1,gil,id234,revisao{\_}V1,revisao{\_}scopus,tutux5},
number = {1},
pages = {50},
publisher = {Inderscience Publishers},
title = {{3D and 2D face recognition using integral projection curves based depth and intensity images}},
url = {http://www.inderscience.com/link.php?id=72219},
volume = {14},
year = {2015}
}
@article{ISI:000367113200011,
abstract = {The Fortran subroutine package PENGEOM provides a complete set of tools to handle quadric geometries in Monte Carlo simulations of radiation transport. The material structure where radiation propagates is assumed to consist of homogeneous bodies limited by quadric surfaces. The PENGEOM subroutines (a subset of the PENELOPE code) track particles through the material structure, independently of the details of the physics models adopted to describe the interactions. Although these subroutines are designed for detailed simulations of photon and electron transport, where all individual interactions are simulated sequentially, they can also be used in mixed (class II) schemes for simulating the transport of high-energy charged particles, where the effect of soft interactions is described by the random-hinge method. The definition of the geometry and the details of the tracking algorithm are tailored to optimize simulation speed. The use of fuzzy quadric surfaces minimizes the impact of round-off errors. The provided software includes a Java graphical user interface for editing and debugging the geometry definition file and for visualizing the material structure. Images of the structure are generated by using the tracking subroutines and, hence, they describe the geometry actually passed to the simulation code. Program summary Program title: Pengeom Catalogue identifier: AEYH{\_}v1{\_}0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYH{\_}v1{\_}0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 89390 No. of bytes in distributed program, including test data, etc.: 5062646 Distribution format: tar.gz Programming language: Fortran, Java. Computer: PC with Java Runtime Environment installed. Operating system: Windows, Linux. RAM: 210 MiB Classification: 21.1, 14. Nature of problem: The Fortran subroutines perform all geometry operations in Monte Carlo simulations of radiation transport with arbitrary interaction models. They track particles through material systems consisting of homogeneous bodies limited by quadric surfaces. Particles are moved in steps (free flights) of a given length, which is dictated by the simulation program, and are halted when they cross an interface between media of different compositions or when they enter selected bodies. Solution method: The pengeom subroutines are tailored to optimize simulation speed and accuracy. Fast tracking is accomplished by the use of quadric surfaces, which facilitate the calculation of ray intersections, and of modules (connected volumes limited by quadric surfaces) organized in a hierarchical structure. Optimal accuracy is obtained by considering fuzzy surfaces, with the aid of a simple algorithm that keeps control of multiple intersections of a ray and a surface. The Java GUI PenGeomJar provides a geometry toolbox; it allows building and debugging the geometry definition file, as well as visualizing the resulting geometry in two and three dimensions. Restrictions: By default pengeom can handle systems with up to 5000 bodies and 10,000 surfaces. These numbers can be increased by editing the Fortran source file. Unusual features: All geometrical operations are performed internally. The connection between the steering main program and the tracking routines is through a Fortran module, which contains the state variables of the transported particle, and the input-output arguments of the subroutine step. Rendering of two- and three-dimensional images is performed by using the pengeom subroutines, so that displayed images correspond to the definitions passed to the simulation program. Additional comments: Java editor and viewer (PenGeomJar), geometry examples, translator to POV-Ray (TM) format, detailed manual. The Fortran subroutine package pengeom is part of the penelope code system {\{}[{\}}1]. Running time: The running time much depends on the complexity of the material system. The most complicated example provided, phantom, an anthropomorphic phantom, has 264 surfaces and 169 bodies and modules, with different levels of grouping; the largest module contains 51 daughters. The rendering of a 3D image of phantom with 1680x1050 pixels takes about 25 s (i.e., about 1.5 . 10(-5) seconds per ray) on an Intel Core 17-3520M CPU, with Windows 7 and subroutines compiled with gfortran. References: {\{}[{\}}1] F. Salvat, PENELOPE-2014: A Code System for Monte Carlo Simulation of Electron and Photon Transport, OECD/NEA Data Bank, Issy-les-Moulineaux, France, 2015. Available from http://www.nea.fr/lists/penelope.html.(C) 2015 Elsevier B.V. All rights reserved.},
author = {Almansa, Julio and Salvat-Pujol, Francesc and Diaz-Londono, Gloria and Carnicer, Artur and Lallena, Antonio M and Salvat, Francesc},
doi = {10.1016/j.cpc.2015.09.019},
issn = {0010-4655},
journal = {COMPUTER PHYSICS COMMUNICATIONS},
keywords = {revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}webofscience,tutux9},
month = {feb},
pages = {102--113},
title = {{PENGEOM-A general-purpose geometry package for Monte Carlo simulation of radiation transport in material systems defined by quadric surfaces}},
volume = {199},
year = {2016}
}
@inproceedings{Beumier20152291,
abstract = {In the context of 3D face recognition, facial surfaces are advantageously captured by a structured light acquisition system, which is typically quick, low cost and uses off-the-shelve components. The light pattern projected, a key aspect of the structured light approach, makes the major difference between developed systems. In most of them, elements of the light pattern must be identified by a property such as element thickness or colour. We present in this paper the design of projected patterns that led to the realisation of three 3D acquisition prototypes. {\textcopyright} 2004 EUSIPCO.},
annote = {cited By 0},
author = {Beumier, C},
booktitle = {European Signal Processing Conference},
keywords = {revisao{\_}V1,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutux9},
pages = {2291--2294},
title = {{Design of coded structured light pattern for 3D facial surface capture}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979948896{\&}partnerID=40{\&}md5=b09f05fe749fab292462602ba77b999e},
volume = {06-10-Sept},
year = {2015}
}
@inproceedings{ISI:000382327100024,
abstract = {Synthetic aperture radar interferometry (InSAR) has been an established method for long term large area monitoring. Since the launch of meter-resolution spaceborne SAR sensors, the InSAR community has shown that even individual buildings can be monitored in high level of detail. However, the current deformation analysis still remains at a primitive stage of pixel-wise motion parameter inversion and manual identification of the regions of interest. We are aiming at developing an automatic urban infrastructure monitoring approach by combining InSAR and the semantics derived from optical images, so that the deformation analysis can be done systematically in the semantic/object level. This paper explains how we transfer the semantic meaning derived from optical image to the InSAR point clouds, and hence different semantic classes in the InSAR point cloud can be automatically extracted and monitored. Examples on bridges and railway monitoring are demonstrated.},
annote = {ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015},
author = {Wang, Yuanyuan and Zhu, Xiao Xiang},
booktitle = {ISPRS GEOSPATIAL WEEK 2015},
doi = {10.5194/isprsarchives-XL-3-W3-153-2015},
editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane}, G},
issn = {2194-9034},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {W3},
organization = {WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging {\&} Sensors; WG VII 7 Synergy Radar; ISPRS},
pages = {153--160},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{SEMANTIC INTERPRETATION OF INSAR ESTIMATES USING OPTICAL IMAGES WITH APPLICATION TO URBAN INFRASTRUCTURE MONITORING}},
volume = {40-3},
year = {2015}
}
@article{ISI:000457692800013,
abstract = {BackgroundIndirect anthropometry (IA) is one of the craniofacial anthropometry methods to perform the measurements on the digital facial images. In order to get the linear measurements, a few definable points on the structures of individual facial images have to be plotted as landmark points. Currently, most anthropometric studies use landmark points that are manually plotted on a 3D facial image by the examiner. This method is time-consuming and leads to human biases, which will vary from intra-examiners to inter-examiners when involving large data sets. Biased judgment also leads to a wider gap in measurement error. Thus, this work aims to automate the process of landmarks detection to help in enhancing the accuracy of measurement. In this work, automated craniofacial landmarks (ACL) on a 3D facial image system was developed using geometry characteristics information to identify the nasion (n), pronasale (prn), subnasale (sn), alare (al), labiale superius (ls), stomion (sto), labiale inferius (li), and chelion (ch). These landmarks were detected on the 3D facial image in .obj file format. The IA was also performed by manually plotting the craniofacial landmarks using Mirror software. In both methods, once all landmarks were detected, the eight linear measurements were then extracted. Paired t-test was performed to check the validity of ACL (i) between the subjects and (ii) between the two methods, by comparing the linear measurements extracted from both ACL and AI. The tests were performed on 60 subjects (30 males and 30 females).ResultsThe results on the validity of the ACL against IA between the subjects show accurate detection of n, sn, prn, sto, ls and li landmarks. The paired t-test showed that the seven linear measurements were statistically significant when p{\textless}0.05. As for the results on the validity of the ACL against IA between the methods, ACL is more accurate when p approximate to 0.03.ConclusionsIn conclusion, ACL has been validated with the eight landmarks and is suitable for automated facial recognition. ACL has proved its validity and demonstrated the practicability to be used as an alternative for IA, as it is time-saving and free from human biases.},
annote = {17th International Conference on Bioinformatics (InCoB), Jawaharlal
Nehru Univ, New Delhi, INDIA, SEP 26-28, 2018},
author = {Abu, Arpah and Ngo, Chee Guan and Abu-Hassan, Nur Idayu Adira and Othman, Siti Adibah},
doi = {10.1186/s12859-018-2548-9},
issn = {1471-2105},
journal = {BMC BIOINFORMATICS},
keywords = {lerdepois,revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {feb},
number = {13},
title = {{Automated craniofacial landmarks detection on 3D image using geometry characteristics information}},
volume = {19},
year = {2019}
}
@inproceedings{ISI:000365181700047,
abstract = {Moving Object detection based on video, of late has gained momentum in the field of research. Moving object detection has extensive application areas and is used for monitoring intelligence interaction between human and computer, transportation of intelligence, and navigating visual robotics, clarity in steering systems. It is also used in various other fields for diagnosing, compressing images, reconstructing 3D images, retrieving video images and so on. Since surveillance of human movement detection is subjective, the human objects are precisely detected to the framework proposed for human detection based on the Locomotive Object Extraction. The issue of illumination changes and crowded human image is discriminated. The image is detected through the detection feature that identifies head and shoulder and is the loci for the proposed framework. The detection of individual objects has been revamped appreciably over the recent years but even now environmental factors and crowd-scene detection remains significantly difficult for detection of moving object. The proposed framework subtracts the background through Gaussian mixture model and the area of significance is extracted. The area of significance is transformed to white and black picture by picture binarization. Then, Wiener filter is employed to scale the background level for optimizing the results of the object in motion. The object is finally identified. The performance in every stage is measured and is evaluated. The result in each stage is compared and the performance of the proposed framework is that of the existing system proves satisfactory.},
annote = {3rd International Conference on Frontiers in Intelligent Computing -
Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014},
author = {Sivasankar, C and Srinivasan, A},
booktitle = {PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2},
doi = {10.1007/978-3-319-12012-6_47},
editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal}, JK},
isbn = {978-3-319-12012-6; 978-3-319-12011-9},
issn = {2194-5357},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol {\&} Sci, CSI Student Branch},
pages = {431--439},
series = {Advances in Intelligent Systems and Computing},
title = {{A Framework for Human Recognition Based on Locomotive Object Extraction}},
volume = {328},
year = {2015}
}
@article{ISI:000353807700021,
abstract = {BACKGROUND: Using three-dimensional (3D) photography, exact images of the human body can be produced. Over the last few years, this technique is mainly being developed in the field of maxillofacial reconstructive surgery, creating fusion images with computed tomography (CT) data for accurate planning and prediction of treatment outcome. However, in hand surgery, 3D photography is not yet being used in clinical settings. METHODS: The aim of this study was to develop a valid method for imaging the hand using 3D stereophotogrammetry. The reproducibility of 30 soft tissue landmarks was determined using 3D stereophotogrammetric images. Analysis was performed by two observers on 20 3D photographs. Reproducibility and reliability of the landmark identification were determined using statistical analysis. RESULTS: The intra-and interobserver reproducibility of the landmarks were high. This study showed a high reliability coefficient for intraobserver (1.00) and interobserver reliability (0.99). Identification of the landmarks on the palmar aspect of individual fingers was more precise than the identification of landmarks of the thumb. CONCLUSIONS: This study shows that 3D photography can safely produce accurate and reproducible images of the hand, which makes the technique a reliable method for soft tissue analysis. 3D images can be a helpful tool in pre- and postoperative evaluation of reconstructive trauma surgery, in aesthetic surgery of the hand, and for educational purposes. The use in everyday practice of hand surgery and the concept of fusing 3D photography images with radiologic images of the interior hand structures needs to be further explored. (C) 2014 British Association of Plastic, Reconstructive and Aesthetic Surgeons. Published by Elsevier Ltd. All rights reserved.},
author = {Hoevenaren, Inge A and Maal, Thomas J J and Krikken, E and de Haan, A F J and Berge, S J and Ulrich, D J O},
doi = {10.1016/j.bjps.2014.12.025},
issn = {1748-6815},
journal = {JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {may},
number = {5},
pages = {709--716},
title = {{Development of a three-dimensional hand model using 3D stereophotogrammetry: Evaluation of landmark reproducibility}},
volume = {68},
year = {2015}
}
@article{ISI:000442764500004,
abstract = {Purpose: BCCT.core (Breast Cancer Conservative Treatment. cosmetic results) is a software created for the objective evaluation of aesthetic result of breast cancer conservative treatment using a single patient frontal photography. The lack of volume information has been one criticism, as the use of 3D information might improve accuracy in aesthetic evaluation. In this study, we have evaluated the added value of 3D information to two methods of aesthetic evaluation: a panel of experts; and an augmented version of the computational model - BCCT.core3d. Material and methods: Within the scope of EU Seventh Framework Programme Project PICTURE, 2D and 3D images from 106 patients from three clinical centres were evaluated by a panel of 17 experts and the BCCT.core. Agreement between all methods was calculated using the kappa (K) and weighted kappa (wK) statistics. Results: Subjective agreement between 2D and 3D individual evaluation was fair to moderate. The agreement between the expert classification and the BCCT.core software with both 2D and 3D features was also fair to moderate. Conclusions: The inclusion of 3D images did not add significant information to the aesthetic evaluation either by the panel or the software. Evaluation of aesthetic outcome can be performed using of the BCCT.core software, with a single frontal image. (C) 2018 Elsevier Ltd. All rights reserved.},
author = {Cardoso, Maria Joao and Vrieling, Conny and Cardoso, Jaime S and Oliveira, Helder P and Williams, Norman R and Dixon, J M and Tea, PICTURE Project Clinical Trial and Panel, PICTURE Project Delphi},
doi = {10.1016/j.breast.2018.06.008},
issn = {0960-9776},
journal = {BREAST},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {oct},
pages = {19--24},
title = {{The value of 3D images in the aesthetic evaluation of breast cancer conservative treatment. Results from a prospective multicentric clinical trial}},
volume = {41},
year = {2018}
}
@inproceedings{ISI:000418793200017,
abstract = {Today it is easily possible to generate dense point clouds of the sensor environment using 360 degrees LiDAR (Light Detection and Ranging) sensors which are available since a number of years. The interpretation of these data is much more challenging. For the automated data evaluation the detection and classification of objects is a fundamental task. Especially in urban scenarios moving objects like persons or vehicles are of particular interest, for instance in automatic collision avoidance, for mobile sensor platforms or surveillance tasks. In literature there are several approaches for automated person detection in point clouds. While most techniques show acceptable results in object detection, the computation time is often crucial. The runtime can be problematic, especially due to the amount of data in the panoramic 360 degrees point clouds. On the other hand, for most applications an object detection and classification in real time is needed. The paper presents a proposal for a fast, real-time capable algorithm for person detection, classification and tracking in panoramic point clouds.},
annote = {Conference on Electro-Optical Remote Sensing XI, Warsaw, POLAND, SEP
11-12, 2017},
author = {Hammer, Marcus and Hebel, Marcus and Arens, Michael},
booktitle = {ELECTRO-OPTICAL REMOTE SENSING XI},
doi = {10.1117/12.2278215},
editor = {{Kamerman, G and Steinvall}, O},
isbn = {978-1-5106-1333-1; 978-1-5106-1332-4},
issn = {0277-786X},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Person detection and tracking with a 360 degrees LiDAR system}},
volume = {10434},
year = {2017}
}
@article{ISI:000379266300013,
abstract = {Paper introduces a 3-D shape representation scheme for automatic face analysis and identification, and demonstrates its invariance to facial expression. The core of this scheme lies on the combination of statistical shape modelling and non-rigid deformation matching. While the former matches 3-D faces with facial expression, the latter provides a low-dimensional feature vector that controls the deformation of model for matching the shape of new input, thereby enabling robust identification of 3-D faces. The proposed scheme is also able to handle the pose variation without large part of missing data. To assist the establishment of dense point correspondences, a modified free-form-deformation based on B-spline warping is applied with the help of extracted landmarks. The hybrid iterative closest point method is introduced for matching the models and new data. The feasibility and effectiveness of the proposed method was investigated using standard publicly available Gavab and BU-3DFE datasets, which contain faces with expression and pose changes. The performance of the system was compared with that of nine benchmark approaches. The experimental results demonstrate that the proposed scheme provides a competitive solution for face recognition.},
author = {Quan, Wei and Matuszewski, Bogdan J and Shark, Lik-Kwan},
doi = {10.1007/s10044-014-0439-x},
issn = {1433-7541},
journal = {PATTERN ANALYSIS AND APPLICATIONS},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {3},
pages = {765--781},
title = {{Statistical shape modelling for expression-invariant face analysis and recognition}},
volume = {19},
year = {2016}
}
@article{ISI:000403860600008,
abstract = {The semantic classification of point clouds is a fundamental part of three-dimensional urban reconstruction. For datasets with high spatial resolution but significantly more noises, a general trend is to exploit more contexture information to surmount the decrease of discrimination of features for classification. However, previous works on adoption of contexture information are either too restrictive or only in a small region and in this paper, we propose a point cloud classification method based on multi-level semantic relationships, including point-homogeneity, supervoxel-adjacency and class-knowledge constraints, which is more versatile and incrementally propagate the classification cues from individual points to the object level and formulate them as a graphical model. The point-homogeneity constraint clusters points with similar geometric and radiometric properties into regular-shaped supervoxels that correspond to the vertices in the graphical model. The supervoxel-adjacency constraint contributes to the pairwise interactions by providing explicit adjacent relationships between supervoxels. The class knowledge constraint operates at the object level based on semantic rules, guaranteeing the classification correctness of supervoxel clusters at that level. International Society of Photogrammetry and Remote Sensing (ISPRS) benchmark tests have shown that the proposed method achieves state-of-the-art performance with an average per-area completeness and correctness of 93.88{\%} and 95.78{\%}, respectively. The evaluation of classification of photogrammetric point clouds and DSM generated from aerial imagery confirms the method's reliability in several challenging urban scenes. (C) 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.},
author = {Zhu, Qing and Li, Yuan and Hu, Han and Wu, Bo},
doi = {10.1016/j.isprsjprs.2017.04.022},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jul},
pages = {86--102},
title = {{Robust point cloud classification based on multi-level semantic relationships for urban scenes}},
volume = {129},
year = {2017}
}
@inproceedings{Varney2015,
abstract = {One of the most difficult challenges of working with LiDAR data is the large amount of data points that are produced. Analysing these large data sets is an extremely time consuming process. For this reason, automatic perception of LiDAR scenes is a growing area of research. Currently, most LiDAR feature extraction relies on geometrical features specific to the point cloud of interest. These geometrical features are scene-specific, and often rely on the scale and orientation of the object for classification. This paper proposes a robust method for reduced dimensionality feature extraction of 3D objects using a volume component analysis (VCA) approach.1 This VCA approach is based on principal component analysis (PCA). PCA is a method of reduced feature extraction that computes a covariance matrix from the original input vector. The eigenvectors corresponding to the largest eigenvalues of the covariance matrix are used to describe an image. Block-based PCA is an adapted method for feature extraction in facial images because PCA, when performed in local areas of the image, can extract more significant features than can be extracted when the entire image is considered. The image space is split into several of these blocks, and PCA is computed individually for each block. This VCA proposes that a LiDAR point cloud can be represented as a series of voxels whose values correspond to the point density within that relative location. From this voxelized space, block-based PCA is used to analyze sections of the space where the sections, when combined, will represent features of the entire 3-D object. These features are then used as the input to a support vector machine which is trained to identify four classes of objects, vegetation, vehicles, buildings and barriers with an overall accuracy of 93.8{\%}. {\textcopyright} 2015 SPIE.},
annote = {cited By 0},
author = {Varney, N M and Asari, V K},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2179268},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {January},
title = {{Volume component analysis for classification of LiDAR data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937231577{\&}doi=10.1117{\%}2F12.2179268{\&}partnerID=40{\&}md5=f90b0a608ec94d6fb687c0271123252b},
volume = {9477},
year = {2015}
}
@inproceedings{ISI:000380393900083,
abstract = {The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15-20 years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region.},
annote = {IEEE 11th International Conference and Workshops on Automatic Face and
Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015},
author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic, Maja},
booktitle = {2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG), VOL. 5},
isbn = {978-1-4799-6026-2},
issn = {2326-5396},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IEEE; IEEE Comp Soc; IEEE Biometric Council},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{Active Nonrigid ICP Algorithm}},
year = {2015}
}
@inproceedings{7327791,
abstract = {Chemometric analysis was applied on terahertz absorbance 3D images, in transmission. The goal is to automatically discriminate some explosives on images and quantify mixtures of RDX/PETN in the frequency range of 0.2-3 THz. Partial Least Square (PLS) was applied on THz absorbance multispectral images to quantify individual product inside pure samples and mixtures at each pixel on the image. Then the best score obtained is used to display the samples' images and provide the optimal frequencies combination for recognition purpose.},
author = {Sleiman, J B and Perraud, J B and Bousquet, B and Palka, N and Guillet, J P and Mounaix, P},
booktitle = {2015 40th International Conference on Infrared, Millimeter, and Terahertz waves (IRMMW-THz)},
doi = {10.1109/IRMMW-THz.2015.7327791},
issn = {2162-2035},
keywords = {chemical variables measurement,electromagnetic wav,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {aug},
pages = {1},
title = {{Chemical imaging and quantification of RDX/PETN mixtures by PLS applied on terahertz time-domain spectroscopy}},
year = {2015}
}
@article{ISI:000380771500016,
abstract = {Height models based on high-altitude aerial images provide a low-cost means of generating detailed 3D models of the forest canopy. In this study, the performance of these height models in the detection of individual trees was evaluated in a commercially managed boreal forest. Airborne digital stereo imagery (DSI) was captured from a flight altitude of 5 km with a ground sample distance of 50 cm and corresponds to regular national topographic airborne data capture programs operated in many countries. Tree tops were detected from smoothed canopy height models (CHM) using watershed segmentation. The relative amount of detected trees varied between 26{\%} and 140{\%}, and the RMSE of plot-level arithmetic mean height between 2.2 m and 3.1 m. Both the dominant tree species and the filter used for smoothing affected the results. Even though the spatial resolution of DSI-based CHM was sufficient, detecting individual trees from the data proved to be demanding because of the shading effect of the dominant trees and the limited amount of data from lower canopy levels and near the ground.},
author = {Tanhuanpaa, Topi and Saarinen, Ninni and Kankare, Ville and Nurminen, Kimmo and Vastaranta, Mikko and Honkavaara, Eija and Karjalainen, Mika and Yu, Xiaowei and Holopainen, Markus and Hyyppa, Juha},
doi = {10.3390/f7070143},
issn = {1999-4907},
journal = {FORESTS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jul},
number = {7},
title = {{Evaluating the Performance of High-Altitude Aerial Image-Based Digital Surface Models in Detecting Individual Tree Crowns in Mature Boreal Forests}},
volume = {7},
year = {2016}
}
@article{ISI:000349271500019,
abstract = {Facial expressions are a powerful tool that communicates a person's emotional state and subsequently his/her intentions. Compared to 2D face images, 3D face images offer more granular cues that are not available in the 2D images. However, one major setback of 3D faces is that they impose a higher dimensionality than 2D faces. In this paper, we attempt to address this problem by proposing a fully automatic 3D facial expression recognition model that tackles the high dimensionality problem in a twofold solution. First, we transform the 3D faces into the 2D plane using conformal mapping. Second, we propose a Differential Evolution (DE) based optimization algorithm to select the optimal facial feature set and the classifier parameters simultaneously. The optimal features are selected from a pool of Speed Up Robust Features (SURF) descriptors of all the prospective facial points. The proposed model yielded an average recognition accuracy of 79{\%} using the Bosphorus database and 79.36{\%} using the BU-3DFE database. In addition, we exploit the facial muscular movements to enhance the probability estimation (PE) of Support Vector Machine (SVM). Joint application of feature selection with the proposed enhanced PE (EPE) yielded an average recognition accuracy of 84{\%} using the Bosphorus database and 85.81{\%} using the BU-3DFE database, which is statistically significantly better (at p {\textless} 0.01 and p {\textless} 0.001, respectively) if compared to the individual exploit of the optimal features only. (C) 2014 Elsevier Ltd. All rights reserved.},
author = {Azazi, Amal and Lotfi, Syaheerah Lebai and Venkat, Ibrahim and Fernandez-Martinez, Fernando},
doi = {10.1016/j.eswa.2014.10.042},
issn = {0957-4174},
journal = {EXPERT SYSTEMS WITH APPLICATIONS},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
number = {6},
pages = {3056--3066},
title = {{Towards a robust affect recognition: Automatic facial expression recognition in 3D faces}},
volume = {42},
year = {2015}
}
@article{ISI:000449578500012,
abstract = {In recent years, information technology is developing continuously and set off a burst of artificial intelligence boom in the field of science. The development of advanced technologies such as unmanned driving and AI chips, is the extensive application of artificial intelligence. Face-related technologies have a wide range of applications because of intuitive results and good concealment. Since 3D face information can provide more comprehensive facial information than 2D face information, and it can solve many difficulties that cannot be solved in 2D face recognition. Therefore, more and more researchers have studied 3D face recognition in recent years. Under the new circumstances, the research on face are experiencing all kinds of challenges. With the tireless of many scientists, the new technology is also making a constant progress, and in the development of many technologies it still maintained its leading position. In this paper, we simply sort out the present development process of facial correlation technology, and the general evolution of this technology is outlined. Finally, the practical significance of this technology development is briefly discussed. (C) 2018 Published by Elsevier Inc.},
author = {Fei, Hongyan and Tu, Bing and Chen, Ququ and He, Danbing and Zhou, Chengle and Peng, Yishu},
doi = {10.1016/j.jvcir.2018.09.012},
issn = {1047-3203},
journal = {JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux8},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux8},
pages = {139--143},
title = {{An overview of face-related technologies}},
volume = {56},
year = {2018}
}
@article{ISI:000401888600006,
abstract = {Tropical forests are a key component of the global carbon cycle, and mapping their carbon density is essential for understanding human influences on climate and for ecosystem-service-based payments for forest protection. Discrete-return airborne laser scanning (ALS) is increasingly recognised as a high-quality technology for mapping tropical forest carbon, because it generates 3D point clouds of forest structure from which aboveground carbon density (ACD) can be estimated. Area-based models are state of the art when it comes to estimating ACD from ALS data, but discard tree-level information contained within the ALS point cloud. This paper compares area based and tree-centric models for estimating ACD in lowland old-growth forests in Sabah, Malaysia. These forests are challenging to map because of their immense height. We compare the performance of (a) an area-based model developed by Asner and Mascaro (2014), and used primarily in the neotropics hitherto, with (b) a tree-centric approach that uses a new algorithm (itcSegment) to locate trees within the ALS canopy height model, measures their heights and crown widths, and calculates biomass from these dimensions. We find that Asner and Mascaro's model needed regional calibration, reflecting the distinctive structure of Southeast Asian forests. We also discover that forest basal area is closely related to canopy gap fraction measured by ALS, and use this finding to refine Asner and Mascaro's model. Finally, we show that our tree-centric approach is less accurate at estimating ACD than the best-performing area-based model (RMSE 18{\%} vs 13{\%}). Tree-centric modelling is appealing because it is based on summing the biomass of individual trees, but until algorithms can detect understory trees reliably and estimate biomass from crown dimensions precisely, areas-based modelling will remain the method of choice. (C) 2017 The Authors. Published by Elsevier Inc.},
author = {Coomes, David A and Dalponte, Michele and Jucker, Tommaso and Asner, Gregory P and Banin, Lindsay F and Burslem, David F R P and Lewis, Simon L and Nilus, Reuben and Phillips, Oliver L and Phua, Mui-How and Qie, Lan},
doi = {10.1016/j.rse.2017.03.017},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
month = {jun},
pages = {77--88},
title = {{Area-based vs tree-centric approaches to mapping forest carbon in Southeast Asian forests from airborne laser scanning data}},
volume = {194},
year = {2017}
}
@article{ISI:000411548000002,
abstract = {The microstructure of three different grades of WC-Co cemented carbides (hardmetals) has been reconstructed in three dimensions after sequential images obtained by focused ion beam. The three dimensional microstructual parameters are compared against the well-known two dimensional parameters of grain size, phase percentages and mean free path. Results show good agreement with the exception of individual grain recognition, which could not be univocally segmented. In the case of mean free path, the three-dimensional image depicts a more realistic description of the metal interconnections in the composite. Aiming for a simple example of direct application of these FIB tomography outcomes, reconstructed real microstructure for the coarser hardmetal grade studied was translated in a finite element modelling mesh, and elastic residual stresses were estimated from sintering to room temperature. Calculated thermal stresses agree with experimental results and show significant local variations in their value due to the complex microstructure of cemented carbides.},
author = {Jimenez-Pique, E and Turon-Vinas, M and Chen, H and Trifonov, T and Fair, J and Tarres, E and Llanes, L},
doi = {10.1016/j.ijrmhm.2017.04.007},
issn = {0263-4368},
journal = {INTERNATIONAL JOURNAL OF REFRACTORY METALS {\&} HARD MATERIALS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
pages = {9--17},
title = {{Focused ion beam tomography of WC-Co cemented carbides}},
volume = {67},
year = {2017}
}
@inproceedings{ISI:000417429000016,
abstract = {Fast and robust 3D reconstruction of facial geometric structure from a single image is a challenging task with numerous applications, but there exist two problems when applied ``in the wild{\{}''{\}}: the 3D estimates are unstable for different photos of the same subject; the 3D estimates are over-regularized and generic. In response, a robust method for regressing discriminative 3D morphable face models(3DMM) is described to support face recognition and 3D mask printing. Combining the local data sets with the public data sets, improving the exiting 3DMM fitting method and then using a convolutional neural network(CNN) to improve reconstruction effect. The ground truth 3D faces of the CNN are the pooled 3DMM parameters extracted from the photos of the same subject. Using CNN to regress 3DMM shape and texture parameters directly from an input photo and offering a method for generating huge numbers of labeled examples. There are two key points of the paper: one is the training data generation for the model training; the other is the training of 3D reconstruction model. Experimental results and analysis show that this method costs much less time than traditional methods of 3D face modeling, and it is improved for different races on photos with any angles than the existing methods based on deep learning, and the system has better robustness.},
annote = {10th International Conference on Intelligent Computation Technology and
Automation (ICICTA), Changsha, PEOPLES R CHINA, OCT 09-10, 2017},
author = {Fangmin, Li and Ke, Chen and Xinhua, Liu},
booktitle = {2017 10TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTATION TECHNOLOGY AND AUTOMATION (ICICTA 2017)},
doi = {10.1109/ICICTA.2017.23},
isbn = {978-1-5386-1230-9},
issn = {1949-1263},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {Changsha Univ Sci {\&} Technol, Commun Res Inst; Cent S Univ, Shenzhen Res Inst; Hunan City Coll, Dept Urban Management},
pages = {71--74},
series = {International Conference on Intelligent Computation Technology and Automation},
title = {{3D Face Reconstruction Based on Convolutional Neural Network}},
year = {2017}
}
@article{ISI:000423097800017,
abstract = {To discover specific variants with relatively large effects on the human face, we have devised an approach to identifying facial features with high heritability. This is based on using twin data to estimate the additive genetic value of each point on a face, as provided by a 3D camera system. In addition, we have used the ethnic difference between East Asian and European faces as a further source of face genetic variation. We use principal components (PCs) analysis to provide a fine definition of the surface features of human faces around the eyes and of the profile, and chose upper and lower 10{\%} extremes of the most heritable PCs for looking for genetic associations. Using this strategy for the analysis of 3D images of 1,832 unique volunteers from the well-characterized People of the British Isles study and 1,567 unique twin images from the TwinsUK cohort, together with genetic data for 500,000 SNPs, we have identified three specific genetic variants with notable effects on facial profiles and eyes.},
author = {Crouch, Daniel J M and Winney, Bruce and Koppen, Willem P and Christmas, William J and Hutnik, Katarzyna and Day, Tammy and Meena, Devendra and Boumertit, Abdelhamid and Hysi, Pirro and Nessa, Ayrun and Spector, Tim D and Kittler, Josef and Bodmer, Walter F},
doi = {10.1073/pnas.1708207114},
issn = {0027-8424},
journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jan},
number = {4},
pages = {E676--E685},
title = {{Genetics of the human face: Identification of large-effect single gene variants}},
volume = {115},
year = {2018}
}
@article{ISI:000387670700012,
abstract = {1. Forests are a major component of the global carbon cycle, and accurate estimation of forest carbon stocks and fluxes is important in the context of anthropogenic global change. Airborne laser scanning (ALS) data sets are increasingly recognized as outstanding data sources for high-fidelity mapping of carbon stocks at regional scales. 2. We develop a tree-centric approach to carbon mapping, based on identifying individual tree crowns (ITCs) and species from airborne remote sensing data, from which individual tree carbon stocks are calculated. We identify ITCs from the laser scanning point cloud using a region-growing algorithm and identifying species from airborne hyperspectral data by machine learning. For each detected tree, we predict stem diameter from its height and crown-width estimate. From that point on, we use well-established approaches developed for field-based inventories: above-ground biomasses of trees are estimated using published allometries and summed within plots to estimate carbon density. 3. We show this approach is highly reliable: tests in the Italian Alps demonstrated a close relationship between field-and ALS-based estimates of carbon stocks (r(2) = 0.98). Small trees are invisible from the air, and a correction factor is required to accommodate this effect. 4. An advantage of the tree-centric approach over existing area-based methods is that it can produce maps at any scale and is fundamentally based on field-based inventory methods, making it intuitive and transparent. Airborne laser scanning, hyperspectral sensing and computational power are all advancing rapidly, making it increasingly feasible to use ITC approaches for effective mapping of forest carbon density also inside wider carbon mapping programs like REDD++.},
author = {Dalponte, Michele and Coomes, David A},
doi = {10.1111/2041-210X.12575},
issn = {2041-210X},
journal = {METHODS IN ECOLOGY AND EVOLUTION},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {oct},
number = {10},
pages = {1236--1245},
title = {{Tree-centric mapping of forest carbon density from airborne laser scanning and hyperspectral data}},
volume = {7},
year = {2016}
}
@article{ISI:000360999400005,
abstract = {Face recognition is being widely accepted as a biometric technique because of its non-intrusive nature. Despite extensive research on 2-D face recognition, it suffers from poor recognition rate due to pose, illumination, expression, ageing, makeup variations and occlusions. In recent years, the research focus has shifted toward face recognition using 3-D facial surface and shape which represent more discriminating features by the virtue of increased dimensionality. This paper presents an extensive survey of recent 3-D face recognition techniques in terms of feature detection, classifiers as well as published algorithms that address expression and occlusion variation challenges followed by our critical comments on the published work. It also summarizes remarkable 3-D face databases and their features used for performance evaluation. Finally we suggest vital steps of a robust 3-D face recognition system based on the surveyed work and identify a few possible directions for research in this area.},
author = {Patil, Hemprasad and Kothari, Ashwin and Bhurchandi, Kishor},
doi = {10.1007/s10462-015-9431-0},
issn = {0269-2821},
journal = {ARTIFICIAL INTELLIGENCE REVIEW},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux8},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux8},
number = {3},
pages = {393--441},
title = {{3-D face recognition: features, databases, algorithms and challenges}},
volume = {44},
year = {2015}
}
@inproceedings{ISI:000406771302059,
abstract = {We aim to reconstruct an accurate neutral 3D face model from an RGB-D video in the presence of extreme expression changes. Since each depth frame, taken by a low-cost sensor, is noisy, point clouds from multiple frames can be registered and aggregated to build an accurate 3D model. However, direct aggregation of multiple data produces erroneous results in natural interaction (e.g., talking and showing expressions). We propose to analyze facial expression from an RGB frame and neutralize the corresponding 3D point cloud if needed. We first estimate the person's expression by fitting blend-shape coefficients using 2D facial landmarks for each frame and calculate an expression deformity (expression score). With the estimated expression score, we determine whether an input face is neutral or non-neutral. If the face is non-neutral, we proceed to neutralize the expression of the 3D point cloud in that frame. To neutralize the 3D point cloud of a face, we deform our generic 3D face model by applying the estimated blendshape coefficients, find displacement vectors from the deformed generic face to a neutral generic face, and apply the displacement vectors to the input 3D point cloud. After preprocessing frames in a video, we rank frames based on the expression scores and register the ranked frames into a single 3D model. Our system produces a neutral 3D face model in the presence of extreme expression changes even when neutral faces do not exist in the video.},
annote = {23rd International Conference on Pattern Recognition (ICPR), Mexican
Assoc Comp Vis Robot {\&} Neural Comp, Cancun, MEXICO, DEC 04-08, 2016},
author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni, Gerard},
booktitle = {2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)},
isbn = {978-1-5090-4847-2},
issn = {1051-4651},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
organization = {Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm; Elsevier; IBM Res; INTEL; CONACYT},
pages = {2362--2367},
series = {International Conference on Pattern Recognition},
title = {{Expression Invariant 3D Face Modeling from an RGB-D Video}},
year = {2016}
}
@article{7331662,
abstract = {The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.},
author = {v. Landesberger, T and Basgier, D and Becker, M},
doi = {10.1109/TVCG.2015.2501813},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {data analysis,data visualisation,ear,image segment,revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
month = {dec},
number = {12},
pages = {2537--2549},
title = {{Comparative Local Quality Assessment of 3D Medical Image Segmentations with Focus on Statistical Shape Model-Based Algorithms}},
volume = {22},
year = {2016}
}
@inproceedings{ISI:000405560700088,
abstract = {We propose a new biometric approach where the tissue thickness of a person's forehead is used as a biometric feature. Given that the spatial registration of two 3D laser scans of the same human face usually produces a low error value, the principle of point cloud registration and its error metric can be applied to human classification techniques. However, by only considering the spatial error, it is not possible to reliably verify a person's identity. We propose to use a novel near-infrared laser-based head tracking system to determine an additional feature, the tissue thickness, and include this in the error metric. Using MRI as a ground truth, data from the foreheads of 30 subjects was collected from which a 4D reference point cloud was created for each subject. The measurements from the near-infrared system were registered with all reference point clouds using the ICP algorithm. Afterwards, the spatial and tissue thickness errors were extracted, forming a 2D feature space. For all subjects, the lowest feature distance resulted from the registration of a measurement and the reference point cloud of the same person. The combined registration error features yielded two clusters in the feature space, one from the same subject and another from the other subjects. When only the tissue thickness error was considered, these clusters were less distinct but still present. These findings could help to raise safety standards for head and neck cancer patients and lays the foundation for a future human identification technique.},
annote = {Conference on Medical Imaging - Image-Guided Procedures, Robotic
Interventions, and Modeling, Orlando, FL, FEB 14-16, 2017},
author = {Manit, Jirapong and Bremer, Christina and Schweikard, Achim and Ernst, Floris},
booktitle = {MEDICAL IMAGING 2017: IMAGE-GUIDED PROCEDURES, ROBOTIC INTERVENTIONS, AND MODELING},
doi = {10.1117/12.2254963},
editor = {{Webster, RJ and Fei}, B},
isbn = {978-1-5106-0715-6; 978-1-5106-0716-3},
issn = {0277-786X},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
organization = {SPIE; Alpin Med Syst; Siemens Healthineers; No Digital Inc},
series = {Proceedings of SPIE},
title = {{Patient identification using a near-infrared lasers canner}},
volume = {10135},
year = {2017}
}
@inproceedings{ISI:000382326300052,
abstract = {This paper presents an approach that automatically (but parametrically) reconstructs 2-D/3-D building footprints using 3-D synthetic aperture radar (SAR) tomography (TomoSAR) point clouds. These point clouds are generated by processing SAR image stacks via SAR tomographic inversion. The proposed approach reconstructs the building outline by exploiting both the roof and facade points. Initial building footprints are derived by applying the alpha shapes method on pre-segmented point clusters of individual buildings. A recursive angular deviation based refinement is then carried out to obtain refined/smoothed 2-D polygonal boundaries. A robust fusion framework then fuses the information pertaining to building facades to the smoothed polygons. Afterwards, a rectilinear building identification procedure is adopted and constraints are added to yield geometrically correct and visually aesthetic building shapes. The proposed approach is illustrated and validated using TomoSAR point clouds generated from a stack of TerraSAR-X high-resolution spotlight images from ascending orbit covering approximately 1.5 km(2) area in the city of Berlin, Germany.},
annote = {ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015},
author = {Shahzad, M and Zhu, X X},
booktitle = {ISPRS GEOSPATIAL WEEK 2015},
doi = {10.5194/isprsannals-II-3-W5-385-2015},
editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Rottensteiner, F and Yang, M and Christophe, S and Coltekin, A and Bredif}, M},
issn = {2194-9034},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {W5},
organization = {WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging {\&} Sensors; WG VII 7 Synergy Radar; ISPRS},
pages = {385--392},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{RECONSTRUCTION OF BUILDING FOOTPRINTS USING SPACEBORNE TOMOSAR POINT CLOUDS}},
volume = {II-3},
year = {2015}
}
@inproceedings{ISI:000382327100086,
abstract = {The European FP7 project IQmulus yearly organizes several processing contests, where submissions are requested for novel algorithms for point cloud and other big geodata processing. This paper describes the set-up and execution of a contest having the purpose to evaluate state-of-the-art algorithms for Mobile Mapping System point clouds, in order to detect and identify (individual) trees. By the nature of MMS these are trees in the vicinity of the road network (rather than in forests). Therefore, part of the challenge is distinguishing between trees and other objects, such as buildings, street furniture, cars etc. Three submitted segmentation and classification algorithms are thus evaluated.},
annote = {ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015},
author = {Gorte, Ben and Elberink, Sander Oude and Sirmacek, Beril and Wang, Jinhu},
booktitle = {ISPRS GEOSPATIAL WEEK 2015},
doi = {10.5194/isprsarchives-XL-3-W3-607-2015},
editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane}, G},
issn = {2194-9034},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {W3},
organization = {WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging {\&} Sensors; WG VII 7 Synergy Radar; ISPRS},
pages = {607--612},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{IQPC 2015 TRACK: TREE SEPARATION AND CLASSIFICATION IN MOBILE MAPPING LIDAR DATA}},
volume = {40-3},
year = {2015}
}
@inproceedings{7533187,
abstract = {This paper reports on a novel application of computer vision and image processing technologies to an interdisciplinary project in architectural history that seeks to help identify and visualize differences between homologous buildings constructed to a common template design. By identifying the mutations in homologous buildings, we assist humanists in giving voice to the contributions of the myriad additional “authors” for these buildings beyond their primary designers. We develop a framework for comparing 3D point cloud representations of homologous buildings captured using lidar: focusing on identifying similarities and differences, both among 3D scans of different buildings and between the 3D scans and the design specifications of architectural drawings. The framework addresses global and local alignment for highlighting gross differences as well as differences in individual structural elements and provides methods for readily highlighting the differences via suitable visualizations. The framework is demonstrated on pairs of homologous buildings selected from the Canadian and Ottoman rail networks. Results demonstrate the utility of the framework confirming differences already apparent to the humanist researchers and also revealing new differences that were not previously observed.},
author = {Ding, L and Elliethy, A and Freedenberg, E and Wolf-Johnson, S A and Romphf, J and Christensen, P and Sharma, G},
booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7533187},
issn = {2381-8549},
keywords = {buildings (structures),design engineering,optical,revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
pages = {4378--4382},
title = {{Comparative analysis of homologous buildings using range imaging}},
year = {2016}
}
@inproceedings{ISI:000389381200037,
abstract = {In this paper, we propose a novel idea for automatic facial expression analysis with the aim of resolving the existing challenges in 2D images. The subtle combination of the geometry-based method with the appearance-based features in depth and color images contributes to increasing in distinguishable features among various facial expressions. Particular functions are utilised to calculate the correlation between expressions in order to determine the exact facial expression. Our approach consists of a sequence of steps including estimating the normal vector of facial surface, then extracting the geometric features such as the orientation of normal vector in the point cloud. The useful color information is known as LBP. According to the result of the experiment, we demonstrate that the effective fusion scheme of texture and shape feature on color and depth images. In comparison with the non fusion scheme, our fusion scheme has resulted in the increase of recognition under low and high illuminated light, about 19.84{\%} and 1.59{\%}, respectively.},
annote = {8th Asian Conference on Intelligent Information and Database Systems
(ACIIDS), Da Nang, VIETNAM, MAR 14-16, 2016},
author = {Truong, Trung and Ly, Ngoc},
booktitle = {Intelligent Information and Database Systems, ACIIDS 2016, Pt II},
doi = {10.1007/978-3-662-49390-8_37},
editor = {{Nguyen, NT and Trawinski, B and Fujita, H and Hong}, TP},
isbn = {978-3-662-49390-8; 978-3-662-49389-2},
issn = {0302-9743},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
organization = {Vietnam Korea Friendship Informat Technol Coll; Wroclaw Univ Technol; IEEE SMC Tech Comm Computat Collect Intelligence; Bina Nusantara Univ; Ton Duc Thang Univ; Quang Binh Univ},
pages = {377--387},
series = {Lecture Notes in Artificial Intelligence},
title = {{Building the Facial Expressions Recognition System Based on RGB-D Images in High Performance}},
volume = {9622},
year = {2016}
}
@inproceedings{Cohen:2018:BSB:3177148.3180081,
address = {New York, NY, USA},
author = {Cohen, Fernand S and Li, Chenxi},
booktitle = {Proceedings of the 2Nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
doi = {10.1145/3177148.3180081},
isbn = {978-1-4503-5290-1},
keywords = {3D building reconstruction,GPS,invariants,localization,revisao{\_}V1,revisao{\_}acm,salient features,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,tutux9},
pages = {44--51},
publisher = {ACM},
series = {MedPRAI '18},
title = {{3D Building Synthesis Based on Images and Affine Invariant Salient Features}},
url = {http://doi.acm.org/10.1145/3177148.3180081},
year = {2018}
}
@article{ISI:000398720100102,
abstract = {This paper presents an automated and effective framework for classifying airborne laser scanning (ALS) point clouds. The framework is composed of four stages: (i) step-wise point cloud segmentation, (ii) feature extraction, (iii) Random Forests (RF) based feature selection and classification, and (iv) post-processing. First, a step-wise point cloud segmentation method is proposed to extract three kinds of segments, including planar, smooth and rough surfaces. Second, a segment, rather than an individual point, is taken as the basic processing unit to extract features. Third, RF is employed to select features and classify these segments. Finally, semantic rules are employed to optimize the classification result. Three datasets provided by Open Topography are utilized to test the proposed method. Experiments show that our method achieves a superior classification result with an overall classification accuracy larger than 91.17{\%}, and kappa coefficient larger than 83.79{\%}.},
author = {Ni, Huan and Lin, Xiangguo and Zhang, Jixian},
doi = {10.3390/rs9030288},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
number = {3},
title = {{Classification of ALS Point Cloud with Improved Point Cloud Segmentation and Random Forests}},
volume = {9},
year = {2017}
}
@article{ISI:000355003600024,
abstract = {The technique of forensic facial approximation, or reconstruction, is one of many facets of the field of mummy studies. Although far from a rigorous scientific technique, evidence-based visualization of antemortem appearance may supplement radiological, chemical, histological, and epidemiological studies of ancient remains. Published guidelines exist for creating facial approximations, but few approximations are published with documentation of the specific process and references used. Additionally, significant new research has taken place in recent years which helps define best practices in the field. This case study records the facial approximation of a 3,000-year-old ancient Egyptian woman using medical imaging data and the digital sculpting program, ZBrush. It represents a synthesis of current published techniques based on the most solid anatomical and/or statistical evidence. Through this study, it was found that although certain improvements have been made in developing repeatable, evidence-based guidelines for facial approximation, there are many proposed methods still awaiting confirmation from comprehensive studies. This study attempts to assist artists, anthropologists, and forensic investigators working in facial approximation by presenting the recommended methods in a chronological and usable format. Anat Rec, 298:1144-1161, 2015. (c) 2015 Wiley Periodicals, Inc.},
author = {Lindsay, Kaitlin E and Ruehli, Frank J and Deleon, Valerie Burke},
doi = {10.1002/ar.23146},
issn = {1932-8486},
journal = {ANATOMICAL RECORD-ADVANCES IN INTEGRATIVE ANATOMY AND EVOLUTIONARY BIOLOGY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jun},
number = {6},
pages = {1144--1161},
title = {{Revealing the Face of an Ancient Egyptian: Synthesis of Current and Traditional Approaches to Evidence-Based Facial Approximation}},
volume = {298},
year = {2015}
}
@article{ISI:000456899900003,
abstract = {Building damage assessment is a critical task following major hurricane events. Use of remotely sensed data to support building damage assessment is a logical choice considering the difficulty of gaining ground access to the impacted areas immediately after hurricane events. However, a remote sensing based damage assessment approach is often only capable of detecting severely damaged buildings. In this study, an airborne LiDAR based approach is proposed to assess multi-level hurricane damage at the community scale. In the proposed approach, building clusters are first extracted using a density-based algorithm. A novel cluster matching algorithm is proposed to robustly match post-event and pre-event building clusters. Multiple features including roof area and volume, roof orientation, and roof shape are computed as building damage indicators. A hierarchical determination process is then employed to identify the extent of damage to each building object. The results of this study suggest that our proposed approach is capable of 1) recognizing building objects, 2) extracting damage features, and 3) characterizing the extent of damage to individual building properties.},
author = {Zhou, Zixiang and Gong, Jie and Hu, Xuan},
doi = {10.1016/j.autcon.2018.10.018},
issn = {0926-5805},
journal = {AUTOMATION IN CONSTRUCTION},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {30--45},
title = {{Community-scale multi-level post-hurricane damage assessment of residential buildings using multi-temporal airborne LiDAR data}},
volume = {98},
year = {2019}
}
@article{ISI:000356107200001,
abstract = {Building information models (BIMs) are increasingly being applied throughout a building's lifecycle for various applications, such as progressive construction monitoring and defect detection, building renovation, energy simulation, and building system analysis in the Architectural, Engineering, Construction, and Facility Management (AEC/FM) domains. In conventional approaches, as-is BIM is primarily manually created from point clouds, which is labor-intensive, costly, and time consuming. This paper proposes a method for automatically extracting building geometries from unorganized point clouds. The collected raw data undergo data downsizing, boundary detection, and building component categorization, resulting in the building components being recognized as individual objects and their visualization as polygons. The results of tests conducted on three collected as-is building data to validate the technical feasibility and evaluate the performance of the proposed method indicate that it can simplify and accelerate the as-is building model from the point cloud creation process. (C) 2015 Elsevier B.V. All rights reserved.},
author = {Wang, Chao and Cho, Yong K and Kim, Changwan},
doi = {10.1016/j.autcon.2015.04.001},
issn = {0926-5805},
journal = {AUTOMATION IN CONSTRUCTION},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {1--13},
title = {{Automatic BIM component extraction from point clouds of existing buildings for sustainability applications}},
volume = {56},
year = {2015}
}
@article{ISI:000372355200007,
abstract = {In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBPs) on a mesh for 3D face recognition. Using a recently proposed framework, we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared with its depth-image counterpart, our approach is distinguished by the following features: 1) inherits the intrinsic advantages of mesh surface (e.g., preservation of the full geometry); 2) does not require normalization; and 3) can accommodate partial matching. In addition, it allows early level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data, also in comparison to the state-of-the-art solutions.},
author = {Werghi, Naoufel and Tortorici, Claudio and Berretti, Stefano and {Del Bimbo}, Alberto},
doi = {10.1109/TIFS.2016.2515505},
issn = {1556-6013},
journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
number = {5},
pages = {964--979},
title = {{Boosting 3D LBP-Based Face Recognition by Fusing Shape and Texture Descriptors on the Mesh}},
volume = {11},
year = {2016}
}
@article{ISI:000451733800040,
abstract = {Bricks are the vital component of most masonry structures. Their maintenance is critical to the protection of masonry buildings. Terrestrial Light Detection and Ranging (TLidar) systems provide massive point cloud data in an accurate and fast way. TLidar enables us to sample and store the state of a brick surface in a practical way. This article aims to extract individual bricks from an unorganized pile of bricks sampled by a dense point cloud. The method automatically segments and models the individual bricks. The methodology is divided into five main steps: Filter needless points, brick boundary points removal, coarse segmentation using 3D component analysis, planar segmentation and grouping, and brick reconstruction. A novel voting scheme is used to segment the planar patches in an effective way. Brick reconstruction is based on the geometry of single brick and its corresponding nominal size (length, width and height). The number of bricks reconstructed is around 75{\%}. An accuracy assessment is performed by comparing 3D coordinates of the reconstructed vertices to the manually picked vertices. The standard deviations of differences along x, y and z axes are 4.55 mm, 4.53 mm and 4.60 mm, respectively. The comparison results indicate that the accuracy of reconstruction based on the introduced methodology is high and reliable. The work presented in this paper provides a theoretical basis and reference for large scene applications in brick-like structures. Meanwhile, the high-accuracy brick reconstruction lays the foundation for further brick displacement estimation.},
author = {Shen, Yueqian and Lindenbergh, Roderik and Wang, Jinguo and Ferreira, Vagner G},
doi = {10.3390/rs10111709},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
month = {nov},
number = {11},
title = {{Extracting Individual Bricks from a Laser Scan Point Cloud of an Unorganized Pile of Bricks}},
volume = {10},
year = {2018}
}
@article{ISI:000439703500027,
abstract = {Purpose: Modern 3-dimensional (3D) image acquisition systems represent a crucial technologic development in facial anatomy because of their accuracy and precision. The recently introduced portable devices can improve facial databases by increasing the number of applications. In the present study, the VECTRA H1 portable stereophotogrammetric device was validated to verify its applicability to 3D facial analysis. Materials and Methods: Fifty volunteers underwent 4 facial scans using portable VECTRA H1 and static VECTRA M3 devices (2 for each instrument). Repeatability of linear, angular, surface area, and volume measurements was verified within the device and between devices using the Bland-Altman test and the calculation of absolute and relative technical errors of measurement (TEM and rTEM, respectively). In addition, the 2 scans obtained by the same device and the 2 scans obtained by different devices were registered and superimposed to calculate the root mean square (RMS; point-to-point) distance between the 2 surfaces. Results: Most linear, angular, and surface area measurements had high repeatability in M3 versus M3, H1 versus H1, and M3 versus H1 comparisons (range, 82.2 to 98.7{\%}; TEM range, 0.3 to 2.0 mm, 0.4 degrees to 1.8 degrees; rTEM range, 0.2 to 3.1{\%}). In contrast, volumes and RMS distances showed evident differences in M3 versus M3 and H1 versus H1 comparisons and reached the maximum when scans from the 2 different devices were compared. Conclusion: The portable VECTRA H1 device proved reliable for assessing linear measurements, angles, and surface areas; conversely, the influence of involuntary facial movements on volumes and RMS distances was more important compared with the static device. (C) 2018 American Association of Oral and Maxillofacial Surgeons},
author = {Gibelli, Daniele and Pucciarelli, Valentina and Cappella, Annalisa and Dolci, Claudia and Sforza, Chiarella},
doi = {10.1016/j.joms.2018.01.021},
issn = {0278-2391},
journal = {JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux8},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux8},
month = {aug},
number = {8},
pages = {1772--1784},
title = {{Are Portable Stereophotogrammetric Devices Reliable in Facial Imaging? A Validation Study of VECTRA H1 Device}},
volume = {76},
year = {2018}
}
@inproceedings{ISI:000392743800052,
abstract = {Airborne LiDAR (Light Detection and Ranging) data have a high potential to provide 3D information from trees. Most proposed methods to extract individual trees detect points of tree top or bottom firstly and then using them as starting points in a segmentation algorithm. Hence, in these methods, the number and the locations of detected peak points heavily effect on the process of detecting individual trees. In this study, a new method is presented to extract individual tree segments using LiDAR points with 10cm point density. In this method, a two-step strategy is performed for the extraction of individual tree LiDAR points: finding deterministic segments of individual trees points and allocation of other LiDAR points based on these segments. This research is performed on two study areas in Zeebrugge, Bruges, Belgium (51.33 degrees N, 3.20 degrees E). The accuracy assessment of this method showed that it could correctly classified 74.51{\%} of trees with 21.57{\%} and 3.92{\%} under- and over-segmentation errors respectively.},
annote = {23rd Congress of the
International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
Prague, CZECH REPUBLIC, JUL 12-19, 2016},
author = {Moradi, A and Satari, M and Momeni, M},
booktitle = {XXIII ISPRS CONGRESS, COMMISSION III},
doi = {10.5194/isprsarchives-XLI-B3-337-2016},
editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla}, U},
issn = {2194-9034},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {B3},
organization = {Int Soc Photogrammetry {\&} Remote Sensing},
pages = {337--343},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{INDIVIDUAL TREE OF URBAN FOREST EXTRACTION FROM VERY HIGH DENSITY LIDAR DATA}},
volume = {41},
year = {2016}
}
@article{7587405,
abstract = {Using synthetic aperture radar (SAR) interferometry to monitor long-term millimeter-level deformation of urban infrastructures, such as individual buildings and bridges, is an emerging and important field in remote sensing. In the state-of-the-art methods, deformation parameters are retrieved and monitored on a pixel basis solely in the SAR image domain. However, the inevitable side-looking imaging geometry of SAR results in undesired occlusion and layover in urban area, rendering the current method less competent for a semantic-level monitoring of different urban infrastructures. This paper presents a framework of a semantic-level deformation monitoring by linking the precise deformation estimates of SAR interferometry and the semantic classification labels of optical images via a 3-D geometric fusion and semantic texturing. The proposed approach provides the first “SARptical” point cloud of an urban area, which is the SAR tomography point cloud textured with attributes from optical images. This opens a new perspective of InSAR deformation monitoring. Interesting examples on bridge and railway monitoring are demonstrated.},
author = {Wang, Y and Zhu, X X and Zeisl, B and Pollefeys, M},
doi = {10.1109/TGRS.2016.2554563},
issn = {0196-2892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {optical images,radar interferometry,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,synthetic aper,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {jan},
number = {1},
pages = {14--26},
title = {{Fusing Meter-Resolution 4-D InSAR Point Clouds and Optical Images for Semantic Urban Infrastructure Monitoring}},
volume = {55},
year = {2017}
}
@inproceedings{ISI:000457843609059,
abstract = {This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website.(1)},
annote = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Salt Lake City, UT, JUN 18-23, 2018},
author = {Li, Jiaxin and Chen, Ben M and Lee, Gim Hee},
booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2018.00979},
isbn = {978-1-5386-6420-9},
issn = {1063-6919},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IEEE; CVF; IEEE Comp Soc},
pages = {9397--9406},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{SO-Net: Self-Organizing Network for Point Cloud Analysis}},
year = {2018}
}
@article{ISI:000395521200012,
abstract = {Recognizing construction assets (e.g.,materials, equipment, labor) from point cloud data of construction environments provides essential information for engineering and management applications including progress monitoring, safety management, supply-chain management, and quality control. This study introduces a novel principal axes descriptor (PAD) for construction-equipment classification from point cloud data. Scattered as-is point clouds are first processed with downsampling, segmentation, and clustering steps to obtain individual instances of construction equipment. A geometric descriptor consisting of dimensional variation, occupancy distribution, shape profile, and plane counting features is then calculated to encode three-dimensional (3D) characteristics of each equipment category. Using the derived features, machine learning methods such as k-nearest neighbors and support vector machine are employed to determine class membership among major construction-equipment categories such as backhoe loader, bulldozer, dump truck, excavator, and front loader. Construction-equipment classification with the proposed PAD was validated using computer-aided design (CAD)-generated point clouds as training data and laser-scanned point clouds from an equipment yard as testing data. The recognition performance was further evaluated using point clouds from a construction site as well as a pose variation data set. PAD was shown to achieve a higher recall rate and lower computation time compared to competing 3D descriptors. The results indicate that the proposed descriptor is a viable solution for construction-equipment classification from point cloud data. (C) 2016 American Society of Civil Engineers.},
author = {Chen, Jingdao and Fang, Yihai and Cho, Yong K and Kim, Changwan},
doi = {10.1061/(ASCE)CP.1943-5487.0000628},
issn = {0887-3801},
journal = {JOURNAL OF COMPUTING IN CIVIL ENGINEERING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
month = {mar},
number = {2},
title = {{Principal Axes Descriptor for Automated Construction-Equipment Classification from Point Clouds}},
volume = {31},
year = {2017}
}
@inproceedings{ISI:000361841100052,
abstract = {We present a systematic study on the relationship between the 3D shape of a hand that is about to grasp an object and recognition of the object to be grasped. In this paper, we investigate the direction from the shape of the hand to object recognition for unimpaired users. Our work shows that the 3D shape of a grasping hand from an egocentric point of view can help improve recognition of the objects being grasped. Previous work has attempted to exploit hand interactions or gaze information in the egocentric setting to guide object segmentation. However, all such analyses are conducted in 2D. We hypothesize that the 3D shape of a grasping hand is highly correlated to the physical attributes of the object being grasped. Hence, it can provide very beneficial visual information for object recognition. We validate this hypothesis by first building a 3D, egocentric vision pipeline to segment and reconstruct dense 3D point clouds of the grasping hands. Then, visual descriptors are extracted from the point cloud and subsequently fed into an object recognition system to recognize the object being grasped. Our experiments demonstrate that the 3D hand shape can indeed greatly help improve the visual recognition accuracy, when compared with the baseline where only 2D image features are utilized.},
annote = {13th European Conference on Computer Vision (ECCV), Zurich, SWITZERLAND,
SEP 06-12, 2014},
author = {Lin, Yizhou and Hua, Gang and Mordohai, Philippos},
booktitle = {COMPUTER VISION - ECCV 2014 WORKSHOPS, PT III},
doi = {10.1007/978-3-319-16199-0_52},
editor = {{Agapito, L and Bronstein, MM and Rother}, C},
isbn = {978-3-319-16199-0; 978-3-319-16198-3},
issn = {0302-9743},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {746--762},
series = {Lecture Notes in Computer Science},
title = {{Egocentric Object Recognition Leveraging the 3D Shape of the Grasping Hand}},
volume = {8927},
year = {2015}
}
@inproceedings{ISI:000399004300032,
abstract = {Person identification using face as a cue is one of the most prominent and robust technique. This paper presents 3D face recognition system using Radial curves and Back Propagation Neural Networks (BPNN). The face images used for experimentation are under various challenges like illumination, pose variation, expression and occlusions. The features of images are extracted using Eigen vectors. These features are compared using radial curves on the face starting from center of the face to the end of the face. Each corresponding curve is matched using Euclidean Distance classifier. The BPNN is used to train the features for face matching. The proposed algorithms are tested on ORL and DMCE database. The performance analysis is based on recognition rate accuracy of the system. The proposed radial curve system yields recognition rate accuracy of 100 {\%} for images from the ORL database and 98 {\%} for the images from DMCE database.},
annote = {1st International Conference on Data Engineering and Communication
Technology (ICDECT), Christ Inst Management, Lavasa, INDIA, MAR 10-11,
2016},
author = {Keshwani, Latasha and Pete, Dnyandeo},
booktitle = {PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON DATA ENGINEERING AND COMMUNICATION TECHNOLOGY, ICDECT 2016, VOL 2},
doi = {10.1007/978-981-10-1678-3_32},
editor = {{Satapathy, SC and Bhateja, V and Joshi}, A},
isbn = {978-981-10-1678-3; 978-981-10-1677-6},
issn = {2194-5357},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {Aspire Res Fdn},
pages = {333--344},
series = {Advances in Intelligent Systems and Computing},
title = {{Comparative Analysis of Frontal Face Recognition Using Radial Curves and Back Propagation Neural Network}},
volume = {469},
year = {2017}
}
@article{ISI:000397013700050,
abstract = {Identifying individual trees and delineating their canopy structures from the forest point cloud data acquired by an airborne LiDAR (Light Detection And Ranging) has significant implications in forestry inventory. Once accurately identified, tree structural attributes such as tree height, crown diameter, canopy based height and diameter at breast height can be derived. This paper focuses on a novel computationally efficient method to adaptively calibrate the kernel bandwidth of a computational scheme based on mean shift-a non-parametric probability density-based clustering technique-to segment the 3D (three-dimensional) forest point clouds and identify individual tree crowns. The basic concept of this method is to partition the 3D space over each test plot into small vertical units (irregular columns containing 3D spatial features from one or more trees) first, by using a fixed bandwidth mean shift procedure and a small square grouping technique, and then rough estimation of crown sizes for distinct trees within a unit, based on an original 2D (two-dimensional) incremental grid projection technique, is applied to provide a basis for dynamical calibration of the kernel bandwidth for an adaptive mean shift procedure performed in each partition. The adaptive mean shift-based scheme, which incorporates our proposed bandwidth calibration method, is validated on 10 test plots of a dense, multi-layered evergreen broad-leaved forest located in South China. Experimental results reveal that this approach can work effectively and when compared to the conventional point-based approaches (e.g., region growing, k-means clustering, fixed bandwidth or multi-scale mean shift), its accuracies are relatively high: it detects 86 percent of the trees ({\{}''{\}}recall{\{}''{\}}) and 92 percent of the identified trees are correct ({\{}''{\}}precision{\{}''{\}}), showing good potential for use in the area of forest inventory.},
author = {Hu, Xingbo and Chen, Wei and Xu, Weiyang},
doi = {10.3390/rs9020148},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {2},
title = {{Adaptive Mean Shift-Based Identification of Individual Trees Using Airborne LiDAR Data}},
volume = {9},
year = {2017}
}
@inproceedings{Fischer:2015:AFI:2756601.2756619,
address = {New York, New York, USA},
author = {Fischer, Robert and Vielhauer, Claus},
booktitle = {Proceedings of the 3rd ACM Workshop on Information Hiding and Multimedia Security - IH{\&}MMSec '15},
doi = {10.1145/2756601.2756619},
isbn = {9781450335874},
keywords = {digital crime scene analysis,digitized forensics,firearm identification,firing pin shape matching,multiple slice shape,new forensic features,pattern classification,revisao{\_}V2,revisao{\_}acm,topography processing,tutux1},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux1},
pages = {161--171},
publisher = {ACM Press},
series = {IH{\&}{\#}38;MMSec '15},
title = {{Automated Firearm Identification}},
url = {http://doi.acm.org/10.1145/2756601.2756619 http://dl.acm.org/citation.cfm?doid=2756601.2756619},
year = {2015}
}
@inproceedings{Pollok:2018:NMD:3301506.3301542,
address = {New York, NY, USA},
author = {Pollok, Thomas},
booktitle = {Proceedings of the 2018 the 2Nd International Conference on Video and Image Processing},
doi = {10.1145/3301506.3301542},
isbn = {978-1-4503-6613-7},
keywords = {Crime Scene Investigation,Dataset,Mobile Camera,Multi-Camera Calibration,Stereo,Surveillance Camera,revisao{\_}V2,revisao{\_}acm,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux9},
pages = {171--175},
publisher = {ACM},
series = {ICVIP 2018},
title = {{A New Multi-Camera Dataset with Surveillance, Mobile and Stereo Cameras for Tracking, Situation Analysis and Crime Scene Investigation Applications}},
url = {http://doi.acm.org/10.1145/3301506.3301542},
year = {2018}
}
@article{ISI:000398720100091,
abstract = {In this paper, we present a novel framework for detecting individual trees in densely sampled 3D point cloud data acquired in urban areas. Given a 3D point cloud, the objective is to assign point-wise labels that are both class-aware and instance-aware, a task that is known as instance-level segmentation. To achieve this, our framework addresses two successive steps. The first step of our framework is given by the use of geometric features for a binary point-wise semantic classification with the objective of assigning semantic class labels to irregularly distributed 3D points, whereby the labels are defined as ``tree points{\{}''{\}} and ``other points{\{}''{\}}. The second step of our framework is given by a semantic segmentation with the objective of separating individual trees within the ``tree points{\{}''{\}}. This is achieved by applying an efficient adaptation of the mean shift algorithm and a subsequent segment-based shape analysis relying on semantic rules to only retain plausible tree segments. We demonstrate the performance of our framework on a publicly available benchmark dataset, which has been acquired with a mobile mapping system in the city of Delft in the Netherlands. This dataset contains 10.13 M labeled 3D points among which 17.6{\%} are labeled as ``tree points{\{}''{\}}. The derived results clearly reveal a semantic classification of high accuracy (up to 90.77{\%}) and an instance-level segmentation of high plausibility, while the simplicity, applicability and efficiency of the involved methods even allow applying the complete framework on a standard laptop computer with a reasonable processing time (less than 2.5 h).},
author = {Weinmann, Martin and Weinmann, Michael and Mallet, Clement and Bredif, Mathieu},
doi = {10.3390/rs9030277},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
month = {mar},
number = {3},
title = {{A Classification-Segmentation Framework for the Detection of Individual Trees in Dense MMS Point Cloud Data Acquired in Urban Areas}},
volume = {9},
year = {2017}
}
@article{ISI:000357545400014,
abstract = {The localization and reconstruction of individual trees as well as the extraction of their geometrical parameters is an important field of research in both forestry and remote sensing. While the current state-of-the-art mostly focuses on the exploitation of optical imagery and airborne LiDAR data, modern SAR sensors have not yet met the interest of the research community in that regard. This paper presents a prototypical processing chain for the reconstruction of individual deciduous trees: First, single-pass multi-baseline InSAR data acquired from multiple aspect angles are used for the generation of a layover- and shadow-free 3D point cloud by tomographic SAR processing. The resulting point cloud is then segmented by unsupervised mean shift clustering, before ellipsoid models are fitted to the points of each cluster. From these 3D ellipsoids the relevant geometrical tree parameters are extracted. Evaluation with respect to a manually derived reference dataset prove that almost 74{\%} of all trees are successfully segmented and reconstructed, thus providing a promising perspective for further research toward individual tree recognition from SAR data. (C) 2015 The Authors. Published by Elsevier Inc This is an open access article under the CC BY-NC-ND license.},
author = {Schmitt, Michael and Shahzad, Muhammad and Zhu, Xiao Xiang},
doi = {10.1016/j.rse.2015.05.012},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {175--185},
title = {{Reconstruction of individual trees from multi-aspect TomoSAR data}},
volume = {165},
year = {2015}
}
@article{ISI:000419961800018,
abstract = {We describe an approach for synthesizing a three-dimensional (3-D) face structure from an image or images of a human face taken at a priori unknown poses using gender and ethnicity specific 3-D generic models. The synthesis process starts with a generic model, which is personalized as images of the person become available using preselected landmark points that are tessellated to form a high-resolution triangular mesh. From a single image, two of the three coordinates of the model are reconstructed in accordance with the given image of the person, while the third coordinate is sampled from the generic model, and the appearance is made in accordance with the image. With multiple images, all coordinates and appearance are reconstructed in accordance with the observed images. This method allows for accurate pose estimation as well as face identification in 3-D rendering of a difficult two-dimensional (2-D) face recognition problem into a much simpler 3-D surface matching problem. The estimation of the unknown pose is achieved using the Levenberg-Marquardt optimization process. Encouraging experimental results are obtained in a controlled environment with high-resolution images under a good illumination condition, as well as for images taken in an uncontrolled environment under arbitrary illumination with low-resolution cameras. (C) 2017 SPIE and IS{\&}T},
author = {Liu, Zexi and Cohen, Fernand},
doi = {10.1117/1.JEI.26.6.063005},
issn = {1017-9909},
journal = {JOURNAL OF ELECTRONIC IMAGING},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
month = {nov},
number = {6},
title = {{Synthesis and identification of three-dimensional faces from image(s) and three-dimensional generic models}},
volume = {26},
year = {2017}
}
@article{ISI:000369200900006,
abstract = {Full-waveform airborne laser scanning (ALS) is a powerful tool for characterizing and monitoring forest structure over large areas at the individual tree level. Most of the existing ALS-based algorithms for individual tree delineation from the point cloud are top-down, which are accurate for delineating cone-shaped conifers, but have lower delineation accuracies over more structurally complex broad-leaf forests. Therefore, in this study we developed a new bottom-up algorithm for detecting trunks and delineating individual trees with complex shapes, such as eucalypts. Experiments were conducted in the largest river red gum forest in the world, located in the southeast of Australia, that experienced severe dieback over the past six decades. For detection of individual tree trunks, we used a novel approach based on conditional Euclidean distance clustering that takes advantage of spacing between laser returns. Overall, the algorithm developed in our study was able to detect up to 67{\%} of field-measured trees with diameter larger than or equal to 13 cm. By filtering ALS based on the intensity, return number and returned pulse width values, we were able to differentiate between woody and leaf tree components, thus improving the accuracy of tree trunk detections by 5{\%} as compared to non-filtered ALS. The detected trunks were used to seed random walks on graph algorithm for tree crown delineation. The accuracy of tree crown delineation for different ALS point cloud densities was assessed in terms of tree height and crown width and resulted in up to 68{\%} of field-measured trees being correctly delineated. The double increase in point density from similar to 12 points/m(2) to similar to 24 points/m(2) resulted in tree trunk detection increase of 11{\%} (from 56{\%} to 67{\%}) and percentage of correctly delineated crowns increase of 13{\%} (from 55{\%} to 68{\%}). Our results confirm an algorithm that can be used to accurately delineate individual trees with complex structures (e.g. eucalypts and other broad leaves) and highlight the importance of full-waveform ALS for individual tree delineation. (C) 2015 Elsevier Inc. All rights reserved.},
annote = {2014 ForestSAT Conference, Riva del Garda, ITALY, NOV 04-07, 2014},
author = {Shendryk, Iurii and Broich, Mark and Tulbure, Mirela G and Alexandrov, Sergey V},
doi = {10.1016/j.rse.2015.11.008},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {feb},
pages = {69--83},
title = {{Bottom-up delineation of individual trees from full-waveform airborne laser scans in a structurally complex eucalypt forest}},
volume = {173},
year = {2016}
}
@article{ISI:000357869200002,
abstract = {Topography affects forest canopy height retrieval based on airborne Light Detection and Ranging (LiDAR) data a lot. This paper proposes a method for correcting deviations caused by topography based on individual tree crown segmentation. The point cloud of an individual tree was extracted according to crown boundaries of isolated individual trees from digital orthophoto maps (DOMs). Normalized canopy height was calculated by subtracting the elevation of centres of gravity from the elevation of point cloud. First, individual tree crown boundaries are obtained by carrying out segmentation on the DOM. Second, point clouds of the individual trees are extracted based on the boundaries. Third, precise DEM is derived from the point cloud which is classified by a multi-scale curvature classification algorithm. Finally, a height weighted correction method is applied to correct the topological effects. The method is applied to LiDAR data acquired in South China, and its effectiveness is tested using 41 field survey plots. The results show that the terrain impacts the canopy height of individual trees in that the downslope side of the tree trunk is elevated and the upslope side is depressed. This further affects the extraction of the location and crown of individual trees. A strong correlation was detected between the slope gradient and the proportions of returns with height differences more than 0.3, 0.5 and 0.8 m in the total returns, with coefficient of determination R-2 of 0.83, 0.76, and 0.60 (n = 41), respectively.},
author = {Duan, Zhugeng and Zhao, Dan and Zeng, Yuan and Zhao, Yujin and Wu, Bingfang and Zhu, Jianjun},
doi = {10.3390/s150612133},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jun},
number = {6},
pages = {12133--12155},
title = {{Assessing and Correcting Topographic Effects on Forest Canopy Height Retrieval Using Airborne LiDAR Data}},
volume = {15},
year = {2015}
}
@inproceedings{ISI:000392743800092,
abstract = {Three dimensional models obtained from imagery have an arbitrary scale and therefore have to be scaled. Automatically scaling these models requires the detection of objects in these models which can be computationally intensive. Real-time object detection may pose problems for applications such as indoor navigation. This investigation poses the idea that relational cues, specifically height ratios, within indoor environments may offer an easier means to obtain scales for models created using imagery. The investigation aimed to show two things, (a) that the size of objects, especially the height off ground is consistent within an environment, and (b) that based on this consistency, objects can be identified and their general size used to scale a model. To test the idea a hypothesis is first tested on a terrestrial lidar scan of an indoor environment. Later as a proof of concept the same test is applied to a model created using imagery. The most notable finding was that the detection of objects can be more readily done by studying the ratio between the dimensions of objects that have their dimensions defined by human physiology. For example the dimensions of desks and chairs are related to the height of an average person. In the test, the difference between generalised and actual dimensions of objects were assessed. A maximum difference of 3.96{\%} (2.93cm) was observed from automated scaling. By analysing the ratio between the heights (distance from the floor) of the tops of objects in a room, identification was also achieved.},
annote = {23rd Congress of the
International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
Prague, CZECH REPUBLIC, JUL 12-19, 2016},
author = {Kadamen, Jayren and Sithole, George},
booktitle = {XXIII ISPRS CONGRESS, COMMISSION III},
doi = {10.5194/isprsarchives-XLI-B3-617-2016},
editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla}, U},
issn = {2194-9034},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {B3},
organization = {Int Soc Photogrammetry {\&} Remote Sensing},
pages = {617--624},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{AUTOMATICALLY DETERMINING SCALE WITHIN UNSTRUCTU RED POINT CLOUDS}},
volume = {41},
year = {2016}
}
@inproceedings{8358484,
abstract = {Emotions are an incredibly important aspect of human life. Research on emotion recognition for the past few decades have resulted in development of several fields. In the current scenario, it is necessary that machines/robots need to identify human emotions and respond accordingly. Applications in this field can be seen in security, entertainment and Human Machine Interface/Human Robot Interface. Recent works on 3D images have gained importance due to its accuracy in real life applications as emotions can be recognised at different head poses. The intention of this work has been to develop an algorithm for recognition of emotion from facial expressions, which recognizes 6 basic emotions, which are anger, fear, happy, disgust, sad and surprise from 3D images in 7 yaw angles (+45° to -45°) and 3 pitch angles (+15°,0°, -15°). Most of the reported work considers + yaw angles. While in the current work, both positive as well as negative pitch and yaw angles are considered. BU3DFE database is used for the implementation. The proposed method resulted in improved accuracy and is comparable with the literature.},
author = {Swetha, K M and Suja, P},
booktitle = {2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon)},
doi = {10.1109/SmartTechCon.2017.8358484},
keywords = {emotion recognition,face recognition,geometry,pose,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux7},
month = {aug},
pages = {805--809},
title = {{A geometric approach for recognizing emotions from 3D images with pose variations}},
year = {2017}
}
@inproceedings{ISI:000400012304105,
abstract = {Multilinear models are widely used to represent the statistical variations of 3D human faces as they decouple shape changes due to identity and expression. Existing methods to learn a multilinear face model degrade if not every person is captured in every expression, if face scans are noisy or partially occluded, if expressions are erroneously labeled, or if the vertex correspondence is inaccurate. These limitations impose requirements on the training data that disqualify large amounts of available 3D face data from being usable to learn a multilinear model. To overcome this, we introduce the first framework to robustly learn a multilinear model from 3D face databases with missing data, corrupt data, wrong semantic correspondence, and inaccurate vertex correspondence. To achieve this robustness to erroneous training data, our framework jointly learns a multilinear model and fixes the data. We evaluate our framework on two publicly available 3D face databases, and show that our framework achieves a data completion accuracy that is comparable to state-of-the-art tensor completion methods. Our method reconstructs corrupt data more accurately than state-of-the-art methods, and improves the quality of the learned model significantly for erroneously labeled expressions.},
annote = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Seattle, WA, JUN 27-30, 2016},
author = {Bolkart, Timo and Wuhrer, Stefanie},
booktitle = {2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2016.531},
isbn = {978-1-4673-8851-1},
issn = {1063-6919},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IEEE Comp Soc; Comp Vis Fdn},
pages = {4911--4919},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{A Robust Multilinear Model Learning Framework for 3D Faces}},
year = {2016}
}
@article{ISI:000391965900001,
abstract = {This paper presents a novel approach to recognize and estimate pose of the 3D objects in cluttered range images. The key technical breakthrough of the developed approach can enable robust object recognition and localization under undesirable condition such as environmental illumination variation as well as optical occlusion to viewing the object partially. First, the acquired point clouds are segmented into individual object point clouds based on the developed 3D object segmentation for randomly stacked objects. Second, an efficient shape-matching algorithm called Sub-OBB based object recognition by using the proposed oriented bounding box (OBB) regional area-based descriptor is performed to reliably recognize the object. Then, the 3D position and orientation of the object can be roughly estimated by aligning the OBB of segmented object point cloud with OBB of matched point cloud in a database generated from CAD model and 3D virtual camera. To detect accurate pose of the object, the iterative closest point (ICP) algorithm is used to match the object model with the segmented point clouds. From the feasibility test of several scenarios, the developed approach is verified to be feasible for object pose recognition and localization.},
author = {Hoang, Dinh-Cuong and Chen, Liang-Chia and Nguyen, Thanh-Hung},
doi = {10.1088/1361-6501/aa513a},
issn = {0957-0233},
journal = {MEASUREMENT SCIENCE AND TECHNOLOGY},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {2},
title = {{Sub-OBB based object recognition and localization algorithm using range images}},
volume = {28},
year = {2017}
}
@article{ISI:000349308600007,
abstract = {Three-dimensional (3D) facial data offer the potential to overcome the difficulties caused by the variation of head pose and illumination in 2D face recognition. In 3D face recognition, localisation of nose tip is essential to face normalisation, face registration and pose correction etc. Most of the existing methods of nose tip detection on 3D face deal mainly with frontal or near-frontal poses or are rotation sensitive. Many of them are training-based or model-based. In this study, a novel method of nose tip detection is proposed. Using pose-invariant differential surface features - high-order and low-order curvatures, it can detect nose tip on 3D faces under various poses automatically and accurately. Moreover, it does not require training and does not depend on any particular model. Experimental results on GavabDB verify the robustness and accuracy of the proposed method.},
author = {Li, Ye and Wang, YingHui and Wang, BingBo and Sui, LianSheng},
doi = {10.1049/iet-cvi.2014.0070},
issn = {1751-9632},
journal = {IET COMPUTER VISION},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {1},
pages = {75--84},
title = {{Nose tip detection on three-dimensional faces using pose-invariant differential surface features}},
volume = {9},
year = {2015}
}
@inproceedings{7292772,
abstract = {Facial expression recognition plays a major role in non verbal communication. Recognition by machine is still a challenging problem. To automate the recognition for human machine interaction, a system is proposed in this paper. The proposed system uses shape descriptors to identify twelve land marks which mainly contribute to the facial expression recognition. From the location and the size or boundary of the land marks by matching with Facial Landmark Model (FLM), basic expressions are identified. The experimental results show that the shape descriptors and post processing correctly identifies landmarks automatically. The architectural distortion of action units is used to identify the basic facial expressions and tested on Bosphorous data set.},
author = {Sindhuja, C and Mala, K},
booktitle = {2015 International Conference on Computing and Communications Technologies (ICCCT)},
doi = {10.1109/ICCCT2.2015.7292772},
keywords = {emotion recognition,face recognition,landmark iden,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux7},
month = {feb},
pages = {338--343},
title = {{Landmark identification in 3D image for facial expression recognition}},
year = {2015}
}
@inproceedings{ISI:000446394502083,
abstract = {This work proposes a process for efficiently searching over combinations of individual object 6D pose hypotheses in cluttered scenes, especially in cases involving occlusions and objects resting on each other. The initial set of candidate object poses is generated from state-of-the-art object detection and global point cloud registration techniques. The best scored pose per object by using these techniques may not be accurate due to overlaps and occlusions. Nevertheless, experimental indications provided in this work show that object poses with lower ranks may be closer to the real poses than ones with high ranks according to registration techniques. This motivates a global optimization process for improving these poses by taking into account scene-level physical interactions between objects. It also implies that the Cartesian product of candidate poses for interacting objects must be searched so as to identify the best scene-level hypothesis. To perform the search efficiently, the candidate poses for each object are clustered so as to reduce their number but still keep a sufficient diversity. Then, searching over the combinations of candidate object poses is performed through a Monte Carlo Tree Search (MCTS) process that uses the similarity between the observed depth image of the scene and a rendering of the scene given the hypothesized pose as a score that guides the search procedure. MCTS handles in a principled way the tradeoff between fine-tuning the most promising poses and exploring new ones, by using the Upper Confidence Bound (UCB) technique. Experimental results indicate that this process is able to quickly identify in cluttered scenes physically-consistent object poses that are significantly closer to ground truth compared to poses found by point cloud registration methods.},
annote = {IEEE International Conference on Robotics and Automation (ICRA),
Brisbane, AUSTRALIA, MAY 21-25, 2018},
author = {Mitash, Chaitanya and Boularias, Abdeslam and Bekris, Kostas E},
booktitle = {2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
isbn = {978-1-5386-3081-5},
issn = {1050-4729},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
organization = {IEEE; CSIRO; Australian Govt, Dept Def Sci {\&} Technol; DJI; Queensland Univ Technol; Woodside; Baidu; Bosch; Houston Mechatron; Kinova Robot; KUKA; Hit Robot Grp; Honda Res Inst; iRobot; Mathworks; NuTonomy; Ouster; Uber},
pages = {3331--3338},
series = {IEEE International Conference on Robotics and Automation ICRA},
title = {{Improving 6D Pose Estimation of Objects in Clutter via Physics-aware Monte Carlo Tree Search}},
year = {2018}
}
@article{ElSayed2017393,
abstract = {Face detection has an essential role in many applications. In this paper, we propose an efficient and robust method for face detection on a 3D point cloud represented by a weighted graph. This method classifies graph vertices as skin and non-skin regions based on a data mining predictive model. Then, the saliency degree of vertices is computed to identify the possible candidate face features. Finally, the matching between non-skin regions representing eyes, mouth and eyebrows and salient regions is done by detecting collisions between polytopes, representing these two regions. This method extracts faces from situations where pose variation and change of expressions can be found. The robustness is showed through different experimental results. Moreover, we study the stability of our method according to noise. Furthermore, we show that our method deals with 2D images. {\textcopyright} 2017 The Royal Photographic Society.},
annote = {cited By 0},
author = {{El Sayed}, A R and {El Chakik}, A and Alabboud, H and Yassine, A},
doi = {10.1080/13682199.2017.1358528},
journal = {Imaging Science Journal},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {7},
pages = {393--408},
title = {{3D face detection based on salient features extraction and skin colour detection using data mining}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028655269{\&}doi=10.1080{\%}2F13682199.2017.1358528{\&}partnerID=40{\&}md5=e5e603adbb5733b9bb5cace2dfe18ffc},
volume = {65},
year = {2017}
}
@article{ISI:000399118600008,
abstract = {The vulnerability of face recognition systems to presentation attacks (also known as direct attacks or spoof attacks) has received a great deal of interest from the biometric community. The rapid evolution of face recognition systems into real-time applications has raised new concerns about their ability to resist presentation attacks, particularly in unattended application scenarios such as automated border control. The goal of a presentation attack is to subvert the face recognition system by presenting a facial biometric artifact. Popular face biometric artifacts include a printed photo, the electronic display of a facial photo, replaying video using an electronic display, and 3D face masks. These have demonstrated a high security risk for state-of-the-art face recognition systems. However, several presentation attack detection (PAD) algorithms (also known as countermeasures or antispoofing methods) have been proposed that can automatically detect and mitigate such targeted attacks. The goal of this survey is to present a systematic overview of the existing work on face presentation attack detection that has been carried out. This paper describes the various aspects of face presentation attacks, including different types of face artifacts, state-of-the-art PAD algorithms and an overview of the respective research labs working in this domain, vulnerability assessments and performance evaluation metrics, the outcomes of competitions, the availability of public databases for benchmarking new PAD algorithms in a reproducible manner, and finally a summary of the relevant international standardization in this field. Furthermore, we discuss the open challenges and future work that need to be addressed in this evolving field of biometrics.},
author = {Ramachandra, Raghavendra and Busch, Christoph},
doi = {10.1145/3038924},
issn = {0360-0300},
journal = {ACM COMPUTING SURVEYS},
keywords = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutux8},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutux8},
number = {1},
title = {{Presentation Attack Detection Methods for Face Recognition Systems: A Comprehensive Survey}},
volume = {50},
year = {2017}
}
@article{ISI:000442238900004,
abstract = {In this study, a framework for view-invariant gait recognition on the basis of markerless motion tracking and dynamic time warping (DTW) transform is presented. The system consists of a proposed markerless motion capture system as well as introduced classification method of mocap data. The markerless system estimates the three-dimensional locations of skeleton driven joints. Such skeleton-driven point clouds represent poses over time. The authors align point clouds in every pair of frames by calculating the minimal sum of squared distances between the corresponding joints. A point cloud distance measure with temporal context has been utilised in k-nearest neighbours algorithm to compare time instants of motion sequences. To enhance the generalisation of the recognition and to shorten the processing time, for every individual a single multidimensional time series among several multidimensional time series describing the individual's gait is established. The correct classification rate has been determined on the basis of a real dataset of human gait. It contains 230 gait cycles of 22 subjects. The tracking results on the basis of markerless motion capture are referenced to Vicon system, whereas the achieved accuracies of recognition are compared with the ones obtained by DTW that is based on rotational data.},
author = {Switonski, Adam and Krzeszowski, Tomasz and Josinski, Henryk and Kwolek, Bogdan and Wojciechowski, Konrad},
doi = {10.1049/iet-bmt.2017.0134},
issn = {2047-4938},
journal = {IET BIOMETRICS},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {5},
pages = {415--422},
title = {{Gait recognition on the basis of markerless motion tracking and DTW transform}},
volume = {7},
year = {2018}
}
@article{ISI:000385150400002,
abstract = {3D image registration aims at aligning two 3D data sets in a common coordinate system, which has been widely used in computer vision, pattern recognition and computer assisted surgery. One challenging problem in 3D registration is that point-wise correspondences between two point sets are often unknown apriori. In this work, we develop an automatic algorithm for 3D maxillofacial models registration including facial surface model and skull model. Our proposed registration algorithm can achieve a good alignment result between partial and whole maxillofacial model in spite of ambiguous matching, which has a potential application in the oral and maxillofacial reparative and reconstructive surgery. The proposed algorithm includes three steps: (1) 3D-SIFT features extraction and FPFH descriptors construction; (2) feature matching using SAC-IA; (3) coarse rigid alignment and refinement by ICP. Experiments on facial surfaces and mandible skull models demonstrate the efficiency and robustness of our algorithm.},
author = {Qiu, Luwen and Zhou, Zhongwei and Guo, Jixiang and Lv, Jiancheng},
doi = {10.1007/s13319-016-0083-x},
issn = {2092-6731},
journal = {3D RESEARCH},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
number = {3},
title = {{An Automatic Registration Algorithm for 3D Maxillofacial Model}},
volume = {7},
year = {2016}
}
@article{ISI:000430653400001,
abstract = {Purpose: To study an original 3D visualization of head and neck squamous cell carcinoma extending to the mandible by using {\{}[{\}}18F]-NaF PET/CT and {\{}[{\}}18F]-FDG PET/CT imaging along with a new innovative FDG and NaF image analysis using dedicated software. The main interest of the 3D evaluation is to have a better visualization of bone extension in such cancers and that could also avoid unsatisfying surgical treatment later on. Patients and methods: A prospective study was carried out from November 2016 to September 2017. Twenty patients with head and neck squamous cell carcinoma extending to the mandible (stage 4 in the UICC classification) underwent {\{}[{\}}18F]-NaF and {\{}[{\}}18F]-FDG PET/CT. We compared the delineation of 3D quantification obtained with {\{}[{\}}18F]-NaF and {\{}[{\}}18F]-FDG PET/CT. In order to carry out this comparison, a method of visualisation and quantification of PET images was developed. This new approach was based on a process of quantification of radioactive activity within the mandibular bone that objectively defined the significant limits of this activity on PET images and on a 3D visualization. Furthermore, the spatial limits obtained by analysis of the PET/CT 3D images were compared to those obtained by histopathological examination of mandibular resection which confirmed intraosseous extension to the mandible. Results: The {\{}[{\}}18F]-NaF PET/CT imaging confirmed the mandibular extension in 85{\%} of cases and was not shown in {\{}[{\}}18F]-FDG PET/CT imaging. The {\{}[{\}}18F]-NaF PET/CT was significantly more accurate than {\{}[{\}}18F]-FDG PET/CT in 3D assessment of intraosseous extension of head and neck squamous cell carcinoma. This new 3D information shows the importance in the imaging approach of cancers. All cases of mandibular extension suspected on {\{}[{\}}18F]-NaF PET/CT imaging were confirmed based on histopathological results as a reference. Conclusions: The {\{}[{\}}18F]-NaF PET/CT 3D visualization should be included in the pre-treatment workups of head and neck cancers. With the use of a dedicated software which enables objective delineation of radioactive activity within the bone, it gives a very encouraging results. The {\{}[{\}}18F]-FDG PET/CT appears insufficient to confirm mandibular extension. This new 3D simulation management is expected to avoid under treatment of patients with intraosseous mandibular extension of head and neck cancers. However, there is also a need for a further study that will compare the interest of PET/CT and PET/MRI in this indication. (C) 2018 European Association for Cranio-Maxillo-Facial Surgery. Published by Elsevier Ltd. All rights reserved.},
author = {Lopez, R and Gantet, P and Julian, A and Hitzel, A and Herbault-Barres, B and Alshehri, S and Payoux, P},
doi = {10.1016/j.jcms.2018.02.007},
issn = {1010-5182},
journal = {JOURNAL OF CRANIO-MAXILLOFACIAL SURGERY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {may},
number = {5},
pages = {743--748},
title = {{Value of PET/CT 3D visualization of head and neck squamous cell carcinoma extended to mandible}},
volume = {46},
year = {2018}
}
@article{ISI:000350839700012,
abstract = {The tendency of particles to aggregate depends on particle-particle and particle-fluid interactions. These interactions can be characterized but it requires accurate 3D measurements of particle distributions. We introduce the application of an off-axis digital holographic microscopy for measuring distributions of dense micrometer (2 mu m) particles in a liquid solution. We demonstrate that digital holographic microscopy is capable of recording the instantaneous 3D position of particles in a flow volume. A new reconstruction method that aids identification of particle images was used in this work. About 62{\%} of the expected number of particles within the interrogated flow volume was detected. Based on the 3D position of individual particles, the tendency of particle to aggregate is investigated. Results show that relatively few particles (around 5-10 of a cohort of 1500) were aggregates. This number did not change significantly with time. (C) 2014 Elsevier Ltd. All rights reserved.},
author = {Tamrin, K F and Rahmatullah, B and Samuri, S M},
doi = {10.1016/j.optlaseng.2014.12.011},
issn = {0143-8166},
journal = {OPTICS AND LASERS IN ENGINEERING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {may},
pages = {93--103},
title = {{An experimental investigation of three-dimensional particle aggregation using digital holographic microscopy}},
volume = {68},
year = {2015}
}
@article{ISI:000407961700001,
abstract = {We introduce a method for modeling a configuration of objects in 2D or 3D images using a mathematical ``skeletal linking structure{\{}''{\}} which will simultaneously capture the individual shape features of the objects and their positional information relative to one another. The objects may either have smooth boundaries and be disjoint from the others or share common portions of their boundaries with other objects in a piecewise smooth manner. These structures include a special class of ``Blum medial linking structures{\{}''{\}}, which are intrinsically associated to the configuration and build upon the Blum medial axes of the individual objects. We give a classification of the properties of Blum linking structures for generic configurations. The skeletal linking structures add increased flexibility for modeling configurations of objects by relaxing the Blum conditions and they extend in a minimal way the individual ``skeletal structures{\{}''{\}} which have been previously used for modeling individual objects and capturing their geometric properties. This allows for the mathematical methods introduced for single objects to be significantly extended to the entire configuration of objects. These methods not only capture the internal shape structures of the individual objects but also the external structure of the neighboring regions of the objects. In the subsequent second paper (Damon and Gasparovic in Shape and positional geometry of multi-object configurations) we use these structures to identify specific external regions which capture positional information about neighboring objects, and we develop numerical measures for closeness of portions of objects and their significance for the configuration. This allows us to use the same mathematical structures to simultaneously analyze both the shape properties of the individual objects and positional properties of the configuration. This provides a framework for analyzing the statistical properties of collections of similar configurations such as for applications to medical imaging.},
author = {Damon, James and Gasparovic, Ellen},
doi = {10.1007/s11263-017-1019-5},
issn = {0920-5691},
journal = {INTERNATIONAL JOURNAL OF COMPUTER VISION},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
number = {3},
pages = {255--272},
title = {{Modeling Multi-object Configurations via Medial/Skeletal Linking Structures}},
volume = {124},
year = {2017}
}
@inproceedings{7351535,
abstract = {In this work, we propose a generative way of modeling faces, where the 3D shape of a face is generated by a supervised learning procedure involving coupled sparse feature learning. To learn dictionaries using the proposed method, we use the USF-HUMAN ID database [1]. We provide as input to our training system, paired correspondences of 2D and 3D images of individuals and aim to learn the low-level patches both in 2D and 3D domains that describe the corresponding subspaces in a sparse manner. We demonstrate the efficacy of our method by quantitative results on the 3D database and qualitative results on images drawn from the internet.},
author = {Sankaranarayanan, S and Patel, V M and Chellappa, R},
booktitle = {2015 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2015.7351535},
keywords = {face recognition,learning (artificial intelligence,lerdepois,revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
pages = {3896--3900},
title = {{3D facial model synthesis using coupled dictionaries}},
year = {2015}
}
@inproceedings{ISI:000406996500085,
abstract = {In this work, we address the problem of human skeleton estimation when multiple depth cameras are available. We propose a system that takes advantage of the knowledge of the camera poses to create a collaborative virtual depth image of the person in the scene which consists of points from all the cameras and that represents the person in a frontal pose. This depth image is fed as input to the open-source body part detector in the Point Cloud Library. A further contribution of this work is the improvement of this detector obtained by introducing two new components: as a pre-processing, a people detector is applied to remove the background from the depth map before estimating the skeleton, while an alpha-beta tracking is added as a post-processing step for filtering the obtained joint positions over time. The overall system has been proven to effectively improve the skeleton estimation on two sequences of people in different poses acquired from two first-generation Microsoft Kinect.},
annote = {14th International Conference on Intelligent Autonomous Systems (IAS),
Shanghai Jiao Tong Univ, Shanghai, PEOPLES R CHINA, JUL 03-07, 2016},
author = {Carraro, Marco and Munaro, Matteo and Roitberg, Alina and Menegatti, Emanuele},
booktitle = {INTELLIGENT AUTONOMOUS SYSTEMS 14},
doi = {10.1007/978-3-319-48036-7_85},
editor = {{Chen, W and Hosoda, K and Menegatti, E and Shimizu, M and Wang}, H},
isbn = {978-3-319-48036-7; 978-3-319-48035-0},
issn = {2194-5357},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {1155--1167},
series = {Advances in Intelligent Systems and Computing},
title = {{Improved Skeleton Estimation by Means of Depth Data Fusion from Multiple Depth Cameras}},
volume = {531},
year = {2017}
}
@inproceedings{7823978,
abstract = {This paper presents a multimodal face recognition using spectral transformation by Local Binary Pattern (LBP) and Polynomial Coefficients. Here 2D image and 3D image are combined to get multimodal face recognition. In this method a novel feature extraction is done using LBP and Polynomial Coefficients. Then these features are spectrally transformed using Discrete Fourier Transform (DFT). These spectrally transformed features extracted from texture image using the two methods are combined at the score level. Similarly this is done in depth image. Finally feature information from texture and depth are combined at the score level which gives better results than the individual results.},
author = {Naveen, S and Ahalya, R K and Moni, R S},
booktitle = {2016 International Conference on Communication Systems and Networks (ComNet)},
doi = {10.1109/CSN.2016.7823978},
keywords = {discrete Fourier transforms,face recognition,featu,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux5},
month = {jul},
pages = {13--17},
title = {{Multimodal face recognition using spectral transformation by LBP and polynomial coefficients}},
year = {2016}
}
@inproceedings{Kheffache:2015:MNG:2820926.2820939,
abstract = {Multiverse scenarios in cosmology assume that other universes exist "beyond" our own universe. They are an exciting challenge both for empirical and theoretical research as well as for philosophy of science. They could be necessary to understand why the big bang occurred, why (some of) the laws of nature and the values of certain physical constants are the way they are, and why there is an arrow of time. This essay clarifies competing notions of "universe" and "multiverse"; it proposes a classification of different multiverse types according to various aspects how the universes are or are not separated from each other; it reviews the main reasons for assuming the existence of other universes: empirical evidence, theoretical explanation, and philosophical arguments. The Himalayan Physics Vol. 5, No. 5, Nov. 2014 Page: 109-111},
address = {New York, New York, USA},
author = {Baral, Sanjaya and Chhetri, Dhurba},
booktitle = {Himalayan Physics},
doi = {10.3126/hj.v5i0.12887},
isbn = {9781450339261},
issn = {2542-2545},
keywords = {revisao{\_}V2,revisao{\_}acm,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux9},
pages = {109--111},
publisher = {ACM Press},
series = {SA '15},
title = {{Multiverse}},
url = {http://doi.acm.org/10.1145/2820926.2820939 http://dl.acm.org/citation.cfm?doid=2820926.2820939},
volume = {5},
year = {2015}
}
@inproceedings{8003592,
abstract = {This paper is about detection and tracking a person by mobile robots in in-door environments, such as shopping center and hospital. It uses vision based approaches to recognize texture of clothes. The paper proposes a method to use depth (distance) reference along with scale invariant features (SIFT) to recognize patterns in various orientation, distance and illumination. SIFT is an important feature detection algorithm that is robust against rotation, translation, and scaling in 2D images and to some extent against variations in lighting conditions. But it suffers inadequate performance for visual patterns rotated in 3D space. To overcome this issue, reference inputs given to the algorithm was extended to include images taken from different angles. The proposed algorithm showed considerably improved performance in detection for real-time applications.},
author = {Asl, A S and Oskoei, M A},
booktitle = {2017 5th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)},
doi = {10.1109/CFIS.2017.8003592},
keywords = {feature extraction,image sensors,image texture,mob,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {mar},
pages = {29--34},
title = {{Depth dependent invariant features applied to person detection using 3D camera}},
year = {2017}
}
@article{ISI:000384777300027,
abstract = {The extraction of ground points and breaklines is a crucial step during generation of high quality digital elevation models (DEMs) from airborne LiDAR point clouds. In this study, we propose a novel automated method for this task. To overcome the disadvantages of applying a single filtering method in areas with various types of terrain, the proposed method first classifies the points into a set of segments and one set of individual points, which are filtered by segment-based filtering and multi-scale morphological filtering, respectively. In the process of multi-scale morphological filtering, the proposed method removes amorphous objects from the set of individual points to decrease the effect of the maximum scale on the filtering result. The proposed method then extracts the breaklines from the ground points, which provide a good foundation for generation of a high quality DEM. Finally, the experimental results demonstrate that the proposed method extracts ground points in a robust manner while preserving the breaklines. (C) 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.},
author = {Yang, Bisheng and Huang, Ronggang and Dong, Zhen and Zang, Yufu and Li, Jianping},
doi = {10.1016/j.isprsjprs.2016.07.002},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
pages = {373--389},
title = {{Two-step adaptive extraction method for ground points and breaklines from lidar point clouds}},
volume = {119},
year = {2016}
}
@article{ISI:000369518500015,
abstract = {In this work, we present a robust face authentication approach merging multiple descriptors and exploiting both 3D and 2D information. First, we correct the heads rotation in 3D by iterative closest point algorithm, followed by an efficient preprocessing phase. Then, we extract different features namely: multi-scale local binary patterns (MSLBP), novel statistical local features (SLF), Gabor wavelets, and scale invariant feature transform (SIFT). The principal component analysis followed by enhanced fisher linear discriminant model is used for dimensionality reduction and classification. Finally, fusion at the score level is carried out using two-class support vector machines. Extensive experiments are conducted on the CASIA 3D faces database. The evaluation of individual descriptors clearly showed the superiority of the proposed SLF features. In addition, applying the (3D + 2D) multimodal score level fusion, the best result is obtained by combining the SLF with the MSLBP + SIFT descriptor yielding in an equal error rate of 0.98{\%} and a recognition rate of RR = 97.22 {\%}.},
author = {Ouamane, A and Belahcene, M and Benakcha, A and Bourennane, S and Taleb-Ahmed, A},
doi = {10.1007/s11760-014-0712-x},
issn = {1863-1703},
journal = {SIGNAL IMAGE AND VIDEO PROCESSING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
month = {jan},
number = {1},
pages = {129--137},
title = {{Robust multimodal 2D and 3D face authentication using local feature fusion}},
volume = {10},
year = {2016}
}
@article{7312454,
abstract = {In this paper, we present a novel approach to automatic 3D facial landmarking using 2D Gabor wavelets. Our algorithm considers the face to be a surface and uses map projections to derive 2D features from raw data. Extracted features include texture, relief map, and transformations thereof. We extend an established 2D landmarking method for simultaneous evaluation of these data. The method is validated by performing landmarking experiments on two data sets using 21 landmarks and compared with an active shape model implementation. On average, landmarking error for our method was 1.9 mm, whereas the active shape model resulted in an average landmarking error of 2.3 mm. A second study investigating facial shape heritability in related individuals concludes that automatic landmarking is on par with manual landmarking for some landmarks. Our algorithm can be trained in 30 min to automatically landmark 3D facial data sets of any size, and allows for fast and robust landmarking of 3D faces.},
author = {de Jong, M A and Wollstein, A and Ruff, C and Dunaway, D and Hysi, P and Spector, T and Liu, F and Niessen, W and Koudstaal, M J and Kayser, M and Wolvius, E B and B{\"{o}}hringer, S},
doi = {10.1109/TIP.2015.2496183},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Automated,Gabor filters,Pattern Recognition,Three-Dimensional,Wavelet Analysis,face recognition,fatima,feature extraction,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {fatima,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {2},
pages = {580--588},
title = {{An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}},
volume = {25},
year = {2016}
}
@article{ISI:000348880300019,
abstract = {In this paper, we present a fully automated multimodal Curvelet-based approach for textured 3D face recognition. The proposed approach relies on a novel multimodal keypoint detector capable of repeatably identifying keypoints on textured 3D face surfaces. Unique local surface descriptors are then constructed around each detected keypoint by integrating Curvelet elements of different orientations, resulting in highly descriptive rotation invariant features. Unlike previously reported Curvelet-based face recognition algorithms which extract global features from textured faces only, our algorithm extracts both texture and 3D local features. In addition, this is achieved across a number of frequency bands to achieve robust and accurate recognition under varying illumination conditions and facial expressions. The proposed algorithm was evaluated using three well-known and challenging datasets, namely FRGC v2, BU-3DFE and Bosphorus datasets. Reported results show superior performance compared to prior art, with 99.2{\%}, 95.1{\%} and 91{\%} verification rates at 0.001 FAR for FRGC v2, BU-3DFE and Bosphorus datasets, respectively. (C) 2014 Elsevier Ltd. All rights reserved.},
author = {Elaiwat, S and Bennamoun, M and Boussaid, F and El-Sallam, A},
doi = {10.1016/j.patcog.2014.10.013},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
number = {4},
pages = {1235--1246},
title = {{A Curve let-based approach for textured 3D face recognition}},
volume = {48},
year = {2015}
}
@inproceedings{ISI:000385794300011,
abstract = {Light detection and ranging (LIDAR) technology offers the capability to rapidly capture high-resolution, 3-dimensional surface data with centimeter-level accuracy for a large variety of applications. Due to the foliage-penetrating properties of LIDAR systems, these geospatial data sets can detect ground surfaces beneath trees, enabling the production of high-fidelity bare earth elevation models. Precise characterization of the ground surface allows for identification of terrain and non-terrain points within the point cloud, and facilitates further discernment between natural and man-made objects based solely on structural aspects and relative neighboring parameterizations. A framework is presented here for automated extraction of natural and man-made features that does not rely on coincident ortho-imagery or point RGB attributes. The TEXAS (Terrain EXtraction And Segmentation) algorithm is used first to generate a bare earth surface from a lidar survey, which is then used to classify points as terrain or non-terrain. Further classifications are assigned at the point level by leveraging local spatial information. Similarly classed points are then clustered together into regions to identify individual features. Descriptions of the spatial attributes of each region are generated, resulting in the identification of individual tree locations, forest extents, building footprints, and 3-dimensional building shapes, among others. Results of the fully-automated feature extraction algorithm are then compared to ground truth to assess completeness and accuracy of the methodology.},
annote = {Conference on Laser Radar Technology and Applications XXI, Baltimore,
MD, APR 19-20, 2016},
author = {Magruder, Lori A and Leigh, Holly W and Soderlund, Alexander and Clymer, Bradley and Baer, Jessica and Neuenschwander, Amy L},
booktitle = {LASER RADAR TECHNOLOGY AND APPLICATIONS XXI},
doi = {10.1117/12.2223845},
editor = {{Turner, MD and Kamerman}, GW},
isbn = {978-1-5106-0073-7},
issn = {0277-786X},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Automated feature extraction for 3-dimensional point clouds}},
volume = {9832},
year = {2016}
}
@article{ISI:000385488000039,
abstract = {The accurate estimation of individual tree level aboveground biomass (AGB) is critical for understanding the carbon cycle, detecting potential biofuels and managing forest ecosystems. In this study, we assessed the capability of the metrics of point clouds, extracted from the full-waveform Airborne Laser Scanning (ALS) data, and of composite waveforms, calculated based on a voxel-based approach, for estimating tree level AGB individually and in combination, over a planted forest in the coastal region of east China. To do so, we investigated the importance of point cloud and waveform metrics for estimating tree-level AGB by all subsets models and relative weight indices. We also assessed the capability of the point cloud and waveform metrics based models and combo model (including the combination of both point cloud and waveform metrics) for tree-level AGB estimation and evaluated the accuracies of these models. The results demonstrated that most of the waveform metrics have relatively low correlation coefficients ({\textless}0.60) with other metrics. The combo models (Adjusted R-2 = 0.78-0.89), including both point cloud and waveform metrics, have a relatively higher performance than the models fitted by point cloud metrics-only (Adjusted R-2 = 0.74-0.86) and waveform metrics-only (Adjusted R-2 = 0.72-0.84), with the mostly selected metrics of the 95th percentile height (H-95), mean of height of median energy (HOME) and mean of the height/median ratio (HTMR). Based on the relative weights (i.e., the percentage of contribution for R-2) of the mostly selected metrics for all subsets, the metric of 95th percentile height (H-95) has the highest relative importance for AGB estimation (19.23{\%}), followed by 75th percentile height (H-75) (18.02{\%}) and coefficient of variation of heights (H-cv) (15.18{\%}) in the point cloud metrics based models. For the waveform metrics based models, the metric of mean of height of median energy (HOME) has the highest relative importance for AGB estimation (17.86{\%}), followed by mean of the height/median ratio (HTMR) (16.23{\%}) and standard deviation of height of median energy (HOME sigma) (14.78{\%}). This study demonstrated benefits of using full-waveform ALS data for estimating biomass at tree level, for sustainable forest management and mitigating climate change by planted forest, as China has the largest area of planted forest in the world, and these forests contribute to a large amount of carbon sequestration in terrestrial ecosystems.},
author = {Cao, Lin and Gao, Sha and Li, Pinghao and Yun, Ting and Shen, Xin and Ruan, Honghua},
doi = {10.3390/rs8090729},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
number = {9},
title = {{Aboveground Biomass Estimation of Individual Trees in a Coastal Planted Forest Using Full-Waveform Airborne Laser Scanning Data}},
volume = {8},
year = {2016}
}
@article{ISI:000425652100003,
abstract = {The Dual-Wavelength Echidna Lidar (DWEL), a full waveform terrestrial laser scanner (TLS), has been used to scan a variety of forested and agricultural environments. From these scanning campaigns, we summarize the benefits and challenges given by DWEL's novel coaxial dual-wavelength scanning technology, particularly for the three-dimensional (3D) classification of vegetation elements. Simultaneous scanning at both 1064 nm and 1548 nm by DWEL instruments provides a new spectral dimension to TLS data that joins the 3D spatial dimension of lidar as an information source. Our point cloud classification algorithm explores the utilization of both spectral and spatial attributes of individual points from DWEL scans and highlights the strengths and weaknesses of each attribute domain. The spectral and spatial attributes for vegetation element classification each perform better in different parts of vegetation (canopy interior, fine branches, coarse trunks, etc.) and under different vegetation conditions (dead or live, leaf-on or leaf-off, water content, etc.). These environmental characteristics of vegetation, convolved with the lidar instrument specifications and lidar data quality, result in the actual capabilities of spectral and spatial attributes to classify vegetation elements in 3D space. The spectral and spatial information domains thus complement each other in the classification process. The joint use of both not only enhances the classification accuracy but also reduces its variance across the multiple vegetation types we have examined, highlighting the value of the DWEL as a new source of 3D spectral information. Wider deployment of the DWEL instruments is in practice currently held back by challenges in instrument development and the demands of data processing required by coaxial dual-or multi-wavelength scanning. But the simultaneous 3D acquisition of both spectral and spatial features, offered by new multispectral scanning instruments such as the DWEL, opens doors to study biophysical and biochemical properties of forested and agricultural ecosystems at more detailed scales.},
author = {Li, Zhan and Schaefer, Michael and Strahler, Alan and Schaaf, Crystal and Jupp, David},
doi = {10.1098/rsfs.2017.0039},
issn = {2042-8898},
journal = {INTERFACE FOCUS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {apr},
number = {2},
title = {{On the utilization of novel spectral laser scanning for three-dimensional classification of vegetation elements}},
volume = {8},
year = {2018}
}
@inproceedings{Zeng:2019:PDG:3324320.3324370,
address = {USA},
author = {Zeng, Yingjie and Nie, Lanshun},
booktitle = {Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks},
isbn = {978-0-9949886-3-8},
keywords = {revisao{\_}V1,revisao{\_}acm,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,tutux9},
pages = {254--255},
publisher = {Junction Publishing},
series = {EWSN ?19},
title = {{Poster: Deep Gait Recognition via Millimeter Wave}},
url = {http://dl.acm.org/citation.cfm?id=3324320.3324370},
year = {2019}
}
@inproceedings{ISI:000457881301017,
abstract = {Recently, automated emergency brake systems for pedestrian have been commercialized. However, they cannot detect crossing pedestrians when turning at intersections because the field of view is not wide enough. Thus, we propose to utilize a surround view camera system becoming popular by making it into stereo vision which is robust for the pedestrian recognition. However, conventional stereo camera technologies cannot be applied due to fisheye cameras and uncalibrated camera poses. Thus we have created the new method to absorb difference of the pedestrian appearance between cameras by machine learning for the stereo vision. The method of stereo matching between image patches in each camera image was designed by combining D-Brief and NCC with SVM. Good generalization performance was achieved by it compared with individual conventional algorithms. Furthermore, feature amounts of the point cloud reconstructed by the stereo pairs are utilized with Random Forest to discriminate pedestrians. The algorithm was evaluated for the actual camera images of crossing pedestrians at various intersections, and 96.0{\%} of pedestrian tracking rate with high position detection accuracy was achieved. They were compared with Faster R-CNN as the best pattern recognition technique, and our proposed method indicated better detection performance.},
annote = {21st IEEE International Conference on Intelligent Transportation Systems
(ITSC), Maui, HI, NOV 04-07, 2018},
author = {Akita, Tokihiko and Yamauchi, Yuji and Fujiyoshi, Hironobu},
booktitle = {2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)},
isbn = {978-1-7281-0323-5},
issn = {2153-0009},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IEEE; IEEE Intelligent Transportat Syst Soc},
pages = {1103--1108},
series = {IEEE International Conference on Intelligent Transportation Systems-ITSC},
title = {{Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye Cameras}},
year = {2018}
}
@article{ISI:000353891500010,
abstract = {There are more than 1,000 tornadoes in the United States each year, yet engineers do not typically design for tornadoes because of insufficient information about wind loads. Collecting building-level damage data in the aftermath of tornadoes can improve the understanding of tornado winds, but these data are difficult to collect because of safety, time, and access constraints. This study presents and tests an automated geographic information system (GIS) method using postevent point cloud data collected by terrestrial scanners and preevent aerial images to calculate the percentage of roof and wall damage and estimate wind speeds at an individual building scale. Simulations determined that for typical point cloud density ({\textgreater}25points/m2), a GIS raster cell size of 40-50cm resulted in less than 10{\%} error in damaged roof and wall detection. Data collected after recent tornadoes were used to correlate wind speed estimates and the percent of detected damage. The developed method estimated wind speeds from damage data collected after the 2011 Tuscaloosa, AL tornado at finer scales than the typical large-scale assessments done by reconnaissance engineers.},
author = {Kashani, Alireza G and Crawford, Patrick S and Biswas, Sufal K and Graettinger, Andrew J and Grau, David},
doi = {10.1061/(ASCE)CP.1943-5487.0000389},
issn = {0887-3801},
journal = {JOURNAL OF COMPUTING IN CIVIL ENGINEERING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {3},
title = {{Automated Tornado Damage Assessment and Wind Speed Estimation Based on Terrestrial Laser Scanning}},
volume = {29},
year = {2015}
}
@inproceedings{ISI:000393154600016,
abstract = {The knowledge about individual trees in forest is highly beneficial in forest management. High density small foot- print multi-return airborne Light Detection and Ranging (LiDAR) data can provide a very accurate information about the structural properties of individual trees in forests. Every tree species has a unique set of crown structural characteristics that can be used for tree species classification. In this paper, we use both the internal and external crown structural information of a conifer tree crown, derived from a high density small foot-print multi-return LiDAR data acquisition for species classification. Considering the fact that branches are the major building blocks of a conifer tree crown, we obtain the internal crown structural information using a branch level analysis. The structure of each conifer branch is represented using clusters in the LiDAR point cloud. We propose the joint use of the k-means clustering and geometric shape fitting, on the LiDAR data projected onto a novel 3-dimensional space, to identify branch clusters. After mapping the identified clusters back to the original space, six internal geometric features are estimated using a branch-level analysis. The external crown characteristics are modeled by using six least correlated features based on cone fitting and convex hull. Species classification is performed using a sparse Support Vector Machines (sparse SVM) classifier.},
annote = {Conference on Image and Signal Processing for Remote Sensing XXII,
Edinburgh, SCOTLAND, SEP 26-28, 2016},
author = {Harikumar, A and Bovolo, F and Bruzzone, L},
booktitle = {IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXII},
doi = {10.1117/12.2241452},
editor = {{Bruzzone, L and Bovolo}, F},
isbn = {978-1-5106-0412-4; 978-1-5106-0413-1},
issn = {0277-786X},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{A novel approach to internal crown characterization for coniferous tree species classification}},
volume = {10004},
year = {2016}
}
@article{ISI:000365704000006,
abstract = {Photogrammetric point clouds (PPC) obtained by stereomatching of aerial photographs now have a resolution sufficient to discern individual trees. We have produced such PPCs of a boreal forest and delineated individual tree crowns using a segmentation algorithm applied to the canopy height model derived from the PPC and a lidar terrain model. The crowns were characterized in terms of height and species (spruce, fir, and deciduous). Species classification used the 3D shape of the single crowns and their reflectance properties. The same was performed on a lidar dataset. Results show that the quality of PPC data generally approaches that of airborne lidar. For pixel-based canopy height models, viewing geometry in aerial images, forest structure (dense vs. open canopies), and composition (deciduous vs. conifers) influenced the quality of the 3D reconstruction of PPCs relative to lidar. Nevertheless, when individual tree height distributions were analyzed, PPC-based results were very similar to those extracted from lidar. The random forest classification (RF) of individual trees performed better in the lidar case when only 3D metrics were used (83{\%} accuracy for lidar, 79{\%} for PPC). However, when 3D and intensity or multispectral data were used together, the accuracy of PPCs (89{\%}) surpassed that of lidar (86{\%}).},
author = {St-Onge, Benoit and Audet, Felix-Antoine and Begin, Jean},
doi = {10.3390/f6113899},
issn = {1999-4907},
journal = {FORESTS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {nov},
number = {11},
pages = {3899--3922},
title = {{Characterizing the Height Structure and Composition of a Boreal Forest Using an Individual Tree Crown Approach Applied to Photogrammetric Point Clouds}},
volume = {6},
year = {2015}
}
@article{ISI:000367181400024,
abstract = {OBJECTIVE. The goal of this study was to compare the perceived quality of 3-T axial T2-weighted high-resolution 2D and high-resolution 3D fast spin-echo (FSE) endorectal MR images of the prostate. MATERIALS AND METHODS. Six radiologists independently reviewed paired 3-T axial T2-weighted high-resolution 2D and 3D FSE endorectal MR images of the prostates of 85 men in two sessions. In the first session (n = 85), each reader selected his or her preferred images; in the second session (n = 28), they determined their confidence in tumor identification and compared the depiction of the prostatic anatomy, tumor conspicuity, and subjective intrinsic image quality of images. A meta-analysis using a random-effects model, logistic regression, and the paired Wilcoxon rank-sum test were used for statistical analyses. RESULTS. Three readers preferred the 2D acquisition (67-89{\%}), and the other three preferred the 3D images (70-80{\%}). The option for one of the techniques was not associated with any of the predictor variables. The 2D FSE images were significantly sharper than 3D FSE (p {\textless} 0.001) and significantly more likely to exhibit other (nonmotion) artifacts (p = 0.002). No other statistically significant differences were found. CONCLUSION. Our results suggest that there are strong individual preferences for the 2D or 3D FSE MR images, but there was a wide variability among radiologists. There were differences in image quality (image sharpness and presence of artifacts not related to motion) but not in the sequences' ability to delineate the glandular anatomy and depict a cancerous tumor.},
author = {Westphalen, Antonio C and Noworolski, Susan M and Harisinghani, Mukesh and Jhaveri, Kartik S and Raman, Steve S and Rosenkrantz, Andrew B and Wang, Zhen J and Zagoria, Ronald J and Kurhanewicz, John},
doi = {10.2214/AJR.14.14065},
issn = {0361-803X},
journal = {AMERICAN JOURNAL OF ROENTGENOLOGY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jan},
number = {1},
pages = {86--91},
title = {{High-Resolution 3-T Endorectal Prostate MRI: A Multireader Study of Radiologist Preference and Perceived Interpretive Quality of 2D and 3D T2-Weighted Fast Spin-Echo MR Images}},
volume = {206},
year = {2016}
}
@article{ISI:000398720100002,
abstract = {Small unmanned aerial vehicle (UAV) based remote sensing is a rapidly evolving technology. Novel sensors and methods are entering the market, offering completely new possibilities to carry out remote sensing tasks. Three-dimensional (3D) hyperspectral remote sensing is a novel and powerful technology that has recently become available to small UAVs. This study investigated the performance of UAV-based photogrammetry and hyperspectral imaging in individual tree detection and tree species classification in boreal forests. Eleven test sites with 4151 reference trees representing various tree species and developmental stages were collected in June 2014 using a UAV remote sensing system equipped with a frame format hyperspectral camera and an RGB camera in highly variable weather conditions. Dense point clouds were measured photogrammetrically by automatic image matching using high resolution RGB images with a 5 cm point interval. Spectral features were obtained from the hyperspectral image blocks, the large radiometric variation of which was compensated for by using a novel approach based on radiometric block adjustment with the support of in-flight irradiance observations. Spectral and 3D point cloud features were used in the classification experiment with various classifiers. The best results were obtained with Random Forest and Multilayer Perceptron (MLP) which both gave 95{\%} overall accuracies and an F-score of 0.93. Accuracy of individual tree identification from the photogrammetric point clouds varied between 40{\%} and 95{\%}, depending on the characteristics of the area. Challenges in reference measurements might also have reduced these numbers. Results were promising, indicating that hyperspectral 3D remote sensing was operational from a UAV platform even in very difficult conditions. These novel methods are expected to provide a powerful tool for automating various environmental close-range remote sensing tasks in the very near future.},
author = {Nevalainen, Olli and Honkavaara, Eija and Tuominen, Sakari and Viljanen, Niko and Hakala, Teemu and Yu, Xiaowei and Hyyppa, Juha and Saari, Heikki and Polonen, Ilkka and Imai, Nilton N and Tommaselli, Antonio M G},
doi = {10.3390/rs9030185},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
month = {mar},
number = {3},
title = {{Individual Tree Detection and Classification with UAV-Based Photogrammetric Point Clouds and Hyperspectral Imaging}},
volume = {9},
year = {2017}
}
@inproceedings{ISI:000370814200017,
abstract = {Sections of the mining industry depend on regular analysis of rock fragmentation to detect trends that may affect safety or production. The limitations inherent in 2D imaging analysis mean that human input is typically needed for delineating individual rock fragments. Although recent advances in 3D image processing have diminished the need for human input, it is often infeasible for many mines to upgrade their existing 2D imaging systems to 3D. Hence there is still a need to improve delineation in 2D images. This paper proposes a method for delineating rock fragments by classifying compressed Haar-like features extracted from small image patches. The optimum size of the image patches and the number of compressed features are determined empirically. Experimental results show the proposed method gives superior results to the commonly used watershed algorithm, and compressing features improves computational efficiency such that a machine learning approach is practical.},
annote = {International Joint Conference on Computer Vision, Imaging and Computer
Graphics Theory and Applications (VISIGRAPP), Lisbon, PORTUGAL, JAN
05-08, 2014},
author = {Bull, Geoff and Gao, Junbin and Antolovich, Michael},
booktitle = {COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS - THEORY AND APPLICATIONS, VISIGRAPP 2014},
doi = {10.1007/978-3-319-25117-2_17},
editor = {{Battiato, S and Coquillart, S and Pettre, J and Laramee, RS and Kerren, A and Braz}, J},
isbn = {978-3-319-25117-2; 978-3-319-25116-5},
issn = {1865-0929},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {Inst Syst {\&} Technologies Informat, Control {\&} Commun; Eurographics; IEEE Comp Soc; IEEE VGMT; IEEE TCMC},
pages = {273--286},
series = {Communications in Computer and Information Science},
title = {{Rock Fragment Boundary Detection Using Compressed Random Features}},
volume = {550},
year = {2015}
}
@article{ISI:000344204000007,
abstract = {The 3D Morphable Model (3DMM) and the Structure from Motion (SfM) methods are widely used for 3D facial reconstruction from 2D single-view or multiple-view images. However, model-based methods suffer from disadvantages such as high computational costs and vulnerability to local minima and head pose variations. The SfM-based methods require multiple facial images in various poses. To overcome these disadvantages, we propose a single-view-based 3D facial reconstruction method that is person-specific and robust to pose variations. Our proposed method combines the simplified 3DMM and the SfM methods. First, 2D initial frontal Facial Feature Points (FFPs) are estimated from a preliminary 3D facial image that is reconstructed by the simplified 3DMM. Second, a bilateral symmetric facial image and its corresponding FFPs are obtained from the original side-view image and corresponding FFPs by using the mirroring technique. Finally, a more accurate the 3D facial shape is reconstructed by the SfM using the frontal, original, and bilateral symmetric FFPs. We evaluated the proposed method using facial images in 35 different poses. The reconstructed facial images and the ground-truth 3D facial shapes obtained from the scanner were compared. The proposed method proved more robust to pose variations than 3DMM. The average 3D Root Mean Square Error (RMSE) between the reconstructed and ground-truth 3D faces was less than 2.6 mm when 2D FFPs were manually annotated, and less than 3.5 mm when automatically annotated. (C) 2014 Elsevier Ltd. All rights reserved.},
author = {Jo, Jaeik and Choi, Heeseung and Kim, Ig-Jae and Kim, Jaihie},
doi = {10.1016/j.patcog.2014.07.013},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}webofscience,tutux9},
month = {jan},
number = {1},
pages = {73--85},
title = {{Single-view-based 3D facial reconstruction method robust against pose variations}},
volume = {48},
year = {2015}
}
@inproceedings{ISI:000385794300020,
abstract = {The importance of creating 3D imagery is increasing and has many applications in the field of disaster response, digital elevation models, object recognition, and cultural heritage. Several methods have been proposed to register texel images, which consist of fused lidar and digital imagery. The previous methods were limited to registering up to two texel images or multiple texel swaths having only one strip of lidar data per swath. One area of focus still remains to register multiple texel images to create a 3D model. The process of creating true 3D images using multiple texel images is described. The texel camera fuses the 2D digital image and calibrated 3D lidar data to form a texel image. The images are then taken from several perspectives and registered. The advantage of using multiple full frame texel images over 3D- or 2D-only methods is that there will be better registration between images because of the overlapping 3D points as well as 2D texture used in the joint registration process. The individual position and rotation mapping to a common world coordinate frame is calculated for each image and optimized. The proposed methods incorporate bundle adjustment for jointly optimizing the registration of multiple images. Sparsity is exploited as there is a lack of interaction between parameters of different cameras. Examples of the 3D model are shown and analyzed for numerical accuracy.},
annote = {Conference on Laser Radar Technology and Applications XXI, Baltimore,
MD, APR 19-20, 2016},
author = {Khatiwada, Bikalpa and Budge, Scott E},
booktitle = {LASER RADAR TECHNOLOGY AND APPLICATIONS XXI},
doi = {10.1117/12.2223259},
editor = {{Turner, MD and Kamerman}, GW},
isbn = {978-1-5106-0073-7},
issn = {0277-786X},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Three-dimensional image reconstruction using bundle adjustment applied to multiple texel images}},
volume = {9832},
year = {2016}
}
@inproceedings{ISI:000392743800047,
abstract = {Current 3D data capturing as implemented on for example airborne or mobile laser scanning systems is able to efficiently sample the surface of a city by billions of unselective points during one working day. What is still difficult is to extract and visualize meaningful information hidden in these point clouds with the same efficiency. This is where the FP7 IQmulus project enters the scene. IQmulus is an interactive facility for processing and visualizing big spatial data. In this study the potential of IQmulus is demonstrated on a laser mobile mapping point cloud of 1 billion points sampling " 10 km of street environment in Toulouse, France. After the data is uploaded to the IQmulus Hadoop Distributed File System, a workflow is defined by the user consisting of retiling the data followed by a PCA driven local dimensionality analysis, which runs efficiently on the IQmulus cloud facility using a Spark implementation. Points scattering in 3 directions are clustered in the tree class, and are separated next into individual trees. Five hours of processing at the 12 node computing cluster results in the automatic identification of 4000+ urban trees. Visualization of the results in the IQmulus fat client helps users to appreciate the results, and developers to identify remaining flaws in the processing workflow.},
annote = {From Duplicate 1 (THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS - Boehm, J; Bredif, M; Gierlinger, T; Kraemer, M; Lindenbergh, R; Liu, K; Michel, F; Sirmacek, B)

23rd Congress of the
International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
Prague, CZECH REPUBLIC, JUL 12-19, 2016

From Duplicate 2 (The Iqmulus urban showcase: Automatic tree classification and identification in huge mobile mapping point clouds - B{\"{o}}hm, J; Bredif, M; Gierlinger, T; Kr{\"{a}}mer, M; Lindenbergh, R; Liu, K; Michel, F; Sirmacek, B)

cited By 3

From Duplicate 3 (THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS - Boehm, J; Bredif, M; Gierlinger, T; Kraemer, M; Lindenbergh, R; Liu, K; Michel, F; Sirmacek, B; B{\"{o}}hm, J; Bredif, M; Gierlinger, T; Kr{\"{a}}mer, M; Lindenbergh, R; Liu, K; Michel, F; Sirmacek, B)

From Duplicate 1 (The Iqmulus urban showcase: Automatic tree classification and identification in huge mobile mapping point clouds - B{\"{o}}hm, J; Bredif, M; Gierlinger, T; Kr{\"{a}}mer, M; Lindenbergh, R; Liu, K; Michel, F; Sirmacek, B)

cited By 3

From Duplicate 2 (THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS - Boehm, J; Bredif, M; Gierlinger, T; Kraemer, M; Lindenbergh, R; Liu, K; Michel, F; Sirmacek, B)

23rd Congress of the
International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
Prague, CZECH REPUBLIC, JUL 12-19, 2016},
author = {Boehm, J and Bredif, M and Gierlinger, T and Kraemer, M and Lindenbergh, R and Liu, K and Michel, F and Sirmacek, B and B{\"{o}}hm, J and Bredif, M and Gierlinger, T and Kr{\"{a}}mer, M and Lindenbergh, R and Liu, K and Michel, F and Sirmacek, B and Boehm, J and Bredif, M and Gierlinger, T and Kraemer, M and Lindenbergh, R and Liu, K and Michel, F and Sirmacek, B},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprsarchives-XLI-B3-301-2016},
editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla}, U},
issn = {2194-9034},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {B3},
organization = {Int Soc Photogrammetry {\&} Remote Sensing},
pages = {301--307},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978159544{\&}doi=10.5194{\%}2Fisprsarchives-XLI-B3-301-2016{\&}partnerID=40{\&}md5=2fbed8958deed902b55eef60e98a087e},
volume = {41},
year = {2016}
}
@inproceedings{ISI:000359292400017,
abstract = {Current diagnostic techniques in dentistry rely predominantly on X-rays to monitor dental caries. Terahertz Pulsed Imaging (TPI) has great potential for medical applications since it is a nondestructive imaging method. It does not cause any ionization hazard on biological samples due to low energy of THz radiation. Even though it is strongly absorbed by water which exhibits very unique chemical and physical properties that contribute to strong interaction with THz radiation, teeth can still be investigated in three dimensions. Recent investigations suggest that this method can be used in the early identification of dental diseases and imperfections in the tooth structure without the hazards of using techniques which rely on x-rays. We constructed a continuous wave (CW) and time-domain reflection mode raster scan THz imaging system that enables us to investigate various teeth samples in two or three dimensions. The samples comprised of either slices of individual tooth samples or rows of teeth embedded in wax, and the imaging was done by scanning the sample across the focus of the THz beam. 2D images were generated by acquiring the intensity of the THz radiation at each pixel, while 3D images were generated by collecting the amplitude of the reflected signal at each pixel. After analyzing the measurements in both the spatial and frequency domains, the results suggest that the THz pulse is sensitive to variations in the structure of the samples that suggest that this method can be useful in detecting the presence of caries.},
annote = {Conference on Medical Laser Applications and Laser-Tissue Interactions
VII, Munich, GERMANY, JUN 21-23, 2015},
author = {Karagoz, Burcu and Altan, Hakan and Kamburoglu, Kivanc},
booktitle = {MEDICAL LASER APPLICATIONS AND LASER-TISSUE INTERACTIONS VII},
doi = {10.1117/12.2183673},
editor = {{Lilge, LD and Sroka}, R},
isbn = {978-1-62841-707-4},
issn = {0277-786X},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {SPIE; Opt Soc America},
series = {Proceedings of SPIE},
title = {{Terahertz pulsed imaging study of dental caries}},
volume = {9542},
year = {2015}
}
@article{ISI:000445204800002,
abstract = {Manually monitoring and documenting trees is labour intensive. Lidar provides a possible solution for automatic tree-inventory generation. Existing approaches for segmenting trees from original point cloud data lack scalable and efficient methods that separate individual trees sampled by different laser-scanning systems with sufficient quality under all circumstances. In this study a new algorithm for efficient individual tree delineation from lidar point clouds is presented and validated. The proposed algorithm first resamples the points using cuboid (modified voxel) cells. Consecutively connected cells are accumulated by vertically traversing cell layers. Trees in close proximity are identified, based on a novel cell-adjacency analysis. The scalable performance of this algorithm is validated on airborne, mobile and terrestrial laser-scanning point clouds. Validation against ground truth demonstrates an improvement from 89{\%} to 94{\%} relative to a state-of-the-art method while computation time is similar. Resume La detection et la documentation manuelle des arbres est une tache fastidieuse. Le lidar offre une solution possible pour l'inventaire automatique des arbres. Les approches existantes pour la segmentation des arbres dans des nuages bruts de points ne proposent pas de methodes efficaces et adaptees a toutes les echelles pour separer des arbres individuels echantillonnes par differents systemes lidar avec une qualite acceptable en toute circonstance. Cette etude propose et valide un nouvel algorithme pour la delimitation efficace d'arbres individuels a partir de nuages de points lidar. L'algorithme propose commence par reechantillonner les points dans des cellules cubiques (voxels), puis regroupe les cellules connexes en traversant verticalement les couches de cellules. Les arbres proches sont identifies grace a une nouvelle analyse d'adjacence de cellules. La performance de cetalgorithme en termes d'adaptabilite au changement d'echelle est validee a partir de nuages de points issus de systemes laser a balayage aerien, mobile et terrestre. Une validation basee sur des donnees de terrain de reference fait etat d'une amelioration de 89{\%} a 94{\%} par rapport a des methodes connues pour un temps de calcul comparable. Zusammenfassung Eine uberwachung und Dokumentation von Baumen ist sehr arbeitsaufwandig. Lidar bietet das Potential fur automatische Bauminventur. Es gibt Ansatze zur Segmentierung von Baumen aus Punktwolken, die allerdings noch nicht in der Lage sind, einzelne Baume in Punktwolken verschiedener Lidar-Systeme zuverlassig und mit ausreichender Qualitat unter vielfaltigen realen Bedingungen zu separieren. Diese Studie stellt einen neuen Algorithmus zur effizienten Erfassung von Baumen in Lidar-Punktwolken dar. Der Algorithmus bildet Punkte mit Hilfe von quaderformigen (Voxel) Zellen um. Nacheinander verbundene Zellen werden durch vertikale Traverse der Zellschichten akkumuliert. Baume in nachster Nachbarschaft werden durch eine neuartige Zellanalyse identifiziert. Der Vorteil der Skalierbarkeit des Algorithmus wird an flugzeuggestutzten, mobilen und terrestrischen Laserscanpunktwolken validiert. Anhand von Solldaten ist festzustellen, dass bei gleicher Rechenzeit, eine Verbesserung von 89{\%} bis 94{\%} im Vergleich zu aktuellen Verfahren erzielt werden kann. Resumen Monitorizar y documentar manualmente arboles es un trabajo intensivo. El lidar proporciona una posible solucion para la generacion automatica del inventario de arboles. Los enfoques existentes para segmentar arboles a partir originalmente de nubes de puntos lidar carecen de metodos escalables y eficientes que separen arboles individuales muestreados por diferentes sistemas lidar con calidad suficiente bajo todas las circunstancias. En este estudio, se presenta y valida un algoritmo nuevo para la delimitacion eficiente de arboles individuales a partir de nubes de puntos lidar. El algoritmo propuesto primero remuestrea los puntos usando celulas cuboides (voxels). Los voxels adyacentes se acumulan atravesando verticalmente las capas de voxels. Basados en un nuevo analisis de adyacencia de voxels se identifican arboles que estan proximos. El rendimiento escalable de este algoritmo se valida con nubes de puntos lidar aerotransportados, moviles y terrestres. La validacion con verdad terreno demuestra una mejora del 89{\%} al 94{\%} en comparacion con un metodo de vanguardia, mientras que el tiempo de calculo es similar.},
author = {Wang, Jinhu and Lindenbergh, Roderik and Menenti, Massimo},
doi = {10.1111/phor.12247},
issn = {0031-868X},
journal = {PHOTOGRAMMETRIC RECORD},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {163},
pages = {315--340},
title = {{Scalable individual tree delineation in 3D point clouds}},
volume = {33},
year = {2018}
}
@article{ISI:000386910000105,
abstract = {In this study, we propose a novel markerless motion capture system (MCS) for monkeys, in which 3D surface images of monkeys were reconstructed by integrating data from four depth cameras, and a skeleton model of the monkey was fitted onto 3D images of monkeys in each frame of the video. To validate the MCS, first, estimated 3D positions of body parts were compared between the 3D MCS-assisted estimation and manual estimation based on visual inspection when a monkey performed a shuttling behavior in which it had to avoid obstacles in various positions. The mean estimation error of the positions of body parts (3-14 cm) and of head rotation (35-43 degrees) between the 3D MCS-assisted and manual estimation were comparable to the errors between two different experimenters performing manual estimation. Furthermore, the MCS could identify specific monkey actions, and there was no false positive nor false negative detection of actions compared with those in manual estimation. Second, to check the reproducibility of MCS-assisted estimation, the same analyses of the above experiments were repeated by a different user. The estimation errors of positions of most body parts between the two experimenters were significantly smaller in the MCS-assisted estimation than in the manual estimation. Third, effects of methamphetamine (MAP) administration on the spontaneous behaviors of four monkeys were analyzed using the MCS. MAP significantly increased head movements, tended to decrease locomotion speed, and had no significant effect on total path length. The results were comparable to previous human clinical data. Furthermore, estimated data following MAP injection (total path length, walking speed, and speed of head rotation) correlated significantly between the two experimenters in the MCS-assisted estimation (r = 0.863 to 0.999). The results suggest that the presented MCS in monkeys is useful in investigating neural mechanisms underlying various psychiatric disorders and developing pharmacological interventions.},
author = {Nakamura, Tomoya and Matsumoto, Jumpei and Nishimaru, Hiroshi and Bretas, Rafael Vieira and Takamura, Yusaku and Hori, Etsuro and Ono, Taketoshi and Nishijo, Hisao},
doi = {10.1371/journal.pone.0166154},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {nov},
number = {11},
title = {{A Markerless 3D Computerized Motion Capture System Incorporating a Skeleton Model for Monkeys}},
volume = {11},
year = {2016}
}
@article{ISI:000440272800007,
abstract = {BACKGROUND Current treatment of facial capillary malformations (CM) has limited efficacy. OBJECTIVE To assess the efficacy of large spot 532 nm lasers for the treatment of previously treated facial CM with the use of 3-dimensional (3D) image analysis. PATIENTS AND METHODS Forty-three white patients aged 6 to 59 were included in this study. Patients had 3D photography performed before and after treatment with a 532 nm Nd:YAG laser with large spot and contact cooling. Objective analysis of percentage improvement based on 3D digital assessment of combined color and area improvement (global clearance effect {\{}[{\}}GCE]) were performed. RESULTS The median maximal improvement achieved during the treatment (GCE(max)) was 59.1{\%}. The mean number of laser procedures required to achieve this improvement was 6.2 (range 1-16). Improvement of minimum 25{\%} (GCE25) was achieved by 88.4{\%} of patients, a minimum of 50{\%} (GCE50) by 61.1{\%}, a minimum of 75{\%} (GCE75) by 25.6{\%}, and a minimum of 90{\%} (GCE90) by 4.6{\%}. Patients previously treated with pulsed dye lasers had a significantly less response than those treated with other modalities (GCE (max) 37.3{\%} vs 61.8{\%}, respectively). CONCLUSION A large spot 532 nm laser is effective in previously treated patients with facial CM.},
author = {Kwiek, Bartlomiej and Ambroziak, Marcin and Osipowicz, Katarzyna and Kowalewski, Cezary and Rozalski, Michal},
doi = {10.1097/DSS.0000000000001447},
issn = {1076-0512},
journal = {DERMATOLOGIC SURGERY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jun},
number = {6},
pages = {803--813},
title = {{Treatment of Previously Treated Facial Capillary Malformations: Results of Single-Center Retrospective Objective 3-Dimensional Analysis of the Efficacy of Large Spot 532 nm Lasers}},
volume = {44},
year = {2018}
}
@inproceedings{ISI:000455343100004,
abstract = {RGB-D cameras, such as the Microsoft Kinect, provide us with the 3D information, color and depth, associated with the scene. Interactive 3D Tele-Immersion (i3DTI) systems use such RGB-D cameras to capture the person present in the scene in order to collaborate with other remote users and interact with the virtual objects present in the environment. Using a single camera, it becomes difficult to estimate an accurate skeletal pose and complete 3D model of the person, especially when the person is not in the complete view of the camera. With multiple cameras, even with partial views, it is possible to get a more accurate estimate of the skeleton of the person leading to a better and complete 3D model. In this paper, we present a real-time skeletal pose identification approach that leverages on the inaccurate skeletons of the individual Kinects, and provides a combined optimized skeleton. We estimate the Probability of an Accurate Joint (PAJ) for each joint from all of the Kinect skeletons. We determine the correct direction of the person and assign the correct joint sides for each skeleton. We then use a greedy consensus approach to combine the highly probable and accurate joints to estimate the combined skeleton. Using the individual skeletons, we segment the point clouds from all the cameras. We use the already computed PAJ values to obtain the Probability of an Accurate Bone (PAB). The individual point clouds are then combined one segment after another using the calculated PAB values. The generated combined point cloud is a complete and accurate 3D representation of the person present in the scene. We validate our estimated skeleton against two well-known methods by computing the error distance between the best view Kinect skeleton and the estimated skeleton. An exhaustive analysis is performed by using around 500000 skeletal frames in total, captured using 7 users and 7 cameras. Visual analysis is performed by checking whether the estimated skeleton is completely present within the human model. We also develop a 3D Holo-Bubble game to showcase the real-time performance of the combined skeleton and point cloud. Our results show that our method performs better than the state-of-the-art approaches that use multiple Kinects, in terms of objective error, visual quality and real-time user performance.},
annote = {9th ACM Multimedia Systems Conference (MMSys), Centrum Wiskunde {\&}
Informatica Amsterdam, Amsterdam, NETHERLANDS, JUN 12-15, 2018},
author = {Desai, Kevin and Prabhakaran, Balakrishnan and Raghuraman, Suraj},
booktitle = {PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18)},
doi = {10.1145/3204949.3204958},
isbn = {978-1-4503-5192-8},
keywords = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {Assoc Comp Machinery; ACM SIGMM; ACM SIGOPS; ACM SIGMOBILE; ACM SIGCOMM},
pages = {40--51},
title = {{Combining Skeletal Poses for 3D Human Model Generation using Multiple Kinects}},
year = {2018}
}
@inproceedings{7350744,
annote = {24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
24/04/2018 Exclu{\'{i}}do (etapa 1)},
booktitle = {2015 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2015.7350744},
isbn = {978-1-4799-8339-1},
keywords = {Big Data,compressed sensing,data protection,data v,etapa1,gil,id198,ieeexplore,revisao{\_}V2,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux1},
mendeley-tags = {etapa1,gil,id198,ieeexplore,revisao{\_}V2,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux1},
month = {sep},
pages = {1--1},
publisher = {IEEE},
title = {{[Title page]}},
url = {http://ieeexplore.ieee.org/document/7350744/},
year = {2015}
}
@article{ISI:000401423700003,
abstract = {Large trees are important to a wide variety of wildlife, including many species of conservation concern, such as the California spotted owl (Strix occidentalis occidentalis). Light detection and ranging (LiDAR) has been successfully utilized to identify the density of large-diameter trees, either by segmenting the LiDAR point cloud into individual trees, or by building regression models between variables extracted from the LiDAR point cloud and field data. Neither of these methods is easily accessible for most land managers due to the reliance on specialized software, and much available LiDAR data are being underutilized due to the steep learning curve required for advanced processing using these programs. This study derived a simple, yet effective method for estimating the density of large-stemmed trees from the LiDAR canopy height model, a standard raster product derived from the LiDAR point cloud that is often delivered with the LiDAR and is easy to process by personnel trained in geographic information systems (GIS). Ground plots needed to be large (1 ha) to build a robust model, but the spatial accuracy of plot center was less crucial to model accuracy. We also showed that predicted large tree density is positively linked to California spotted owl nest sites.},
author = {Kramer, Heather A and Collins, Brandon M and Gallagher, Claire V and Keane, John J and Stephens, Scott L and Kelly, Maggi},
doi = {10.1002/ecs2.1593},
issn = {2150-8925},
journal = {ECOSPHERE},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {12},
title = {{Accessible light detection and ranging: estimating large tree density for habitat identification}},
volume = {7},
year = {2016}
}
@article{ISI:000413384000029,
abstract = {The invasive phytopathogen Phytophthora ramorum has caused extensive infection of larch forest across areas of the UK, particularly in Southwest England, South Wales and Southwest Scotland. At present, landscape level assessment of the disease in these areas is conducted manually by tree health surveyors during helicopter surveys. Airborne laser scanning (ALS), also known as LiDAR, has previously been applied to the segmentation of larch tree crowns infected by P. ramorum infection and the detection of insect pests in coniferous tree species. This study evaluates metrics from high-density discrete ALS point clouds (24 points/m(2)) and canopy height models (CHMs) to identify individual trees infected with P. ramorum and to discriminate between four disease severity categories (NI: not infected, 1: light, 2: moderate, 3: heavy). The metrics derived from ALS point clouds include canopy cover, skewness, and bicentiles (B60, B70, B80 and B90) calculated using both a static (1 m) and a variable (50{\%} of tree height) cut-off height. Significant differences are found between all disease severity categories, except in the case of healthy individuals (NI) and those in the early stages of infection (category 1). In addition, fragmentation metrics are shown to identify the increased patchiness and infra-crown height irregularities of CHMs associated with individual trees subject to heavy infection (category 3) of P. ramorum. Classifications using a k-nearest neighbour (k-NN) classifier and ALS point cloud metrics to classify disease presence/absence and severity yielded overall accuracies of 72{\%} and 65{\%} respectively. The results indicate that ALS can be used to identify individual tree crowns subject to moderate and heavy P. ramorum infection in larch forests. This information demonstrates the potential applications of ALS for the development of a targeted phytosanitary approach for the management of P. ramorum.},
author = {Barnes, Chloe and Balzter, Heiko and Barrett, Kirsten and Eddy, James and Milrier, Sam and Suarez, Juan C},
doi = {10.1016/j.foreco.2017.08.052},
issn = {0378-1127},
journal = {FOREST ECOLOGY AND MANAGEMENT},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {nov},
pages = {294--305},
title = {{Airborne laser scanning and tree crown fragmentation metrics for the assessment of Phytophthora ramorum infected larch forest stands}},
volume = {404},
year = {2017}
}
@inproceedings{7976644,
abstract = {In this paper we propose a new technique to detect and extract any part that we need from a 3D face. The regions of interest of our approach are the eyes and the nose. Moreover, we are interested in the extraction of the salient landmarks in 3D face. Therefore, in this paper a new method is proposed to detect the nose tip and eye corners from a three-dimensional face range image. Our original technique allows fully automated processing, treating incomplete and noisy input data, and automatically rejecting non-facial areas. Besides, it is robust against holes in 3D image and insensitive to facial expressions. Besides, it is stable against translation and rotation of the face, and it is suitable for different resolutions of images. All the experiments have been performed on the GAVAB and FRAV 3D databases. After applying the proposed method to the 3D face, experimental results show that it is comparable to state-of-the-art methods in terms of its accuracy and flexibility.},
author = {Sghaier, S and Souani, C and Faeidh, H and Besbes, K},
booktitle = {2016 Global Summit on Computer Information Technology (GSCIT)},
doi = {10.1109/GSCIT.2016.17},
keywords = {face recognition,image segmentation,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9,visual databas},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {27--31},
title = {{Novel Technique for 3D Face Segmentation and Landmarking}},
year = {2016}
}
@article{7050293,
abstract = {Scale-dependent statistical depictions of surface morphology offer the potential to parameterize complex geometrical scaling relationships with greater detail than traditional fractal measures. Using multiscale operators, it is possible to identify points belonging to rough discontinuous surfaces in noisy point clouds solely on the basis of their local geometry. Many strategies for point cloud feature classification have been developed since the proliferation of laser scanning systems. Most of the techniques which are applicable to natural scenes employ external data sources such as hyperspectral imagery, return pulse intensity, and waveform data. In this letter, multiscale geometric parameters are used to identify individual point observations corresponding to rock surfaces in point clouds acquired by terrestrial laser scanning in scenes with man-made clutter and scanning artifacts. Three multiscale operators, namely, the approximate shape and density of a defined neighborhood and the distance of its mean point from its geometric center, are fused into a single feature vector. The procedure is demonstrated using real point cloud data acquired in a mine drift, with the goal of identifying points belonging to the rock face obscured by an overlying wire support mesh. Using the extra-trees classifier, extraneous returns caused by the mesh were excluded from the point cloud with a 97{\%} success rate, while 87{\%} of the desired surface points were retained.},
author = {Mills, G and Fotopoulos, G},
doi = {10.1109/LGRS.2015.2398814},
issn = {1545-598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {feature extraction,geometry,geophysical signal pro,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {jun},
number = {6},
pages = {1322--1326},
title = {{Rock Surface Classification in a Mine Drift Using Multiscale Geometric Features}},
volume = {12},
year = {2015}
}
@inproceedings{ISI:000365181700036,
abstract = {In this paper, we present a novel and robust approach for 3D faces registration based on Energy Range Face Image (ERFI). ERFI is the frontal face model for the individual people from the database. It can be considered as a mean frontal range face image for each person. Thus, the total energy of the frontal range face images has been preserved by ERFI. For registration purpose, an interesting point or a land mark, which is the nose tip (or `pronasal') from face surface is extracted. Then, this landmark is exploited to correct the oriented faces by applying the 3D geometrical rotation technique with respect to the ERFI model for registration purpose. During the error calculation phase, Manhattan distance metric between the localized `pronasal' landmark on face image and that of ERFI model is determined on Euclidian space. The accuracy is quantified with selection of cut-points `T' on measured Manhattan distances along yaw, pitch and roll. The proposed method has been tested on Frav3D database and achieved 82.5{\%} accurate pose registration.},
annote = {3rd International Conference on Frontiers in Intelligent Computing -
Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014},
author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
booktitle = {PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2},
doi = {10.1007/978-3-319-12012-6_36},
editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal}, JK},
isbn = {978-3-319-12012-6; 978-3-319-12011-9},
issn = {2194-5357},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}webofscience,tutux9},
organization = {Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol {\&} Sci, CSI Student Branch},
pages = {323--333},
series = {Advances in Intelligent Systems and Computing},
title = {{Range Face Image Registration Using ERFI from 3D Images}},
volume = {328},
year = {2015}
}
@article{ISI:000354988900026,
abstract = {Expression, occlusion, and pose variations are three main challenges for 3D face recognition. A novel method is presented to address 3D face recognition using scale-invariant feature transform (SIFT) features on 3D meshes. After preprocessing, shape index extrema on the 3D facial surface are selected as keypoints in the difference scale space and the unstable keypoints are removed after two screening steps. Then, a local coordinate system for each keypoint is established by principal component analysis (PCA). Next, two local geometric features are extracted around each keypoint through the local coordinate system. Additionally, the features are augmented by the symmetrization according to the approximate left-right symmetry in human face. The proposed method is evaluated on the Bosphorus, BU-3DFE, and Gavab databases, respectively. Good results are achieved on these three datasets. As a result, the proposed method proves robust to facial expression variations, partial external occlusions and large pose changes. {\textcopyright} 2015, Central South University Press and Springer-Verlag Berlin Heidelberg.},
annote = {From Duplicate 1 (Face recognition using SIFT features under 3D meshes - Zhang, C; Gu, Y.-Z.; Hu, K.-L.; Wang, Y.-G.)

cited By 5

From Duplicate 3 (Face recognition using SIFT features under 3D meshes - Zhang, C; Gu, Y.-Z.; Hu, K.-L.; Wang, Y.-G.; Cheng, Zhang; Yu-zhang, Gu; Ke-li, Hu; Ying-guan, Wang)

From Duplicate 2 (Face recognition using SIFT features under 3D meshes - Zhang, C; Gu, Y.-Z.; Hu, K.-L.; Wang, Y.-G.)

cited By 5},
author = {Zhang, C and Gu, Y.-Z. and Hu, K.-L. and Wang, Y.-G. and Cheng, Zhang and Yu-zhang, Gu and Ke-li, Hu and Ying-guan, Wang and Zhang, C and Gu, Y.-Z. and Hu, K.-L. and Wang, Y.-G.},
doi = {10.1007/s11771-015-2700-x},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2015 - Face recognition using SIFT features under 3D meshes.pdf:pdf},
issn = {2095-2899},
journal = {JOURNAL OF CENTRAL SOUTH UNIVERSITY},
keywords = {revisao{\_}V1,revisao{\_}V2,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}V2,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
number = {5},
pages = {1817--1825},
title = {{Face recognition using SIFT features under 3D meshes}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930008100{\&}doi=10.1007{\%}2Fs11771-015-2700-x{\&}partnerID=40{\&}md5=569ac008f1ef201b7fcbffd9e0faa163},
volume = {22},
year = {2015}
}
@article{ISI:000391303000155,
abstract = {In viticulture, phenotypic data are traditionally collected directly in the field via visual and manual means by an experienced person. This approach is time consuming, subjective and prone to human errors. In recent years, research therefore has focused strongly on developing automated and non-invasive sensor-based methods to increase data acquisition speed, enhance measurement accuracy and objectivity and to reduce labor costs. While many 2D methods based on image processing have been proposed for field phenotyping, only a few 3D solutions are found in the literature. A track-driven vehicle consisting of a camera system, a real-time-kinematic GPS system for positioning, as well as hardware for vehicle control, image storage and acquisition is used to visually capture a whole vine row canopy with georeferenced RGB images. In the first post-processing step, these images were used within a multi-view-stereo software to reconstruct a textured 3D point cloud of the whole grapevine row. A classification algorithm is then used in the second step to automatically classify the raw point cloud data into the semantic plant components, grape bunches and canopy. In the third step, phenotypic data for the semantic objects is gathered using the classification results obtaining the quantity of grape bunches, berries and the berry diameter.},
author = {Rose, Johann Christian and Kicherer, Anna and Wieland, Markus and Klingbeil, Lasse and Toepfer, Reinhard and Kuhlmann, Heiner},
doi = {10.3390/s16122136},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {dec},
number = {12},
title = {{Towards Automated Large-Scale 3D Phenotyping of Vineyards under Field Conditions}},
volume = {16},
year = {2016}
}
@article{ISI:000410059200037,
abstract = {We combine confocal imaging, microfluidics, and image analysis to record 3D-images of cells in flow. This enables us to recover the full 3D representation of several hundred living cells per minute. Whereas 3D confocal imaging has thus far been limited to steady specimens, we overcome this restriction and present a method to access the 3D shape of moving objects. The key of our principle is a tilted arrangement of the micro-channel with respect to the focal plane of the microscope. This forces cells to traverse the focal plane in an inclined manner. As a consequence, individual layers of passing cells are recorded, which can then be assembled to obtain the volumetric representation. The full 3D information allows for a detailed comparison with theoretical and numerical predictions unfeasible with, e.g., 2D imaging. Our technique is exemplified by studying flowing red blood cells in a micro-channel reflecting the conditions prevailing in the microvasculature. We observe two very different types of shapes: ``croissants{\{}''{\}} and ``slippers.{\{}''{\}} Additionally, we perform 3D numerical simulations of our experiment to confirm the observations. Since 3D confocal imaging of cells in flow has not yet been realized, we see high potential in the field of flow cytometry where cell classification thus far mostly relies on 1D scattering and fluorescence signals. Published by AIP Publishing.},
author = {Quint, S and Christ, A F and Guckenberger, A and Himbert, S and Kaestner, L and Gekle, S and Wagner, C},
doi = {10.1063/1.4986392},
issn = {0003-6951},
journal = {APPLIED PHYSICS LETTERS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
number = {10},
title = {{3D tomography of cells in micro-channels}},
volume = {111},
year = {2017}
}
@inproceedings{Korn:2017:DAE:3064663.3064755,
address = {New York, NY, USA},
author = {Korn, Oliver and Stamm, Lukas and Moeckl, Gerd},
booktitle = {Proceedings of the 2017 Conference on Designing Interactive Systems},
doi = {10.1145/3064663.3064755},
isbn = {978-1-4503-4922-2},
keywords = {affective,animation,body cues,design,emotion,emotion recognition,facial expressions,games,perception,revisao{\_}V2,revisao{\_}acm,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux9},
pages = {477--487},
publisher = {ACM},
series = {DIS '17},
title = {{Designing Authentic Emotions for Non-Human Characters: A Study Evaluating Virtual Affective Behavior}},
url = {http://doi.acm.org/10.1145/3064663.3064755},
year = {2017}
}
@inproceedings{ISI:000380377400012,
abstract = {Facial Expression Recognition (FER) is one of the most active topics in the domain of computer vision and pattern recognition, and it has received increasing attention for its wide application potentials as well as attractive scientific challenges. In this paper, we present a novel method to automatic 3D FER based on geometric scattering representation. A set of maps of shape features in terms of multiple order differential quantities, i.e. the Normal Maps (NOM) and the Shape Index Maps (SIM), are first jointly adopted to comprehensively describe geometry attributes of the facial surface. The scattering operator is then introduced to further highlight expression related cues on these maps, thereby constructing geometric scattering representations of 3D faces for classification. The scattering descriptor not only encodes distinct local shape changes of various expressions as by several milestone descriptors, such as SIFT, HOG, etc., but also captures subtle information hidden in high frequencies, which is quite crucial to better distinguish expressions that are easily confused. We evaluate the proposed approach on the BU-3DFE database, and the performance is up to 84.8{\%} and 82.7{\%} with two commonly used protocols respectively which is superior to the state of the art ones.},
annote = {IEEE 11th International Conference and Workshops on Automatic Face and
Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015},
author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
booktitle = {2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG), VOL. 5},
isbn = {978-1-4799-6026-2},
issn = {2326-5396},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
organization = {IEEE; IEEE Comp Soc; IEEE Biometric Council},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering Representation}},
year = {2015}
}
@inproceedings{8525654,
abstract = {We introduce a framework for detection and classification of spatio-temporal person-object interactions. Our method clusters similar semantic contexts from interactions detected from RGB-D data. 2-D object detection (YOLO) is run on RGB data from a Kinect v2 sensor on a mobile robot navigating an office and observing persons and desk spaces. Person and object detections are converted into 3-D point cloud time series via RGB-Depth co-registration and successive Euclidean and k-means spatial clustering. 3-D person and object point cloud streams are used to create time-series occupancy maps and person-object co-localization maps. From these maps, spatiotemporal correlations between persons and distinct objects are computed. Correlation patterns are clustered using k-means to obtain distinct human-object interactions, i.e. segment semantic context over time. We evaluated the performance of our approach to detect person-object correlations and cluster semantic context by recording 90 30-second RGB-D data episodes, with three persons handling representative objects (books, cups, bottles). Experimental results show that our framework is able to consistently assign semantic context to the same cluster in {\textgreater} 79{\%} of cases (scene frames). Semantic contexts in visual scenes can be distinguished without the need to provide prior information, allowing mobile agents to learn and explore in new environments.},
author = {Zapf, M P and Gupta, A and {Morales Saiki}, L Y and Kawanabe, M},
booktitle = {2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
doi = {10.1109/ROMAN.2018.8525654},
issn = {1944-9437},
keywords = {feature extraction,image classification,image colo,revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
month = {aug},
pages = {180--187},
title = {{Data-Driven, 3-D Classification of Person-Object Relationships and Semantic Context Clustering for Robotics and AI Applications}},
year = {2018}
}
@article{ISI:000368942000004,
abstract = {The ability of robots to meticulously cover large areas while gathering sensor data has widespread applications in precision agriculture. For autonomous operations in orchards, a suitable information management system is required, within which we can gather and process data relating to the state and performance of the crop over time, such as distinct yield count, canopy volume, and crop health. An efficient way to structure an information system is to discretize it to the individual tree, for which tree segmentation/detection is a key component. This paper presents a tree trunk detection pipeline for identifying individual trees in a trellis structured apple orchard, using ground-based lidar and image data. A coarse observation of trunk candidates is initially made using a Hough transformation on point cloud lidar data. These candidates are projected into the camera images, where pixelwise classification is used to update their likelihood of being a tree trunk. Detection is achieved by using a hidden semi-Markov model to leverage from contextual information provided by the repetitive structure of an orchard. By repeating this over individual orchard rows, we are able to build a tree map over the farm, which can be either GPS localized or represented topologically by the row and tree number. The pipeline was evaluated at a commercial apple orchard near Melbourne, Australia. Data were collected at different times of year, covering an area of 1.6 ha containing different apple varieties planted on two types of trellis systems: a vertical I-trellis structure and a Guttingen V-trellis structure. The results show good trunk detection performance for both apple varieties and trellis structures during the preharvest season (87-96{\%} accuracy) and near perfect trunk detection performance (99{\%} accuracy) during the flowering season. (C) 2015 Wiley Periodicals, Inc.},
annote = {9th International Conference on Field and Service Robotics (FSR),
Brisbane, AUSTRALIA, NOV 09-11, 2013},
author = {Bargoti, Suchet and Underwood, James P and Nieto, Juan I and Sukkarieh, Salah},
doi = {10.1002/rob.21583},
issn = {1556-4959},
journal = {JOURNAL OF FIELD ROBOTICS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {dec},
number = {8, SI},
pages = {1075--1094},
title = {{A Pipeline for Trunk Detection in Trellis Structured Apple Orchards}},
volume = {32},
year = {2015}
}
@inproceedings{7443288,
abstract = {Information about the emotional state of a person can be inferred from facial expressions. Emotion recognition has become an active research area in recent years in various fields such as Human Robot Interaction (HRI), medicine, intelligent vehicle, etc., The challenges in emotion recognition from images with pose variations, motivates researchers to explore further. In this paper, we have proposed a method based on geometric features, considering images of 7 yaw angles (-45°,-30°,-15°,0°,+15°,+30°,+45°) from BU3DFE database. Most of the work that has been reported considered only positive yaw angles. In this work, we have included both positive and negative yaw angles. In the proposed method, feature extraction is carried out by concatenating distance and angle vectors between the feature points, and classification is performed using neural network. The results obtained for images with pose variations are encouraging and comparable with literature where work has been performed on pitch and yaw angles. Using our proposed method non-frontal views achieve similar accuracy when compared to frontal view thus making it pose invariant. The proposed method may be implemented for pitch and yaw angles in future.},
annote = {From Duplicate 2 (Pose Invariant Method for Emotion Recognition from 3D Images - Suja, P; Krishnasri, D; Tripathi, Shikha)

12 IEEE Int C Elect Energy Env Communications Computer Control, New
Delhi, INDIA, DEC 17-20, 2015},
author = {Suja, P and Krishnasri, D and Tripathi, Shikha and {and S. Tripathi}},
booktitle = {2015 Annual IEEE India Conference (INDICON)},
doi = {10.1109/INDICON.2015.7443288},
isbn = {978-1-4673-6540-6},
issn = {2325-9418},
keywords = {emotion recognition,feature extraction,image class,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux6,tutux7},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux6,tutux7},
month = {dec},
pages = {1--5},
series = {Annual IEEE India Conference},
title = {{Pose invariant method for emotion recognition from 3D images}},
year = {2015}
}
@article{ISI:000455069600031,
abstract = {Individual tree crown (ITC) segmentation is an approach to isolate individual tree from the background vegetation and delineate precisely the crown boundaries for forest management and inventory purposes. ITC detection and delineation have been commonly generated from canopy height model (CHM) derived from light detection and ranging (LiDAR) data. Existing ITC segmentation methods, however, are limited in their efficiency for characterizing closed canopies, especially in tropical forests, due to the overlapping structure and irregular shape of tree crowns. Furthermore, the potential of 3-dimensional (3D) LiDAR data is not fully realized by existing CHM-based methods. Thus, the aim of this study was to develop an efficient framework for ITC segmentation in tropical forests using LiDAR-derived CHM and 3D point cloud data in order to accurately estimate tree attributes such as the tree height, mean crown width and aboveground biomass (AGB). The proposed framework entails five major steps: (1) automatically identifying dominant tree crowns by implementing semi-variogram statistics and morphological analysis; (2) generating initial tree segments using a watershed algorithm based on mathematical morphology; (3) identifying problematic segments based on predetermined set of rules; (4) tuning the problematic segments using a modified distance-based algorithm (DBA); and (5) segmenting and counting the number of individual trees based on the 3D LiDAR point clouds within each of the identified segment. This approach was developed in a way such that the 3D LiDAR points were only examined on problematic segments identified for further evaluations. 209 reference trees with diameter at breast height (DBH) 10 cm were selected in the field in two study areas in order to validate ITC detection and delineation results of the proposed framework. We computed tree crown metrics (e.g., maximum crown height and mean crown width) to estimate aboveground biomass (AGB) at tree level using previously published allometric equations. Accuracy assessment was performed to calculate percentage of correctly detected trees, omission and commission errors. Our method correctly identified individual tree crowns with detection accuracy exceeding 80 percent at both forest sites. Also, our results showed high agreement (R-2 {\textgreater} 0.64) in terms of AGB estimates using 3D LiDAR metrics and variables measured in the field, for both sites. The findings from our study demonstrate the efficacy of the proposed framework in delineating tree crowns, even in high canopy density areas such as tropical rainforests, where, usually the traditional algorithms are limited in their performances. Moreover, the high tree delineation accuracy in the two study areas emphasizes the potential robustness and transferability of our approach to other densely forested areas across the globe.},
author = {Jaafar, Wan Shafrina Wan Mohd and Woodhouse, Iain Hector and Silva, Carlos Alberto and Omar, Hamdan and Maulud, Khairul Nizam Abdul and Hudak, Andrew Thomas and Klauberg, Carine and Cardil, Adrian and Mohan, Midhun},
doi = {10.3390/f9120759},
issn = {1999-4907},
journal = {FORESTS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {dec},
number = {12},
title = {{Improving Individual Tree Crown Delineation and Attributes Estimation of Tropical Forests Using Airborne LiDAR Data}},
volume = {9},
year = {2018}
}
@inproceedings{ISI:000380407300092,
abstract = {This paper presents an implementation of face recognition, which is a very important task of human face identification. Line scratch detection in images is a highly challenging situation because of various characteristics of this defect. Few characteristics are considered with the different texture and geometry of images. We propose a useful algorithm for frame-by-frame line scratch detection in face image which deals with 3D approach and a filtering of detection. The temporary filtering algorithm can be used to remove false detection due to thin vertical structures by detecting the scratches on an image. Experimental evaluation can be detecting the lines and scratches on a face image and they used to solve this difficult approach. Our method is used with missing parts in an image. Three-dimensional face recognition is an extended method of facial recognition is considered according with the geometry and texture of a face. It has been elaborated that 3D face recognition methods can provide high accuracy as well as high detection with a comparison of 2D recognition. 3D avoids such mismatch effect of 2D face recognition algorithms. Additionally, most 3D scanners achieve both a 3D mesh and the texture of a face image. This allows combining the output of pure 3D matches with the more traditional algorithms of 2D face recognition, thus producing better performance.},
annote = {International Conference on Pervasive Computing (ICPC), Pune, INDIA, JAN
08-10, 2015},
author = {Pawar, Asmita A and Patil, Nitin N},
booktitle = {2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)},
isbn = {978-1-4799-6272-3},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
organization = {IEEE Pune Sect; IEEE Comp Soc; Savitribai Phule Pune Univ; IEEE Commun Soc Pune Chapter; Sinhgad Inst; Sakal Times},
title = {{Recognition of 3-D Faces with Missing Parts and Line Scratch Removal using New Technique}},
year = {2015}
}
@inproceedings{Shah2016,
abstract = {In applications such as 3D face synthesis and animation, a prominent face landmark is required to enable 3D face normalization, pose correction, 3D face recognition and reconstruction. Due to variations in facial expressions, automatic 3D face landmark localization remains a challenge. Nose tip is one of the salient landmarks in a human face. In this paper, a novel nose tip localization technique is proposed. In the proposed approach, the rotation of the 3D vector field is analyzed for robust and efficient nose tip localization. The proposed technique has the following three characteristics: (1) it does not require any training; (2) it does not rely on any particular model; (3) it is very efficient, requiring an average time of only 1.9s for nose tip detection. We tested the proposed technique on BU3DFE and Shrec'10 datasets. Experimental results show that the proposed technique is robust to variations in facial expressions, achieving a 100{\%} detection rate on these publicly available 3D face datasets. {\textcopyright} 2015 IEEE.},
annote = {From Duplicate 1 (Automatic 3D face landmark localization based on 3D vector field analysis - Shah, S A A; Bennamoun, M; Boussaid, F)

cited By 0

From Duplicate 2 (Automatic 3D face landmark localization based on 3D vector field analysis - Shah, Syed Afaq Ali; Bennamoun, Mohammed; Boussaid, Farid)

22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)

From Duplicate 3 (Automatic 3D face landmark localization based on 3D vector field analysis - Shah, Syed Afaq Ali; Bennamoun, Mohammed; Boussaid, Farid)

From Duplicate 1 (Automatic 3D face landmark localization based on 3D vector field analysis - Shah, Syed Afaq Ali; Bennamoun, Mohammed; Boussaid, Farid)

22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)

From Duplicate 2 (Automatic 3D face landmark localization based on 3D vector field analysis - Shah, S A A; Bennamoun, M; Boussaid, F)

cited By 0},
author = {Shah, Syed Afaq Ali and Bennamoun, Mohammed and Boussaid, Farid},
booktitle = {International Conference Image and Vision Computing New Zealand},
doi = {10.1109/IVCNZ.2015.7761526},
isbn = {978-1-5090-0357-0},
keywords = {computer animation,etapa1,face recognition,gil,id139,ieeexplore,image reconstr,lerdepois,revisao{\_}V1,revisao{\_}V2,revisao{\_}etapa1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {etapa1,gil,id139,ieeexplore,lerdepois,revisao{\_}V1,revisao{\_}V2,revisao{\_}etapa1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
month = {nov},
pages = {1--6},
publisher = {IEEE},
title = {{Automatic 3D face landmark localization based on 3D vector field analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006942604{\&}doi=10.1109{\%}2FIVCNZ.2015.7761526{\&}partnerID=40{\&}md5=5da7d80624be2691571e8fafab5b9dbf http://ieeexplore.ieee.org/document/7761526/},
volume = {2016-Novem},
year = {2016}
}
@article{ISI:000351134600001,
abstract = {The application of terrestrial laser scanning (TLS) in capturing forest inventory parameters such as diameter at breast height, height and diameters along stem profiles, and in monitoring forest growth, was investigated and validated by comparison with conventionally measured individual tree parameters and plot-level forest growth in a stand of Sitka spruce (Picea sitchensis (Bong.) Carr.) in Ireland. The data acquisition for all the plots with different tree sizes and different slopes was carried out using a terrestrial laser scanner (FARO LS 800 HE80) in November 2007 and November 2009, using the same plot centres and measurement procedures. The point cloud data were processed with Autostem (TM) software. The results showed that TLS enables the acquisition of forest stand parameters with an acceptable accuracy. Pruning of the lower branches did not improve tree recognition and the number of (partly) occluded trees stayed the same. Over the 2-year period, the average difference between the volume increment of the trees visible to the scanner derived using the conventional method and Autostem (TM) was 4.77 m(3) ha(-1) and resulted in scanner-derived estimates that were lower than the estimates obtained by conventional method by 6.1 {\%}. Using a simple correction factor to account for occlusion in the laser scanner data, the difference between these estimates for all trees in the stand became an over-estimation by 6.96 m(3) ha(-1) (8.1 {\%}). At heights up along the stems {\textgreater} 15 m, the errors in stem diameter estimates started to escalate.},
author = {Mengesha, Taye and Hawkins, Michael and Nieuwenhuis, Maarten},
doi = {10.1007/s10342-014-0844-0},
issn = {1612-4669},
journal = {EUROPEAN JOURNAL OF FOREST RESEARCH},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
month = {mar},
number = {2},
pages = {211--222},
title = {{Validation of terrestrial laser scanning data using conventional forest inventory methods}},
volume = {134},
year = {2015}
}
@inproceedings{ISI:000455305000011,
abstract = {Tree species information is crucial for accurate forest parameter estimation. Small footprint high density multi-return Light Detection and Ranging (LiDAR) data contain a large amount of structural details for modelling and thus distinguishing individual tree species. To fully exploit the potential of these data, we propose a data-driven tree species classification approach based on a volumetric analysis of single-tree-point-cloud that extracts features that are able to characterize both the internal and the external crown structure. The method captures the spatial distribution of the LiDAR points within the crown by generating a feature vector representing the three-dimensional (3D) crown information. Each element in the feature vector uniquely corresponds to an Elementary Quantization Volume (EQV) of the crown. Three strategies have been defined to generate unique EQVs that model different representations of the crown components. The classification is performed by using a Support Vector Machines (C-SVM) classifier using the histogram intersection kernel that has the enhanced ability to give maximum preference to the key features in high dimensional feature space. All the experiments were performed on a set of 200 trees belonging to Norway Spruce, European Larch, Swiss Pine, and Silver Fir (i.e., 50 trees per species). The classifier is trained using 120 trees and tested on an independent set of 80 trees. The proposed method outperforms the classification performance of the state-of-the-art method used for comparison.},
annote = {Conference on Image and Signal Processing for Remote Sensing XXIV,
Berlin, GERMANY, SEP 10-12, 2018},
author = {Harikumar, A and Paris, C and Bovolo, F and Bruzzone, L},
booktitle = {IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXIV},
doi = {10.1117/12.2325634},
editor = {{Bruzzone, L and Bovolo}, F},
isbn = {978-1-5106-2162-6},
issn = {0277-786X},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{A novel data-driven approach to tree species classification using high density multireturn airborne lidar data}},
volume = {10789},
year = {2018}
}
@article{ISI:000215156100010,
abstract = {Automatic human facial recognition is an important and complicated task; it is necessary to design algorithms capable of recognizing the constant patterns in the face and to use computing resources efficiently. In this paper we present a novel algorithm to recognize the human face in real time; the system's input is the depth and color data from the Microsoft KinectTM device. The algorithm recognizes patterns/shapes on the point cloud topography. The template of the face is based in facial geometry; the forensic theory classifies the human face with respect to constant patterns: cephalometric points, lines, and areas of the face. The topography, relative position, and symmetry are directly related to the craniometric points. The similarity between a point cloud cluster and a pattern description is measured by a fuzzy pattern theory algorithm. The face identification is composed by two phases: the first phase calculates the face pattern hypothesis of the facial points, configures each point shape, the related location in the areas, and lines of the face. Then, in the second phase, the algorithm performs a search on these face point configurations.},
author = {Fernandez-Cervantes, Victor and Garcia, Arturo and {Antonio Ramos}, Marco and Mendez, Andres},
doi = {10.13053/CyS-19-3-2015},
issn = {1405-5546},
journal = {COMPUTACION Y SISTEMAS},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux5},
number = {3},
pages = {529--546},
title = {{Facial Geometry Identification through Fuzzy Patterns with RGBD Sensor}},
volume = {19},
year = {2015}
}
@article{ISI:000429200400012,
abstract = {The gray short-tailed opossum (Monodelphis domestica) is a small marsupial gaining recognition as a laboratory animal in biomedical research. Despite numerous studies on opossum neuroanatomy, a consistent and comprehensive neuroanatomical reference for this species is still missing. Here we present the first three-dimensional, multimodal atlas of the Monodelphis opossum brain. It is based on four complementary imaging modalities: high resolution ex vivo magnetic resonance images, micro-computed tomography scans of the cranium, images of the face of the cutting block, and series of sections stained with the Nissl method and for myelinated fibers. Individual imaging modalities were reconstructed into a three-dimensional form and then registered to the MR image by means of affine and deformable registration routines. Based on a superimposition of the 3D images, 113 anatomical structures were demarcated and the volumes of individual regions were measured. The stereotaxic coordinate system was defined using a set of cranial landmarks: interaural line, bregma, and lambda, which allows for easy expression of any location within the brain with respect to the skull. The atlas is released under the Creative Commons license and available through various digital atlasing web services.},
author = {Majka, Piotr and Chlodzinska, Natalia and Turlejski, Krzysztof and Banasik, Tomasz and Djavadian, Ruzanna L and Weglarz, Wladyslaw P and Wojcik, Daniel K},
doi = {10.1007/s00429-017-1540-x},
issn = {1863-2653},
journal = {BRAIN STRUCTURE {\&} FUNCTION},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {may},
number = {4},
pages = {1779--1795},
title = {{A three-dimensional stereotaxic atlas of the gray short-tailed opossum (Monodelphis domestica) brain}},
volume = {223},
year = {2018}
}
@inproceedings{ISI:000352725200030,
abstract = {The analysis of individual trees is an important field of research in the forest remote sensing community. While the current state-of-the-art mostly focuses on the exploitation of optical imagery and airborne LiDAR data, modern SAR sensors have not yet met the interest of the research community in that regard. This paper describes how several critical parameters of individual deciduous trees can be extraced from airborne multi-aspect TomoSAR point clouds: First, the point cloud is segmented by unsupervised mean shift clustering. Then ellipsoid models are fitted to the points of each cluster. Finally, from these 3D ellipsoids the geometrical tree parameters location, height and crown radius are extracted. Evaluation with respect to a manually derived reference dataset prove that almost 86{\%} of all trees are localized, thus providing a promising perspective for further research towards individual tree recognition from SAR data.},
annote = {Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
Univ Munchen, Munich, GERMANY, MAR 25-27, 2015},
author = {Shahzad, M and Schmitt, M and Zhu, X X},
booktitle = {PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I},
doi = {10.5194/isprsarchives-XL-3-W2-205-2015},
editor = {{Stilla, U and Heipke}, C},
issn = {2194-9034},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {W2},
organization = {ISPRS},
pages = {205--209},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{SEGMENTATION AND CROWN PARAMETER EXTRACTION OF INDIVIDUAL TREES IN AN AIRBORNE TOMOSAR POINT CLOUD}},
volume = {40-3},
year = {2015}
}
@article{ISI:000403031400006,
abstract = {Canopy transmittance is a directional and wavelength-specific physical parameter that quantifies the amount of radiation attenuated when passing through a vegetation layer. The parameter has been estimated from LiDAR data in many different ways over the years. While early LiDAR methods treated each returned echo equally or weighted the echoes according to their return order, recent methods have focused more on the echo energy. In this study, we suggest a new method of estimating the total canopy transmittance considering only the energy of ground echoes. Therefore, this method does not require assumptions for the reflectance or absorption behavior of vegetation. As the oblique looking geometry of LiDAR is explicitly considered, canopy transmittance can be derived for individual laser beams and can be mapped spatially. The method was applied on a contemporary full-waveform LiDAR data set collected under leaf-off conditions and over a study site that contains two sub regions: one with a mixed (coniferous and deciduous) forest and another that is predominantly a deciduous forest in an alluvial plain. The resulting canopy transmittance map was analyzed for both sub regions and compared to aerial photos and the well-known fractional cover method. A visual comparison with aerial photos showed that even single trees and small canopy openings are visible in the canopy transmittance map. In comparison with the fractional cover method, the canopy transmittance map showed no saturation, i.e., there was better separability between patches with different vegetation structure. (C) 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.},
author = {Milenkovic, Milutin and Wagner, Wolfgang and Quast, Raphael and Hollaus, Markus and Ressl, Camillo and Pfeifer, Norbert},
doi = {10.1016/j.isprsjprs.2017.03.008},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jun},
pages = {61--72},
title = {{Total canopy transmittance estimated from small-footprint, full-waveform airborne LiDAR}},
volume = {128},
year = {2017}
}
@article{ISI:000436148300003,
abstract = {In this paper, we propose a bimodal 3D facial recognition method aimed at increasing the recognition rate and reducing the effect of illumination, pose, expression, ages, and occlusion on facial recognition. There are two features extracted from the multiscale sub-blocks in both the 3D mode depth map and 2D mode intensity map, which are the local gradient pattern (LGP) feature and the weighted histogram of gradient orientation (WHGO) feature. LGP and WHGO features are cascaded to form the 3D facial feature vector LGP-WHGO, and are further trained and identified by the support vector machine (SVM). Experiments on the CASIA database, FRGC v2.0 database, and Bosphorus database show that, the proposed method can efficiently extract the structure information and texture information of the facial image, and have a robustness to illumination, expression, occlusion and pose.},
author = {Guo, Yingchun and Wei, Ruoyu and Liu, Yi},
doi = {10.3390/info9030048},
issn = {2078-2489},
journal = {INFORMATION},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
month = {mar},
number = {3},
title = {{Weighted Gradient Feature Extraction Based on Multiscale Sub-Blocks for 3D Facial Recognition in Bimodal Images}},
volume = {9},
year = {2018}
}
@inproceedings{7081271,
abstract = {The analysis of facial appearance is significant to an early diagnosis of medical genetic diseases. The fast development of image processing and machine learning techniques facilitates the detection of facial dysmorphic features. This paper is a survey of the recent studies developed for the screening of genetic abnormalities across the facial features obtained from two dimensional and three dimensional images.},
annote = {From Duplicate 1 (Using facial images for the diagnosis of genetic syndromes: A survey - Rai, Marwa Chendeb E L; Werghi, Naoufel; Al Muhairi, Hassan; Alsafar, Habiba)

2015 International Conference on Communications, Signal Processing, and
their Applications (ICCSPA'15), Sharjah, U ARAB EMIRATES, FEB 17-19,
2015},
author = {{EL Rai}, M C and Werghi, Naoufel and {Al Muhairi}, Hassan and Alsafar, Habiba and Rai, Marwa Chendeb E L and Werghi, Naoufel and {Al Muhairi}, Hassan and Alsafar, Habiba},
booktitle = {2015 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, SIGNAL PROCESSING, AND THEIR APPLICATIONS (ICCSPA'15)},
doi = {10.1109/ICCSPA.2015.7081271},
isbn = {978-1-4799-6532-8},
keywords = {diseases,face recognition,learning (artificial int,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {feb},
organization = {Amer Univ Sharjah; IEEE},
pages = {1--6},
title = {{Using facial images for the diagnosis of genetic syndromes: A survey}},
year = {2015}
}
@article{ISI:000354623800005,
abstract = {Several anatomic structures of the middle ear are not optimally depicted in the standard axial and coronal planes. Several 2D and 3D image-processing modalities are currently available for CT examinations in clinical radiology departments. Till now 3D reconstructions of the temporal bone have not been widely used yet, and attracted only academic interest. The aim of this study was to compare axial (source images), 2D and 3DCT post-processing modalities, and to evaluate the value of 3D reconstructed images/virtual endoscopy (VE) in assessment of various middle ear disorders for identification of the best modality/view for assessment of a particular middle ear structure or pathology. 40 patients with various middle ear disorders, planned for surgical intervention were included in prospective study. Multi-slice CT was performed for all patients. Scans were acquired in the axial plane. The axial source datasets were utilized for generation of 2D reformations and 3D reconstructed images. All studied images were divided into three categories: axial (source images), 2D reformations (MPR and sliding-thin-slab MIP) and 3D reconstruction (virtual endoscopy). The visibility of middle ear structures and pathologies with each modality were scored qualitatively using three-point scoring system in reference to operative findings. Stapes superstructure and footplate, incudostapedial joint, oval and round windows, tympanic segment of the facial nerve and tegmen were not optimally depicted in the axial plane. Sinus tympani and facial recess were best visualized with axial images or VE. 3D reconstruction/VE allowed good visualization of all parts of ossicular chain except stapes superstructure. Regarding pathologic changes, 2D reformations and 3D reconstructed images allowed better visualization of erosion of ossicles and tegmen. 3D reconstruction/VE did not allow detection of foci of otospongiosis. 2D reformations can be considered the mainstay in assessment of most middle ear structures and pathologies. 3D reconstruction/VE seems to provide a useful method for a preoperative general overview of the middle ear anatomy, particularly for the ossicular chain, round window and retrotympanum.},
author = {Mehanna, Ahmed Mohamed and Baki, Fatthi Abdel and Eid, Mohamed and Negm, Magdy},
doi = {10.1007/s00405-014-2920-y},
issn = {0937-4477},
journal = {EUROPEAN ARCHIVES OF OTO-RHINO-LARYNGOLOGY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jun},
number = {6},
pages = {1357--1370},
title = {{Comparison of different computed tomography post-processing modalities in assessment of various middle ear disorders}},
volume = {272},
year = {2015}
}
@article{ISI:000355894900023,
abstract = {3D scene analysis in terms of automatically assigning 3D points a respective semantic label has become a topic of great importance in photogrammetiy, remote sensing, computer vision and robotics. In this paper, we address the issue of how to increase the distinctiveness of geometric features and select the most relevant ones among these for 3D scene analysis. We present a new, fully automated and versatile framework composed of four components: (i) neighborhood selection, (ii) feature extraction, (iii) feature selection and (iv) classification. For each component, we consider a variety of approaches which allow applicability in terms of simplicity, efficiency and reproducibility, so that end-users can easily apply the different components and do not require expert knowledge in the respective domains. In a detailed evaluation involving 7 neighborhood definitions, 21 geometric features, 7 approaches for feature selection, 10 classifiers and 2 benchmark datasets, we demonstrate that the selection of optimal neighborhoods for individual 3D points significantly improves the results of 3D scene analysis. Additionally, we show that the selection of adequate feature subsets may even further increase the quality of the derived results while significantly reducing both processing time and memory consumption. (C) 2015 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.},
author = {Weinmann, Martin and Jutzi, Boris and Hinz, Stefan and Mallet, Clement},
doi = {10.1016/j.isprsjprs.2015.01.016},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jul},
pages = {286--304},
title = {{Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers}},
volume = {105},
year = {2015}
}
@article{ISI:000430067700001,
abstract = {Background: Along with the conventional deposition of physical types at natural history museums, the deposition of 3-dimensional (3D) image data has been proposed for rare and valuable museum specimens, such as irrepla ceable type material. Findings: Micro computed tomography (mu CT) scan data of 5 hermit crab species from South Africa, including rare specimens and type material, depicted main identification characteristics of calcified body parts. However, low-image contrasts, especially in larger ({\textgreater} 50 mm total length) specimens, did not allow sufficient 3D reconstructions of weakly calcified and fine characteristics, such as soft tissue of the pleon, mouthparts, gills, and setation. Reconstructions of soft tissue were sometimes possible, depending on individual sample and scanning characteristics. The raw data of seven scans are publicly available for download from the GigaDB repository. Conclusions: Calcified body parts visualized from mu CT data can aid taxonomic validation and provide additional, virtual deposition of rare specimens. The use of a nondestructive, nonstaining mu CT approach for taxonomy, reconstructions of soft tissue structures, microscopic spines, and setae depend on species characteristics. Constrained to these limitations, the presented dataset can be used for future morphological studies. However, our virtual specimens will be most valuable to taxonomists who can download a digital avatar for 3D examination. Simultaneously, in the event of physical damage to or loss of the original physical specimen, this dataset serves as a vital insurance policy.},
author = {Landschoff, Jannes and {Du Plessis}, Anton and Griffiths, Charles L},
doi = {10.1093/gigascience/giy022},
issn = {2047-217X},
journal = {GIGASCIENCE},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
number = {4},
title = {{A micro X-ray computed tomography dataset of South African hermit crabs (Crustacea: Decapoda: Anomura: Paguroidea) containing scans of two rare specimens and three recently described species}},
volume = {7},
year = {2018}
}
@inproceedings{7507339,
abstract = {The latest generation of medical visualizations systems that provide gesture based interaction usually rely on closed source software modules. This paper presents a novel approach since the interaction with the rendered 3D images is done via a Web browser. The entire system is based on open source software components and this way eliminates the requirement to have a specific operating system preinstalled. Our team used a Leap Motion controller that allows the rotation, panning, scaling and selection of individual slices of a reconstructed 3D model based on a prior CT (Computed Tomography) or MRI (Magnetic Resonance Imaging) scan of a patient. The results showed that is feasible to build such a system and that the interaction with the model can be done in real-time. It was concluded that this Web oriented architecture could provide a sustainable alternative for interacting with medical images.},
author = {Virag, I and Stoicu-Tivadar, L and Crişan-Vida, M},
booktitle = {2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI)},
doi = {10.1109/SACI.2016.7507339},
keywords = {biomedical MRI,computerised tomography,gesture rec,revisao{\_}V2,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
month = {may},
pages = {519--523},
title = {{Gesture-based interaction in medical interfaces}},
year = {2016}
}
@article{ISI:000382679900012,
abstract = {A robust technique for recognition of 3D faces which performs well with face images with various poses, expressions and occlusions. In this method, the face images represented in 3D mesh format are smoothed using trilinear interpolation and then converted to 2.5D image or range images. Nose-tip which is the most prominent feature on human face is detected first on the corner points selected by 3D Harris corner and curvedness at those corner points. K-Means clustering is applied to group those corner points in 2 groups. The cluster of points with larger curvedness values represents the possible locations of nose-tip. Nose-tip is finally localized using Mean-Gaussian curvature values of the prospective corner points in that cluster. Using the nose-tip location, other facial landmarks namely corners of the eyes and mouth are located and a facial graph is generated. The dimensionality of 2.5D feature space is that, depth values are stored at each (x, y) grid of the 2.5D image, so a 3D face image uses some function to map the depth value at any pixel position to the intensity with which that pixel will be displayed. Here finally extracted features for each subject is of dimensionality {\{}[{\}}1x21], taking into account the Euclidean distances in three dimensional form between each feature points detected automatically. Taking Euclidean distances between all pairs of landmark points as features, face images are classified using Multilayer Perceptron (MLP), as well as Support Vector Machines (SVM). Maximum recognition rates of 75 and 87.5 {\%} have been obtained in case of Bosphorus Databases, 62.5 and 87.5 {\%} in case of GavabDB databases, 75 and 87.5 {\%} in case of Frav3D Databases by Multilayer Perceptron and Support Vector Machines respectively.},
author = {Bagchi, Parama and Bhattacharjee, Debotosh and Nasipuri, Mita},
doi = {10.1007/s11042-015-2835-7},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {18},
pages = {11059--11096},
title = {{A robust analysis, detection and recognition of facial features in 2.5D images}},
volume = {75},
year = {2016}
}
@inproceedings{ISI:000457913100097,
abstract = {In this paper, we present our latest progress in Emotion Recognition techniques, which combines acoustic features and facial features in both non-temporal and temporal mode. This paper presents the details of our techniques used in the Audio-Video Emotion Recognition subtask in the 2018 Emotion Recognition in the Wild (EmotiW) Challenge. After the multimodal results fusion, our final accuracy in Acted Facial Expression in Wild (AFEW) test dataset achieves 61.87{\%}, which is 1.53{\%} higher than the best results last year. Such improvements prove the effectiveness of our methods.},
annote = {20th ACM International Conference on Multimodal Interaction (ICMI),
Boulder, CO, OCT 16-20, 2018},
author = {Liu, Chuanhe and Tang, Tianhao and Lv, Kui and Wang, Minghao},
booktitle = {ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION},
doi = {10.1145/3242969.3264989},
isbn = {978-1-4503-5692-3},
keywords = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {Assoc Comp Machinery; Assoc Comp Machinery SIGCHI; Openstream; Microsoft; Univ Colorado Boulder, Inst Cognit Sci; audEERING},
pages = {630--634},
title = {{Multi-Feature Based Emotion Recognition for Video Clips}},
year = {2018}
}
@article{ISI:000410870200008,
abstract = {Humans present clear demographic traits which allow their peers to recognize their gender and ethnic groups as well as estimate their age. Abundant literature has investigated the problem of automated gender, ethnicity and age recognition from facial images. However, despite the co-existence of these traits, most of the studies have addressed them separately, very little attention has been given to their correlations. In this work, we address the problem of joint demographic estimation and investigate the correlation through the morphological differences in 3D facial shapes. To this end, a set of facial features are extracted to capture the 3D shape differences among the demographic groups. Then, a correlation-based feature selection is applied to highlight salient features and remove redundancy. These features are later fed to Random Forest for gender and ethnicity classification, and age estimation. Extensive experiments conducted on FRGCv2 dataset, under Expression-Dependent and Expression-Independent settings, demonstrate the effectiveness of the proposed approaches for the three traits, and also show the accuracy improvement when considering their correlations. To the best of our knowledge, this is the first study exploring the correlations of these facial soft-biometric traits using 3D faces. This is also the first work which studies the problem of age estimation from 3D Faces.(1) (C) 2017 Elsevier B.V. All rights reserved.},
author = {Xia, Baiqiang and {Ben Amor}, Boulbaba and Daoudi, Mohamed},
doi = {10.1016/j.imavis.2017.06.004},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {90--102},
title = {{= Joint gender, ethnicity and age estimation from 3D faces An experimental illustration of their correlations}},
volume = {64},
year = {2017}
}
@inproceedings{ISI:000407106200038,
abstract = {Most industrial products have (challenging) 3D shapes, many of them require traceability and individual marking. Although some laser marking systems on the market have 3D capabilities, they require the 3D shape to be loaded in the laser controller and the part to be precisely located. However, many industrial processes requiring direct part identification cannot fulfill those precise positioning requirements. To overcome these limitations, a 3D laser marker with integrated 3D imaging system was developed. This imaging system obtains the 3D image of the piece, and then the laser controller starts the marking process so that the focus fits on the part surface. The whole 3D data acquisition and transfer takes less than 3 s. This solves the problem of part positioning and simplifies the integration, while also providing 3D data of the surface that can be used for quality control.},
annote = {146th TMS Annual Meeting and Exhibition / Conference on Light Metals,
San Diego, CA, FEB 26-MAR 02, 2017},
author = {Fraser, Alex and Dallaire, Michael and Godmaire, Xavier P},
booktitle = {LIGHT METALS 2017},
doi = {10.1007/978-3-319-51541-0_38},
editor = {Ratvik, AP},
isbn = {978-3-319-51541-0; 978-3-319-51540-3},
issn = {2367-1181},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {Minerals Metals {\&} Mat Soc},
pages = {289--292},
series = {Minerals Metals {\&} Materials Series},
title = {{Laser Marking and 3D Imaging of Aluminum Products}},
year = {2017}
}
@article{Banita20182325,
abstract = {Face recognition are of great interest to researchers in terms of Image processing and Computer Graphics. In recent years, various factors become popular which clearly affect the face model. Which are ageing, universal facial expressions, and muscle movement. Similarly in terms of medical terminology the facial paralysis can be peripheral or central depending on the level of motor neuron lesion which can be below the nucleus of the nerve or supra nuclear. The various medical therapy used for facial paralysis are electroaccupunture, electro-therapy, laser acupuncture, manual acupuncture which is a traditional form of acupuncture. Imaging plays a great role in evaluation of degree of paralysis and also for faces recognition. There is a wide research in terms of facial expressions and facial recognition but lim-ited research work is available in facial paralysis. House- Brackmann Grading system is one of the simplest and easiest method to evalu-ate the degree of facial paralysis. During evaluation common facial expressions are recorded and are further evaluated by considering the focal points of the left or the right side of the face. This paper presents the classification of face recognition and its respective fuzzy rules to remove uncertainty in the result after evaluation of facial paralysis. {\textcopyright} 2018 Banita, Dr. Poonam Tanwar.},
annote = {cited By 0},
author = {Banita and Tanwar, P},
doi = {10.14419/ijet.v7i4.13619},
journal = {International Journal of Engineering and Technology(UAE)},
keywords = {revisao{\_}V1,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutux9},
number = {4},
pages = {2325--2331},
title = {{Evaluation of 3d facial paralysis using fuzzy logic}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053901681{\&}doi=10.14419{\%}2Fijet.v7i4.13619{\&}partnerID=40{\&}md5=f68480ef298793b35ad707b17c64ab59},
volume = {7},
year = {2018}
}
@inproceedings{8099972,
abstract = {We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face models visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.},
author = {Sheng, L and Cai, J and Cham, T and Pavlovic, V and Ngan, K N},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.489},
issn = {1063-6919},
keywords = {face recognition,object tracking,pose estimation,r,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {4598--4607},
title = {{A Generative Model for Depth-Based Robust 3D Facial Pose Tracking}},
year = {2017}
}
@inproceedings{Nozawa:2016:FGR:2945078.2945102,
abstract = {3D facial shape reconstruction in the wild environments is an important research task in the field of CG and CV. This is because it can be applied to a lot of products, such as 3DCG video games and face recognition. One of the most popular 3D facial shape reconstruction techniques is 3D Model-based approach. This approach approximates a facial shape by using 3D face model, which is calculated by principal component analysis. [Blanz and Vetter 1999] performed a 3D facial reconstruction by fitting points from facial feature points of an input of single facial image to vertex of template 3D facial model named 3D Morphable Model. This method can reconstruct a facial shape from a variety of images which include different lighting and face orientation, as long as facial feature points can be detected. However, representation quality of the result depends on the number of 3D model resolution.},
address = {New York, New York, USA},
author = {Nozawa, Tsukasa and Kato, Takuya and Savkin, Pavel A. and Nozawa, Naoki and Morishima, Shigeo},
booktitle = {ACM SIGGRAPH 2016 Posters on - SIGGRAPH '16},
doi = {10.1145/2945078.2945102},
isbn = {9781450343718},
keywords = {3D reconstruction,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,shape from X,texture synthesis,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
pages = {1--2},
publisher = {ACM Press},
series = {SIGGRAPH '16},
title = {{3D facial geometry reconstruction using patch database}},
url = {http://doi.acm.org/10.1145/2945078.2945102 http://dl.acm.org/citation.cfm?doid=2945078.2945102},
year = {2016}
}
@article{ISI:000447286200001,
abstract = {Due to object recognition accuracy limitations, unmanned ground vehicles (UGVs) must perceive their environments for local path planning and object avoidance. To gather high-precision information about the UGV's surroundings, Light Detection and Ranging (LiDAR) is frequently used to collect large-scale point clouds. However, the complex spatial features of these clouds, such as being unstructured, diffuse, and disordered, make it difficult to segment and recognize individual objects. This paper therefore develops an object feature extraction and classification system that uses LiDAR point clouds to classify 3D objects in urban environments. After eliminating the ground points via a height threshold method, this describes the 3D objects in terms of their geometrical features, namely their volume, density, and eigenvalues. A back-propagation neural network (BPNN) model is trained (over the course of many iterations) to use these extracted features to classify objects into five types. During the training period, the parameters in each layer of the BPNN model are continually changed and modified via back-propagation using a non-linear sigmoid function. In the system, the object segmentation process supports obstacle detection for autonomous driving, and the object recognition method provides an environment perception function for terrain modeling. Our experimental results indicate that the object recognition accuracy achieve 91.5{\%} in outdoor environment.},
author = {Song, Wei and Zou, Shuanghui and Tian, Yifei and Fong, Simon and Cho, Kyungeun},
doi = {10.1186/s13673-018-0152-7},
issn = {2192-1962},
journal = {HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
title = {{Classifying 3D objects in LiDAR point clouds with a back-propagation neural network}},
volume = {8},
year = {2018}
}
@inproceedings{7852823,
abstract = {Recently, the synthesis of 3D dynamic expressions has become an important concern in computer graphics, facial recognition, etc. In this study, we propose a regression based joint subspace learning method for the automatic synthesis of 3D dynamic expression images. This method synthesizes 3D dynamic expression images from a single 2D facial image. We use two subspaces (the view subspace and the frame subspace) to synthesize a 3D image. First, we use the view subspace to estimate multi-view facial images from a front image. Next, we construct a 3D image using the estimated multi-view facial images. Finally, we estimate the 3D images in different frames by using the frame subspace to synthesis 3D dynamic expression images. This approach is unlike the conventional joint subspace learning in which, the coefficients estimated by the input image are directly used for synthesis. Furthermore, we propose using textural information to improve the accuracy of synthesized images.},
booktitle = {2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
doi = {10.1109/CISP-BMEI.2016.7852823},
keywords = {computer graphics,estimation theory,face recogniti,lerdepois,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {oct},
pages = {820--824},
title = {{Joint subspace learning for reconstruction of 3D facial dynamic expression from single image}},
year = {2016}
}
@inproceedings{7780547,
abstract = {We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds {\textgreater} 107 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.},
author = {Hackel, T and Wegner, J D and Schindler, K},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.178},
issn = {1063-6919},
keywords = {edge detection,feature extraction,graph theory,ima,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {1610--1618},
title = {{Contour Detection in Unstructured 3D Point Clouds}},
year = {2016}
}
@article{ISI:000427548400003,
abstract = {Macromolecular interactions occur with widely varying affinities. Strong interactions form well defined interfaces but weak interactions are more dynamic and variable. Weak interactions can collectively lead to large structures such as microvilli via cooperativity and are often the precursors of much stronger interactions, e.g. the initial actin-myosin interaction during muscle contraction. Electron tomography combined with subvolume alignment and classification is an ideal method for the study of weak interactions because a 3-D image is obtained for the individual interactions, which subsequently are characterized collectively. Here we describe a method to characterize heterogeneous F-actin-aldolase interactions in 2-D rafts using electron tomography. By forming separate averages of the two constituents and fitting an atomic structure to each average, together with the alignment information which relates the raw motif to the average, an atomic model of each crosslink is determined and a frequency map of contact residues is computed. The approach should be applicable to any large structure composed of constituents that interact weakly and heterogeneously.},
author = {Hu, Guiqing and Taylor, Dianne W and Liu, Jun and Taylor, Kenneth A},
doi = {10.1016/j.jsb.2017.11.005},
issn = {1047-8477},
journal = {JOURNAL OF STRUCTURAL BIOLOGY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
number = {3},
pages = {199--209},
title = {{Identification of interfaces involved in weak interactions with application to F-actin-aldolase rafts}},
volume = {201},
year = {2018}
}
@inproceedings{ISI:000413068300005,
abstract = {In this paper, we propose and compare three methods for recognizing emotions from facial expressions using 4D videos. In the first two methods, the 3D faces are re-sampled by using curves to extract the feature information. Two different methods are presented to resample the faces in an intelligent way using parallel curves and radial curves. The movement of the face is measured through these curves using two frames: neutral and peak frame. The deformation matrix is formed by computing the distance point to point on the corresponding curves of the neutral frame and peak frame. This matrix is used to create the feature vector that will be used for classification using Support Vector Machine (SVM). The third method proposed is to extract the feature information from the face by using surface normals. At every point on the frame, surface normals are extracted. The deformation matrix is formed by computing the Euclidean distances between the corresponding normals at a point on neutral and peak frames. This matrix is used to create the feature vector that will be used for classification of emotions using SVM. The proposed methods are analyzed and they showed improvement over existing literature.},
annote = {8th International Conference on Intelligent Human Computer Interaction
(IHCI), Pilani, INDIA, DEC 12-13, 2016},
author = {Prathusha, Sai S and Suja, P and Tripathi, Shikha and Louis, R},
booktitle = {INTELLIGENT HUMAN COMPUTER INTERACTION, IHCI 2016},
doi = {10.1007/978-3-319-52503-7_5},
editor = {{Basu, A and Das, S and Horain, P and Bhattacharya}, S},
isbn = {978-3-319-52503-7; 978-3-319-52502-0},
issn = {0302-9743},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
organization = {Council Sci {\&} Ind Res, Cent Elect Engn Res Inst Pilani; Birla Inst Technol {\&} Sci Pilani; Indian Inst Informat Technol},
pages = {51--64},
series = {Lecture Notes in Computer Science},
title = {{Emotion Recognition from Facial Expressions of 4D Videos Using Curves and Surface Normals}},
volume = {10127},
year = {2017}
}
@article{Yuan:2018:GIH:3205271.3205279,
address = {Los Alamitos, CA, USA},
author = {Yuan, Lin and Chen, Fanglin and Zeng, Ling-Li and Wang, Lubin and Hu, Dewen},
doi = {10.1109/TCBB.2015.2448081},
issn = {1545-5963},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
keywords = {revisao{\_}V2,revisao{\_}acm,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {mar},
number = {2},
pages = {551--561},
publisher = {IEEE Computer Society Press},
title = {{Gender Identification of Human Brain Image with A Novel 3D Descriptor}},
url = {https://doi.org/10.1109/TCBB.2015.2448081},
volume = {15},
year = {2018}
}
@inproceedings{7558813,
abstract = {Using Kinect acquired RGB-D image to obtain a face feature parameters and three-dimensional coordinates of the characteristic parameters, and to select the characteristic parameter Facial by Candide-3 model, and feature extraction and normalization. Smile face expression data collection through Kinect, SVM collected to smiley face data classify and output the result of recognition, and the results compared with two-dimensional image of smiling face expression recognition results. Experimental results show that three-dimensional image of smiling face expression recognition accuracy than the two-dimensional image of smiling face. This research has important significance for the research and application of facial expression recognition technology.},
author = {Liu, S and Chen, X and Fan, D and Chen, X and Meng, F and Huang, Q},
booktitle = {2016 IEEE International Conference on Mechatronics and Automation},
doi = {10.1109/ICMA.2016.7558813},
issn = {2152-744X},
keywords = {emotion recognition,face recognition,feature extra,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux7},
month = {aug},
pages = {1661--1666},
title = {{3D smiling facial expression recognition based on SVM}},
year = {2016}
}
@inproceedings{ISI:000423869700004,
abstract = {The concept of remote sensing is to provide information about a wide-range area without making physical contact with this area. If, additionally to satellite imagery, images and videos taken by drones provide a more up-to-date data at a higher resolution, or accurate vector data is downloadable from the Internet, one speaks of sensor data fusion. The concept of sensor data fusion is relevant for many applications, such as virtual tourism, automatic navigation, hazard assessment, etc. In this work, we describe sensor data fusion aiming to create a semantic 3D model of an extremely interesting yet challenging dataset: An alpine region in Southern Germany. A particular challenge of this work is that rock faces including overhangs are present in the input airborne laser point cloud. The proposed procedure for identification and reconstruction of overhangs from point clouds comprises four steps: Point cloud preparation, filtering out vegetation, mesh generation and texturing. Further object types are extracted in several interesting subsections of the dataset: Building models with textures from UAV (Unmanned Aerial Vehicle) videos, hills reconstructed as generic surfaces and textured by the orthophoto, individual trees detected by the watershed algorithm, as well as the vector data for roads retrieved from openly available shape files and GPS-device tracks. We pursue geo-specific reconstruction by assigning texture and width to roads of several pre-determined types and modeling isolated trees and rocks using commercial software. For visualization and simulation of the area, we have chosen the simulation system Virtual Battlespace 3 (VBS3). It becomes clear that the proposed concept of sensor data fusion allows a coarse reconstruction of a large scene and, at the same time, an accurate and up-to-date representation of its relevant subsections, in which simulation can take place.},
annote = {17th SPIE Conference on Earth Resources and Environmental Remote
Sensing/GIS Applications, Warsaw, POLAND, SEP 12-14, 2017},
author = {Haufel, Gisela and Bulatov, Dimitri and Solbrig, Peter},
booktitle = {EARTH RESOURCES AND ENVIRONMENTAL REMOTE SENSING/GIS APPLICATIONS VIII},
doi = {10.1117/12.2278237},
editor = {{Michel, U and Schulz, K and Nikolakopoulos, KG and Civco}, D},
isbn = {978-1-5106-1321-8; 978-1-5106-1320-1},
issn = {0277-786X},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Sensor Data Fusion for Textured Reconstruction and Virtual Representation of Alpine Scenes}},
volume = {10428},
year = {2017}
}
@inproceedings{8634657,
abstract = {How will I look afterwards? is a common question asked by the patients undergoing a cosmetic procedure. Cosmetic practitioners at present can only offer subjective and descriptive replies. This subjective prediction is a serious concern for patients undergoing cosmetic treatment and therefore necessitates the development of automatic techniques for facial quantification. This paper proposes a novel machine learning approach to quantify and predict the outcome of 3D facial rejuvenation prior to actual cosmetic procedure. The facial rejuvenation prediction results are achieved by estimating the dermal filler volume in 3D faces. This involves estimation of structural changes in 3D face images and to learn underlying structural mapping. Our preliminary experimental results show that the proposed model achieves superior prediction accuracy on real world dataset compared to baseline methods. The computational time analysis shows that the proposed technique is very efficient (at test time) which makes it suitable for real time applications.},
author = {{Ali Shah}, S A and Bennamoun, M and Molton, M},
booktitle = {2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)},
doi = {10.1109/IVCNZ.2018.8634657},
issn = {2151-2205},
keywords = {cosmetics,face recognition,learning (artificial in,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
month = {nov},
pages = {1--6},
title = {{A Fully Automatic Framework for Prediction of 3D Facial Rejuvenation}},
year = {2018}
}
@inproceedings{ISI:000425498402048,
abstract = {Given an image of a street scene in a city, this paper develops a new method that can quickly and precisely pinpoint at which location (as well as viewing direction) the image was taken, against a pre-stored large-scale 3D point-cloud map of the city. We adopt the recently developed 2D-3D direct feature matching framework for this task {\{}[{\}}23,31,32,42-44]. This is a challenging task especially for large-scale problems. As the map size grows bigger, many 3D points in the wider geographical area can be visually very similar-or even identical-causing severe ambiguities in 2D-3D feature matching. The key is to quickly and unambiguously find the correct matches between a query image and the large 3D map. Existing methods solve this problem mainly via comparing individual features' visual similarities in a local and per feature manner, thus only local solutions can be found, inadequate for large-scale applications. In this paper, we introduce a global method which harnesses global contextual information exhibited both within the query image and among all the 3D points in the map. This is achieved by a novel global ranking algorithm, applied to a Markov network built upon the 3D map, which takes account of not only visual similarities between individual 2D-3D matches, but also their global compatibilities (as measured by co-visibility) among all matching pairs found in the scene. Tests on standard benchmark datasets show that our method achieved both higher precision and comparable recall, compared with the state-of-the-art.},
annote = {16th IEEE International Conference on Computer Vision (ICCV), Venice,
ITALY, OCT 22-29, 2017},
author = {Liu, Liu and Li, Hongdong and Dai, Yuchao},
booktitle = {2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)},
doi = {10.1109/ICCV.2017.260},
isbn = {978-1-5386-1032-9},
issn = {1550-5499},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
organization = {IEEE; IEEE Comp Soc},
pages = {2391--2400},
series = {IEEE International Conference on Computer Vision},
title = {{Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map}},
year = {2017}
}
@inproceedings{ISI:000413240500077,
abstract = {This paper proposes a 3D face recognition approach based on facial pose estimation, which is robust to large pose variations in the unconstrained scene. Deep learning method is used to facial pose estimation, and the generation of partial MARS (Multimodal fAce and eaR Spherical) map reduces the probability of feature points appearing in the deformed region. Then we extract the features from the depth and texture maps. Finally, the matching scores from two types of maps should be calculated by Bayes decision to generate the final result. In the large pose variations, the recognition rate of the method in this paper is 94.6{\%}. The experimental results show that our approach has superior performance than the existing methods used on the MARS map, and has potential to deal with 3D face recognition in unconstrained scene.},
annote = {6th International Conference on Pattern Recognition Applications and
Methods (ICPRAM), Porto, PORTUGAL, FEB 24-26, 2017},
author = {Zhang, Tingting and Mu, Zhichun and Li, Yihang and Liu, Qing and Zhang, Yi},
booktitle = {ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS},
doi = {10.5220/0006244206330637},
editor = {{DeMarsico, M and DiBaja, GS and Fred}, A},
isbn = {978-989-758-222-6},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
pages = {633--637},
title = {{3D Face and Ear Recognition based on Partial MARS Map}},
year = {2017}
}
@inproceedings{ISI:000380483800005,
abstract = {Being the most distinct feature point in 3D facial landmarks, nose tip plays a significant role in 3D facial studies such as face detection, face recognition, facial features extraction, face alignment, etc. Successful detection of nose tip can facilitate many tasks of 3D facial studies. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, needs not training, can handle large rotations and occlusions. To reduce computational cost, we first remove small isolated regions from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, the Multi-angle Energy (ME) of each point is computed and sorted in descending order. Then the first m points in the descending order list are obtained and hierarchical clustering method is used to cluster these points. In the first h largest clusters, we can find one point with the largest ME. For all scales of the scale-space, we get a series of such points which are treated as nose tip candidates. For these candidates, we apply hierarchical clustering again. In the obtained largest cluster, we compute the mean value of ME. The ME of nose tip will be closest to the mean value. We evaluate our method in two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The experimental results verify the robustness of our method with a high nose tip detection rate. {\textcopyright} 2015 IEEE.},
annote = {From Duplicate 1 (Robust nose tip detection for face range images based on local features in scale-space - Liu, J; Zhang, Q; Zhang, C; Tang, C; and)

From Duplicate 2 (Robust nose tip detection for face range images based on local features in scale-space - Liu, J; Zhang, Q; Zhang, C; Tang, C)

cited By 1

From Duplicate 3 (Robust nose tip detection for face range images based on local features in scale-space - Liu, J; Zhang, Q; Zhang, C; Tang, C; and)

From Duplicate 2 (Robust nose tip detection for face range images based on local features in scale-space - Liu, J; Zhang, Q; Zhang, C; Tang, C)

cited By 1

From Duplicate 2 (ROBUST NOSE TIP DETECTION FOR FACE RANGE IMAGES BASED ON LOCAL FEATURES IN SCALE- SPACE - Liu, Jian; Zhang, Quan; Zhang, Chen; Tang, Chaojing)

International Conference on 3D Imaging (IC3D), Liege, BELGIUM, DEC
14-15, 2015},
author = {Liu, Jian and Zhang, Quan and Zhang, Chen and Tang, Chaojing and And},
booktitle = {2015 International Conference on 3D Imaging (IC3D)},
doi = {10.1109/IC3D.2015.7391814},
isbn = {978-1-5090-1265-7},
issn = {2379-1772},
keywords = {face recognition,feature extraction,lerdepois,object detecti,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {The Inst of Elect and Elect Engn; Signal Proc Soc; 3D Stereo Media},
pages = {1--8},
series = {International Conference on 3D Imaging},
title = {{ROBUST NOSE TIP DETECTION FOR FACE RANGE IMAGES BASED ON LOCAL FEATURES IN SCALE- SPACE}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963541984{\&}doi=10.1109{\%}2FIC3D.2015.7391814{\&}partnerID=40{\&}md5=df7d12e321f564181753e5d93c034bad},
year = {2016}
}
@article{ISI:000371787800087,
abstract = {Augmented user experiences in the cultural heritage domain are in increasing demand by the new digital native tourists of 21st century. In this paper, we propose a novel solution that aims at assisting the visitor during an outdoor tour of a cultural site using the unique first person perspective of wearable cameras. In particular, the approach exploits computer vision techniques to retrieve the details by proposing a robust descriptor based on the covariance of local features. Using a lightweight wearable board, the solution can localize the user with respect to the 3D point cloud of the historical landmark and provide him with information about the details at which he is currently looking. Experimental results validate the method both in terms of accuracy and computational effort. Furthermore, user evaluation based on real-world experiments shows that the proposal is deemed effective in enriching a cultural experience.},
author = {Alletto, Stefano and Abati, Davide and Serra, Giuseppe and Cucchiara, Rita},
doi = {10.3390/s16020237},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {2},
title = {{Exploring Architectural Details Through a Wearable Egocentric Vision Device}},
volume = {16},
year = {2016}
}
@inproceedings{7910452,
abstract = {An animated character has its own characteristics and behaviour. The animator needs to be skilled enough to make a complex animation, especially on making face expression. a Well defined facial expression can represent the emotional condition and make the animation more expressing the mood. But most facial animation is done by manually, frame-by-frame. It is very time-consuming. This research proposed a combination methods for handling the facial animation based on marker location and face surface from the 3D character. The motion data captured based on the location and movement of the marker then implemented on 3D face model to generate the motion guidance. As a guidance, this marker data role as a centroid of a vertex cluster. The cluster provided by implementing segmentation fp-NN Clustering method based on surface and can visualize the deformation using linear blend skinning methods. The result from this research shows that this system can automatically generate facial animations based on the marker data and the surface segmentation. The visualization of deformation arranges accordingly to the motion captured data and organized sequentially.},
annote = {24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
24/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Gunanto, Samuel Gandang and Hariadi, Mochamad and Yuniarno, Eko Mulyanto},
booktitle = {2016 2nd International Conference of Industrial, Mechanical, Electrical, and Chemical Engineering (ICIMECE)},
doi = {10.1109/ICIMECE.2016.7910452},
isbn = {978-1-5090-4161-9},
keywords = {computer animation,data visualisation,estela,etapa1,face recogni,id207,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {estela,etapa1,id207,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {260--263},
publisher = {IEEE},
title = {{Computer facial animation with synthesize marker on 3D faces surface}},
url = {http://ieeexplore.ieee.org/document/7910452/},
year = {2016}
}
@article{ISI:000424962000005,
abstract = {Due to constraints in manufacturing and construction, buildings and many of the manmade objects within them are often rectangular and composed of planar parts. Detection and analysis of planes is, therefore, central to processing point clouds captured in these spaces. This paper presents a study of the semantic information stored in the planar objects of noisy building point clouds. The dataset considered is the Scene Meshes Dataset with aNNotations (SceneNN), a collection of over 100 indoor scenes captured by consumer-grade depth cameras. All planar objects within the dataset are detected using a new point cloud segmentation method that applies Density Based Spatial Clustering of Applications with Noise (DBSCAN) in a six dimensional clustering space. With all planes isolated, an extensive list of features describing the planes is extracted and studied using feature selection. Then dimensionality reduction and unsupervised learning are used to explore the discriminative ability of the final feature set as well as emergent class groupings. Finally, we train a bagged decision tree classifier that achieves 71.2{\%} accuracy in predicting the object class from which individual planes originate.},
author = {Czerniawski, T and Sankaran, B and Nahangi, M and Haas, C and Leite, F},
doi = {10.1016/j.autcon.2017.12.029},
issn = {0926-5805},
journal = {AUTOMATION IN CONSTRUCTION},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {apr},
pages = {44--58},
title = {{6D DBSCAN-based segmentation of building point clouds for planar object classification}},
volume = {88},
year = {2018}
}
@article{ISI:000411153300086,
abstract = {The study evaluated the prognostic effect of standardized tumor volume in patients with advanced nasopharyngeal carcinoma (NPC) treated with concurrent chemoradiotherapy. Between Jan 1, 2009 and December 30, 2012, 143 patients diagnosed with NPC in UICC stage III-IVb by histopathology were enrolled in the study. These patients underwent intensity-modulated radiotherapy combined with concurrent chemotherapy. The three-dimensional images of tumor volume were reconstructed automatically by the treatment planning system. SGTVnx was calculated based on GTVnx/person's volume. SGTVnd was calculated based on GTVnd/person's volume. SGTVnx was significantly associated with the 5-year overall survival (OS), disease-free survival (DFS), DMFS, and LRFS rates in univariate and multivariate analyses. Although SGTVnd was associated with the 5-year OS, DFS, and DMFS rates, it was not an independent prognostic factor for LRFS. In receiver operating characteristic (ROC) curve analysis, 1.091 and 0.273 were determined as the cut-off points for SGTVnx and SGTVnd, respectively. The 5-year OS, DFS, DMFS, and LRFS rates for patients with a SGTVnx {\textgreater} 1.091 vs. SGTVnx {\textless}= 1.091 was 65.4{\%} vs. 93.4{\%} (P {\textless} 0.001), 65.2{\%} vs. 94.8{\%} (P {\textless} 0.001), 71.4{\%} vs. 97.4{\%} (P {\textless} 0.001), and 84.8{\%} vs. 97.3{\%} (P = 0.003), respectively, for SGTVnd {\textgreater} 0.273 vs. SGTVnd {\textless}= 0.273 was 70.3{\%} vs. 96.5{\%} (P {\textless} 0.001), 70.1{\%} vs. 94.8{\%} (P {\textless} 0.001), 77.5{\%} vs. 98.2{\%} (P {\textless} 0.001), and 88.5{\%} vs. 96.6{\%} (P = 0.049), respectively. UICC stage grouping, T classification, N classification, and sex were not found to be independent prognostic factors for NPC. Standardized tumor volume was an independent prognostic factor for NPC that might improve the current NPC TNM classification system and provide new clinical evidence for personalized treatment strategies.},
author = {Liu, Ting and Lv, Jun and Qin, Yutao},
doi = {10.18632/oncotarget.20313},
issn = {1949-2553},
journal = {ONCOTARGET},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
number = {41},
pages = {70299--70309},
title = {{Standardized tumor volume: an independent prognostic factor in advanced nasopharyngeal carcinoma}},
volume = {8},
year = {2017}
}
@article{ISI:000388855500007,
abstract = {Failure analysis aims at collecting information about how and why a failure is produced. The first step in this process is a visual inspection on the flaw surface that will reveal the features, marks, and texture, which characterize each type of fracture. This is generally carried out by personnel with no experience that usually lack the knowledge to do it. This paper proposes a classification method for three kinds of fractures in crystalline materials: brittle, fatigue, and ductile. The method uses 3D vision, and it is expected to support failure analysis. The features used in this work were: i) Haralick's features and ii) the fractal dimension. These features were applied to 3D images obtained from a confocal laser scanning microscopy Zeiss LSM 700. For the classification, we evaluated two classifiers: Artificial Neural Networks and Support Vector Machine. The performance evaluation was made by extracting four marginal relations from the confusion matrix: accuracy, sensitivity, specificity, and precision, plus three evaluation methods: Receiver Operating Characteristic space, the Individual Classification Success Index, and the Jaccard's coefficient. Despite the classification percentage obtained by an expert is better than the one obtained with the algorithm, the algorithm achieves a classification percentage near or exceeding the 60 {\%} accuracy for the analyzed failure modes. The results presented here provide a good approach to address future research on texture analysis using 3D data.},
author = {{Ximena Bastidas-Rodriguez}, Maria and Prieto-Ortiz, Flavio A and Espejo-Mora, Edgar},
doi = {10.19053/01211129.v25.n43.2016.5301},
issn = {0121-1129},
journal = {REVISTA FACULTAD DE INGENIERIA, UNIVERSIDAD PEDAGOGICA Y TECNOLOGICA DE COLOMBIA},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {43},
pages = {83--96},
title = {{Fractographic classification in metallic materials by using 3D processing and computer vision techniques}},
volume = {25},
year = {2016}
}
@inproceedings{ISI:000454677900019,
abstract = {Facial Recognition is a commonly used technology in security-related applications. It has been thoroughly studied and scrutinized for its number of practical real-world applications. On the road ahead of understanding this technology, there remain several obstacles. In this paper, methods of 3D face recognition are examined by measuring quantifiable applications and results. In facial recognition, three Dimensional Morphable Model (3DMM) techniques have attracted more and more attention as effectiveness in use increases over time. 3DMM provides automation and more accurate image rendering when compared to other traditional techniques. The accuracy in image rendering comes at a cost; as 3DMM requires more focus on texture estimation, shape-controlling limits, and extrinsic variations, accurately matching fitting models, feature tracking and precision identification. We have underlined different issues in comparison based on these methods.},
annote = {1st International Conference on Emerging Technologies in Computing
(ICETIC), London Metropolitan Univ, London, ENGLAND, AUG 23-24, 2018},
author = {Khan, Muhammad Sajid and Jehanzeb, Muhammad and Babar, Muhammad Imran and Faisal, Shah and Ullah, Zabeeh and Amin, Siti Zulaikha Binti Mohamad},
booktitle = {EMERGING TECHNOLOGIES IN COMPUTING, ICETIC 2018},
doi = {10.1007/978-3-319-95450-9_19},
editor = {{Miraz, MH and Excell, P and Ware, A and Soomro, S and Ali}, M},
isbn = {978-3-319-95450-9; 978-3-319-95449-3},
issn = {1867-8211},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
organization = {Int Assoc Educators {\&} Researchers; IEEE ComSoc Bahrain Chapter; British Comp Soc, N Wales Branch; EAI},
pages = {220--236},
series = {Lecture Notes of the Institute for Computer Sciences Social Informatics and Telecommunications Engineering},
title = {{Face Recognition Analysis Using 3D Model}},
volume = {200},
year = {2018}
}
@inproceedings{Fang:2018:RCB:3197768.3201576,
address = {New York, NY, USA},
author = {Fang, Qinyuan and Kyrarini, Maria and Ristic-Durrant, Danijela and Gr{\"{a}}ser, Axel},
booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
doi = {10.1145/3197768.3201576},
isbn = {978-1-4503-6390-7},
keywords = {3D Point Cloud,Assistive Robotics,Mouth Detection,RGB-D Camera,Robot Control,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
pages = {391--396},
publisher = {ACM},
series = {PETRA '18},
title = {{RGB-D Camera Based 3D Human Mouth Detection and Tracking Towards Robotic Feeding Assistance}},
url = {http://doi.acm.org/10.1145/3197768.3201576},
year = {2018}
}
@inproceedings{Butler:2016:CFE:2851581.2892535,
address = {New York, New York, USA},
author = {Butler, Crystal and Subramanian, Lakshmi and Michalowicz, Stephanie},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA '16},
doi = {10.1145/2851581.2892535},
isbn = {9781450340823},
keywords = {3D facial modeling,avatars,crowdsourcing,expression recognition,facial expressions,facs,revisao{\_}V2,revisao{\_}acm,tutux7},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux7},
pages = {2798--2804},
publisher = {ACM Press},
series = {CHI EA '16},
title = {{Crowdsourced Facial Expression Mapping Using a 3D Avatar}},
url = {http://doi.acm.org/10.1145/2851581.2892535 http://dl.acm.org/citation.cfm?doid=2851581.2892535},
year = {2016}
}
@article{ISI:000427313700006,
abstract = {In this paper, we propose a method to crowdsource the task of complex three-dimensional information extraction from 3D point clouds. We design web-based 3D micro tasks tailored to assess segmented LiDAR point clouds of urban trees and investigate the quality of the approach in an empirical user study. Our results for three different experiments with increasing complexity indicate that a single crowd sourcing task can be solved in a very short time of less than five seconds on average. Furthermore, the results of our empirical case study reveal that the accuracy, sensitivity and precision of 3D crowdsourcing are high for most information extraction problems. For our first experiment (binary classification with single answer) we obtain an accuracy of 91{\%}, a sensitivity of 95{\%} and a precision of 92{\%}. For the more complex tasks of the second Experiment 2 (multiple answer classification) the accuracy ranges from 65{\%} to 99{\%} depending on the label class. Regarding the third experiment - the determination of the crown base height of individual trees - our study highlights that crowdsourcing can be a tool to obtain values with even higher accuracy in comparison to an automated computer-based approach. Finally, we found out that the accuracy of the crowdsourced results for all experiments is hardly influenced by characteristics of the input point cloud data and of the users. Importantly, the results' accuracy can be estimated using agreement among volunteers as an intrinsic indicator, which makes a broad application of 3D micro-mapping very promising. (C) 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.},
author = {Herfort, Benjamin and Hoefle, Bernhard and Klonner, Carolin},
doi = {10.1016/j.isprsjprs.2018.01.009},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
pages = {73--83},
title = {{3D micro-mapping: Towards assessing the quality of crowdsourcing to support 3D point cloud analysis}},
volume = {137},
year = {2018}
}
@inproceedings{Amir:2016:DEV:3001773.3001804,
address = {New York, NY, USA},
author = {Amir, Mohd Hezri and Quek, Albert and Sulaiman, Nur Rasyid Bin and See, John},
booktitle = {Proceedings of the 13th International Conference on Advances in Computer Entertainment Technology},
doi = {10.1145/3001773.3001804},
isbn = {978-1-4503-4773-0},
keywords = {First-Person-Shooter,Gestures Recognition,Immersive Gameplay,Virtual Reality,revisao{\_}V2,revisao{\_}acm,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux9},
pages = {35:1----35:6},
publisher = {ACM},
series = {ACE '16},
title = {{DUKE: Enhancing Virtual Reality Based FPS Game with Full-body Interactions}},
url = {http://doi.acm.org/10.1145/3001773.3001804},
year = {2016}
}
@inproceedings{ISI:000371977804130,
abstract = {The need to compare separate objects arises in a wide range of applications. In one approach for comparing objects, `ShapeDNA' is constructed to give a numerical fingerprint representing an individual object. Shape-DNA is a cropped set of eigenvalues of the Laplace-Beltrami operator for the surface of the object. In this paper, we compute the Shape-DNA of surfaces using the closest point method. Our approach may be applied to a variety of surface representations including triangulations, point clouds and certain analytical shapes. A 2D multidimensional scaling plot illustrates that similar objects form groups based on the Shape-DNAs. Our method has the benefit that it may be applied to surfaces defined by dense point clouds without requiring the construction of point connectivity.},
annote = {IEEE International Conference on Image Processing (ICIP), Quebec City,
CANADA, SEP 27-30, 2015},
author = {Arteaga, Reynaldo J and Ruuth, Steven J},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4799-8339-1},
issn = {1522-4880},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
organization = {Inst Elect {\&} Elect Engineers; IEEE Signal Proc Soc},
pages = {4511--4515},
series = {IEEE International Conference on Image Processing ICIP},
title = {{LAPLACE-BELTRAMI SPECTRA FOR SHAPE COMPARISON OF SURFACES IN 3D USING THE CLOSEST POINT METHOD}},
year = {2015}
}
@article{ISI:000397995100002,
abstract = {The use of multispectral cameras deployed on unmanned aerial vehicles (UAVs) in land cover and vegetation mapping applications continues to improve and receive increasing recognition and adoption by resource management and forest survey practitioners. Comparisons of different camera data and platform performance characteristics are an important contribution in understanding the role and operational capability of this technology. In this article, object-based classification accuracies for different cover types and vegetation species of interest in central Ontario were examined using data from three UAV-based multispectral cameras. Five land-cover classes (forest, shrub, herbaceous, bare soil, and built-up) were determined to be up to 95{\%} correct overall with calibrated multispectral Parrot Sequoia digital camera data compared to independent field observations. The levels of classification accuracy decreased approximately 10-15{\%} when spectrally less capable consumer-grade RGB sensors were used. Multispectral Parrot Sequoia classification accuracy was approximately 89{\%} when more detailed vegetation classes, including individual deciduous tree species, shrub communities and agricultural crops, were analysed. Additional work is suggested in the use of such UAV multispectral and point cloud data in ash tree discrimination to support emerald ash borer infestation detection and management, and in analysis of functional and structural vegetation characteristics (e.g. leaf area index).},
annote = {Conference on Small Unmanned Aerial Systems (sUAS) for Environmental
Research, Univ Worcester, Worcester, ENGLAND, JUN, 2016},
author = {Ahmed, Oumer S and Shemrock, Adam and Chabot, Dominique and Dillon, Chris and Williams, Griffin and Wasson, Rachel and Franklin, Steven E},
doi = {10.1080/01431161.2017.1294781},
issn = {0143-1161},
journal = {INTERNATIONAL JOURNAL OF REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {8-10},
pages = {2037--2052},
title = {{Hierarchical land cover and vegetation classification using multispectral data acquired from an unmanned aerial vehicle}},
volume = {38},
year = {2017}
}
@article{ISI:000457939400106,
abstract = {Tree stem detection is a key step toward retrieving detailed stem attributes from terrestrial laser scanning (TLS) data. Various point-based methods have been proposed for the stem point extraction at both individual tree and plot levels. The main limitation of the point-based methods is their high computing demand when dealing with plot-level TLS data. Although segment-based methods can reduce the computational burden and uncertainties of point cloud classification, its application is largely limited to urban scenes due to the complexity of the algorithm, as well as the conditions of natural forests. Here we propose a novel and simple segment-based method for efficient stem detection at the plot level, which is based on the curvature feature of the points and connected component segmentation. We tested our method using a public TLS dataset with six forest plots that were collected for the international TLS benchmarking project in Evo, Finland. Results showed that the mean accuracies of the stem point extraction were comparable to the state-of-art methods ({\textgreater}95{\%}). The accuracies of the stem mappings were also comparable to the methods tested in the international TLS benchmarking project. Additionally, our method was applicable to a wide range of stem forms. In short, the proposed method is accurate and simple; it is a sensible solution for the stem detection of standing trees using TLS data.},
author = {Zhang, Wuming and Wan, Peng and Wang, Tiejun and Cai, Shangshu and Chen, Yiming and Jin, Xiuliang and Yan, Guangjian},
doi = {10.3390/rs11020211},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jan},
number = {2},
title = {{A Novel Approach for the Detection of Standing Tree Stems from Plot-Level Terrestrial Laser Scanning Data}},
volume = {11},
year = {2019}
}
@article{ISI:000370350100005,
abstract = {Accurate separation of photosynthetic and nonphotosynthetic components in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a challenging but of key importance to understand the spatial distribution of the radiation regime, photosynthetic processes, and carbon and water exchanges of the forest canopy. The objective of this paper was to improve current methods for separating photosynthetic and nonphotosynthetic components in TLS data of forest canopies by adding two additional filters only based on its geometric information. By comparing the proposed approach with the eigenvalues plus color information-based method, we found that the proposed approach could effectively improve the overall producer's accuracy from 62.12{\%} to 95.45{\%}, and the overall classification producer's accuracy would increase from 84.28{\%} to 97.80{\%} as the forest leaf area index (LAI) decreases from 4.15 to 3.13. In addition, variations in tree species had negligible effects on the final classification accuracy, as shown by the overall producer's accuracy for coniferous (93.09{\%}) and broadleaf (94.96{\%}) trees. To remove quantitatively the effects of the woody materials in a forest canopy for improving TLS-based LAI estimates, we also computed the ``woody-to-total area ratio{\{}''{\}} based on the classified linear class points from an individual tree. Automatic classification of the forest point cloud data set will facilitate the application of TLS on retrieving 3-D forest canopy structural parameters, including LAI and leaf and woody area ratios.},
author = {Ma, Lixia and Zheng, Guang and Eitel, Jan U H and Moskal, L Monika and He, Wei and Huang, Huabing},
doi = {10.1109/TGRS.2015.2459716},
issn = {0196-2892},
journal = {IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {2},
pages = {679--696},
title = {{Improved Salient Feature-Based Approach for Automatically Separating Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar Point Cloud Data of Forest Canopies}},
volume = {54},
year = {2016}
}
@inproceedings{ISI:000406771301004,
abstract = {Spoofing detection is essential for practical face recognition system. Based on the fact that genuine face has special geometric curvatures across surface, this paper brings forward an ultra-fast yet accurate spoofing detection approach using a low-cost stereo camera. To obtain curvatures, the three dimensional shapes of selected facial landmarks are analyzed, by fitting point cloud around each landmark to a specific partial face surface. Spoofing detection is then performed by evaluating curvatures of each landmark and integrating them together. Experiments verify that the approach is able to detect spoofed faces in printed photographs without or with various bending at FAR equal to 0.00{\%}. Meanwhile, genuine faces have a trivial opportunity to be falsely rejected: FRR is 0.59{\%} for near frontal faces and less than 5{\%} for faces with large varying poses. Detection time is 51 milliseconds when executed on a single processor {\{}[{\}}1] running at a clock frequency of 266M Hz, this makes the detection very suitable for embedded face recognition system.},
annote = {23rd International Conference on Pattern Recognition (ICPR), Mexican
Assoc Comp Vis Robot {\&} Neural Comp, Cancun, MEXICO, DEC 04-08, 2016},
author = {Tian, Guifen and Mori, Tatusya and Okuda, Yuji},
booktitle = {2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)},
isbn = {978-1-5090-4847-2},
issn = {1051-4651},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
organization = {Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm; Elsevier; IBM Res; INTEL; CONACYT},
pages = {1017--1022},
series = {International Conference on Pattern Recognition},
title = {{Spoofing Detection for Embedded Face Recognition System using A Low Cost Stereo Camera}},
year = {2016}
}
@inproceedings{8346387,
abstract = {In many scenarios related to image processing, 3D face reconstruction is considered an essential part. A robust and fast method of 3D face reconstruction from a single 2D image is presented. This method consists of three main steps including extraction of features from single image, calculating the depth of image and adjustment of a 3D model on the direction of Z-axis. In the first step, the features of image are extracted by using supervised descent method (SDM). Using SDM, face regions like facial components (eyes, nose lips) and face contours are detected. Second step consists of depth prediction by implementation of multivariate Gaussian distribution. Finally, 3D face is constructed with the help of features and the depth information and 3D database. The proposed method has been verified by conducting several experiments depicted in evaluation section. Our method is robust in nature and gives good results even using a single image, comparing to other methods that use multiple images for reconstruction of 3D images.},
author = {Afzal, H M R and Luo, S and Afzal, M K},
booktitle = {2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)},
doi = {10.1109/ICOMET.2018.8346387},
keywords = {Gaussian distr,face recognition,feature extraction,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {1--5},
title = {{Reconstruction of 3D facial image using a single 2D image}},
year = {2018}
}
@inproceedings{ISI:000371977802159,
abstract = {In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBP) for 3D face recognition. Using the framework proposed in {\{}[{\}}1], we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared to its depth-image counterpart, our approach is distinguished by the following features: a) inherits the intrinsic advantages of mesh surface; b) does not require normalization; c) can accommodate partial matching. In addition, it allows early-level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data.},
annote = {IEEE International Conference on Image Processing (ICIP), Quebec City,
CANADA, SEP 27-30, 2015},
author = {Tortorici, Claudio and Werghi, Naoufel and Berretti, Stefano},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4799-8339-1},
issn = {1522-4880},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
organization = {Inst Elect {\&} Elect Engineers; IEEE Signal Proc Soc},
pages = {2670--2674},
series = {IEEE International Conference on Image Processing ICIP},
title = {{BOOSTING 3D LBP-BASED FACE RECOGNITION BY FUSING SHAPE AND TEXTURE DESCRIPTORS ON THE MESH}},
year = {2015}
}
@article{ISI:000385597700004,
abstract = {Street trees interlaced with other objects in cluttered point clouds of urban scenes inhibit the automatic extraction of individual trees. This paper proposes a method for the automatic extraction of individual trees from mobile laser scanning data, according to the general constitution of trees. Two components of each individual tree - a trunk and a crown can be extracted by the dual growing method. This method consists of coarse classification, through which most of artifacts are removed; the automatic selection of appropriate seeds for individual trees, by which the common manual initial setting is avoided; a dual growing process that separates one tree from others by circumscribing a trunk in an adaptive growing radius and segmenting a crown in constrained growing regions; and a refining process that draws a singular trunk from the interlaced other objects. The method is verified by two datasets with over 98{\%} completeness and over 96{\%} correctness. The low mean absolute percentage errors in capturing the morphological parameters of individual trees indicate that this method can output individual trees with high precision. (C) 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.},
author = {Li, Lin and Li, Dalin and Zhu, Haihong and Li, You},
doi = {10.1016/j.isprsjprs.2016.07.009},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {oct},
pages = {37--52},
title = {{A dual growing method for the automatic extraction of individual trees from mobile laser scanning data}},
volume = {120},
year = {2016}
}
@article{ISI:000460046500034,
abstract = {This paper presents a review of the data acquisition procedures of geotechnical parameters for rock slope stability assessment and the proposal of some new improvements. For this purpose, a piece of research based on the slope mass rating classification system using close-range terrestrial digital photogrammetry (CR-TDP) has led to improvements in quality and timing of discontinuity data acquisition, and analyzes the suitability of each one of the parameters when applied to weak foliated rocks. TDP allows rapid 3D image acquisition of a rock slope, which can be analyzed using software to determine the geometrical parameters that affect stability. A fast procedure to perform the photogrammetric, non-contact survey in order to obtain the 3D images is shown in this paper. Being a rapid and single-person task, this procedure provides enough precision to be applied to weak foliated rock slopes with non-well-defined geometry. Furthermore, the study has focused on highly foliated rock outcrops, in which high resolution in the 3D images is very desirable. This research was applied to mountain road cuts, in which the use of TDP with a very close range was necessary. Through an application on weak rocks in the Alpujarras (Andalusia, Spain), this work analyzes the bias when applying TDP to materials such as these, under progressive weathering processes.},
author = {Alameda-Hernandez, Pedro and {El Hamdouni}, Rachid and Irigaray, Clemente and Chacon, Jose},
doi = {10.1007/s10064-017-1119-z},
issn = {1435-9529},
journal = {BULLETIN OF ENGINEERING GEOLOGY AND THE ENVIRONMENT},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
number = {2},
pages = {1157--1171},
title = {{Weak foliated rock slope stability analysis with ultra-close-range terrestrial digital photogrammetry}},
volume = {78},
year = {2019}
}
@article{ISI:000457037300020,
abstract = {Delineation of individual deciduous trees with Light Detection and Ranging (LiDAR) data has long been sought for accurate forest inventory in temperate forests. Previous attempts mainly focused on high-density LiDAR data to obtain reliable delineation results, which may have limited applications due to the high cost and low availability of such data. Here, the feasibility of individual deciduous tree delineation with low-density LiDAR data was examined using a point-density-based algorithm. First a high-resolution point density model (PDM) was developed from low-density LiDAR point cloud to locate individual trees through the horizontal spatial distribution of LiDAR points. Then, individual tree crowns and associated attributes were delineated with a 2D marker-controlled watershed segmentation. Additionally, the PDM-based approach was compared with a conventional canopy height model (CHM) based delineation. The results demonstrated that the PDM-based approach produced an 89{\%} detection accuracy to identify deciduous trees in our study area. The tree attributes derived from the PDM-based algorithm explained 81{\%} and 83{\%} of tree height and crown width variations of forest stands, respectively. The conventional CHM-based tree attributes, on the other hand, could explain only 71{\%} and 66{\%} of tree height and crown width, respectively. Our results suggest that the application of the PDM-based individual tree identification in deciduous forests with low-density LiDAR data is feasible and has relatively high accuracy to predict tree height and crown width, which are highly desired in large-scale forest inventory and analysis.},
author = {Shao, Gang and Shao, Guofan and Fei, Songlin},
doi = {10.1080/01431161.2018.1513664},
issn = {0143-1161},
journal = {INTERNATIONAL JOURNAL OF REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
month = {jan},
number = {1},
pages = {346--363},
title = {{Delineation of individual deciduous trees in plantations with low-density LiDAR data}},
volume = {40},
year = {2019}
}
@article{ISI:000441334300307,
abstract = {This article presents the results of research on a new method of spatial analysis of walls and buildings moisture. Due to the fact that destructive methods are not suitable for historical buildings of great architectural significance, a non-destructive method based on electrical tomography has been adopted. A hybrid tomograph with special sensors was developed for the measurements. This device enables the acquisition of data, which are then reconstructed by appropriately developed methods enabling spatial analysis of wet buildings. Special electrodes that ensure good contact with the surface of porous building materials such as bricks and cement were introduced. During the research, a group of algorithms enabling supervised machine learning was analyzed. They have been used in the process of converting input electrical values into conductance depicted by the output image pixels. The conductance values of individual pixels of the output vector made it possible to obtain images of the interior of building walls as both flat intersections (2D) and spatial (3D) images. The presented group of algorithms has a high application value. The main advantages of the new methods are: high accuracy of imaging, low costs, high processing speed, ease of application to walls of various thickness and irregular surface. By comparing the results of tomographic reconstructions, the most efficient algorithms were identified.},
author = {Rymarczyk, Tomasz and Klosowski, Grzegorz and Kozlowski, Edward},
doi = {10.3390/s18072285},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jul},
number = {7},
title = {{A Non-Destructive System Based on Electrical Tomography and Machine Learning to Analyze the Moisture of Buildings}},
volume = {18},
year = {2018}
}
@article{7879309,
abstract = {This paper presents a 6-degree of freedom (DOF) pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired. The PE method involves two-graph simultaneous localization and mapping (SLAM) processes to reduce the accumulative pose error of the device. In the first step, the floor plane is extracted from the 3-D camera's point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch, and Zerrors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y, and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a visually impaired person in an indoor environment. The system uses the estimated pose and floor plan to locate the device user in a building and guides the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.},
author = {Zhang, H and Ye, C},
doi = {10.1109/TNSRE.2017.2682265},
issn = {1534-4320},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
keywords = {Patient Identification Systems,Three-Dimensional,biomedical engineering,geometry,indoor navigation,revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
number = {9},
pages = {1592--1604},
title = {{An Indoor Wayfinding System Based on Geometric Features Aided Graph SLAM for the Visually Impaired}},
volume = {25},
year = {2017}
}
@article{ISI:000397013700010,
abstract = {This paper investigated the potential of multispectral airborne laser scanning (ALS) data for individual tree detection and tree species classification. The aim was to develop a single-sensor solution for forest mapping that is capable of providing species-specific information, required for forest management and planning purposes. Experiments were conducted using 1903 ground measured trees from 22 sample plots and multispectral ALS data, acquired with an Optech Titan scanner over a boreal forest, mainly consisting of Scots pine (Pinus Sylvestris), Norway spruce (Picea Abies), and birch (Betula sp.), in southern Finland. ALS-features used as predictors for tree species were extracted from segmented tree objects and used in random forest classification. Different combinations of features, including point cloud features, and intensity features of single and multiple channels, were tested. Among the field-measured trees, 61.3{\%} were correctly detected. The best overall accuracy (OA) of tree species classification achieved for correctly-detected trees was 85.9{\%} (Kappa = 0.75), using a point cloud and single-channel intensity features combination, which was not significantly different from the ones that were obtained either using all features (OA = 85.6{\%}, Kappa = 0.75), or single-channel intensity features alone (OA = 85.4{\%}, Kappa = 0.75). Point cloud features alone achieved the lowest accuracy, with an OA of 76.0{\%}. Field-measured trees were also divided into four categories. An examination of the classification accuracy for four categories of trees showed that isolated and dominant trees can be detected with a detection rate of 91.9{\%}, and classified with a high overall accuracy of 90.5{\%}. The corresponding detection rate and accuracy were 81.5{\%} and 89.8{\%} for a group of trees, 26.4{\%} and 79.1{\%} for trees next to a larger tree, and 7.2{\%} and 53.9{\%} for trees situated under a larger tree, respectively. The results suggest that Channel 2 (1064 nm) contains more information for separating pine, spruce, and birch, followed by channel 1 (1550 nm) and channel 3 (532 nm) with an overall accuracy of 81.9{\%}, 78.3{\%}, and 69.1{\%}, respectively. Our results indicate that the use of multispectral ALS data has great potential to lead to a single-sensor solution for forest mapping.},
author = {Yu, Xiaowei and Hyyppa, Juha and Litkey, Paula and Kaartinen, Harri and Vastaranta, Mikko and Holopainen, Markus},
doi = {10.3390/rs9020108},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {2},
title = {{Single-Sensor Solution to Tree Species Classification Using Multispectral Airborne Laser Scanning}},
volume = {9},
year = {2017}
}
@inproceedings{ISI:000352727000035,
abstract = {The fully automated analysis of 3D point clouds is of great importance in photogrammetry, remote sensing and computer vision. For reliably extracting objects such as buildings, road inventory or vegetation, many approaches rely on the results of a point cloud classification, where each 3D point is assigned a respective semantic class label. Such an assignment, in turn, typically involves statistical methods for feature extraction and machine learning. Whereas the different components in the processing workflow have extensively, but separately been investigated in recent years, the respective connection by sharing the results of crucial tasks across all components has not yet been addressed. This connection not only encapsulates the interrelated issues of neighborhood selection and feature extraction, but also the issue of how to involve spatial context in the classification step. In this paper, we present a novel and generic approach for 3D scene analysis which relies on (i) individually optimized 3D neighborhoods for (ii) the extraction of distinctive geometric features and (iii) the contextual classification of point cloud data. For a labeled benchmark dataset, we demonstrate the beneficial impact of involving contextual information in the classification process and that using individual 3D neighborhoods of optimal size significantly increases the quality of the results for both pointwise and contextual classification.},
annote = {Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
Univ Munchen, Munich, GERMANY, MAR 25-27, 2015},
author = {Weinmann, M and Schmidt, A and Mallet, C and Hinz, S and Rottensteiner, F and Jutzi, B},
booktitle = {PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. II},
doi = {10.5194/isprsannals-II-3-W4-271-2015},
editor = {{Stilla, U and Heipke}, C},
issn = {2194-9034},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {W4},
organization = {ISPRS},
pages = {271--278},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{CONTEXTUAL CLASSIFICATION OF POINT CLOUD DATA BY EXPLOITING INDIVIDUAL 3D NEIGBOURHOODS}},
volume = {2-3},
year = {2015}
}
@inproceedings{ISI:000451039807048,
abstract = {The objective of this paper was to develop a new algorithm to segment individual trees directly by using the three-dimensional space characteristic of airborne light detection and ranging point cloud data. The local maximum method was used in the initial segmentation and the error identification tree exclusion. On the basis of the point cloud spatial distribution of individual trees and the adjacent relationship with the other trees, a point cloud clustering method was developed to decide the points belonging to the individual trees. This algorithm was tested by 6 forest plots in the Genhe forestry reserve. The results showed that this algorithm could segment individual trees quickly and accurately, and the overall accuracy of this algorithm was 96.3{\%}.},
annote = {38th IEEE International Geoscience and Remote Sensing Symposium
(IGARSS), Valencia, SPAIN, JUL 22-27, 2018},
author = {Li, Shihua and Su, Lian and Liu, Yuhan and He, Ze},
booktitle = {IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM},
isbn = {978-1-5386-7150-4},
issn = {2153-6996},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers Geoscience {\&} Remote Sensing Soc; European Space Agcy},
pages = {7520--7523},
series = {IEEE International Symposium on Geoscience and Remote Sensing IGARSS},
title = {{SEGMENTATION OF INDIVIDUAL TREES BASED ON A POINT CLOUD CLUSTERING METHOD USING AIRBORNE LIDAR DATA}},
year = {2018}
}
@inproceedings{7428562,
abstract = {3D facial landmarks play a significant role in many 3D facial studies. Due to the complex geometry of high resolution 3D human face, landmark detection remains to be a challenge problem. This paper proposes a novel method to detect landmarks automatically for high resolution 3D faces without learning or training, solely using geometric information. For high resolution 3D faces, geodesic remeshing is used to reduce the vertices number, then the remeshed 3D faces are parameterized and interpolated on a regular grid, which enable us to compute the differential geometric features more efficiently and accurately. To detect landmarks, we extract global and local constraints for each landmark by considering relative positions of landmarks and differential geometric features, then landmarks are detected by combine these constraints. We evaluate our method and compare it with other methods which are mostly related to ours in a large scale publicly available 3D face data set, BJUT-3D face database. The experimental results show that our method is robust and effective, and achieves better performance than existing methods.},
annote = {From Duplicate 1 (Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information - Liu, J; Zhang, Q; Tang, C)

From Duplicate 2 (Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information - Liu, J; Zhang, Q; Tang, C)

cited By 0

From Duplicate 3 (Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information - Liu, J; Zhang, Q; Tang, C)

cited By 0},
author = {Liu, J and Zhang, Q and Tang, C},
booktitle = {2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
doi = {10.1109/IAEAC.2015.7428562},
keywords = {face recognition,feature extraction,image resoluti,lerdepois,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {276--284},
title = {{Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966359479{\&}doi=10.1109{\%}2FIAEAC.2015.7428562{\&}partnerID=40{\&}md5=d07bdc62c3beb1c60304198eb193ba5c},
year = {2016}
}
@article{ISI:000395034500015,
abstract = {The assessment of facial mimicry is important in forensic anthropology; in addition, the application of modern 3D image acquisition systems may help for the analysis of facial surfaces. This study aimed at exposing a novel method for comparing 3D profiles in different facial expressions. Ten male adults, aged between 30 and 40 years, underwent acquisitions by stereophotogrammetry (VECTRA-3D (R)) with different expressions (neutral, happy, sad, angry, surprised). The acquisition of each individual was then superimposed on the neutral one according to nine landmarks, and the root mean square (RMS) value between the two expressions was calculated. The highest difference in comparison with the neutral standard was shown by the happy expression (RMS 4.11 mm), followed by the surprised (RMS 2.74 mm), sad (RMS 1.3 mm), and angry ones (RMS 1.21 mm). This pilot study shows that the 3D-3D superimposition may provide reliable results concerning facial alteration due to mimicry.},
author = {Gibelli, Daniele and {De Angelis}, Danilo and Poppa, Pasquale and Sforza, Chiarella and Cattaneo, Cristina},
doi = {10.1111/1556-4029.13295},
issn = {0022-1198},
journal = {JOURNAL OF FORENSIC SCIENCES},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux7},
month = {mar},
number = {2},
pages = {405--410},
title = {{An Assessment of How Facial Mimicry Can Change Facial Morphology: Implications for Identification}},
volume = {62},
year = {2017}
}
@inproceedings{8373915,
abstract = {In this paper, we propose a novel method for reconstructing 3D faces from 2D images. The method is characterized in three aspects. (i) It utilizes only geometric cues in the input images, i.e., 2D facial landmarks. (ii) It works for an arbitrary number of unconstrained images, both single and multiple images. (iii) It can effectively exploit complementary information in multiple images of varying poses and expressions. The method is implemented based on cascaded regression in shape space. We have evaluated the method on three databases and observed from the experimental results that (i) the reconstruction error is reduced as more images of different poses are used, (ii) the proposed method can obtain comparable reconstruction results by using state-of-the-art automated methods to detect the 2D landmarks, and (iii) the proposed method is robust to variations in facial expressions and image qualities.},
author = {Tian, W and Liu, F and Zhao, Q},
booktitle = {2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
doi = {10.1109/FG.2018.00122},
keywords = {face recognition,image reconstruction,lerdepois,regression a,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {774--779},
title = {{Landmark-Based 3D Face Reconstruction from an Arbitrary Number of Unconstrained Images}},
year = {2018}
}
@inproceedings{ISI:000427083300058,
abstract = {Autism is a developmental disorder involving qualitative impairments in social interaction. One source of those impairments are difficulties with facial expressions of emotion. Autistic people often have difficulty to recognize or to understand other people's emotions and feelings, or expressing their own. This work proposes a method to automatically recognize seven basic emotions among autistic children in real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and Neutral. The method uses the Microsoft Kinect sensor to track and identify points of interest from the 3D face model and it is based on the {\$}P point-cloud recognizer to identify multi-stroke emotions as point-clouds. The experimental results show that our system can achieve above 94.28{\%} recognition rate. Our study provides a novel clinical tool to help children with autism to assisting doctors in operating rooms.},
annote = {Intelligent Systems and Computer Vision (ISCV), Fez, MOROCCO, APR 17-19,
2017},
author = {Jazouli, Maha and Majda, Aicha and Zarghili, Arsalane},
booktitle = {2017 INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV)},
editor = {{Nfaoui, E and Boumhidi, J and Sabri, A and Yahyaouy, A and Bouchaffra}, D},
isbn = {978-1-5090-4062-9},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
organization = {IEEE; Univ Sidi Mohamed ben Abdellah; IEEE Comp Soc; IEEE Morocco Sect},
title = {{A {\$}P Recognizer for Automatic Facial Emotion Recognition using Kinect Sensor}},
year = {2017}
}
@article{ISI:000363075300013,
abstract = {Tree detection and tree species recognition are bottlenecks of the airborne remote sensing-based single tree inventories. The effect of these factors in forest attribute estimation can be reduced if airborne measurements are aided with tree mapping information that is collected from the ground. The main objective here was to demonstrate the use of terrestrial laser scanning-derived (TLS) tree maps in aiding airborne laser scanning-based (ALS) single tree inventory (multisource single tree inventory, MS-STI) and its capability in predicting diameter distribution in various forest conditions. Automatic measurement of TLS point clouds provided the tree maps and the required reference information from the tree attributes. The study area was located in Evo, Finland, and the reference data was acquired from 27 different sample plots with varying forest conditions. The workflow of MS-STI included: (1) creation of automatic tree map from TLS point clouds, (2) automatic diameter at breast height (DBH) measurement from TLS point clouds, (3) individual tree detection (ITD) based on ALS, (4) matching the ITD segments to the field-measured reference, (5) ALS point cloud metric extraction from the single tree segments and (6) DBH estimation based on the derived metrics. MS-STI proved to be accurate and efficient method for DBH estimation and predicting diameter distribution. The overall accuracy (root mean squared error, RMSE) of the DBH was 36.9 mm. Results showed that the DBH accuracy decreased if the tree density (trees/ha) increased. The highest accuracies were found in old-growth forests (tree densities less than 500 stems/ha). MS-STI resulted in the best accuracies regarding Norway spruce (Picea abies (L.) H. Karst.)-dominated forests (RMSE of 29.9 mm). Diameter distributions were predicted with low error indices, thereby resulting in a good fit compared to the reference. Based on the results, diameter distribution estimation with MS-STI is highly dependent on the forest structure and the accuracy of the tree maps that are used. The most important development step in the future for the MS-STI and automatic measurements of the TLS point cloud is to develop tree species recognition methods and further develop tree detection techniques. The possibility of using MLS or harvester data as a basis for the required tree maps should also be assessed in the future. (C) 2015 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.},
author = {Kankare, Ville and Liang, Xinlian and Vastaranta, Mikko and Yu, Xiaowei and Holopainen, Markus and Hyyppa, Juha},
doi = {10.1016/j.isprsjprs.2015.07.007},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {161--171},
title = {{Diameter distribution estimation with laser scanning based multisource single tree inventory}},
volume = {108},
year = {2015}
}
@inproceedings{7378673,
abstract = {Three-dimensional body measurement is determined by measuring the size of each part of the body and body shape, in order to study the morphological characteristics of the human body, and provide basic information for human industrial design, ergonomics, engineering design, anthropological research, and medicine. This paper presents a three-dimensional point cloud data extraction method of facial feature points by doing nose points with examples, using this method can accurately locate the feature points, and there is no specific stations toward requirements in the process of scanning point cloud of the human body, it is convenient and quick. The results show that this method can meet the ergonomics and other features to quickly locate and measure the body size requirements. {\textcopyright} 2015 IEEE.},
annote = {From Duplicate 2 (A method of extracting human facial feature points based on 3D laser scanning point cloud data - and Xia, H; Huang, T; Chen, G)

From Duplicate 1 (A method of extracting human facial feature points based on 3D laser scanning point cloud data - Xia, H; Huang, T; Chen, G)

cited By 0

From Duplicate 3 (A method of extracting human facial feature points based on 3D laser scanning point cloud data - and; Xia, H; Huang, T; Chen, G)

From Duplicate 1 (A method of extracting human facial feature points based on 3D laser scanning point cloud data - Xia, H; Huang, T; Chen, G)

cited By 0

From Duplicate 4 (A method of extracting human facial feature points based on 3D laser scanning point cloud data - Xia, H; Huang, T; Chen, G)

cited By 0},
author = {Xia, H and Huang, T and Chen, G and And and Xia, H and Huang, T and Chen, G and and Xia, H and Huang, T and Chen, G and And},
booktitle = {2015 23rd International Conference on Geoinformatics},
doi = {10.1109/GEOINFORMATICS.2015.7378673},
issn = {2161-024X},
keywords = {computer graphics,face recognition,feature extract,lerdepois,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {1--3},
title = {{A method of extracting human facial feature points based on 3D laser scanning point cloud data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962437313{\&}doi=10.1109{\%}2FGEOINFORMATICS.2015.7378673{\&}partnerID=40{\&}md5=5604340c250b337bda78d65c17e2cb5d},
volume = {2016-Janua},
year = {2016}
}
@inproceedings{ISI:000371977803080,
abstract = {With the growing availability and wide distribution of low-cost, high-performance 3D imaging sensors, the image analysis community has witnessed an increased demand for solutions to the challenges of activity recognition and person identification. We propose an integrated framework, based on graph signal processing, that simultaneously performs both tasks using a single set of features. The novelty of our approach is based on the fact that the set of features used for activity recognition accommodates person identification without additional computation. The analysis is based on the extracted structure-invariant graph (skeleton). The Laplacian of the skeleton is used both to identify the person and recognize the performed activity. While person identification is achieved directly from the analysis of the Laplacian, activity recognition is obtained after transformation, into the graph spectral domain, of the vectorized form of the skeletal joints 3D coordinates. Feature vectors for activity recognition are then derived, in this domain, from the covariance matrices evaluated over fixed-length sequential video segments. Both classification tasks are implemented using linear support vector machines (SVM). When applied to real activity datasets, our approach shows an improved performance over the existing state-of-the-art.},
annote = {IEEE International Conference on Image Processing (ICIP), Quebec City,
CANADA, SEP 27-30, 2015},
author = {Batabyal, Tamal and Vaccari, Andrea and Acton, Scott T},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4799-8339-1},
issn = {1522-4880},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {Inst Elect {\&} Elect Engineers; IEEE Signal Proc Soc},
pages = {3270--3274},
series = {IEEE International Conference on Image Processing ICIP},
title = {{UGraSP: A UNIFIED FRAMEWORK FOR ACTIVITY RECOGNITION AND PERSON IDENTIFICATION USING GRAPH SIGNAL PROCESSING}},
year = {2015}
}
@article{ISI:000373271800001,
abstract = {Today, microbial drinking water quality is monitored through either time-consuming laboratory methods or indirect on-line measurements. Results are thus either delayed or insufficient to support proactive action. A novel, optical, on-line bacteria sensor with a 10-minute time resolution has been developed. The sensor is based on 3D image recognition, and the obtained pictures are analyzed with algorithms considering 59 quantified image parameters. The sensor counts individual suspended particles and classifies them as either bacteria or abiotic particles. The technology is capable of distinguishing and quantifying bacteria and particles in pure and mixed suspensions, and the quantification correlates with total bacterial counts. Several field applications have demonstrated that the technology can monitor changes in the concentration of bacteria, and is thus well suited for rapid detection of critical conditions such as pollution events in drinking water.},
author = {Hojris, Bo and Christensen, Sarah Christine Boesgaard and Albrechtsen, Hans-Jorgen and Smith, Christian and Dahlqvist, Mathis},
doi = {10.1038/srep23935},
issn = {2045-2322},
journal = {SCIENTIFIC REPORTS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {apr},
title = {{A novel, optical, on-line bacteria sensor for monitoring drinking water quality}},
volume = {6},
year = {2016}
}
@article{ISI:000428490900009,
abstract = {3D face recognition is an increasing popular modality for biometric authentication, for example in the iPhoneX. Landmarking plays a significant role in region based face recognition algorithms. The accuracy and consistency of the landmarking will directly determine the effectiveness of feature extraction and hence the overall recognition performance. While surface normals have been shown to provide high performing features for face recognition, their use in landmarking has not been widely explored. To this end, a new 3D facial landmarking algorithm based on thresholded surface normal maps is proposed, which is applicable to widely used 3D face databases. The benefits of employing surface normals are demonstrated for both facial roll and yaw rotation calibration and nasal landmarks localization. Results on the Bosphorus, FRGC and BU-3DFE databases show that the detected landmarks possess high within class consistency and accuracy under different expressions. For several key landmarks the performance achieved surpasses that of state-of-the-art techniques and is also training free and computationally efficient. The use of surface normals therefore provides a useful representation of the 3D surface and the proposed landmarking algorithm provides an effective approach to localising the key nasal landmarks. (C) 2018 Elsevier Ltd. All rights reserved.},
author = {Gao, Jiangning and Evans, Adrian N},
doi = {10.1016/j.patcog.2018.01.011},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
month = {jun},
pages = {120--132},
title = {{Expression robust 3D face landmarking using thresholded surface normals}},
volume = {78},
year = {2018}
}
@inproceedings{7729385,
abstract = {Informal settlement upgrading projects require high-resolution and up-to-date thematic maps in order to plan and design effective interventions. To this end, Unmanned Aerial Vehicles (UAVs) provide the opportunity to obtain very high resolution 2D orthomosaics and 3D point clouds where and when needed. The heterogeneous, dense structures which typically make up an informal settlement motivate the importance of integrating complex 2D and 3D features obtained from UAV data into a single classification problem. Multiple Kernel Learning (MKL) Support Vector Machines (SVMs) maintain the distinct characteristics of the different feature spaces by optimizing individual kernels for specific feature groups which are later combined into a single kernel used for classification. Both the kernel parameters and kernel weights can be optimized by considering the alignment between the kernel and an ideal kernel which would perfectly classify the samples. This paper demonstrates how extracting high-level features from both the 2D orthomosaic as well as the 3D point cloud (obtained by an UAV), and integrating them through a MKL approach, can obtain an Overall Accuracy of 90.29{\%}, a 4{\%} increase over the results obtained using single kernel methods.},
author = {Gevaert, C and Persello, C and Sliuzas, R and Vosselman, G},
booktitle = {2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
doi = {10.1109/IGARSS.2016.7729385},
issn = {2153-7003},
keywords = {autonomous aerial vehicles,geophysical image proce,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {jul},
pages = {1508--1511},
title = {{Integration of 2D and 3D features from UAV imagery for informal settlement classification using Multiple Kernel Learning}},
year = {2016}
}
@article{ISI:000386741300011,
abstract = {In this paper we propose a robust face recognition algorithm for low resolution RGB-D Kinect data. Many techniques are proposed for image preprocessing due to the noisy depth data. First, facial symmetry is exploited based on the 3D point cloud to obtain a canonical frontal view image irrespective of the initial pose and then depth data is converted to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are then used to project RGB to DCS (Discriminant Color Space) and normal maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust Sparse Coding method is proposed that codes the multiple channels (DCS or DNM) of a test image as a sparse combination of training samples with different pixel weighting. Weights are calculated dynamically in an iterative process to achieve robustness against variations in pose, illumination, facial expressions and disguise. In contrast to existing techniques, our multi-channel approach is more robust to variations. Reconstruction errors of the test image (DCS and DNM) are normalized and fused to decide its identity. The proposed algorithm is evaluated on four public databases. It achieves 98.4{\%} identification rate on CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects. Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2 databases, the proposed algorithm achieves 97.6{\%}, 95.6{\%} and 95.2{\%} identification rates respectively. To the best of our knowledge, these are the highest identification rates reported so far for the first three databases. (C) 2016 Elsevier B.V. All rights reserved.},
author = {Li, Billy Y L and Xue, Mingliang and Mian, Ajmal and Liu, Wanquan and Krishna, Aneesh},
doi = {10.1016/j.neucom.2016.06.012},
issn = {0925-2312},
journal = {NEUROCOMPUTING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
month = {nov},
pages = {93--108},
title = {{Robust RGB-D face recognition using Kinect sensor}},
volume = {214},
year = {2016}
}
@inproceedings{ISI:000406534300014,
abstract = {Data from the Optech Titan airborne laser scanner were collected over Monterey, CA, in three wavelengths (532 nm, 1064 nm, and 1550 nm), in May 2016, by the National Center for Airborne LiDAR Mapping (NCALM). Analysis techniques have been developed using spectral technology largely derived from the analysis of spectral imagery. Data are analyzed as individual points, vs techniques that emphasize spatial binning. The primary tool which allows for this exploitation is the N-Dimensional Visualizer contained in the ENVI software package. The results allow for significant improvement in classification accuracy compared to results obtained from techniques derived from standard LiDAR analysis tools.},
annote = {Conference on Laser Radar Technology and Applications XXI, Anaheim, CA,
APR 11-12, 2017},
author = {McIver, Charles A and Metcalf, Jeremy P and Olsen, Richard C},
booktitle = {LASER RADAR TECHNOLOGY AND APPLICATIONS XXII},
doi = {10.1117/12.2276658},
editor = {{Turner, MD and Kamerman}, GW},
isbn = {978-1-5106-0883-2; 978-1-5106-0884-9},
issn = {0277-786X},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Spectral LiDAR Analysis for Terrain Classification}},
volume = {10191},
year = {2017}
}
@article{ISI:000390884300006,
abstract = {The ability to recognize an unfamiliar individual on the basis of prior exposure to a photograph is notoriously poor and prone to errors, but recognition accuracy is improved when multiple photographs are available. In applied situations, when only limited real images are available (e.g., from a mugshot or CCTV image), the generation of new images might provide a technological prosthesis for otherwise fallible human recognition. We report two experiments examining the effects of providing computer-generated additional views of a target face. In Experiment 1, provision of computer-generated views supported better target face recognition than exposure to the target image alone and equivalent performance to that for exposure of multiple photograph views. Experiment 2 replicated the advantage of providing generated views, but also indicated an advantage for multiple viewings of the single target photograph. These results strengthen the claim that identifying a target face can be improved by providing multiple synthesized views based on a single target image. In addition, our results suggest that the degree of advantage provided by synthesized views may be affected by the quality of synthesized material.},
author = {Jones, Scott P and Dwyer, Dominic M and Lewis, Michael B},
doi = {10.1080/17470218.2016.1158302},
issn = {1747-0218},
journal = {QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {5},
pages = {906--918},
title = {{The utility of multiple synthesized views in the recognition of unfamiliar faces}},
volume = {70},
year = {2017}
}
@inproceedings{Baig:2018:MDL:3240508.3241394,
address = {New York, NY, USA},
author = {Baig, Mohammed Habibullah and Varghese, Jibin Rajan and Wang, Zhangyang},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
doi = {10.1145/3240508.3241394},
isbn = {978-1-4503-5665-7},
keywords = {clustering,deep learning,music classification,revisao{\_}V2,revisao{\_}acm,tutux9,visualization},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux9},
pages = {1253--1255},
publisher = {ACM},
series = {MM '18},
title = {{MusicMapp: A Deep Learning Based Solution for Music Exploration and Visual Interaction}},
url = {http://doi.acm.org/10.1145/3240508.3241394},
year = {2018}
}
@inproceedings{ISI:000426886000048,
abstract = {To detect genotypes with high water use efficiency (WUE) in apple (Malus x domestica), 193 genotypes from an INRA core collection were evaluated in 2014. Eight grafted replicates per genotype grown as one-year-old scions were studied in a high-throughput phenotyping platform (PhenoArch). Individual pot weight was recorded twice a day and irrigation was scheduled for 46 days according to two irrigation treatments: well-watered (WW), maintaining soil water content (SWC) at 1.4 g g(-1); and water stress (WS), reducing SWC until 0.7 g g(-1) and maintaining this value for ten days. For each genotype, half of the replicates were WW while the other half were grown under WS. Plant 3D images were automatically acquired every two days. Analysis of images and pot weight differences allowed the estimation of the accumulated whole-plant biomass (A{\_}Bio) and transpiration (Plant{\_}T) during the experiment. WUE was calculated as the ratio A{\_}Bio/Plant{\_}T. A{\_}Bio and WUE had a higher genetic variation than Plant{\_}T under WW and WS conditions. The genetic variation in WUE is a promising result, indicating that available genetic resources such as the INRA core collection could be useful to improve apple plant material for the use of water. WS reduced A{\_}Bio and Plant{\_}T but the reduction was less evident in WUE. Some genotypes had similar WUE values under WW and WS conditions. We identified of a group of 38 genotypes with high WUE under WW and WS. The existence of genotypes with high WUE whatever the water regime in apple may encourage apple breeders to consider the use of these genotypes as potential parents for improving apple plant material for the use of water.},
annote = {8th International Symposium on Irrigation of Horticultural Crops,
Lleida, SPAIN, JUN 08-11, 2015},
author = {Lopez, G and Pallas, B and Martinez, S and Lauri, P E and Regnard, J L and Durel, C E and Costes, E},
booktitle = {VIII INTERNATIONAL SYMPOSIUM ON IRRIGATION OF HORTICULTURAL CROPS},
doi = {10.17660/ActaHortic.2017.1150.48},
editor = {{Marsal, J and Girona}, J},
isbn = {978-94-62611-45-0},
issn = {0567-7572},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {Int Soc Horticultural Sci},
pages = {335--340},
series = {Acta Horticulturae},
title = {{High-throughput phenotyping of an apple core collection: identification of genotypes with high water use efficiency}},
volume = {1150},
year = {2017}
}
@article{ISI:000422943700008,
abstract = {This paper presents new approaches for gait and activity analysis based on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed algorithms are embedded into an integrated 4D vision and visualization system, which is able to analyze and interactively display real scenarios in natural outdoor environments with walking pedestrians. The main focus of the investigations is gait-based person reidentification during tracking and recognition of specific activity patterns, such as bending, waving, making phone calls, and checking the time looking at wristwatches. The descriptors for training and recognition are observed and extracted from realistic outdoor surveillance scenarios, where multiple pedestrians are walking in the field of interest following possibly intersecting trajectories; thus, the observations might often be affected by occlusions or background noise. Since there is no public database available for such scenarios, we created and published a new Lidar-based outdoor gait and activity data set on our website that contains point cloud sequences of 28 different persons extracted and aggregated from 35-min-long measurements. The presented results confirm that both efficient gait-based identification and activity recognition are achievable in the sparse point clouds of a single RMB Lidar sensor. After extracting the people trajectories, we synthesized a free-viewpoint video, in which moving avatar models follow the trajectories of the observed pedestrians in real time, ensuring that the leg movements of the animated avatars are synchronized with the real gait cycles observed in the Lidar stream.},
author = {Benedek, Csaba and Galai, Bence and Nagy, Balazs and Janko, Zsolt},
doi = {10.1109/TCSVT.2016.2595331},
issn = {1051-8215},
journal = {IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
month = {jan},
number = {1},
pages = {101--113},
title = {{Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance System}},
volume = {28},
year = {2018}
}
@article{ISI:000458017400007,
abstract = {Making machines understand human expressions enables various useful applications in human-machine interaction. In this article, we present a novel facial expression recognition approach with 3D Mesh Convolutional Neural Networks (3DMCNN) and a visual analytics-guided 3DMCNN design and optimization scheme. From an RGBD camera, we first reconstruct a 3D face model of a subject with facial expressions and then compute the geometric properties of the surface. Instead of using regular Convolutional Neural Networks (CNNs) to learn intensities of the facial images, we convolve the geometric properties on the surface of the 3D model using 3DMCNN. We design a geodesic distance-based convolution method to overcome the difficulties raised from the irregular sampling of the face surface mesh. We further present interactive visual analytics for the purpose of designing and modifying the networks to analyze the learned features and cluster similar nodes in 3DMCNN. By removing low-activity nodes in the network, the performance of the network is greatly improved. We compare our method with the regular CNN-based method by interactively visualizing each layer of the networks and analyze the effectiveness of our method by studying representative cases. Testing on public datasets, our method achieves a higher recognition accuracy than traditional image-based CNN and other 3D CNNs. The proposed framework, including 3DMCNN and interactive visual analytics of the CNN, can be extended to other applications.},
address = {New York, NY, USA},
author = {Jin, Hai and Lian, Yuanfeng and Hua, Jing},
doi = {10.1145/3200572},
issn = {2157-6904},
journal = {ACM Trans. Intell. Syst. Technol.},
keywords = {3D mesh convolutional neural networks,Facial expression analysis,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutux7,visual analysis},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
month = {jan},
number = {1, SI},
pages = {7:1----7:22},
publisher = {ACM},
title = {{Learning Facial Expressions with 3D Mesh Convolutional Neural Network}},
url = {http://doi.acm.org/10.1145/3200572},
volume = {10},
year = {2018}
}
@article{ISI:000396382500043,
abstract = {Remote sensing data allow large scale observation of forested ecosystems. Forest assessment benefits from information about individual trees. Multibaseline SAR interferometry (InSAR) is able to generate dense point clouds of forest canopies, similar to airborne laser scanning (ALS). This type of point cloud was generated using data from the Ka-band MEMPHIS system, acquired over a mainly coniferous forest near Vordemwald in the Swiss Midlands. This point cloud was segmented using an advanced clustering technique to detect individual trees and derive their positions, heights, and crown diameters. To evaluate the InSAR point cloud properties and limitations, it was compared to products derived from ALS and stereo-photogrammetry. All point clouds showed similar geolocation accuracies with 02-0.3 m relative shifts. Both InSAR and photogrammetry techniques yielded points predominantly located in the upper levels of the forest vegetation, while ALS provided points from the top of the canopy down to the understory and forest floor. The canopy height models agreed very well with each other, with R-2 values between 0.84 and 0.89. The detected trees and their estimated physical and structural parameters were validated by comparing them to reference forestry data. A detection rate of similar to 90{\%} was achieved for larger trees, corresponding to half of the reference trees. The smaller trees were detected with a success rate of similar to 50{\%}. The tree height was slightly underestimated, with a R-2 value of 0.63. The estimated crown diameter agreed on an average sense, however with a relatively low R-2 value of 0.19. Very high success rates ({\textgreater}90{\%}) were obtained when matching the trees detected from the InSAR-data with those detected from the ALS- and photogrammetry-data. There, InSAR tree heights were in the mean 1-1.5 m lower, with high R-2 values ranging between 0.8 and 0.9. Our results demonstrate the use of millimeter wave SAR interferometry data as an alternative to ALS- and photogrammetry-based data for forest monitoring. (C) 2016 Elsevier Inc. All rights reserved.},
author = {Magnard, Christophe and Morsdorf, Felix and Small, David and Stilla, Uwe and Schaepman, Michael E and Meier, Erich},
doi = {10.1016/j.rse.2016.09.018},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
pages = {567--580},
title = {{Single tree identification using airborne multibaseline SAR interferometry data}},
volume = {186},
year = {2016}
}
@inproceedings{7852696,
abstract = {This paper presents a method for correcting the posture of 3D face with unknown position and posture based on the standard face pose. We define a standard posture of face which is not affected by the head gesture. Then the algorithms of principal component analysis and iterative closest point are used to achieve the preliminary adjustment of the target model. For the precise calibration, we use the method of depth values iterative searching to find the point of nose precisely, and propose an algorithm of searching along a straight line to find the ridge line of nose. Ultimately we determine the difference between the posture of the target model and standard posture to obtain accurate calculation of the target model posture.},
author = {Li, F and Lai, C and Jin, S and Peng, Y},
booktitle = {2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
doi = {10.1109/CISP-BMEI.2016.7852696},
keywords = {calibration,computer graphics,face recognition,lerdepois,pri,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {135--139},
title = {{Automatic calibration of 3D human faces}},
year = {2016}
}
@article{ISI:000347672900002,
abstract = {Objective: To evaluate the reliability of three-dimensional (3D) landmark identification in cone-beam computed tomography (CBCT) using two different visualization techniques. Materials and Methods: Twelve CBCT images were randomly selected. Three observers independently repeated three times the identification of 30 landmarks using 3D reconstructions and 28 landmarks using multiplanar views. The values of the coordinates X, Y, and Z of each point were obtained and the intraclass correlation coefficient (ICC) was calculated. Results: The ICC of the 3D visualization was rated {\textgreater}0.90 in 67.76{\%} and 45.56{\%}, and {\textless}= 0.45 in 13.33{\%} and 14.46{\%} of the intraobserver and interobserver assessments, respectively. The ICC of the multiplanar visualization was rated {\textgreater}0.90 in 82.16{\%} and 78.56{\%} and {\textless}= 0.45 in only 16.7{\%} and 8.33{\%} of the intraobserver and interobserver assessments, respectively. An individual landmark classification was done according to ICC values. Conclusions: The frequency of highly reliable values was greater for multiplanar than 3D reconstructions. Overall, lower reliability was found for points on the condyle and higher reliability for those on the midsagittal plane. Depending on the anatomic region, the observer must choose the most reliable type of image visualization.},
author = {da Neiva, Marcelo Baiao and Soares, Alvaro Cavalheiro and Lisboa, Cinthia de Oliveira and Vilella, Oswaldo de Vasconcellos and Motta, Alexandre Trindade},
doi = {10.2319/120413-891.1},
issn = {0003-3219},
journal = {ANGLE ORTHODONTIST},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jan},
number = {1},
pages = {11--17},
title = {{Evaluation of cephalometric landmark identification on CBCT multiplanar and 3D reconstructions}},
volume = {85},
year = {2015}
}
@inproceedings{8661454,
abstract = {In Text-Independent speaker identification, the individual that produced some captured speech signal has to be identified without his collaboration, he might not even know that he is being the subject of an identification process. The system could not ask the individual to utter some specific word or phrase, which is precisely what is done in Text-Dependent speaker recognition. Text-Independent speaker identification is far more complicated since we cannot simply measure the similarity of an utterance of a word or phrase to another utterance made by the same speaker of the same word or phrase in which case we could use the dynamics of the speech signal. In this paper we search in the speech signal looking for voiced speech segments and estimate its first three formants, so we end up with a three-dimensional point cloud for each speaker of the collection of known speakers. To identify a speaker we have to measure the similarity of a point-cloud from an unknown speaker to the point-clouds that belong to known speakers, we do that by searching for local structures in the cloud in a way that is highly scalable and robust. We performed tests with both a collection of our own in Spanish and with the English Language Speech Database for Speaker Recognition (ELSDSR) from the Technical University of Denmark achieving results that improve recent published work with ELSDSR.},
author = {Camarena-Ibarrola, A and Castro-Coria, M and Figueroa, K},
booktitle = {2018 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)},
doi = {10.1109/ROPEC.2018.8661454},
issn = {2573-0770},
keywords = {cloud point,revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,speaker recognition,speech processing,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
month = {nov},
pages = {1--6},
title = {{Cloud Point Matching for Text-Independent Speaker Identification}},
year = {2018}
}
@article{ISI:000435193700027,
abstract = {Many biophysical forest properties such as wood volume and leaf area index (LAI) require prior knowledge on either photosynthetic or non-photosynthetic components. Laser scanning appears to be a helpful technique in nondestructively quantifying forest structures, as it can acquire an accurate three-dimensional point cloud of objects. In this study, we propose an unsupervised geometry-based method named Dynamic Segment Merging (DSM) to identify non-photosynthetic components of trees by semantically segmenting tree point clouds, and examining the linear shape prior of each resulting segment. We tested our method using one single tree dataset and four plot-level datasets, and compared our results to a supervised machine learning method. We further demonstrated that by using an optimal neighborhood selection method that involves multi-scale analysis, the results were improved. Our results showed that the overall accuracy ranged from 81.8{\%} to 92.0{\%} with an average value of 87.7{\%}. The supervised machine learning method had an average overall accuracy of 86.4{\%} for all datasets, on account of a collection of manually delineated representative training data. Our study indicates that separating tree photosynthetic and non-photosynthetic components from laser scanning data can be achieved in a fully unsupervised manner without the need of training data and user intervention.},
author = {Wang, Di and Brunner, Jasmin and Ma, Zhenyu and Lu, Hao and Hollaus, Markus and Pang, Yong and Pfeifer, Norbert},
doi = {10.3390/f9050252},
issn = {1999-4907},
journal = {FORESTS},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {5},
title = {{Separating Tree Photosynthetic and Non-Photosynthetic Components from Point Cloud Data Using Dynamic Segment Merging}},
volume = {9},
year = {2018}
}
@article{ISI:000399162400002,
abstract = {Purpose of Review The rapid development of remote sensing technology hasmade dense 3D data available from airborne laser scanning and recently also photogrammetric point clouds. This paper reviews methods for extraction of individual trees from 3D data and their applications in forestry and ecology. Recent Findings Methods for analysis of 3D data at tree level have been developed since the turn of the century. The first algorithms were based on 2D surface models of the upper contours of tree crowns. These methods are robust and provide information about the trees in the top-most canopy. There are also methods that use the complete 3D data. However, development of these 3D methods is still needed to include use of geometric properties. To detect a large fraction of the tallest trees, a surface model method generally gives the best results, but detection of smaller trees below the top-most canopy requires methods utilizing the whole point cloud. Several new sensors are now available with capability to describe the upper part of the canopy, which can be used to frequently update vegetation maps. Highly sensitive laser photo detectors have become available for civilian applications, which will enable acquisition of high-resolution 3D laser data for large areas to much lower costs. Summary Methods for ITC delineation from 3D data provide information about a large fraction of the trees, but there is still a challenge to make optimal use of the information from whole point cloud. Newly developed sensors might make ITC methods cheaper and feasible for large areas.},
author = {Lindberg, Eva and Holmgren, Johan},
doi = {10.1007/s40725-017-0051-6},
issn = {2198-6436},
journal = {CURRENT FORESTRY REPORTS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
number = {1},
pages = {19--31},
title = {{Individual Tree Crown Methods for 3D Data from Remote Sensing}},
volume = {3},
year = {2017}
}
@article{ISI:000433909100002,
abstract = {The paper presents a dictionary integration algorithm using 3D morphable face models (3DMM) for pose-invariant collaborative-representation-based face classification. To this end, we first fit a 3DMM to the 2D face images of a dictionary to reconstruct the 3D shape and texture of each image. The 3D faces are used to render a number of virtual 2D face images with arbitrary pose variations to augment the training data, by merging the original and rendered virtual samples to create an extended dictionary. Second, to reduce the information redundancy of the extended dictionary and improve the sparsity of reconstruction coefficient vectors using collaborative-representation-based classification (CRC), we exploit an on-line class elimination scheme to optimise the extended dictionary by identifying the training samples of the most representative classes for a given query. The final goal is to perform pose-invariant face classification using the proposed dictionary integration method and the on-line pruning strategy under the CRC framework. Experimental results obtained for a set of well-known face data sets demonstrate the merits of the proposed method, especially its robustness to pose variations.},
author = {Song, Xiaoning and Feng, Zhen-Hua and Hu, Guosheng and Kittler, Josef and Wu, Xiao-Jun},
doi = {10.1109/TIFS.2018.2833052},
issn = {1556-6013},
journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {nov},
number = {11},
pages = {2734--2745},
title = {{Dictionary Integration Using 3D Morphable Face Models for Pose-Invariant Collaborative-Representation-Based Classification}},
volume = {13},
year = {2018}
}
@article{ISI:000401097300027,
abstract = {The identification of bodies through the examination of skeletal remains holds a prominent place in the field of forensic investigations. Technological advancements in 3D facial acquisition techniques have led to the proposal of a new body identification technique that involves a combination of craniofacial superimposition and photogrammetry. The aim of this study was to test the method by superimposing various computerized 3D images of skulls onto various photographs of missing people taken while they were still alive in cases when there was a suspicion that the skulls in question belonged to them. The technique is divided into four phases: preparatory phase, 3d acquisition phase, superimposition phase, and metric image analysis 3d. The actual superimposition of the images was carried out in the fourth step. and was done so by comparing the skull images with the selected photos. Using a specific software, the two images (i.e. the 3D avatar and the photo of the missing person) were superimposed. Cross-comparisons of 5 skulls discovered in a mass grave, and of 2 skulls retrieved in the crawlspace of a house were performed. The morphologyc phase reveals a full overlap between skulls and photos of disappeared persons. Metric phase reveals that correlation coefficients of this values, higher than 0.998-0,997 allow to confirm identification hypothesis. (C) 2017 Elsevier B.V. All rights reserved.},
author = {Santoro, Valeria and Lubelli, Sergio and {De Donno}, Antonio and Inchingolo, Alessio and Lavecchia, Fulvio and Introna, Francesco},
doi = {10.1016/j.forsciint.2017.02.006},
issn = {0379-0738},
journal = {FORENSIC SCIENCE INTERNATIONAL},
keywords = {lerdepois,revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {apr},
pages = {168--174},
title = {{Photogrammetric 3D skull/photo superimposition: A pilot study}},
volume = {273},
year = {2017}
}
@inproceedings{8571963,
abstract = {Cleft lip is a kind of congenital facial morphological abnormality. In the clinical field of cleft lip, it is necessary to analyze symmetric shape. However, there is no method to analyze the cleft lip technique based on symmetrical viewpoints. On the other hand, in our previous method to find a symmetric axis using a 2D image, since the middle line is extracted only from the front view of the face moire image. There was a problem that low accuracy was obtained by slight rotation of the face and it was not possible to consider 3D information. In this paper, we propose a method to extract the median plane of the face by analyzing based on bilateral symmetry by using 3D point cloud on the face of front. By extracting the median plane, we believe that not only surgical assistance of doctor be possible but also become a clue to development of simulation software which is the end goal.},
author = {Yamada, S and Lu, H and Tan, J K and Kim, H and Kimura, N and Okawachi, T and Nozoe, E and Nakamura, N},
booktitle = {2018 18th International Conference on Control, Automation and Systems (ICCAS)},
keywords = {biomechanics,face recognition,feature extraction,i,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {1347--1350},
title = {{Extraction of Median Plane from Facial 3D Point Cloud Based on Symmetry Analysis Using ICP Algorithm}},
year = {2018}
}
@inproceedings{Kopinski:2016:DLA:2994374.2994392,
address = {New York, NY, USA},
author = {Kopinski, Thomas and Sachara, Fabian and Handmann, Uwe},
booktitle = {Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
doi = {10.1145/2994374.2994392},
isbn = {978-1-4503-4750-1},
keywords = {Deep Learning,Object Recognition,mid-air gestures,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
pages = {1--9},
publisher = {ACM},
series = {MOBIQUITOUS 2016},
title = {{A Deep Learning Approach to Mid-air Gesture Interaction for Mobile Devices from Time-of-Flight Data}},
url = {http://doi.acm.org/10.1145/2994374.2994392},
year = {2016}
}
@article{ISI:000351880000193,
abstract = {Classification of neural signals at the single-trial level and the study of their relevance in affective and cognitive neuroscience are still in their infancy. Here we investigated the neurophysiological correlates of conditions of increasing social scene complexity using 3D human models as targets of attention, which may also be important in autism research. Challenging single-trial statistical classification of EEG neural signals was attempted for detection of oddball stimuli with increasing social scene complexity. Stimuli had an oddball structure and were as follows: 1) flashed schematic eyes, 2) simple 3D faces flashed between averted and non-averted gaze (only eye position changing), 3) simple 3D faces flashed between averted and non-averted gaze (head and eye position changing), 4) animated avatar alternated its gaze direction to the left and to the right (head and eye position), 5) environment with 4 animated avatars all of which change gaze and one of which is the target of attention. We found a late ({\textgreater} 300 ms) neurophysiological oddball correlate for all conditions irrespective of their complexity as assessed by repeated measures ANOVA. We attempted single-trial detection of this signal with automatic classifiers and obtained a significant balanced accuracy classification of around 79{\%}, which is noteworthy given the amount of scene complexity. Lateralization analysis showed a specific right lateralization only for more complex realistic social scenes. In sum, complex ecological animations with social content elicit neurophysiological events which can be characterized even at the single-trial level. These signals are right lateralized. These finding paves the way for neuroscientific studies in affective neuroscience based on complex social scenes, and given the detectability at the single trial level this suggests the feasibility of brain computer interfaces that can be applied to social cognition disorders such as autism.},
author = {Amaral, Carlos P and Simoes, Marco A and Castelo-Branco, Miguel S},
doi = {10.1371/journal.pone.0121970},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
number = {3},
title = {{Neural Signals Evoked by Stimuli of Increasing Social Scene Complexity Are Detectable at the Single-Trial Level and Right Lateralized}},
volume = {10},
year = {2015}
}
@inproceedings{ISI:000457843605028,
abstract = {The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.},
annote = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Salt Lake City, UT, JUN 18-23, 2018},
author = {Cheng, Shiyang and Kotsia, Irene and Pantic, Maja and Zafeiriou, Stefanos},
booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2018.00537},
isbn = {978-1-5386-6420-9},
issn = {1063-6919},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux7},
organization = {IEEE; CVF; IEEE Comp Soc},
pages = {5117--5126},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications}},
year = {2018}
}
@inproceedings{ISI:000370974903006,
abstract = {Understanding social context is an important skill for robots that share a space with humans. In this paper, we address the problem of recognizing gender, a key piece of information when interacting with people and understanding human social relations and rules. Unlike previous work which typically considered faces or frontal body views in image data, we address the problem of recognizing gender in RGB-D data from side and back views as well. We present a large, gender-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the Kinect v1 and Kinect v2 sensor. We then learn and compare several classifiers on the Kinect v2 data using a HOG baseline, two state-of-the-art deep-learning methods, and a recent tessellation-based learning approach. Originally developed for person detection in 3D data, the latter is able to learn the best selection, location and scale of a set of simple point cloud features. We show that for gender recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 Hz.},
annote = {IEEE International Conference on Robotics and Automation (ICRA),
Seattle, WA, MAY 26-30, 2015},
author = {Linder, Timm and Wehner, Sven and Arras, Kai O},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
isbn = {978-1-4799-6923-4},
issn = {1050-4729},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IEEE},
pages = {3039--3045},
series = {IEEE International Conference on Robotics and Automation ICRA},
title = {{Real-Time Full-Body Human Gender Recognition in (RGB)-D Data}},
year = {2015}
}
@inproceedings{7550083,
abstract = {The use of non-negative matrix factorisation (NMF) on 2D face images has been shown to result in sparse feature vectors that encode for local patches on the face, and thus provides a statistically justified approach to learning parts from wholes. However successful on 2D images, the method has so far not been extended to 3D images. The main reason for this is that 3D space is a continuum and so it is not apparent how to represent 3D coordinates in a non-negative fashion. This work compares different non-negative representations for spatial coordinates, and demonstrates that not all non-negative representations are suitable. We analyse the representational properties that make NMF a successful method to learn sparse 3D facial features. Using our proposed representation, the factorisation results in sparse and interpretable facial features.},
author = {Koppen, W P and Christmas, W J and Crouch, D J M and Bodmer, W F and Kittler, J V},
booktitle = {2016 International Conference on Biometrics (ICB)},
doi = {10.1109/ICB.2016.7550083},
keywords = {face recognition,feature extraction,image registra,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
pages = {1--8},
title = {{Extending non-negative matrix factorisation to 3D registered data}},
year = {2016}
}
@inproceedings{ISI:000352725200038,
abstract = {Numerous applications related to urban scene analysis demand automatic recognition of buildings and distinct sub-elements. For example, if LiDAR data is available, only 3D information could be leveraged for the segmentation. However, this poses several risks, for instance, the in-plane objects cannot be distinguished from their surroundings. On the other hand, if only image based segmentation is performed, the geometric features (e.g., normal orientation, planarity) are not readily available. This renders the task of detecting the distinct sub-elements of the building with similar radiometric characteristic infeasible. In this paper the individual sub-elements of buildings are recognized through sub-segmentation of the building using geometric and radiometric characteristics jointly. 3D points generated from Unmanned Aerial Vehicle (UAV) images are used for inferring the geometric characteristics of roofs and facades of the building. However, the image-based 3D points are noisy, error prone and often contain gaps. Hence the segmentation in 3D space is not appropriate. Therefore, we propose to perform segmentation in image space using geometric features from the 3D point cloud along with the radiometric features. The initial detection of buildings in 3D point cloud is followed by the segmentation in image space using the region growing approach by utilizing various radiometric and 3D point cloud features. The developed method was tested using two data sets obtained with UAV images with a ground resolution of around 1-2 cm. The developed method accurately segmented most of the building elements when compared to the plane-based segmentation using 3D point cloud alone.},
annote = {Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
Univ Munchen, Munich, GERMANY, MAR 25-27, 2015},
author = {Vetrivel, A and Gerke, M and Kerle, N and Vosselman, G},
booktitle = {PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I},
doi = {10.5194/isprsarchives-XL-3-W2-261-2015},
editor = {{Stilla, U and Heipke}, C},
issn = {2194-9034},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {W2},
organization = {ISPRS},
pages = {261--268},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{Segmentation of UAV-based images incorporating 3D point cloud information}},
volume = {40-3},
year = {2015}
}
@article{ISI:000458711300008,
abstract = {This paper tackles the problem of people re-identification by using soft biometrics features. The method works on RGB-D data (color point clouds) to determine the best matching among a database of possible users. For each subject under testing, skeletal information in three-dimensions is used to regularize the pose and to create a skeleton standard posture (SSP). A partition grid, whose sizes depend on the SSP, groups the samples of the point cloud accordingly to their position. Every group is then studied to build the person signature. The same grid is then used for the other subjects of the database to preserve information about possible shape differences among users. The effectiveness of this novel method has been tested on three public datasets. Numerical experiments demonstrate an improvement of results with reference to the current state-of-the-art, with recognition rates of 97.84{\%} (on a partition of BIWI RGBD-ID), 61.97{\%} (KinectREID) and 89.71{\%} (RGBD-ID), respectively. (C) 2019 Elsevier Ltd. All rights reserved.},
author = {Patruno, Cosimo and Marani, Roberto and Cicirelli, Grazia and Stella, Ettore and D'Orazio, Tiziana},
doi = {10.1016/j.patcog.2019.01.003},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
pages = {77--90},
title = {{People re-identification using skeleton standard posture and color descriptors from RGB-D data}},
volume = {89},
year = {2019}
}
@article{ISI:000448059300007,
abstract = {This article presents the results of analyzing the behavior of the Cone Curvature shape descriptor (CC) in the task of recognition of facial expressions in 3D images. The CC descriptor is a representation of the 3D model computed from a set of waves modeling for each vertex of a polygon mesh. The 3D Facial Expression Database (BU-3DFE) was used, which contains images with six facial expressions. With the use of the CC descriptor, the expressions were recognized in an average percentage of 76.67{\%} with a neural network, and of 78.88{\%} with a Bayesian classifier. By combining the CC descriptor with other descriptors such as DESIRE and Spherical Spin Image, it was achieved an average percentage of gesture recognition of 90.27{\%} and 97.2{\%}, using the mentioned classifiers.},
author = {Rodriguez, Julian S and Prieto, Flavio},
doi = {10.14483/udistrital.jour.reving.2015.2.a06},
issn = {0121-750X},
journal = {INGENIERIA},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {2},
pages = {261--275},
title = {{Analysis and comparison of the cone curvature descriptor in facial gesture recognition tasks}},
volume = {20},
year = {2015}
}
@article{ISI:000412378800003,
abstract = {Automatic human Facial Expressions Recognition (FER) is becoming of increased interest. FER finds its applications in many emerging areas such as affective computing and intelligent human computer interaction. Most of the existing work on FER has been done using 2D data which suffers from inherent problems of illumination changes and pose variations. With the development of 3D image capturing technologies, the acquisition of 3D data is becoming a more feasible task. The 3D data brings a more effective solution in addressing the issues raised by its 2D counterpart. State-of-the-art 3D FER methods are often based on a single descriptor which may fail to handle the large inter-class and intra-class variability of the human facial expressions. In this work, we explore, for the first time, the usage of covariance matrices of descriptors, instead of the descriptors themselves, in 3D FER. Since covariance matrices are elements of the non-linear manifold of Symmetric Positive Definite (SPD) matrices, we particularly look at the application of manifold-based classification to the problem of 3D FER. We evaluate the performance of the proposed framework on the BU-3DFE and the Bosphorus datasets, and demonstrate its superiority compared to the state-of-the-art methods. (C) 2017 Elsevier Ltd. All rights reserved.},
author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Benouareth, Abdallah and Declercq, David},
doi = {10.1016/j.engappai.2017.05.009},
issn = {0952-1976},
journal = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
pages = {25--32},
title = {{3D facial expression recognition using kernel methods on Riemannian manifold}},
volume = {64},
year = {2017}
}
@inproceedings{Asteriadis:2015:SHA:2769493.2769569,
abstract = {Automatic human action recognition is a research topic that has attracted significant attention lately, mainly due to the advancements in sensing technologies and the improvements in computational systems' power. However, complexity in human movements, input devices' noise and person-specific pattern variability impose a series of challenges that still remain to be overcome. In the proposed work, a novel human action recognition method using Microsoft Kinect depth sensing technology is presented for handling the above mentioned issues. Each action is represented as a basis vector and spectral analysis is performed on an affinity matrix of new action feature vectors. Using simple kernel regressors for computing the affinity matrix, complexity is reduced and robust low-dimensional representations are achieved. The proposed scheme loosens action detection accuracy demands, while it can be extended for accommodating multiple modalities, in a dynamic fashion.},
address = {New York, New York, USA},
author = {Asteriadis, Stylianos and Daras, Petros},
booktitle = {Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments - PETRA '15},
doi = {10.1145/2769493.2769569},
isbn = {9781450334525},
keywords = {action recognition,gesture recognition,kinect data analysis,revisao{\_}V2,revisao{\_}acm,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}acm,tutux9},
pages = {1--4},
publisher = {ACM Press},
series = {PETRA '15},
title = {{Skeleton-based human action recognition using basis vectors}},
url = {http://doi.acm.org/10.1145/2769493.2769569 http://dl.acm.org/citation.cfm?doid=2769493.2769569},
year = {2016}
}
@inproceedings{7410401,
abstract = {Detection of elongated structures in 2D images and 3D image stacks is a critical prerequisite in many applications and Machine Learning-based approaches have recently been shown to deliver superior performance. However, these methods essentially classify individual locations and do not explicitly model the strong relationship that exists between neighboring ones. As a result, isolated erroneous responses, discontinuities, and topological errors are present in the resulting score maps. We solve this problem by projecting patches of the score map to their nearest neighbors in a set of ground truth training patches. Our algorithm induces global spatial consistency on the classifier score map and returns results that are provably geometrically consistent. We apply our algorithm to challenging datasets in four different domains and show that it compares favorably to state-of-the-art methods.},
author = {Sironi, A and Lepetit, V and Fua, P},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.44},
issn = {2380-7504},
keywords = {feature extraction,image classification,learning (,revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
month = {dec},
pages = {316--324},
title = {{Projection onto the Manifold of Elongated Structures for Accurate Extraction}},
year = {2015}
}
@article{ISI:000445398200037,
abstract = {Purpose: The use of nonionizing 3-dimensional (3D) imaging in cleft lip and palate (CLP) research is well-established; however, general guidelines concerning the assessment of these images are lacking. The aim of the present study was to review the methods for quantification of soft tissue changes on 3D surface images acquired before and after an orthopedic or surgical intervention in CLP patients. Materials and Methods: A systematic literature search was performed using the databases MEDLINE (through PubMed), CENTRAL, Web of Science, and EMBASE. The literature search and eligibility assessment were performed by 2 independent reviewers in a nonblinded standardized manner. Only longitudinal studies reporting the assessment of pre- and postoperative 3D surface images and at least 10 CLP patients were considered eligible. Results: Fifteen unique studies (reported from 1996 to 2017) were identified after an eligibility assessment. The assessment of the 3D images was performed with landmark-dependent analyses, mostly supported by superimposition of the pre- and postoperative images. A wide spectrum of superimposition techniques has been reported. The reliability of these assessment methods was often not reported or was insufficiently reported. Conclusions: Soft tissue changes subsequent to a surgical or an orthopedic intervention can be quantified on 3D surface images using assessment methods that are primarily based on landmark identification, whether or not followed by superimposition. Operator bias is inherently enclosed in landmark-dependent analyses. The reliability of these methods has been insufficiently reported. (C) 2018 American Association of Oral and Maxillofacial Surgeons},
author = {Thierens, Laurent A M and {De Roo}, Noemi M C and {De Pauw}, Guy A M and Brusselaers, Nele},
doi = {10.1016/j.joms.2018.05.020},
issn = {0278-2391},
journal = {JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {oct},
number = {10},
title = {{Quantifying Soft Tissue Changes in Cleft Lip and Palate Using Nonionizing Three-Dimensional Imaging: A Systematic Review}},
volume = {76},
year = {2018}
}
@article{ISI:000397373000001,
abstract = {Robust and effective capture and reconstruction of 3D face models directly by smartphone users enables many applications. This paper presents a novel 3D face modeling and reconstruction solution that robustly and accurately acquire 3D face models from a couple of images captured by a single smartphone camera. Two selfie photos of a subject taken from the front and side are first used to guide our Non-Negative Matrix Factorization (NMF) induced part-based face model to iteratively reconstruct an initial 3D face of the subject. Then, an iterative detail updating method is applied to the initial generated 3D face to reconstruct facial details through optimizing lighting parameters and local depths. Our iterative 3D face reconstruction method permits fully automatic registration of a part based face representation to the acquired face data and the detailed 2D/3D features to build a high-quality 3D face model. The NMF part-based face representation learned from a 3D face database facilitates effective global and adaptive local detail data fitting alternatively. Our system is flexible and it allows users to conduct the capture in any uncontrolled environment. We demonstrate the capability of our method by allowing users to capture and reconstruct their 3D faces by themselves. (C) 2016 Elsevier B.V. All rights reserved.},
author = {Jin, Hai and Wang, Xun and Zhong, Zichun and Hua, Jing},
doi = {10.1016/j.cagd.2016.11.001},
issn = {0167-8396},
journal = {COMPUTER AIDED GEOMETRIC DESIGN},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
month = {jan},
pages = {1--13},
title = {{Robust 3D face modeling and reconstruction from frontal and side images}},
volume = {50},
year = {2017}
}
@article{ISI:000397032500012,
abstract = {Objective: The aim of this retrospective study was to evaluate the practice and the feasibility of Osirix, a free and open-source medical imaging software, in performing accurate video-assisted thoracoscopic lobectomy and segmentectomy. Methods: From July 2014 to April 2016, 63 patients received anatomical video-assisted thoracoscopic surgery (VATS), either lobectomy or segmentectomy, in our department. Three-dimensional (3D) reconstruction images of 61 (96.8{\%}) patients were preoperatively obtained with contrast-enhanced computed tomography (CT). Preoperative resection simulations were accomplished with patient-individual reconstructed 3D images. For lobectomy, pulmonary lobar veins, arteries and bronchi were identified meticulously by carefully reviewing the 3D images on the display. For segmentectomy, the intrasegmental veins in the affected segment for division and the intersegmental veins to be preserved were identified on the 3D images. Patient preoperative characteristics, surgical outcomes and postoperative data were reviewed from a prospective database. Results: The study cohort of 63 patients included 33 (52.4{\%}) men and 30 (47.6{\%}) women, of whom 46 (73.0{\%}) underwent VATS lobectomy and 17 (27.0{\%}) underwent VATS segmentectomy. There was 1 conversion from VATS lobectomy to open thoracotomy because of fibrocalcified lymph nodes. A VATS lobectomy was performed in 1 case after completing the segmentectomy because invasive adenocarcinoma was detected by intraoperative frozen-section analysis. There were no 30-day or 90-day operative mortalities Conclusions: The free, simple, and user-friendly software program Osirix can provide a 3D anatomic structure of pulmonary vessels and a clear vision into the space between the lesion and adjacent tissues, which allows surgeons to make preoperative simulations and improve the accuracy and safety of actual surgery. (C) 2017 IJS Publishing Group Ltd. Published by Elsevier Ltd. All rights reserved.},
author = {Yao, Fei and Wang, Jian and Yao, Ju and Hang, Fangrong and Lei, Xu and Cao, Yongke},
doi = {10.1016/j.ijsu.2017.01.079},
issn = {1743-9191},
journal = {INTERNATIONAL JOURNAL OF SURGERY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
pages = {16--22},
title = {{Three-dimensional image reconstruction with free open-source OsiriX software in video-assisted thoracoscopic lobectomy and segmentectomy}},
volume = {39},
year = {2017}
}
@inproceedings{8035340,
abstract = {In order to better learn the distributions of 2D and 3D faces and the mapping between them with limited training samples, a new 3D face reconstruction method based on progressive cascade regression is proposed. Firstly, it learns the mapping between 2D and 3D facial landmarks to estimate the initial 3D facial landmarks with a coupled space learning method. Secondly, a deformed space is constructed with the difference between the estimated initial landmarks and the ground truth of training samples; and more accurate 3D facial landmarks are reconstructed by modifying the initial 3D ones with shape compensations which are calculated by minimizing an objective function. Finally, the realistic 3D faces are reconstructed by a method that is based on a simple sparse regulation and shape deformation. The results on BJUT 3D face database demonstrate the effectiveness of the proposed method. In addition, compared with some typical methods, the method can get better subjective and objective results, especially in details.},
author = {Han, L and Xiao, Q and Liang, X},
booktitle = {2017 International Conference on Computer, Information and Telecommunication Systems (CITS)},
doi = {10.1109/CITS.2017.8035340},
keywords = {face recognition,image reconstruction,learning (ar,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {297--301},
title = {{3D face reconstruction based on progressive cascade regression}},
year = {2017}
}
@inproceedings{7785124,
abstract = {We present a system which is able to reconstruct human faces on mobile devices with only on-device processing using the sensors which are typically built into a current commodity smart phone. Such technology can for example be used for facial authentication purposes or as a fast preview for further post-processing. Our method uses recently proposed techniques which compute depth maps by passive multi-view stereo directly on the device. We propose an efficient method which recovers the geometry of the face from the typically noisy point cloud. First, we show that we can safely restrict the reconstruction to a 2.5D height map representation. Therefore we then propose a novel low dimensional height map shape model for faces which can be fitted to the input data efficiently even on a mobile phone. In order to be able to represent instance specific shape details, such as moles, we augment the reconstruction from the shape model with a distance map which can be regularized efficiently. We thoroughly evaluate our approach on synthetic and real data, thereby we use both high resolution depth data acquired using high quality multi-view stereo and depth data directly computed on mobile phones.},
author = {Maninchedda, F and H{\"{a}}ne, C and Oswald, M R and Pollefeys, M},
booktitle = {2016 Fourth International Conference on 3D Vision (3DV)},
doi = {10.1109/3DV.2016.59},
keywords = {face recognition,image reconstruction,image resolu,lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {489--498},
title = {{Face Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization}},
year = {2016}
}
@article{ISI:000418758100001,
abstract = {This paper presents an approach to process raw unmanned aircraft vehicle (UAV) image-derived point clouds for automatically detecting, segmenting and regularizing buildings of complex urban landscapes. For regularizing, we mean the extraction of the building footprints with precise position and details. In the first step, vegetation points were extracted using a support vector machine (SVM) classifier based on vegetation indexes calculated from color information, then the traditional hierarchical stripping classification method was applied to classify and segment individual buildings. In the second step, we first determined the building boundary points with a modified convex hull algorithm. Then, we further segmented these points such that each point was assigned to a fitting line using a line growing algorithm. Then, two mutually perpendicular directions of each individual building were determined through a W-k-means clustering algorithm which used the slop information and principal direction constraints. Eventually, the building edges were regularized to form the final building footprints. Qualitative and quantitative measures were used to evaluate the performance of the proposed approach by comparing the digitized results from ortho images.},
author = {Dai, Yucheng and Gong, Jianhua and Li, Yi and Feng, Quanlong},
doi = {10.1080/17538947.2016.1269841},
issn = {1753-8947},
journal = {INTERNATIONAL JOURNAL OF DIGITAL EARTH},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {11},
pages = {1077--1097},
title = {{Building segmentation and outline extraction from UAV image-derived point clouds by a line growing algorithm}},
volume = {10},
year = {2017}
}
@article{ISI:000435372100001,
abstract = {Background: Scoliosis is a complex three-dimensional deformity. While the frontal profile is well understood, increasing attention has turned to balance in the sagittal plane. The present study evaluated changes in sagittal spino-pelvic parameters in a large Hungarian population with adolescent idiopathic scoliosis. Methods: EOS 2D/3D images of 458 scoliotic and 69 control cases were analyzed. After performing 3D reconstructions, the sagittal parameters were assessed as a whole and by curve type using independent sample t test and linear regression analysis. Results: Patients with scoliosis had significantly decreased thoracic kyphosis (p {\textless} 0.001) with values T1-T12, 34.1 +/- 17.1 degrees vs. 43.4 +/- 12.7 degrees in control; T4-T12, 27.1 +/- 18.8 degrees vs. 37.7 +/- 15.1 degrees in control; and T5-T12, 24.9 +/- 15.8 degrees vs. 32.9 +/- 15. 0 degrees in control. Changes in thoracic kyphosis correlated with magnitude of the Cobb angle (p {\textless} 0.001). No significant change was found in lumbar lordosis and the pelvic parameters. After substratification according to the Lenke classification and individually evaluating subgroups, results were similar with a significant decrease in only the thoracic kyphosis. A strong correlation was seen between sacral slope, pelvic incidence, and lumbar lordosis, and between pelvic version and thoracic kyphosis in control and scoliotic groups, whereas pelvic incidence was also seen to be correlated with thoracic kyphosis in scoliosis patients. Conclusion: Adolescent idiopathic scoliosis patients showed a significant decrease in thoracic kyphosis, and the magnitude of the decrease was directly related to the Cobb angle. Changes in pelvic incidence were minimal but were also significantly correlated with thoracic changes. Changes were similar though not identical to those seen in other Caucasian studies and differed from those in other ethnicities. Scoliotic curves and their effect on pelvic balance must still be regarded as individual to each patient, necessitating individual assessment, although changes perhaps can be predicted by patient ethnicity.},
author = {Burkus, Mate and Schlegl, Adam Tibor and O'Sullivan, Ian and Markus, Istvan and Vermes, Csaba and Tunyogi-Csapo, Miklos},
doi = {10.1186/s13013-018-0156-0},
issn = {2397-1789},
journal = {SCOLIOSIS AND SPINAL DISORDERS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jun},
title = {{Sagittal plane assessment of spino-pelvic complex in a Central European population with adolescent idiopathic scoliosis: a case control study}},
volume = {13},
year = {2018}
}
@article{ISI:000449993800083,
abstract = {To meet a growing demand for accurate high-fidelity vegetation cover mapping in urban areas toward biodiversity conservation and assessing the impact of climate change, this paper proposes a complete approach to species and vitality classification at single tree level by synergistic use of multimodality 3D remote sensing data. So far, airborne laser scanning system (ALS or airborne LiDAR) has shown promising results in tree cover mapping for urban areas. This paper analyzes the potential of mobile laser scanning system/mobile mapping system (MLS/MMS)-based methods for recognition of urban plant species and characterization of growth conditions using ultra-dense LiDAR point clouds and provides an objective comparison with the ALS-based methods. Firstly, to solve the extremely intensive computational burden caused by the classification of ultra-dense MLS data, a new method for the semantic labeling of LiDAR data in the urban road environment is developed based on combining a conditional random field (CRF) for the context-based classification of 3D point clouds with shape priors. These priors encode geometric primitives found in the scene through sample consensus segmentation. Then, single trees are segmented from the labelled tree points using the 3D graph cuts algorithm. Multinomial logistic regression classifiers are used to determine the fine deciduous urban tree species of conversation concern and their growth vitality. Finally, the weight-of-evidence (WofE) based decision fusion method is applied to combine the probability outputs of classification results from the MLS and ALS data. The experiment results obtained in city road corridors demonstrated that point cloud data acquired from the airborne platform achieved even slightly better results in terms of tree detection rate, tree species and vitality classification accuracy, although the tree vitality distribution in the test site is less balanced compared to the species distribution. When combined with MLS data, overall accuracies of 78{\%} and 74{\%} for tree species and vitality classification can be achieved, which has improved by 5.7{\%} and 4.64{\%} respectively compared to the usage of airborne data only.},
author = {Wu, Jianwei and Yao, Wei and Polewski, Przemyslaw},
doi = {10.3390/rs10091403},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {9},
title = {{Mapping Individual Tree Species and Vitality along Urban Road Corridors with LiDAR and Imaging Sensors: Point Density versus View Perspective}},
volume = {10},
year = {2018}
}
@article{MatíasdiMartino2015176,
abstract = {In this work we describe a novel one-shot face recognition setup. Instead of using a 3D scanner to reconstruct the face, we acquire a single photo of the face of a person while a rectangular pattern is been projected over it. Using this unique image, it is possible to extract 3D low-level geometrical features without the explicit 3D reconstruction. To handle expression variations and occlusions that may occur (e.g. wearing a scarf or a bonnet), we extract information just from the eyes-forehead and nose regions which tend to be less influenced by facial expressions. Once features are extracted, SVM hyper-planes are obtained from each subject on the database (one vs all approach), then new instances can be classified according to its distance to each of those hyper-planes. The advantage of our method with respect to other ones published in the literature, is that we do not need and explicit 3D reconstruction. Experiments with the Texas 3D Database and with new acquired data are presented, which shows the potential of the presented framework to handle different illumination conditions, pose and facial expressions. {\textcopyright} Springer International Publishing Switzerland 2015.},
annote = {cited By 0},
author = {di Martino, J and Fern{\'{a}}ndez, A and Ferrari, J},
doi = {10.1007/978-3-319-25751-8_22},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {lerdepois,revisao{\_}V2,revisao{\_}scopus,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V2,revisao{\_}scopus,tutux9},
pages = {176--183},
title = {{One-shot 3D-gradient method applied to face recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983485990{\&}doi=10.1007{\%}2F978-3-319-25751-8{\_}22{\&}partnerID=40{\&}md5=d1ca3b0d2eb47528de1f16f2734dc918},
volume = {9423},
year = {2015}
}
@inproceedings{ISI:000387959204074,
abstract = {We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morphologically significant. The proposed algorithm evolves level set curves with adaptive geometric speed functions to automatically extract effective seed points for dense correspondence. Correspondences are established by minimizing the bending energy between patches around seed points of given faces to those of a reference face. Given its hierarchical structure, our algorithm is capable of establishing thousands of correspondences between a large number of faces. Finally, a morphable model based on the dense corresponding points is fitted to an unseen query face for transfer of correspondences and hence automatic detection of landmarks. The proposed algorithm can detect any number of pre-defined landmarks including subtle landmarks that are even difficult to detect manually. Extensive experimental comparison on two benchmark databases containing 6, 507 scans shows that our algorithm outperforms six state of the art algorithms.},
annote = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Boston, MA, JUN 07-12, 2015},
author = {Gilani, Syed Zulqarnain and Shafait, Faisal and Mian, Ajmal},
booktitle = {2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
isbn = {978-1-4673-6964-0},
issn = {1063-6919},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IEEE},
pages = {4639--4648},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Shape-based Automatic Detection of a Large Number of 3D Facial Landmarks}},
year = {2015}
}
@inproceedings{ISI:000353328200021,
abstract = {In the context of face modeling, probably the most well-known approach to represent 3D faces is the 3D Morphable Model (3DMM). When 3DMM is fitted to a 2D image, the shape as well as the texture and illumination parameters are simultaneously estimated. However, if real facial texture is needed, texture extraction from the 2D image is necessary. This paper addresses the possible problems in texture extraction of a single image caused by self-occlusion. Unlike common approaches that leverage the symmetric property of the face by mirroring the visible facial part, which is sensitive to inhomogeneous illumination, this work first generates a virtual texture map for the skin area iteratively by averaging the color of neighbored vertices. Although this step creates unrealistic, overly smoothed texture, illumination stays constant between the real and virtual texture. In the second pass, the mirrored texture is gradually blended with the real or generated texture according to the visibility. This scheme ensures a gentle handling of illumination and yet yields realistic texture. Because the blending area only relates to non-informative area, main facial features still have unique appearance in different face halves. Evaluation results reveal realistic rendering in novel poses robust to challenging illumination conditions and small registration errors.},
annote = {Conference on Image Processing - Machine Vision Applications VIII, San
Francisco, CA, FEB 10-11, 2015},
author = {Qu, Chengchao and Monari, Eduardo and Schuchert, Tobias and Beyerer, Juergen},
booktitle = {IMAGE PROCESSING: MACHINE VISION APPLICATIONS VIII},
editor = {{Lam, EY and Niel}, KS},
isbn = {978-1-62841-495-0},
issn = {0277-786X},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {Soc Imaging Sci {\&} Technol; SPIE},
series = {Proceedings of SPIE},
title = {{Realistic Texture Extraction for 3D Face Models Robust to Self-Occlusion}},
volume = {9405},
year = {2015}
}
@article{ISI:000463151400027,
abstract = {Recognition of sleep posture and its changes are related to information monitoring in a number of health-related applications such as apnea prevention and elderly care. This paper uses a less privacy-invading approach to classify sleep postures of a person in various configurations including side and supine postures. In order to accomplish this, a single depth sensor has been utilized to collect selective depth signals and populated a dataset associated with the depth data. The data is then analyzed by a novel frequency-based feature selection approach. These extracted features were then correlated in order to rank their information content in various 2D scans from the 3D point cloud in order to train a support vector machine (SVM). The data of subjects are collected under two conditions. First when they were covered with a thin blanket and second without any blanket. In order to reduce the dimensionality of the feature space, a T-test approach is employed to determine the most dominant set of features in the frequency domain. The proposed recognition approach based on the frequency domain is also compared with an approach using feature vector defined based on skeleton joints. The comparative studies are performed given various scenarios and by a variety of datasets. Through our study, it is shown that our proposed method offers better performance to that of the joint-based method.},
author = {Rasouli, Maryam S D and Payandeh, Shahram},
doi = {10.1007/s12652-018-0796-1},
issn = {1868-5137},
journal = {JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {5, SI},
pages = {1999--2014},
title = {{A novel depth image analysis for sleep posture estimation}},
volume = {10},
year = {2019}
}
@inproceedings{ISI:000427598702135,
abstract = {This paper explores the novel use of multiple depth sensors to overcome occlusions and improve localization and tracking of body extremities. The usage of data from only depth sensors not only overcomes visual challenges associated with RGB sensors under low illumination, but also protects the identity of surveyed person with high confidentiality. For integrating depth information from multiple sources, the paper presents first an overview of a novel calibration method for multiple depth sensors. In case of occlusion of any fiducial point in the primary sensor's depth image, co-ordinates of the point can be obtained from the frame of other sensors using the calibration parameters. To localize salient body parts such as hands, head and feet, a surface triangular mesh is applied on generated 3D point cloud from the primary sensor. The geodesic extrema from the mesh coincide with body extremities. The body extremities can be identified based on those relative geodesic distances between the extremities. Once the body parts are labelled, a portion of body can be targeted and evaluated for specific gait analysis and visualization. For the performance evaluation, our calibration method has fared well in comparison to other available techniques. Also, our proposed localization of salient body parts is able to successfully tag the specific body part i.e. the head region.},
annote = {IEEE International Conference on Systems, Man, and Cybernetics (SMC),
Banff, CANADA, OCT 05-08, 2017},
author = {Mohsin, Nasreen and Payandeh, Shahram},
booktitle = {2017 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC)},
isbn = {978-1-5386-1645-1},
issn = {1062-922X},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IEEE},
pages = {2736--2741},
series = {IEEE International Conference on Systems Man and Cybernetics Conference Proceedings},
title = {{Localization and Identification of Body Extremities Based on Data from Multiple Depth Sensors}},
year = {2017}
}
@article{ISI:000358893300004,
abstract = {Automated and semi-automated detection and segmentation of spinal and vertebral structures from computed tomography (CT) images is a challenging task due to a relatively high degree of anatomical complexity, presence of unclear boundaries and articulation of vertebrae with each other, as well as due to insufficient image spatial resolution, partial volume effects, presence of image artifacts, intensity variations and low signal-to-noise ratio. In this paper, we describe a novel framework for automated spine and vertebrae detection and segmentation from 3-D CT images. A novel optimization technique based on interpolation theory is applied to detect the location of the whole spine in the 3-D image and, using the obtained location of the whole spine, to further detect the location of individual vertebrae within the spinal column. The obtained vertebra detection results represent a robust and accurate initialization for the subsequent segmentation of individual vertebrae, which is performed by an improved shape-constrained deformable model approach. The framework was evaluated on two publicly available CT spine image databases of 50 lumbar and 170 thoracolumbar vertebrae. Quantitative comparison against corresponding reference vertebra segmentations yielded an overall mean centroid-to-centroid distance of 1.1 mm and Dice coefficient of 83.6{\%} for vertebra detection, and an overall mean symmetric surface distance of 0.3 mm and Dice coefficient of 94.6{\%} for vertebra segmentation. The results indicate that by applying the proposed automated detection and segmentation framework, vertebrae can be successfully detected and accurately segmented in 3-D from CT spine images.},
author = {Korez, Robert and Ibragimov, Bulat and Likar, Bostjan and Pernus, Franjo and Vrtovec, Tomaz},
doi = {10.1109/TMI.2015.2389334},
issn = {0278-0062},
journal = {IEEE TRANSACTIONS ON MEDICAL IMAGING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {aug},
number = {8, SI},
pages = {1649--1662},
title = {{A Framework for Automated Spine and Vertebrae Interpolation-Based Detection and Model-Based Segmentation}},
volume = {34},
year = {2015}
}
@article{ISI:000349588900008,
abstract = {This paper presents a representation of 3D facial motion sequences that allows performing statistical analysis of 3D face shapes in motion. The resulting statistical analysis is applied to automatically generate realistic facial animations and to recognize dynamic facial expressions. To perform statistical analysis of 3D facial shapes in motion over different subjects and different motion sequences, a large database of motion sequences needs to be brought in full correspondence. Existing algorithms that compute correspondences between 3D facial motion sequences either require manual input or suffer from instabilities caused by drift. For large databases, algorithms that require manual interaction are not practical. We propose an approach to robustly compute correspondences between a large set of facial motion sequences in a fully automatic way using a multilinear model as statistical prior. In order to register the motion sequences, a good initialization is needed. We obtain this initialization by introducing a landmark prediction method for 3D motion sequences based on Markov Random Fields. Using this motion sequence registration, we find a compact representation of each motion sequence consisting of one vector of coefficients for identity and a high dimensional curve for expression. Based on this representation, we synthesize new motion sequences and perform expression recognition. We show experimentally that the obtained registration is of high quality, where 56{\%} of all vertices are at distance at most 1 mm from the input data, and that our synthesized motion sequences look realistic. (C) 2014 Elsevier Inc. All rights reserved.},
author = {Bolkart, Timo and Wuhrer, Stefanie},
doi = {10.1016/j.cviu.2014.06.013},
issn = {1077-3142},
journal = {COMPUTER VISION AND IMAGE UNDERSTANDING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {100--115},
title = {{3D faces in motion: Fully automatic registration and statistical analysis}},
volume = {131},
year = {2015}
}
@inproceedings{7338568,
abstract = {Due to the improvements in computer technology, interaction between computer and human has been evolved from command-line based interfaces to natural user interfaces that enables interaction in a more human-human way such as by speech, hand and body gestures, facial expressions and eye gaze. In this study controlling three dimensional images with gestures and speech using a three dimensional depth camera is realized in order to ensure human computer interaction in a more natural way. For this purpose realized system allows starting and closing the application and interaction with the three dimensional images using only speech and gestures but not using keyboard and mouse. System allows three different speech commands to start, close the application and reset the three dimensional image. Furthermore gesture-based commands are used to rotate, pan and zoom the three dimensional image.},
author = {Erg{\"{u}}ner, F and Durdu, P O},
booktitle = {2015 9th International Conference on Application of Information and Communication Technologies (AICT)},
doi = {10.1109/ICAICT.2015.7338568},
keywords = {gesture recognition,human computer interaction,key,revisao{\_}V2,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}etapa1,revisao{\_}ieeexplore,tutux9},
month = {oct},
pages = {305--309},
title = {{Multimodal natural interaction for 3D images}},
year = {2015}
}
@inproceedings{ISI:000380558900020,
abstract = {3D modeling of point clouds is an important but time-consuming process, inspiring extensive research in automatic methods. Prior efforts focus on primitive geometry, street structures or indoor objects, but industrial data has rarely been pursued. Our work presents a method for automatic modeling and recognition of 3D industrial site point clouds, dividing the task into 3 separate sub-problems: pipe modeling, plane classification, and object recognition. The results are integrated to obtain the complete model, revealing some issues during the integration, solved by utilizing information gained from each individual process. Experiments show that the presented method automatically models large and complex industrial scenes with a quality that outperforms leading commercial modeling software and is comparable to professional hand-made models.},
annote = {14th IAPR International Conference on Machine Vision Applications (MVA),
Tokyo, JAPAN, MAY 18-22, 2015},
author = {Pang, Guan and Qiu, Rongqi and Huang, Jing and You, Suya and Neumann, Ulrich},
booktitle = {2015 14th IAPR International Conference on Machine Vision Applications (MVA)},
isbn = {978-4-9011-2214-6},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {MVA Organisation; IAPR TC-8; National Institute of Advanced Industrial Science and Technology (AIST); The Telecommunications Advancement Foundation; KDDI Foundation},
pages = {22--25},
title = {{Automatic 3D Industrial Point Cloud Modeling and Recognition}},
year = {2015}
}
@article{ISI:000427009100019,
abstract = {This paper presents a 3-D object recognition method and its implementation on a robotic navigation aid to allow real-time detection of indoor structural objects for the navigation of a blind person. The method segments a point cloud into numerous planar patches and extracts their inter-plane relationships (IPRs). Based on the existing IPRs of the object models, the method defines six high level features (HLFs) and determines the HLFs for each patch. A Gaussian-mixture-model-based plane classifier is then devised to classify each planar patch into one belonging to a particular object model. Finally, a recursive plane clustering procedure is used to cluster the classified planes into the model objects. As the proposed method uses geometric context to detect an object, it is robust to the object's visual appearance change. As a result, it is ideal for detecting structural objects (e.g., stairways, doorways, and so on). In addition, it has high scalability and parallelism. The method is also capable of detecting some indoor nonstructural objects. Experimental results demonstrate that the proposed method has a high success rate in object recognition.},
author = {Ye, Cang and Qian, Xiangfei},
doi = {10.1109/TNSRE.2017.2748419},
issn = {1534-4320},
journal = {IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {2},
pages = {441--450},
title = {{3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired}},
volume = {26},
year = {2018}
}
@inproceedings{7332643,
abstract = {This paper proposes a method to analyse human-made environments in order to verify the presence of ascending steps or stairs. Our system is intended to assist visually impaired people by providing acoustic information about the scene in front of a low-resolution Time-of-Flight (ToF) camera that is fixed to a mobile vehicle (rollator). Detailed instructions to the user about potentially hazardous situations are provided. This paper in particular deals with a fast approach on classification of ascending steps in 3D point clouds. This method is part of a system that aims on enhancing visually impaired persons understand the environment and help prevent collisions.},
author = {Stahlschmidt, C and Gavriilidis, A and Kummert, A},
booktitle = {2015 IEEE 9th International Workshop on Multidimensional (nD) Systems (nDS)},
doi = {10.1109/NDS.2015.7332643},
keywords = {computer graphics,image classification,image resol,revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,tutux9},
pages = {1--6},
title = {{Classification of ascending steps and stairs using Time-of-Flight sensor data}},
year = {2015}
}
@article{ISI:000422927300005,
abstract = {Euphrates turtle is the only soft shell turtle of Iran, and unfortunately is in danger of extinction due to multiple reasons. Imaging techniques, in addition to their importance in diagnosis of injuries to animals, have been used as non-invasive methods to provide normal anatomic views. A few studies have been conducted to understand body structure of the Euphrates turtle. Since there is only general information about the anatomy of turtle limbs, the normal skeleton of the Euphrates limbs was studied. For this purpose four adult Euphrates turtles were used. Digital radiographic examination was performed by computed radiographic (CR) in dorsoventral (DV) and lateral (L) positions. Spiral CT-scanning was done and 3D images of the bones were reconstructed for anatomical evaluation. For skeletal preparation, the skeleton was cleaned by a combination of boiling and mealworm methods and limbs' bones were examined anatomically. In the present study, simultaneous anatomic, radiographic and CT studies of bones in individual turtles made us possible to describe bones anatomically and provided comparable and complementary conditions to represent the abilities of the radiography and CT for better understanding of the anatomy. Arrangement and the number of carpal and tarsal bones are used in turtles' classification. Among the studied species, Euphrates turtle carpal and tarsal bones show the most similarities to the Apolone spinifera. (c) 2016 Urmia University. All rights reserved.},
author = {Ahranjani, Behnaz Asadi and Shojaei, Bahador and Tootian, Zahra and Masoudifard, Madjid and Rostami, Amir},
issn = {2008-8140},
journal = {VETERINARY RESEARCH FORUM},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {2},
pages = {117--124},
title = {{Anatomical, radiographical and computed tomographic study of the limbs skeleton of the Euphrates soft shell turtle (Rafetus euphraticus)}},
volume = {7},
year = {2016}
}
@inproceedings{7943450,
abstract = {Palmprint is a very unique and distinctive biometric trait because of features such as a person's inimitable principal lines, wrinkles, delta points, and minutiae. These constitute the main reasons why palmprint verification is considered as one of the most reliable personal identification methods. However, a clear majority of the research on palm-prints are concentrated on 2-D palmprint images irrespective of the fact that the human palm is a 3D-surface. While 2-D palmprint recognition has proved to be efficient in terms of verification rate, it has some essential downsides. These restrictions can adversely affect the performance and robustness of the palmprint recognition system. One of the possible solutions to resolve the limitations associated with 2-D palm print authentication systems is (i) to use a 3-D scanning system and to produce high quality 3-D images with depth information; (ii) to map 3-D palm-print images into 2-D images which may support the usage of 3-D images with both biometric palmprint 2-D image databases and 2-D palmprint recognition tools. The bloom of 3-D technologies has made it easier to capture and store 3-D images. The problem of a direct mapping approach is that a large section of the palm is hard-pressed on the scanner surface during 2-D based acquisition. This paper proposes a novel technique to unravel/map 3-D palm images to its equivalent 2-D palm-print image. This image can be then used to perform efficient and accurate 2-D identification/ verification. Experimental results and discussions will also be presented.},
author = {Rajeev, S and Shreyas, K K M and Panetta, K and Agaian, S},
booktitle = {2017 IEEE International Symposium on Technologies for Homeland Security (HST)},
doi = {10.1109/THS.2017.7943450},
keywords = {authorisation,biometrics (access control),computer,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux5},
month = {apr},
pages = {1--6},
title = {{3-D palmprint modeling for biometric verification}},
year = {2017}
}
@inproceedings{Pham20171851,
abstract = {We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker capable of handling a wide range of target scene depths, whose performances are demonstrated in our extensive experiments better than those of the state-of-the-arts. {\textcopyright} 2016 IEEE.},
annote = {From Duplicate 2 (Robust real-time performance-driven 3D face tracking - Pham, H X; Pavlovic, V; Cai, J; Cham, T.-J.)

cited By 2

From Duplicate 3 (Robust real-time performance-driven 3D face tracking - Pham, H X; Pavlovic, V; Cai, J; Cham, T.-J.; and)

From Duplicate 2 (Robust real-time performance-driven 3D face tracking - Pham, H X; Pavlovic, V; Cai, J; Cham, T.-J.)

cited By 2},
author = {Pham, H X and Pavlovic, V and Cai, J and Cham, T.-J. and And},
booktitle = {2016 23rd International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2016.7899906},
keywords = {cameras,computational geometry,face recognition,im,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {1851--1856},
title = {{Robust real-time performance-driven 3D face tracking}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019106844{\&}doi=10.1109{\%}2FICPR.2016.7899906{\&}partnerID=40{\&}md5=3e60c1814d147400cdda963f8f555dd2},
year = {2016}
}
@inproceedings{7298920,
abstract = {Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.},
author = {Ji, R},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298920},
issn = {1063-6919},
keywords = {Boltzmann machines,feature extraction,image colour,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {3013--3021},
title = {{Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery}},
year = {2015}
}
@article{ISI:000424092300038,
abstract = {A plethora of information contained in full-waveform (FW) Light Detection and Ranging (LiDAR) data offers prospects for characterizing vegetation structures. This study aims to investigate the capacity of FW LiDAR data alone for tree species identification through the integration of waveform metrics with machine learning methods and Bayesian inference. Specifically, we first conducted automatic tree segmentation based on the waveform-based canopy height model (CHM) using three approaches including TreeVaW, watershed algorithms and the combination of TreeVaW and watershed (TW) algorithms. Subsequently, the Random forests (RF) and Conditional inference forests (CF) models were employed to identify important tree-level waveform metrics derived from three distinct sources, such as raw waveforms, composite waveforms, the waveform-based point cloud and the combined variables from these three sources. Further, we discriminated tree (gray pine, blue oak, interior live oak) and shrub species through the RF, CF and Bayesian multinomial logistic regression (BMLR) using important waveform metrics identified in this study. Results of the tree segmentation demonstrated that the TW algorithms outperformed other algorithms for delineating individual tree crowns. The CF model overcomes waveform metrics selection bias caused by the RF model which favors correlated metrics and enhances the accuracy of subsequent classification. We also found that composite waveforms are more informative than raw waveforms and waveform-based point cloud for characterizing tree species in our study area. Both classical machine learning methods (the RF and CF) and the BMLR generated satisfactory average overall accuracy (74{\%} for the RF, 77{\%} for the CF and 81{\%} for the BMLR) and the BMLR slightly outperformed the other two methods. However, these three methods suffered from low individual classification accuracy for the blue oak which is prone to being misclassified as the interior live oak due to the similar characteristics of blue oak and interior live oak. Uncertainty estimates from the BMLR method compensate for this downside by providing classification results in a probabilistic sense and rendering users with more confidence in interpreting and applying classification results to real-world tasks such as forest inventory. Overall, this study recommends the CF method for feature selection and suggests that BMLR could be a superior alternative to classical machining learning methods.},
author = {Zhou, Tan and Popescu, Sorin C and Lawing, A Michelle and Eriksson, Marian and Strimbu, Bogdan M and Buerkner, Paul C},
doi = {10.3390/rs10010039},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux9},
month = {jan},
number = {1},
title = {{Bayesian and Classical Machine Learning Methods: A Comparison for Tree Species Classification with LiDAR Waveform Signatures}},
volume = {10},
year = {2018}
}
@inproceedings{ISI:000392739800107,
abstract = {(Semi)-automatic facade reconstruction from terrestrial LiDAR point clouds is often affected by both quality of point cloud itself and imperfectness of object recognition algorithms. In this paper, we employ regularities, which exist on facades, to mitigate these problems. For example, doors, windows and balconies often have orthogonal and parallel boundaries. Many windows are constructed with the same shape. They may be arranged at the same lines and distance intervals, so do different windows. By identifying regularities among objects with relatively poor quality, these can be applied to calibrate the objects and improve their quality. The paper focuses on the regularities among the windows, which is the majority of objects on the wall. Regularities are classified into three categories: within an individual window, among similar windows and among different windows. Nine cases are specified as a reference for exploration. A hierarchical clustering method is employed to identify and apply regularities in a feature space, where regularities can be identified from clusters. To find the corresponding features in the nine cases of regularities, two phases are distinguished for similar and different windows. In the first phase, ICP (iterative closest points) is used to identify groups of similar windows. The registered points and a number of transformation matrices are used to identify and apply regularities among similar windows. In the second phase, features are extracted from the boundaries of the different windows. When applying regularities by relocating windows, the connections, called chains, established among the similar windows in the first phase are preserved. To test the performance of the algorithms, two datasets from terrestrial LiDAR point clouds are used. Both show good effects on the reconstructed model, while still matching with original point cloud, preventing over or under-regularization.},
annote = {23rd Congress of the
International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
Prague, CZECH REPUBLIC, JUL 12-19, 2016},
author = {Zhou, K and Gorte, B and Zlatanova, S},
booktitle = {XXIII ISPRS Congress, Commission V},
doi = {10.5194/isprsarchives-XLI-B5-749-2016},
editor = {{Halounova, L and Safar, V and Remondino, F and Hodac, J and Pavelka, K and Shortis, M and Rinaudo, F and Scaioni, M and Boehm, J and RiekeZapp}, D},
issn = {2194-9034},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {B5},
organization = {Int Soc Photogrammetry {\&} Remote Sensing},
pages = {749--755},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{EXPLORING REGULARITIES FOR IMPROVING FACADE RECONSTRUCTION FROM POINT CLOUDS}},
volume = {41},
year = {2016}
}
@article{ISI:000358942000004,
abstract = {In this paper four different delineation methods based on airborne laser scanning (ALS) and hyperspectral data are compared over a forest area in the Italian Alps. The comparison was carried out in terms of detected trees, while the ALS based methods are compared also in terms of attributes estimated (e.g. height). From the experimental results emerged that ALS methods outperformed hyperspectral one in terms of tree detection rate in two of three cases. The best results were achieved with a method based on region growing on an ALS image, and by one based on clustering of raw ALS point cloud. Regarding the estimates of the tree attributes all the ALS methods provided good results with very high accuracies when considering only big trees.},
author = {Dalponte, Michele and Reyes, Francesco and Kandare, Kaja and Gianelle, Damiano},
doi = {10.5721/EuJRS20154821},
issn = {2279-7254},
journal = {EUROPEAN JOURNAL OF REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
pages = {365--382},
title = {{Delineation of Individual Tree Crowns from ALS and Hyperspectral data: a comparison among four methods}},
volume = {48},
year = {2015}
}
@article{ISI:000368511500001,
abstract = {Nanotechnology has been used for every single modality in the molecular imaging arena for imaging purposes. Synergic advantages can be explored when multiple molecular imaging modalities are combined with respect to single imaging modalities. Multifunctional nanoparticles have large surface areas, where multiple functional moieties can be incorporated, including ligands for site-specific targeting and radionuclides, which can be detected to create 3D images. Recently, radiolabeled nanoparticles with individual properties have attracted great interest regarding their use in multimodality tumor imaging. Multifunctional nanoparticles can combine diagnostic and therapeutic capabilities for both target-specific diagnosis and the treatment of a given disease. The future of nanomedicine lies in multifunctional nanoplatforms that combine the diagnostic ability and therapeutic effects using appropriate ligands, drugs, responses and technological devices, which together are collectively called theranostic drugs. Co-delivery of radiolabeled nanoparticles is useful in multifunctional molecular imaging areas because it comprises several advantages based on nanoparticles architecture, pharmacokinetics and pharmacodynamic properties.},
author = {Mirshojaei, Seyedeh Fatemeh and Ahmadi, Amirhossein and Morales-Avila, Enrique and Ortiz-Reynoso, Mariana and Reyes-Perez, Horacio},
doi = {10.3109/1061186X.2015.1048516},
issn = {1061-186X},
journal = {JOURNAL OF DRUG TARGETING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {feb},
number = {2},
pages = {91--101},
title = {{Radiolabelled nanoparticles: novel classification of radiopharmaceuticals for molecular imaging of cancer}},
volume = {24},
year = {2016}
}
@inproceedings{ISI:000414287400080,
abstract = {3D scanning and 3D printing techniques, as the technical impetus of 3D face recognition, also boost unconsciously the security threat against it from the spoofing attacks via manufactured mask. In order to improve the robustness of 3D face recognition system, several countermeasures against mask attacks based on photometric features have been reported in recent years. However, the anti-spoofing approach involving 3D meshed face scan and the related 3D facial features have not been studied yet. For filling this gap, in this paper, we propose to exploit the anti-spoofing performance of geometric attributes based 3D facial description. It synthesises the advantages of the selected geometric attributes, named principal curvature measures, and the meshSIFT-based feature descriptor. Specifically, the estimation of geometric attributes is coherent to the property of discrete surface, and the feature related to them can accurately describe the shape of facial surface. These characteristics are beneficial to discovering the geometry-based dissimilarity between genuine face and fraud mask. In the experiment part, the baselines of verification and anti-spoofing performance are evaluated on the Morpho database. Furthermore, for simulating a real-world scenario and testing the corresponding anti-spoofing performance, the size of genuine face set is massively extended by uniting the Morpho database and the FRGC v2.0 database to increase the ratio of genuine faces to fraud masks. The evaluation results prove that the proposed 3D face verification system can guarantee competitive verification accuracy for genuine faces and promising anti-spoofing performance against mask attacks.},
annote = {12th IEEE International Conference on Automatic Face and Gesture
Recognition (FG), Washington, DC, MAY 30-JUN 03, 2017},
author = {Tang, Yinhang and Chen, Liming},
booktitle = {2017 12TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2017)},
doi = {10.1109/FG.2017.74},
isbn = {978-1-5090-4023-0},
issn = {2326-5396},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
organization = {IEEE; Baidu; Mitsubishi Elect Res Labs Inc; 3dMD; DI4D; Syst {\&} Technol Res; ObjectVideo Labs; MUKH Technologies; IEEE Comp Soc},
pages = {589--595},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{3D Facial Geometric Attributes based Anti-spoofing Approach against Mask Attacks}},
year = {2017}
}
@inproceedings{8317846,
abstract = {This paper describes a super-sensor that enables 360-degree environment perception for automated vehicles in urban traffic scenarios. We use four fisheye cameras, four 360-degree LIDARs and a GPS/IMU sensor mounted on an automated vehicle to build a super-sensor that offers an enhanced low-level representation of the environment by harmonizing all the available sensor measurements. Individual sensors cannot provide a robust 360-degree perception due to their limitations: field of view, range, orientation, number of scanning rays, etc. The novelty of this work consists of segmenting the 3D LIDAR point cloud by associating it with the 2D image semantic segmentation. Another contribution is the sensor configuration that enables 360-degree environment perception. The following steps are involved in the process: calibration, timestamp synchronization, fisheye image unwarping, motion correction of LIDAR points, point cloud projection onto the images and semantic segmentation of images. The enhanced low-level representation will improve the high-level perception environment tasks such as object detection, classification and tracking.},
author = {Varga, R and Costea, A and Florea, H and Giosan, I and Nedevschi, S},
booktitle = {2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)},
doi = {10.1109/ITSC.2017.8317846},
issn = {2153-0017},
keywords = {image classification,image motion analysis,image r,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {oct},
pages = {1--8},
title = {{Super-sensor for 360-degree environment perception: Point cloud segmentation using image features}},
year = {2017}
}
@article{ISI:000395844700002,
abstract = {The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15-20years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means Of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region. (C) 2016 Elsevier B.V. All rights reserved.},
author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic, Maja},
doi = {10.1016/j.imavis.2016.10.007},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {3--12},
title = {{Statistical non-rigid ICP algorithm and its application to 3D face alignment}},
volume = {58},
year = {2017}
}
@inproceedings{ISI:000360175900188,
abstract = {3D Face recognition has been an area of interest for the past few decades in pattern recognition. This paper focuses on problems of person identification using 3D Face data. Here unregistered Face data, i.e. both texture and depth is fed to classifier in spectral representations of data. 2D Discrete Fourier Transform (DFT) is used for spectral representation. Fusion of scores improves the recognition accuracy significantly since use of depth information alone in spectral representation was not sufficient to increase accuracy. Statistical method seems to degrade performance of system when applied to texture data and was effective for depth data. (C) 2015 The Authors. Published by Elsevier B.V.},
annote = {International Conference on Information and Communication Technologies
(ICICT), Kochi, INDIA, DEC 03-05, 2014},
author = {Naveen, S and Moni, R S},
booktitle = {PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES, ICICT 2014},
doi = {10.1016/j.procs.2015.02.078},
editor = {Samuel, P},
issn = {1877-0509},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux5},
organization = {Cochin Uni Sci {\&} Technol, Sch Engn; TEQIP Phase II},
pages = {1537--1545},
series = {Procedia Computer Science},
title = {{Multimodal Face Recognition System using Spectral Transformation of 2D Texture feature and Statistical processing of Face Range Images}},
volume = {46},
year = {2015}
}
@inproceedings{ISI:000406771300100,
abstract = {Efficient detection of three dimensional (3D) objects in point clouds is a challenging problem. Performing 3D descriptor matching or 3D scanning-window search with detector are both time-consuming due to the 3-dimensional complexity. One solution is to project 3D point cloud into 2D images and thus transform the 3D detection problem into 2D space, but projection at multiple viewpoints and rotations produce a large amount of 2D detection tasks, which limit the performance and complexity of the 2D detection algorithm choice. We propose to use convolutional neural network (CNN) for the 2D detection task, because it can handle all viewpoints and rotations for the same class of object together, as well as predicting multiple classes of objects with the same network, without the need for individual detector for each object class. We further improve the detection efficiency by concatenating two extra levels of early rejection networks with binary outputs before the multi-class detection network. Experiments show that our method has competitive overall performance with at least one-order of magnitude speedup comparing with latest 3D point cloud detection methods.},
annote = {23rd International Conference on Pattern Recognition (ICPR), Mexican
Assoc Comp Vis Robot {\&} Neural Comp, Cancun, MEXICO, DEC 04-08, 2016},
author = {Pang, Guan and Neumann, Ulrich},
booktitle = {2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)},
isbn = {978-1-5090-4847-2},
issn = {1051-4651},
keywords = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
organization = {Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm; Elsevier; IBM Res; INTEL; CONACYT},
pages = {585--590},
series = {International Conference on Pattern Recognition},
title = {{3D Point Cloud Object Detection with Multi-View Convolutional Neural Network}},
year = {2016}
}
@inproceedings{ISI:000391015300024,
abstract = {Urban green spaces, particularly urban trees, play a key role in enhancing the liveability of cities. The availability of accurate and up-to-date maps of tree canopy cover is important for sustainable development of urban green spaces. LiDAR point clouds are widely used for the mapping of buildings and trees, and several LiDAR point cloud classification techniques have been proposed for automatic mapping. However, the effectiveness of point cloud classification techniques for automated tree extraction from LiDAR data can be impacted to the point of failure by the complexity of tree canopy shapes in urban areas. Multispectral imagery, which provides complementary information to LiDAR data, can improve point cloud classification quality. This paper proposes a reliable method for the extraction of tree canopy cover from fused LiDAR point cloud and multispectral satellite imagery data. The proposed method initially associates each LiDAR point with spectral information from the co-registered satellite imagery data. It calculates the normalised difference vegetation index (NDVI) value for each LiDAR point and corrects tree points which have been misclassified as buildings. Then, region growing of tree points, taking the NDVI value into account, is applied. Finally, the LiDAR points classified as tree points are utilised to generate a canopy cover map. The performance of the proposed tree canopy cover mapping method is experimentally evaluated on a data set of airborne LiDAR and WorldView 2 imagery covering a suburb in Melbourne, Australia.},
annote = {23rd ISPRS Congress, Prague, CZECH REPUBLIC, JUL 12-19, 2016},
author = {Parmehr, Ebadat G and Amati, Marco and Fraser, Clive S},
booktitle = {XXIII ISPRS CONGRESS, COMMISSION VII},
doi = {10.5194/isprsannals-III-7-181-2016},
editor = {{Halounova, L and Sunar, F and Potuckova, M and Patkova, L and Yoshimura, M and Soergel}, U},
issn = {2194-9034},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
number = {7},
organization = {Int Soc Photogrammetry {\&} Remote Sensing},
pages = {181--186},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{MAPPING URBAN TREE CANOPY COVER USING FUSED AIRBORNE LIDAR AND SATELLITE IMAGERY DATA}},
volume = {3},
year = {2016}
}
@article{ISI:000384777300010,
abstract = {New technologies such as lidar enable the rapid collection of massive datasets to model a 3D scene as a point cloud. However, while hardware technology continues to advance, processing 3D point clouds into informative models remains complex and time consuming. A common approach to increase processing efficiently is to segment the point cloud into smaller sections. This paper proposes a novel approach for point cloud segmentation using computer vision algorithms to analyze panoramic representations of individual laser scans. These panoramas can be quickly created using an inherent neighborhood structure that is established during the scanning process, which scans at fixed angular increments in a cylindrical or spherical coordinate system. In the proposed approach, a selected image segmentation algorithm is applied on several input layers exploiting this angular structure including laser intensity, range, normal vectors, and color information. These segments are then mapped back to the 3D point cloud so that modeling can be completed more efficiently. This approach does not depend on pre-defined mathematical models and consequently setting parameters for them. Unlike common geometrical point cloud segmentation methods, the proposed method employs the colorimetric and intensity data as another source of information. The proposed algorithm is demonstrated on several datasets encompassing variety of scenes and objects. Results show a very high perceptual (visual) level of segmentation and thereby the feasibility of the proposed algorithm. The proposed method is also more efficient compared to Random Sample Consensus (RANSAC), which is a common approach for point cloud segmentation. (C) 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.},
author = {Mahmoudabadi, Hamid and Olsen, Michael J and Todorovic, Sinisa},
doi = {10.1016/j.isprsjprs.2016.05.015},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
pages = {135--150},
title = {{Efficient terrestrial laser scan segmentation exploiting data structure}},
volume = {119},
year = {2016}
}
@inproceedings{ISI:000418397700008,
abstract = {Recently, researchers have shown an increased interest in the detection of activities of daily living (ADLs) for ambient assisted living (AAL) applications. In this study, we present an algorithm that detects activities related to personal hygiene. The approach is based on the evaluation of pose information and a person's proximity to objects belonging to the typical equipment of bathrooms, such as sink, toilet and shower. In addition to this high-level reasoning, we developed a skeleton-based algorithm that recognises actions using a supervised learning model. Therefore, we analysed several feature vectors, especially with regard to the representation of joint trajectories in the frequency domain. The results gave evidence that this high-level reasoning algorithm can reliably recognise hygiene-related activities. An evaluation of the skeleton-based algorithm shows that the defined actions were successfully classified with a rate of 96.66{\%}.},
annote = {5th International conference on Pattern Recognition Applications and
Methods (ICPRAM), Rome, ITALY, FEB 24-26, 2016},
author = {Richter, Julia and Wiede, Christian and Dayangac, Enes and Shahenshah, Ahsan and Hirtz, Gangolf},
booktitle = {PATTERN RECOGNITION APPLICATIONS AND METHODS, ICPRAM 2016},
doi = {10.1007/978-3-319-53375-9_8},
editor = {{Fred, A and DeMarsico, M and DiBaja}, GS},
isbn = {978-3-319-53375-9; 978-3-319-53374-2},
issn = {0302-9743},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
pages = {139--155},
series = {Lecture Notes in Computer Science},
title = {{Activity Recognition for Elderly Care by Evaluating Proximity to Objects and Human Skeleton Data}},
volume = {10163},
year = {2017}
}
@article{ISI:000423587100013,
abstract = {3D face similarity is a critical issue in computer vision, computer graphics and face recognition and so on. Since Fr,chet distance is an effective metric for measuring curve similarity, a novel 3D face similarity measure method based on Fr,chet distances of geodesics is proposed in this paper. In our method, the surface similarity between two 3D faces is measured by the similarity between two sets of 3D curves on them. Due to the intrinsic property of geodesics, we select geodesics as the comparison curves. Firstly, the geodesics on each 3D facial model emanating from the nose tip point are extracted in the same initial direction with equal angular increment. Secondly, the Fr,chet distances between the two sets of geodesics on the two compared facial models are computed. At last, the similarity between the two facial models is computed based on the Fr,chet distances of the geodesics obtained in the second step. We verify our method both theoretically and practically. In theory, we prove that the similarity of our method satisfies three properties: reflexivity, symmetry, and triangle inequality. And in practice, experiments are conducted on the open 3D face database GavaDB, Texas 3D Face Recognition database, and our 3D face database. After the comparison with iso-geodesic and Hausdorff distance method, the results illustrate that our method has good discrimination ability and can not only identify the facial models of the same person, but also distinguish the facial models of any two different persons.},
author = {Zhao, Jun-Li and Wu, Zhong-Ke and Pan, Zhen-Kuan and Duan, Fu-Qing and Li, Jin-Hua and Lv, Zhi-Han and Wang, Kang and Chen, Yu-Cong},
doi = {10.1007/s11390-018-1814-7},
issn = {1000-9000},
journal = {JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutui1},
month = {jan},
number = {1},
pages = {207--222},
title = {{3D Face Similarity Measure by Fr,chet Distances of Geodesics}},
volume = {33},
year = {2018}
}
@article{Siqueira2018,
abstract = {This work presents a multiple slicing model for 3D images of human face, using the Frontal, Sagittal and Transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2D spatial moments of Hu and 3D spatial Invariant Rotation Moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs non-neutral ROC I experiment, applying Linear Discriminant Analysis as classifier and more than one sample for training, achieved 98.7{\%} of verification rate at 0.1{\%} of false acceptance rate. By using the Support Vector Machine as classifier the rank1 experiment recognition rates of 99{\%} and 95.4{\%} have been achieved for a neutral vs neutral and for a neutral vs non-neutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when Anger and Disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity.},
author = {Siqueira, Robson S. and Alexandre, Gilderlane R. and Soares, Jose M. and The, George A.P.},
doi = {10.1109/LRA.2018.2854295},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siqueira et al. - 2018 - Triaxial Slicing for 3-D Face Recognition From Adapted Rotational Invariants Spatial Moments and Minimal Keypoi.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Computer vision for automation,recognition,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,surveillance systems,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {oct},
number = {4},
pages = {3513--3520},
title = {{Triaxial slicing for 3-D face recognition from adapted rotational invariants spatial moments and minimal keypoints dependence}},
url = {https://ieeexplore.ieee.org/document/8408720/},
volume = {3},
year = {2018}
}
@article{ISI:000368956300004,
abstract = {Background: Accurate segmentation of human head on medical images is an important process in a wide array of applications such as diagnosis, facial surgery planning, prosthesis design, and forensic identification. Objectives: In this study, a Bayesian method for segmentation of facial tissues is presented. Segmentation classes include muscle, bone, fat, air and skin. Methods: The method presented incorporates information fusion from multiple modalities, modelling of image resolution (measurement blurring), image noise, two priors helping to reduce noise and partial volume. Image resolution modelling employed facilitates resolution enhancement and superresolution capabilities during image segmentation. Regularization based on isotropic and directional Markov Random Field priors is integrated. The Bayesian model is solved iteratively yielding tissue class labels at every voxel of the image. Sub methods as variations of the main method are generated by using a combination of the models. Results: Testing of the sub-methods is performed on two patients using single modality three-dimensional (3D) image (magnetic resonance, MR or computerized tomography, CT) as well as registered MR-CT images with information fusion. Numerical, visual and statistical analyses of the methods are conducted. High segmentation accuracy values are obtained by the use of image resolution and partial volume models as well as information fusion from MR and CT images. The methods are also compared with our Bayesian segmentation method proposed in a previous study. The performance is found to be similar to our previous Bayesian approach, but the presented methods here eliminates ad hoc parameter tuning needed by the previous approach which is system and data acquisition setting dependent. Conclusions: The Bayesian approach presented provides resolution enhanced segmentation of very thin structures of the human head. Meanwhile, free parameters of the algorithm can be adjusted for different imaging systems and data acquisition settings in a more systematic way as compared with our previous study. (C) 2015 Elsevier Ireland Ltd. All rights reserved.},
author = {Sener, Emre and Mumcuoglu, Erkan U and Hamcan, Salih},
doi = {10.1016/j.cmpb.2015.10.009},
issn = {0169-2607},
journal = {COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {feb},
pages = {31--44},
title = {{Bayesian segmentation of human facial tissue using 3D MR-CT information fusion, resolution enhancement and partial volume modelling}},
volume = {124},
year = {2016}
}
@inproceedings{ISI:000377348700061,
abstract = {As the most distinct feature point in facial landmarks, nose tip plays a significant role in 3D facial studies. Successful detection of nose tip can facilitate many 3D facial studies tasks. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, need not training, can handle large rotations and occlusions. We first remove small isolated connected regions and noise from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, we compute multi-angle energy of each point, then we use hierarchical clustering method to cluster the points whose multi-angle energies are larger than a threshold value. In the largest cluster, we can find one point with the largest multi-angle energy. For all scales of the scale-space, we get a series of such points and apply hierarchical clustering again for these points, nose tip will have the largest multi-angle energy in the largest cluster. We evaluate our method in FRGC v2.0 3D face database and BOSPHORUS 3D face database. The experimental results verify the robustness of our method with a high nose tip detection rate.},
annote = {IEEE Advanced Information Technology, Electronic and Automation Control
Conference (IAEAC), Chongqing, PEOPLES R CHINA, DEC 19-20, 2015},
author = {Liu, Jian and Zhang, Quan and Tang, Chaojing},
booktitle = {2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC)},
isbn = {978-1-4799-1980-2},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
organization = {IEEE; IEEE Beijing Sect; Global Union Acad Sci {\&} Technol; Chongqing Global Union Acad Sci {\&} Technol},
pages = {309--315},
title = {{CoMES: A Novel Method for Robust Nose Tip Detection in Face Range Images}},
year = {2015}
}
@article{ISI:000409180500015,
abstract = {In order to solve the problem of low recognition accuracy in later period which is caused by the too few extracted parameters in the 3D face recognition, and the incapable formation of completed point cloud structure. An automatic iterative interpolation algorithm is proposed. The new and more accurate 3D face data points are obtained by automatic iteration. This algorithm can be used to restore the data point cloud information of 3D facial feature in 2D images by means of facial three-legged structure formed by 3D face and automatic interpolation. Thus, it can realize to shape the 3D facial dynamic model which can be recognized and has high saturability. Experimental results show that the interpolation algorithm can achieve the complete the construction of facial feature based on the facial feature after 3D dynamic reconstruction, and the validity is higher.},
author = {Sui, Dan and Hou, Deheng and Duan, Xinyu},
doi = {10.1007/s11042-015-3233-x},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
number = {19},
pages = {19575--19589},
title = {{An interpolation algorithm fitted for dynamic 3D face reconstruction}},
volume = {76},
year = {2017}
}
@article{ISI:000459798800005,
abstract = {Facial landmarking is a fundamental task in automatic machine-based face analysis. The majority of existing techniques for such a problem are based on 2D images; however, they suffer from illumination and pose variations that may largely degrade landmarking performance. The emergence of 3D data theoretically provides an alternative to overcome these weaknesses in the 2D domain. This article proposes a novel approach to 3D facial landmarking, which combines both the advantages of feature-based methods as well as model-based ones in a progressive three-stage coarse-to-fine manner (initial, intermediate, and fine stages). For the initial stage, a few fiducial landmarks (i.e., the nose tip and two inner eye corners) are robustly detected through curvature analysis, and these points are further exploited to initialize the subsequent stage. For the intermediate stage, a statistical model is learned in the feature space of three normal components of the facial point-cloud rather than the smooth original coordinates, namely Active Normal Model (ANM). For the fine stage, cascaded regression is employed to locally refine the landmarks according to their geometry attributes. The proposed approach can accurately localize dozens of fiducial points on each 3D face scan, greatly surpassing the feature-based ones, and it also improves the state of the art of the model-based ones in two aspects: sensitivity to initialization and deficiency in discrimination. The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE databases, and competitive results are achieved in comparison with counterparts in the literature, clearly demonstrating its effectiveness.},
author = {Sun, Jia and Huang, Di and Wang, Yunhong and Chen, Liming},
doi = {10.1145/3282833},
issn = {1551-6857},
journal = {ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}webofscience,tutux9},
number = {1},
title = {{Expression Robust 3D Facial Landmarking via Progressive Coarse-to-Fine Tuning}},
volume = {15},
year = {2019}
}
@inproceedings{ISI:000380533900053,
abstract = {Estimating the traversability of terrain in an unstructured outdoor environment is one of the challenging issues in autonomous vehicles. When dealing with a large 3D point cloud, the computational cost of processing all of the individual points is very high. Thus voxelization methods are used extensively. In this paper, we propose a more fine-grained voxelization algorithm in the context of unstructured terrain classification. While the current shape of a voxel is a fixed-length cubic, we construct a flexible shape voxel which has spatial and geometrical properties. Furthermore, we propose a new shape histogram feature that represents the statistical characteristics of 3D points. The proposed method was tested using data obtained from unstructured outdoor environments for performance evaluation.},
annote = {3rd International Conference on Robot Intelligence Technology and
Applications, Beijing, PEOPLES R CHINA, NOV 06-08, 2014},
author = {Song, Soohwan and Jo, Sungho},
booktitle = {ROBOT INTELLIGENCE TECHNOLOGY ANDAPPLICATIONS 3},
doi = {10.1007/978-3-319-16841-8_53},
editor = {{Kim, JH and Yang, W and Jo, J and Sincak, P and Myung}, H},
isbn = {978-3-319-16841-8; 978-3-319-16840-1},
issn = {2194-5357},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
pages = {595--604},
series = {Advances in Intelligent Systems and Computing},
title = {{Traversability Classification Using Super-voxel Method in Unstructured Terrain}},
volume = {345},
year = {2015}
}
@article{ISI:000440350800018,
abstract = {The temporalis muscle transposition is a reliable, one-stage reanimation technique for longstanding facial paralysis. In the variation described by Rubin, the muscle is released from the temporal bone and folded over the zygomatic arch towards the modiolus. This results in unsightly temporal hollowing and zygomatic bulging. We present a modification of this technique, which preserves the temporal fat pad in its anatomical location as well as conceals temporal hollowing and prevents zygomatic bulging. The data of 23 patients treated with this modification were analysed. May classification was used for evaluation of mouth reanimation. Experts and patients scored visibility of the contour deformity on a 100-mm visual analogue scale (VAS) (score 0 = poor/100 = best). 3D images of the face were used to measure temporal hollowing and zygomatic bulging. 3D images were compared to those of controls with a similar gender and age distribution. After a median follow-up of 5.7 years, all patients achieved symmetry at rest. Eleven patients achieved symmetry while smiling with closed lips (May classification ``Good{\{}''{\}}). A median (interquartile range {\{}[{\}}IQR]) VAS score of 19 (6; 41) was given by experts and 25 (5; 59) by patients themselves. 3D volumes of zygomatic bulging differed from those of control subjects, although all volume differences were small (median {\textless}3.3 ml) and temporal hollowing did not differ significantly. On the basis of our results, we conclude that our modified Rubin temporalis transposition technique provides an elegant way to conceal bulging over the zygomatic arch and prevents temporal hollowing, without the need for fascial extensions to reach the modiolus. (C) 2018 British Association of Plastic, Reconstructive and Aesthetic Surgeons. Published by Elsevier Ltd. All rights reserved.},
author = {van Veen, Martinus M and Korteweg, Steven F S and Dijkstra, Pieter U and Werker, Paul M N},
doi = {10.1016/j.bjps.2018.04.007},
issn = {1748-6815},
journal = {JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {aug},
number = {8},
pages = {1181--1187},
title = {{Keeping the fat on the right spot prevents contour deformity in temporalis muscle transposition}},
volume = {71},
year = {2018}
}
@inproceedings{ISI:000412830800019,
abstract = {Terrestrial lidar is commonly used for detailed documentation in the field of forest inventory investigation. Recent improvements of point cloud processing techniques enabled efficient and precise computation of an individual tree shape parameters, such as breast-height diameter, height, and volume. However, tree species are manually specified by skilled workers to date. Previous works for automatic tree species classification mainly focused on aerial or satellite images, and few works have been reported for classification techniques using ground-based sensor data. Several candidate sensors can be considered for classification, such as RGB or multi/hyper spectral cameras. Above all candidates, we use terrestrial lidar because it can obtain high resolution point cloud in the dark forest. We selected bark texture for the classification criteria, since they clearly represent unique characteristics of each tree and do not change their appearance under seasonable variation and aged deterioration. In this paper, we propose a new method for automatic individual tree species classification based on terrestrial lidar using Convolutional Neural Network (CNN). The key component is the creation step of a depth image which well describe the characteristics of each species from a point cloud. We focus on Japanese cedar and cypress which cover the large part of domestic forest. Our experimental results demonstrate the effectiveness of our proposed method.},
annote = {Conference on Videometrics, Range Imaging, and Applications XIV, Munich,
GERMANY, JUN 26-27, 2017},
author = {Mizoguchi, Tomohiro and Ishii, Akira and Nakamura, Hiroyuki and Inoue, Tsuyoshi and Takamatsu, Hisashi},
booktitle = {VIDEOMETRICS, RANGE IMAGING, AND APPLICATIONS XIV},
doi = {10.1117/12.2270123},
editor = {{Remondino, F and Shortis}, MR},
isbn = {978-1-5106-1110-8; 978-1-5106-1109-2},
issn = {0277-786X},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Lidar-based Individual Tree Species Classification using Convolutional Neural Network}},
volume = {10332},
year = {2017}
}
@inproceedings{ISI:000418371405064,
abstract = {Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration for the synthesis of 3D faces by assuming that faces are in a linear class. However, the human face is in a nonlinear manifold, and a new test are always not in the pre-learnt subspace accurately because of the disparity brought by ethnicity, age, gender, etc. In the paper, we propose a parametric T-spline morphable model (T-splineMM) for 3D face representation, which has great advantages of fitting data from an unknown source accurately. In the model, we describe a face by C-2 T-spline surface, and divide the face surface into several shape units (SUs), according to facial action coding system (FACS), on T-mesh instead of on the surface directly. A fitting algorithm is proposed to optimize coefficients of T-spline control point components along pre-learnt identity and expression subspaces, as well as to optimize the details in refinement progress. As any pre-learnt subspace is not complete to handle the variety and details of faces and expressions, it covers a limited span of morphing. SUs division and detail refinement make the model fitting the facial muscle deformation in a larger span of morphing subspace. We conduct experiments on face scan data, kinect data as well as the space-time data to test the performance of detail fitting, robustness to missing data and noise, and to demonstrate the effectiveness of our model. Convincing results are illustrated to demonstrate the effectiveness of our model compared with the popular methods.},
annote = {30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Honolulu, HI, JUL 21-26, 2017},
author = {Peng, Weilong and Feng, Zhiyong and Xu, Chao and Su, Yong},
booktitle = {30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2017)},
doi = {10.1109/CVPR.2017.585},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
organization = {IEEE; IEEE Comp Soc; CVF},
pages = {5515--5523},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Parametric T-spline Face Morphable Model for Detailed Fitting in Shape Subspace}},
year = {2017}
}
@inproceedings{7167461,
abstract = {Landmark-based morphometric analyses are used by anthropologists, developmental and evolutionary biologists to understand shape and size differences (eg. in the cranioskeleton) between groups of specimens. The standard, laborintensive approach is for researchers to manually place landmarks on 3D image datasets. As landmark recognition is subject to inaccuracies of human perception, digitization of landmark coordinates is typically repeated (often by more than one person) and the mean coordinates are used. In an attempt to improve efficiency and reproducibility between researchers, we have developed an algorithm to locate landmarks on CT mouse hemi-mandible data. The method is evaluated on 3D meshes of 28-day old mice, and results compared to landmarks manually identified by experts. Quantitative shape comparison between two inbred mouse strains demonstrate that data obtained using our algorithm also has enhanced statistical power when compared to data obtained by manual landmarking.},
author = {Aneja, D and Vora, S R and Camci, E D and Shapiro, L G and Cox, T C},
booktitle = {2015 IEEE 28th International Symposium on Computer-Based Medical Systems},
doi = {10.1109/CBMS.2015.86},
issn = {2372-9198},
keywords = {automated 3D landmark,bone,computerised tomography,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}webofscience,tutux9},
month = {jun},
pages = {78--83},
title = {{Automated Detection of 3D Landmarks for the Elimination of Non-Biological Variation in Geometric Morphometric Analyses}},
year = {2015}
}
@article{ISI:000416554100095,
abstract = {Accurate classification of tree-species is essential for sustainably managing forest resources and effectively monitoring species diversity. In this study, we used simultaneously acquired hyperspectral and LiDAR data from LiCHy (Hyperspectral, LiDAR and CCD) airborne system to classify tree-species in subtropical forests of southeast China. First, each individual tree crown was extracted using the LiDAR data by a point cloud segmentation algorithm (PCS) and the sunlit portion of each crown was selected using the hyperspectral data. Second, different suites of hyperspectral and LiDAR metrics were extracted and selected by the indices of Principal Component Analysis (PCA) and the mean decrease in Gini index (MDG) from Random Forest (RF). Finally, both hyperspectral metrics (based on whole crown and sunlit crown) and LiDAR metrics were assessed and used as inputs to Random Forest classifier to discriminate five tree-species at two levels of classification. The results showed that the tree delineation approach (point cloud segmentation algorithm) was suitable for detecting individual tree in this study (overall accuracy = 82.9{\%}). The classification approach provided a relatively high accuracy (overall accuracy {\textgreater} 85.4{\%}) for classifying five tree-species in the study site. The classification using both hyperspectral and LiDAR metrics resulted in higher accuracies than only hyperspectral metrics (the improvement of overall accuracies = 0.4-5.6{\%}). In addition, compared with the classification using whole crown metrics (overall accuracies = 85.4-89.3{\%}), using sunlit crown metrics (overall accuracies = 87.1-91.5{\%}) improved the overall accuracies of 2.3{\%}. The results also suggested that fewer of the most important metrics can be used to classify tree-species effectively (overall accuracies = 85.8-91.0{\%}).},
author = {Shen, Xin and Cao, Lin},
doi = {10.3390/rs9111180},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {nov},
number = {11},
title = {{Tree-Species Classification in Subtropical Forests Using Airborne Hyperspectral and LiDAR Data}},
volume = {9},
year = {2017}
}
@inproceedings{7495382,
abstract = {In this work, we exploit 3D Constrained Local Model (CLM) for facial landmark detection. Our approach integrates the geometric information of 3D face scans. The fast increase demand of 3D data invite to develop 3D image processing methods for many applications and especially for automatic landmark detection. The new step in this paper is the introduction of mesh histogram of gradients (meshHOG) as local descriptors around every landmark location. The proposed work is evaluated on the publicly available Bosphorus database. A comparison with the other descriptors mesh LBP and mesh SIFT are also depicted.},
author = {Rai, M C E and Tortorici, C and Al-Muhairi, H and Safar, H A and Werghi, N},
booktitle = {2016 18th Mediterranean Electrotechnical Conference (MELECON)},
doi = {10.1109/MELCON.2016.7495382},
issn = {2158-8481},
keywords = {computational geometry,face recognition,lerdepois,mesh gener,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {1--5},
title = {{Landmarks detection on 3D face scans using local histogram descriptors}},
year = {2016}
}
@inproceedings{ISI:000384248300039,
abstract = {Our group is developing a method to examine biological specimens in cellular detail using synchrotron microCT. The method can acquire 3D images of tissue at micrometer-scale resolutions, allowing for individual cell types to be visualized in the context of the entire specimen. For model organism research, this tool will enable the rapid characterization of tissue architecture and cellular morphology from every organ system. This characterization is critical for proposed and ongoing ``phenome{\{}''{\}} projects that aim to phenotype whole-organism mutants and diseased tissues from different organisms including humans. With the envisioned collection of hundreds to thousands of images for a phenome project, it is important to develop quantitative image analysis tools for the automated scoring of organism phenotypes across organ systems. Here we present a first step towards that goal, demonstrating the use of support vector machines (SVM) in detecting retinal cell nuclei in 3D images of wild-type zebrafish. In addition, we apply the SVM classifier on a mutant zebrafish to examine whether SVMs can be used to capture phenotypic differences in these images. The long-term goal of this work is to allow cellular and tissue morphology to be characterized quantitatively for many organ systems, at the level of the whole-organism.},
annote = {Conference on Medical Imaging - Digital Pathology, San Diego, CA, MAR
02-03, 2016},
author = {Ding, Yifu and Tavolara, Thomas and Cheng, Keith},
booktitle = {MEDICAL IMAGING 2016: DIGITAL PATHOLOGY},
doi = {10.1117/12.2216940},
editor = {{Gurcan, MN and Madabhushi}, A},
isbn = {978-1-5106-0026-3},
issn = {0277-786X},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
organization = {SPIE; Modus Med Devices Inc; Bruker; Poco Graphite; ImXPAD},
series = {Proceedings of SPIE},
title = {{Automated Detection of Retinal Cell Nuclei in 3D Micro-CT Images of Zebrafish using Support Vector Machine Classification}},
volume = {9791},
year = {2016}
}
@article{ISI:000451673800001,
abstract = {Understanding how people make decisions from risky choices has attracted increasing attention of researchers in economics, psychology and neuroscience. While economists try to evaluate individual's risk preference through mathematical modeling, neuroscientists answer the question by exploring the neural activities of the brain. We propose a model-free method, 3-dimensional image functional principal component analysis (3DIF), to provide a connection between active risk related brain region detection and individual's risk preference. The 3DIF methodology is directly applicable to 3-dimensional image data without artificial vectorization or mapping and simultaneously guarantees the contiguity of risk related brain regions rather than discrete voxels. Simulation study evidences an accurate and reasonable region detection using the 3DIF method. In real data analysis, five important risk related brain regions are detected, including parietal cortex (PC), ventrolateral prefrontal cortex (VLPFC), lateral orbifrontal cortex (IOFC), anterior insula (aINS) and dorsolateral prefrontal cortex (DLPFC), while the alternative methods only identify limited risk related regions. Moreover, the 3DIF method is useful for extraction of subjective specific signature scores that carry explanatory power for individual's risk attitude. In particular, the 3DIF method perfectly classifies both strongly and weakly risk averse subjects for in-sample analysis. In out-of-sample experiment, it achieves 73{\%}-88{\%} overall accuracy, among which 90{\%}-100{\%} strongly risk averse subjects and 49{\%}-71{\%} weakly risk averse subjects are correctly classified with leave-k-out cross validations.},
author = {Chen, Ying and Haerdie, Wolfgang K and He, Qiang and Majer, Piotr},
doi = {10.1515/strm-2017-0011},
issn = {2193-1402},
journal = {STATISTICS {\&} RISK MODELING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jul},
number = {3-4},
pages = {89--110},
title = {{Risk related brain regions detection and individual risk classification with 3D image FPCA}},
volume = {35},
year = {2018}
}
@inproceedings{ISI:000381427400036,
abstract = {Human face recognition based on geometrical structure has been an area of interest among researchers for the past few decades especially in pattern recognition. 3D Face recognition systems are of interest in this context. The main advantage of 3D Face recognition is the availability of geometrical information of the face structure which is more or less unique for a subject. This paper focuses on the problems of person identification using 3D Face data. Use of unregistered 3D Face data for feature extraction significantly increases the operational speed of the system with huge database enrollment. In this work, unregistered Face data, i.e. both texture and depth is fed to a classifier in spectral representations of the same data. 2-D Discrete Contourlet Transform and 2-D Discrete Fourier Transform is used here for the spectral representation which forms the feature matrix. Fusion of texture and depth statistical information of face is proposed in this paper since the individual schemes are of lower performance. Application of statistical method seems to degrade the performance of the system when applied to texture data and was effective in the case of depth data. Fusion of the matching scores proves that the recognition accuracy can be improved significantly by fusion of scores of multiple representations. FRAV3D database is used for testing the algorithm.},
annote = {International Symposium on Intelligent Systems Technologies and
Applications (ISTA), SCMS Grp Inst, SCMS Sch Engn {\&} Technol, Kochi,
INDIA, AUG 10-13, 2015},
author = {Naveen, S and Moni, R S},
booktitle = {INTELLIGENT SYSTEMS TECHNOLOGIES AND APPLICATIONS, VOL 1},
doi = {10.1007/978-3-319-23036-8_36},
editor = {{Berretti, S and Thampi, SM and Srivastava}, PR},
isbn = {978-3-319-23036-8; 978-3-319-23035-1},
issn = {2194-5357},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux5},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux5},
pages = {411--425},
series = {Advances in Intelligent Systems and Computing},
title = {{Contourlet and Fourier Transform Features Based 3D Face Recognition System}},
volume = {384},
year = {2016}
}
@inproceedings{Kempfle:2018:RRE:3266157.3266208,
address = {New York, NY, USA},
author = {Kempfle, Jochen and {Van Laerhoven}, Kristof},
booktitle = {Proceedings of the 5th International Workshop on Sensor-based Activity Recognition and Interaction},
doi = {10.1145/3266157.3266208},
isbn = {978-1-4503-6487-4},
keywords = {Kinect v2,ToF sensing,non-contact measurement,respiration measurement,respiratory rate,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
pages = {4:1----4:10},
publisher = {ACM},
series = {iWOAR '18},
title = {{Respiration Rate Estimation with Depth Cameras: An Evaluation of Parameters}},
url = {http://doi.acm.org/10.1145/3266157.3266208},
year = {2018}
}
@inproceedings{8687254,
abstract = {Various applications have been developed in facial research to aid the recognition of personal attributes, analysis of race, and personal authentication for the security industry and other research fields. Thus, it is possible to identify the differences in facial shape based on place of birth or country. The present study analyzes facial shape using scanned 3D facial images and investigates ways to extract facial landmarks from the 3D facial images. The detection of the facial landmark requires the normalization of the facial scale and position among in the 3D image data to analyze the facial shape. Therefore, it is difficult to obtain accurate facial landmarks from 3D facial images. Our method decomposes the task into the following three parts: (a) conversion of data from the 3D facial image to a 2D image, (b) extraction of facial landmarks from the 3D image using Convolutional Neural Network (CNN), (c) inversion of the identified facial landmarks from 2D to 3D images. In experiments, we compared the accuracy of this model for facial landmark detection.},
author = {Terada, T and Chen, Y and Kimura, R},
booktitle = {2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
doi = {10.1109/FSKD.2018.8687254},
keywords = {3d facial image,component,landmarks detection,lerdepois,poin,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}ieeexplore,tutux9},
pages = {390--393},
title = {{3D Facial Landmark Detection Using Deep Convolutional Neural Networks}},
year = {2018}
}
@inproceedings{Guo:2018:SVF:3240876.3240913,
address = {New York, NY, USA},
author = {Guo, Xingyan and Jin, Yi and Li, Yidong and Xing, Junliang and Lang, Congyan},
booktitle = {Proceedings of the 10th International Conference on Internet Multimedia Computing and Service},
doi = {10.1145/3240876.3240913},
isbn = {978-1-4503-6520-8},
keywords = {3D face model,landmark detection,landmark smoothing,revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,tutux9},
pages = {20:1----20:7},
publisher = {ACM},
series = {ICIMCS '18},
title = {{Stabilizing Video Facial Landmark Detection and Tracking via Global and Local Filtering}},
url = {http://doi.acm.org/10.1145/3240876.3240913},
year = {2018}
}
@article{ISI:000410870200001,
abstract = {Three-dimensional (3D) facial modeling and stereo matching-based methods are widely used for 3D facial reconstruction from 2D single-view and multiple-view images. However, these methods cannot realistically reconstruct 3D faces because they use insufficient numbers of macro-level Facial Feature Points (FFPs). This paper proposes an accurate and person-specific 3D facial reconstruction method that uses ample numbers of macro and micro-level FFPs to enable coverage of all facial regions of high resolution facial images. Comparisons of 3D facial images reconstructed using the proposed method for ground-truth 3D facial images from the Bosphorus 3D database show that the method is superior to a conventional Active Appearance Model-Structure from Motion (AAM + SfM)-based method in terms of average 3D root mean square error between the reconstructed and ground-truth 3D faces. Further, the proposed method achieved outstanding accuracy in local facial regions such as the cheek areas where extraction of FFPs is difficult for existing methods. (C) 2017 Elsevier B.V. All rights reserved.},
author = {Jo, Jaeik and Kim, Hyunjun and Kim, Jaihie},
doi = {10.1016/jimavis.2017.05.001},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
pages = {1--9},
title = {{3D facial shape reconstruction using macro- and micro-level features from high resolution facial images}},
volume = {64},
year = {2017}
}
@inproceedings{7823997,
abstract = {Face is being considered as one of the most commonly used biometric modality. The inaccuracy in two dimensional face recognition systems is mainly due to pose variations, occlusions, illumination etc. Among this, changes in illumination condition do not affect 3D face recognition systems. But pose variation drastically changes the appearance of face images. To solve the problems with depth map and texture images corrupted by head pose variations and the occlusions generated due to these pose variations, a reconstruction method is proposed which consist of three stages. In the first stage, the pose correction is done by Iterative Closest Point (ICP) algorithm and in the second stage the occluded region of the face is reconstructed by a resurfacing method called patch cloning. It is followed by the wrapping of reconstructed depth map by its texture to generate a 3D model. The statistical error between the original face and the reconstructed face is also evaluated. In this work, facial symmetry is used as a prior knowledge. Experiments are done with the FRAV3D database.},
annote = {From Duplicate 1 (3D face reconstruction by pose correction, patch cloning and texture wrapping - Naveen, S; Rugmini, K P; Moni, R S)

cited By 0

From Duplicate 2 (3D face reconstruction by pose correction, patch cloning and texture wrapping - Naveen, S; Rugmini, K P; Moni, R S)

26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)

From Duplicate 3 (3D face reconstruction by pose correction, patch cloning and texture wrapping - Naveen, S; Rugmini, K P; Moni, R S)

From Duplicate 1 (3D face reconstruction by pose correction, patch cloning and texture wrapping - Naveen, S; Rugmini, K P; Moni, R S)

cited By 0

From Duplicate 2 (3D face reconstruction by pose correction, patch cloning and texture wrapping - Naveen, S; Rugmini, K P; Moni, R S)

26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Naveen, S and Rugmini, K P and Moni, R S},
booktitle = {2016 International Conference on Communication Systems and Networks (ComNet)},
doi = {10.1109/CSN.2016.7823997},
isbn = {978-1-5090-3349-2},
keywords = {biometrics (access control),estela,etapa1,face recognition,id308,ieeexplore,image,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
mendeley-tags = {estela,etapa1,id308,ieeexplore,revisao{\_}V1,revisao{\_}V2,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutux9},
month = {jul},
pages = {112--116},
publisher = {IEEE},
title = {{3D face reconstruction by pose correction, patch cloning and texture wrapping}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014911379{\&}doi=10.1109{\%}2FCSN.2016.7823997{\&}partnerID=40{\&}md5=d83e42a8548567d921eea89900252c10 http://ieeexplore.ieee.org/document/7823997/},
year = {2016}
}
@article{ISI:000455637600091,
abstract = {Full waveform (FW) LiDAR holds great potential for retrieving vegetation structure parameters at a high level of detail, but this prospect is constrained by practical factors such as the lack of available handy processing tools and the technical intricacy of waveform processing. This study introduces a new product named the Hyper Point Cloud (HPC), derived from FW LiDAR data, and explores its potential applications, such as tree crown delineation using the HPC-based intensity and percentile height (PH) surfaces, which shows promise as a solution to the constraints of using FW LiDAR data. The results of the HPC present a new direction for handling FW LiDAR data and offer prospects for studying the mid-story and understory of vegetation with high point density (similar to 182 points/m(2)). The intensity-derived digital surface model (DSM) generated from the HPC shows that the ground region has higher maximum intensity (MAXI) and mean intensity (MI) than the vegetation region, while having lower total intensity (TI) and number of intensities (NI) at a given grid cell. Our analysis of intensity distribution contours at the individual tree level exhibit similar patterns, indicating that the MAXI and MI decrease from the tree crown center to the tree boundary, while a rising trend is observed for TI and NI. These intensity variable contours provide a theoretical justification for using HPC-based intensity surfaces to segment tree crowns and exploit their potential for extracting tree attributes. The HPC-based intensity surfaces and the HPC-based PH Canopy Height Models (CHM) demonstrate promising tree segmentation results comparable to the LiDAR-derived CHM for estimating tree attributes such as tree locations, crown widths and tree heights. We envision that products such as the HPC and the HPC-based intensity and height surfaces introduced in this study can open new perspectives for the use of FW LiDAR data and alleviate the technical barrier of exploring FW LiDAR data for detailed vegetation structure characterization.},
author = {Zhou, Tan and Popescu, Sorin and Malambo, Lonesome and Zhao, Kaiguang and Krause, Keith},
doi = {10.3390/rs10121949},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {dec},
number = {12},
title = {{From LiDAR Waveforms to Hyper Point Clouds: A Novel Data Product to Characterize Vegetation Structure}},
volume = {10},
year = {2018}
}
@inproceedings{8453724,
abstract = {This paper proposes a new approach for the detection of tree locations around forest machines producing a situational model based on on-site terrestrial LiDAR data collected during harvesting operation. A triangularized ground model is used to planarize the point cloud in order to simplify the tree detection. The planarized ground makes the vertical cutting of the point cloud systematical. Tree stem lines detected from individual trees at individual scan views are used to guide the final alignment into global coordinates. The setup is numerically efficient and does not rely on any positioning and orientation system (POS) based e.g. on an inertial measurement unit (IMU) or global navigation satellite system (GNSS) or wheel rotation counter on the autonomous vehicle.},
author = {Sihvo, S and Virjonen, P and Nevalainen, P and Heikkonen, J},
booktitle = {2018 Baltic Geodetic Congress (BGC Geomatics)},
doi = {10.1109/BGC-Geomatics.2018.00075},
keywords = {optical radar,revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,revisao{\_}scopus,satellite navigation,tree detection,tutux9},
mendeley-tags = {revisao{\_}V1,revisao{\_}etapa1,revisao{\_}ieeexplore,revisao{\_}scopus,tutux9},
pages = {364--367},
title = {{Tree Detection around Forest Harvester Based on Onboard LiDAR Measurements}},
year = {2018}
}
@inproceedings{ISI:000364991200038,
abstract = {We propose a method for extracting fiducial points from human faces that uses 3D information only and is based on two key steps: multi-scale curvature analysis, and the reliable tracking of features in a scale-space based on curvature. Our scale-space analysis, coupled to careful use of prior information based on variability boundaries of anthropometric facial proportions, does not require a training step, because it makes direct use of morphological characteristics of the analyzed surface. The proposed method precisely identifies important fiducial points and is able to extract new fiducial points that were previously unrecognized, thus paving the way to more effective recognition algorithms.},
annote = {18th International Conference on Image Analysis and Processing (ICIAP),
Genoa, ITALY, SEP 07-11, 2015},
author = {{De Giorgis}, Nikolas and Rocca, Luigi and Puppo, Enrico},
booktitle = {IMAGE ANALYSIS AND PROCESSING - ICIAP 2015, PT I},
doi = {10.1007/978-3-319-23231-7_38},
editor = {{Murino, V and Puppo}, E},
isbn = {978-3-319-23231-7; 978-3-319-23230-0},
issn = {0302-9743},
keywords = {lerdepois,revisao{\_}V1,revisao{\_}webofscience,tutux9},
mendeley-tags = {lerdepois,revisao{\_}V1,revisao{\_}webofscience,tutux9},
organization = {Datalogic; Google Inc; Centro Studi Gruppo Orizzonti Holding; Ansaldo Energia; EBIT Esaote; Softeco; eVS embedded Vis Syst S r l; 3DFlow S r l; Camelot Biomed Syst S r l; Ist Italiano Tecnologia, Pattern Anal {\&} Comp Vis Dept; Univ Genova; Univ Verona; Cam},
pages = {421--431},
series = {Lecture Notes in Computer Science},
title = {{Scale-Space Techniques for Fiducial Points Extraction from 3D Faces}},
volume = {9279},
year = {2015}
}
@article{ISI:000418312200002,
abstract = {Distinguishing tree species is relevant in many contexts of remote sensing assisted forest inventory. Accurate tree species maps support management and conservation planning, pest and disease control and biomass estimation. This study evaluated the performance of applying ensemble techniques with the goal of automatically distinguishing Pinus sylvestris L. and Pinus uncinata Mill. Ex Mirb within a 1.3 km(2) mountainous area in Barcelonnette (France). Three modelling schemes were examined, based on: (1) high-density LiDAR data (160 returns m(-2)), (2) Worldview-2 multispectral imagery, and (3) Worldview-2 and LiDAR in combination. Variables related to the crown structure and height of individual trees were extracted from the normalized LiDAR point cloud at individual-tree level, after performing individual tree crown (ITC) delineation. Vegetation indices and the Haralick texture indices were derived from Worldview-2 images and served as independent spectral variables. Selection of the best predictor subset was done after a comparison of three variable selection procedures: (1) Random Forests with cross validation (AUCREcv), (2) Akaike Information Criterion (AIC) and (3) Bayesian Information Criterion (BIC). To classify the species, 9 regression techniques were combined using ensemble models. Predictions were evaluated using cross validation and an independent dataset. Integration of datasets and models improved individual tree species classification (True Skills Statistic, TSS; from 0.67 to 0.81) over individual techniques and maintained strong predictive power (Relative Operating Characteristic, ROC = 0.91). Assemblage of regression models and integration of the datasets provided more reliable species distribution maps and associated tree-scale mapping uncertainties. Our study highlights the potential of model and data assemblage at improving species classifications needed in present-day forest planning and management.},
author = {Kukunda, Collins B and Duque-Lazo, Joaquin and Gonzalez-Ferreiro, Eduardo and Thaden, Hauke and Kleinn, Christoph},
doi = {10.1016/j.jag.2017.09.016},
issn = {0303-2434},
journal = {INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {mar},
pages = {12--23},
title = {{Ensemble classification of individual Pinus crowns from multispectral satellite imagery and airborne LiDAR}},
volume = {65},
year = {2018}
}
@article{ISI:000386995100031,
abstract = {The plant breeding company Strube, in cooperation with the German Fraunhofer Institute for non-destructive testing, has developed an automated high-throughput germination test for sugar beet seeds. The phenoTest permits objective measurement and classification of germinating seeds and resulting seedlings. It is therefore more accurate and provides more information than the conventional ISTA (International Seed Testing Association)-germination test, which relies purely on visual assessment and classification into the categories ``normal{\{}''{\}} or ``abnormal{\{}''{\}}. This differentiation is difficult to standardise, and is, to a significant degree, subjective. The phenoTest is based on three-dimensional (3D) X-ray images. Repeated tests of the same plants enable an objective assessment of seedling development over the course of time (4D phenotyping). The individual organs of each plant (radicle, hypocotyl and cotyledons) are automatically identified and measured. The method provides detailed information on germinating capacity and vigour, as well as the homogeneity of a seed lot. Results are documented as measurement values and 3D-images of each individual plant at different time points. The data is used to compare seed lots concerning their natural germination capacity and especially vigour, the processing or priming technologies they experienced, the pelleting and seed treatment applied etc. in order to predict their field emergence potential even under difficult growing conditions. These analyses also helps to optimise all these processes in commercial seed production to obtain a quick, homogeneous and complete field emergence, making full use of the genetic yield potential of sugar beet.},
author = {Wolff, Antje and Gotz, Yvonne},
issn = {0020-8841},
journal = {INTERNATIONAL SUGAR JOURNAL},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {nov},
number = {1415},
pages = {836--839},
title = {{4D phenotyping of germinating seeds and seedlings as a tool to objectively measure seed quality and improve field establishment and yield of sugar beets}},
volume = {118},
year = {2016}
}
@inproceedings{ISI:000390287300035,
abstract = {Real time user independent facial expression recognition is important for virtual agents but challenging. However, since in real time recognition users are not necessarily presenting all the emotions, some proposed methods are not applicable. In this paper, we present a new approach that instead of using the traditional base face normalization on whole face shapes, performs normalization on the point cloud of each landmark. The result shows that our method outperforms the other two when the user input does not contain all six universal emotions.},
annote = {16th International Conference on Intelligent Virtual Agents (IVA), Los
Angeles, CA, SEP 20-23, 2016},
author = {Pan, Zhenglin and Polceanu, Mihai and Lisetti, Christine},
booktitle = {INTELLIGENT VIRTUAL AGENTS, IVA 2016},
doi = {10.1007/978-3-319-47665-0_35},
editor = {{Traum, D and Swartout, W and Khooshabeh, P and Kopp, S and Scherer, S and Leuski}, A},
isbn = {978-3-319-47665-0; 978-3-319-47664-3},
issn = {0302-9743},
keywords = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
mendeley-tags = {revisao{\_}V1,revisao{\_}webofscience,tutux7},
organization = {Alelo; Springer; Univ So Calif, Inst Creat Technologies},
pages = {369--372},
series = {Lecture Notes in Artificial Intelligence},
title = {{On Constrained Local Model Feature Normalization for Facial Expression Recognition}},
volume = {10011},
year = {2016}
}
@inproceedings{ISI:000354613300002,
abstract = {Face recognition based on machine vision has achieved great advances and been widely used in the various fields. However, there are some challenges on the face recognition, such as facial pose, variations in illumination, and facial expression. So, this paper gives the recent advances in 3D face recognition. 3D face recognition approaches are categorized into four groups: minutiae approach, space transform approach, geometric features approach, model approach. Several typical approaches are compared in detail, including feature extraction, recognition algorithm, and the performance of the algorithm. Finally, this paper summarized the challenge existing in 3D face recognition and the future trend. This paper aims to help the researches majoring on face recognition.},
annote = {6th International Conference on Graphic and Image Processing (ICGIP),
Beijing, PEOPLES R CHINA, OCT 24-26, 2014},
author = {Luo, Jing and Geng, Shu Ze and Xiao, Zhao Xia and Xiu, Chun Bo},
booktitle = {SIXTH INTERNATIONAL CONFERENCE ON GRAPHIC AND IMAGE PROCESSING (ICGIP 2014)},
doi = {10.1117/12.2178750},
editor = {{Wang, Y and Jiang, X and Zhang}, D},
isbn = {978-1-62841-558-2},
issn = {0277-786X},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux8},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutux8},
organization = {Int Assoc Comp Sci {\&} Informat Technol; Wuhan Univ},
series = {Proceedings of SPIE},
title = {{A Review of recent advances in 3D Face Recognition}},
volume = {9443},
year = {2015}
}
@article{ISI:000445436100006,
abstract = {The aim of the following paper was to present the geomatic process of transforming low-level aerial imagery obtained with unmanned aerial vehicles (UAV) into a digital terrain model (DTM) and implementing the model into a virtual reality system (VR). The object of the study was a natural aggretage heap of an irregular shape and denivelations up to 11 m. Based on the obtained photos, three point clouds (varying in the level of detail) were generated for the 20,000-m(2)-area. For further analyses, the researchers selected the point cloud with the best ratio of accuracy to output file size. This choice was made based on seven control points of the heap surveyed in the field and the corresponding points in the generated 3D model. The obtained several-centimetre differences between the control points in the field and the ones from the model might testify to the usefulness of the described algorithm for creating large-scale DTMs for engineering purposes. Finally, the chosen model was implemented into the VR system, which enables the most lifelike exploration of 3D terrain plasticity in real time, thanks to the first person view mode (FPV). In this mode, the user observes an object with the aid of a Head- mounted display (HMD), experiencing the geovisualisation from the inside, and virtually analysing the terrain as a direct animator of the observations.},
author = {Halik, Lukasz and Smaczynski, Maciej},
doi = {10.1007/s00024-017-1755-z},
issn = {0033-4553},
journal = {PURE AND APPLIED GEOPHYSICS},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {sep},
number = {9},
pages = {3209--3221},
title = {{Geovisualisation of Relief in a Virtual Reality System on the Basis of Low-Level Aerial Imagery}},
volume = {175},
year = {2018}
}
@article{ISI:000381843600009,
abstract = {The utilization of radiological imaging methods in anthropometric studies is being expanded by the application of modern imaging methods, leading to a decrease in costs, a decrease in the time required for analysis and the ability to create three-dimensional images. This retrospective study investigated 400 patients within the 18-45-years age group (mean age: 30.7 +/- 11.2 years) using cranial computed tomography images. We measured 14 anthropometric parameters (basion-bregma height, basion-prosthion length, maximum cranial length and cranial base lengths, maximum cranial breadth, bizygomatic diameter, upper facial breadth, bimastoid diameter, orbital breadth, orbital length, biorbital breadth, interorbital breadth, foramen magnum breadth and foramen magnum length) of cranial measurements. The intra- and inter-observer repeatability and consistency were good. From the results of logistic regression analysis using morphometric measurements, the most conspicuous measurements in terms of dimorphism were maximum cranial length, bizygomatic diameter, basion-bregma height, and cranial base length. The most dimorphic structure was the bizygomatic diameter with an accuracy rate of 83{\%} in females and 77{\%} in males. In this study, 87.5{\%} of females and 87.0{\%} of males were classified accurately by this model including four parameters with a sensitivity of 91.5{\%} and specificity of 85.0{\%}. In conclusion, CT cranial morphometric analysis may be reliable for the assessment of sex in the Turkish population and is recommended for comparison of data of modern populations with those of former populations. Additionally, cranial morphometric data that we obtained from modern Turkish population may reveal population specific data, which may help current criminal investigations and identification of disaster victims. (C) 2016 Elsevier Ireland Ltd. All rights reserved.},
author = {Ekizoglu, Oguzhan and Hocaoglu, Elif and Inci, Ercan and Can, Ismail Ozgur and Solmaz, Dilek and Aksoy, Sema and Buran, Cudi Ferat and Sayin, Ibrahim},
doi = {10.1016/j.legalmed.2016.06.001},
issn = {1344-6223},
journal = {LEGAL MEDICINE},
keywords = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
mendeley-tags = {revisao{\_}V2,revisao{\_}webofscience,tutux9},
month = {jul},
pages = {45--52},
title = {{Assessment of sex in a modern Turkish population using cranial anthropometric parameters}},
volume = {21},
year = {2016}
}
@inproceedings{ISI:000460471100001,
abstract = {This paper presents an efficient 3D face recognition method to handle facial expression. The proposed method uses the Surfaces Empirical Mode Decomposition (SEMD), facial curves and local shape descriptor in a matching process to overcome the distortions caused by expressions in faces. The basic idea is that, the face is presented at different scales by SEMD. Then the isometric-invariant features on each scale are extracted. After that, the geometric information is obtained on the 3D surface in terms of radial and level facial curves. Finally, the feature vectors on each scale are associated with their corresponding geometric information. The presented method is validated on GavabDB database resulting a rank 1 recognition rate (RR) of 98.9{\%} for all faces with neutral and non-neutral expressions. This result outperforms other 3D expression-invariant face recognition methods on the same database.},
annote = {2nd Mediterranean Conference on Pattern Recognition and Artificial
Intelligence (MedPRAI), Ibn Tofail Univ, Rabat, MOROCCO, MAR 27-28, 2018},
author = {Abbad, Abdelghafour and Abbad, Khalid and Tairi, Hamid},
booktitle = {PROCEEDINGS OF THE 2ND MEDITERRANEAN CONFERENCE ON PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE (MEDPRAI-2018)},
doi = {10.1145/3177148.3180087},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abbad, Abbad, Tairi - 2018 - 3D face recognition in the presence of facial expressions based on empirical mode decomposition.pdf:pdf},
isbn = {978-1-4503-5290-1},
keywords = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
organization = {Bahria Univ; Univ Larbi Tebessi Tebessa; OCP; ENSIAS; Int Assoc Pattern Recognit},
pages = {1--6},
title = {{3D face recognition in the presence of facial expressions based on empirical mode decomposition}},
year = {2018}
}
@inproceedings{8578301,
abstract = {Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10{\%}. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem.},
author = {{Zulqarnain Gilani}, Syed and Mian, Ajmal},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00203},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zulqarnain Gilani, Mian - 2018 - Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
keywords = {convolutional neural nets,face recognition,learnin,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,tutui1},
pages = {1896--1905},
title = {{Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition}},
year = {2018}
}
@article{ISI:000449193300001,
abstract = {3D face recognition is an important topic in the field of pattern recognition and computer graphic. We propose a novel approach for 3D face recognition using local conformal parameterization and iso-geodesic stripes. In our framework, the 3D facial surface is considered as a Riemannian 2-manifold. The surface is mapped into the 2D circle parameter domain using local conformal parameterization. In the parameter domain, the geometric features are extracted from the iso-geodesic stripes. Combining the relative position measure, Chain 2D Weighted Walkthroughs (C2DWW), the 3D face matching results can be obtained. The geometric features from iso-geodesic stripes in parameter domain are robust in terms of head poses, facial expressions, and some occlusions. In the experiments, our method achieves a high recognition accuracy of 3D facial data from the Texas3D and Bosphorus3D face database.},
author = {Lv, Chenlei and Zhao, Junli},
doi = {10.1155/2018/4707954},
file = {:home/tutu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lv, Zhao - 2018 - 3D Face Recognition based on Local Conformal Parameterization and Iso-Geodesic Stripes Analysis.pdf:pdf},
issn = {1024-123X},
journal = {Mathematical Problems in Engineering},
keywords = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
pages = {1--10},
title = {{3D Face Recognition based on Local Conformal Parameterization and Iso-Geodesic Stripes Analysis}},
volume = {2018},
year = {2018}
}
@inproceedings{8314888,
abstract = {This manuscript introduces a novel 3D face authentication system inspired from the advantageous capacities of Gabor-Edge filters. The approach studies 3D face difficulties such as expression variety, different rotations and exposure to illuminations. The proposed systems starts by preprocessing the 3D face images to resolve acquisition problems. Then, a filtering process is performed by implanting our 3D Gabor-Edge technique extended based on the classic 3D Gabor masks. The next step is to achieve the classification of facial features from the edge saliency by the artificial Neural Network Classifier (NNC). The evaluation of the adopted system is achieved by exporting common datasets from GavabDB database. Experimental results are reported to prove the high accuracy rates of our method compared to the recent researches in the same biometric field.},
annote = {05/06/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
05/06/2018 Exclu{\'{i}}do (etapa 1)},
author = {Torkhani, Ghada and Ladgham, Anis and Sakly, Anis},
booktitle = {2017 18th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering, STA 2017 - Proceedings},
doi = {10.1109/STA.2017.8314888},
file = {:home/tutu/artigos{\_}revisao/08314888.pdf:pdf},
isbn = {9781538610848},
keywords = {3D images,Face authentication,Gabor filtering,Saliency,estela,etapa1,fatima,id511,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
mendeley-tags = {estela,etapa1,fatima,id511,ieeexplore,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}webofscience,tutui1},
pages = {578--582},
title = {{3D gabor-edge filters applied to face depth images}},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Ganguly2016275,
abstract = {Face and facial attributes represent meaningful definition about a variety of information to discriminate an individual from others and for developing a computational model for automatic face recognition purpose. However, in this work, selection of relevant features from newly created face space is the pivotal contribution of the authors. Here, authors have demonstrated a new face space 'Complement Component' that have been used to extract the four basic components along X, and Y axes in four directions. Later, authors have experimented the discriminative attributes from these face spaces for recognition purpose. Here, comparison of the proposed method has been reported by examining its success on two well accepted 3D face databases, namely: Frav3D and Texas3D. In case of 2D face images, it does not contain depth like information i.e. Z-values in X-Y plane through intensity values. Therefore, it has not been undertaken during this investigation.},
annote = {cited By 1},
author = {Ganguly, Suranjan and Bhattachaijee, Debotosh and Nasipuri, Mita},
booktitle = {2015 IEEE International Conference on Computer Graphics, Vision and Information Security, CGVIS 2015},
doi = {10.1109/CGVIS.2015.7449936},
file = {:home/tutu/artigos{\_}revisao/07449936.pdf:pdf},
isbn = {9781467374378},
keywords = {3D face image,Complement Component,Face recognition,range face image,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
pages = {275--278},
title = {{3D face recognition from complement component range face images}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966632732{\&}doi=10.1109{\%}2FCGVIS.2015.7449936{\&}partnerID=40{\&}md5=ac5698f1d98fa8b29941ffc837f84a7b},
year = {2016}
}
@inproceedings{ISI:000390841700083,
abstract = {We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate.},
annote = {2nd International Conference on Advanced Technologies for Signal and
Image Processing (ATSIP), Monastir, TUNISIA, MAR 21-23, 2016},
author = {Torkhani, Ghada and Ladgham, Anis and Mansouri, Mohamed Nejib and Sakly, Anis},
booktitle = {2nd International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2016},
doi = {10.1109/ATSIP.2016.7523133},
file = {:home/tutu/artigos{\_}revisao/07523133.pdf:pdf},
isbn = {9781467385268},
keywords = {3D face recognition,Gabor wavelet,SVM,deformed mesh model,facial curvatures,fatima,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,salient points,tutui1},
mendeley-tags = {fatima,revisao{\_}V1,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,tutui1},
month = {mar},
organization = {IEEE; IEEE Tunisia Sect; ATMS Lab; ATSI; IEEE Explore; EMB; ENIS Sch; Telecom Paris; Supelec; CESBIO; Telecom SudParis; ENIT; Univ Paris Sud; IEEE Signal Proc Soc Tunisia Chapter; ISAAM Inst; Minist Higher Educ Res; IEEE EMP Tunisia Chapter; Novartis Comp},
pages = {447--452},
publisher = {IEEE},
title = {{Gabor-SVM applied to 3D-2D deformed mesh model}},
url = {http://ieeexplore.ieee.org/document/7523133/},
year = {2016}
}
@inproceedings{Ahdid201873,
abstract = {In this paper, we present an automatic 3D face recognition system. This system is based on the representation of human faces surfaces as collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two facial surfaces, we compute a geodesic distance between a pair of facial curves using a Riemannian geometry. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008).},
annote = {cited By 0},
author = {Ahdid, Rachid and Taifi, Khaddouj and Said, Said and Fakir, Mohamed and Manaut, Bouzid},
booktitle = {Proceedings - 2017 14th International Conference on Computer Graphics, Imaging and Visualization, CGiV 2017},
doi = {10.1109/CGiV.2017.25},
file = {:home/tutu/artigos{\_}revisao/08361546.pdf:pdf},
isbn = {9781538608524},
keywords = {3D face recognition,Riemannian geometry,facial surfaces,geodesic distance,iso-geodesic curves,revisao{\_}V1,revisao{\_}scopus,tutui1},
mendeley-tags = {revisao{\_}V1,revisao{\_}scopus,tutui1},
pages = {73--78},
title = {{Automatic face recognition system using iso-geodesic curves in riemanian manifold}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048323921{\&}doi=10.1109{\%}2FCGiV.2017.25{\&}partnerID=40{\&}md5=73b9e65d367cee00cdcb28e7b5ef55cb},
year = {2018}
}
