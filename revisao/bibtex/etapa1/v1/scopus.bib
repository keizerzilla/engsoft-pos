
@ARTICLE{Fu201974,
author={Fu, Y. and Ruan, Q. and Luo, Z. and Jin, Y. and An, G. and Wan, J.},
title={FERLrTc: 2D+3D facial expression recognition via low-rank tensor completion},
journal={Signal Processing},
year={2019},
volume={161},
pages={74-88},
doi={10.1016/j.sigpro.2019.03.015},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063351781&doi=10.1016%2fj.sigpro.2019.03.015&partnerID=40&md5=3336ddf68e4404bf074c46d33c559e5d},
affiliation={Institute of Information Science, Beijing Jiaotong University, Beijing, 100044, China; School of Computer Science & Engineering, Shijiazhuang University, Shijiazhuang, 050035, China; Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, 100044, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, 100044, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China},
abstract={In this paper, a 4D tensor model is firstly constructed to explore efficient structural information and correlations from multi-modal data (both 2D and 3D face data). As the dimensionality of the generated 4D tensor is high, a tensor dimensionality reduction technique is in need. Since many real-world high-order data often reside in a low dimensional subspace, Tucker decomposition as a powerful technique is utilized to capture multilinear low-rank structure and to extract useful information from the generated 4D tensor data. Our goal is to use Tucker decomposition to obtain a set of core tensors with smaller sizes and factor matrices which are projected into the 4D tensor data for classification prediction. To characterize the involved similarities of the 4D tensor, the low-rank and sparse representation is built in terms of the low-rank structure of factor matrices and the sparsity of the core tensor in the Tucker decomposition of the generated 4D tensor. A tensor completion (TC) framework is embedded to recover the missing information in the 4D tensor modeling process. Thus, a novel tensor dimensionality reduction approach for 2D+3D facial expression recognition via low-rank tensor completion (FERLrTC) is proposed to solve the factor matrices in a majorization–minimization manner by using a rank reduction strategy. Numerical experiments are conducted with a full implementation on the BU-3DFE and Bosphorus databases and synthetic data to illustrate the effectiveness of the proposed approach. © 2019 Elsevier B.V.},
author_keywords={2D+3D Facial expression recognition;  Multi-modality;  Tensor completion;  Tensor low-rank representation;  Tensor Tucker decomposition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2019164,
author={Zhang, J. and Fisher, R.B.},
title={3D Visual passcode: Speech-driven 3D facial dynamics for behaviometrics},
journal={Signal Processing},
year={2019},
volume={160},
pages={164-177},
doi={10.1016/j.sigpro.2019.02.025},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062151576&doi=10.1016%2fj.sigpro.2019.02.025&partnerID=40&md5=fd48929e8e498db458e84db847056125},
affiliation={School of Computer and Information Engineering, Beijing Technology and Business University, China; Beihang University, China; School of Informatics, the University of Edinburgh, United Kingdom},
abstract={Face biometrics have achieved remarkable performance over the past decades, but unexpected spoofing of the static faces poses a threat to information security. There is an increasing demand for stable and discriminative biological modalities which are hard to be mimicked and deceived. Speech-driven 3D facial motion is a distinctive and measurable behavior-signature that is promising for biometrics. In this paper, we propose a novel 3D behaviometrics framework based on a “3D visual passcode” derived from speech-driven 3D facial dynamics. The 3D facial dynamics are jointly represented by 3D-keypoint-based measurements and 3D shape patch features, extracted from both static and speech-driven dynamic regions. An ensemble of subject-specific classifiers are then trained over selected discriminative features, which allows for a discriminant speech-driven 3D facial dynamics representation. We construct the first publicly available Speech-driven 3D Facial Motion dataset (S3DFM) that includes 2D-3D face video plus audio samples from 77 participants. The experimental results on the S3DFM show that the proposed pipeline achieves a face identification rate of 96.1%. Detailed discussions are presented, concerning anti-spoofing, head pose variation, video frame rate, and applicability cases. We also give comparison with other baselines on “deep” and “shallow” 2D face features. © 2019 Elsevier B.V.},
author_keywords={3D Face database;  Behaviometrics;  Dynamic face recognition;  Lip motion;  Speech-driven},
document_type={Article},
source={Scopus},
}

@ARTICLE{Soltanpour20193020,
author={Soltanpour, S. and Wu, Q.M.J.},
title={Weighted Extreme Sparse Classifier and Local Derivative Pattern for 3D Face Recognition},
journal={IEEE Transactions on Image Processing},
year={2019},
volume={28},
number={6},
pages={3020-3033},
doi={10.1109/TIP.2019.2893524},
art_number={8618459},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064447162&doi=10.1109%2fTIP.2019.2893524&partnerID=40&md5=bec44ad02f8394d9e55c96f5107221ba},
affiliation={Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON  N9B 3P4, Canada},
abstract={A novel weighted hybrid classifier and a high-order, local normal derivative pattern descriptor are proposed for 3D face recognition. The local derivative pattern (LDP) captures the detailed information based on the local derivative variation in different directions. The LDP is computed on three normal maps in x -, y -, and z -directions and on different scales. The surface normal captures the orientation of a surface at each point of 3D data. More informative local shape information is extracted using the surface normal, as compared to depth. The n th-order LDP on the surface normal is proposed to encode the more detailed features from the (n-1) th-order's local derivative direction variations. An extreme learning machine (ELM)-based autoencoder, using a multilayer network structure, is employed to select more discriminant features and to provide a faster training speed. A weighted hybrid framework is proposed to handle facial challenges using a combination of the ELM and the sparse representation classifier (SRC). The advantage of speed for the ELM and the accuracy for the SRC in a weighted scheme is used to enhance the performance of the recognition system. Experimental results regarding four famous 3D face databases illustrate the generalization and effectiveness of the proposed method in terms of both computational cost and recognition accuracy. © 1992-2012 IEEE.},
author_keywords={extreme learning machine;  Face recognition;  local derivative pattern;  sparse representation;  surface normals;  weighted classifier},
document_type={Article},
source={Scopus},
}

@ARTICLE{Patruno201977,
author={Patruno, C. and Marani, R. and Cicirelli, G. and Stella, E. and D'Orazio, T.},
title={People re-identification using skeleton standard posture and color descriptors from RGB-D data},
journal={Pattern Recognition},
year={2019},
volume={89},
pages={77-90},
doi={10.1016/j.patcog.2019.01.003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059593874&doi=10.1016%2fj.patcog.2019.01.003&partnerID=40&md5=a4e3a90a16f731889cec7d2c7778921f},
affiliation={Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing, CNR, Via Amendola 122 D/O, Bari, Italy},
abstract={This paper tackles the problem of people re-identification by using soft biometrics features. The method works on RGB-D data (color point clouds) to determine the best matching among a database of possible users. For each subject under testing, skeletal information in three-dimensions is used to regularize the pose and to create a skeleton standard posture (SSP). A partition grid, whose sizes depend on the SSP, groups the samples of the point cloud accordingly to their position. Every group is then studied to build the person signature. The same grid is then used for the other subjects of the database to preserve information about possible shape differences among users. The effectiveness of this novel method has been tested on three public datasets. Numerical experiments demonstrate an improvement of results with reference to the current state-of-the-art, with recognition rates of 97.84% (on a partition of BIWI RGBD-ID), 61.97% (KinectREID) and 89.71% (RGBD-ID), respectively. © 2019 Elsevier Ltd},
author_keywords={Color point cloud;  Color-based descriptor;  Partition grid;  People re-identification;  RGB-D sensor;  Skeleton standard posture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rasouli20191999,
author={Rasouli, M.S.D. and Payandeh, S.},
title={A novel depth image analysis for sleep posture estimation},
journal={Journal of Ambient Intelligence and Humanized Computing},
year={2019},
volume={10},
number={5},
pages={1999-2014},
doi={10.1007/s12652-018-0796-1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049595359&doi=10.1007%2fs12652-018-0796-1&partnerID=40&md5=46f7eaf4911a2e1eb32905941d1861d9},
affiliation={Networked Robotics and Sensing Laboratory, School of Engineering Science, Simon Fraser University, Burnaby, Canada},
abstract={Recognition of sleep posture and its changes are related to information monitoring in a number of health-related applications such as apnea prevention and elderly care. This paper uses a less privacy-invading approach to classify sleep postures of a person in various configurations including side and supine postures. In order to accomplish this, a single depth sensor has been utilized to collect selective depth signals and populated a dataset associated with the depth data. The data is then analyzed by a novel frequency-based feature selection approach. These extracted features were then correlated in order to rank their information content in various 2D scans from the 3D point cloud in order to train a support vector machine (SVM). The data of subjects are collected under two conditions. First when they were covered with a thin blanket and second without any blanket. In order to reduce the dimensionality of the feature space, a T-test approach is employed to determine the most dominant set of features in the frequency domain. The proposed recognition approach based on the frequency domain is also compared with an approach using feature vector defined based on skeleton joints. The comparative studies are performed given various scenarios and by a variety of datasets. Through our study, it is shown that our proposed method offers better performance to that of the joint-based method. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Depth data;  Human posture estimation;  Machine learning;  Sleep posture estimation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pala201969,
author={Pala, P. and Seidenari, L. and Berretti, S. and Del Bimbo, A.},
title={Enhanced skeleton and face 3D data for person re-identification from depth cameras},
journal={Computers and Graphics (Pergamon)},
year={2019},
volume={79},
pages={69-80},
doi={10.1016/j.cag.2019.01.003},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061087860&doi=10.1016%2fj.cag.2019.01.003&partnerID=40&md5=afb907b270f0a71f88a9deb9cc6b6907},
affiliation={Media Integration and Communication Center, viale Morgagni 65, Florence, 50134, Italy},
abstract={Person re-identification is typically performed using 2D still images or videos, where photometric appearance is the main visual cue used to discover the presence of a target subject when switching from different camera views across time. This invalidates any application where a person may change dress across subsequent acquisitions as can be the case of patients monitoring at home. Differently from RGB data, 3D information as acquired by depth cameras can open the way to person re-identification based on biometric cues such as distinguishing traits of the body or face. However, the accuracy of skeleton and face geometry extracted from depth data is not always adequate to enable person recognition, since both these features are affected by the pose of the subject and the distance from the camera. In this paper, we propose a method to derive a robust skeleton representation from a depth sequence and to complement it with a highly discriminative face feature. This is obtained by selecting skeleton and face samples based on their quality and using the temporal redundancy across the sequence to derive and refine cumulated models for both of them. Extracting skeleton and face features from such cumulated models and combining them for the recognition allow us to improve rank-1 re-identification accuracy compared to individual cues. A comparative evaluation on three benchmark datasets also shows results at the state-of-the-art. © 2019 Elsevier Ltd},
author_keywords={3D face;  Body skeleton;  Boosting;  Depth camera;  Person re-identification},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pribanić2019532,
author={Pribanić, T. and Petković, T. and Đonlić, M.},
title={3D registration based on the direction sensor measurements},
journal={Pattern Recognition},
year={2019},
volume={88},
pages={532-546},
doi={10.1016/j.patcog.2018.12.008},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058502105&doi=10.1016%2fj.patcog.2018.12.008&partnerID=40&md5=fb0b59a93a24cf968774e117763621f5},
affiliation={University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, Zagreb, HR-10000, Croatia},
abstract={3D registration is a very active topic, spanning research areas such as computational geometry, computer graphics and pattern recognition. It aims to solve spatial transformation that aligns two point clouds. In this work we propose the use of a single direction sensor, such as an accelerometer or a magnetometer, commonly available on contemporary mobile platforms, such as tablets and smartphones. Both sensors have been heavily investigated earlier, but only for joint use with other sensors, such as gyroscopes and GPS. We show a time-efficient and accurate 3D registration method that takes advantage of only either an accelerometer or a magnetometer. We demonstrate a 3D reconstruction of individual point clouds and the proposed 3D registration method on a tablet equipped with an accelerometer or a magnetometer. However, we point out that the proposed method is not restricted to mobile platforms. Indeed, it can easily be applied in any 3D measurement system that is upgradable with some ubiquitous direction sensor, for example by adding a smartphone equipped with either an accelerometer or a magnetometer. We compare the proposed method against several state-of-the-art methods implemented in the open source Point Cloud Library (PCL). The proposed method outperforms the PCL methods tested, both in terms of processing time and accuracy. © 2018 Elsevier Ltd},
author_keywords={3D reconstruction;  3D rigid registration;  Accelerometer;  Magnetometer;  Smartphone;  Structured light pattern;  Tablet},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sayed2019,
author={Sayed, M. and Bhatti, Z. and Ismaili, I.A.},
title={Proposed model for facial animation using covariance matrix and mahalanobis distance algorithms},
journal={2019 2nd International Conference on Computing, Mathematics and Engineering Technologies, iCoMET 2019},
year={2019},
doi={10.1109/ICOMET.2019.8673465},
art_number={8673465},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064124710&doi=10.1109%2fICOMET.2019.8673465&partnerID=40&md5=0c6134a675285139195fe6bc3ef4ade3},
affiliation={Institute of Information and Communication Technology, University of Sindh, Jamshoro, Pakistan},
abstract={In this research work, an automated 3D face expression generation technique is presented, which is extracted from real life video of face motion. The face expression is extracted from real human face using Covariance Matrix for detecting face marker points and Mahalanobis Distance to calculate the distance of each marker points within frames. The technique of tracking points on face uses markers placed on key positions of face muscles, then by getting its position from all frames of pre-recoded face video using the distance algorithm the movement of each face muscle is detected and measured. The face muscles are marked with particular tracking markers that are detected and tracked by the system. This tracking occurs by using color segmentation using Huffman Transform, where we detect color of points and track the location and distance of each tracker points. The original and translated position values of each marker points are obtained and recorded in text file in vector values. The tracked values will be transferred in a 3D Animation software like MAYA and applied on a pre-Rigged 3D model of Human face. The 3D face will be rigged using joints to emulate the face muscle behavior. © 2019 IEEE.},
author_keywords={Covariance Matrix;  Expression;  Face Tracking;  Mahalanobis Distance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang20191826,
author={Wang, Q. and Zhang, Y. and Yuan, J. and Lu, Y.},
title={Space-time event clouds for gesture recognition: From RGB cameras to event cameras},
journal={Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019},
year={2019},
pages={1826-1835},
doi={10.1109/WACV.2019.00199},
art_number={8659288},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063575027&doi=10.1109%2fWACV.2019.00199&partnerID=40&md5=d8c48628e34a61213d411eca9d1c6850},
affiliation={Nanyang Technological University, Singapore; State University of New York, Buffalo, United States},
abstract={The recently developed event cameras can directly sense the motion by generating an asynchronous sequence of events, i.e., an event stream, where each individual event (x, y, t) corresponds to the space-time location when a pixel sensor captures an intensity change. Compared with RGB cameras, event cameras are frameless but can capture much faster motion, therefore have great potential for recognizing gestures of fast motions. To deal with the unique output of event cameras, previous methods often treat event streams as time sequences, thus do not fully explore the space-time sparsity and structure of the event stream data. In this work, we treat the event stream as a set of 3D points in space-time, i.e., space-time event clouds. To analyze event clouds and recognize gestures, we propose to leverage PointNet, a neural network architecture originally designed for matching and recognizing 3D point clouds. We adapt PointNet to cater to event clouds for real-time gesture recognition. On the benchmark dataset of event camera based gesture recognition, i.e., IBM DVS128 Gesture dataset, our proposed method achieves a high accuracy of 97.08% and performs the best among existing methods. © 2019 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Quintana2019,
author={Quintana, M. and Karaoglu, S. and Alvarez, F. and Menendez, J.M. and Gevers, T.},
title={Three-D wide faces (3DWF): Facial landmark detection and 3D reconstruction over a new RGB–D multi-camera dataset},
journal={Sensors (Switzerland)},
year={2019},
volume={19},
number={5},
doi={10.3390/s19051103},
art_number={1103},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062633100&doi=10.3390%2fs19051103&partnerID=40&md5=2954b668b0f943b6846dfd89cda77de6},
affiliation={Grupo de Aplicación de Tele comunicaciones Visuales, Universidad Politecnica de Madrid, Madrid, 28040, Spain; Computer Vision Lab, University of Amsterdam, Amsterdam, 1098 XH, Netherlands; 3DUniversum, Amsterdam, 1098 XH, Netherlands},
abstract={Latest advances of deep learning paradigm and 3D imaging systems have raised the necessity for more complete datasets that allow exploitation of facial features such as pose, gender or age. In our work, we propose a new facial dataset collected with an innovative RGB–D multi-camera setup whose optimization is presented and validated. 3DWF includes 3D raw and registered data collection for 92 persons from low-cost RGB–D sensing devices to commercial scanners with great accuracy. 3DWF provides a complete dataset with relevant and accurate visual information for different tasks related to facial properties such as face tracking or 3D face reconstruction by means of annotated density normalized 2K clouds and RGB–D streams. In addition, we validate the reliability of our proposal by an original data augmentation method from a massive set of face meshes for facial landmark detection in 2D domain, and by head pose classification through common Machine Learning techniques directed towards proving alignment of collected data. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={3D data collection;  3D face modelling;  Deep learning;  Face landmark detection;  Head pose classification},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen201929,
author={Chen, K. and Jia, K. and Huttunen, H. and Matas, J. and Kämäräinen, J.-K.},
title={Cumulative attribute space regression for head pose estimation and color constancy},
journal={Pattern Recognition},
year={2019},
volume={87},
pages={29-37},
doi={10.1016/j.patcog.2018.10.015},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054715565&doi=10.1016%2fj.patcog.2018.10.015&partnerID=40&md5=41f23130fd14bf52f929166f78c162b4},
affiliation={Laboratory of Signal Processing, Tampere University of Technology, Finland; School of Electronic and Information Engineering, South China University of Technology, China; Department of Cybernetics, Czech Technical University, Prague, Czech Republic},
abstract={Two-stage Cumulative Attribute (CA) regression has been found effective in regression problems of computer vision such as facial age and crowd density estimation. The first stage regression maps input features to cumulative attributes that encode correlations between target values. The previous works have dealt with single output regression. In this work, we propose cumulative attribute spaces for 2- and 3-output (multivariate) regression. We show how the original CA space can be generalized to multiple output by the Cartesian product (CartCA). However, for target spaces with more than two outputs the CartCA becomes computationally infeasible and therefore we propose an approximate solution - multi-view CA (MvCA) - where CartCA is applied to output pairs. We experimentally verify improved performance of the CartCA and MvCA spaces in 2D and 3D face pose estimation and three-output (RGB) illuminant estimation for color constancy. © 2018},
author_keywords={Color constancy;  Cumulative attribute space;  Head pose;  Multivariate regression},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mokhayeri2019757,
author={Mokhayeri, F. and Granger, E. and Bilodeau, G.-A.},
title={Domain-specific face synthesis for video face recognition from a single sample per person},
journal={IEEE Transactions on Information Forensics and Security},
year={2019},
volume={14},
number={3},
pages={757-772},
doi={10.1109/TIFS.2018.2866295},
art_number={8443382},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044377615&doi=10.1109%2fTIFS.2018.2866295&partnerID=40&md5=0e7c7e2b415519dea52e13a9a1518099},
affiliation={Laboratory for Imagery, Vision and Artificial Intelligence, École de Technologie Supérieure, Université du Québec, Montreal, QC  H3C1K3, Canada; Image and Video Processing Laboratory, Polytechnique Montreál, Montreal, QC  H3T1J4, Canada},
abstract={In video surveillance, face recognition (FR) systems are employed to detect individuals of interest appearing over a distributed network of cameras. The performance of still-to-video FR systems can decline significantly because faces captured in unconstrained operational domain (OD) over multiple video cameras have a different underlying data distribution compared to faces captured under controlled conditions in the enrollment domain with a still camera. This is particularly true when individuals are enrolled to the system using a single reference still. To improve the robustness of these systems, it is possible to augment the reference set by generating synthetic faces based on the original still. However, without the knowledge of the OD, many synthetic images must be generated to account for all possible capture conditions. FR systems may, therefore, require complex implementations and yield lower accuracy when training on many less relevant images. This paper introduces an algorithm for domain-specific face synthesis (DSFS) that exploits the representative intra-class variation information available from the OD. Prior to operation (during camera calibration), a compact set of faces from unknown persons appearing in the OD is selected through affinity propagation clustering in the captured condition space (defined by pose and illumination estimation). The domain-specific variations of these face images are then projected onto the reference still of each individual by integrating an image-based face relighting technique inside the 3-D reconstruction framework. A compact set of synthetic faces is generated that resemble individuals of interest under the capture conditions relevant to the OD. In a particular implementation based on sparse representation classification, the synthetic faces generated with the DSFS are employed to form a cross-domain dictionary that accounts for structured sparsity, where the dictionary blocks combine the original and synthetic faces of each individual. Experimental results obtained with videos from the Chokepoint and COX-S2V data sets reveal that augmenting the reference gallery set of still-to-video FR systems using the proposed DSFS approach can provide a significantly higher level of accuracy compared with the state-of-the-art approaches, with only a moderate increase in its computational complexity. © 2018 IEEE.},
author_keywords={3D face reconstruction;  Face recognition;  face synthesis;  illumination transferring;  single sample per person;  sparse representation-based classification;  video surveillance},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shi2019455,
author={Shi, B. and Zang, H. and Zheng, R. and Zhan, S.},
title={An efficient 3D face recognition approach using Frenet feature of iso-geodesic curves},
journal={Journal of Visual Communication and Image Representation},
year={2019},
volume={59},
pages={455-460},
doi={10.1016/j.jvcir.2019.02.002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061129042&doi=10.1016%2fj.jvcir.2019.02.002&partnerID=40&md5=8f0fe3cd51ff2049c71bfe756a9bf6d6},
affiliation={School of Computer and Information, Hefei University of Technology, Hefei, 230009, China},
abstract={Extracting efficient features from the large volume of 3D facial data directly is extremely difficult in 3D face recognition (3D-FR) with the latest methods, which mostly require heavy computations and manual processing steps. This paper presents a computationally efficient 3D-FR system based on a novel Frenet frame-based feature that is derived from the 3D facial iso-geodesic curves. In terms of the evaluation of the proposed method, we conducted a number of experiments on the CASIA 3D face database, and a superior recognition performance has been achieved. The performance evaluation suggests that the pose invariance attribute of the features relieves the need of an expensive 3D face registration in the face preprocessing procedure, where we take less time to process conversely. Our experiments further demonstrate that the proposed method not only achieves competitive recognition performance when compared with some existing techniques for 3D-FR, but also is computationally efficient. © 2019 Elsevier Inc.},
author_keywords={3D face recognition;  Facial curves;  Frenet framework;  Iso-geodesic;  Pose invariant},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chong2019358,
author={Chong, L.Y. and Ong, T.S. and Teoh, A.B.J.},
title={Feature fusions for 2.5D face recognition in Random Maxout Extreme Learning Machine},
journal={Applied Soft Computing Journal},
year={2019},
volume={75},
pages={358-372},
doi={10.1016/j.asoc.2018.11.024},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057242678&doi=10.1016%2fj.asoc.2018.11.024&partnerID=40&md5=15265fe1ef6fe5a11fe3fdcfb74ad15a},
affiliation={Faculty of Information Science and Technology, Multimedia University, Jalan Ayer Keroh Lama, Melaka, 75450, Malaysia; School of Electrical and Electronic Engineering, College of Engineering, Yonsei University, 50 Yonsei-ro Seodaemun-gu, Seoul, 03722, South Korea},
abstract={Contemporary face recognition system is often based on either 2D (texture) or 3D (texture + shape) face modality. An alternative modality that utilizes range (depth) facial images, namely 2.5D face recognition emerges. In this paper, we propose a 2.5D face descriptor that based on the Regional Covariance Matrix (RCM), a powerful means of feature fusion technique and a novel classifier dubbed Random Maxout Extreme Learning Machine (RMELM). The RCM of interest is constructed based on the Principal Component Analysis (PCA) filters responses of facial texture and/or range image, wherein the PCA filters are learned from a two-layer PCA network. The RMELM is an ELM variant where the activation function is based on the locally linear maxout function, in place of typical global non-linear functions in ELM. Since the RCM is a special case of symmetric positive definite matrix that resides on the Tensor manifold; a gap exists in between RCM and RMELM, which is a vector-based classifier. To bridge the gap, we flatten the manifold by transforming the RCM to a feature vector via a matrix logarithm operator. Experimental results from two public 3D face databases, FRGC v2.0 database and Gavab database, validated our proposed method is promising in 2.5D face recognition. © 2018 Elsevier B.V.},
author_keywords={Extreme learning machine;  Fusion;  Random maxout;  Regional covariance matrix;  Tensor manifold},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kamanditya2019,
author={Kamanditya, B. and Kuswara, R.P. and Nugroho, M.A. and Kusumoputro, B.},
title={Convolution Neural Network for Pose Estimation of Noisy Three-Dimensional Face Images},
journal={2018 IEEE 5th International Conference on Engineering Technologies and Applied Sciences, ICETAS 2018},
year={2019},
doi={10.1109/ICETAS.2018.8629150},
art_number={8629150},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062888878&doi=10.1109%2fICETAS.2018.8629150&partnerID=40&md5=232ba33a7341381ec65ef455b6e0245c},
affiliation={Dept. of Electrical Engineering, Universitas Indonesia, Jakarta, Indonesia},
abstract={From limited two-dimensional recognition, facial recognition has now been developed to be able to recognize three-dimensional face images, which usually involves process of face pose estimation. As the conventional artificial neural networks has shown low recognition rate to this problem, Convolution Neural Network have been the most potential classifier to determine the pose estimation of a three-dimensional face images. Convolution operation is expected to minimize the effect of distortion and disorientation of the object, and able to efficiently reduce the required parameters. Results show that the CNN system could estimate the pose position of the 3D face images with high recognition rate, however, this recognition rate decline significantly for the noisy buried face images, showing the CNN still need improvement to deal with noisy environments. © 2018 IEEE.},
author_keywords={convolutional neural network;  face recognition;  head pose estimation;  hyperparameter evaluation;  image noises},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lin2019,
author={Lin, G. and Tang, Y. and Zou, X. and Xiong, J. and Li, J.},
title={Guava detection and pose estimation using a low-cost RGB-D sensor in the field},
journal={Sensors (Switzerland)},
year={2019},
volume={19},
number={2},
doi={10.3390/s19020428},
art_number={428},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060395875&doi=10.3390%2fs19020428&partnerID=40&md5=9762acf3fef74ebb7d469ecb6dbc9e32},
affiliation={Key Laboratory of Key Technology on Agricultural Machine and Equipment, Ministry of Education, South China Agricultural University, Guangzhou, 510642, China; College of Mechanical and Automotive Engineering, Chuzhou University, Chuzhou, 239000, China; School of Urban and Rural Construction, Zhongkai University of Agriculture and Engineering, Guangzhou, 510006, China},
abstract={Fruit detection in real outdoor conditions is necessary for automatic guava harvesting, and the branch-dependent pose of fruits is also crucial to guide a robot to approach and detach the target fruit without colliding with its mother branch. To conduct automatic, collision-free picking, this study investigates a fruit detection and pose estimation method by using a low-cost red–green–blue–depth (RGB-D) sensor. A state-of-the-art fully convolutional network is first deployed to segment the RGB image to output a fruit and branch binary map. Based on the fruit binary map and RGB-D depth image, Euclidean clustering is then applied to group the point cloud into a set of individual fruits. Next, a multiple three-dimensional (3D) line-segments detection method is developed to reconstruct the segmented branches. Finally, the 3D pose of the fruit is estimated using its center position and nearest branch information. A dataset was acquired in an outdoor orchard to evaluate the performance of the proposed method. Quantitative experiments showed that the precision and recall of guava fruit detection were 0.983 and 0.948, respectively; the 3D pose error was 23.43 ◦ ± 14.18 ◦ ; and the execution time per fruit was 0.565 s. The results demonstrate that the developed method can be applied to a guava-harvesting robot. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Branch reconstruction;  Fully convolutional network;  Guava detection;  Pose estimation;  RGB-D sensor},
document_type={Article},
source={Scopus},
}

@ARTICLE{Anbarjafari2019125,
author={Anbarjafari, G. and Haamer, R.E. and LÜSi, I. and Tikk, T. and Valgma, L.},
title={3D face reconstruction with region based best fit blending using mobile phone for virtual reality based social media},
journal={Bulletin of the Polish Academy of Sciences: Technical Sciences},
year={2019},
volume={67},
number={1},
pages={125-132},
doi={10.24425/bpas.2019.127341},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063808749&doi=10.24425%2fbpas.2019.127341&partnerID=40&md5=bb3ab46a3eef3258f21d34c06e624c89},
affiliation={ICV Lab, Institute of Technology, University of Tartu, Tartu, 50411, Estonia; Department of Electrical and Electronic Engineering, Hasan Kalyoncu University, Gaziantep, Turkey},
abstract={The use of virtual reality (VR) has been exponentially increasing and due to that many researchers have started to work on developing new VR based social media. For this purpose it is important to have an avatar of the user which look like them to be easily generated by the devices which are accessible, such as mobile phones. In this paper, we propose a novel method of recreating a 3D human face model captured with a phone camera image or video data. The method focuses more on model shape than texture in order to make the face recognizable. We detect 68 facial feature points and use them to separate a face into four regions. For each area the best fitting models are found and are further morphed combined to find the best fitting models for each area. These are then combined and further morphed in order to restore the original facial proportions. We also present a method of texturing the resulting model, where the aforementioned feature points are used to generate a texture for the resulting model. © 2019 Polish Academy of Sciences. All rights reserved.},
author_keywords={3D faces;  Blendshape;  Computer vision;  Deformable model;  Facial modeling;  Morphing;  Similarity calculation;  Stretching;  Virtual reality},
document_type={Article},
source={Scopus},
}

@ARTICLE{Neves2019151,
author={Neves, J. and Proenca, H.},
title={'A leopard cannot change its spots': Improving face recognition using 3D-based caricatures},
journal={IEEE Transactions on Information Forensics and Security},
year={2019},
volume={14},
number={1},
pages={151-161},
doi={10.1109/TIFS.2018.2846617},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048519687&doi=10.1109%2fTIFS.2018.2846617&partnerID=40&md5=e72b3f3f2d8fe2df37170675f36c43d3},
affiliation={Department of Computer Science, Instituto de Telecomunicações, University of Beira Interior, Covilhã, 6200-001, Portugal},
abstract={Caricatures refer to a representation of a person, in which the distinctive features are deliberately exaggerated, with several studies showing that humans perform better at recognizing people from caricatures than using original images. Inspired by this observation, this paper introduces the first fully automated caricature-based face recognition approach capable of working with data acquired in the wild. Our approach leverages the 3D face structure from a single 2D image and compares it with a reference model for obtaining a compact representation of face features deviations. This descriptor is subsequently deformed using a 'measure locally, weight globally' strategy to resemble the caricature drawing process. The deformed deviations are incorporated in the 3D model using the Laplacian mesh deformation algorithm, and the 2D face caricature image is obtained by projecting the deformed model in the original camera view. To demonstrate the advantages of caricature-based face recognition, we train the VGG-face network from scratch using either original face images (baseline) or caricatured images and use these models for extracting face descriptors from the LFW, IJB-A, and MegaFace data sets. The experiments show an increase in the recognition accuracy when using caricatures rather than original images. Moreover, our approach achieves competitive results with the state-of-the-art face recognition methods, even without explicitly tuning the network for any of the evaluation sets. © 2005-2012 IEEE.},
author_keywords={3D caricature generation;  caricature-based face recognition;  Face recognition;  facial feature analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu20185216,
author={Liu, F. and Zhu, R. and Zeng, D. and Zhao, Q. and Liu, X.},
title={Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2018},
pages={5216-5225},
doi={10.1109/CVPR.2018.00547},
art_number={8578645},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062864427&doi=10.1109%2fCVPR.2018.00547&partnerID=40&md5=f059edc28251146b675678e4d4d4b6cf},
affiliation={College of Computer Science, Sichuan University, China; Department of Computer Science and Engineering, Michigan State University, United States},
abstract={This paper proposes an encoder-decoder network to disentangle shape features during 3D face reconstruction from single 2D images, such that the tasks of reconstructing accurate 3D face shapes and learning discriminative shape features for face recognition can be accomplished simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. To construct training data we develop a method for fitting 3D morphable model (3DMM) to multiple 2D images of a subject. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Deng20187093,
author={Deng, J. and Cheng, S. and Xue, N. and Zhou, Y. and Zafeiriou, S.},
title={UV-GAN: Adversarial Facial UV Map Completion for Pose-Invariant Face Recognition},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2018},
pages={7093-7102},
doi={10.1109/CVPR.2018.00741},
art_number={8578839},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052731286&doi=10.1109%2fCVPR.2018.00741&partnerID=40&md5=06c0a4e35e20723e25367ff87e39b00b},
affiliation={Imperial College London, United Kingdom},
abstract={Recently proposed robust 3D face alignment methods establish either dense or sparse correspondence between a 3D face model and a 2D facial image. The use of these methods presents new challenges as well as opportunities for facial texture analysis. In particular, by sampling the image using the fitted model, a facial UV can be created. Unfortunately, due to self-occlusion, such a UV map is always incomplete. In this paper, we propose a framework for training Deep Convolutional Neural Network (DCNN) to complete the facial UV map extracted from in-the-wild images. To this end, we first gather complete UV maps by fitting a 3D Morphable Model (3DMM) to various multiview image and video datasets, as well as leveraging on a new 3D dataset with over 3,000 identities. Second, we devise a meticulously designed architecture that combines local and global adversarial DCNNs to learn an identity-preserving facial UV completion model. We demonstrate that by attaching the completed UV to the fitted mesh and generating instances of arbitrary poses, we can increase pose variations for training deep face recognition/verification models, and minimise pose discrepancy during testing, which lead to better performance. Experiments on both controlled and in-the-wild UV datasets prove the effectiveness of our adversarial UV completion model. We achieve state-of-the-art verification accuracy, 94.05%, under the CFP frontal-profile protocol only by combining pose augmentation during training and pose discrepancy reduction during testing. We will release the first in-the-wild UV dataset (we refer as WildUV) that comprises of complete facial UV maps from 1,892 identities for research purposes. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{ZulqarnainGilani20181896,
author={Zulqarnain Gilani, S. and Mian, A.},
title={Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2018},
pages={1896-1905},
doi={10.1109/CVPR.2018.00203},
art_number={8578301},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060207182&doi=10.1109%2fCVPR.2018.00203&partnerID=40&md5=7a2227c884147feae86f1d25240e9a7a},
affiliation={Computer Science and Software Engineering, University of Western Australia, Australia},
abstract={Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheng20185117,
author={Cheng, S. and Kotsia, I. and Pantic, M. and Zafeiriou, S.},
title={4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2018},
pages={5117-5126},
doi={10.1109/CVPR.2018.00537},
art_number={8578635},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061785301&doi=10.1109%2fCVPR.2018.00537&partnerID=40&md5=d490b44275936f075cd3098b881319a6},
affiliation={Imperial College London, United Kingdom; Middlesex University London, United Kingdom},
abstract={The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Genova20188377,
author={Genova, K. and Cole, F. and Maschinot, A. and Sarna, A. and Vlasic, D. and Freeman, W.T.},
title={Unsupervised Training for 3D Morphable Model Regression},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2018},
pages={8377-8386},
doi={10.1109/CVPR.2018.00874},
art_number={8578972},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062824108&doi=10.1109%2fCVPR.2018.00874&partnerID=40&md5=8364c732206cbcaec51ad231798d90ec},
affiliation={Princeton University, United States; Google Research, United States; MIT, CSAIL, United States},
abstract={We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: A batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li20189397,
author={Li, J. and Chen, B.M. and Lee, G.H.},
title={SO-Net: Self-Organizing Network for Point Cloud Analysis},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2018},
pages={9397-9406},
doi={10.1109/CVPR.2018.00979},
art_number={8579077},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052978187&doi=10.1109%2fCVPR.2018.00979&partnerID=40&md5=e5e869f1c807e52b8968d2c5e014fcfd},
affiliation={National University of Singapore, Singapore},
abstract={This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website.1 © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Meyer201871,
author={Meyer, G. and Do, M.},
title={Real-time 3D face verification with a consumer depth camera},
journal={Proceedings - 2018 15th Conference on Computer and Robot Vision, CRV 2018},
year={2018},
pages={71-79},
doi={10.1109/CRV.2018.00020},
art_number={8575738},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060520302&doi=10.1109%2fCRV.2018.00020&partnerID=40&md5=26850015dc636728cafbe54bb7c227e8},
affiliation={University of Illinois at Urbana-Champaign, United States},
abstract={We present a system for accurate real-time 3D face verification using a low-quality consumer depth camera. To verify the identity of a subject, we built a high-quality reference model offline by fitting a 3D morphable model to a sequence of low-quality depth images. At runtime, we compare the similarity between the reference model and a single depth image by aligning the model to the image and measuring differences between every point on the two facial surfaces. The model and the image will not match exactly due to sensor noise, occlusions, as well as changes in expression, hairstyle, and eye-wear; therefore, we leverage a data driven approach to determine whether or not the model and the image match. We train a random decision forest to verify the identity of a subject where the point-to-point distances between the reference model and the depth image are used as input features to the classifier. Our approach runs in real-time and is designed to continuously authenticate a user as he/she uses his/her device. In addition, our proposed method outperforms existing 2D and 3D face verification methods on a benchmark data set. © 2018 IEEE.},
author_keywords={3D Computer Vision;  Depth Cameras;  Face Recognition;  Face Verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsu2018693,
author={Hsu, G.-S.J. and Huang, W.-F. and Kang, J.-H.},
title={Hierarchical network for facial palsy detection},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2018},
volume={2018-June},
pages={693-699},
doi={10.1109/CVPRW.2018.00100},
art_number={8575249},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060846803&doi=10.1109%2fCVPRW.2018.00100&partnerID=40&md5=c9670dafef77c9aca5ca86a0b5a75134},
affiliation={National Taiwan University of Science and Technology, Taipei, Taiwan; Taipei Medical University, Taipei, Taiwan},
abstract={We propose the Hierarchical Detection Network (HDN) for the detection of facial palsy syndrome. This can be the first deep-learning based approach for the facial palsy detection. The proposed HDN consists of three component networks, the first detects faces, the second detects the landmarks on the detected faces, and the third detects the local palsy regions. The first and the third component networks are built on the Darknet framework, but with fewer layers of convolutions for shorter processing speed. The second component network employs the latest 3D face alignment network for locating the landmarks. The first component network employs a Na × Na grid over the overall input image, while the third component network employs a Nb × Nb grid over each detected face, making the HDN capable of efficiently locating the affected palsy regions. As previous approaches were evaluated on proprietary databases, we have collected 32 videos from YouTube and made the first public database for facial palsy study. To enhance the robustness against expression variations, we include the CK+ facial expression database in the training and testing phases. We show that the HDN does not just detect the local palsy regions, but also captures the frequency of the intensity, enabling the video-to-description diagnosis of the syndrome. Experiments show that the proposed approach offers an accurate and efficient solution for facial palsy detection/diagnosis. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2018,
author={Wang, Z. and Wang, R. and Yin, G. and Wang, Z. and Zhou, H. and Zhang, H. and Zhao, H. and Zhang, K.},
title={Mesh establishment method based on 2D portrait},
journal={ACM International Conference Proceeding Series},
year={2018},
doi={10.1145/3148453.3306301},
art_number={3306301},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062788447&doi=10.1145%2f3148453.3306301&partnerID=40&md5=7b50588a35d91fd06707024ecbbdecd9},
affiliation={School of Computer and Communication Engineering No.30, China},
abstract={This paper proposes a method of mesh construction based on 2D portrait, which realizes the final 3d face model for arbitrary 2D face images by adjusting mesh. First of all need to 2D portrait photo for face detection, based on the photo collection of facial feature points, set up related parameters of the feature points, then according to the name of the feature points and corresponding coordinate parameters, the model of overall adjustment and local mesh vertex, get the final face image more realistic 3D face models. © 2018 Association for Computing Machinery.},
author_keywords={2D face image;  3D face model;  Mesh vertex},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Akita20181103,
author={Akita, T. and Yamauchi, Y. and Fujiyoshi, H.},
title={Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye Cameras},
journal={IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
year={2018},
volume={2018-November},
pages={1103-1108},
doi={10.1109/ITSC.2018.8569716},
art_number={8569716},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060488689&doi=10.1109%2fITSC.2018.8569716&partnerID=40&md5=d3f58376c0864a7c21f35575aaae164f},
affiliation={Research Center for Smart Vehicles, Toyota Technological Institute, Nagoya, Aichi Pref., Japan; Department of Robotic Science and Technology, Chubu University, Kasugai, Aichi Pref., Japan},
abstract={Recently, automated emergency brake systems for pedestrian have been commercialized. However, they cannot detect crossing pedestrians when turning at intersections because the field of view is not wide enough. Thus, we propose to utilize a surround view camera system becoming popular by making it into stereo vision which is robust for the pedestrian recognition. However, conventional stereo camera technologies cannot be applied due to fisheye cameras and uncalibrated camera poses. Thus we have created the new method to absorb difference of the pedestrian appearance between cameras by machine learning for the stereo vision. The method of stereo matching between image patches in each camera image was designed by combining D-Brief and NCC with SVM. Good generalization performance was achieved by it compared with individual conventional algorithms. Furthermore, feature amounts of the point cloud reconstructed by the stereo pairs are utilized with Random Forest to discriminate pedestrians. The algorithm was evaluated for the actual camera images of crossing pedestrians at various intersections, and 96.0% of pedestrian tracking rate with high position detection accuracy was achieved. They were compared with Faster R-CNN as the best pattern recognition technique, and our proposed method indicated better detection performance. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kang2018873,
author={Kang, S. and Lee, J. and Bong, K. and Kim, C. and Kim, Y. and Yoo, H.-J.},
title={Low-power scalable 3-d face frontalization processor for CNN-based face recognition in mobile devices},
journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
year={2018},
volume={8},
number={4},
pages={873-883},
doi={10.1109/JETCAS.2018.2845663},
art_number={8376028},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048502031&doi=10.1109%2fJETCAS.2018.2845663&partnerID=40&md5=2a2ca9f90eeb53992187fe131971764e},
affiliation={School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, 34141, South Korea},
abstract={A low-power scalable 3-D face frontalization processor is proposed for accurate face recognition in mobile devices. In spite of recent improvement in face recognition accuracy mainly from convolutional neural networks (CNNs), their performance is limited to face images with frontal view. For face recognition with human-level accuracy in real-life environment, in which most of the face images are captured from arbitrary angles, 3-D face frontalization is essential as a preprocessing stage for CNN-based face recognition algorithms. The proposed face frontalization processor shows scalability in two aspects: Image resolution and accuracy. For low-power consumption and scalability, the processor proposes three features: 1) scalable processing element (PE) architecture with workload adaptation; 2) accuracy scalable regression weight quantization to reduce the external memory access (EMA) down to 81.3%; and 3) pipelined memory-level zero-skipping to further reduce the EMA by 98.4% without any latency overhead. From the proposed EMA reduction features, the EMA is reduced by 99.7% with little accuracy degradation in face frontalization results. The proposed face frontalization processor is implemented in 65-nm CMOS process, and it shows 4.73 frames/s) throughput. Moreover, power consumption of the implemented face frontalization processor is 0.53 mW, which is suitable for applications on mobile devices. © 2018 IEEE.},
author_keywords={3-D face frontalization;  Accuracy scalability;  Convolutional neural network;  Face recognition;  Low-power processor},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dou201880,
author={Dou, P. and Kakadiaris, I.A.},
title={Multi-view 3D face reconstruction with deep recurrent neural networks},
journal={Image and Vision Computing},
year={2018},
volume={80},
pages={80-91},
doi={10.1016/j.imavis.2018.09.004},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055215226&doi=10.1016%2fj.imavis.2018.09.004&partnerID=40&md5=c8663dc6ac10ab6e357dc635c9ea8d2d},
affiliation={Department of Computer Science, Computational Biomedicine Lab, University of Houston, Houston, TX  77204, United States},
abstract={Image-based 3D face reconstruction has great potential in different areas, such as facial recognition, facial analysis, and facial animation. Due to the variations in image quality, single-image-based 3D face reconstruction might not be sufficient to accurately reconstruct a 3D face. To overcome this limitation, multi-view 3D face reconstruction uses multiple images of the same subject and aggregates complementary information for better accuracy. Though appealing, there are multiple challenges in practice. Among these challenges, the most significant is the difficulty to establish coherent and accurate correspondence among a set of images, especially when these images are captured under unconstrained in-the-wild condition. This work proposes a method, Deep Recurrent 3D FAce Reconstruction (DRFAR), to solve the task of multi-view 3D face reconstruction using a subspace representation of the 3D facial shape and a deep recurrent neural network that consists of both a deep convolutional neural network (DCNN) and a recurrent neural network (RNN). The DCNN disentangles the facial identity and the facial expression components for each single image independently, while the RNN fuses identity-related features from the DCNN and aggregates the identity specific contextual information, or the identity signal, from the whole set of images to estimate the facial identity parameter, which is robust to variations in image quality and is consistent over the whole set of images. Experimental results indicate significant improvement over state-of-the-art in both the accuracy and the consistency of 3D face reconstruction. Moreover, face recognition results on IJB-A with the UR2D face recognition pipeline indicate that, compared to single-view 3D face reconstruction, the proposed multi-view 3D face reconstruction algorithm can improve the face identification accuracy of UR2D by two percentage points in Rank-1 identification rate. © 2018},
author_keywords={3D face reconstruction;  Face recognition;  Long-short term memory;  Recurrent neural network},
document_type={Article},
source={Scopus},
}

@ARTICLE{Song2018,
author={Song, W. and Zou, S. and Tian, Y. and Fong, S. and Cho, K.},
title={Classifying 3D objects in LiDAR point clouds with a back-propagation neural network},
journal={Human-centric Computing and Information Sciences},
year={2018},
volume={8},
number={1},
doi={10.1186/s13673-018-0152-7},
art_number={29},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054606266&doi=10.1186%2fs13673-018-0152-7&partnerID=40&md5=ba2e756874ef662a6256625654d79c1d},
affiliation={North China University of Technology, Beijing, China; Dept. Computer and Information Science, University of Macau, Macau; Dept. Multimedia Engineering, Dongguk University, Seoul, South Korea},
abstract={Due to object recognition accuracy limitations, unmanned ground vehicles (UGVs) must perceive their environments for local path planning and object avoidance. To gather high-precision information about the UGV’s surroundings, Light Detection and Ranging (LiDAR) is frequently used to collect large-scale point clouds. However, the complex spatial features of these clouds, such as being unstructured, diffuse, and disordered, make it difficult to segment and recognize individual objects. This paper therefore develops an object feature extraction and classification system that uses LiDAR point clouds to classify 3D objects in urban environments. After eliminating the ground points via a height threshold method, this describes the 3D objects in terms of their geometrical features, namely their volume, density, and eigenvalues. A back-propagation neural network (BPNN) model is trained (over the course of many iterations) to use these extracted features to classify objects into five types. During the training period, the parameters in each layer of the BPNN model are continually changed and modified via back-propagation using a non-linear sigmoid function. In the system, the object segmentation process supports obstacle detection for autonomous driving, and the object recognition method provides an environment perception function for terrain modeling. Our experimental results indicate that the object recognition accuracy achieve 91.5% in outdoor environment. © 2018, The Author(s).},
author_keywords={3D object recognition;  Back-propagation neural network;  Feature extraction;  LiDAR point cloud},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang20182202,
author={Zhang, H. and Li, Q. and Sun, Z.},
title={Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark Localization},
journal={Proceedings - International Conference on Pattern Recognition},
year={2018},
volume={2018-August},
pages={2202-2208},
doi={10.1109/ICPR.2018.8546220},
art_number={8546220},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059738498&doi=10.1109%2fICPR.2018.8546220&partnerID=40&md5=e6f83488ba78d7a4aea6b189ecad8cd2},
affiliation={Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China; National Laboratory of Pattern Recognition (NLPR), China; Institute of Automation, Chinese Academy of Sciences (CASIA), China; University of Chinese Academy of Sciences (UCAS), China; Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), CAS, China},
abstract={3D face shape is more expressive and viewpoint-consistent than its 2D counterpart. However, 3D facial landmark localization in a single image is challenging due to the ambiguous nature of landmarks under 3D perspective. Existing approaches typically adopt a suboptimal two-step strategy, performing 2D landmark localization followed by depth estimation. In this paper, we propose the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial landmark localization, addressing it more effectively in an end-to-end fashion. First, a compact volumetric representation is proposed to encode the per-voxel likelihood of positions being the 3D landmarks. The dimensionality of such a representation is fixed regardless of the number of target landmarks, so that the curse of dimensionality could be avoided. Then, a stacked hourglass network is adopted to estimate the volumetric representation from coarse to fine, followed by a 3D convolution network that takes the estimated volume as input and regresses 3D coordinates of the face shape. In this way, the 3D structural constraints between landmarks could be learned by the neural network in a more efficient manner. Moreover, the proposed pipeline enables end-to-end training and improves the robustness and accuracy of 3D facial landmark localization. The effectiveness of our approach is validated on the 3DFAW and AFLW2000-3D datasets. Experimental results show that the proposed method achieves state-of-the-art performance in comparison with existing methods. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Qian2018,
author={Qian, K. and Su, K. and Zhang, J. and Li, Y.},
title={A 3D face registration algorithm based on conformal mapping},
journal={Concurrency Computation},
year={2018},
volume={30},
number={22},
doi={10.1002/cpe.4654},
art_number={e4654},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051076314&doi=10.1002%2fcpe.4654&partnerID=40&md5=5acefb80d0b632d21b20cd98aef48e69},
affiliation={School of Civil Engineering and Architecture, Kunming University of Science and TechnologyKunming, China; School of Computer Science, Wuhan UniversityWuhan, China; Faculty of Science, Kunming University of Science and TechnologyKunming, China},
abstract={Recently, 3D facial datasets are more easily available, and at the same time, the research of 3D face becomes more and more important. One of the most important research fields is 3D face registration, which plays an important role in face recognition, face shape analysis, and face animation. However, one of the most challenging issues in 3D face registration is to obtain a unique mapping for faces with different expression and landmark constraints. In this paper, we propose a novel conformal mapping algorithm to deal with the 3D face registration. Besides, the calculation is about harmonic energy, which makes our method applicable to low-quality meshes. We begin with a harmonic mapping, then minimize the harmonic energy by a specific boundary condition on surfaces, and to obtain the conformal mapping, finally, we use a landmark-constrained surface registration algorithm to register faces. Numerical experiments on various surfaces demonstrate the efficiency and robustness of our method. © 2018 John Wiley & Sons, Ltd.},
author_keywords={conformal mapping;  harmonic energy;  landmark constraints;  non-linear diffusion;  surface registration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kusnadi2018,
author={Kusnadi, A. and Wella and Winantyo, R. and Pane, I.Z.},
title={Evaluation of Feature Detectors on Repeatability Quality of Facial Keypoints in Triangulation Method},
journal={2018 International Conference on Smart Computing and Electronic Enterprise, ICSCEE 2018},
year={2018},
doi={10.1109/ICSCEE.2018.8538385},
art_number={8538385},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059422904&doi=10.1109%2fICSCEE.2018.8538385&partnerID=40&md5=917e7eada19a31a1d069b869b62ff3fd},
affiliation={Department of Informatics, Universitas Multimedia Nusantara, Tangerang, Indonesia},
abstract={This study derived from a research focusing on 3D face recognition using ToF camera. But the system can't be used outdoors, because of a backlight. To solve this problem, a commercial digital single-lens reflex (DSLR) camera will be used. It can be approached y solving the stereo-view reconstruction problem for each pair of consecutive images. To reconstruct an object, projection matrix estimation from 2D point correspondences will be needed. The accuracy of 3D reconstruction is highly dependent on the corresponding points of 2D data projections from images to other images. In this research, The detectors are Harris-Stephens, SURF, FAST, Minimum Eigenvalue, and BRISK have been tested and analyzed through black box test. To evaluate feature detectors performance, the repeatability score for a given pair of images is computed. To do that it can use recall and precision. The best detector is the Harris Stephens detector because it has the best F-measure values of 0.46. © 2018 IEEE.},
author_keywords={feature detectiont;  keypoint;  precision, F-measure;  recall;  repeatability;  triangulation method},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reji2018,
author={Reji, R. and Sojanlal, P.},
title={Region Based 3D Face Recognition},
journal={2017 IEEE International Conference on Computational Intelligence and Computing Research, ICCIC 2017},
year={2018},
doi={10.1109/ICCIC.2017.8524581},
art_number={8524581},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057946157&doi=10.1109%2fICCIC.2017.8524581&partnerID=40&md5=92c092fa4e2838ab444fcc0356fcdb75},
affiliation={Mahatma Gandhi University, School of Computer Sciences, Kottayam, Kerala, India; Mar-Baselious Institute of Technology Science, Kothamangalam, Kerala, India},
abstract={This paper focuses on a region based methodology for expression in sensitive 3D face recognition process. Considering facial regions that are comparatively unchanging during expressions, results shows that using fifteen sub regions on the face can attain high 3D face recognition. We use a modified face recognition algorithm along with hierarchical contour based image registration for finding the similarity score. Our method operates in two modes: verification mode and confirmation mode. Crop 100 mm of frontal face region, apply preprocessing and automatically detect nose tip, translate the face image to origin and crop fifteen sub regions. The cropped sub regions are defined by cuboids which occupy more volumetric data, Nose Tip is the most projecting point of the face with the highest value along Z-axis so consider it as origin. The modified face recognition algorithm reduces the effects caused by facial expressions and artifacts. Finally a Hierarchical contour based image registration technique is applied which yields better results. The approach is applied on Bosphorus 3D datasets and achieved a verification rate of 95.3% at 0.1% false acceptance rate. In the identification scenario 99.3% rank one recognition is achieved. © 2017 IEEE.},
author_keywords={3D face recognition;  Biometrics;  Contour based image registration;  MFRA;  Rank based Score},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rajakumari2018665,
author={Rajakumari, K. and Nalini, C.},
title={An enhancement of various techniques using 2D and 3D face recognition with different datasets},
journal={International Journal of Mechanical and Production Engineering Research and Development},
year={2018},
volume={8},
pages={665-671},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063461054&partnerID=40&md5=5d205a407838f83ab702e65899a26b48},
affiliation={Department of CSE, Bharath Institute of Higher Education and Research (BIHER), Chennai, Tamil Nadu, India},
abstract={The Face Recognition has been an active research area since the 1960 and is one of the most successful applications of image analysis and understanding. Face Recognition is an important part of many biometric, security and surveillance system as well as image and video indexing systems. Face Recognition is used to identify or verify a person from still image and video. 2D pictures are for the most part less demanding and more affordable to secure. The apparent advantages from utilizing 3D in respect to 2D information incorporate less variety saw because of elements, for example, cosmetics and less affectability to enlightenment changes. Essentially there are two principle purposes behind such a pattern (a) the first is extensive variety of business and law requirement applications (b) the accessibility of doable innovations. Many research conducted by 2D based face recognition techniques and their problems like increasing gallery size, illuminate variation, expression variation and pose variation have well known. My research with the help of PCA and HAAR (Wavelet Transform) and it will be effective for deciding an image or video. Here we displays a few tests keeping in mind the end goal to indicated light, posture and structure of the image. © TJPRC Pvt. Ltd.},
author_keywords={2D & 3D face recognition;  Comparison;  Identification;  Verification},
document_type={Article},
source={Scopus},
}

@ARTICLE{Derkach201886,
author={Derkach, D. and Sukno, F.M.},
title={Automatic local shape spectrum analysis for 3D facial expression recognition},
journal={Image and Vision Computing},
year={2018},
volume={79},
pages={86-98},
doi={10.1016/j.imavis.2018.09.007},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054162814&doi=10.1016%2fj.imavis.2018.09.007&partnerID=40&md5=1c112d306fdc7bc5f5bbd2e2e62dbbba},
affiliation={Department of Information and Communication Technologies, Pompeu Fabra University, Barcelona, Spain},
abstract={We investigate the problem of Facial Expression Recognition (FER) using 3D data. Building from one of the most successful frameworks for facial analysis using exclusively 3D geometry, we extend the analysis from a curve-based representation into a spectral representation, which allows a complete description of the underlying surface that can be further tuned to the desired level of detail. Spectral representations are based on the decomposition of the geometry in its spatial frequency components, much like a Fourier transform, which are related to intrinsic characteristics of the surface. In this work, we propose the use of Graph Laplacian Features (GLFs), which result from the projection of local surface patches into a common basis obtained from the Graph Laplacian eigenspace. We extract patches around facial landmarks and include a state-of-the-art localization algorithm to allow for fully-automatic operation. The proposed approach is tested on the three most popular databases for 3D FER (BU-3DFE, Bosphorus and BU-4DFE) in terms of expression and AU recognition. Our results show that the proposed GLFs consistently outperform the curves-based approach as well as the most popular alternative for spectral representation, Shape-DNA, which is based on the Laplace Beltrami Operator and cannot provide a stable basis that guarantee that the extracted signatures for the different patches are directly comparable. Interestingly, the accuracy improvement brought by GLFs is obtained also at a lower computational cost. Considering the extraction of patches as a common step between the three compared approaches, the curves-based framework requires a costly elastic deformation between corresponding curves (e.g. based on splines) and Shape-DNA requires computing an eigen-decomposition of every new patch to be analyzed. In contrast, GLFs only require the projection of the patch geometry into the Graph Laplacian eigenspace, which is common to all patches and can therefore be pre-computed off-line. We also show that 14 automatically detected landmarks are enough to achieve high FER and AU detection rates, only slightly below those obtained when using sets of manually annotated landmarks. © 2018 Elsevier B.V.},
author_keywords={3D face;  Facial expression recognition;  Laplace operators;  Spectral shape analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jin2018,
author={Jin, H. and Lian, Y. and Hua, J.},
title={Learning facial expressions with 3D mesh convolutional neural network},
journal={ACM Transactions on Intelligent Systems and Technology},
year={2018},
volume={10},
number={1},
doi={10.1145/3200572},
art_number={a5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057576901&doi=10.1145%2f3200572&partnerID=40&md5=71e1c77f0d3938a58a8a175f01015566},
affiliation={Department of Computer Science, Wayne State University, 5057 Woodward Ave, Detroit, MI  48202, United States},
abstract={Making machines understand human expressions enables various useful applications in human-machine interaction. In this article, we present a novel facial expression recognition approach with 3D Mesh Convolutional Neural Networks (3DMCNN) and a visual analytics-guided 3DMCNN design and optimization scheme. From an RGBD camera, we first reconstruct a 3D face model of a subject with facial expressions and then compute the geometric properties of the surface. Instead of using regular Convolutional Neural Networks (CNNs) to learn intensities of the facial images, we convolve the geometric properties on the surface of the 3D model using 3DMCNN. We design a geodesic distance-based convolution method to overcome the difficulties raised from the irregular sampling of the face surface mesh. We further present interactive visual analytics for the purpose of designing and modifying the networks to analyze the learned features and cluster similar nodes in 3DMCNN. By removing low-activity nodes in the network, the performance of the network is greatly improved. We compare our method with the regular CNN-based method by interactively visualizing each layer of the networks and analyze the effectiveness of our method by studying representative cases. Testing on public datasets, our method achieves a higher recognition accuracy than traditional image-based CNN and other 3D CNNs. The proposed framework, including 3DMCNN and interactive visual analytics of the CNN, can be extended to other applications. © 2018 Association for Computing Machinery.},
author_keywords={3D mesh convolutional neural networks;  Facial expression analysis;  Visual analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li20187520,
author={Li, S. and Su, L. and Liu, Y. and He, Z.},
title={Segmentation of individual trees based on a point cloud clustering method using airborne LiDAR data},
journal={International Geoscience and Remote Sensing Symposium (IGARSS)},
year={2018},
volume={2018-July},
pages={7520-7523},
doi={10.1109/IGARSS.2018.8518089},
art_number={8518089},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063162349&doi=10.1109%2fIGARSS.2018.8518089&partnerID=40&md5=7998e0b9c2506b65acb004049bf28154},
affiliation={School of Resources and Environment, University of Electronic Science and Technology of China, No. 2006, Xiyuan Ave, West Hi-Tech Zone, Chengdu, 611731, China; Center for Information Geoscience, University of Electronic Science and Technology of China, No. 2006, Xiyuan Ave, West Hi-Tech Zone, Chengdu, 611731, China},
abstract={The objective of this paper was to develop a new algorithm to segment individual trees directly by using the three-dimensional space characteristic of airborne light detection and ranging point cloud data. The local maximum method was used in the initial segmentation and the error identification tree exclusion. On the basis of the point cloud spatial distribution of individual trees and the adjacent relationship with the other trees, a point cloud clustering method was developed to decide the points belonging to the individual trees. This algorithm was tested by 6 forest plots in the Genhe forestry reserve. The results showed that this algorithm could segment individual trees quickly and accurately, and the overall accuracy of this algorithm was 96.3%. © 2018 IEEE},
author_keywords={Clustering;  LiDAR;  Segmentation;  Tree},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ganapathi-Subramanian2018672,
author={Ganapathi-Subramanian, V. and Diamanti, O. and Pirk, S. and Tang, C. and Niessner, M. and Guibas, L.},
title={Parsing geometry using structure-aware shape templates},
journal={Proceedings - 2018 International Conference on 3D Vision, 3DV 2018},
year={2018},
pages={672-681},
doi={10.1109/3DV.2018.00082},
art_number={8491020},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056790125&doi=10.1109%2f3DV.2018.00082&partnerID=40&md5=2cab7fd610063052d24a883601fd3067},
affiliation={Stanford University, United States; Autodesk Inc., Stanford University, United States; Google Brain, Stanford University, United States; TU Munich, Stanford University, Germany},
abstract={Real-life man-made objects often exhibit strong and easily-identifiable structure, as a direct result of their design or their intended functionality. Structure typically appears in the form of individual parts and their arrangement. Knowing about object structure can be an important cue for object recognition and scene understanding - a key goal for various AR and robotics applications. However, commodity RGB-D sensors used in these scenarios only produce raw, unorganized point clouds, without structural information about the captured scene. Moreover, the generated data is commonly partial and susceptible to artifacts and noise, which makes inferring the structure of scanned objects challenging. In this paper, we organize large shape collections into parameterized shape templates to capture the underlying structure of the objects. The templates allow us to transfer the structural information onto new objects and incomplete scans. We employ a deep neural network that matches the partial scan with one of the shape templates, then match and fit it to complete and detailed models from the collection. This allows us to faithfully label its parts and to guide the reconstruction of the scanned object. We showcase the effectiveness of our method by comparing it to other state-of-the-art approaches. © 2018 IEEE.},
author_keywords={Partial Shape Recovery;  Shape Primitives;  Shape Reconstruction;  Shape Templates;  Template Fitting},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xu2018,
author={Xu, Y. and Sun, Z. and Hoegner, L. and Stilla, U. and Yao, W.},
title={Instance segmentation of trees in urban areas from MLS point clouds using supervoxel contexts and graph-based optimization},
journal={2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing, PRRS 2018},
year={2018},
doi={10.1109/PRRS.2018.8486220},
art_number={8486220},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056548636&doi=10.1109%2fPRRS.2018.8486220&partnerID=40&md5=a370a2800eee69885fbaa013f60c824a},
affiliation={Photogrammetry and Remote Sensing Technische, Universität Munchen, Munich, Germany; Department of Land Surveying and Geo-Informatics, Hong Kong Polytechnic University, Hung Hom, Hong Kong},
abstract={In this paper, an instance segmentation method for tree extraction from MLS data sets in urban scenes is developed. The proposed method utilizes a supervoxel structure to organize the point clouds, and then extracts the detrended geometric features from the local context of supervoxels. Combined with the detrended features of the local context, the Random Forest (RF) classifier will be adopted to obtain the initial semantic labeling results of trees from point clouds. Afterwards, a local context-based regularization is iteratively performed to achieve global optimum on a global graphical model, in order to spatially smoothing the semantic labeling results. Finally, a graph-based segmentation is conducted to separate individual trees according to the semantic labeling results. The use of supervoxel structure can preserve the geometric boundaries of objects in the scene, and compared with point-based solutions, the supervoxel-based method can largely decrease the number of basic elements during the processing. Besides, the introduction of supervoxel contexts can extract the local information of an object making the feature extraction more robust and representative. Detrended geometric features can get over the redundant and in-salient information in the local context, so that discriminative features are obtained. Benefiting from the regularization process, the spatial smoothing is obtained based on initial labeling results from classic classifications such as RF classification. As a result, misclassification errors are removed to a large degree and semantic labeling results are thus smoothed. Based on the constructed global graphical model during the spatially smoothing process, a graph-based segmentation is applied to partition the graphical model for the clustering the instances of trees. The experiments on two test datasets have shown promising results, with an accuracy of the semantic labeling of trees reaching around 0.9. The segmentation of trees using graph-based algorithm also show acceptable results, with trees having simple structures and sparse distributions correctly separated, but for those cramped trees with complex structures, the points are over- or under-segmented. © 2018 IEEE.},
author_keywords={Graph-based segmentation;  Instance segmentation;  Local context;  MLS;  Supervoxels;  Trees;  Urban areas},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2018630,
author={Liu, C. and Tang, T. and Wang, M. and Lv, K.},
title={Multi-feature based emotion recognition for video clips},
journal={ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction},
year={2018},
pages={630-634},
doi={10.1145/3242969.3264989},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056667541&doi=10.1145%2f3242969.3264989&partnerID=40&md5=1ba25802b479fbffcb6c85fe92b29597},
affiliation={Situ AI Labs Beijing, Beijing, 100089, China},
abstract={In this paper, we present our latest progress in Emotion Recognition techniques, which combines acoustic features and facial features in both non-temporal and temporal mode. This paper presents the details of our techniques used in the Audio-Video Emotion Recognition subtask in the 2018 Emotion Recognition in the Wild (EmotiW) Challenge. After the multimodal results fusion, our final accuracy in Acted Facial Expression in Wild (AFEW) test dataset achieves 61.87%, which is 1.53% higher than the best results last year. Such improvements prove the effectiveness of our methods. © 2018 Association for Computing Machinery.},
author_keywords={3D Face Landmark;  Deep Learning;  DenseNet;  Emotion Recognition;  EmotiW 2018;  Inception Net;  LSTM;  SoundNet},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fei2018139,
author={Fei, H. and Tu, B. and Chen, Q. and He, D. and Zhou, C. and Peng, Y.},
title={An overview of face-related technologies},
journal={Journal of Visual Communication and Image Representation},
year={2018},
volume={56},
pages={139-143},
doi={10.1016/j.jvcir.2018.09.012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053774931&doi=10.1016%2fj.jvcir.2018.09.012&partnerID=40&md5=ddfc4618812dd5bf104f59915ffc5af0},
affiliation={College of Information and Communication Engineering, School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Computer and Information, Hefei University of Technology, Hefei, China},
abstract={In recent years, information technology is developing continuously and set off a burst of artificial intelligence boom in the field of science. The development of advanced technologies such as unmanned driving and AI chips, is the extensive application of artificial intelligence. Face-related technologies have a wide range of applications because of intuitive results and good concealment. Since 3D face information can provide more comprehensive facial information than 2D face information, and it can solve many difficulties that cannot be solved in 2D face recognition. Therefore, more and more researchers have studied 3D face recognition in recent years. Under the new circumstances, the research on face are experiencing all kinds of challenges. With the tireless of many scientists, the new technology is also making a constant progress, and in the development of many technologies it still maintained its leading position. In this paper, we simply sort out the present development process of facial correlation technology, and the general evolution of this technology is outlined. Finally, the practical significance of this technology development is briefly discussed. © 2018},
author_keywords={3D face reconstruction;  Deep learning;  Face enhancement;  Face recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Siqueira20183513,
author={Siqueira, R.S. and Alexandre, G.R. and Soares, J.M. and The, G.A.P.},
title={Triaxial slicing for 3-D face recognition from adapted rotational invariants spatial moments and minimal keypoints dependence},
journal={IEEE Robotics and Automation Letters},
year={2018},
volume={3},
number={4},
pages={3513-3520},
doi={10.1109/LRA.2018.2854295},
art_number={8408720},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063307357&doi=10.1109%2fLRA.2018.2854295&partnerID=40&md5=5205595c202573ef77808eaf4aee1626},
affiliation={Faculty of Electrical Engineering, Instituto Federal de Educação, Ciência e Tecnologia Do Ceará, Fortaleza, CE 60410-42, Brazil; Department of Teleinformatics Engineering, Universidade Federal Do Ceará, Fortaleza, CE 60020-181, Brazil},
abstract={This letter presents a multiple slicing model for three-dimensional (3-D) images of human face, using the frontal, sagittal, and transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2-D spatial moments of Hu and 3-D spatial invariant rotation moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs nonneutral ROC I experiment, applying linear discriminant analysis as classifier and more than one sample for training, achieved 98.7% of verification rate at 0.1% of false acceptance rate. By using the support vector machine as classifier the rank1 experiment recognition rates of 99% and 95.4% have been achieved for a neutral vs neutral and for a neutral vs non-neutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when anger and disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity. © 2016 IEEE.},
author_keywords={Computer vision for automation;  recognition;  surveillance systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{ElSayed20181353,
author={ElSayed, A. and Kongar, E. and Mahmood, A. and Sobh, T.},
title={Unsupervised face recognition in the wild using high-dimensional features under super-resolution and 3D alignment effect},
journal={Signal, Image and Video Processing},
year={2018},
volume={12},
number={7},
pages={1353-1360},
doi={10.1007/s11760-018-1289-6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046724882&doi=10.1007%2fs11760-018-1289-6&partnerID=40&md5=857fb5c085b51ae7c6fd9bc5a34bcce3},
affiliation={221 University Ave, Bridgeport, CT  06604, United States},
abstract={Face recognition algorithms customarily utilize query faces captured from uncontrolled, in the wild, environments. The quality of these facial images is affected by various internal factors, including the quality of sensors used in outdoor cameras as well as external ones, such as the quality and direction of light. These factors adversely affect the overall quality of the captured images often causing blurring and/or low resolution, a phenomena commonly referred to as image degradation. Super-resolution algorithms are highly effective in improving the resolution of degraded images, more so if the captured face is small requiring scaling up. With this motivation, this research aims at demonstrating the effect of one of the state-of-the-art image super-resolution algorithms on the labeled faces in the wild (lfw) dataset. In this regard, several cases are analyzed to demonstrate the effectiveness of the super-resolution algorithm. Each case is then investigated independently comparing the order of execution before or after the 3D face alignment step. Following this, resulting images are tested on a closed set face recognition protocol using unsupervised algorithms with high-dimensional extracted features. The inclusion of super-resolution resulted in improvement in the recognition rate compared to unsupervised algorithm results reported in the literature. © 2018, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Face recognition;  High-dimensional features;  Label faces in the wild (lfw);  Super-resolution;  Unsupervised learning},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Dutta2018,
author={Dutta, K. and Bhattacharjee, D. and Nasipuri, M.},
title={TR-LBP: A modified Local Binary Pattem-based technique for 3D face recognition},
journal={Proceedings of 5th International Conference on Emerging Applications of Information Technology, EAIT 2018},
year={2018},
doi={10.1109/EAIT.2018.8470411},
art_number={8470411},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055526527&doi=10.1109%2fEAIT.2018.8470411&partnerID=40&md5=21057f38e0de811706653f9777ab8004},
affiliation={Department of Computer Science and Engineering, Jadavpur University, Kolkata, India},
abstract={In this paper, a novel technique has been introduced for 3D face recognition based on the modified local binary pattern extracted from a 3D range image. The new LBP technique is applied to shape index of 3D facial surface data. The novelty of this technique illustrates that a modified local binary pattern termed as Triangular Local Binary Pattern (TR-LBP), which gives new texture representation of 3D facial surface for improvement of facial texture classification performance compared to other variants of LBP. In this paper, authors have also described the TR-LBP technique by extending it on 2D intensity image of same subjects. Entropy-based feature extraction is used for feature vector creation. Further, KNN is used for calculating classification accuracy on two popular 3D face databases: Frav3D and Bosphorous. Here the classifications results are compared with other two existing LBP techniques applied to range images and shape index (SI) form of range image respectively. © 2018 IEEE.},
author_keywords={2D Intensity Image;  Entropy-based feature.;  Range Image;  Shape Index;  TR-LBP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kempfle2018,
author={Kempfle, J. and Van Laerhoven, K.},
title={Respiration rate estimation with depth cameras: An evaluation of parameters},
journal={ACM International Conference Proceeding Series},
year={2018},
doi={10.1145/3266157.3266208},
art_number={a4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055638420&doi=10.1145%2f3266157.3266208&partnerID=40&md5=a6d114d735ba9c53380a5377468d9a49},
affiliation={University of Siegen, North Rhine-Westphalia, Siegen, Germany},
abstract={Depth cameras have been known to be capable of picking up the small changes in distance from users' torsos, to estimate respiration rate. Several studies have shown that under certain conditions, the respiration rate from a non-mobile user facing the camera can be accurately estimated from parts of the depth data. It is however to date not clear, what factors might hinder the application of this technology in any setting, what areas of the torso need to be observed, and how readings are affected for persons at larger distances from the RGB-D camera. In this paper, we present a benchmark dataset that consists of the point cloud data from a depth camera, which monitors 7 volunteers at variable distances, for variable methods to pin-point the person's torso, and at variable breathing rates. Our findings show that the respiration signal's signal-to-noise ratio becomes debilitating as the distance to the person approaches 4 metres, and that bigger windows over the person's chest work particularly well. The sampling rate of the depth camera was also found to impact the signal's quality significantly. © 2018 Association for Computing Machinery.},
author_keywords={Kinect v2;  Non-contact measurement;  Respiration measurement;  Respiratory rate;  ToF sensing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liew2018105,
author={Liew, S.C. and Huang, X. and Lin, E.S. and Shi, C. and Yee, A.T.K. and Tandon, A.},
title={Integration of tree database derived from satellite imagery and LiDAR point cloud data},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2018},
volume={42},
number={4/W10},
pages={105-111},
doi={10.5194/isprs-archives-XLII-4-W10-105-2018},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056170398&doi=10.5194%2fisprs-archives-XLII-4-W10-105-2018&partnerID=40&md5=1e8199f1042e01555d0bd86203b981ab},
affiliation={Center for Remote Imaging, Sensing and Processing, National University of Singapore, Singapore; National Parks Board, Singapore},
abstract={3D tree database provides essential information of tree species abundance, spatial distribution and tree height for forest mapping, sustainable urban planning and 3D city modelling. Fusion of passive optical satellite imagery and active Lidar data can potentially be exploited for operational forest inventory. However, such fusion requires very high geometric accuracy for both data sets. This paper proposes an approach for 3D tree information extracted from passive and active data integrating into existing tree database by effectively using geometric information of satellite camera model and laser scanner scanning geometry. The paper also presents the individual methods for tree crown identification and delineation from satellite images and lidar point cloud data respectively, the geometric correction of tree position from tree top to tree base. The ground truth accuracy assessment for the tree extracted is also present. © Authors 2018. CC BY 4.0 License.},
author_keywords={Camera Model;  Canopy Height Model;  Lidar Point Cloud;  Tree Crown Identification and Delineation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Elsayed2018,
author={Elsayed, A. and Mahmood, A. and Sobh, T.},
title={Effect of super resolution on high dimensional features for unsupervised face recognition in the wild},
journal={Proceedings - Applied Imagery Pattern Recognition Workshop},
year={2018},
volume={2017-October},
doi={10.1109/AIPR.2017.8457967},
art_number={8457967},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057564222&doi=10.1109%2fAIPR.2017.8457967&partnerID=40&md5=5bfeb78c1dcfc5925ab065e0fb215e88},
affiliation={Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, CT  06604, United States},
abstract={Majority of the face recognition algorithms use query faces captured from uncontrolled, in the wild, environment. Because of cameras' limited capabilities, it is common for these captured facial images to be blurred or low resolution. Super resolution algorithms are therefore crucial in improving the resolution of such images especially when the image size is small and enlargement is required. This paper aims to demonstrate the effect of one of the state-of-the-art algorithms in the field of image super resolution. To demonstrate the functionality of the algorithm, various before and after 3D face alignment cases are provided using the images from the Labeled Faces in the Wild (lfw) dataset. Resulting images are subject to test on a closed set recognition protocol using unsupervised algorithms with high dimensional extracted features. The inclusion of super resolution algorithm resulted in significant improvement in recognition rate over recently reported results obtained from unsupervised algorithms on the same dataset. © 2017 IEEE.},
author_keywords={face recognition;  high dimensional features;  label faces in the wild (lfw);  Super-Resolution;  unsupervised learning},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Switonski2018415,
author={Switonski, A. and Krzeszowski, T. and Josinski, H. and Kwolek, B. and Wojciechowski, K.},
title={Gait recognition on the basis of markerless motion tracking and DTW transform},
journal={IET Biometrics},
year={2018},
volume={7},
number={5},
pages={415-422},
doi={10.1049/iet-bmt.2017.0134},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051943808&doi=10.1049%2fiet-bmt.2017.0134&partnerID=40&md5=517b82bdcc3d7a7339bb5e189710f78a},
affiliation={Faculty of Automatic Control, Electronics and Computer Science, Silesian University of Technology, ul. Akademicka 16, Gliwice, 44-100, Poland; Faculty of Electrical and Computer Engineering, Rzeszow University of Technology, ul. Wincentego Pola 2, Rzeszow, 35-959, Poland; Faculty of Computer Science, Electronics and Telecommunications, AGH University of Science and Technology, 30 Mickiewicza Av., Krakow, 30-059, Poland; Research and Development Center of Polish-Japanese Academy of Information Technology, Aleja Legionow 2, Bytom, 41-902, Poland},
abstract={In this study, a framework for view-invariant gait recognition on the basis of markerless motion tracking and dynamic time warping (DTW) transform is presented. The system consists of a proposed markerless motion capture system as well as introduced classification method of mocap data. The markerless system estimates the three-dimensional locations of skeleton driven joints. Such skeleton-driven point clouds represent poses over time. The authors align point clouds in every pair of frames by calculating the minimal sum of squared distances between the corresponding joints. A point cloud distance measure with temporal context has been utilised in k-nearest neighbours algorithm to compare time instants of motion sequences. To enhance the generalisation of the recognition and to shorten the processing time, for every individual a single multidimensional time series among several multidimensional time series describing the individual's gait is established. The correct classification rate has been determined on the basis of a real dataset of human gait. It contains 230 gait cycles of 22 subjects. The tracking results on the basis of markerless motion capture are referenced to Vicon system, whereas the achieved accuracies of recognition are compared with the ones obtained by DTW that is based on rotational data. © The Institution of Engineering and Technology 2018.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sihvo2018364,
author={Sihvo, S. and Virjonen, P. and Nevalainen, P. and Heikkonen, J.},
title={Tree Detection around Forest Harvester Based on Onboard LiDAR Measurements},
journal={Proceedings - 2018 Baltic Geodetic Congress, BGC-Geomatics 2018},
year={2018},
pages={364-367},
doi={10.1109/BGC-Geomatics.2018.00075},
art_number={8453724},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053891627&doi=10.1109%2fBGC-Geomatics.2018.00075&partnerID=40&md5=c9978eb75c38edd1ac09a48b8d293c14},
affiliation={Faculty of Science and Engineering, Turku University, Turku, Finland},
abstract={This paper proposes a new approach for the detection of tree locations around forest machines producing a situational model based on on-site terrestrial LiDAR data collected during harvesting operation. A triangularized ground model is used to planarize the point cloud in order to simplify the tree detection. The planarized ground makes the vertical cutting of the point cloud systematical. Tree stem lines detected from individual trees at individual scan views are used to guide the final alignment into global coordinates. The setup is numerically efficient and does not rely on any positioning and orientation system (POS) based e.g. on an inertial measurement unit (IMU) or global navigation satellite system (GNSS) or wheel rotation counter on the autonomous vehicle. © 2018 IEEE.},
author_keywords={Autonomous vehicles forestry;  Collision avoidance;  Object recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Alqahtani2018,
author={Alqahtani, F. and Chandran, V. and Banks, J.},
title={3D Face Tracking Using Stereo Camera},
journal={1st International Conference on Computer Applications and Information Security, ICCAIS 2018},
year={2018},
doi={10.1109/CAIS.2018.8441988},
art_number={8441988},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053554349&doi=10.1109%2fCAIS.2018.8441988&partnerID=40&md5=bff2dd175e861cd18ea97ac45749ed96},
affiliation={Queensland University of Technology, Brisbane, Australia},
abstract={The content of this article explores the use of 3D face tracking systems by implementing of active stereo vision cameras to ascertain the position of a person's face. The various experiments conducted in-depth have produced both promising and satisfying results for images that have enabled examiners to determine the disparity between images. The paper also explores some of the various challenges researchers are facing with the implementation of algorithms to construct cloud-points in from stereo-based images. The reviewed recommendations suggest on better software components that would avail final 3D computational images or reconstructions that can easily be matched to the original. The tracking system modules address the challenges of the practical application of face tracking including pose illumination and occlusion. The content of the paper evaluates the putting into practice of multi-view or multiple stereo cameras enhance the field of view to improve the performance of a 3D tracking system. Face tracking using functional multi-view stereo camera systems can significantly solve the correspondence problem and the issue of comparing scenes or image points given that extra views reduced ambiguity in matching. © 2018 IEEE.},
author_keywords={3D;  and stereo camera;  face tracking;  stereo vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo2018,
author={Guo, X. and Jin, Y. and Li, Y. and Xing, J. and Lang, C.},
title={Stabilizing video facial landmark detection and tracking via global and local filtering},
journal={ACM International Conference Proceeding Series},
year={2018},
doi={10.1145/3240876.3240913},
art_number={3240913},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055673775&doi=10.1145%2f3240876.3240913&partnerID=40&md5=f8fdac8dafcfcc1f4f5b69c6b0b145b5},
affiliation={School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China},
abstract={Video facial landmark detection and tracking are important computer vision tasks with many applications such as face anti-spoofing, animation and recognition. Most of existing facial landmark detection and tracking methods, however, often suffer from the instability issue which will affect their effectiveness in real world applications. In this work we present a novel solution for stabilized facial landmark detection and tracking in video frames. The proposed solution addresses various kinds of challenging situations and provides effective remedies to solve them. The main contribution within our solution is a novel global and local filtering strategy, which guarantees the robustness of global whole facial shape tracking and the adaptivity of local facial parts tracking. The proposed solution does not depend on specific face detection and alignment algorithms, thus can be easily deployed into existing systems. Extensive experimental evaluations and analyses on different benchmark datasets verify the effectiveness of the proposed approach. © 2018 ACM.},
author_keywords={3D face model;  Landmark detection;  Landmark smoothing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Abbad2018525,
author={Abbad, A. and Abbad, K. and Tairi, H.},
title={3D face recognition: Multi-scale strategy based on geometric and local descriptors},
journal={Computers and Electrical Engineering},
year={2018},
volume={70},
pages={525-537},
doi={10.1016/j.compeleceng.2017.08.017},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028606046&doi=10.1016%2fj.compeleceng.2017.08.017&partnerID=40&md5=5414b01946f345f018c49654dff50957},
affiliation={LIIAN, Department of Computer Science, Faculty of Sciences Dhar El Mahraz University Sidi Mohamed Ben Abdelah, Fez, Morocco; LISA, Department of Computer Science, Faculty of Science and Technology University Sidi Mohamed Ben Abdelah, Fez, Morocco},
abstract={Most human expression variations cause a non-rigid deformation of face scans, which is a challenge today. In this article, we present a novel framework for 3D face recognition that uses a geometry and local shape descriptor in a matching process to overcome the distortions caused by expressions in faces. This algorithm consists of four major components. First, the 3D face model is presented at different scales. Second, isometric-invariant features on each scale are extracted. Third, the geometric information is obtained on the 3D surface in terms of radial and level facial curves. Fourth, the feature vectors on each scale are concatenated with their corresponding geometric information. We conducted a number of experiments using two well-known and challenging datasets, namely, the GavabDB and Bosphorus datasets, and superior recognition performance has been achieved. The new system displays an overall rank-1 identification rate of 98.9% for all faces with neutral and non-neutral expressions on the GavabDB database. © 2017 Elsevier Ltd},
author_keywords={3D face recognition;  Expression;  Facial curves;  Geometric features;  Local features},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chouchane201820697,
author={Chouchane, A. and Ouamane, A. and Boutellaa, E. and Belahcene, M. and Bourennane, S.},
title={3D face verification across pose based on euler rotation and tensors},
journal={Multimedia Tools and Applications},
year={2018},
volume={77},
number={16},
pages={20697-20714},
doi={10.1007/s11042-017-5478-z},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038398719&doi=10.1007%2fs11042-017-5478-z&partnerID=40&md5=12acec4e42338aaa06dc62d88e8d5184},
affiliation={Université de Biskra, Biskra, Algeria; Center for Machine Vision and Signal Analysis, University of Oulu, Oulu, Finland; Institut Fresnel, Université de Marseille, Marseille, France},
abstract={In this paper, we propose a new approach for 3D face verification based on tensor representation. Face challenges, such as illumination, expression and pose, are modeled as a multilinear algebra problem where facial images are represented as high order tensors. Particularly, to account for head pose variations, several pose scans are generated from a single depth image using Euler transformation. Multi-bloc local phase quantization (MB-LPQ) histogram features are extracted from depth face images and arranged as a third order tensor. The dimensionality of the tensor is reduced based on the higher-order singular value decomposition (HOSVD). HOSVD projects the input tensor in a new subspace in which the dimension of each tensor mode is reduced. To discriminate faces of different persons, we utilize the Enhanced Fisher Model (EFM). Experimental evaluations on CASIA-3D database, which contains large head pose variations, demonstrate the effectiveness of the proposed approach. A verification rate of 98.60% is obtained. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={3D face verification;  Euler angles;  Multilinear dimensionality reduction;  Tensor analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dagnes2018789,
author={Dagnes, N. and Vezzetti, E. and Marcolin, F. and Tornincasa, S.},
title={Occlusion detection and restoration techniques for 3D face recognition: a literature review},
journal={Machine Vision and Applications},
year={2018},
volume={29},
number={5},
pages={789-813},
doi={10.1007/s00138-018-0933-z},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046038922&doi=10.1007%2fs00138-018-0933-z&partnerID=40&md5=cd843f734cfadbdf41fb70008a766f8d},
affiliation={Department of Management and Production Engineering, Politecnico di Torino, Corso Duca degli Abruzzi 24, Turin, 10129, Italy},
abstract={Methodologies for 3D face recognition which work in the presence of occlusions are core for the current needs in the field of identification of suspects, as criminals try to take advantage of the weaknesses among the implemented security systems by camouflaging themselves and occluding their face with eyeglasses, hair, hands, or covering their face with scarves and hats. Recent occlusion detection and restoration strategies for recognition purposes of 3D partially occluded faces with unforeseen objects are here presented in a literature review. The research community has worked on face recognition systems under controlled environments, but uncontrolled conditions have been investigated in a lesser extent. The paper details the experiments and databases used to handle the problem of occlusion and the results obtained by different authors. Lastly, a comparison of various techniques is presented and some conclusions are drawn referring to the best outcomes. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={3D face analysis;  Face detection;  Face recognition;  Facial occlusions;  Facial restoration},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mohammadi2018,
author={Mohammadi, S. and Gervei, O.},
title={Using nonlocal filtering and feature extraction approaches in three-dimensional face recognition by Kinect},
journal={International Journal of Advanced Robotic Systems},
year={2018},
volume={15},
number={4},
doi={10.1177/1729881418787743},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052601093&doi=10.1177%2f1729881418787743&partnerID=40&md5=214c3867dc2c09e2681c6c98d065912e},
affiliation={Department of Electrical Engineering, University of Zanjan, Zanjan, Iran},
abstract={To use low-cost depth sensors such as Kinect for three-dimensional face recognition with an acceptable rate of recognition, the challenges of filling up nonmeasured pixels and smoothing of noisy data need to be addressed. The main goal of this article is presenting solutions for aforementioned challenges as well as offering feature extraction methods to reach the highest level of accuracy in the presence of different facial expressions and occlusions. To use this method, a domestic database was created. First, the noisy pixels-called holes-of depth image is removed by solving multiple linear equations resulted from the values of the surrounding pixels of the holes. Then, bilateral and block matching 3-D filtering approaches, as representatives of local and nonlocal filtering approaches, are used for depth image smoothing. Curvelet transform as a well-known nonlocal feature extraction technique applied on both RGB and depth images. Two unsupervised dimension reduction techniques, namely, principal component analysis and independent component analysis, are used to reduce the dimension of extracted features. Finally, support vector machine is used for classification. Experimental results show a recognition rate of 90% for just depth images and 100% when combining RGB and depth data of a Kinect sensor which is much higher than other recently proposed algorithms. © The Author(s) 2018.},
author_keywords={block matching 3-D filter;  curvelet transform;  hole filling;  low-cost depth sensor Kinect;  Three-dimensional face recognition;  unsupervised dimension reduction},
document_type={Article},
source={Scopus},
}

@ARTICLE{Huang201879,
author={Huang, D. and Du, S. and Li, G. and Zhao, C. and Deng, Y.},
title={Detection and monitoring of defects on three-dimensional curved surfaces based on high-density point cloud data},
journal={Precision Engineering},
year={2018},
volume={53},
pages={79-95},
doi={10.1016/j.precisioneng.2018.03.001},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043974843&doi=10.1016%2fj.precisioneng.2018.03.001&partnerID=40&md5=8d1c655e4763e68c78898cdd76f7b6cf},
affiliation={State Key Lab of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, 200240, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China},
abstract={The surface quality of three-dimensional (3-D) curved surfaces is one of the most important factors that can directly influence the performance of the final product. This paper presents a systematic approach for detection and monitoring of defects on 3-D curved surfaces based on high-density point cloud data. Firstly, an algorithm to remove outliers and a boundary recognition algorithm are proposed to divide the entire 3-D curved surface including millions of measured points into multiple sub-regions. Secondly, two new evaluation indexes based on wavelet packet entropy and normal vector are explored to represent the features of the multiple sub-regions to determine whether the sub-regions are out-of-limit (OOL) of specifications. Thirdly, three quality parameters representing quality characteristics of a curved surface are presented and their values are calculated based on the clusters of OOL sub-regions. Finally, three individual control charts are presented to monitor the three quality parameters. As long as any quality parameter is out of the control range, the manufacturing process of the curved surface is determined to be out-of-control (OOC). The results of a case study show that the proposed approach can effectively identify the OOC manufacturing process and detect defects on 3-D curved surfaces. © 2018 Elsevier Inc.},
author_keywords={Defect detection and monitoring;  High-density point cloud data;  Three-dimensional curved surface},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fang2018391,
author={Fang, Q. and Kyrarini, M. and Gräser, A. and Ristić-Durrant, D.},
title={RGB-D camera based 3D human mouth detection and tracking towards robotic feeding assistance},
journal={ACM International Conference Proceeding Series},
year={2018},
pages={391-396},
doi={10.1145/3197768.3201576},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049907106&doi=10.1145%2f3197768.3201576&partnerID=40&md5=51b1e19e4ad84cb6dd3f2152ef771113},
affiliation={IAT, University of Bremen, Germany},
abstract={In this paper, an RGB-D camera based framework for the recognition and tracking of the human mouth for the purpose of autonomous r obotic feeding is presented. The method employs the state-of-the-art face detection algorithm to acquire the 2D facial landmarks, and the corresponding 3D position of the human mouth is calculated using the depth information. In addition, a 3D point cloud visualizer of the human face with marked facial landmarks is provided for the purpose of visualization. The proposed system is applied in real-time vision-based robot control. Experiments indicate the validity of the proposed work in localising the mouth of different subjects served by the robot with a cup of water. © 2018 Association for Computing Machinery.},
author_keywords={3D point cloud;  Assistive robotics;  Mouth detection;  RGB-D camera;  Robot control},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Geetha20181882,
author={Geetha, G. and Safa, M. and Fancy, C. and Chittal, K.},
title={3D face recognition using Hadoop},
journal={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing, ICECDS 2017},
year={2018},
pages={1882-1885},
doi={10.1109/ICECDS.2017.8389776},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050073609&doi=10.1109%2fICECDS.2017.8389776&partnerID=40&md5=426f7e7f8b23f4019df744e279422d63},
affiliation={Department of Information Technology, SRM University, India; Department of Mechatronics, SRM University, India},
abstract={Face Recognition is one of the biometric technique to vestige the given faces. We present a 3D face recognition method using Hadoop to recognize 3D faces under varying expressions, lighting and different poses to overcome the challenges of the 2D face recognition. In this paper, a threshold facial region of an image is detected and preprocessing is done based through the image excellence. If the selected face is frontal face with good lighting, extract the prerequisite features and do the necessary comparison steps to recognize the faces. In case, if the selected face is in bad lighting, then perform histogram equalization and normalization to increase the contrast. Different Poses and expressions are the very challenging zones which require surplus pre-processing to improve the performance of the face recognition system. Hence an enhanced normalization method called 3D Morphable Model are used as a pre- processing technique to create a frontal view from a non-frontal view and also merge images with different views in to a single frontal view. Next to diminish the number of features used for recognition process; we emulate the linear discriminant analysis method for further classification. Eventually, we used an open-source Hadoop Image Processing Interface (HIPI) to act as an interface for MapReduce technology for recognition. © 2017 IEEE.},
author_keywords={Hadoop;  Image Processing;  Linear Discriminant analysis;  Map Reduce},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Desai201840,
author={Desai, K. and Prabhakaran, B. and Raghuraman, S.},
title={Combining skeletal poses for 3D human model generation using multiple kinects},
journal={Proceedings of the 9th ACM Multimedia Systems Conference, MMSys 2018},
year={2018},
pages={40-51},
doi={10.1145/3204949.3204958},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050671247&doi=10.1145%2f3204949.3204958&partnerID=40&md5=2d562fce2168625ce0dae3f784d9bdde},
affiliation={University of Texas at Dallas, Richardson, TX, United States; Mobiweb Inc, Plano, TX, United States},
abstract={RGB-D cameras, such as the Microsoft Kinect, provide us with the 3D information, color and depth, associated with the scene. Interactive 3D Tele-Immersion (i3DTI) systems use such RGB-D cameras to capture the person present in the scene in order to collaborate with other remote users and interact with the virtual objects present in the environment. Using a single camera, it becomes difficult to estimate an accurate skeletal pose and complete 3D model of the person, especially when the person is not in the complete view of the camera. With multiple cameras, even with partial views, it is possible to get a more accurate estimate of the skeleton of the person leading to a better and complete 3D model. In this paper, we present a real-time skeletal pose identification approach that leverages on the inaccurate skeletons of the individual Kinects, and provides a combined optimized skeleton. We estimate the Probability of an Accurate Joint (PAJ) for each joint from all of the Kinect skeletons. We determine the correct direction of the person and assign the correct joint sides for each skeleton. We then use a greedy consensus approach to combine the highly probable and accurate joints to estimate the combined skeleton. Using the individual skeletons, we segment the point clouds from all the cameras. We use the already computed PAJ values to obtain the Probability of an Accurate Bone (PAB). The individual point clouds are then combined one segment after another using the calculated PAB values. The generated combined point cloud is a complete and accurate 3D representation of the person present in the scene. We validate our estimated skeleton against two well-known methods by computing the error distance between the best view Kinect skeleton and the estimated skeleton. An exhaustive analysis is performed by using around 500000 skeletal frames in total, captured using 7 users and 7 cameras. Visual analysis is performed by checking whether the estimated skeleton is completely present within the human model. We also develop a 3D Holo-Bubble game to showcase the real-time performance of the combined skeleton and point cloud. Our results show that our method performs better than the state-of-the-art approaches that use multiple Kinects, in terms of objective error, visual quality and real-time user performance. © 2018 Association for Computing Machinery.},
author_keywords={3D Model;  Combined Skeleton;  Interactive 3D Tele-Immersion;  Point Cloud Combination},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Deng2018399,
author={Deng, J. and Zhou, Y. and Cheng, S. and Zaferiou, S.},
title={Cascade multi-view hourglass model for robust 3D face alignment},
journal={Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018},
year={2018},
pages={399-403},
doi={10.1109/FG.2018.00064},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049385118&doi=10.1109%2fFG.2018.00064&partnerID=40&md5=5b389fcd3588c281f4275e77ad3e9537},
affiliation={Department of Computing, Imperial College London, United Kingdom; Centre for Machine Vision and Signal Analysis, University of Oulu, Finland},
abstract={Estimating the 3D facial landmarks from a 2D image remains a challenging problem. Even though state-of-the-art 2D alignment methods are able to predict accurate landmarks for semi-frontal faces, the majority of them fail to provide semantically consistent landmarks for profile faces. A de facto solution to this problem is through 3D face alignment that preserves correspondence across different poses. In this paper, we proposed a Cascade Multi-view Hourglass Model for 3D face alignment, where the first Hourglass model is explored to jointly predict semi-frontal and profile 2D facial landmarks, after removing spatial transformations, another Hourglass model is employed to estimate the 3D facial shapes. To improve the capacity without sacrificing the computational complexity, the original residual bottleneck block in the Hourglass model is replaced by a parallel, multi-scale inception-resnet block. Extensive experiments on two challenging 3D face alignment datasets, AFLW2000-3D and Menpo-3D, show the robustness of the proposed method under continuous pose changes. © 2018 IEEE.},
author_keywords={3D Face Alignment;  Cascade Hourglass;  Multiview},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tian2018774,
author={Tian, W. and Liu, F. and Zhao, Q.},
title={Landmark-based 3D face reconstruction from an arbitrary number of unconstrained images},
journal={Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018},
year={2018},
pages={774-779},
doi={10.1109/FG.2018.00122},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049398138&doi=10.1109%2fFG.2018.00122&partnerID=40&md5=4605f8bf2c7a7d4fa0a9f9423081c87a},
affiliation={College of Computer Science, Sichuan University, Chengdu, China},
abstract={In this paper, we propose a novel method for reconstructing 3D faces from 2D images. The method is characterized in three aspects. (i) It utilizes only geometric cues in the input images, i.e., 2D facial landmarks. (ii) It works for an arbitrary number of unconstrained images, both single and multiple images. (iii) It can effectively exploit complementary information in multiple images of varying poses and expressions. The method is implemented based on cascaded regression in shape space. We have evaluated the method on three databases and observed from the experimental results that (i) the reconstruction error is reduced as more images of different poses are used, (ii) the proposed method can obtain comparable reconstruction results by using state-of-the-art automated methods to detect the 2D landmarks, and (iii) the proposed method is robust to variations in facial expressions and image qualities. © 2018 IEEE.},
author_keywords={3D face reconstruction;  Cascade regression;  Landmark based;  Unconstrained images},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Feng2018780,
author={Feng, Z.-H. and Huber, P. and Kittler, J. and Hancock, P. and Wu, X.-J. and Zhao, Q. and Koppen, P. and Raetsch, M.},
title={Evaluation of dense 3D reconstruction from 2D face images in the wild},
journal={Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018},
year={2018},
pages={780-786},
doi={10.1109/FG.2018.00123},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049408233&doi=10.1109%2fFG.2018.00123&partnerID=40&md5=0b33a514f5ee5594a6e79a758eb02c5b},
affiliation={Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, GU2 7XH, United Kingdom; Faculty of Natural Sciences, University of Stirling, Stirling, FK9 4LA, United Kingdom; School of Internet of Things Engineering, Jiangnan University, Wuxi, 214122, China; Biometrics Research Lab, College of Computer Science, Sichuan University, Chengdu, 610065, China; Image Understanding and Interactive Robotics, Reutlingen University, Reutlingen, 72762, Germany},
abstract={This paper investigates the evaluation of dense 3D face reconstruction from a single 2D image in the wild. To this end, we organise a competition that provides a new benchmark dataset that contains 2000 2D facial images of 135 subjects as well as their 3D ground truth face scans. In contrast to previous competitions or challenges, the aim of this new benchmark dataset is to evaluate the accuracy of a 3D dense face reconstruction algorithm using real, accurate and high-resolution 3D ground truth face scans. In addition to the dataset, we provide a standard protocol as well as a Python script for the evaluation. Last, we report the results obtained by three state-of-the-art 3D face reconstruction systems on the new benchmark dataset. The competition is organised along with the 2018 13th IEEE Conference on Automatic Face Gesture Recognition. © 2018 IEEE.},
author_keywords={3D Dense Face Reconstruction;  3D Morphable Face Model;  Evaluation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Morales2018731,
author={Morales, A. and Piella, G. and Martinez, O. and Sukno, F.M.},
title={A quantitative comparison of methods for 3D face reconstruction from 2D images},
journal={Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018},
year={2018},
pages={731-738},
doi={10.1109/FG.2018.00115},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049385237&doi=10.1109%2fFG.2018.00115&partnerID=40&md5=6b249117c3ee7cc84f5a5a1689626fad},
affiliation={Department of Information and Communication Technologies, Pompeu Fabra University, Barcelona, Spain},
abstract={In the past years, many studies have highlighted the relation between deviations from normal facial morphology (dysmorphology) and some genetic and mental disorders. Recent advances in methods for reconstructing the 3D geometry of the face from 2D images opens new possibilities for dysmorphology research without the need for specialized 3D imaging equipment. However, it is unclear whether these methods could reconstruct the facial geometry with the required accuracy. In this paper we present a comparative study of some of the most relevant approaches for 3D face reconstruction from 2D images, including photometric-stereo, deep learning and 3D Morphable Model fitting. We address the comparison in qualitatively and quantitatively terms using a public database consisting of 2D images and 3D scans from 100 people. Interestingly, we find that some methods produce quite noisy reconstructions that do not seem realistic, whereas others look more natural. However, the latter do not seem to adequately capture the geometric variability that exists between different subjects and produce reconstructions that look always very similar across individuals, thus questioning their fidelity. © 2018 IEEE.},
author_keywords={3D face reconstruction;  3D Morphable Model;  Craniofacial geometry;  Deep learning;  Photometric stereo},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jan2018466,
author={Jan, A. and Ding, H. and Meng, H. and Chen, L. and Li, H.},
title={Accurate facial parts localization and deep learning for 3D facial expression recognition},
journal={Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018},
year={2018},
pages={466-472},
doi={10.1109/FG.2018.00075},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049391384&doi=10.1109%2fFG.2018.00075&partnerID=40&md5=a05c40410dbf77d812432ec5e96d8d91},
affiliation={Department of Electronic and Computer Engineering, Brunel University London, United Kingdom; LIRIS Laboratory UMR CNRS 5205, Ecole Centrale de Lyon, France; School of Mathematics and Statistics, Xi'An Jiaotong University, China},
abstract={Meaningful facial parts can convey key cues for both facial action unit detection and expression prediction. Textured 3D face scan can provide both detailed 3D geometric shape and 2D texture appearance cues of the face which are beneficial for Facial Expression Recognition (FER). However, accurate facial parts extraction as well as their fusion are challenging tasks. In this paper, a novel system for 3D FER is designed based on accurate facial parts extraction and deep feature fusion of facial parts. Experiments are conducted on the BU-3DFE database, demonstrating the effectiveness of combing different facial parts, texture and depth cues and reporting the state-of-the-art results in comparison with all existing methods under the same setting. © 2018 IEEE.},
author_keywords={Affective Computing;  Facial expression recognition;  Human-Computer Interaction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2018210,
author={Zhang, G. and Han, H. and Shan, S. and Song, X. and Chen, X.},
title={Face alignment across large pose via MT-CNN Based 3D shape reconstruction},
journal={Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018},
year={2018},
pages={210-217},
doi={10.1109/FG.2018.00039},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049387103&doi=10.1109%2fFG.2018.00039&partnerID=40&md5=22410b5669f2222f13d1b7b0186edc02},
affiliation={Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China; CAS Center for Excellence in Brain Science and Intelligence Technology, China; Huawei Technologies Co., Ltd, China},
abstract={Face alignment plays an important role for robust face recognition and analysis applications in the wild. While a number of face alignment methods are available, large-pose face alignment remains a very challenging problem due to the ambiguity of facial keypoints in 2D face images. Recent attempts to solve this problem via 3D model fitting show more robustness against large poses and 2D ambiguity, but their accuracy and speed are still limited. We propose a 3D reconstruction based method to quickly and accurately detect 2D facial landmarks and estimate their visibilities. By designing a cascaded multi-task CNN model, we can efficiently reconstruct the 3D face shape, together with pose estimation as an auxiliary task. Finally, the landmarks on 3D shape are projected to the 2D face image to get the 2D landmarks and their visibilities. Experimental results on the challenging 300W-LP, AFLW2000-3D, and AFLW databases show that the proposed approach can be comparable with the state-of-the-art methods and is able to run in real time (32ms per image) on 3.4 GHz CPU. © 2018 IEEE.},
author_keywords={3D shape regression;  Landmark detection;  Large-pose face alignment;  Multitask CNN},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gao2018120,
author={Gao, J. and Evans, A.N.},
title={Expression robust 3D face landmarking using thresholded surface normals},
journal={Pattern Recognition},
year={2018},
volume={78},
pages={120-132},
doi={10.1016/j.patcog.2018.01.011},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042375361&doi=10.1016%2fj.patcog.2018.01.011&partnerID=40&md5=a93105edc5d4f49296e9c43d397e9898},
affiliation={Department of Medical Biochemistry and Microbiology, Uppsala University, Uppsala, Sweden; Department of Electronic and Electrical Engineering, University of Bath, Bath, United Kingdom},
abstract={3D face recognition is an increasing popular modality for biometric authentication, for example in the iPhoneX. Landmarking plays a significant role in region based face recognition algorithms. The accuracy and consistency of the landmarking will directly determine the effectiveness of feature extraction and hence the overall recognition performance. While surface normals have been shown to provide high performing features for face recognition, their use in landmarking has not been widely explored. To this end, a new 3D facial landmarking algorithm based on thresholded surface normal maps is proposed, which is applicable to widely used 3D face databases. The benefits of employing surface normals are demonstrated for both facial roll and yaw rotation calibration and nasal landmarks localization. Results on the Bosphorus, FRGC and BU-3DFE databases show that the detected landmarks possess high within-class consistency and accuracy under different expressions. For several key landmarks the performance achieved surpasses that of state-of-the-art techniques and is also training free and computationally efficient. The use of surface normals therefore provides a useful representation of the 3D surface and the proposed landmarking algorithm provides an effective approach to localising the key nasal landmarks. © 2018 Elsevier Ltd},
author_keywords={3D face landmarking;  Surface normals},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hada20181,
author={Hada, M. and Yamada, R. and Akamatsu, S.},
title={How does the transformation of an avatar face giving a favorable impression affect human recognition of the face?},
journal={2018 International Workshop on Advanced Image Technology, IWAIT 2018},
year={2018},
pages={1-3},
doi={10.1109/IWAIT.2018.8369655},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048763668&doi=10.1109%2fIWAIT.2018.8369655&partnerID=40&md5=2d97f418d7a8180b8da4cc0896a56d73},
affiliation={Hosei University, 3-7-2 Kajino-cho, Koganei-shi, Tokyo, 184-8584, Japan},
abstract={We investigated how different appearances in the favorable impressions of 3D avatar faces affect face-recognition performances by humans. We conducted an encoding and testing experiment using synthesized facial images and artificially manipulated the strength of the perceived impressions in three different dimensions. We also subjectively assessed the favorability of the synthesized faces that were used as visual stimuli in face-recognition tests and found that facial transformation, which decreased the favorability impressions, generally deteriorates human face-recognition performance. © 2018 IEEE.},
author_keywords={face memory;  facial impression manipulation;  morphable 3D face model;  Social impression of face;  Thurston's paired comparison},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mayr2018691,
author={Mayr, A. and Rutzinger, M. and Geitner, C.},
title={Multitemporal analysis of objects in 3D point clouds for landslide monitoring},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2018},
volume={42},
number={2},
pages={691-697},
doi={10.5194/isprs-archives-XLII-2-691-2018},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048356667&doi=10.5194%2fisprs-archives-XLII-2-691-2018&partnerID=40&md5=54120583b761740bb74e66a716ea4370},
affiliation={Institute of Geography, University of Innsbruck, Innsbruck, Austria; Institute for Interdisciplinary Mountain Research, Austrian Academy of Sciences, Innsbruck, Austria},
abstract={To date multi-temporal 3D point clouds from close-range sensing are used for landslide and erosion monitoring in an operational manner. Morphological changes are typically derived by calculating distances between points from different acquisition epochs. The identification of the underlying processes resulting in surface changes, however, is often challenging, for example due to the complex surface structures and influences from seasonal vegetation dynamics. We present an approach for object-based 3D landslide monitoring based on topographic LiDAR point cloud time series separating specific surface change types automatically. The workflow removes vegetation and relates surface changes derived from a point cloud time series directly to (i) geomorphological object classes (landslide scarp, eroded area, deposit) and (ii) to individual, spatially contiguous objects (such as parts of the landslide scarp and clods of material moving in the landslide). We apply this approach to a time series of nine point cloud epochs from a slope affected by two shallow landslides. A parameter test addresses the influence of the registration error and the associated level of detection on the magnitude of derived object changes. The results of our case study are in accordance with field observations at the test site as well as conceptual landslide models, where retrogressive erosion of the scarp and downslope movement of the sliding mass are major principles of secondary landslide development. We conclude that the presented methods are well suited to extract information on geomorphological process dynamics from the complex point clouds and aggregate it at different levels of abstraction to assist landslide and erosion assessment. © Authors 2018. CC BY 4.0 License.},
author_keywords={Change detection;  Erosion;  Geomorphology;  Object-based analysis;  Terrestrial laser scanning;  Topographic LiDAR},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bessaoudi20181,
author={Bessaoudi, M. and Belahcene, M. and Ouamane, A. and Bourennane, S.},
title={A novel approach based on high order tensor and multi-scale locals features for 3D face recognition},
journal={2018 4th International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2018},
year={2018},
pages={1-5},
doi={10.1109/ATSIP.2018.8364461},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048474613&doi=10.1109%2fATSIP.2018.8364461&partnerID=40&md5=a3d3238410015813c361264d49d4dbc9},
affiliation={LI3C Laboratory, Department of Electrical Engineering, University of Biskra, BP 145 RP, Biskra, 07000, Algeria; Institut Fresnel, UMR CNRS 7249, Ecole Centrale Marseille, France},
abstract={This paper presents an efficient framework for verification using 3D information based on high order tensor representation in uncontrolled conditions. The 3D depth images are subdivided into sub-blocks and the Multi-Scale Local Binarised Statistical Image Features (MSBSIF) + Multi-Scale local phase quantization (MSLPQ) histograms are extracted and concatenated from each block and organized as a 3rd order tensor. Moreover, two steps of dimensionally reduction to the face tensor are used. Firstly, Multilinear Principal Component Analysis (MPCA) is used to project the face tensor in a new subspace features in which the dimension of each mode tensor is reduced. After that, Enhanced Fisher Model (EFM) is applied to discriminate the faces of diverse persons in the database. Finally, the corresponding is achieved based distance measurement. The proposed approach (MPCA+EFM) has been evaluated on the challenging face database Bosporus 3D. The experimental results demonstrate that our method attains a high authentication performance. © 2018 IEEE.},
author_keywords={3D face Authentication;  Locals Features;  Multilinear subspace projection;  Tensor representation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ahdid201873,
author={Ahdid, R. and Taifi, K. and Said, S. and Fakir, M. and Manaut, B.},
title={Automatic face recognition system using iso-geodesic curves in riemanian manifold},
journal={Proceedings - 2017 14th International Conference on Computer Graphics, Imaging and Visualization, CGiV 2017},
year={2018},
pages={73-78},
doi={10.1109/CGiV.2017.25},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048323921&doi=10.1109%2fCGiV.2017.25&partnerID=40&md5=73b9e65d367cee00cdcb28e7b5ef55cb},
affiliation={Sultan Moulay Slimane University, Beni Mellal, Morocco},
abstract={In this paper, we present an automatic 3D face recognition system. This system is based on the representation of human faces surfaces as collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two facial surfaces, we compute a geodesic distance between a pair of facial curves using a Riemannian geometry. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008). © 2017 IEEE.},
author_keywords={3D face recognition;  facial surfaces;  geodesic distance;  iso-geodesic curves;  Riemannian geometry},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kowalski2018141,
author={Kowalski, M. and Nasarzewski, Z. and Galinski, G. and Garbat, P.},
title={HoloFace: Augmenting Human-to-Human Interactions on HoloLens},
journal={Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
year={2018},
volume={2018-January},
pages={141-149},
doi={10.1109/WACV.2018.00022},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050958372&doi=10.1109%2fWACV.2018.00022&partnerID=40&md5=258f1fd7bd664c48518e6bd97cf3e54c},
affiliation={Warsaw University of Technology, Poland},
abstract={We present HoloFace, an open-source framework for face alignment, head pose estimation and facial attribute retrieval for Microsoft HoloLens. HoloFace implements two state-of-the-art face alignment methods which can be used interchangeably: one running locally and one running on a remote backend. Head pose estimation is accomplished by fitting a deformable 3D model to the landmarks localized using face alignment. The head pose provides both the rotation of the head and a position in the world space. The parameters of the fitted 3D face model provide estimates of facial attributes such as mouth opening or smile. Together the above information can be used to augment the faces of people seen by the HoloLens user, and thus their interaction. Potential usage scenarios include facial recognition, emotion recognition, eye gaze tracking and many others. We demonstrate the capabilities of our framework by augmenting the faces of people seen through the HoloLens with various objects and animations. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Deng2018426,
author={Deng, W. and Hu, J. and Wu, Z. and Guo, J.},
title={From one to many: Pose-Aware Metric Learning for single-sample face recognition},
journal={Pattern Recognition},
year={2018},
volume={77},
pages={426-437},
doi={10.1016/j.patcog.2017.10.020},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032363096&doi=10.1016%2fj.patcog.2017.10.020&partnerID=40&md5=4a5ba797e5c3e9fd126ffc40d52efbd0},
affiliation={School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China},
abstract={Pose and illumination variations are very challenging for face recognition with a single sample per person (SSPP). In this paper, we address this issue by a Pose-Aware Metric Learning (PAML) approach. Our primary idea is “from one to many”: Synthesizing many images of sufficient pose and illumination variability from the single training image, based on which metric learning approach is applied to reduce these “synthesized” variations at each quantified pose. For this purpose, given a single frontal training image, a multi-depth generic elastic model and an extended generic elastic model are developed to synthesize facial images of the target pose with varying 3D shape (depth) and illumination variations respectively. To reduce these “synthesized” variability, Pose-Aware Metric spaces are separately learnt by linear regression analysis at each quantized pose, and pose-invariant recognition is performed in the corresponding metric space. By preserving the detailed texture and reducing the shape variability, the PAML method achieves an 100% accuracy on the Multi-PIE database under the test setting across poses, which is significantly better than the traditional methods that use a large generic image ensemble to learn the cross-pose transformations. On the more challenging setting across both poses and illuminations, PAML outperforms the recent deep learning approaches by over 10% accuracy. © 2017 Elsevier Ltd},
author_keywords={3D face construction;  3D generic elastic model;  Face re-rendering;  Face recognition;  Metric learning;  Single sample per person},
document_type={Article},
source={Scopus},
}

@CONFERENCE{AlQahtani201828,
author={AlQahtani, F. and Banks, J. and Chandran, V. and Zhang, J.},
title={Three-dimensional head pose estimation using a stereo camera arrangement},
journal={ACM International Conference Proceeding Series},
year={2018},
volume={Part F137705},
pages={28-35},
doi={10.1145/3220511.3220522},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053684284&doi=10.1145%2f3220511.3220522&partnerID=40&md5=b0d1c7a7d932c95512b13e71cede2a15},
affiliation={Science and Engineering Faculty, Queensland University of Technology, Brisbane, Australia},
abstract={Head-pose estimation is a crucial component for analysing human behaviour through various 2D and 3D applications. However, the usage of the strategies based on 2D technologies is not very effective, as the sources of data are limited. In contrast, the usage of the strategies based on 3D technologies is a promising area. The 3D HPE methods are also imperfect, especially when they are applied in situations of inconsistent illumination or occlusion. An analysis of related works helps to establish an appropriate framework for the current research, and this paper extends previous work by creating an algorithm that further improves the framework. The method relies upon a stereo camera arrangement and utilises a method to detect and track key landmark points of the human face to evaluate improvement in 3D-head-pose estimation. © 2018 ACM.},
author_keywords={3D Face tracking;  Camera calibration;  Face detection;  Facial landmark tracking;  Head-pose estimation;  Real-time tracking;  Stereo camera},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NourbakhshKaashki201866,
author={Nourbakhsh Kaashki, N. and Safabakhsh, R.},
title={RGB-D face recognition under various conditions via 3D constrained local model},
journal={Journal of Visual Communication and Image Representation},
year={2018},
volume={52},
pages={66-85},
doi={10.1016/j.jvcir.2018.02.003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042322207&doi=10.1016%2fj.jvcir.2018.02.003&partnerID=40&md5=2b91afeff8a19c987036cf8a30b610dc},
affiliation={Department of Computer Engineering, Amirkabir University of Technology, Tehran, Iran},
abstract={This research proposes a method for 3D face recognition in various conditions using 3D constrained local model (CLM-Z). In this method, a combination of 2D images (RGBs) and depth images (Ds) captured by Kinect has been used. After detecting the face and smoothing the depth image, CLM-Z model has been used to model and detect the important points of the face. These points are described using Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), and 3D Local Binary Patterns (3DLBP). Finally, each face is recognized by a Support Vector Machine (SVM). The challenging situations are changes of lighting, facial expression and head pose. The results on CurtinFaces and IIIT-D datasets demonstrate that the proposed method outperformed state-of-the-art methods under illumination, expression and pitch pose conditions and comparable results were obtained in other cases. Additionally, our proposed method is robust even when the training data has not been carefully collected. © 2018 Elsevier Inc.},
author_keywords={3D constrained local model;  3D face recognition;  Depth image;  Face model;  Facial expression;  Feature descriptor;  Head pose;  Kinect;  Lighting},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Abbad20181,
author={Abbad, A. and Abbad, K. and Tairi, H.},
title={3D face recognition in the presence of facial expressions based on empirical mode decomposition},
journal={ACM International Conference Proceeding Series},
year={2018},
volume={2018-March},
pages={1-6},
doi={10.1145/3177148.3180087},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047112949&doi=10.1145%2f3177148.3180087&partnerID=40&md5=93db67c347fc7dadc31256f7eb69e983},
affiliation={Laboratory LIIAN, Faculty of Science, Dhar EL Mahraz, Morocco; Laboratory ISA, Faculty of Science and Technology, Morocco},
abstract={This paper presents an efficient 3D face recognition method to handle facial expression. The proposed method uses the Surfaces Empirical Mode Decomposition (SEMD), facial curves and local shape descriptor in a matching process to overcome the distortions caused by expressions in faces. The basic idea is that, the face is presented at different scales by SEMD. Then the isometric-invariant features on each scale are extracted. After that, the geometric information is obtained on the 3D surface in terms of radial and level facial curves. Finally, the feature vectors on each scale are associated with their corresponding geometric information. The presented method is validated on GavabDB database resulting a rank 1 recognition rate (RR) of 98.9% for all faces with neutral and non-neutral expressions. This result outperforms other 3D expression-invariant face recognition methods on the same database. © 2018 Association for Computing Machinery.},
author_keywords={3D face recognition;  EMD;  Expression;  Facial curves;  Geometric features;  Local features},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lim20181,
author={Lim, S.-J. and Hwang, B.-W. and Yoon, S.-U. and Choi, J.S. and Park, C.-J.},
title={Automatic 3D face component analysis technique},
journal={2018 IEEE International Conference on Consumer Electronics, ICCE 2018},
year={2018},
volume={2018-January},
pages={1-2},
doi={10.1109/ICCE.2018.8326087},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048741847&doi=10.1109%2fICCE.2018.8326087&partnerID=40&md5=9e463e424ed761278ba91cfd621f3be0},
affiliation={Electronics and Telecommunications Research Institute (ETRI), United States},
abstract={This paper proposes a technique to perform segmentation for the meaningful regions that part of the face captured by 3D scanners or 3D sensors, automatically. Each part recognition of the scanned face is vital for the 3D applications such as modeling, animation and 3D printing. We transfer the template model labeled with the meaningful part to the scanned face model to find the corresponding part of each meaningful part of the template model. This technique can be used to the several applications such as 3D face modeling, facial animation, virtual facial surgery and 3D printing. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sachara2018959,
author={Sachara, F. and Kopinski, T. and Gepperth, A. and Handmann, U.},
title={Free-hand gesture recognition with 3D-CNNs for in-car infotainment control in real-time},
journal={IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
year={2018},
volume={2018-March},
pages={959-964},
doi={10.1109/ITSC.2017.8317684},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046260257&doi=10.1109%2fITSC.2017.8317684&partnerID=40&md5=0733742868868fdbfb5b88aa0a45c1e7},
affiliation={Computer Science Institute, Hochschule Ruhr West, Bottrop, 46236, Germany; Fachhochschule Südwestfalen, Lindenstraße 52, Meschede, 59872, Germany; Applied Computer Science, University of Applied Sciences, Fulda, 36037, Germany},
abstract={In this contribution we present a novel approach to transform data from time-of-flight (ToF) sensors to be interpretable by Convolutional Neural Networks (CNNs). As ToF data tends to be overly noisy depending on various factors such as illumination, reflection coefficient and distance, the need for a robust algorithmic approach becomes evident. By spanning a three-dimensional grid of fixed size around each point cloud we are able to transform three-dimensional input to become processable by CNNs. This simple and effective neighborhood-preserving methodology demonstrates that CNNs are indeed able to extract the relevant information and learn a set of filters, enabling them to differentiate a complex set of ten different gestures obtained from 20 different individuals and containing 600.000 samples overall. Our 20-fold cross-validation shows the generalization performance of the network, achieving an accuracy of up to 98.5% on validation sets comprising 20.000 data samples. The real-time applicability of our system is demonstrated via an interactive validation on an infotainment system running with up to 40fps on an iPad in the vehicle interior. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gao2018,
author={Gao, W. and Zhao, X. and An, J. and Zou, J.},
title={Multi-pose 3D facial texture refinement for face recognition},
journal={International Journal of Wavelets, Multiresolution and Information Processing},
year={2018},
volume={16},
number={2},
doi={10.1142/S0219691318400064},
art_number={1840006},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042747680&doi=10.1142%2fS0219691318400064&partnerID=40&md5=a40bb4f5c797b5c13aa52832aa7a66c3},
affiliation={School of Electrical and Information Engineering, Xi'An Jiaotong University, Xi'an, 710049, China; School of Management, Xi'An Jiaotong University, Xi'an, 710049, China; School of Management, Northwestern Polytechnical University, Xi'an, 710072, China},
abstract={In this paper, we propose a novel approach for 3D face reconstruction from multi-facial images. Given original pose-variant images, coarse 3D face templates are initialized to reconstruct a refined 3D face mesh in an iteration manner. Then, we warp original facial images to the 2D meshes projected from 3D using Sparse Mesh Affine Warp (SMAW). Finally, we weight the face patches in each view respectively and map the patch with higher weight to a canonical UV space. For facial images with arbitrary pose, their invisible regions are filled with the corresponding UV patches. Poisson editing is applied to blend different patches seamlessly. We evaluate the proposed method on LFW dataset in terms of texture refinement and face recognition. The results demonstrate competitive performance compared to state-of-the-art methods. © 2018 World Scientific Publishing Company.},
author_keywords={3D face reconstruction;  face recognition;  sparse mesh affine warp;  Texture refinement},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo2018,
author={Guo, Y. and Wei, R. and Liu, Y.},
title={Weighted gradient feature extraction based on multiscale sub-blocks for 3d facial recognition in bimodal images},
journal={Information (Switzerland)},
year={2018},
volume={9},
number={3},
doi={10.3390/info9030048},
art_number={48},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042711391&doi=10.3390%2finfo9030048&partnerID=40&md5=fe75121af64143005ff6ec84435ed59d},
affiliation={School of Computer Science and Engineering, Hebei University of Technology, Tianjin, 300400, China},
abstract={In this paper, we propose a bimodal 3D facial recognition method aimed at increasing the recognition rate and reducing the effect of illumination, pose, expression, ages, and occlusion on facial recognition. There are two features extracted from the multiscale sub-blocks in both the 3D mode depth map and 2D mode intensity map, which are the local gradient pattern (LGP) feature and the weighted histogram of gradient orientation (WHGO) feature. LGP and WHGO features are cascaded to form the 3D facial feature vector LGP-WHGO, and are further trained and identified by the support vector machine (SVM). Experiments on the CASIA database, FRGC v2.0 database, and Bosphorus database show that, the proposed method can efficiently extract the structure information and texture information of the facial image, and have a robustness to illumination, expression, occlusion and pose. © 2018 by the authors.},
author_keywords={3D face recognition;  Bimodal;  Depth map;  Intensitymap;  LGP-WHGO;  Multiscale sub-blocks},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gou20182906,
author={Gou, C. and Wu, Y. and Wang, F.-Y. and Ji, Q.},
title={Coupled cascade regression for simultaneous facial landmark detection and head pose estimation},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2018},
volume={2017-September},
pages={2906-2910},
doi={10.1109/ICIP.2017.8296814},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045296105&doi=10.1109%2fICIP.2017.8296814&partnerID=40&md5=bbf355c9bb4fa4416e5d8af78c33014d},
affiliation={Institute of Automation, Chinese Academy of Sciences, China; Rensselaer Polytechnic Institute, United States; Qingdao Academy of Intelligent Industries, China},
abstract={Current approaches for facial landmark detection and head pose estimation first perform landmark detection, followed by fitting 3D face model or regression model to estimate head pose. Different from the existing methods, in this paper, we propose a unified method, called Coupled Cascade Regression (CCR), for simultaneous facial landmark detection and head pose estimation. At each cascade level, two separate regressors are learned to update the landmark locations and 3D face model parameters based on the local appearance features, respectively. Since 2D facial landmark locations and head pose parameters are related, we further apply the projection model to refine the prediction results in each cascade iteration and make them consistent. As a result, CCR can leverage both the learning methods and the projection model to simultaneously perform facial landmark detection and pose estimation to enhance the performances of both tasks. Experimental results on 300-W and BU datasets indicate that our proposed CCR method outperforms many conventional methods both for landmark detection and head pose estimation. © 2017 IEEE.},
author_keywords={Coupled cascade regression;  Facial landmark detection;  Head pose estimation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Soltanpour20182811,
author={Soltanpour, S. and Wu, Q.M.J.},
title={High-order local normal derivative pattern (LNDP) for 3D face recognition},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2018},
volume={2017-September},
pages={2811-2815},
doi={10.1109/ICIP.2017.8296795},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045292850&doi=10.1109%2fICIP.2017.8296795&partnerID=40&md5=12c33512c432220af4c4c49f5116433e},
affiliation={University of Windsor, Department of Electrical and Computer Engineering, Windsor, Canada},
abstract={This paper proposes a novel descriptor based on the local derivative pattern (LDP) for 3D face recognition. Compared to the local binary pattern (LBP), LDP can capture more detailed information by encoding directional pattern features. It is based on the local derivative variations that extract high-order local information. We propose a novel discriminative facial shape descriptor, local normal derivative pattern (LNDP) that extracts LDP from the surface normal. Using surface normal, the orientation of a surface at each point is determined as a first-order surface differential. Three normal component images are extracted by estimating three components of normal vectors in x, y, and z channels. Each normal component is divided into several patches and encoded using LDP. The final descriptor is created by concatenating histograms of the LNDP on each patch. Experimental results on two famous 3D face databases, FRGC v2.0 and Bosphorus illustrate the effectiveness of the proposed descriptor. © 2017 IEEE.},
author_keywords={3D face;  High-order local pattern;  Local derivative pattern;  Surface normal},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hong2018825,
author={Hong, S. and Im, W. and Ryu, J. and Yang, H.S.},
title={SSPP-DAN: Deep domain adaptation network for face recognition with single sample per person},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2018},
volume={2017-September},
pages={825-829},
doi={10.1109/ICIP.2017.8296396},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045330406&doi=10.1109%2fICIP.2017.8296396&partnerID=40&md5=5f40daaac3230a175fe0c0171dc4b0ce},
affiliation={School of Computing, KAIST, South Korea},
abstract={Real-world face recognition using a single sample per person (SSPP) is a challenging task. The problem is exacerbated if the conditions under which the gallery image and the probe set are captured are completely different. To address these issues from the perspective of domain adaptation, we introduce an SSPP domain adaptation network (SSPP-DAN). In the proposed approach, domain adaptation, feature extraction, and classification are performed jointly using a deep architecture with domain-adversarial training. However, the SSPP characteristic of one training sample per class is insufficient to train the deep architecture. To overcome this shortage, we generate synthetic images with varying poses using a 3D face model. Experimental evaluations using a realistic SSPP dataset show that deep domain adaptation and image synthesis complement each other and dramatically improve accuracy. Experiments on a benchmark dataset using the proposed approach show state-of-the-art performance. © 2017 IEEE.},
author_keywords={Domain adaptation;  Image synthesis;  SSPP face recognition;  SSPP-DAN;  Surveillance camera},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ye2018441,
author={Ye, C. and Qian, X.},
title={3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired},
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
year={2018},
volume={26},
number={2},
pages={441-450},
doi={10.1109/TNSRE.2017.2748419},
art_number={8024025},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029164431&doi=10.1109%2fTNSRE.2017.2748419&partnerID=40&md5=ef8317fcb58d8906082702137083a6b4},
affiliation={Virginia Commonwealth University, Richmond, VA  23284, United States; HP Headquarters, Palo Alto, CA  94304, United States},
abstract={This paper presents a 3-D object recognition method and its implementation on a robotic navigation aid to allow real-time detection of indoor structural objects for the navigation of a blind person. The method segments a point cloud into numerous planar patches and extracts their inter-plane relationships (IPRs).Based on the existing IPRs of the object models, the method defines six high level features (HLFs) and determines the HLFs for each patch. A Gaussian-mixture-model-based plane classifier is then devised to classify each planar patch into one belonging to a particular object model. Finally, a recursive plane clustering procedure is used to cluster the classified planes into the model objects. As the proposed method uses geometric context to detect an object, it is robust to the object's visual appearance change. As a result, it is ideal for detecting structural objects (e.g., stairways, doorways, and so on). In addition, it has high scalability and parallelism. The method is also capable of detecting some indoor non-structural objects. Experimental results demonstrate that the proposed method has a high success rate in object recognition. © 2001-2011 IEEE.},
author_keywords={3D object recognition;  Gaussian mixture model;  geometric context;  Robotic navigation aid;  visually impaired},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li20181295,
author={Li, Y. and Wang, Y. and Liu, J. and Hao, W.},
title={Expression-insensitive 3D face recognition by the fusion of multiple subject-specific curves},
journal={Neurocomputing},
year={2018},
volume={275},
pages={1295-1307},
doi={10.1016/j.neucom.2017.09.070},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040674183&doi=10.1016%2fj.neucom.2017.09.070&partnerID=40&md5=ecfea39a1b4d26ab1e33c34b50b87558},
affiliation={Institute of Computer Science & Engineering, Xi'an University of Technology, No.5 South Jinhua Road, Xi'an, 710048, China},
abstract={This study proposes a 3D face recognition method using multiple subject-specific curves insensitive to intra-subject distortions caused by expression variations. Considering that most sharp variances in facial convex regions are closely related to the bone structure, the convex crest curves are first extracted as the most vital subject-specific facial curves based on the principal curvature extrema in convex local surfaces. Then, the central profile curve and the horizontal contour curve passing through the nose tip are detected by using the precise localization of the nose tip and symmetry plane. Based on their discriminative power and robustness to expression changes, the three types of curves are fused with appropriate weights at the feature-level and used for matching 3D faces with the iterative closest point algorithm. The combination of multiple expression-insensitive curves is complementary and provides sufficient and stable facial surface features for face recognition. In addition, for each convex crest curve, an expression-irrelevant factor is assigned as the adaptive weight to improve the face matching performance. The results of experiments using two public 3D databases, GavabDB and BU-3DFE, demonstrate the effectiveness of the proposed method, and its recognition rates on both databases reflect an encouraging performance. © 2017 Elsevier B.V.},
author_keywords={3D face recognition;  Expression-insensitive;  Feature-level;  Fusion;  Subject-specific curve},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang201850,
author={Wang, N. and Gao, X. and Tao, D. and Yang, H. and Li, X.},
title={Facial feature point detection: A comprehensive survey},
journal={Neurocomputing},
year={2018},
volume={275},
pages={50-65},
doi={10.1016/j.neucom.2017.05.013},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020477378&doi=10.1016%2fj.neucom.2017.05.013&partnerID=40&md5=b097b72fd7ec44e8ec6b2efc16411f04},
affiliation={The State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi'an City, 710071, China; The State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi'an City, 710071, China; UBTech Sydney Artificial Intelligence Institute, The School of Information Technologies, University of Sydney, J12 Cleveland St, Darlington, NSW  200 8, Australia; ULSee Incorporation, Hangzhou City, 310016, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, China},
abstract={This paper presents a comprehensive survey of facial feature point detection with the assistance of abundant manually labeled images. Facial feature point detection favors many applications such as face recognition, animation, tracking, hallucination, expression analysis and 3D face modeling. Existing methods are categorized into two primary categories according to whether there is the need of a parametric shape model: parametric shape model-based methods and nonparametric shape model-based methods. Parametric shape model-based methods are further divided into two secondary classes according to their appearance models: local part model-based methods (e.g. constrained local model) and holistic model-based methods (e.g. active appearance model). Nonparametric shape model-based methods are divided into several groups according to their model construction process: exemplar-based methods, graphical model-based methods, cascaded regression-based methods, and deep learning based methods. Though significant progress has been made, facial feature point detection is still limited in its success by wild and real-world conditions: large variations across poses, expressions, illuminations, and occlusions. A comparative illustration and analysis of representative methods provides us a holistic understanding and deep insight into facial feature point detection, which also motivates us to further explore more promising future schemes. © 2017 Elsevier B.V.},
author_keywords={Deep learning;  Face alignment;  Facial feature point detection;  Facial landmark localization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kim2018133,
author={Kim, D. and Hernandez, M. and Choi, J. and Medioni, G.},
title={Deep 3D face identification},
journal={IEEE International Joint Conference on Biometrics, IJCB 2017},
year={2018},
volume={2018-January},
pages={133-142},
doi={10.1109/BTAS.2017.8272691},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046247920&doi=10.1109%2fBTAS.2017.8272691&partnerID=40&md5=9660021125b86249b9768912252c19ff},
affiliation={USC Institute for Robotics and Intelligent Systems (IRIS), University of Southern California, United States},
abstract={We propose a novel 3D face recognition algorithm using a deep convolutional neural network (DCNN) and a 3D face expression augmentation technique. The performance of 2D face recognition algorithms has significantly increased by leveraging the representational power of deep neural networks and the use of large-scale labeled training data. In this paper, we show that transfer learning from a CNN trained on 2D face images can effectively work for 3D face recognition by fine-tuning the CNN with an extremely small number of 3D facial scans. We also propose a 3D face expression augmentation technique which synthesizes a number of different facial expressions from a single 3D face scan. Our proposed method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets without using hand-crafted features. The 3D face identification using our deep features also scales well for large databases. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dou2018483,
author={Dou, P. and Kakadiaris, I.A.},
title={Multi-view 3D face reconstruction with deep recurrent neural networks},
journal={IEEE International Joint Conference on Biometrics, IJCB 2017},
year={2018},
volume={2018-January},
pages={483-492},
doi={10.1109/BTAS.2017.8272733},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046246172&doi=10.1109%2fBTAS.2017.8272733&partnerID=40&md5=3abc00e7f78b2e282c7bf11cd49cf800},
affiliation={Computational Biomedicine Lab, University of Houston, 4800 Calhoun Road, Houston, TX  77004, United States},
abstract={Image-based 3D face reconstruction has great potential in different areas, such as facial recognition, facial analysis, and facial animation. Due to the variations in image quality, single-image-based 3D face reconstruction might not be sufficient to accurately reconstruct a 3D face. To overcome this limitation, multi-view 3D face reconstruction uses multiple images of the same subject and aggregates complementary information for better accuracy. Though theoretically appealing, there are multiple challenges in practice. Among these challenges, the most significant is that it is difficult to establish coherent and accurate correspondence among a set of images, especially when these images are captured in different conditions. In this paper, we propose a method, Deep Recurrent 3D FAce Reconstruction (DRFAR), to solve the task ofmulti-view 3D face reconstruction using a subspace representation of the 3D facial shape and a deep recurrent neural network that consists of both a deep con-volutional neural network (DCNN) and a recurrent neural network (RNN). The DCNN disentangles the facial identity and the facial expression components for each single image independently, while the RNN fuses identity-related features from the DCNN and aggregates the identity specific contextual information, or the identity signal, from the whole set of images to predict the facial identity parameter, which is robust to variations in image quality and is consistent over the whole set of images. Through extensive experiments, we evaluate our proposed method and demonstrate its superiority over existing methods. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2018234,
author={Li, H. and Sun, J. and Chen, L.},
title={Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition},
journal={IEEE International Joint Conference on Biometrics, IJCB 2017},
year={2018},
volume={2018-January},
pages={234-242},
doi={10.1109/BTAS.2017.8272703},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046291057&doi=10.1109%2fBTAS.2017.8272703&partnerID=40&md5=e5c4065442f8bc899597efdc443ed01c},
affiliation={School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, 710049, China; Department of Mathematics and Informatics, Ecole Centrale de Lyon, Lyon, 69134, France},
abstract={This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01%, 97.60%, and 96.13% on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zafeiriou20182503,
author={Zafeiriou, S. and Chrysos, G.G. and Roussos, A. and Ververas, E. and Deng, J. and Trigeorgis, G.},
title={The 3D Menpo Facial Landmark Tracking Challenge},
journal={Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
year={2018},
volume={2018-January},
pages={2503-2511},
doi={10.1109/ICCVW.2017.16},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046289153&doi=10.1109%2fICCVW.2017.16&partnerID=40&md5=5d07f5e693e1ebbbb60a3281183dea15},
affiliation={Department of Computing, Imperial College London, United Kingdom; Center for Machine Vision and Signal Analysis, University of Oulu, Finland; Department of Computer Science, University of Exeter, United Kingdom},
abstract={Recently, deformable face alignment is synonymous to the task of locating a set of 2D sparse landmarks in intensity images. Currently, discriminatively trained Deep Convolutional Neural Networks (DCNNs) are the state-of-the-art in the task of face alignment. DCNNs exploit large amount of high quality annotations that emerged the last few years. Nevertheless, the provided 2D annotations rarely capture the 3D structure of the face (this is especially evident in the facial boundary). That is, the annotations neither provide an estimate of the depth nor correspond to the 2D projections of the 3D facial structure. This paper summarises our efforts to develop (a) a very large database suitable to be used to train 3D face alignment algorithms in images captured 'in-the-wild' and (b) to train and evaluate new methods for 3D face landmark tracking. Finally, we report the results of the first challenge in 3D face tracking 'in-the-wild'. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu20181619,
author={Liu, Y. and Jourabloo, A. and Ren, W. and Liu, X.},
title={Dense Face Alignment},
journal={Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
year={2018},
volume={2018-January},
pages={1619-1628},
doi={10.1109/ICCVW.2017.190},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046269553&doi=10.1109%2fICCVW.2017.190&partnerID=40&md5=180ea024a11b87baf1e8ef371cefd050},
affiliation={Department of Computer Science and Engineering, Michigan State UniversityMI, United States; Monta Vist, Cupertino, CA, United States},
abstract={Face alignment is a classic problem in the computer vision field. Previous works mostly focus on sparse alignment with a limited number of facial landmark points, i.e., facial landmark detection. In this paper, for the first time, we aim at providing a very dense 3D alignment for large-pose face images. To achieve this, we train a CNN to estimate the 3D face shape, which not only aligns limited facial landmarks but also fits face contours and SIFT feature points. Moreover, we also address the bottleneck of training CNN with multiple datasets, due to different landmark markups on different datasets, such as 5, 34, 68. Experimental results show our method not only provides high-quality, dense 3D face fitting but also outperforms the state-of-the-art facial landmark detection methods on challenging datasets. Our model can run at real time during testing and it's available at http:///cvlab.cse.msu.edu/project-pifa.html. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chang20181599,
author={Chang, F.-J. and Tran, A.T. and Hassner, T. and Masi, I. and Nevatia, R. and Medioni, G.},
title={FacePoseNet: Making a Case for Landmark-Free Face Alignment},
journal={Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
year={2018},
volume={2018-January},
pages={1599-1608},
doi={10.1109/ICCVW.2017.188},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046252824&doi=10.1109%2fICCVW.2017.188&partnerID=40&md5=c5a847d100291fd1aa9c96b8a6885c46},
affiliation={Institute for Robotics and Intelligent Systems, USCCA, United States; Information Sciences Institute, USCCA, United States; Open University of Israel, Israel},
abstract={We show how a simple convolutional neural network (CNN) can be trained to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose, directly from image intensities. We further explain how this FacePoseNet (FPN) can be used to align faces in 2D and 3D as an alternative to explicit facial landmark detection for these tasks. We claim that in many cases the standard means of measuring landmark detector accuracy can be misleading when comparing different face alignments. Instead, we compare our FPN with existing methods by evaluating how they affect face recognition accuracy on the IJB-A and IJB-B benchmarks: using the same recognition pipeline, but varying the face alignment method. Our results show that (a) better landmark detection accuracy measured on the 300W benchmark does not necessarily imply better face recognition accuracy. (b) Our FPN provides superior 2D and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces at a small fraction of the computational cost of comparably accurate landmark detectors. For many purposes, FPN is thus a far faster and far more accurate face alignment method than using facial landmark detectors. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Le20182555,
author={Le, H.A. and Kakadiaris, I.A.},
title={UHDB31: A dataset for better understanding face recognition across pose and illumination variation},
journal={Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
year={2018},
volume={2018-January},
pages={2555-2563},
doi={10.1109/ICCVW.2017.300},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045561999&doi=10.1109%2fICCVW.2017.300&partnerID=40&md5=4a88cc6c019a2d8f3cb21196b1ab0a70},
affiliation={Department of Computer Science, Computational Biomedicine Lab, University of Houston, United States},
abstract={Face datasets are a fundamental tool to analyze the performance of face recognition algorithms. However, the accuracy achieved on current benchmark datasets is saturated. Although multiple face datasets have been published recently, they only focus on the number of samples and lack diversity on facial appearance factors, such as pose and illumination. In addition, while 3D data have been demonstrated improved face recognition accuracy by a significant margin, only a few 3D face datasets provide high quality 2D and 3D data. In this paper, we introduce a new and challenging dataset, called UHDB31, which not only allows direct measurement of the influence of pose, illumination, and resolution on face recognition but also facilitates different experimental configurations with both 2D and 3D data. We conduct a series of experiments with various face recognition algorithms and point out how far they are from solving the face recognition problem under pose, illumination, and resolution variation. The dataset is publicly available and free for research use1. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Deng2018222,
author={Deng, W. and Fang, Y. and Xu, Z. and Hu, J.},
title={Facial landmark localization by enhanced convolutional neural network},
journal={Neurocomputing},
year={2018},
volume={273},
pages={222-229},
doi={10.1016/j.neucom.2017.07.052},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028364319&doi=10.1016%2fj.neucom.2017.07.052&partnerID=40&md5=75eda9437e4eaa93fb84b2dc8f305bc6},
affiliation={School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China; International School, Beijing University of Posts and Telecommunications, Beijing, 100876, China},
abstract={Facial landmark localization is important to many facial recognition and analysis tasks, such as face attributes analysis, head pose estimation, 3D face modeling, and facial expression analysis. In this paper, we propose a new approach to localizing landmarks in facial image by deep convolutional neural network (DCNN). We make two enhancements on the CNN to adapt it to the feature localization task as follows. First, we replace the commonly used max pooling by depth-wise convolution to obtain better localization performance. Second, we define a response map for each facial points as a 2D probability map indicating the presence likelihood, and train our model with a KL divergence loss. To obtain robust localization results, our approach first takes the expectations of the response maps of enhanced CNN and then applies auto-encoder model to the global shape vector, which is effective to rectify the outlier points by the prior global landmark configurations. The proposed ECNN method achieves 5.32% mean error on the experiments on the 300-W dataset, which is comparable to the state-of-the-art performance on this standard benchmark, showing the effectiveness of our methods. © 2017 Elsevier B.V.},
author_keywords={Convolutional neural network;  Deep learning;  Face alignment;  Face recognition;  Facial landmark localization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li20181,
author={Li, H. and Li, Y. and Liu, W. and Dong, H.},
title={Coarse-to-fine facial landmarks localization based on convolutional feature},
journal={Proceedings of 4th International Conference on Behavioral, Economic, and Socio-Cultural Computing, BESC 2017},
year={2018},
volume={2018-January},
pages={1-6},
doi={10.1109/BESC.2017.8256378},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050556634&doi=10.1109%2fBESC.2017.8256378&partnerID=40&md5=0c3f54df7232e018dff1fe3ee83b9068},
affiliation={School of Computer and Information Technology, Beijing Jiaotong University, Beijing, 100044, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, 100044, China},
abstract={Accurate facial landmarks localization (FLL) plays an important role in face recognition, face tracking and 3D face reconstruction. It can be formulated as a regression problem, which outputs facial landmarks positions from the detected face image. Deep constitutional neural network (CNN) has achieved great success in vision tasks, but it is insignificant to use it directly. In this paper, instead of adopting CNN model straightforwardly, we combine different convolutional features with extreme machine learning (ELM) in a cascade framework to achieve accurate FLL. Specifically, we extract globally and spatially convolutional feature in the first stage for containing better localization property by training deep CNN, which takes the whole face region as input and concatenates lower layers with higher layers. Then, we extract locally and correlatedly convolutional feature in the following stages for preserving shape constraint by building multi-objective CNN, which inputs local patches centered at the current landmarks and concatenates independent subnetwork of each landmark together. Moreover, the regressor embedded in CNN is replaced by the robust ELM for accurate shape regression. Extensive experiments demonstrate that our method performs better in challenging datasets. © 2017 IEEE.},
author_keywords={cascade framework;  convolutional feature;  extreme machine learning;  facial landmark localization;  unconstrained environment},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Maalej20181,
author={Maalej, Y. and Sorour, S. and Abdel-Rahim, A. and Guizani, M.},
title={VANETs Meet Autonomous Vehicles: A Multimodal 3D Environment Learning Approach},
journal={2017 IEEE Global Communications Conference, GLOBECOM 2017 - Proceedings},
year={2018},
volume={2018-January},
pages={1-6},
doi={10.1109/GLOCOM.2017.8254480},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046348567&doi=10.1109%2fGLOCOM.2017.8254480&partnerID=40&md5=181825ce0c3c81cae852013f14856e5e},
affiliation={University of Idaho, Moscow, ID, United States},
abstract={In this paper, we design a multimodal framework for object detection, recognition and mapping based on the fusion of stereo camera frames, point cloud Velodyne LIDAR scans, and Vehicle-to-Vehicle (V2V) Basic Safety Messages (BSMs) that are exchanged using Dedicated Short Range Communication (DSRC). We merge the key features of rich texture descriptions of objects from 2D images using Convolutional Neural Networks (CNN). In addition, depth and distance between objects are provided by the 3D LIDAR point cloud and the awareness of hidden vehicles is achieved from BSMs' beacons. We present a joint pixel to point cloud and pixel to V2V correspondence of objects in frames of driving sequences in the KITTI Vision Benchmark Suite. We achieve this by using a semi-supervised manifold alignment approach to achieve camera-LIDAR and camera-V2V mapping of their recognized persons and cars that have the same underlying manifold. © 2017 IEEE.},
author_keywords={BSMs;  CNN;  DSRC;  KITTI;  LIDAR;  Manifold Alignment;  Point Cloud;  V2V;  Velodyne},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zavan20189,
author={Zavan, F.H.D.B. and Gasparin, N. and Batista, J.C. and E Silva, L.P. and Albiero, V. and Bellon, O.R.P. and Silva, L.},
title={Face Analysis in the Wild},
journal={Proceedings - 2017 30th SIBGRAPI Conference on Graphics, Patterns and Images Tutorials SIBGRAPI-T 2017},
year={2018},
volume={2018-January},
pages={9-16},
doi={10.1109/SIBGRAPI-T.2017.11},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050220731&doi=10.1109%2fSIBGRAPI-T.2017.11&partnerID=40&md5=3744f98d04fd75ae2f03f957b471838f},
affiliation={IMAGO Research Group, Universidade Federal do Paraná (UFPR), Av. Francisco H. dos Santos, 100, Curitiba, PR, 81531-980, Brazil},
abstract={With the global demand for extra security systems, and the growing of human-machine interaction, facial analysis in unconstrained environments (in the wild) became a hot-topic in recent computer vision research. Unconstrained environments include surveillance footage, social media photos and live broadcasts. This type of images and videos include no control over illumination, position, size, occlusion, and facial expressions. Successful facial processing methods for controlled scenarios are unable to pledge with challenging circumstances. Consequently, methods tailored for handling those situations are indispensable for the face analysis research progress. This work presents a comprehensive review of state-of-the-art methods, drawing attention to the complications derived from in the wild scenarios and the behavior differences when applied to the controlled images. The main topics to be covered are: (1) face detection; (2) facial image quality; (3) head pose estimation; (4) face alignment; (5) 3D face reconstruction; (6) gender and age estimation; (7) facial expressions and emotions; and (8) face recognition. Finally, available code and applications for in the wild face analysis are presented, followed by a discussion on future directions. © 2017 IEEE.},
author_keywords={face analysis;  face processing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheng2018555,
author={Cheng, Z. and Shi, T. and Cui, W. and Dong, Y. and Fang, X.},
title={3D face recognition based on kinect depth data},
journal={2017 4th International Conference on Systems and Informatics, ICSAI 2017},
year={2018},
volume={2018-January},
pages={555-559},
doi={10.1109/ICSAI.2017.8248353},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046681940&doi=10.1109%2fICSAI.2017.8248353&partnerID=40&md5=196c95096dd8ecb21cab2899f3232315},
affiliation={School of International Finance and Banking, University of Science and Technology Liaoning, Anshan, 114051, China; School of International Studies, Sun Yat-sen University, Guangzhou, China},
abstract={In this paper, a contour map human facial recognition algorithm is proposed to implement the three-dimensional (3D) face recognition with the Kinect Xbox One. Since the scale of 3D depth data collected from Kinect is tremendous, the face recognition process cannot be handled in real time. To improve the speed and accuracy of the recognition process, the proposed algorithm turns the 3D depth data to the two-dimensional (2D) contour map. Furthermore, due to the 3D depth data obtained by Kinect, there is no need of expensive, ponderous and slow 3D scanners. Ten male and female subjects were involved in the validation experiment and the results verify that the proposed algorithm was feasible for face recognition. In addition, compared with other methods, Eigenface, Local Binary Patterns (LBP) and Linear Discriminant Analysis (LDA), the proposed algorithm has the better security and reliability. © 2017 IEEE.},
author_keywords={2D contour map;  3D depth data;  face recognition;  Kinect},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Leo2018619,
author={Leo, M.J. and Suchitra, S.},
title={SVM based expression-invariant 3D face recognition system},
journal={Procedia Computer Science},
year={2018},
volume={143},
pages={619-625},
doi={10.1016/j.procs.2018.10.441},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058271138&doi=10.1016%2fj.procs.2018.10.441&partnerID=40&md5=4a50565188c5152b1c5f2e68a4a95066},
affiliation={Department of CSE, Hindustan Institute of Technology and Science, Padur, Chennai, 603103, India},
abstract={The main objective of expression-invariant 3D face recognition system is to recognize 3D human faces even under various expressions. This paper focuses on such face recognition system using an efficient combination of 3D Principal Component Analysis (PCA) and Support Vector Machine (SVM). In the proposed method, each face is registered initially using the novel Mean Landmark Points (MLPs) based registration which facilitates the accurate extraction of distinct features from facial region using 3D PCA. SVM based classification is then done on the extracted features and it is found that the recognition rate is improved considerably by carefully selecting the training dataset. Experimental results reported on Bosphorus 3D face database prove that the proposed approach achieves the rank-1 recognition rate of 96.29% on near frontal 3D faces comprising of rich facial expressions. © 2018 The Authors. Published by Elsevier B.V.},
author_keywords={3D Face Registration;  3D Principal Component Analysis;  Bosphorus 3D Face Database;  Expression-invariant 3DFace Recognition;  Support Vector Machine},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhao2018207,
author={Zhao, J.-L. and Wu, Z.-K. and Pan, Z.-K. and Duan, F.-Q. and Li, J.-H. and Lv, Z.-H. and Wang, K. and Chen, Y.-C.},
title={3D Face Similarity Measure by Fréchet Distances of Geodesics},
journal={Journal of Computer Science and Technology},
year={2018},
volume={33},
number={1},
pages={207-222},
doi={10.1007/s11390-018-1814-7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041342736&doi=10.1007%2fs11390-018-1814-7&partnerID=40&md5=8478bdecdc0eeeb6cb4dda3bb1ecda52},
affiliation={School of Data Science and Software Engineering, Qingdao University, Qingdao, 266071, China; College of Automation and Electrical Engineering, Qingdao University, Qingdao, 266071, China; College of Information Science and Technology, Beijing Normal University, Beijing, 100087, China; College of Computer Science and Technology, Qingdao University, Qingdao, 266071, China; School of Management, Capital Normal University, Beijing, 100048, China},
abstract={3D face similarity is a critical issue in computer vision, computer graphics and face recognition and so on. Since Fréchet distance is an effective metric for measuring curve similarity, a novel 3D face similarity measure method based on Fréchet distances of geodesics is proposed in this paper. In our method, the surface similarity between two 3D faces is measured by the similarity between two sets of 3D curves on them. Due to the intrinsic property of geodesics, we select geodesics as the comparison curves. Firstly, the geodesics on each 3D facial model emanating from the nose tip point are extracted in the same initial direction with equal angular increment. Secondly, the Fréchet distances between the two sets of geodesics on the two compared facial models are computed. At last, the similarity between the two facial models is computed based on the Fréchet distances of the geodesics obtained in the second step. We verify our method both theoretically and practically. In theory, we prove that the similarity of our method satisfies three properties: reflexivity, symmetry, and triangle inequality. And in practice, experiments are conducted on the open 3D face database GavaDB, Texas 3D Face Recognition database, and our 3D face database. After the comparison with iso-geodesic and Hausdorff distance method, the results illustrate that our method has good discrimination ability and can not only identify the facial models of the same person, but also distinguish the facial models of any two different persons. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={3D face;  Fréchet distance;  geodesic;  similarity measure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lv2018,
author={Lv, C. and Zhao, J.},
title={3D Face Recognition based on Local Conformal Parameterization and Iso-Geodesic Stripes Analysis},
journal={Mathematical Problems in Engineering},
year={2018},
volume={2018},
doi={10.1155/2018/4707954},
art_number={4707954},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056267138&doi=10.1155%2f2018%2f4707954&partnerID=40&md5=6b9608dc984f786b68071250cc5e66cb},
affiliation={College of Information Science and Technology, Beijing Normal University, Beijing, China; Engineering Research Center of Virtual Reality and Applications, Ministry of Education, Beijing Key Laboratory of Digital Preservation, Beijing, 100875, China; School of Data Science and Software Engineering, Qingdao University, Qingdao, China; College of Automation and Electrical Engineering, Qingdao University, Qingdao, China},
abstract={3D face recognition is an important topic in the field of pattern recognition and computer graphic. We propose a novel approach for 3D face recognition using local conformal parameterization and iso-geodesic stripes. In our framework, the 3D facial surface is considered as a Riemannian 2-manifold. The surface is mapped into the 2D circle parameter domain using local conformal parameterization. In the parameter domain, the geometric features are extracted from the iso-geodesic stripes. Combining the relative position measure, Chain 2D Weighted Walkthroughs (C2DWW), the 3D face matching results can be obtained. The geometric features from iso-geodesic stripes in parameter domain are robust in terms of head poses, facial expressions, and some occlusions. In the experiments, our method achieves a high recognition accuracy of 3D facial data from the Texas3D and Bosphorus3D face database. © 2018 Chenlei Lv and Junli Zhao.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sghaier201860,
author={Sghaier, S. and Farhat, W. and Souani, C.},
title={Novel technique for 3D face recognition using anthropometric methodology},
journal={International Journal of Ambient Computing and Intelligence},
year={2018},
volume={9},
number={1},
pages={60-77},
doi={10.4018/IJACI.2018010104},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033368802&doi=10.4018%2fIJACI.2018010104&partnerID=40&md5=1e050c1862e71c9d68adc7216127aea2},
affiliation={Faculty of Sciences, Monastir University, Monastir, Tunisia; National School of Engineers, Sousse University, Sousse, Tunisia; Higher Institute of Applied Sciences and Technology, Sousse University, Sousse, Tunisia},
abstract={This manuscript presents an improved system research that can detect and recognize the person in 3D space automatically and without the interaction of the people's faces. This system is based not only on a quantum computation and measurements to extract the vector features in the phase of characterization but also on learning algorithm (using SVM) to classify and recognize the person. This research presents an improved technique for automatic 3D face recognition using anthropometric proportions and measurement to detect and extract the area of interest which is unaffected by facial expression. This approach is able to treat incomplete and noisy images and reject the non-facial areas automatically. Moreover, it can deal with the presence of holes in the meshed and textured 3D image. It is also stable against small translation and rotation of the face. All the experimental tests have been done with two 3D face datasets FRAV 3D and GAVAB. Therefore, the test's results of the proposed approach are promising because they showed that it is competitive comparable to similar approaches in terms of accuracy, robustness, and flexibility. It achieves a high recognition performance rate of 95.35% for faces with neutral and non-neutral expressions for the identification and 98.36% for the authentification with GAVAB and 100% with some gallery of FRAV 3D datasets. © Copyright 2018, IGI Global.},
author_keywords={3D Face;  Anthropometric;  Euclidean Distance;  Eye Corners;  Feature Extraction;  Learning;  Measurements;  Nose Tip},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ratyal20184903,
author={Ratyal, N. and Taj, I. and Bajwa, U. and Sajid, M.},
title={Pose and expression invariant alignment based multi-view 3d face recognition},
journal={KSII Transactions on Internet and Information Systems},
year={2018},
volume={12},
number={10},
pages={4903-4929},
doi={10.3837/tiis.2018.10.016},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057280833&doi=10.3837%2ftiis.2018.10.016&partnerID=40&md5=b7937933dbf1206dd6e5a81101a545c0},
affiliation={Vision and Pattern Recognition Systems Research Group, Capital University of Science and Technology (CUST), Islamabad, Pakistan; Department of Computer Science, COMSATS Institute of Information Technology, Lahore, Pakistan},
abstract={In this study, a fully automatic pose and expression invariant 3D face alignment algorithm is proposed to handle frontal and profile face images which is based on a two pass course to fine alignment strategy. The first pass of the algorithm coarsely aligns the face images to an intrinsic coordinate system (ICS) through a single 3D rotation and the second pass aligns them at fine level using a minimum nose tip-scanner distance (MNSD) approach. For facial recognition, multi-view faces are synthesized to exploit real 3D information and test the efficacy of the proposed system. Due to optimal separating hyper plane (OSH), Support Vector Machine (SVM) is employed in multi-view face verification (FV) task. In addition, a multi stage unified classifier based face identification (FI) algorithm is employed which combines results from seven base classifiers, two parallel face recognition algorithms and an exponential rank combiner, all in a hierarchical manner. The performance figures of the proposed methodology are corroborated by extensive experiments performed on four benchmark datasets: GavabDB, Bosphorus, UMB-DB and FRGC v2.0. Results show mark improvement in alignment accuracy and recognition rates. Moreover, a computational complexity analysis has been carried out for the proposed algorithm which reveals its superiority in terms of computational efficiency as well. © 2018 KSII.},
author_keywords={3D alignment;  3D FR;  Profile face;  SVM;  Unified classifier},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dai2018939,
author={Dai, P. and Wang, X. and Zhang, W.},
title={Coarse-to-fine multiview 3d face reconstruction using multiple geometrical features},
journal={Multimedia Tools and Applications},
year={2018},
volume={77},
number={1},
pages={939-966},
doi={10.1007/s11042-016-4325-y},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008517643&doi=10.1007%2fs11042-016-4325-y&partnerID=40&md5=dd454616b801c79f2d39e615445e75ca},
affiliation={State Key Laboratory of Precision Measurement Technology and Instruments, Department of Precision Instrument, Tsinghua University, Beijng, 100084, China},
abstract={3D face reconstruction from multi-view video sequences has become a hotspot in computer vision for the last decades. Structure from Motion (SfM) methods, which have been widely used for multi-view 3D face reconstruction, have two main limitations. First, self-occlusion causes certain facial feature points (FFPs) to be invisible in the images, which will lead to missing data. The existing SfM methods could recover the missing data through iterative calculation, however, with high computational costs and long processing time. Second, the SfM methods cannot reconstruct the accurate 3D facial shapes of cheeks because there are no FFPs in this area. This paper proposes a novel “coarse-to-fine” multi-view 3D face reconstruction method by taking the advantage of the complementarity between FFPs and occluding contours, i.e., the boundary lines depicted between the facial region and the background. In this method, a block SfM algorithm is firstly proposed to reconstruct a “coarse” 3D facial shape by utilizing sparse FFPs. The block SfM algorithm does not estimate the true locations of the self-occluded FFPs iteratively. Thus, the computational cost is significantly reduced. Then, a kernel partial least squares (KPLS) algorithm is introduced to refine the “coarse” 3D facial shape. The KPLS method applies occluding contours to remedy the limitation of sparse FFPs correspondence-based SfM method. The proposed method is evaluated on the synthetic sequences generated from the BJUT-3D face database and the real-world multi-view video sequences obtained in a controlled indoor environment. The results show improvements in both accuracy and efficiency. © 2017, Springer Science+Business Media New York.},
author_keywords={3D face reconstruction;  Coarse-to-fine;  Facial feature points;  Multi-view;  Occluding contours;  Structure from motion},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pala201895,
author={Pala, P. and Seidenari, L. and Berretti, S. and Del Bimbo, A.},
title={Person re-identification from depth cameras using skeleton and 3D face data},
journal={Eurographics Workshop on 3D Object Retrieval, EG 3DOR},
year={2018},
volume={2018-April},
pages={95-101},
doi={10.2312/3dor.20181058},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061202945&doi=10.2312%2f3dor.20181058&partnerID=40&md5=e3983b8db3fafe0ac7edc126398a0093},
affiliation={University of Florence, Media Integration and Communication Center (MICC), Italy},
abstract={In the typical approach, person re-identification is performed using appearance in 2D still images or videos, thus invalidating any application in which a person may change dress across subsequent acquisitions. For example, this is a relevant scenario for home patient monitoring. Depth cameras enable person re-identification exploiting 3D information that captures biometric cues such as face and characteristic dimensions of the body. Unfortunately, face and skeleton quality is not always enough to grant a correct recognition from depth data. Both features are affected by the pose of the subject and the distance from the camera. In this paper, we propose a model to incorporate a robust skeleton representation with a highly discriminative face feature, weighting samples by their quality. Our method combining face and skeleton data improves rank-1 accuracy compared to individual cues especially on short realistic sequences. © 2018 The Eurographics Association.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen201856,
author={Nguyen, D.T. and Khachumov, V.M. and Khachumov, M.V. and Salpagarov, S.I. and Yakovlev, K.S.},
title={On the estimation of accuracy and stability of 3D face modeling using a pair of stereo cameras},
journal={CEUR Workshop Proceedings},
year={2018},
volume={2236},
pages={56-64},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062424913&partnerID=40&md5=0817bd89dd1130fd1d2eb176bc68be47},
affiliation={Department of Information Technologies, Peoples’ Friendship University of Russia, 6 Miklukho-Maklaya str., Moscow, 117198, Russian Federation; Federal Research Centre “Computer Science and Control” of RAS, 44/2 Vavilova str., Moscow, 119333, Russian Federation},
abstract={The study deals with the problem of estimating the accuracy and stability of 3D face models obtained by a stereo pair. The problem of the conditionality of the fundamental matrix, which is a mathematical stereo pair model, is considered. We prove that small changes of stereo camera parameters result in small changes in the solution of the problem of reconstructing three-dimensional coordinates. Several types of three-dimensional reconstruction optimization problems that are based on quality criteria are formulated. The paper also considers the issues of determining an object orientation in a three-dimensional space by position lines. A 3D image coding system utilizing invariant moments is proposed and the theoretical sensitivity of 3D invariants to geometric distortions is investigated. These results are used to obtain scaling invariants. Designing and studying such models is the important step to solve the analysis problem and determine the proximity of images, which is therefore necessary for their clustering and recognition. Copyright © 2018 for the individual papers by the papers’ authors.},
author_keywords={Face models;  Face recognition;  Image;  Invariant moments;  Reconstruction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Banita20182325,
author={Banita and Tanwar, P.},
title={Evaluation of 3d facial paralysis using fuzzy logic},
journal={International Journal of Engineering and Technology(UAE)},
year={2018},
volume={7},
number={4},
pages={2325-2331},
doi={10.14419/ijet.v7i4.13619},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053901681&doi=10.14419%2fijet.v7i4.13619&partnerID=40&md5=f68480ef298793b35ad707b17c64ab59},
affiliation={Lingaya's University, Faridabad, India; Manav Rachana University, Faridabad, India},
abstract={Face recognition are of great interest to researchers in terms of Image processing and Computer Graphics. In recent years, various factors become popular which clearly affect the face model. Which are ageing, universal facial expressions, and muscle movement. Similarly in terms of medical terminology the facial paralysis can be peripheral or central depending on the level of motor neuron lesion which can be below the nucleus of the nerve or supra nuclear. The various medical therapy used for facial paralysis are electroaccupunture, electro-therapy, laser acupuncture, manual acupuncture which is a traditional form of acupuncture. Imaging plays a great role in evaluation of degree of paralysis and also for faces recognition. There is a wide research in terms of facial expressions and facial recognition but lim-ited research work is available in facial paralysis. House- Brackmann Grading system is one of the simplest and easiest method to evalu-ate the degree of facial paralysis. During evaluation common facial expressions are recorded and are further evaluated by considering the focal points of the left or the right side of the face. This paper presents the classification of face recognition and its respective fuzzy rules to remove uncertainty in the result after evaluation of facial paralysis. © 2018 Banita, Dr. Poonam Tanwar.},
author_keywords={3D face recognition;  CNN;  Evaluation of facial paralysis;  MAMDANI model;  Stages of face recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mohanraj20181411,
author={Mohanraj, V. and Sibi Chakkaravarthy, S. and Gogul, I. and Sathiesh Kumar, V. and Kumar, R. and Vaidehi, V.},
title={Hybrid feature descriptors to detect face spoof attacks},
journal={Journal of Intelligent and Fuzzy Systems},
year={2018},
volume={34},
number={3},
pages={1411-1419},
doi={10.3233/JIFS-169436},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044718133&doi=10.3233%2fJIFS-169436&partnerID=40&md5=2cdd965696efd65b1b7469b3b81f32f0},
affiliation={Department of Electronics Engineering, Madras Institute of Technology, Anna University, Chennai, India; Department of Atomic Energy, Nuclear Controls and Atomic Energy, Mumbai, India; Department of Computer Science and Engineering, VIT University, Chennai Campus, India},
abstract={Face Recognition is widely used applications such as of mobile phone unlocking, credit card authentication and person authentication in airports. The face biometric authentication system can be easily spoofed by printed photograph, replay video of the legitimate user and 3D face mask. This paper proposes hybrid feature descriptors to detect the face spoofing attack (printed photograph and replay video attacks). The proposed method extracts three different feature descriptors such as Color moment, Haralick texture and Color Local Binary Pattern (CLBP) feature descriptors. The extracted features are concatenated and classified by Logistic Regression. The performance of the proposed method is evaluated on the Michigan State University Mobile Face Spoofing Database (MSU-MFSD) dataset and found to achieve better results than state-of-the-art methods. © 2018 - IOS Press and the authors. All rights reserved.},
author_keywords={color moment;  Face recognition;  Haralick texture;  Local Binary Pattern (LBP);  spoof detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mosslah201810482,
author={Mosslah, A.A. and Mahdi, R.H.},
title={3DMM fitting for 3D face reconstruction},
journal={Journal of Engineering and Applied Sciences},
year={2018},
volume={13},
number={24},
pages={10482-10489},
doi={10.3923/jeasci.2018.10482.10489},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059119186&doi=10.3923%2fjeasci.2018.10482.10489&partnerID=40&md5=70419672a342ff6bb1c80c911d7ebc4a},
affiliation={College of Islamic Science, University of Anbar, Anbar, Iraq; Department of Computer Science, College of Science, University of Mustansiriyah, Baghdad, Iraq},
abstract={Most facial recognition techniques are self-contained on a three-dimensional model to ensure the challenge posed by facial expressions that are variable depending on the situation. In this research we aim to provide the latest SDK package which is used to identify faces with attention to flipping because the face changes its expressions and using the technique of 3D-expandable Model 3DMM we can introduce facial changes, 3DMM also enables us to isolate identity variations from those resulting from changes in facial expressions. We face two problems that need to be addressed: accurate measurement of the parameters of the situation and computational efficiency. When the verification is performed with the adjustment, a new face view is created where the situation is corrected and the expression is disabled to define the expression we provide two methods for it. The first depends on the prior knowledge to illustrate the neutral expression image of the input image. While the second method is based on the idea of verification on the transfer of expression of the exposed face to the probe. Experiments using neutral and equivalent view with the FR SDK commercial standard are demonstrated on two-face databases, PIE and AR, thus, demonstrating a significant improvement in the experimental SDK's performance in terms of expression. © Medwell Journals, 2018.},
author_keywords={3DMM;  Facial expressions;  Flipping;  Identity;  SDK package;  Significant},
document_type={Article},
source={Scopus},
}

@ARTICLE{Benedek2018101,
author={Benedek, C. and Galai, B. and Nagy, B. and Janko, Z.},
title={Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance System},
journal={IEEE Transactions on Circuits and Systems for Video Technology},
year={2018},
volume={28},
number={1},
pages={101-113},
doi={10.1109/TCSVT.2016.2595331},
art_number={7524772},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040620999&doi=10.1109%2fTCSVT.2016.2595331&partnerID=40&md5=f25fc1605a4bc2ffe152bccff543d975},
affiliation={Distributed Events Analysis Research Laboratory, Institute for Computer Science and Control, Hungarian Academy of Sciences, Budapest, 1111, Hungary; Péter Pázmány Catholic University, Budapest, 1083, Hungary},
abstract={This paper presents new approaches for gait and activity analysis based on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed algorithms are embedded into an integrated 4D vision and visualization system, which is able to analyze and interactively display real scenarios in natural outdoor environments with walking pedestrians. The main focus of the investigations is gait-based person reidentification during tracking and recognition of specific activity patterns, such as bending, waving, making phone calls, and checking the time looking at wristwatches. The descriptors for training and recognition are observed and extracted from realistic outdoor surveillance scenarios, where multiple pedestrians are walking in the field of interest following possibly intersecting trajectories; thus, the observations might often be affected by occlusions or background noise. Since there is no public database available for such scenarios, we created and published a new Lidar-based outdoor gait and activity data set on our website that contains point cloud sequences of 28 different persons extracted and aggregated from 35-min-long measurements. The presented results confirm that both efficient gait-based identification and activity recognition are achievable in the sparse point clouds of a single RMB Lidar sensor. After extracting the people trajectories, we synthesized a free-viewpoint video, in which moving avatar models follow the trajectories of the observed pedestrians in real time, ensuring that the leg movements of the animated avatars are synchronized with the real gait cycles observed in the Lidar stream. © 1991-2012 IEEE.},
author_keywords={4D reconstruction;  activity recognition;  gait recognition;  multibeam Lidar},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lane2018,
author={Lane, S. and Kira, Z. and James, R. and Carr, D. and Tuell, G.},
title={Landing zone identification for autonomous UAV applications using fused hyperspectral imagery and LIDAR point clouds},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2018},
volume={10644},
doi={10.1117/12.2305136},
art_number={106440D},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050863160&doi=10.1117%2f12.2305136&partnerID=40&md5=c903d04c71626afc6ebbb9d747c8a92b},
affiliation={Georgia Tech Research Institute, Electro-Optical Systems Laboratory, 925 Dalney Street, Atlanta, GA  30332, United States; Georgia Tech Research Institute, Aerospace, Transportation and Advanced Systems Laboratory, 250 14th Street, Atlanta, GA  30332, United States; 3D Ideas, LLC, 651 North Main Street, Madison, GA  30650, United States},
abstract={Multi-modal data fusion for situational awareness is of interest because fusion of data can provide more information than the individual modalities alone. However, many questions remain, including what data is beneficial, what algorithms work the best or are fastest, and where in the processing pipeline should data be fused? In this paper, we explore some of these questions through a processing pipeline designed for multi-modal data fusion in an autonomous UAV landing scenario. In this paper, we assess landing zone identification methods using two data modalities: hyperspectral imagery and LIDAR point clouds. Using hyperspectral image and LIDAR data from two datasets of Maui and a university campus, we assess the accuracies of different landing zone identification methods, compare rule-based and machine learning based classifications, and show that depending on the dataset, fusion does not always increase performance. However, we show that machine learning methods can be used to ascertain the usefulness of individual modalities and their resulting attributes when used to perform classification. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author_keywords={Autonomous UAV;  Hyperspectral imagery;  LIDAR;  Multi-modal data fusion},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu20174733,
author={Yu, R. and Saito, S. and Li, H. and Ceylan, D. and Li, H.},
title={Learning Dense Facial Correspondences in Unconstrained Images},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2017},
volume={2017-October},
pages={4733-4742},
doi={10.1109/ICCV.2017.506},
art_number={8237768},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041899370&doi=10.1109%2fICCV.2017.506&partnerID=40&md5=efc44367116100c6e512022358ecff96},
affiliation={University of Southern California, United States; Adobe Research, United States; Pinscreen, United States; USC Institute for Creative Technologies, United States},
abstract={We present a minimalists but effective neural network that computes dense facial correspondences in highly unconstrained RGB images. Our network learns a per-pixel flow and a matchability mask between 2D input photographs of a person and the projection of a textured 3D face model. To train such a network, we generate a massive dataset of synthetic faces with dense labels using renderings of a morphable face model with variations in pose, expressions, lighting, and occlusions. We found that a training refinement using real photographs is required to drastically improve the ability to handle real images. When combined with a facial detection and 3D face fitting step, we show that our approach outperforms the state-of-the-art face alignment methods in terms of accuracy and speed. By directly estimating dense correspondences, we do not rely on the full visibility of sparse facial landmarks and are not limited to the model space of regression-based approaches. We also assess our method on video frames and demonstrate successful per-frame processing under extreme pose variations, occlusions, and lighting conditions. Compared to existing 3D facial tracking techniques, our fitting does not rely on previous frames or frontal facial initialization and is robust to imperfect face detections. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xiao20171642,
author={Xiao, S. and Feng, J. and Liu, L. and Nie, X. and Wang, W. and Yan, S. and Kassim, A.},
title={Recurrent 3D-2D Dual Learning for Large-Pose Facial Landmark Detection},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2017},
volume={2017-October},
pages={1642-1651},
doi={10.1109/ICCV.2017.181},
art_number={8237443},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041905307&doi=10.1109%2fICCV.2017.181&partnerID=40&md5=ac281f2831b41093e971a16ed8236474},
affiliation={National University of Singapore, Singapore; Qihoo-360, Singapore; University of Trento, Italy},
abstract={Despite remarkable progress of face analysis techniques, detecting landmarks on large-pose faces is still difficult due to self-occlusion, subtle landmark difference and incomplete information. To address these challenging issues, we introduce a novel recurrent 3D-2D dual learning model that alternatively performs 2D-based 3D face model refinement and 3D-to-2D projection based 2D landmark refinement to reliably reason about self-occluded landmarks, precisely capture the subtle landmark displacement and accurately detect landmarks even in presence of extremely large poses. The proposed model presents the first loop-closed learning framework that effectively exploits the informative feedback from the 3D-2D learning and its dual 2D-3D refinement tasks in a recurrent manner. Benefiting from these two mutual-boosting steps, our proposed model demonstrates appealing robustness to large poses (up to profile pose) and outstanding ability to capture fine-scale landmark displacement compared with existing 3D models. It achieves new state-of-the-art on the challenging AFLW benchmark. Moreover, our proposed model introduces a new architectural design that economically utilizes intermediate features and achieves 4× faster speed than its deep learning based counterparts. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hu2017133,
author={Hu, H. and Shah, S.A.A. and Bennamoun, M. and Molton, M.},
title={2D and 3D face recognition using convolutional neural network},
journal={IEEE Region 10 Annual International Conference, Proceedings/TENCON},
year={2017},
volume={2017-December},
pages={133-138},
doi={10.1109/TENCON.2017.8227850},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044210743&doi=10.1109%2fTENCON.2017.8227850&partnerID=40&md5=12f531513eddb3716404e891cd002a29},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA  6009, Australia},
abstract={Face recognition remains a challenge today as recognition performance is strongly affected by variability such as illumination, expressions and poses. In this work we apply Convolutional Neural Networks (CNNs) on the challenging task of both 2D and 3D face recognition. We constructed two CNN models, namely CNN-1 (two convolutional layers) and CNN-2 (one convolutional layer) for testing on 2D and 3D dataset. A comprehensive parametric study of two CNN models on face recognition is represented in which different combinations of activation function, learning rate and filter size are investigated. We find that CNN-2 has a better accuracy performance on both 2D and 3D face recognition. Our experimental results show that an accuracy of 85.15% was accomplished using CNN-2 on depth images with FRGCv2.0 dataset (4950 images with 557 objectives). An accuracy of 95% was achieved using CNN-2 on 2D raw image with the AT&T dataset (400 images with 40 objectives). The results indicate that the proposed CNN model is capable to handle complex information from facial images in different dimensions. These results provide valuable insights into further application of CNN on 3D face recognition. © 2017 IEEE.},
author_keywords={Convolutional Neural Networks;  Depth Image;  Face Recognition},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li20172816,
author={Li, H. and Sun, J. and Xu, Z. and Chen, L.},
title={Multimodal 2D+3D Facial Expression Recognition with Deep Fusion Convolutional Neural Network},
journal={IEEE Transactions on Multimedia},
year={2017},
volume={19},
number={12},
pages={2816-2831},
doi={10.1109/TMM.2017.2713408},
art_number={7944639},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021792202&doi=10.1109%2fTMM.2017.2713408&partnerID=40&md5=625884c31512d33a29adc41b2d2ff61b},
affiliation={Institute for Information and System Sciences, School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, 710049, China; LIRIS UMR 5205, Department of Mathematics and Informatics, Ecole Centrale DeLyon, Lyon, 69134, France},
abstract={This paper presents a novel and efficient deep fusion convolutional neural network (DF-CNN) for multimodal 2D+3D facial expression recognition (FER). DF-CNN comprises a feature extraction subnet, a feature fusion subnet, and a softmax layer. In particular, each textured three-dimensional (3D) face scan is represented as six types of 2D facial attribute maps (i.e., geometry map, three normal maps, curvature map, and texture map), all of which are jointly fed into DF-CNN for feature learning and fusion learning, resulting in a highly concentrated facial representation (32-dimensional). Expression prediction is performed by two ways: 1) learning linear support vector machine classifiers using the 32-dimensional fused deep features, or 2) directly performing softmax prediction using the six-dimensional expression probability vectors. Different from existing 3D FER methods, DF-CNN combines feature learning and fusion learning into a single end-to-end training framework. To demonstrate the effectiveness of DF-CNN, we conducted comprehensive experiments to compare the performance of DF-CNN with handcrafted features, pre-trained deep features, fine-tuned deep features, and state-of-the-art methods on three 3D face datasets (i.e., BU-3DFE Subset I, BU-3DFE Subset II, and Bosphorus Subset). In all cases, DF-CNN consistently achieved the best results. To the best of our knowledge, this is the first work of introducing deep CNN to 3D FER and deep learning-based feature-level fusion for multimodal 2D+3D FER. © 2017 IEEE.},
author_keywords={Deep fusion convolutional neural network (DF-CNN);  facial expression recognition (FER);  multimodal;  textured three-dimensional (3D) face scan},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sibbing2017285,
author={Sibbing, D. and Kobbelt, L.},
title={Building a Large Database of Facial Movements for Deformation Model-Based 3D Face Tracking},
journal={Computer Graphics Forum},
year={2017},
volume={36},
number={8},
pages={285-301},
doi={10.1111/cgf.13080},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013662978&doi=10.1111%2fcgf.13080&partnerID=40&md5=3bb9743928540ba3c26d8b5a8d7c828d},
affiliation={RWTH Aachen University, Germany},
abstract={We introduce a new markerless 3D face tracking approach for 2D videos captured by a single consumer grade camera. Our approach takes detected 2D facial features as input and matches them with projections of 3D features of a deformable model to determine its pose and shape. To make the tracking and reconstruction more robust we add a smoothness prior for pose and deformation changes of the faces. Our major contribution lies in the formulation of the deformation prior which we derive from a large database of facial animations showing different (dynamic) facial expressions of a fairly large number of subjects. We split these animation sequences into snippets of fixed length which we use to predict the facial motion based on previous frames. In order to keep the deformation model compact and independent from the individual physiognomy, we represent it by deformation gradients (instead of vertex positions) and apply a principal component analysis in deformation gradient space to extract the major modes of facial deformation. Since the facial deformation is optimized during tracking, it is particularly easy to apply them to other physiognomies and thereby re-target the facial expressions. We demonstrate the effectiveness of our technique on a number of examples. © 2017 The Authors Computer Graphics Forum © 2017 The Eurographics Association and John Wiley & Sons Ltd.},
author_keywords={data-driven animation;  facial animation;  markerless performance capture;  tracking},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Soltanpour2017560,
author={Soltanpour, S. and Wu, Q.M.J.},
title={Multiscale depth local derivative pattern for sparse representation based 3D face recognition},
journal={2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017},
year={2017},
volume={2017-January},
pages={560-565},
doi={10.1109/SMC.2017.8122665},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044201680&doi=10.1109%2fSMC.2017.8122665&partnerID=40&md5=5918c06bbe0b6cc90d03ba306b0d1258},
affiliation={Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada},
abstract={3D face recognition is a popular research area due to its vast application in biometrics and security. Local featurebased methods gain importance in the recent years due to their robustness under degradation conditions. In this paper, a novel high-order local pattern descriptor in combination with sparse representation based classifier (SRC) is proposed for expression robust 3D face recognition. 3D point clouds are converted to depth maps after preprocessing. Multi-directional derivatives are applied in spatial space to encode the depth maps based on the local derivative pattern (LDP) scheme. Directional pattern features are calculated according to local derivative variations. Since LDP computes spatial relationship of neighbors in a local region, it extracts distinct information from the depth map. Multiscale depth-LDP is presented as a novel descriptor for 3D face recognition. The descriptor is employed along with the SRC to increase the range data distinctiveness. A histogram on the derivative pattern creates a spatial feature descriptor that represents the distinctive micro-patterns from 3D data. We evaluate the proposed algorithm on two famous 3D face databases, FRGC v2.0 and Bosphorus. The experimental results demonstrate that the proposed approach achieves acceptable performance under facial expression. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mohsin20172736,
author={Mohsin, N. and Payandeh, S.},
title={Localization and identification of body extremities based on data from multiple depth sensors},
journal={2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017},
year={2017},
volume={2017-January},
pages={2736-2741},
doi={10.1109/SMC.2017.8123040},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044185896&doi=10.1109%2fSMC.2017.8123040&partnerID=40&md5=0e7d2fe0dc2a1e53991f329e3459bc8d},
affiliation={Networked Robotics and Sensing Laboratory, School of Engineering Science, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada},
abstract={This paper explores the novel use of multiple depth sensors to overcome occlusions and improve localization and tracking of body extremities. The usage of data from only depth sensors not only overcomes visual challenges associated with RGB sensors under low illumination, but also protects the identity of surveyed person with high confidentiality. For integrating depth information from multiple sources, the paper presents first an overview of a novel calibration method for multiple depth sensors. In case of occlusion of any fiducial point in the primary sensor's depth image, co-ordinates of the point can be obtained from the frame of other sensors using the calibration parameters. To localize salient body parts such as hands, head and feet, a surface triangular mesh is applied on generated 3D point cloud from the primary sensor. The geodesic extrema from the mesh coincide with body extremities. The body extremities can be identified based on those relative geodesic distances between the extremities. Once the body parts are labelled, a portion of body can be targeted and evaluated for specific gait analysis and visualization. For the performance evaluation, our calibration method has fared well in comparison to other available techniques. Also, our proposed localization of salient body parts is able to successfully tag the specific body part i.e. the head region. © 2017 IEEE.},
author_keywords={Body extremities localization;  Geodesic distances;  Kinect II;  Multiple depth sensors calibration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sheng20174598,
author={Sheng, L. and Cai, J. and Cham, T.-J. and Pavlovic, V. and Ngan, K.N.},
title={A generative model for depth-based robust 3D facial pose tracking},
journal={Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
year={2017},
volume={2017-January},
pages={4598-4607},
doi={10.1109/CVPR.2017.489},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035237020&doi=10.1109%2fCVPR.2017.489&partnerID=40&md5=daff569975d02ae32e05b71ff49e2bac},
affiliation={Chinese University of Hong Kong, Hong Kong; Nanyang Technological University, Hong Kong; Rutgers University, United States},
abstract={We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dou20171503,
author={Dou, P. and Shah, S.K. and Kakadiaris, I.A.},
title={End-to-end 3D face reconstruction with deep neural networks},
journal={Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
year={2017},
volume={2017-January},
pages={1503-1512},
doi={10.1109/CVPR.2017.164},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044314381&doi=10.1109%2fCVPR.2017.164&partnerID=40&md5=1615b5d2fe4392da39696f4091862ba8},
affiliation={Computational Biomedicine Lab., University of Houston, 4800 Calhoun Road, Houston, TX  77004, United States},
abstract={Monocular 3D facial shape reconstruction from a single 2D facial image has been an active research area due to its wide applications. Inspired by the success of deep neural networks (DNN), we propose a DNN-based approach for End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different from recent works that reconstruct and refine the 3D face in an iterative manner using both an RGB image and an initial 3D facial shape rendering, our DNN model is end-to-end, and thus the complicated 3D rendering process can be avoided. Moreover, we integrate in the DNN architecture two components, namely a multi-task loss function and a fusion convolutional neural network (CNN) to improve facial expression reconstruction. With the multi-task loss function, 3D face reconstruction is divided into neutral 3D facial shape reconstruction and expressive 3D facial shape reconstruction. The neutral 3D facial shape is class-specific. Therefore, higher layer features are useful. In comparison, the expressive 3D facial shape favors lower or intermediate layer features. With the fusion-CNN, features from different intermediate layers are fused and transformed for predicting the 3D expressive facial shape. Through extensive experiments, we demonstrate the superiority of our end-to-end framework in improving the accuracy of 3D face reconstruction. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Saito20172326,
author={Saito, S. and Wei, L. and Hu, L. and Nagano, K. and Li, H.},
title={Photorealistic facial texture inference using deep neural networks},
journal={Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
year={2017},
volume={2017-January},
pages={2326-2335},
doi={10.1109/CVPR.2017.250},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041916909&doi=10.1109%2fCVPR.2017.250&partnerID=40&md5=85db7359df9e17735da0281404cdcd41},
affiliation={Pinscreen, United States; University of Southern California, United States; USC Institute for Creative Technologies, United States},
abstract={We present a data-driven inference method that can synthesize a photorealistic texture map of a complete 3D face model given a partial 2D view of a person in the wild. After an initial estimation of shape and low-frequency albedo, we compute a high-frequency partial texture map, without the shading component, of the visible face area. To extract the fine appearance details from this incomplete input, we introduce a multi-scale detail analysis technique based on mid-layer feature correlations extracted from a deep convolutional neural network. We demonstrate that fitting a convex combination of feature correlations from a high-resolution face database can yield a semantically plausible facial detail description of the entire face. A complete and photorealistic texture map can then be synthesized by iteratively optimizing for the reconstructed feature correlations. Using these high-resolution textures and a commercial rendering framework, we can produce high-fidelity 3D renderings that are visually comparable to those obtained with state-of-the-art multi-view face capture systems. We demonstrate successful face reconstructions from a wide range of low resolution input images, including those of historical figures. In addition to extensive evaluations, we validate the realism of our results using a crowdsourced user study. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2017,
author={Liu, Z. and Cohen, F.},
title={Synthesis and identification of three-dimensional faces from image(s) and three-dimensional generic models},
journal={Journal of Electronic Imaging},
year={2017},
volume={26},
number={6},
doi={10.1117/1.JEI.26.6.063005},
art_number={063005},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034836161&doi=10.1117%2f1.JEI.26.6.063005&partnerID=40&md5=cbf0b712fa254c1cccec3850f657b1dd},
affiliation={Kepler Group LLC, New York, NY, United States; Drexel University, Electrical and Computer Engineering, Philadelphia, PA, United States},
abstract={We describe an approach for synthesizing a three-dimensional (3-D) face structure from an image or images of a human face taken at a priori unknown poses using gender and ethnicity specific 3-D generic models. The synthesis process starts with a generic model, which is personalized as images of the person become available using preselected landmark points that are tessellated to form a high-resolution triangular mesh. From a single image, two of the three coordinates of the model are reconstructed in accordance with the given image of the person, while the third coordinate is sampled from the generic model, and the appearance is made in accordance with the image. With multiple images, all coordinates and appearance are reconstructed in accordance with the observed images. This method allows for accurate pose estimation as well as face identification in 3-D rendering of a difficult two-dimensional (2-D) face recognition problem into a much simpler 3-D surface matching problem. The estimation of the unknown pose is achieved using the Levenberg-Marquardt optimization process. Encouraging experimental results are obtained in a controlled environment with high-resolution images under a good illumination condition, as well as for images taken in an uncontrolled environment under arbitrary illumination with low-resolution cameras. © 2017 SPIE and IS&T.},
author_keywords={3-D face recognition;  3-D face synthesis;  iterative personalization;  pose estimation;  ray tracing;  subdivision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shui201733,
author={Shui, W. and Zhou, M. and Maddock, S. and He, T. and Wang, X. and Deng, Q.},
title={A PCA-Based method for determining craniofacial relationship and sexual dimorphism of facial shapes},
journal={Computers in Biology and Medicine},
year={2017},
volume={90},
pages={33-49},
doi={10.1016/j.compbiomed.2017.08.023},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029385701&doi=10.1016%2fj.compbiomed.2017.08.023&partnerID=40&md5=e08c10cb34c385e267f3821fc9436eec},
affiliation={College of Information Science and Technology, Beijing Normal University, Beijing, 100875, China; Beijing Key Laboratory of Digital Preservation and Virtual Reality for Cultural Heritage, Beijing, 100875, China; Department of Computer Science, University of Sheffield, Sheffield, S10 2TN, United Kingdom; Affiliated Hospital of Shaanxi University of Chinese Medicine, Xianyang, 712000, China},
abstract={Previous studies have used principal component analysis (PCA) to investigate the craniofacial relationship, as well as sex determination using facial factors. However, few studies have investigated the extent to which the choice of principal components (PCs) affects the analysis of craniofacial relationship and sexual dimorphism. In this paper, we propose a PCA-based method for visual and quantitative analysis, using 140 samples of 3D heads (70 male and 70 female), produced from computed tomography (CT) images. There are two parts to the method. First, skull and facial landmarks are manually marked to guide the model's registration so that dense corresponding vertices occupy the same relative position in every sample. Statistical shape spaces of the skull and face in dense corresponding vertices are constructed using PCA. Variations in these vertices, captured in every principal component (PC), are visualized to observe shape variability. The correlations of skull- and face-based PC scores are analysed, and linear regression is used to fit the craniofacial relationship. We compute the PC coefficients of a face based on this craniofacial relationship and the PC scores of a skull, and apply the coefficients to estimate a 3D face for the skull. To evaluate the accuracy of the computed craniofacial relationship, the mean and standard deviation of every vertex between the two models are computed, where these models are reconstructed using real PC scores and coefficients. Second, each PC in facial space is analysed for sex determination, for which support vector machines (SVMs) are used. We examined the correlation between PCs and sex, and explored the extent to which the choice of PCs affects the expression of sexual dimorphism. Our results suggest that skull- and face-based PCs can be used to describe the craniofacial relationship and that the accuracy of the method can be improved by using an increased number of face-based PCs. The results show that the accuracy of the sex classification is related to the choice of PCs. The highest sex classification rate is 91.43% using our method. © 2017 Elsevier Ltd},
author_keywords={Computed tomography (CT);  Craniofacial relationship;  Dense corresponding vertices;  Sexual dimorphism;  Shape variability;  Statistical shape spaces},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fangmin201771,
author={Fangmin, L. and Ke, C. and Xinhua, L.},
title={3D Face Reconstruction Based on Convolutional Neural Network},
journal={Proceedings - 10th International Conference on Intelligent Computation Technology and Automation, ICICTA 2017},
year={2017},
volume={2017-October},
pages={71-74},
doi={10.1109/ICICTA.2017.23},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047221254&doi=10.1109%2fICICTA.2017.23&partnerID=40&md5=8dc413b13fda3fb79e0a929f02f6c907},
affiliation={School of Information Engineering, Wuhan University of Technology, Wuhan, 430070, China; School of Computer Engineering and Applied Mathematics, Changsha University, Changsha, 410022, China},
abstract={Fast and robust 3D reconstruction of facial geometric structure from a single image is a challenging task with numerous applications, but there exist two problems when applied 'in the wild': the 3D estimates are unstable for different photos of the same subject; the 3D estimates are over-regularized and generic. In response, a robust method for regressing discriminative 3D morphable face models(3DMM) is described to support face recognition and 3D mask printing. Combining the local data sets with the public data sets, improving the exiting 3DMM fitting method and then using a convolutional neural network(CNN) to improve reconstruction effect. The ground truth 3D faces of the CNN are the pooled 3DMM parameters extracted from the photos of the same subject. Using CNN to regress 3DMM shape and texture parameters directly from an input photo and offering a method for generating huge numbers of labeled examples. There are two key points of the paper: one is the training data generation for the model training; the other is the training of 3D reconstruction model. Experimental results and analysis show that this method costs much less time than traditional methods of 3D face modeling, and it is improved for different races on photos with any angles than the existing methods based on deep learning, and the system has better robustness. © 2017 IEEE.},
author_keywords={3D face reconstruction;  3DMM;  Convolutional neural network(CNN);  Shape;  Texture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Frikha2017,
author={Frikha, T. and Chaabane, F. and Said, B. and Drira, H. and Abid, M. and Ben Amar, C. and Lille, L.},
title={Embedded approach for a Riemannian-based framework of analyzing 3D faces},
journal={Proceedings - 3rd International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2017},
year={2017},
doi={10.1109/ATSIP.2017.8075548},
art_number={8075548},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035335463&doi=10.1109%2fATSIP.2017.8075548&partnerID=40&md5=f9c92873ac16b26563e98ecaba5f6bba},
affiliation={REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, ENIS, BP 1173, Gabes University, Higher Institute of Computer Science and Multimedia of Gabes, Sfax, 3038, Tunisia; REGIM-Lab, Sfax, 3038, Tunisia; Multimedia of Gabes, Sfax, 3038, Tunisia; CES-Laboratory Sfax Sud University, National Engineering School of Sfax, Sfax, Tunisia},
abstract={Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform. © 2017 IEEE.},
author_keywords={3D face recognition;  Curve analysis;  elastic analysis algorithm;  embedded architecture;  face detection;  Facial analysis;  Facial expressions;  Riemann geometry},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{ElSayed2017393,
author={El Sayed, A.R. and El Chakik, A. and Alabboud, H. and Yassine, A.},
title={3D face detection based on salient features extraction and skin colour detection using data mining},
journal={Imaging Science Journal},
year={2017},
volume={65},
number={7},
pages={393-408},
doi={10.1080/13682199.2017.1358528},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028655269&doi=10.1080%2f13682199.2017.1358528&partnerID=40&md5=e5e603adbb5733b9bb5cace2dfe18ffc},
affiliation={Mathematics Department, Normandie University, UNIHAVRE, LMAH, Le Havre, France; Computer Science Department, Beirut Arab University, Tripoli, Lebanon; Business and Administration Faculty, Lebanese University, Tripoli, Lebanon; Institut Supérieur d'Etudes Logistiques, Université du Havre, Le Havre, France},
abstract={Face detection has an essential role in many applications. In this paper, we propose an efficient and robust method for face detection on a 3D point cloud represented by a weighted graph. This method classifies graph vertices as skin and non-skin regions based on a data mining predictive model. Then, the saliency degree of vertices is computed to identify the possible candidate face features. Finally, the matching between non-skin regions representing eyes, mouth and eyebrows and salient regions is done by detecting collisions between polytopes, representing these two regions. This method extracts faces from situations where pose variation and change of expressions can be found. The robustness is showed through different experimental results. Moreover, we study the stability of our method according to noise. Furthermore, we show that our method deals with 2D images. © 2017 The Royal Photographic Society.},
author_keywords={3D facial point clouds detection;  3D object processing;  3D point clouds saliency;  3D point clouds segmentation;  classification;  points clouds},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sui201719575,
author={Sui, D. and Hou, D. and Duan, X.},
title={An interpolation algorithm fitted for dynamic 3D face reconstruction},
journal={Multimedia Tools and Applications},
year={2017},
volume={76},
number={19},
pages={19575-19589},
doi={10.1007/s11042-015-3233-x},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954425638&doi=10.1007%2fs11042-015-3233-x&partnerID=40&md5=78e6cc2a30fcda0f0eb7e4d7fd616c46},
affiliation={School of Information Science and Technology, Wuhan University of Technology, Hubei, 430070, China; School of Software Engineering, Anyang Normal University, Anyang, Henan  455000, China; Department of Computer, College of Science California State Polytecnic University-Pomona, California, United States},
abstract={In order to solve the problem of low recognition accuracy in later period which is caused by the too few extracted parameters in the 3D face recognition, and the incapable formation of completed point cloud structure. An automatic iterative interpolation algorithm is proposed. The new and more accurate 3D face data points are obtained by automatic iteration. This algorithm can be used to restore the data point cloud information of 3D facial feature in 2D images by means of facial three-legged structure formed by 3D face and automatic interpolation. Thus, it can realize to shape the 3D facial dynamic model which can be recognized and has high saturability. Experimental results show that the interpolation algorithm can achieve the complete the construction of facial feature based on the facial feature after 3D dynamic reconstruction, and the validity is higher. © 2016, Springer Science+Business Media New York.},
author_keywords={3D face dynamic recognition;  Iterative interpolation algorithm;  Point cloud structure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Deng20171305,
author={Deng, X. and Da, F. and Shao, H.},
title={Adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition},
journal={Signal, Image and Video Processing},
year={2017},
volume={11},
number={7},
pages={1305-1312},
doi={10.1007/s11760-017-1087-6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017095710&doi=10.1007%2fs11760-017-1087-6&partnerID=40&md5=fc06247cc3ca7870221563a085266269},
affiliation={School of Automation, Southeast University, Nanjing, Jiangsu  210096, China; Key Laboratory of Measurement and Control for Complex System, Ministry of Education, Southeast University, Nanjing, Jiangsu  210096, China},
abstract={A novel adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition is proposed in this paper. Firstly, the novel facial coarse-to-fine landmarks localization method based on Active Shape Model and Gabor wavelets transformation is proposed to exactly and automatically locate facial landmarks in range image. Secondly, the multi-scale fusion of the pyramid local binary patterns (F-PLBP) based on the irregular segmentation associated with the located landmarks is proposed to extract the discriminative feature. Thirdly, a sparse representation-based classifier based on the adaptive feature selection (A-SRC) using the distribution of the reconstruction residual is presented to select the expression-robust feature and identify the faces. Finally, the experimental evaluation based on FRGC v2.0 indicates that the adaptive feature selection method using F-PLBP combined with the A-SRC can obtain the high recognition accuracy by performing the higher discriminative power to overcome the influence from the facial expression variations. © 2017, Springer-Verlag London.},
author_keywords={3D face recognition;  Adaptive feature selection;  Facial landmark localization;  Multi-scale fusion},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tang2017211,
author={Tang, Y. and Li, H. and Sun, X. and Morvan, J.-M. and Chen, L.},
title={Principal Curvature Measures Estimation and Application to 3D Face Recognition},
journal={Journal of Mathematical Imaging and Vision},
year={2017},
volume={59},
number={2},
pages={211-233},
doi={10.1007/s10851-017-0728-2},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017163091&doi=10.1007%2fs10851-017-0728-2&partnerID=40&md5=2c39ff544a3c7e9e70105a378f3b5248},
affiliation={Université de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, Lyon, 69134, France; School of Mathematics and Statistics, Xi’an Jiaotong University, No.28, Xianning West Road, Xi’an, Shaanxi  710049, China; King Abdullah University of Science and Technologh, V.C.C. Research Center, Thuwal, 23955-6900, Saudi Arabia; Université de Lyon, CNRS, Université Claude Bernard Lyon 1, ICJ UMR 5208, Villeurbanne, 69622, France},
abstract={This paper presents an effective 3D face keypoint detection, description and matching framework based on three principle curvature measures. These measures give a unified definition of principle curvatures for both smooth and discrete surfaces. They can be reasonably computed based on the normal cycle theory and the geometric measure theory. The strong theoretical basis of these measures provides us a solid discrete estimation method on real 3D face scans represented as triangle meshes. Based on these estimated measures, the proposed method can automatically detect a set of sparse and discriminating 3D facial feature points. The local facial shape around each 3D feature point is comprehensively described by histograms of these principal curvature measures. To guarantee the pose invariance of these descriptors, three principle curvature vectors of these principle curvature measures are employed to assign the canonical directions. Similarity comparison between faces is accomplished by matching all these curvature-based local shape descriptors using the sparse representation-based reconstruction method. The proposed method was evaluated on three public databases, i.e. FRGC v2.0, Bosphorus, and Gavab. Experimental results demonstrated that the three principle curvature measures contain strong complementarity for 3D facial shape description, and their fusion can largely improve the recognition performance. Our approach achieves rank-one recognition rates of 99.6, 95.7, and 97.9% on the neutral subset, expression subset, and the whole FRGC v2.0 databases, respectively. This indicates that our method is robust to moderate facial expression variations. Moreover, it also achieves very competitive performance on the pose subset (over 98.6% except Yaw 90°) and the occlusion subset (98.4%) of the Bosphorus database. Even in the case of extreme pose variations like profiles, it also significantly outperforms the state-of-the-art approaches with a recognition rate of 57.1%. The experiments carried out on the Gavab databases further demonstrate the robustness of our method to varies head pose variations. © 2017, Springer Science+Business Media New York.},
author_keywords={3D keypoint detection, description and matching;  Expression, pose and occlusion;  Mesh-based 3D face recognition;  Principal curvature measures},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jazouli2017,
author={Jazouli, M. and Majda, A. and Zarghili, A.},
title={A $p recognizer for automatic facial emotion recognition using Kinect sensor},
journal={2017 Intelligent Systems and Computer Vision, ISCV 2017},
year={2017},
doi={10.1109/ISACV.2017.8054955},
art_number={8054955},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034640259&doi=10.1109%2fISACV.2017.8054955&partnerID=40&md5=936d92d03935e64cea971dd843897bb5},
affiliation={Laboratory of Intelligent Systems and Applications, Faculty of Science and Technology, Sidi Mohamed Benabdellah University, Fez, Morocco},
abstract={Autism is a developmental disorder involving qualitative impairments in social interaction. One source of those impairments are difficulties with facial expressions of emotion. Autistic people often have difficulty to recognize or to understand other people's emotions and feelings, or expressing their own. This work proposes a method to automatically recognize seven basic emotions among autistic children in real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and Neutral. The method uses the Microsoft Kinect sensor to track and identify points of interest from the 3D face model and it is based on the $P point-cloud recognizer to identify multi-stroke emotions as point-clouds. The experimental results show that our system can achieve above 94.28% recognition rate. Our study provides a novel clinical tool to help children with autism to assisting doctors in operating rooms. © 2017 IEEE.},
author_keywords={$P Recognizer;  ASD;  Autism;  emotion;  face expression;  Kinect},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xie2017417,
author={Xie, L. and Wang, R.},
title={Automatic indoor building reconstruction from mobile laser scanning data},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2017},
volume={42},
number={2W7},
pages={417-422},
doi={10.5194/isprs-archives-XLII-2-W7-417-2017},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030987630&doi=10.5194%2fisprs-archives-XLII-2-W7-417-2017&partnerID=40&md5=ae7f9b3c16f3693b3f672d12939d4e30},
affiliation={Department of Geomatics Engineering, University of Calgary, Calgary, AB  T2N 1N4, Canada},
abstract={Indoor reconstruction from point clouds is a hot topic in photogrammetry, computer vision and computer graphics. Reconstructing indoor scene from point clouds is challenging due to complex room floorplan and line-of-sight occlusions. Most of existing methods deal with stationary terrestrial laser scanning point clouds or RGB-D point clouds. In this paper, we propose an automatic method for reconstructing indoor 3D building models from mobile laser scanning point clouds. The method includes 2D floorplan generation, 3D building modeling, door detection and room segmentation. The main idea behind our approach is to separate wall structure into two different types as the inner wall and the outer wall based on the observation of point distribution. Then we utilize a graph cut based optimization method to solve the labeling problem and generate the 2D floorplan based on the optimization result. Subsequently, we leverage an α-shape based method to detect the doors on the 2D projected point clouds and utilize the floorplan to segment the individual room. The experiments show that this door detection method can achieve a recognition rate at 97% and the room segmentation method can attain the correct segmentation results. We also evaluate the reconstruction accuracy on the synthetic data, which indicates the accuracy of our method is comparable to the state-of-the art.},
author_keywords={Door detection;  Graph cut based optimization;  Indoor building reconstruction;  Mobile LiDAR;  Point clouds;  Room segmentation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yao20171001,
author={Yao, C. and Zhang, X. and Liu, H.},
title={Section-based tree species identification using airborne LiDAR point cloud},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2017},
volume={42},
number={2W7},
pages={1001-1007},
doi={10.5194/isprs-archives-XLII-2-W7-1001-2017},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030988804&doi=10.5194%2fisprs-archives-XLII-2-W7-1001-2017&partnerID=40&md5=1f95514ee02f264362120c9dbae444d6},
affiliation={School of Remote Sensing and Information Engineering, Wuhan University, China; State Power Economic Research Institute, China},
abstract={The application of LiDAR data in forestry initially focused on mapping forest community, particularly and primarily intended for large-scale forest management and planning. Then with the smaller footprint and higher sampling density LiDAR data available, detecting individual tree overstory, estimating crowns parameters and identifying tree species are demonstrated practicable. This paper proposes a section-based protocol of tree species identification taking palm tree as an example. Section-based method is to detect objects through certain profile among different direction, basically along X-axis or Y-axis. And this method improve the utilization of spatial information to generate accurate results. Firstly, separate the tree points from manmade-object points by decision-tree-based rules, and create Crown Height Mode (CHM) by subtracting the Digital Terrain Model (DTM) from the digital surface model (DSM). Then calculate and extract key points to locate individual trees, thus estimate specific tree parameters related to species information, such as crown height, crown radius, and cross point etc. Finally, with parameters we are able to identify certain tree species. Comparing to species information measured on ground, the portion correctly identified trees on all plots could reach up to 90.65%. The identification result in this research demonstrate the ability to distinguish palm tree using LiDAR point cloud. Furthermore, with more prior knowledge, section-based method enable the process to classify trees into different classes. © Authors 2017. CC BY 4.0 License.},
author_keywords={Crown parameter;  Decision tree rule;  LiDAR;  Section based method;  Species identification;  Tree detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gilani2017238,
author={Gilani, S.Z. and Mian, A. and Eastwood, P.},
title={Deep, dense and accurate 3D face correspondence for generating population specific deformable models},
journal={Pattern Recognition},
year={2017},
volume={69},
pages={238-250},
doi={10.1016/j.patcog.2017.04.013},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019402692&doi=10.1016%2fj.patcog.2017.04.013&partnerID=40&md5=6d8bedafce8ce555c3b5363d8c19f8bc},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, Australia; Centre for Sleep Science, School of Anatomy, Physiology and Human Biology, University of Western Australia, Australia},
abstract={We present a multilinear algorithm to automatically establish dense point-to-point correspondence over an arbitrarily large number of population specific 3D faces across identities, facial expressions and poses. The algorithm is initialized with a subset of anthropometric landmarks detected by our proposed Deep Landmark Identification Network which is trained on synthetic images. The landmarks are used to segment the 3D face into Voronoi regions by evolving geodesic level set curves. Exploiting the intrinsic features of these regions, we extract discriminative keypoints on the facial manifold to elastically match the regions across faces for establishing dense correspondence. Finally, we generate a Region based 3D Deformable Model which is fitted to unseen faces to transfer the correspondences. We evaluate our algorithm on the tasks of facial landmark detection and recognition using two benchmark datasets. Comparison with thirteen state-of-the-art techniques shows the efficacy of our algorithm. © 2017 Elsevier Ltd},
author_keywords={3D face morphing;  Deep learning;  Dense 3D face correspondence;  Face recognition;  Keypoint detection;  Landmark identification;  Shape descriptor},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liang201784,
author={Liang, Y. and Zhang, Y. and Zeng, X.-X.},
title={Pose-invariant 3D face recognition using half face},
journal={Signal Processing: Image Communication},
year={2017},
volume={57},
pages={84-90},
doi={10.1016/j.image.2017.05.004},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020037210&doi=10.1016%2fj.image.2017.05.004&partnerID=40&md5=88180b3e1c893d79c59e39647f3ee0a2},
affiliation={Guangdong University of Technology, School of Automation, No.100, Waihuan Xi Road, Guangzhou Higher Education Mega Centre, Guangzhou, 510006, China; South China Normal University, School of Software, Nanhai Information Technology Park, Foshan, 528225, China},
abstract={Pose variations are still challenging problems in 3D face recognition because large pose variations will cause self-occlusion and result in missing data. In this paper, a new method for pose-invariant 3D face recognition is proposed to handle significant pose variations. For pose estimation and registration, a coarse-to-fine strategy is proposed to detect landmarks under large yaw variations. At the coarse search step, candidate landmarks are detected using HK curvature analysis and subdivided according to a facial geometrical structure-based classification strategy. At the fine search step, candidate landmarks are identified and labeled by comparing with a Facial Landmark Model. By using the half face matching, we perform the matching step with respect to frontal scans and side scans. Experiments carried out on the Bosphorus and UND/FRGC v2.0 databases show that our method has high accuracy and robustness to pose variations. © 2017 Elsevier B.V.},
author_keywords={3D face recognition;  Facial landmark localization;  Half face;  Iso-geodesic stripes;  Pose variation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang20171141,
author={Wang, X.-Q. and Yuan, J.-Z. and Li, Q.},
title={3D face recognition using spherical vector norms map},
journal={Journal of Information Science and Engineering},
year={2017},
volume={33},
number={5},
pages={1141-1161},
doi={10.6688/JISE.2017.33.5.3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049875700&doi=10.6688%2fJISE.2017.33.5.3&partnerID=40&md5=cd82489caa82b30e6bb8b05ea1dbaae9},
affiliation={Beijing Key Laboratory of Information Service Engineering, China; Computer Technology Institute, Beijing Union University, Beijing, 100101, China; Beijing Advanced Innovation Center for Imaging Technology Capital Normal University, Beijing, 100048, China},
abstract={In this paper, we introduce a novel, automatic method for 3D face recognition. A new feature called a spherical vector norms map of a 3D face is created using the normal vector of each point. This feature contains more detailed information than the original depth image in regions such as the eyes and nose. For certain flat areas of 3D face, such as the forehead and cheeks, this map could increase the distinguishability of different points. In addition, this feature is robust to facial expression due to an adjustment that is made in the mouth region. Then, the facial representations, which are based on Histograms of Oriented Gradients, are extracted from the spherical vector norms map and the original depth image. A new partitioning strategy is proposed to produce the histogram of eight patches of a given image, in which all of the pixels are binned based on the magnitude and direction of their gradients. In this study, SVNs map and depth image are represented compactly with two histograms of oriented gradients; this approach is completed by Linear Discriminant Analysis and a Nearest Neighbor classifier. © 2017 Institute of Information Science. All Rights Reserved.},
author_keywords={3D face recognition;  Face recognition grand challenge database;  Histograms of oriented gradients;  Linear discriminant analysis;  Spherical vector norms map},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jourabloo2017187,
author={Jourabloo, A. and Liu, X.},
title={Pose-Invariant Face Alignment via CNN-Based Dense 3D Model Fitting},
journal={International Journal of Computer Vision},
year={2017},
volume={124},
number={2},
pages={187-203},
doi={10.1007/s11263-017-1012-z},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018467271&doi=10.1007%2fs11263-017-1012-z&partnerID=40&md5=46287104669ba976525673b865e6647c},
affiliation={Department of Computer Science and Engineering, Michigan State University, East Lansing, MI  48824, United States},
abstract={Pose-invariant face alignment is a very challenging problem in computer vision, which is used as a prerequisite for many facial analysis tasks, e.g., face recognition, expression recognition, and 3D face reconstruction. Recently, there have been a few attempts to tackle this problem, but still more research is needed to achieve higher accuracy. In this paper, we propose a face alignment method that aligns an image with arbitrary poses, by combining the powerful cascaded CNN regressors, 3D Morphable Model (3DMM), and mirrorability constraint. The core of our proposed method is a novel 3DMM fitting algorithm, where the camera projection matrix parameters and 3D shape parameters are estimated by a cascade of CNN-based regressors. Furthermore, we impose the mirrorability constraint during the CNN learning by employing a novel loss function inside the siamese network. The dense 3D shape enables us to design pose-invariant appearance features for effective CNN learning. Extensive experiments are conducted on the challenging large-pose face databases (AFLW and AFW), with comparison to the state of the art. © 2017, Springer Science+Business Media New York.},
author_keywords={Cascaded regressor;  CNN;  Dense model fitting;  Mirrorability constraint;  Pose-invariant face alignment},
document_type={Article},
source={Scopus},
}

@ARTICLE{Savran2017146,
author={Savran, A. and Sankur, B.},
title={Non-rigid registration based model-free 3D facial expression recognition},
journal={Computer Vision and Image Understanding},
year={2017},
volume={162},
pages={146-165},
doi={10.1016/j.cviu.2017.07.005},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028067153&doi=10.1016%2fj.cviu.2017.07.005&partnerID=40&md5=c7bd3ffda1f5172a9766e9f1a1404bdb},
affiliation={Italian Institute of Technology, Genoa 16163, Italy; Department of Electrical and Electronics Engineering, Bogazici University, Istanbul 34342, Turkey},
abstract={We propose a novel feature extraction approach for 3D facial expression recognition by incorporating non-rigid registration in face-model-free analysis, which in turn makes feasible data-driven, i.e., feature-model-free recognition of expressions. The resulting simplicity of feature representation is due to the fact that facial information is adapted to the input faces via shape model-free dense registration, and this provides a dynamic feature extraction mechanism. This approach eliminates the necessity of complex feature representations as required in the case of static feature extraction methods, where the complexity arises from the necessity to model the local context; higher degree of complexity persists in deep feature hierarchies enabled by end-to-end learning on large-scale datasets. Face-model-free recognition implies independence from limitations and biases due to committed face models, bypassing complications of model fitting, and avoiding the burden of manual model construction. We show via information gain maps that non-rigid registration enables extraction of highly informative features, as it provides invariance to local-shifts due to physiognomy (subject invariance) and residual pose misalignments; in addition, it allows estimation of local correspondences of expressions. To maximize the recognition rate, we use the strategy of employing a rich but computationally manageable set of local correspondence structures, and to this effect we propose a framework to optimally select multiple registration references. Our features are re-sampled surface curvature values at individual coordinates which are chosen per expression-class and per reference pair. We show the superior performance of our novel dynamic feature extraction approach on three distinct recognition problems, namely, action unit detection, basic expression recognition, and emotion dimension recognition. © 2017 Elsevier Inc.},
author_keywords={3D face analysis;  Action units;  Facial expression recognition;  Model-free;  Non-rigid registration;  Shift-invariance},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20171592,
author={Zhang, H. and Ye, C.},
title={An Indoor Wayfinding System Based on Geometric Features Aided Graph SLAM for the Visually Impaired},
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
year={2017},
volume={25},
number={9},
pages={1592-1604},
doi={10.1109/TNSRE.2017.2682265},
art_number={7879309},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030032091&doi=10.1109%2fTNSRE.2017.2682265&partnerID=40&md5=a1f91a2b48504c290e74d272f74f7554},
affiliation={Systems Engineering Department, University of Arkansas at Little Rock, Little Rock, AR  72204, United States},
abstract={This paper presents a 6-degree of freedom (DOF) pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired. The PE method involves two-graph simultaneous localization and mapping (SLAM) processes to reduce the accumulative pose error of the device. In the first step, the floor plane is extracted from the 3-D camera's point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch, and Z errors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y, and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a visually impaired person in an indoor environment. The system uses the estimated pose and floor plan to locate the device user in a building and guides the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task. © 2001-2011 IEEE.},
author_keywords={3-D camera;  Blind navigation;  graph SLAM;  pose estimation;  robotic navigation aid;  wayfinding},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Xiao20172060,
author={Xiao, S. and Li, J. and Chen, Y. and Wang, Z. and Feng, J. and Yan, S. and Kassim, A.},
title={3D-Assisted Coarse-to-Fine Extreme-Pose Facial Landmark Detection},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2017},
volume={2017-July},
pages={2060-2068},
doi={10.1109/CVPRW.2017.257},
art_number={8014991},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030267233&doi=10.1109%2fCVPRW.2017.257&partnerID=40&md5=9bb47a067eb0e3766f737705df70f63d},
affiliation={National University of Singapore, Singapore; Qihoo, Singapore; 60 3Olin College, Singapore},
abstract={We propose a novel 3D-assisted coarse-to-fine extreme-pose facial landmark detection system in this work. For a given face image, our system first refines the face bounding box with landmark locations inferred from a 3D face model generated by a Recurrent 3D Regressor at coarse level. Another R3R is then employed to fit a 3D face model onto the 2D face image cropped with the refined bounding box at fine-scale. 2D landmark locations inferred from the fitted 3D face are further adjusted with the popular 2D regression method, i.e. LBF. The 3D-assisted coarse-to-fine strategy and the 2D adjustment process explicitly ensure both the robustness to extreme face poses and bounding box disturbance and the accuracy towards pixel-level landmark displacement. Extensive experiments on the Menpo Challenge test sets demonstrate the superior performance of our system. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xia201790,
author={Xia, B. and Ben Amor, B. and Daoudi, M.},
title={Joint gender, ethnicity and age estimation from 3D faces: An experimental illustration of their correlations},
journal={Image and Vision Computing},
year={2017},
volume={64},
pages={90-102},
doi={10.1016/j.imavis.2017.06.004},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024852319&doi=10.1016%2fj.imavis.2017.06.004&partnerID=40&md5=c2b90bc882b7116bde3f86abf9cc89c9},
affiliation={IMT Lille Douai, Univ. Lille, CNRS, UMR 9189 - CRIStAL-Centre de Recherche en Informatique Signal et Automatique de Lille, Lille, F-59000, France},
abstract={Humans present clear demographic traits which allow their peers to recognize their gender and ethnic groups as well as estimate their age. Abundant literature has investigated the problem of automated gender, ethnicity and age recognition from facial images. However, despite the co-existence of these traits, most of the studies have addressed them separately, very little attention has been given to their correlations. In this work, we address the problem of joint demographic estimation and investigate the correlation through the morphological differences in 3D facial shapes. To this end, a set of facial features are extracted to capture the 3D shape differences among the demographic groups. Then, a correlation-based feature selection is applied to highlight salient features and remove redundancy. These features are later fed to Random Forest for gender and ethnicity classification, and age estimation. Extensive experiments conducted on FRGCv2 dataset, under Expression-Dependent and Expression-Independent settings, demonstrate the effectiveness of the proposed approaches for the three traits, and also show the accuracy improvement when considering their correlations. To the best of our knowledge, this is the first study exploring the correlations of these facial soft-biometric traits using 3D faces. This is also the first work which studies the problem of age estimation from 3D Faces.1 © 2017 Elsevier B.V.},
author_keywords={3D face;  Age;  Correlation;  Ethnicity;  Feature selection;  Gender;  Random Forest},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jo20171,
author={Jo, J. and Kim, H. and Kim, J.},
title={3D facial shape reconstruction using macro- and micro-level features from high resolution facial images},
journal={Image and Vision Computing},
year={2017},
volume={64},
pages={1-9},
doi={10.1016/j.imavis.2017.05.001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020184603&doi=10.1016%2fj.imavis.2017.05.001&partnerID=40&md5=f19a6c225d27495789fd071225962c44},
affiliation={School of Electrical and Electronic Engineering, Yonsei University, Seoul, 120-749, South Korea},
abstract={Three-dimensional (3D) facial modeling and stereo matching-based methods are widely used for 3D facial reconstruction from 2D single-view and multiple-view images. However, these methods cannot realistically reconstruct 3D faces because they use insufficient numbers of macro-level Facial Feature Points (FFPs). This paper proposes an accurate and person-specific 3D facial reconstruction method that uses ample numbers of macro- and micro-level FFPs to enable coverage of all facial regions of high resolution facial images. Comparisons of 3D facial images reconstructed using the proposed method for ground-truth 3D facial images from the Bosphorus 3D database show that the method is superior to a conventional Active Appearance Model-Structure from Motion (AAM + SfM)-based method in terms of average 3D root mean square error between the reconstructed and ground-truth 3D faces. Further, the proposed method achieved outstanding accuracy in local facial regions such as the cheek—areas where extraction of FFPs is difficult for existing methods. © 2017 Elsevier B.V.},
author_keywords={3D facial reconstruction;  Active appearance model;  Macro-level feature;  Micro-level feature;  Stereo matching;  Structure from motion},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Angonese2017779,
author={Angonese, A.T. and Ferreira Rosa, P.F.},
title={Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform},
journal={ICMT 2017 - 6th International Conference on Military Technologies},
year={2017},
pages={779-786},
doi={10.1109/MILTECHS.2017.7988861},
art_number={7988861},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029368093&doi=10.1109%2fMILTECHS.2017.7988861&partnerID=40&md5=19636d79c444d78b4f59444aa7b09760},
affiliation={Instituto Militar de Engenharia, Rio de Janeiro, Brazil},
abstract={This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment. © 2017 IEEE.},
author_keywords={CNN;  Deep Learning;  HOG;  People Detection;  robot operating system ROS;  Simultaneous Localization and Mapping (SLAM)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dube20175266,
author={Dube, R. and Dugas, D. and Stumm, E. and Nieto, J. and Siegwart, R. and Cadena, C.},
title={SegMatch: Segment based place recognition in 3D point clouds},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
year={2017},
pages={5266-5272},
doi={10.1109/ICRA.2017.7989618},
art_number={7989618},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027968710&doi=10.1109%2fICRA.2017.7989618&partnerID=40&md5=c70ca5cddbbac1d6d8faa518ba50d188},
affiliation={Autonomous Systems Lab, ETH, Zurich, Switzerland},
abstract={Place recognition in 3D data is a challenging task that has been commonly approached by adapting image-based solutions. Methods based on local features suffer from ambiguity and from robustness to environment changes while methods based on global features are viewpoint dependent. We propose SegMatch, a reliable place recognition algorithm based on the matching of 3D segments. Segments provide a good compromise between local and global descriptions, incorporating their strengths while reducing their individual drawbacks. SegMatch does not rely on assumptions of 'perfect segmentation', or on the existence of 'objects' in the environment, which allows for reliable execution on large scale, unstructured environments. We quantitatively demonstrate that SegMatch can achieve accurate localization at a frequency of 1Hz on the largest sequence of the KITTI odometry dataset. We furthermore show how this algorithm can reliably detect and close loops in real-time, during online operation. In addition, the source code for the SegMatch algorithm is made publicly available1. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Grashof2017298,
author={Grashof, S. and Ackermann, H. and Kuhnke, F. and Ostermann, J. and Brandt, S.S.},
title={Projective structure from facial motion},
journal={Proceedings of the 15th IAPR International Conference on Machine Vision Applications, MVA 2017},
year={2017},
pages={298-301},
doi={10.23919/MVA.2017.7986860},
art_number={7986860},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027883345&doi=10.23919%2fMVA.2017.7986860&partnerID=40&md5=33cd033ecb39dcbe075d920e3ce3715e},
affiliation={Leibniz Universität, Hannover, Germany; Univ. Copenhagen, Denmark},
abstract={Nonrigid Structure-From-Motion is a well-known approach to estimate time-varying 3D structures from 2D input image sequences. For challenging problems such as the reconstruction of human faces, state-of-the-art approaches estimate statistical shape spaces from training data. It is common practice to use orthographic or weak-perspective camera models to map 3D to 2D points. We propose to use a projective camera model combined with a multilinear tensor-based face model, enabling approximation of a dense 3D face surface by sparse 2D landmarks. Using a projective camera is beneficial, as it is able to handle perspective projections and particular camera motions which are critical for affine models. We show how the nonlinearity of the projective model can be linearized so that its parameters can be estimated by an alternating-least-squares approach. This enables simple and fast estimation of the model parameters. The effectiveness of the proposed algorithm is demonstrated using challenging real image data. © 2017 MVA Organization All Rights Reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Luo2017680,
author={Luo, M. and Luo, Y. and Li, H. and Xia, Z. and Yongkui, Y.},
title={Design and implementation of high resolution face image acquisition system under low illumination based on the open source computer vision library},
journal={2017 2nd International Conference on Image, Vision and Computing, ICIVC 2017},
year={2017},
pages={680-683},
doi={10.1109/ICIVC.2017.7984642},
art_number={7984642},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029382532&doi=10.1109%2fICIVC.2017.7984642&partnerID=40&md5=281b93aacd5e9ef04e857caeeaab4969},
affiliation={Department of Information Engineering, Sichuan Water Conservancy Vocational College, Chongzhou, China; Department of Information Technology Chengdu, Agricultural College, China; Chengdu Research Institute of Huawei Technology Co. Ltd., China},
abstract={Super-resolution face image acquisition system is indispensable in people's life. Under the condition of low illumination, the illumination environment difference is too big or the light is insufficient, which leads to the traditional image acquisition system can not collect the high quality face image, and the limitation is poor. Based on open source computer vision library (OpenCV) in the C++ environment configuration, the use of Three Dimension (3D) face recognition technology algorithm, design a set of low illumination conditions of the super resolution face image acquisition system. Experiments show that the design scheme with real-time focusing speed), fast (single acquisition 0.05 seconds), accurate (facial recognition rate of 99.3%) etc. characteristics, be able to fully meet the needs of low illumination conditions for super-resolution of face image acquisition. © 2017 IEEE.},
author_keywords={C++ environment;  Facial recognition technology 3D algorithm;  OpenCV},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Boukamcha2017340,
author={Boukamcha, H. and Hallek, M. and Smach, F. and Atri, M.},
title={Automatic landmark detection and 3D Face data extraction},
journal={Journal of Computational Science},
year={2017},
volume={21},
pages={340-348},
doi={10.1016/j.jocs.2016.11.015},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008197601&doi=10.1016%2fj.jocs.2016.11.015&partnerID=40&md5=86d160f58c6d2bf09b984c5fc2a1d12b},
affiliation={University of Monastir, Faculty of Science of Monastir, Department Physique, Laboratory of Electronics and Microelectronics, Environment Street, Monastir, 5019, Tunisia; University of Burgundy, Esplanade Erasme, Dijon, 21078, France},
abstract={This paper contributes to 3D facial synthesis by presenting a novel method for parameterization using Landmark Point detection. The approach presented aims at improving facial recognition even in varying facial expressions, and missing data in 3D facial models. As such, the prime objective was to develop an automatically embedded process that can detect any frontal face in 3D face recognition systems, with face segmentation and surface curvature information. Using the hybrid interpolation method, experiments on facial landmarks were performed on 4950 images from Face Recognition Grand Challenge database (FRGC). Distinctive facial landmarks from the nose–tips, Limits mouth and two eye corners formed the statistical inputs for Iterative Closest Point (ICP) in the Point Distribution Model (PDM). Performance or landmark localization is reported by using percentage deviation from the mean 3D profile. Localization results and estimated data on landmark locations demonstrate that the method confirms its effectiveness for proposed application. © 2016 Elsevier B.V.},
author_keywords={3D Face segmentation;  Automatic landmark;  Face recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Henriquez20171467,
author={Henriquez, P. and Matuszewski, B.J. and Andreu, Y. and Bastiani, L. and Colantonio, S. and Coppini, G. and D'Acunto, M. and Favilla, R. and Germanese, D. and Giorgi, D. and Marraccini, P. and Martinelli, M. and Morales, M.-A. and Pascali, M.A. and Righi, M. and Salvetti, O. and Larsson, M. and Stromberg, T. and Randeberg, L.M. and Bjorgan, A. and Giannakakis, G. and Pediaditis, M. and Chiarugi, F. and Christinaki, E. and Marias, K. and Tsiknakis, M.},
title={Mirror Mirror on the Wall... An Unobtrusive Intelligent Multisensory Mirror for Well-Being Status Self-Assessment and Visualization},
journal={IEEE Transactions on Multimedia},
year={2017},
volume={19},
number={7},
pages={1467-1481},
doi={10.1109/TMM.2017.2666545},
art_number={7847427},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021371065&doi=10.1109%2fTMM.2017.2666545&partnerID=40&md5=b185698f8158cc13cf56eff767b94bc2},
affiliation={University of Central Lancashire, Preston, PR1 2HE, United Kingdom; CNR IFC-Institute of Clinical Physiology, Pisa, 56124, Italy; CNR ISTI-Institute of Information Science and Technologies, Pisa, 56124, Italy; CNR ISM-Institute of the Structure of Matter, Roma, 00185, Italy; Linkoping University, Linkoping, 58183, Sweden; Norwegian University of Science and Technology, Trondheim, 7491, Norway; Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, 700 13, Greece; Department of Informatics Engineering, Technological Educational Institute of Crete, Heraklion, 714 10, Greece},
abstract={A person's well-being status is reflected by their face through a combination of facial expressions and physical signs. The SEMEOTICONS project translates the semeiotic code of the human face into measurements and computational descriptors that are automatically extracted from images, videos, and three-dimensional scans of the face. SEMEOTICONS developed a multisensory platform in the form of a smart mirror to identify signs related to cardio-metabolic risk. The aim was to enable users to self-monitor their well-being status over time and guide them to improve their lifestyle. Significant scientific and technological challenges have been addressed to build the multisensory mirror, from touchless data acquisition, to real-time processing and integration of multimodal data. © 1999-2012 IEEE.},
author_keywords={3D face detection and tracking;  3D morphometric analysis;  breath analysis;  Cardio-metabolic risk;  multimodal data integration;  multispectral imaging;  psychosomatic status recognition;  unobtrusive well-being monitoring},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jiang2017245,
author={Jiang, R. and Ho, A.T.S. and Cheheb, I. and Al-Maadeed, N. and Al-Maadeed, S. and Bouridane, A.},
title={Emotion recognition from scrambled facial images via many graph embedding},
journal={Pattern Recognition},
year={2017},
volume={67},
pages={245-251},
doi={10.1016/j.patcog.2017.02.003},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016085782&doi=10.1016%2fj.patcog.2017.02.003&partnerID=40&md5=b94f3283aeb5dda309b578cc9792a2ae},
affiliation={Computer and Information Sciences, Northumbria University, Newcastle upon Tyne, United Kingdom; Department of Computer Science, Surrey University, Tianjin University of Science & Tech., China and Wuhan University of Technology, Wuhan, China; Department of Computer Science and Engineering, Qatar University, Doha, 110353, Qatar},
abstract={Facial expression verification has been extensively exploited due to its wide application in affective computing, robotic vision, man-machine interaction and medical diagnosis. With the recent development of Internet-of-Things (IoT), there is a need of mobile-targeted facial expression verification, where face scrambling has been proposed for privacy protection during image/video distribution over public network. Consequently, facial expression verification needs to be carried out in a scrambled domain, bringing out new challenges in facial expression recognition. An immediate impact from face scrambling is that conventional semantic facial components become not identifiable, and 3D face models cannot be clearly fitted to a scrambled image. Hence, the classical facial action coding system cannot be applied to facial expression recognition in the scrambled domain. To cope with chaotic signals from face scrambling, this paper proposes an new approach – Many Graph Embedding (MGE) to discover discriminative patterns from the subspaces of chaotic patterns, where the facial expression recognition is carried out as a fuzzy combination from many graph embedding. In our experiments, the proposed MGE was evaluated on three scrambled facial expression datasets: JAFFE, MUG and CK++. The benchmark results demonstrated that the proposed method is able to improve the recognition accuracy, making our method a promising candidate for the scrambled facial expression recognition in the emerging privacy-protected IoT applications. © 2017 Elsevier Ltd},
author_keywords={Chaotic patterns;  Emotion recognition;  Facial expression;  Many graph embedding;  User privacy},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tang2017589,
author={Tang, Y. and Chen, L.},
title={3D Facial Geometric Attributes Based Anti-Spoofing Approach against Mask Attacks},
journal={Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017},
year={2017},
pages={589-595},
doi={10.1109/FG.2017.74},
art_number={7961795},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026323809&doi=10.1109%2fFG.2017.74&partnerID=40&md5=ed89e52bd051c85dbee8198cd1d7344e},
affiliation={Universite de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, Lyon, 69134, France},
abstract={3D scanning and 3D printing techniques, as the technical impetus of 3D face recognition, also boost unconsciously the security threat against it from the spoofing attacks via manufactured mask. In order to improve the robustness of 3D face recognition system, several countermeasures against mask attacks based on photometric features have been reported in recent years. However, the anti-spoofing approach involving 3D meshed face scan and the related 3D facial features have not been studied yet. For filling this gap, in this paper, we propose to exploit the anti-spoofing performance of geometric attributes based 3D facial description. It synthesises the advantages of the selected geometric attributes, named principal curvature measures, and the meshSIFT-based feature descriptor. Specifically, the estimation of geometric attributes is coherent to the property of discrete surface, and the feature related to them can accurately describe the shape of facial surface. These characteristics are beneficial to discovering the geometry-based dissimilarity between genuine face and fraud mask. In the experiment part, the baselines of verification and anti-spoofing performance are evaluated on the Morpho database. Furthermore, for simulating a real-world scenario and testing the corresponding anti-spoofing performance, the size of genuine face set is massively extended by uniting the Morpho database and the FRGC v2.0 database to increase the ratio of genuine faces to fraud masks. The evaluation results prove that the proposed 3D face verification system can guarantee competitive verification accuracy for genuine faces and promising anti-spoofing performance against mask attacks. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2017251,
author={Wang, L. and Yu, X. and Metaxas, D.N.},
title={A Coupled Encoder-Decoder Network for Joint Face Detection and Landmark Localization},
journal={Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017},
year={2017},
pages={251-257},
doi={10.1109/FG.2017.40},
art_number={7961749},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026314118&doi=10.1109%2fFG.2017.40&partnerID=40&md5=50f2346d9f51efe033e75c3ced0d6cb0},
affiliation={Rutgers, the State University of New Jersey, United States; NEC Laboratories America, Media Analytics, United States},
abstract={Face detection and landmark localization have been extensively investigated and are the prerequisite for many face applications, such as face recognition and 3D face reconstruction. Most existing methods achieve success on only one of the two problems. In this paper, we propose a coupled encoderdecoder network to jointly detect faces and localize facial key points. The encoder and decoder generate response maps for facial landmark localization. Moreover, we observe that the intermediate feature maps from the encoder and decoder have strong power in describing facial regions, which motivates us to build a unified framework by coupling the feature maps for multi-scale cascaded face detection. Experiments on face detection show strongly competitive results against the existing methods on two public benchmarks. The landmark localization further shows consistently better accuracy than state-of-the-arts on three face-in-the-wild databases. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hosoi2017573,
author={Hosoi, T.},
title={Head Pose and Expression Transfer Using Facial Status Score},
journal={Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017},
year={2017},
pages={573-580},
doi={10.1109/FG.2017.142},
art_number={7961793},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026320589&doi=10.1109%2fFG.2017.142&partnerID=40&md5=cd4d60c3f87708f266cf6fbf10767ef1},
affiliation={Kawasaki City Kanagawa, Japan},
abstract={We propose a method to transfer both head poseand facial expression of a source person in a video to the faceof a target person in an output video. Our method models theentire 2D frame instead of the 3D face, and it generates outputresults using a status score, which includes the relative facialstatus about the head pose and expression in a frame. From thetarget video, the learning process obtains frame features neededfor moving to each frame from the neutral frame for all frames,and generates the basis of these features via principal componentanalysis (PCA). Then, it learns to generate these features from agiven status score sequentially. In the transfer process, it obtainsa status score from a source frame of the video and generatesthe features from the given status score. Then, it generates theoutput frame using the reconstructed features. An output videois generated by repeating these steps for each source frame.Our method generates output results on the trajectory of thetarget video by using the advantage of PCA. Therefore, in theoutput results generated by our methods, both head pose andexpression are transferred correctly while the non-face regionsof the frames are supported. Finally, we experimentally comparethe effectiveness of our method and conventional methods. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Marcolin201713805,
author={Marcolin, F. and Vezzetti, E.},
title={Novel descriptors for geometrical 3D face analysis},
journal={Multimedia Tools and Applications},
year={2017},
volume={76},
number={12},
pages={13805-13834},
doi={10.1007/s11042-016-3741-3},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979205092&doi=10.1007%2fs11042-016-3741-3&partnerID=40&md5=39e34848e0b812bf5b89e9bae88594e4},
affiliation={Department of Management and Production Engineering, Politecnico di Torino, Corso Duca degli Abruzzi, 24, Torino, 10129, Italy},
abstract={3D face was recently investigated for various applications, including biometrics and diagnosis. Describing facial surface, i.e. how it bends and which kinds of patches is composed by, is the aim of studies of Face Analysis, whose ultimate goal is to identify which features could be extracted from three-dimensional faces depending on the application. In this study, we propose 105 novel geometrical descriptors for Face Analysis. They are generated by composing primary geometrical descriptors such as mean, Gaussian, principal curvatures, shape index, curvedness, and the coefficients of the fundamental forms, and by applying standard functions such as sine, cosine, and logarithm to them. The new descriptors were mapped on 217 facial depth maps and analysed in terms of descriptiveness of facial shape and exploitability for localizing landmark points. Automatic landmark extraction stands as the final aim of this analysis. Results showed that some newly generated descriptors were sounder than the primary ones, meaning that their local behaviours in correspondence to a landmark position is thoroughly specific and can be registered with high similarity on every face of our dataset. © 2016, Springer Science+Business Media New York.},
author_keywords={3D face;  Face analysis;  Face expression recognition;  Face recognition;  Geometry;  Landmarks},
document_type={Article},
source={Scopus},
}

@ARTICLE{Echeagaray-Patrón2017648,
author={Echeagaray-Patrón, B.A. and Kober, V.I. and Karnaukhov, V.N. and Kuznetsov, V.V.},
title={A method of face recognition using 3D facial surfaces},
journal={Journal of Communications Technology and Electronics},
year={2017},
volume={62},
number={6},
pages={648-652},
doi={10.1134/S1064226917060067},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024129138&doi=10.1134%2fS1064226917060067&partnerID=40&md5=d4b2a551be78c542e1d7b64631adf914},
affiliation={Department of Computer Sciences, Research and Higher Education (CICESE), Ensenada, B.C.  22860, Mexico; Kharkevich Institute for Information Transmission Problems, Russian Academy of Sciences, Moscow, 127051, Russian Federation; Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation},
abstract={Face recognition is one of the most rapidly developing areas of image processing and computer vision. In this work, a new method for face recognition and identification using 3D facial surfaces is proposed. The method is invariant to facial expression and pose variations in the scene. The method uses 3D shape data without color or texture information. The method is based on conformal mapping of original facial surfaces onto a Riemannian manifold, followed by comparison of conformal and isometric invariants computed in this manifold. Computer results are presented using known 3D face databases that contain significant amount of expression and pose variations. © 2017, Pleiades Publishing, Inc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ding2017144,
author={Ding, C. and Tao, D.},
title={Pose-invariant face recognition with homography-based normalization},
journal={Pattern Recognition},
year={2017},
volume={66},
pages={144-152},
doi={10.1016/j.patcog.2016.11.024},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028267353&doi=10.1016%2fj.patcog.2016.11.024&partnerID=40&md5=72547b1656ae570748a3e9fd3cff6eb3},
affiliation={Centre for Artificial Intelligence, FEIT, University of Technology Sydney, Ultimo, NSW  2007, Australia},
abstract={Pose-invariant face recognition (PIFR) refers to the ability that recognizes face images with arbitrary pose variations. Among existing PIFR algorithms, pose normalization has been proved to be an effective approach which preserves texture fidelity, but usually depends on precise 3D face models or at high computational cost. In this paper, we propose an highly efficient PIFR algorithm that effectively handles the main challenges caused by pose variation. First, a dense grid of 3D facial landmarks are projected to each 2D face image, which enables feature extraction in an pose adaptive manner. Second, for the local patch around each landmark, an optimal warp is estimated based on homography to correct texture deformation caused by pose variations. The reconstructed frontal-view patches are then utilized for face recognition with traditional face descriptors. The homography-based normalization is highly efficient and the synthesized frontal face images are of high quality. Finally, we propose an effective approach for occlusion detection, which enables face recognition with visible patches only. Therefore, the proposed algorithm effectively handles the main challenges in PIFR. Experimental results on four popular face databases demonstrate that the propose approach performs well on both constrained and unconstrained environments. © 2016 Elsevier Ltd},
author_keywords={Face synthesis;  Homography;  Pose normalization;  Pose-invariant face recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Schönborn2017160,
author={Schönborn, S. and Egger, B. and Morel-Forster, A. and Vetter, T.},
title={Markov Chain Monte Carlo for Automated Face Image Analysis},
journal={International Journal of Computer Vision},
year={2017},
volume={123},
number={2},
pages={160-183},
doi={10.1007/s11263-016-0967-5},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028240219&doi=10.1007%2fs11263-016-0967-5&partnerID=40&md5=77d362cf06ebb4646cd34c71dcf3a2e2},
affiliation={Department of Mathematics and Computer Science, University of Basel, Spiegelgasse 1, Basel, 4051, Switzerland},
abstract={We present a novel fully probabilistic method to interpret a single face image with the 3D Morphable Model. The new method is based on Bayesian inference and makes use of unreliable image-based information. Rather than searching a single optimal solution, we infer the posterior distribution of the model parameters given the target image. The method is a stochastic sampling algorithm with a propose-and-verify architecture based on the Metropolis–Hastings algorithm. The stochastic method can robustly integrate unreliable information and therefore does not rely on feed-forward initialization. The integrative concept is based on two ideas, a separation of proposal moves and their verification with the model (Data-Driven Markov Chain Monte Carlo), and filtering with the Metropolis acceptance rule. It does not need gradients and is less prone to local optima than standard fitters. We also introduce a new collective likelihood which models the average difference between the model and the target image rather than individual pixel differences. The average value shows a natural tendency towards a normal distribution, even when the individual pixel-wise difference is not Gaussian. We employ the new fitting method to calculate posterior models of 3D face reconstructions from single real-world images. A direct application of the algorithm with the 3D Morphable Model leads us to a fully automatic face recognition system with competitive performance on the Multi-PIE database without any database adaptation. © 2016, Springer Science+Business Media New York.},
author_keywords={Face image analysis;  Generative models;  Markov chain Monte Carlo;  Model fitting;  Morphable Model;  Top-down and bottom-up integration},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fenglan2017704,
author={Fenglan, H. and Sun, T. and Bu, F.},
title={Generation of person-specific 3D model based on single photograph},
journal={2016 2nd IEEE International Conference on Computer and Communications, ICCC 2016 - Proceedings},
year={2017},
pages={704-707},
doi={10.1109/CompComm.2016.7924793},
art_number={7924793},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020206654&doi=10.1109%2fCompComm.2016.7924793&partnerID=40&md5=f005f39ae06e9a02f8426026f3c1e891},
affiliation={People's Public Security University of China, Beijing, China},
abstract={At present, a person-specific 3D model generation technology has been an active research topic for scientists in the field of computer vision and computer graphics. The paper proposes a 3D modeling method based on single photograph, by analysis of individual difference like geometric features which present the shape and locations of facial components. An extension of the basic Active Appearance Model is proposed to localize some facial feature points in the frontal image and all these feature points are aligned to fit the generic 3D face model to a specialized one to reflect the given person's face, being both more robust and faster. Then an optimized texture mapping of cylindrical projection and model adjustment technologies are presented, to synthesize personal virtual face realistically. This method rectifies these previous defects, such as the difficulty in gaining access to acquiring the accurate data of a 3D face model, slowly manual location methods, incomplete facial information mining. The approach gives a significant improvement in both the reliability and the overall accuracy of 3D modeling. © 2016 IEEE.},
author_keywords={Active appearance model;  Generic face model;  Model adjustment;  Texture mapping},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Emambakhsh2017995,
author={Emambakhsh, M. and Evans, A.},
title={Nasal Patches and Curves for Expression-Robust 3D Face Recognition},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2017},
volume={39},
number={5},
pages={995-1007},
doi={10.1109/TPAMI.2016.2565473},
art_number={7467565},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018498024&doi=10.1109%2fTPAMI.2016.2565473&partnerID=40&md5=e1afb77517c631cf7da8f8d394318b02},
affiliation={Institute of Sensors, Signals and Systems, Heriot-Watt University, Edinburgh, United Kingdom; Department of Electronic and Electrical Engineering, University of Bath, Bath, United Kingdom},
abstract={The potential of the nasal region for expression robust 3D face recognition is thoroughly investigated by a novel five-step algorithm. First, the nose tip location is coarsely detected and the face is segmented, aligned and the nasal region cropped. Then, a very accurate and consistent nasal landmarking algorithm detects seven keypoints on the nasal region. In the third step, a feature extraction algorithm based on the surface normals of Gabor-wavelet filtered depth maps is utilised and, then, a set of spherical patches and curves are localised over the nasal region to provide the feature descriptors. The last step applies a genetic algorithm-based feature selector to detect the most stable patches and curves over different facial expressions. The algorithm provides the highest reported nasal region-based recognition ranks on the FRGC, Bosphorus and BU-3DFE datasets. The results are comparable with, and in many cases better than, many state-of-the-art 3D face recognition algorithms, which use the whole facial domain. The proposed method does not rely on sophisticated alignment or denoising steps, is very robust when only one sample per subject is used in the gallery, and does not require a training step for the landmarking algorithm. © 2016 IEEE.},
author_keywords={Face recognition;  facial landmarking;  feature selection;  Gabor wavelets;  nose region;  surface normals},
document_type={Article},
source={Scopus},
}

@ARTICLE{Giorgi2017549,
author={Giorgi, D. and Pascali, M.A. and Henriquez, P. and Matuszewski, B.J. and Colantonio, S. and Salvetti, O.},
title={Persistent homology to analyse 3D faces and assess body weight gain},
journal={Visual Computer},
year={2017},
volume={33},
number={5},
pages={549-563},
doi={10.1007/s00371-016-1344-7},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007248182&doi=10.1007%2fs00371-016-1344-7&partnerID=40&md5=59ad397deb50f8a2aac90bb60cd3a77b},
affiliation={Institute of Information Science and Technologies, National Research Council of Italy, Via G. Moruzzi 1, Pisa, 56124, Italy; Computer Vision and Machine Learning Research Group, School of Engineering, University of Central Lancashire, Preston, PR1 2HE, United Kingdom},
abstract={In this paper, we analyse patterns in face shape variation due to weight gain. We propose the use of persistent homology descriptors to get geometric and topological information about the configuration of anthropometric 3D face landmarks. In this way, evaluating face changes boils down to comparing the descriptors computed on 3D face scans taken at different times. By applying dimensionality reduction techniques to the dissimilarity matrix of descriptors, we get a space in which each face is a point and face shape variations are encoded as trajectories in that space. Our results show that persistent homology is able to identify features which are well related to overweight and may help assessing individual weight trends. The research was carried out in the context of the European project SEMEOTICONS, which developed a multisensory platform which detects and monitors over time facial signs of cardio-metabolic risk. © 2016, Springer-Verlag Berlin Heidelberg.},
author_keywords={Feature measurement;  Feature representation, size and shape;  Image processing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hu201746,
author={Hu, X. and Peng, S. and Wang, L. and Yang, Z. and Li, Z.},
title={Surveillance video face recognition with single sample per person based on 3D modeling and blurring},
journal={Neurocomputing},
year={2017},
volume={235},
pages={46-58},
doi={10.1016/j.neucom.2016.12.059},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009505545&doi=10.1016%2fj.neucom.2016.12.059&partnerID=40&md5=7065ccff1f63bfc77d9c350e74ef64a0},
affiliation={School of Mechanical and Electrical Engineering, Guangzhou University, Guangzhou, 510006, China},
abstract={Video surveillance has attracted more and more interests in the last decade, video-based Face Recognition (FR) therefore became an important task. However, the surveillance videos include many vague non-frontal faces especially the view of faces looking down and up. As a result, most FR algorithms would perform worse when they were applied in surveillance videos. On the other hand, it was common at video monitoring field that only Single training Sample Per Person (SSPP) is available from their identification card. In order to effectively improve FR for both the SSPP problem and the low-quality problem, this paper proposed an approach to synthesis face images-based on 3D face modeling and blurring. In the proposed algorithm, firstly a 2D frontal face with high-resolution was used to build a 3D face model, then several virtual faces with different poses were synthesized from the 3D model, and finally some degraded face images were constructed from the original and the virtual faces through blurring process. At last multiple face images could be chosen from frontal, virtual and degraded faces to build a training set. Both SCface and LFW databases were employed to evaluate the proposed algorithm by using PCA, FLDA, scale invariant feature transform, compressive sensing and deep learning. The results on both datasets showed that the performance of these methods could be improved when virtual faces were generated to train the classifiers. Furthermore, in SCface database the average recognition rates increased up to 10%, 16.62%, 13.03%, 19.44% and 23.28% respectively for the above-mentioned methods when virtual view and blurred faces were taken to train their classifiers. Experimental results indicated that the proposed method for generating more train samples was effective and could be considered to be applied in intelligent video monitoring system. © 2017 Elsevier B.V.},
author_keywords={Compressive sensing;  Deep learning;  Scale invariant feature transform;  Single training sample per person;  Video surveillance},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kim20172362,
author={Kim, D. and Choi, J. and Leksut, J.T. and Medioni, G.},
title={Expression invariant 3D face modeling from an RGB-D video},
journal={Proceedings - International Conference on Pattern Recognition},
year={2017},
pages={2362-2367},
doi={10.1109/ICPR.2016.7899989},
art_number={7899989},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019102884&doi=10.1109%2fICPR.2016.7899989&partnerID=40&md5=cf697dab534c995d033201bd62a61657},
affiliation={Institute for Robotics and Intelligent Systems, University of Southern California, 3737 Watt way PHE 101, Los Angeles, 90089, United States},
abstract={We aim to reconstruct an accurate neutral 3D face model from an RGB-D video in the presence of extreme expression changes. Since each depth frame, taken by a low-cost sensor, is noisy, point clouds from multiple frames can be registered and aggregated to build an accurate 3D model. However, direct aggregation of multiple data produces erroneous results in natural interaction (e.g., talking and showing expressions). We propose to analyze facial expression from an RGB frame and neutralize the corresponding 3D point cloud if needed. We first estimate the person's expression by fitting blendshape coefficients using 2D facial landmarks for each frame and calculate an expression deformity (expression score). With the estimated expression score, we determine whether an input face is neutral or non-neutral. If the face is non-neutral, we proceed to neutralize the expression of the 3D point cloud in that frame. To neutralize the 3D point cloud of a face, we deform our generic 3D face model by applying the estimated blendshape coefficients, find displacement vectors from the deformed generic face to a neutral generic face, and apply the displacement vectors to the input 3D point cloud. After preprocessing frames in a video, we rank frames based on the expression scores and register the ranked frames into a single 3D model. Our system produces a neutral 3D face model in the presence of extreme expression changes even when neutral faces do not exist in the video. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pham20171851,
author={Pham, H.X. and Pavlovic, V. and Cai, J. and Cham, T.-J.},
title={Robust real-time performance-driven 3D face tracking},
journal={Proceedings - International Conference on Pattern Recognition},
year={2017},
pages={1851-1856},
doi={10.1109/ICPR.2016.7899906},
art_number={7899906},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019106844&doi=10.1109%2fICPR.2016.7899906&partnerID=40&md5=3e60c1814d147400cdda963f8f555dd2},
affiliation={Department of Computer Science, Rutgers University, United States; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore},
abstract={We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker capable of handling a wide range of target scene depths, whose performances are demonstrated in our extensive experiments better than those of the state-of-the-arts. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhen20172252,
author={Zhen, Q. and Huang, D. and Wang, Y. and Drira, H. and Amor, B.B. and Daoudi, M.},
title={Magnifying subtle facial motions for 4D Expression Recognition},
journal={Proceedings - International Conference on Pattern Recognition},
year={2017},
pages={2252-2257},
doi={10.1109/ICPR.2016.7899971},
art_number={7899971},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019103923&doi=10.1109%2fICPR.2016.7899971&partnerID=40&md5=692231105b2782590c4b2fa195758038},
affiliation={IRIP Lab, School of Computer Science and Engineering, Beihang University, China; Institut Mines-Télécom/Télécom Lille, CRIStAL (UMR CNRS 9189), France},
abstract={In this paper, we propose an effective approach for automatic 4D Facial Expression Recognition (FER). The flow of 3D facial scans is first modeled to capture spatial deformations based on the recently-developed Riemannian approach, namely Dense Scalar Fields (DSF), where registration and comparison of neighboring 3D face frames are jointly led. The deformations are then fed into a temporal filtering based magnification step to amplify the slight facial actions over time. The proposed method allows revealing subtle (hidden) deformations which enhances the performance in classification. We evaluate our approach on the BU-4DFE dataset, and the state-of-art accuracy up to 94.18% is achieved, which is superior to the top one so far reported, clearly demonstrating its effectiveness. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tian20171017,
author={Tian, G. and Mori, T. and Okuda, Y.},
title={Spoofing detection for embedded face recognition system using a low cost stereo camera},
journal={Proceedings - International Conference on Pattern Recognition},
year={2017},
pages={1017-1022},
doi={10.1109/ICPR.2016.7899769},
art_number={7899769},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019057068&doi=10.1109%2fICPR.2016.7899769&partnerID=40&md5=bf20d5a07ea9e072ba6cbe319eabd42f},
affiliation={Embedded Core Technology Department, Center for Semiconductor RandD, TOSHIBA, Japan},
abstract={Spoofing detection is essential for practical face recognition system. Based on the fact that genuine face has special geometric curvatures across surface, this paper brings forward an ultra-fast yet accurate spoofing detection approach using a low-cost stereo camera. To obtain curvatures, the three dimensional shapes of selected facial landmarks are analyzed, by fitting point cloud around each landmark to a specific partial face surface. Spoofing detection is then performed by evaluating curvatures of each landmark and integrating them together. Experiments verify that the approach is able to detect spoofed faces in printed photographs without or with various bending at FAR equal to 0.00%. Meanwhile, genuine faces have a trivial opportunity to be falsely rejected: FRR is 0.59% for near frontal faces and less than 5% for faces with large varying poses. Detection time is 51 milliseconds when executed on a single processor [1] running at a clock frequency of 266M Hz, this makes the detection very suitable for embedded face recognition system. © 2016 IEEE.},
author_keywords={Point cloud;  Spoof detection;  Stereo vision;  Surface fitting},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pang2017585,
author={Pang, G. and Neumann, U.},
title={3D point cloud object detection with multi-view convolutional neural network},
journal={Proceedings - International Conference on Pattern Recognition},
year={2017},
pages={585-590},
doi={10.1109/ICPR.2016.7899697},
art_number={7899697},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019119289&doi=10.1109%2fICPR.2016.7899697&partnerID=40&md5=f7ff83a586a0107885e80fea79ee13b2},
affiliation={Department of Computer Science, University of Southern California, Los Angeles, CA  90089, United States},
abstract={Efficient detection of three dimensional (3D) objects in point clouds is a challenging problem. Performing 3D descriptor matching or 3D scanning-window search with detector are both time-consuming due to the 3-dimensional complexity. One solution is to project 3D point cloud into 2D images and thus transform the 3D detection problem into 2D space, but projection at multiple viewpoints and rotations produce a large amount of 2D detection tasks, which limit the performance and complexity of the 2D detection algorithm choice. We propose to use convolutional neural network (CNN) for the 2D detection task, because it can handle all viewpoints and rotations for the same class of object together, as well as predicting multiple classes of objects with the same network, without the need for individual detector for each object class. We further improve the detection efficiency by concatenating two extra levels of early rejection networks with binary outputs before the multi-class detection network. Experiments show that our method has competitive overall performance with at least one-order of magnitude speed-up comparing with latest 3D point cloud detection methods. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hiremath2017,
author={Hiremath, M. and Hiremath, P.S.},
title={3D face recognition based on symbolic FDA using SVM classifier with similarity and dissimilarity distance measure},
journal={International Journal of Pattern Recognition and Artificial Intelligence},
year={2017},
volume={31},
number={4},
doi={10.1142/S0218001417560067},
art_number={1756006},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994627715&doi=10.1142%2fS0218001417560067&partnerID=40&md5=a06c960ed3e2ce3aaa85afa86f73c57b},
affiliation={Department of Computer Science, Christ University, Bangalore, Karnataka, 560029, India; Department of Computer Science (MCA), KLE Technological University, BVBCET, Hubli, Karnataka, 580031, India},
abstract={Human face images are the basis not only for person recognition, but for also identifying other attributes like gender, age, ethnicity, and emotional states of a person. Therefore, face is an important biometric identifier in the law enforcement and human-computer interaction (HCI) systems. The 3D human face recognition is emerging as a significant biometric technology. Research interest into 3D face recognition has increased during recent years due to availability of improved 3D acquisition devices and processing algorithms. A 3D face image is represented by 3D meshes or range images which contain depth information. In this paper, the objective is to propose a new 3D face recognition method based on radon transform and symbolic factorial discriminant analysis using KNN and SVM classifier with similarity and dissimilarity measures, which are applied on 3D facial range images. The experimentation is done using three publicly available databases, namely, Bhosphorus, Texas and CASIA 3D face database. The experimental results demonstrate the effectiveness of the proposed method. © 2017 World Scientific Publishing Company.},
author_keywords={dissimilarity measure;  factorial discriminant analysis;  KNN;  Radon transform;  similarity measure;  SVM},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang2017180,
author={Yang, B. and Dong, Z. and Liu, Y. and Liang, F. and Wang, Y.},
title={Computing multiple aggregation levels and contextual features for road facilities recognition using mobile laser scanning data},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2017},
volume={126},
pages={180-194},
doi={10.1016/j.isprsjprs.2017.02.014},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014280392&doi=10.1016%2fj.isprsjprs.2017.02.014&partnerID=40&md5=e55b1a964646c3a7ef9a46f5371ec8d1},
affiliation={State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, 430079, China; Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing, 210023, China; Key Laboratory of Virtual Geographic Environment, Ministry of Education, Nanjing Normal University, Nanjing, 210093, China; State Key Laboratory Cultivation Base of Geographical Environment Evolution, Nanjing, 210093, China},
abstract={In recent years, updating the inventory of road infrastructures based on field work is labor intensive, time consuming, and costly. Fortunately, vehicle-based mobile laser scanning (MLS) systems provide an efficient solution to rapidly capture three-dimensional (3D) point clouds of road environments with high flexibility and precision. However, robust recognition of road facilities from huge volumes of 3D point clouds is still a challenging issue because of complicated and incomplete structures, occlusions and varied point densities. Most existing methods utilize point or object based features to recognize object candidates, and can only extract limited types of objects with a relatively low recognition rate, especially for incomplete and small objects. To overcome these drawbacks, this paper proposes a semantic labeling framework by combing multiple aggregation levels (point-segment-object) of features and contextual features to recognize road facilities, such as road surfaces, road boundaries, buildings, guardrails, street lamps, traffic signs, roadside-trees, power lines, and cars, for highway infrastructure inventory. The proposed method first identifies ground and non-ground points, and extracts road surfaces facilities from ground points. Non-ground points are segmented into individual candidate objects based on the proposed multi-rule region growing method. Then, the multiple aggregation levels of features and the contextual features (relative positions, relative directions, and spatial patterns) associated with each candidate object are calculated and fed into a SVM classifier to label the corresponding candidate object. The recognition performance of combining multiple aggregation levels and contextual features was compared with single level (point, segment, or object) based features using large-scale highway scene point clouds. Comparative studies demonstrated that the proposed semantic labeling framework significantly improves road facilities recognition precision (90.6%) and recall (91.2%), particularly for incomplete and small objects. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Contextual features;  Multiple aggregation levels features;  Object recognition;  Point clouds processing;  Semantic labeling},
document_type={Article},
source={Scopus},
}

@CONFERENCE{An2017265,
author={An, S. and Ruan, Q.},
title={3D facial expression recognition algorithm using local threshold binary pattern and histogram of oriented gradient},
journal={International Conference on Signal Processing Proceedings, ICSP},
year={2017},
pages={265-270},
doi={10.1109/ICSP.2016.7877838},
art_number={7877838},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016330947&doi=10.1109%2fICSP.2016.7877838&partnerID=40&md5=3bbf297b8f793d91031b7c648e9ddc88},
affiliation={Institution of Information Science, Beijing Jiaotong University, Beijing, China},
abstract={Facial expression which carries rich information of body behavior is the leading carrier of human affective and the symbol of intelligence. The main purpose of this paper is to recognize 3D human facial expression. The research in this paper includes the expression feature extraction algorithm and fusion with different kinds of feature. To contain more local texture feature information, we proposed a new feature of 3D facial expression named Local Threshold Binary Pattern (LTBP) which based on Local Binary Pattern (LBP). We calculate the difference of gray value standard between neighboring pixels and the center pixel as a threshold to binary instead of the traditional LBP operation which only comparison of size between neighboring pixels and the center pixel. After we get the LTBP feature, we fuse the LTBP and HOG (Histogram of Oriented Gradient) features to get multi-feature fusion for 3D facial expression recognition. Our algorithm of 3D facial expression recognition comprises three steps: (1) extracting two sets of feature vectors and establishing the correlation criterion function between the two sets of feature vectors; (2) solving the two sets canonical projective vectors and extracting their canonical correlation features by the framework of canonical correlation analysis algorithm; (3) doing feature fusion for classification by using proposed strategy. We have performed comprehensive experiments on the BU-3DFE database which is presently the largest available 3D face database. We have achieved verification rates of more than 90% for the 3D facial expression recognition. © 2016 IEEE.},
author_keywords={3D expression recognition;  canonical correlation analysis;  Local Threshold Binary Pattern (LTBP);  multi-feature fusion},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rhee2017,
author={Rhee, S.-M. and Yoo, B. and Han, J.-J. and Hwang, W.},
title={Deep neural network using color and synthesized three-dimensional shape for face recognition},
journal={Journal of Electronic Imaging},
year={2017},
volume={26},
number={2},
doi={10.1117/1.JEI.26.2.020502},
art_number={020502},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016440555&doi=10.1117%2f1.JEI.26.2.020502&partnerID=40&md5=0c08bed6dd431cdf462007180e9dda1f},
affiliation={Samsung Advanced Institute of Technology, Suwon City, South Korea; Korea Advanced Institute of Science and Technology, Department of Electrical Engineering, Daejeon City, South Korea; Ajou University, Department of Software and Computer Engineering, Suwon City, South Korea},
abstract={We present an approach for face recognition using synthesized three-dimensional (3-D) shape information together with two-dimensional (2-D) color in a deep convolutional neural network (DCNN). As 3-D facial shape is hardly affected by the extrinsic 2-D texture changes caused by illumination, make-up, and occlusions, it could provide more reliable complementary features in harmony with the 2-D color feature in face recognition. Unlike other approaches that use 3-D shape information with the help of an additional depth sensor, our approach generates a personalized 3-D face model by using only face landmarks in the 2-D input image. Using the personalized 3-D face model, we generate a frontalized 2-D color facial image as well as 3-D facial images (e.g., a depth image and a normal image). In our DCNN, we first feed 2-D and 3-D facial images into independent convolutional layers, where the low-level kernels are successfully learned according to their own characteristics. Then, we merge them and feed into higher-level layers under a single deep neural network. Our proposed approach is evaluated with labeled faces in the wild dataset and the results show that the error rate of the verification rate at false acceptance rate 1% is improved by up to 32.1% compared with the baseline where only a 2-D color image is used. © 2017 SPIE and IS&T.},
author_keywords={convolutional neural network;  face recognition;  three-dimensional face model},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ramachandra2017,
author={Ramachandra, R. and Busch, C.},
title={Presentation attack detection methods for face recognition systems: A comprehensive survey},
journal={ACM Computing Surveys},
year={2017},
volume={50},
number={1},
doi={10.1145/3038924},
art_number={8},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017163319&doi=10.1145%2f3038924&partnerID=40&md5=65f9075a88c74cd973fc3ea10972e815},
affiliation={Norwegian Biometric Laboratory, Norwegian University of Science and Technology (NTNU), Gjøvik, Norway; Norwegian University of Science and Technology (NTNU), Department of Information Security and Communication Technology, Mail Box 191, Gjøvik, NO-2815, Norway},
abstract={The vulnerability of face recognition systems to presentation attacks (also known as direct attacks or spoof attacks) has received a great deal of interest from the biometric community. The rapid evolution of face recognition systems into real-time applications has raised new concerns about their ability to resist presentation attacks, particularly in unattended application scenarios such as automated border control. The goal of a presentation attack is to subvert the face recognition system by presenting a facial biometric artifact. Popular face biometric artifacts include a printed photo, the electronic display of a facial photo, replaying video using an electronic display, and 3D face masks. These have demonstrated a high security risk for state-of-the-art face recognition systems. However, several presentation attack detection (PAD) algorithms (also known as countermeasures or antispoofing methods) have been proposed that can automatically detect and mitigate such targeted attacks. The goal of this survey is to present a systematic overview of the existing work on face presentation attack detection that has been carried out. This paper describes the various aspects of face presentation attacks, including different types of face artifacts, state-of-the-art PAD algorithms and an overview of the respective research labs working in this domain, vulnerability assessments and performance evaluation metrics, the outcomes of competitions, the availability of public databases for benchmarking new PAD algorithms in a reproducible manner, and finally a summary of the relevant international standardization in this field. Furthermore, we discuss the open challenges and future work that need to be addressed in this evolving field of biometrics. 2017 Copyright is held by the owner/author(s).},
author_keywords={Algorithms;  Antispoofing;  Attacks;  Biometrics;  C.2.2 [Pattern Recognition]: Applications;  Countermeasure;  Design;  Face recognition;  Performance;  Security},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2017,
author={Chen, J. and Fang, Y. and Cho, Y.K. and Kim, C.},
title={Principal Axes Descriptor for Automated Construction-Equipment Classification from Point Clouds},
journal={Journal of Computing in Civil Engineering},
year={2017},
volume={31},
number={2},
doi={10.1061/(ASCE)CP.1943-5487.0000628},
art_number={04016058},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013004977&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000628&partnerID=40&md5=5691930acb0d2f6e0e68896e42e58866},
affiliation={School of Electrical and Computer Engineering, Robotics Institute, Georgia Institute of Technology, 777 Atlanta Dr. N.W., Atlanta, GA  30332-0355, United States; School of Civil and Environmental Engineering, Georgia Institute of Technology, 790 Atlanta Dr., Atlanta, GA  30332-0355, United States; Dept. of Architectural Engineering, Chung-Ang Univ., 221 Heuksuk-dong, Dongjak-gu, Seoul, 156-756, South Korea},
abstract={Recognizing construction assets (e.g., materials, equipment, labor) from point cloud data of construction environments provides essential information for engineering and management applications including progress monitoring, safety management, supply-chain management, and quality control. This study introduces a novel principal axes descriptor (PAD) for construction-equipment classification from point cloud data. Scattered as-is point clouds are first processed with downsampling, segmentation, and clustering steps to obtain individual instances of construction equipment. A geometric descriptor consisting of dimensional variation, occupancy distribution, shape profile, and plane counting features is then calculated to encode three-dimensional (3D) characteristics of each equipment category. Using the derived features, machine learning methods such as k-nearest neighbors and support vector machine are employed to determine class membership among major construction-equipment categories such as backhoe loader, bulldozer, dump truck, excavator, and front loader. Construction-equipment classification with the proposed PAD was validated using computer-aided design (CAD)-generated point clouds as training data and laser-scanned point clouds from an equipment yard as testing data. The recognition performance was further evaluated using point clouds from a construction site as well as a pose variation data set. PAD was shown to achieve a higher recall rate and lower computation time compared to competing 3D descriptors. The results indicate that the proposed descriptor is a viable solution for construction-equipment classification from point cloud data. © 2016 American Society of Civil Engineers.},
author_keywords={Laser scanning;  Machine learning;  Object classification;  Object recognition;  Scattered point clouds},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Amin2017602,
author={Amin, R. and Shams, A.F. and Rahman, S.M.M. and Hatzinakos, D.},
title={Evaluation of Discrimination power of facial parts from 3D point cloud data},
journal={Proceedings of 9th International Conference on Electrical and Computer Engineering, ICECE 2016},
year={2017},
pages={602-605},
doi={10.1109/ICECE.2016.7853992},
art_number={7853992},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016176950&doi=10.1109%2fICECE.2016.7853992&partnerID=40&md5=ed19a9638f37d7a968aaf2d3f22af2a1},
affiliation={Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh; Department of Electrical and Computer Engineering University of Toronto, Identity, Privacy and Security Institute, Toronto, ON  M5S2E4, Canada},
abstract={Feature selection from facial regions is a well-known approach to increase the performance of 2D image-based face recognition systems. In case of 3D modality, the approach of region-based feature selection for face recognition is relatively new. In this context, this paper presents an approach to evaluate the discrimination power of different regions of a 3D facial surface for its potential use in face recognition systems. We propose the use of weighted average of unit normal vector on the facial surface as the feature for region-based face recognition from 3D point cloud data (PCD). The iterative closest point algorithm is employed for the registration of segmented regions of facial point clouds. A metric based on angular distance between normals is introduced to indicate the similarity between two surfaces of same facial region. Finally, the intra class correlation based discrimination score is formulated to find out the key facial regions such as the eyes, nose, and mouth that are significant while recognizing a person with facial surface PCD. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yin2017341,
author={Yin, J. and Yang, X.},
title={3D facial reconstruction of based on OpenCV and DirectX},
journal={ICALIP 2016 - 2016 International Conference on Audio, Language and Image Processing - Proceedings},
year={2017},
pages={341-344},
doi={10.1109/ICALIP.2016.7846562},
art_number={7846562},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016115763&doi=10.1109%2fICALIP.2016.7846562&partnerID=40&md5=54854eae7b0e30a6849a20e0cc2a1e0b},
affiliation={College of Information Technology, Shanghai Jianqiao Univercity, Shanghai, China},
abstract={Face as a biometric identification in computer vision is an important medium, in areas such as video surveillance, animation games, security anti-terrorist has a very wide range of applications, creating vivid, strong visibility of 3d face model, now has become a challenging in the field of computer vision is one of the important topics. At first, this paper used the zhongxing-micro ZC301P cameras to build a binocular stereo vision system for recording images. After the camera calibration and binocular calibration, the three-dimensional data of facial images were extracted using the functions of OpenCV computer vision library, and then 3d face model were reconstructed preliminary by DirectX. According the reconstruction process, the human face three-dimensional reconstruction software was designed and developed. The paper laid the foundation for the next step work that is to obtain more clear and strong visibility of 3d face. © 2016 IEEE.},
author_keywords={3d face model;  Computer vision;  DirectX;  OpenCV},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gedat2017627,
author={Gedat, E. and Fechner, P. and Fiebelkorn, R. and Vandenhouten, R.},
title={Multiple human skeleton recognition in RGB and depth images with graph theory, anatomic refinement of point clouds and machine learning},
journal={2016 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2016 - Conference Proceedings},
year={2017},
pages={627-631},
doi={10.1109/SMC.2016.7844310},
art_number={7844310},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015757362&doi=10.1109%2fSMC.2016.7844310&partnerID=40&md5=457718e5de7befb4e3049d644bad9242},
affiliation={Telematics Research Group, Wildau Technical University of Applied Sciences, Wildau, 15745, Germany},
abstract={Computer visual recognition of multiple human poses infers technological benefit in a variety of systems, including security surveillance, medical therapeutics, sports analytics and many more. For this goal the set of detected body parts on color or depth images must be aligned to reconstruct the skeletons of the humans. Here, an algorithm is introduced that models the body part point clouds using principal component analysis to obtain anatomically correct positions of joints, and that assembles the redundant and/or incomplete number of candidate joints with graph theoretical methods using Suurballe's k-shortest disjoint paths algorithm to build the skeletons. The computations were applied to MOCAP database motions rendered in Blender to produce idealized classified point clouds, and to real human depth images classified with decision forests similar to Shotton et al. For MOCAP data, in 68 images showing 3 persons all 204 skeletons were correctly aligned using 4.285 of 4.682 joints with no false assignment. For 33 real human images each showing 3 people, 71 skeletons were correctly detected with 1 false detection and 17 misses, which is promising with respect to non-perfect body part classification in real world. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zeng2017193,
author={Zeng, D. and Zhao, Q. and Long, S. and Li, J.},
title={Examplar coherent 3D face reconstruction from forensic mugshot database},
journal={Image and Vision Computing},
year={2017},
volume={58},
pages={193-203},
doi={10.1016/j.imavis.2016.03.001},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964334521&doi=10.1016%2fj.imavis.2016.03.001&partnerID=40&md5=68792bdc7bed9af6f8bedff2ca82197d},
affiliation={College of Computer Science, Sichuan University, Chengdu, 610065, China},
abstract={Reconstructing 3D face models from 2D face images is usually done by using a single reference 3D face model or some gender/ethnicity specific 3D face models. However, different persons, even those of the same gender or ethnicity, usually have significantly different faces in terms of their overall appearance, which forms the base of person recognition via faces. Consequently, existing 3D reference model based methods have limited capability of reconstructing precise 3D face models for a large variety of persons. In this paper, we propose to explore a reservoir of diverse reference models for 3D face reconstruction from forensic mugshot face images, where facial examplars coherent with the input determine the final shape estimation. Specifically, our 3D face reconstruction is formulated as an energy minimization problem with: 1) shading constraint from multiple input face images, 2) distortion and self-occlusion based color consistency between different views, and 3) depth uncertainty based smoothness constraint on adjacent pixels. The proposed energy is minimized in a coarse to fine way, where the shape refinement step is done by using a multi-label segmentation algorithm. Experimental results on challenging datasets demonstrate that the proposed algorithm is capable of recovering high quality 3D face models. We also show that our reconstructed models successfully boost face recognition accuracy. © 2016 Elsevier B.V.},
author_keywords={3D face reconstruction;  Examplar coherent;  Forensic mugshot images},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cheng20173,
author={Cheng, S. and Marras, I. and Zafeiriou, S. and Pantic, M.},
title={Statistical non-rigid ICP algorithm and its application to 3D face alignment},
journal={Image and Vision Computing},
year={2017},
volume={58},
pages={3-12},
doi={10.1016/j.imavis.2016.10.007},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007508497&doi=10.1016%2fj.imavis.2016.10.007&partnerID=40&md5=ddea725a28181f693ce70578bc198d41},
affiliation={Department of Computing, Imperial College London, United Kingdom; EEMCS, University of Twente, Netherlands},
abstract={The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15–20years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region. © 2016 Elsevier B.V.},
author_keywords={3D face alignment;  BU-4DFE database;  FRGC v2 database;  Non-rigid ICP algorithm;  Statistical shape model},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jeni201713,
author={Jeni, L.A. and Cohn, J.F. and Kanade, T.},
title={Dense 3D face alignment from 2D video for real-time use},
journal={Image and Vision Computing},
year={2017},
volume={58},
pages={13-24},
doi={10.1016/j.imavis.2016.05.009},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006747280&doi=10.1016%2fj.imavis.2016.05.009&partnerID=40&md5=e04eee2e6c2caac1063ee1fce1853635},
affiliation={Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Department of Psychology, University of Pittsburgh, Pittsburgh, PA, United States},
abstract={To enable real-time, person-independent 3D registration from 2D video, we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60°. From a single 2D image of a person's face, a dense 3D shape is registered in real time for each frame. The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression. The algorithm first estimates the location of a dense set of landmarks and their visibility, then reconstructs face shapes by fitting a part-based 3D model. Because no assumptions are required about illumination or surface properties, the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video. The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction, extension to multi-view reconstruction, temporal integration for videos and 3D head-pose estimation. Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video. The software is available online at http://zface.org. © 2016 Elsevier B.V.},
author_keywords={3D face alignment;  Dense 3D model;  Real-time method},
document_type={Article},
source={Scopus},
}

@ARTICLE{Woodward201761,
author={Woodward, A. and Chan, Y.H. and Gong, R. and Nguyen, M. and Gee, T. and Delmas, P. and Gimel'farb, G. and Marquez Flores, J.A.},
title={A low cost framework for real-time marker based 3-D human expression modeling},
journal={Journal of Applied Research and Technology},
year={2017},
volume={15},
number={1},
pages={61-77},
doi={10.1016/j.jart.2017.01.002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013659529&doi=10.1016%2fj.jart.2017.01.002&partnerID=40&md5=21fa52776df8f0831d95326ccfec57eb},
affiliation={Department of General Systems Sciences, The Graduate School of Arts and Sciences, The University of Tokyo, Japan; Department of Computer Science, The University of Auckland, New Zealand; Centro de Ciencias Aplicadas y Desarrollo Tecnológico, Universidad Nacional Autónoma de México, Circuito Exterior S/N, Ciudad Universitaria AP 70-186, C.P. 04510, México, D.F, Mexico},
abstract={This work presents a robust, and low-cost framework for real-time marker based 3-D human expression modeling using off-the-shelf stereo web-cameras and inexpensive adhesive markers applied to the face. The system has low computational requirements, runs on standard hardware, and is portable with minimal set-up time and no training. It does not require a controlled lab environment (lighting or set-up) and is robust under varying conditions, i.e. illumination, facial hair, or skin tone variation. Stereo web-cameras perform 3-D marker tracking to obtain head rigid motion and the non-rigid motion of expressions. Tracked markers are then mapped onto a 3-D face model with a virtual muscle animation system. Muscle inverse kinematics update muscle contraction parameters based on marker motion in order to create a virtual character's expression performance. The parametrization of the muscle-based animation encodes a face performance with little bandwidth. Additionally, a radial basis function mapping approach was used to easily remap motion capture data to any face model. In this way the automated creation of a personalized 3-D face model and animation system from 3-D data is elucidated. The expressive power of the system and its ability to recognize new expressions was evaluated on a group of test subjects with respect to the six universally recognized facial expressions. Results show that the use of abstract muscle definition reduces the effect of potential noise in the motion capture data and allows the seamless animation of any virtual anthropomorphic face model with data acquired through human face performance. © 2017 Universidad Nacional Autónoma de México, Centro de Ciencias Aplicadas y Desarrollo Tecnológico},
author_keywords={Expression recognition;  Facial motion capture;  Low cost;  Marker based motion capture;  Stereo vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hoang2017,
author={Hoang, D.-C. and Chen, L.-C. and Nguyen, T.-H.},
title={Sub-OBB based object recognition and localization algorithm using range images},
journal={Measurement Science and Technology},
year={2017},
volume={28},
number={2},
doi={10.1088/1361-6501/aa513a},
art_number={025401},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010991002&doi=10.1088%2f1361-6501%2faa513a&partnerID=40&md5=e544800a2b72a37bd2a4598668371d54},
affiliation={Department of Mechanical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan; Department of Mechanics of Materials and Structures, School of Mechanical Engineering, Hanoi University of Science and Technology, Hanoi, Viet Nam},
abstract={This paper presents a novel approach to recognize and estimate pose of the 3D objects in cluttered range images. The key technical breakthrough of the developed approach can enable robust object recognition and localization under undesirable condition such as environmental illumination variation as well as optical occlusion to viewing the object partially. First, the acquired point clouds are segmented into individual object point clouds based on the developed 3D object segmentation for randomly stacked objects. Second, an efficient shape-matching algorithm called Sub-OBB based object recognition by using the proposed oriented bounding box (OBB) regional area-based descriptor is performed to reliably recognize the object. Then, the 3D position and orientation of the object can be roughly estimated by aligning the OBB of segmented object point cloud with OBB of matched point cloud in a database generated from CAD model and 3D virtual camera. To detect accurate pose of the object, the iterative closest point (ICP) algorithm is used to match the object model with the segmented point clouds. From the feasibility test of several scenarios, the developed approach is verified to be feasible for object pose recognition and localization. © 2016 IOP Publishing Ltd.},
author_keywords={3D imaging processing;  3D object recognition;  3D point cloud;  pose estimation;  range image},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kopinski2017,
author={Kopinski, T. and Gepperth, A. and Handmann, U.},
title={A time-of-flight-based hand posture database for human-machine interaction},
journal={2016 14th International Conference on Control, Automation, Robotics and Vision, ICARCV 2016},
year={2017},
doi={10.1109/ICARCV.2016.7838613},
art_number={7838613},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010079572&doi=10.1109%2fICARCV.2016.7838613&partnerID=40&md5=f33679e9100a5d979fbe8e4d38009fa2},
affiliation={ENSTA ParisTech, 858 Blvd des Maréchaux, Palaiseau, 91762, France; Computer Science Institute, Hochschule Ruhr West Lützowstrasse 5, Bottrop, 46236, Germany},
abstract={We present a publicly available benchmark database for the problem of hand posture recognition from noisy depth data and fused RGB-D data obtained from low-cost time-of-flight (ToF) sensors. The database is the most extensive database of this kind containing over a million data samples (point clouds) recorded from 35 different individuals for ten different static hand postures. This captures a great amount of variance, due to person-related factors, but also scaling, translation and rotation are explicitly represented. Benchmark results achieved with a standard classification algorithm are computed by cross-validation both over samples and persons, the latter implying training on all persons but one and testing on the remaining one. An important result using this database is that cross-validation performance over samples (which is the standard procedure in machine learning) is systematically higher than cross-validation performance over persons, which is to our mind the true application-relevant measure of generalization performance. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Naveen2017112,
author={Naveen, S. and Rugmini, K.P. and Moni, R.S.},
title={3D face reconstruction by pose correction, patch cloning and texture wrapping},
journal={2016 International Conference on Communication Systems and Networks, ComNet 2016},
year={2017},
pages={112-116},
doi={10.1109/CSN.2016.7823997},
art_number={7823997},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014911379&doi=10.1109%2fCSN.2016.7823997&partnerID=40&md5=d83e42a8548567d921eea89900252c10},
affiliation={Dept. of ECE, LBSITW, Thiruvananthapuram Kerala, India; Dept. of ECE Marian Engineering College, Thiruvananthapuram Kerala, India},
abstract={Face is being considered as one of the most commonly used biometric modality. The inaccuracy in two dimensional face recognition systems is mainly due to pose variations, occlusions, illumination etc. Among this, changes in illumination condition do not affect 3D face recognition systems. But pose variation drastically changes the appearance of face images. To solve the problems with depth map and texture images corrupted by head pose variations and the occlusions generated due to these pose variations, a reconstruction method is proposed which consist of three stages. In the first stage, the pose correction is done by Iterative Closest Point (ICP) algorithm and in the second stage the occluded region of the face is reconstructed by a resurfacing method called patch cloning. It is followed by the wrapping of reconstructed depth map by its texture to generate a 3D model. The statistical error between the original face and the reconstructed face is also evaluated. In this work, facial symmetry is used as a prior knowledge. Experiments are done with the FRAV3D database. © 2016 IEEE.},
author_keywords={Face recognition;  Face Resurfacing;  ICP algorithm;  Patch Cloning;  Pose Correction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jin20171,
author={Jin, H. and Wang, X. and Zhong, Z. and Hua, J.},
title={Robust 3D face modeling and reconstruction from frontal and side images},
journal={Computer Aided Geometric Design},
year={2017},
volume={50},
pages={1-13},
doi={10.1016/j.cagd.2016.11.001},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001950432&doi=10.1016%2fj.cagd.2016.11.001&partnerID=40&md5=867414c091fd12f6f44b38780a9503bc},
affiliation={Department of Computer Science, Wayne State University, Detroit, MI  48098, United States; School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, 310018, China},
abstract={Robust and effective capture and reconstruction of 3D face models directly by smartphone users enables many applications. This paper presents a novel 3D face modeling and reconstruction solution that robustly and accurately acquire 3D face models from a couple of images captured by a single smartphone camera. Two selfie photos of a subject taken from the front and side are first used to guide our Non-Negative Matrix Factorization (NMF) induced part-based face model to iteratively reconstruct an initial 3D face of the subject. Then, an iterative detail updating method is applied to the initial generated 3D face to reconstruct facial details through optimizing lighting parameters and local depths. Our iterative 3D face reconstruction method permits fully automatic registration of a part-based face representation to the acquired face data and the detailed 2D/3D features to build a high-quality 3D face model. The NMF part-based face representation learned from a 3D face database facilitates effective global and adaptive local detail data fitting alternatively. Our system is flexible and it allows users to conduct the capture in any uncontrolled environment. We demonstrate the capability of our method by allowing users to capture and reconstruct their 3D faces by themselves. © 2016 Elsevier B.V.},
author_keywords={3D face reconstruction;  Face modeling;  Non-negative Matrix Factorization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hariri2017187,
author={Hariri, W. and Tabia, H. and Farah, N. and Declercq, D. and Benouareth, A.},
title={Geometrical and visual feature quantization for 3D face recognition},
journal={VISIGRAPP 2017 - Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
year={2017},
volume={5},
pages={187-193},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013991858&partnerID=40&md5=fd026eef10dcc4922fe7269db34feb6c},
affiliation={ETIS, ENSEA, University of Cergy-Pontoise, CNRS, UMR 8051, Cergy-Pontoise, France; Labged Laboratory, Computer Science Department, Badji Mokhtar Annaba University, Annaba, Algeria},
abstract={In this paper, we present an efficient method for 3D face recognition based on vector quantization of both geometrical and visual proprieties of the face. The method starts by describing each 3D face using a set of orderless features, and use then the Bag-of-Features paradigm to construct the face signature. We analyze the performance of three well-known classifiers: the Naïve Bayes, the Multilayer perceptron and the Random forests. The results reported on the FRGCv2 dataset show the effectiveness of our approach and prove that the method is robust to facial expression. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Bag-of-Features;  Codebook;  Depth image;  HoS;  LBP;  Term vector},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2017633,
author={Zhang, T. and Mu, Z. and Li, Y. and Liu, Q. and Zhang, Y.},
title={3D face and ear recognition based on partial mars map},
journal={ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
year={2017},
volume={2017-January},
pages={633-637},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049438191&partnerID=40&md5=142925806e53f82e08f8cdf1fe8539c6},
affiliation={School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China},
abstract={This paper proposes a 3D face recognition approach based on facial pose estimation, which is robust to large pose variations in the unconstrained scene. Deep learning method is used to facial pose estimation, and the generation of partial MARS (Multimodal fAce and eaR Spherical) map reduces the probability of feature points appearing in the deformed region. Then we extract the features from the depth and texture maps. Finally, the matching scores from two types of maps should be calculated by Bayes decision to generate the final result. In the large pose variations, the recognition rate of the method in this paper is 94.6%. The experimental results show that our approach has superior performance than the existing methods used on the MARS map, and has potential to deal with 3D face recognition in unconstrained scene. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={3d face recognition;  Deep learning;  Head pose estimation;  Partial mars map},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tarnowski20171175,
author={Tarnowski, P. and Kołodziej, M. and Majkowski, A. and Rak, R.J.},
title={Emotion recognition using facial expressions},
journal={Procedia Computer Science},
year={2017},
volume={108},
pages={1175-1184},
doi={10.1016/j.procs.2017.05.025},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027373768&doi=10.1016%2fj.procs.2017.05.025&partnerID=40&md5=92303d48cd0cc403a6a117a9b8cf39bc},
affiliation={Warsaw University of Technology, Warsaw, Poland},
abstract={In the article there are presented the results of recognition of seven emotional states (neutral, joy, sadness, surprise, anger, fear, disgust) based on facial expressions. Coefficients describing elements of facial expressions, registered for six subjects, were used as features. The features have been calculated for three-dimensional face model. The classification of features were performed using k-NN classifier and MLP neural network. © 2017 The Authors. Published by Elsevier B.V.},
author_keywords={action units;  computer vision;  emotion recognition;  facial expression;  k-NN;  MLP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Deng201713,
author={Deng, X. and Da, F. and Shao, H.},
title={Expression-robust 3D face recognition based on feature-level fusion and feature-region fusion},
journal={Multimedia Tools and Applications},
year={2017},
volume={76},
number={1},
pages={13-31},
doi={10.1007/s11042-015-3012-8},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945315051&doi=10.1007%2fs11042-015-3012-8&partnerID=40&md5=5f6b0d57d4d091b3769e974b26751540},
affiliation={Department of Automation, Southeast University, Nanjing, Jiangsu  210096, China; Key Laboratory of Measurement and Control for Complex System of Ministry of Education, Southeast University, Nanjing, Jiangsu  210096, China},
abstract={3D face shape is essentially a non-rigid free-form surface, which will produce non-rigid deformation under expression variations. In terms of that problem, a promising solution named Coherent Point Drift (CPD) non-rigid registration for the non-rigid region is applied to eliminate the influence from the facial expression while guarantees 3D surface topology. In order to take full advantage of the extracted discriminative feature of the whole face under facial expression variations, the novel expression-robust 3D face recognition method using feature-level fusion and feature-region fusion is proposed. Furthermore, the Principal Component Analysis and Linear Discriminant Analysis in combination with Rotated Sparse Regression (PL-RSR) dimensionality reduction method is presented to promote the computational efficiency and provide a solution to the curse of dimensionality problem, which benefit the performance optimization. The experimental evaluation indicates that the proposed strategy has achieved the rank-1 recognition rate of 97.91 % and 96.71 % based on Face Recognition Grand Challenge (FRGC) v2.0 and Bosphorus respectively, which means the proposed approach outperforms state-of-the-art approach. © 2015, Springer Science+Business Media New York.},
author_keywords={3D face recognition;  Dimensionality reduction;  Feature-level fusion;  Feature-region fusion;  Non-rigid point set registration},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu201779,
author={Liu, Z. and Cohen, F.},
title={3D face reconstruction from image(s) based on gender and ethnicity generic models},
journal={Proceedings of the International Conferences on Computer Graphics, Visualization, Computer Vision and Image Processing 2017 and Big Data Analytics, Data Mining and Computational Intelligence 2017 - Part of the Multi Conference on Computer Science and Information Systems 2017},
year={2017},
pages={79-86},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040181568&partnerID=40&md5=beb7013a2442a44ce5162bcc0f5793fa},
affiliation={Kepler Group LLC, New York, NY  10016, United States; Drexel University, Electrical and Computer Engineering, Philadelphia, PA  19104, United States},
abstract={In this paper, we describe a novel approach for creating a 3D face model from an image or images of a human face taken at a priori unknown poses using gender and ethnicity specific 3D generic face models. This process starts with a generic face model, which is morphed as images of the person become available using pre-selected point landmarks that are tessellated to form a high resolution triangular mesh. The 3D face synthesis allows for accurate pose estimation as well as face identification in 3D. The estimation of the unknown pose is achieved through the use of a Levenberg-Marquardt optimization process. Encouraging experimental results are obtained in controlled environment with high resolution images under ideal illumination condition, as well as for images taken in uncontrolled environment under arbitrary illumination with low resolution cameras. © 2017.},
author_keywords={3D Reconstruction;  Pose Estimation;  Ray Tracing;  Subdivision},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ferrari20172666,
author={Ferrari, C. and Lisanti, G. and Berretti, S. and Del Bimbo, A.},
title={A Dictionary Learning-Based 3D Morphable Shape Model},
journal={IEEE Transactions on Multimedia},
year={2017},
volume={19},
number={12},
pages={2666-2679},
doi={10.1109/TMM.2017.2707341},
art_number={7932891},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034022009&doi=10.1109%2fTMM.2017.2707341&partnerID=40&md5=872aded70c367e32052b95c981f92b8b},
affiliation={Department of Information Engineering, University of Firenze, Firenze, 50139, Italy},
abstract={Face analysis from 2D images and videos is a central task in many multimedia applications. Methods developed to this end perform either face recognition or facial expression recognition, and in both cases results are negatively influenced by variations in pose, illumination, and resolution of the face. Such variations have a lower impact on 3D face data, which has given the way to the idea of using a 3D morphable model as an intermediate tool to enhance face analysis on 2D data. In this paper, we propose a new approach for constructing a 3D morphable shape model (called DL-3DMM) and show our solution can reach the accuracy of deformation required in applications where fine details of the face are concerned. For constructing the model, we start from a set of 3D face scans with large variability in terms of ethnicity and expressions. Across these training scans, we compute a point-to-point dense alignment, which is accurate also in the presence of topological variations of the face. The DL-3DMM is constructed by learning a dictionary of basis components on the aligned scans. The model is then fitted to 2D target faces using an efficient regularized ridge-regression guided by 2D/3D facial landmark correspondences in order to generate pose-normalized face images. Comparison between the DL-3DMM and the standard PCA-based 3DMM demonstrates that in general a lower reconstruction error can be obtained with our solution. Application to action unit detection and emotion recognition from 2D images and videos shows competitive results with state of the art methods on two benchmark datasets. © 2017 IEEE.},
author_keywords={3D morphable model;  Action unit detection;  Dense correspondence;  Dictionary learning;  Emotion recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kakadiaris2017137,
author={Kakadiaris, I.A. and Toderici, G. and Evangelopoulos, G. and Passalis, G. and Chu, D. and Zhao, X. and Shah, S.K. and Theoharis, T.},
title={3D-2D face recognition with pose and illumination normalization},
journal={Computer Vision and Image Understanding},
year={2017},
volume={154},
pages={137-151},
doi={10.1016/j.cviu.2016.04.012},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979671321&doi=10.1016%2fj.cviu.2016.04.012&partnerID=40&md5=d935472a27865f52a8eeffc16c56d41d},
affiliation={Computational Biomedicine Laboratory (CBL), Department of Computer Science, Univ. of Houston, 4800 Calhoun, Houston, TX 77204, United States; Department of Informatics, Univ. of Athens, TYPA Buildings, Panepistimiopolis, Ilisia, 15784, Athens, Greece; CBL, University of Houston, United States; School of Management, Xian Jiaotong University, Xian, 710049, China; IDI, NTNU, University of Athens, Greece},
abstract={In this paper, we propose a 3D-2D framework for face recognition that is more practical than 3D-3D, yet more accurate than 2D-2D. For 3D-2D face recognition, the gallery data comprises of 3D shape and 2D texture data and the probes are arbitrary 2D images. A 3D-2D system (UR2D) is presented that is based on a 3D deformable face model that allows registration of 3D and 2D data, face alignment, and normalization of pose and illumination. During enrollment, subject-specific 3D models are constructed using 3D+2D data. For recognition, 2D images are represented in a normalized image space using the gallery 3D models and landmark-based 3D-2D projection estimation. A method for bidirectional relighting is applied for non-linear, local illumination normalization between probe and gallery textures, and a global orientation-based correlation metric is used for pairwise similarity scoring. The generated, personalized, pose- and light- normalized signatures can be used for one-to-one verification or one-to-many identification. Results for 3D-2D face recognition on the UHDB11 3D-2D database with 2D images under large illumination and pose variations support our hypothesis that, in challenging datasets, 3D-2D outperforms 2D-2D and decreases the performance gap against 3D-3D face recognition. Evaluations on FRGC v2.0 3D-2D data with frontal facial images, demonstrate that the method can generalize to databases with different and diverse illumination conditions. © 2016},
author_keywords={3D-2D face recognition;  3D-2D model fitting;  Biometrics;  Computer vision;  Face and gesture recognition;  Illumination normalization;  Model-based face recognition;  Object recognition;  Physically-based modeling},
document_type={Article},
source={Scopus},
}

@ARTICLE{Moeini20171,
author={Moeini, A. and Faez, K. and Moeini, H. and Safai, A.M.},
title={Open-set face recognition across look-alike faces in real-world scenarios},
journal={Image and Vision Computing},
year={2017},
volume={57},
pages={1-14},
doi={10.1016/j.imavis.2016.11.002},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999154589&doi=10.1016%2fj.imavis.2016.11.002&partnerID=40&md5=0461345fe88a1ccff90afaed4fc44e4e},
affiliation={Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran; Electrical Engineering Department, Semnan University, Semnan, Iran; Computer Science and Engineering Department, University of California, San Diego, California, United States},
abstract={The open-set problem is among the problems that have significantly changed the performance of face recognition algorithms in real-world scenarios. Open-set operates under the supposition that not all the probes have a pair in the gallery. Most face recognition systems in real-world scenarios focus on handling pose, expression and illumination problems on face recognition. In addition to these challenges, when the number of subjects is increased for face recognition, these problems are intensified by look-alike faces for which there are two subjects with lower intra-class variations. In such challenges, the inter-class similarity is higher than the intra-class variation for these two subjects. In fact, these look-alike faces can be created as intrinsic, situation-based and also by facial plastic surgery. This work introduces three real-world open-set face recognition methods across facial plastic surgery changes and a look-alike face by 3D face reconstruction and sparse representation. Since some real-world databases for face recognition do not have multiple images per person in the gallery, with just one image per subject in the gallery, this paper proposes a novel idea to overcome this challenge by 3D modeling from gallery images and synthesizing them for generating several images. Accordingly, a 3D model is initially reconstructed from frontal face images in a real-world gallery. Then, each 3D reconstructed face in the gallery is synthesized to several possible views and a sparse dictionary is generated based on the synthesized face image for each person. Also, a likeness dictionary is defined and its optimization problem is solved by the proposed method. Finally, the face recognition is performed for open-set face recognition using three proposed representation classifications. Promising results are achieved for face recognition across plastic surgery and look-alike faces on three databases including the plastic surgery face, look-alike face and LFW databases compared to several state-of-the-art methods. Also, several real-world and open-set scenarios are performed to evaluate the proposed method on these databases in real-world scenarios. © 2016 Elsevier B.V.},
author_keywords={Collaborative representation;  Facial plastic surgery;  Look-alike face;  Open-set face recognition;  Real-world scenarios;  Sparse representation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hammer2017,
author={Hammer, M. and Hebel, M. and Arens, M.},
title={Person detection and tracking with a 360° lidar system},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2017},
volume={10434},
doi={10.1117/12.2278215},
art_number={104340L},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446686&doi=10.1117%2f12.2278215&partnerID=40&md5=5843073ab0f3baf3e95aefb00d8747ea},
affiliation={Fraunhofer Institute of Optronics, System Technologies, and Image Exploitation IOSB, Gutleuthausstr. 1, Ettlingen, 76275, Germany},
abstract={Today it is easily possible to generate dense point clouds of the sensor environment using 360° LiDAR (Light Detection and Ranging) sensors which are available since a number of years. The interpretation of these data is much more challenging. For the automated data evaluation the detection and classification of objects is a fundamental task. Especially in urban scenarios moving objects like persons or vehicles are of particular interest, for instance in automatic collision avoidance, for mobile sensor platforms or surveillance tasks. In literature there are several approaches for automated person detection in point clouds. While most techniques show acceptable results in object detection, the computation time is often crucial. The runtime can be problematic, especially due to the amount of data in the panoramic 360° point clouds. On the other hand, for most applications an object detection and classification in real time is needed. The paper presents a proposal for a fast, real-time capable algorithm for person detection, classification and tracking in panoramic point clouds. © 2017 SPIE.},
author_keywords={360° LiDAR;  3D object detection;  organized point cloud;  person detection;  person tracking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Teizer2017157,
author={Teizer, J. and Gerson, A.M. and Hilfert, T. and Perschewski, M. and König, M.},
title={Mobile point cloud assessment for trench Safety audits},
journal={ISARC 2017 - Proceedings of the 34th International Symposium on Automation and Robotics in Construction},
year={2017},
pages={157-164},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032334202&partnerID=40&md5=3512b30e5f5270b207d745c60731289f},
affiliation={Department of Computing in Engineering, Ruhr-University Bochum, Germany; Occupational Safety and Health Professional, Boise, ID, United States},
abstract={Fatalities resulting from cave-in hazards during excavation work in the United States account for 48% of the trench fatalities in construction every year per Occupational Safety and Health Administration (OSHA) data. Recent trends indicate that fatalities from trench and excavation hazards in the US are increasing. Often the experience of safety inspectors and/or the designated competent person (CP) for trenching and excavation is vital when assessing sloping measurements with approved engineering survey tools. The degree of accidents, however, allows the conclusion that proper assessment and/or protection of excavation sites is not performed sufficiently in the field, or safety coordinators and/or adequately trained CPs are not on hand when needed. While existing assessment processes and protection methods are reviewed for potential improvement, this paper proposes a new compliance assistance prototype that incorporates state-of-the-art technology. The proposed prototype creates: (1) mobile field data acquisition with lowcost photo cameras to capture point cloud surveys of the as-built conditions of trenches, and (2) computational data processing to automatically extract trench height, width, and slope values. Early results to field-realistic experiments promise useful applications of the developed prototype for safety coordinators or adequately trained CPs.},
author_keywords={Cave-in hazards;  Hazard recognition;  Mobile application;  Photogrammetry;  Point clouds;  Remote sensing;  Risk management;  Safety education and training;  Trench safety},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Häufel2017,
author={Häufel, G. and Bulatov, D. and Solbrig, P.},
title={Sensor data fusion for textured reconstruction and virtual representation of alpine scenes},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2017},
volume={10428},
doi={10.1117/12.2278237},
art_number={1042805},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040194238&doi=10.1117%2f12.2278237&partnerID=40&md5=d493987a5f33890f50234d791a77f43c},
affiliation={Fraunhofer Institute of Optronics, System Technologies and Image Exploitation (IOSB), Gutleuthausstr. 1, Ettlingen, 76275, Germany},
abstract={The concept of remote sensing is to provide information about a wide-range area without making physical contact with this area. If, additionally to satellite imagery, images and videos taken by drones provide a more up-to-date data at a higher resolution, or accurate vector data is downloadable from the Internet, one speaks of sensor data fusion. The concept of sensor data fusion is relevant for many applications, such as virtual tourism, automatic navigation, hazard assessment, etc. In this work, we describe sensor data fusion aiming to create a semantic 3D model of an extremely interesting yet challenging dataset: An alpine region in Southern Germany. A particular challenge of this work is that rock faces including overhangs are present in the input airborne laser point cloud. The proposed procedure for identification and reconstruction of overhangs from point clouds comprises four steps: Point cloud preparation, filtering out vegetation, mesh generation and texturing. Further object types are extracted in several interesting subsections of the dataset: Building models with textures from UAV (Unmanned Aerial Vehicle) videos, hills reconstructed as generic surfaces and textured by the orthophoto, individual trees detected by the watershed algorithm, as well as the vector data for roads retrieved from openly available shapefiles and GPS-device tracks. We pursue geo-specific reconstruction by assigning texture and width to roads of several pre-determined types and modeling isolated trees and rocks using commercial software. For visualization and simulation of the area, we have chosen the simulation system Virtual Battlespace 3 (VBS3). It becomes clear that the proposed concept of sensor data fusion allows a coarse reconstruction of a large scene and, at the same time, an accurate and up-to-date representation of its relevant subsections, in which simulation can take place. © 2017 SPIE.},
author_keywords={3D-object extraction;  Alpine Area;  Overhang;  Road;  Rock face;  Sensor Data Fusion;  Simulation;  Visualisation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dutta201699,
author={Dutta, K. and Bhattacharjee, D. and Nasipuri, M.},
title={Expression and occlusion invariant 3D face recognition based on region classifier},
journal={Proceedings - 2016 1st International Conference on Information Technology, Information Systems and Electrical Engineering, ICITISEE 2016},
year={2016},
pages={99-104},
doi={10.1109/ICITISEE.2016.7803055},
art_number={7803055},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011277941&doi=10.1109%2fICITISEE.2016.7803055&partnerID=40&md5=1f7c5e310e926e850e0c41e4fc391244},
affiliation={Department of Computer Science and Engineering, Jadavpur University, Kolkata, India},
abstract={In recent year, 3D face images play a major role in face recognition over corresponding 2D images. In this work, the 3D range images are used for face recognition based on region classifiers. Three different regions: eye, nose, and mouth separately classify a face image locally. At first, local binary patterns (LBP) are calculated for all pixels on face images. A new image formed with these LBP values are cropped and then divided into three horizontal regions, namely eye, nose, and mouth. For each such region histogram of oriented gradient (HOG) is used for feature vector creation. Two databases: Frav3D of 106 different subjects and our database consisting of 102 various individuals are used for recognition. Only frontal images with occlusion, expression and neutral images from those databases are utilized for this proposed system. Two fold cross validation technique with a nearest centroid-based classifier is used for classification. In the case of decision level fusion, recognition accuracy is 88.86% on Frav3D database and 77.5% for our newly created database. On the other hand, score level fusion has shown 78.5% recognition accuracy for FRAV3D database and 65.55% for our database. © 2016 IEEE.},
author_keywords={3D range image;  Decision level fusion;  Histogram of the oriented gradient;  Local binary pattern;  Region classifier;  Score level fusion},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xie2016,
author={Xie, L. and Xu, L. and Zhang, N. and Guo, J. and Yan, Y. and Li, Z. and Li, Z. and Xu, X.},
title={Improved face recognition result reranking based on shape contexts},
journal={ACM International Conference Proceeding Series},
year={2016},
doi={10.1145/3028842.3028853},
art_number={11},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015655101&doi=10.1145%2f3028842.3028853&partnerID=40&md5=506d723f871ef5a809f2ffffca1e576e},
affiliation={Institute of Forensic Science, Ministry of Public Security, No.17 Muxidi, Nanli, 100038, China},
abstract={Automatic face recognition techniques applied on particular group or mass database introduces error cases. Error prevention is crucial for the court. Reranking of recognition results based on anthropology analysis can significant improve the accuracy of automatic methods. Previous studies focused on manual facial comparison. This paper proposed a weighted facial similarity computing method based on morphological analysis of components characteristics. Search sequence of face recognition reranked according to similarity, while the interference terms can be removed. Within this research project, standardized photographs, surveillance videos, 3D face images, identity card photographs of 241 male subjects from China were acquired. Sequencing results were modified by modeling selected individual features from the DMV altas. The improved method raises the accuracy of face recognition through anthroposophic or morphologic theory. © 2016 ACM.},
author_keywords={Face recognition;  Reranking;  Shape contexts;  Shape matching;  Similarity calculation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu2016,
author={Yu, X. and Gao, Y. and Zhou, J.},
title={Boosting Radial Strings for 3D Face Recognition with Expressions and Occlusions},
journal={2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
year={2016},
doi={10.1109/DICTA.2016.7797014},
art_number={7797014},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011088593&doi=10.1109%2fDICTA.2016.7797014&partnerID=40&md5=b22aac1e03b48565af8c230b46a10822},
affiliation={School of Engineering, Griffith University, Nathan, QLD, Australia; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia},
abstract={In this paper, we present a new radial string representation and matching approach for 3D face recognition under expression variations and partial occlusions. The radial strings are an indexed collection of strings emanating from the nose tip of a face scan. The matching between two radial strings is conducted through a dynamic programming process, in which a partial matching mechanism is established to effectively find those un-occluded substrings. Moreover, the most discriminative and stable radial strings are selected optimally by the well-known AdaBoost algorithm to achieve a composite classifier for 3D face recognition under facial expression changes. Experimental results on the GavabDB and the Bosphorus databases show that the proposed approach achieves promising results for human face recognition with expressions and occlusions. © 2016 IEEE.},
author_keywords={face recognition;  facial curves;  feature selection;  machine learning;  string matching},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gilani2016,
author={Gilani, S.Z. and Mian, A.},
title={Towards Large-Scale 3D Face Recognition},
journal={2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
year={2016},
doi={10.1109/DICTA.2016.7797090},
art_number={7797090},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011072173&doi=10.1109%2fDICTA.2016.7797090&partnerID=40&md5=8e7de072eb2e5794e1830d7699cecbb6},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, Australia},
abstract={3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Koch2016689,
author={Koch, T. and Korner, M. and Fraundorfer, F.},
title={Automatic Alignment of Indoor and Outdoor Building Models Using 3D Line Segments},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2016},
pages={689-697},
doi={10.1109/CVPRW.2016.91},
art_number={7789581},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010218095&doi=10.1109%2fCVPRW.2016.91&partnerID=40&md5=937da73c58750f8829c3a2d760233242},
affiliation={Remote Sensing Technology, Technical University of Munich, Germany; Institute for Computer Graphics and Vision, Graz University of Technology, Austria},
abstract={This paper presents an approach for automatically aligning the non-overlapping interior and exterior parts of a 3D building model computed from image based 3D reconstructions. We propose a method to align the 3D reconstructions by identifying corresponding 3D structures that are part of the interior and exterior model (e.g. openings like windows). In this context, we point out the potential of using 3D line segments to enrich the information of point clouds generated by SfMs and show how this can be used for interpreting the scene and matching individual reconstructions. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pham2016441,
author={Pham, H.X. and Pavlovic, V.},
title={Robust real-time 3D face tracking from RGBD videos under extreme pose, depth, and expression variation},
journal={Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
year={2016},
pages={441-449},
doi={10.1109/3DV.2016.54},
art_number={7785119},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011271350&doi=10.1109%2f3DV.2016.54&partnerID=40&md5=6924370986886448fd526d2bffffb467},
affiliation={Rutgers University, United States},
abstract={We introduce a novel end-to-end real-time pose-robust 3D face tracking framework from RGBD videos, which is capable of tracking head pose and facial actions simultaneously in unconstrained environment without intervention or pre-calibration from a user. In particular, we emphasize tracking the head pose from profile to profile and improving tracking performance in challenging instances, where the tracked subject is at a considerably large distance from the camera and the quality of data deteriorates severely. To achieve these goals, the tracker is guided by an efficient multi-view 3D shape regressor, trained upon generic RGB datasets, which is able to predict model parameters despite large head rotations or tracking range. Specifically, the shape regressor is made aware of the head pose by inferring the possibility of particular facial landmarks being visible through a joint regression-classification local random forest framework, and piecewise linear regression models effectively map visibility features into shape parameters. In addition, the regressor is combined with a joint 2D+3D optimization that sparsely exploits depth information to further refine shape parameters to maintain tracking accuracy over time. The result is a robust on-line RGBD 3D face tracker that can model extreme head poses and facial expressions accurately in challenging scenes, which are demonstrated in our extensive experiments. © 2016 IEEE.},
author_keywords={3D face tracking;  blendshape;  multi-view;  pose-robust},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bolkart20164911,
author={Bolkart, T. and Wuhrer, S.},
title={A robust multilinear model learning framework for 3D faces},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2016},
volume={2016-December},
pages={4911-4919},
doi={10.1109/CVPR.2016.531},
art_number={7780900},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986290418&doi=10.1109%2fCVPR.2016.531&partnerID=40&md5=9cc83b37487c932d1bb238a554feda32},
affiliation={Saarland University, Germany; Inria Grenoble Rhône-Alpes, France},
abstract={Multilinear models are widely used to represent the statistical variations of 3D human faces as they decouple shape changes due to identity and expression. Existing methods to learn a multilinear face model degrade if not every person is captured in every expression, if face scans are noisy or partially occluded, if expressions are erroneously labeled, or if the vertex correspondence is inaccurate. These limitations impose requirements on the training data that disqualify large amounts of available 3D face data from being usable to learn a multilinear model. To overcome this, we introduce the first framework to robustly learn a multilinear model from 3D face databases with missing data, corrupt data, wrong semantic correspondence, and inaccurate vertex correspondence. To achieve this robustness to erroneous training data, our framework jointly learns a multilinear model and fixes the data. We evaluate our framework on two publicly available 3D face databases, and show that our framework achieves a data completion accuracy that is comparable to state-of-the-art tensor completion methods. Our method reconstructs corrupt data more accurately than state-of-the-art methods, and improves the quality of the learned model significantly for erroneously labeled expressions. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Piotraschke20163418,
author={Piotraschke, M. and Blanz, V.},
title={Automated 3D Face Reconstruction from Multiple Images Using Quality Measures},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2016},
volume={2016-December},
pages={3418-3427},
doi={10.1109/CVPR.2016.372},
art_number={7780741},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986321438&doi=10.1109%2fCVPR.2016.372&partnerID=40&md5=ce380f4f1346e0e20f76e48ccae0b145},
affiliation={Institute for Vision and Graphics, University of Siegen, Germany},
abstract={Automated 3D reconstruction of faces from images is challenging if the image material is difficult in terms of pose, lighting, occlusions and facial expressions, and if the initial 2D feature positions are inaccurate or unreliable. We propose a method that reconstructs individual 3D shapes from multiple single images of one person, judges their quality and then combines the best of all results. This is done separately for different regions of the face. The core element of this algorithm and the focus of our paper is a quality measure that judges a reconstruction without information about the true shape. We evaluate different quality measures, develop a method for combining results, and present a complete processing pipeline for automated reconstruction. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Roth20164197,
author={Roth, J. and Tong, Y. and Liu, X.},
title={Adaptive 3D Face Reconstruction from Unconstrained Photo Collections},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2016},
volume={2016-December},
pages={4197-4206},
doi={10.1109/CVPR.2016.455},
art_number={7780824},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986327444&doi=10.1109%2fCVPR.2016.455&partnerID=40&md5=c96b2d527adbfb6289c41f4a079f76a0},
affiliation={Department of Computer Science and Engineering, Michigan State University, United States},
abstract={Given a collection of 'in-the-wild' face images captured under a variety of unknown pose, expression, and illumination conditions, this paper presents a method for reconstructing a 3D face surface model of an individual along with albedo information. Motivated by the success of recent face reconstruction techniques on large photo collections, we extend prior work to adapt to low quality photo collections with fewer images. We achieve this by fitting a 3D Morphable Model to form a personalized template and developing a novel photometric stereo formulation, under a coarse-to-fine scheme. Superior experimental results are reported on synthetic and real-world photo collections. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhu2016146,
author={Zhu, X. and Lei, Z. and Liu, X. and Shi, H. and Li, S.Z.},
title={Face alignment across large poses: A 3D solution},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2016},
volume={2016-December},
pages={146-155},
doi={10.1109/CVPR.2016.23},
art_number={7780392},
note={cited By 163},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986281478&doi=10.1109%2fCVPR.2016.23&partnerID=40&md5=c03e6dd1d0145cbabbe8ae495d6f24fc},
affiliation={Center for Biometrics and Security Research, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, China; Department of Computer Science and Engineering, Michigan State University, United States},
abstract={Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45), lacking the ability to align faces in large poses up to 90. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hackel20161610,
author={Hackel, T. and Wegner, J.D. and Schindler, K.},
title={Contour detection in unstructured 3D point clouds},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2016},
volume={2016-December},
pages={1610-1618},
doi={10.1109/CVPR.2016.178},
art_number={7780547},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986287982&doi=10.1109%2fCVPR.2016.178&partnerID=40&md5=98a604573bbc2339ab83733830f73b62},
affiliation={Photogrammetry and Remote Sensing, ETH Zürich, Switzerland},
abstract={We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds > 107 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lv2016735,
author={Lv, J.-J. and Huang, J.-S. and Zhou, X.-D. and Zhou, X. and Feng, Y.},
title={Latent face model for across-media face recognition},
journal={Neurocomputing},
year={2016},
volume={216},
pages={735-745},
doi={10.1016/j.neucom.2016.08.036},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994417637&doi=10.1016%2fj.neucom.2016.08.036&partnerID=40&md5=936d7a1174d13fa6cfcaf1c8b92a2e3d},
affiliation={Intelligent Multimedia Technique Research Center, Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, Chongqing, 400714, China},
abstract={Across-media face recognition refers to recognizing face images from different sources (e.g., face sketch, 3D face model, and low resolution image). In spite of promising processes achieved in face recognition recent years, across-media face recognition is still a challenging problem due to the difficulty of feature matching between different modalities. In this paper, we propose a latent face model that creates mappings from a hidden space to different media space. Images from different media of the same person share the same latent vector in hidden space. A coupled Joint Bayesian model is used to calculate the joint probability of two faces from different media. To verify the effectiveness of our proposed method, extensive experiments conducted on various databases: self-collected low-resolution vs. high-resolution database, sketches vs. photos databases, 3D face model vs. photos on LFW database. Experimental results show that our method boosts the performance of face recognition with images from different sources. © 2016 Elsevier B.V.},
author_keywords={Across-Media Face Recognition;  Joint Bayesian model;  Latent Face Model},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bondi20162843,
author={Bondi, E. and Pala, P. and Berretti, S. and Del Bimbo, A.},
title={Reconstructing high-resolution face models from Kinect Depth Sequences},
journal={IEEE Transactions on Information Forensics and Security},
year={2016},
volume={11},
number={12},
pages={2843-2853},
doi={10.1109/TIFS.2016.2601059},
art_number={7544592},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994850602&doi=10.1109%2fTIFS.2016.2601059&partnerID=40&md5=9135551d679310c610a7d538bea2786a},
affiliation={Department of Information Engineering, University of Florence, Florence, 50139, Italy},
abstract={Performing face recognition across 3D scans with different resolution is now attracting an increasing interest thanks to the introduction of a new generation of depth cameras, capable of acquiring color/depth images over time. In fact, these devices acquire and provide depth data with much lower resolution compared with the 3D high-resolution scanners typically used for face recognition applications. If data are acquired without user cooperation, the problem is even more challenging, and the gap of resolution between probe and gallery scans can yield to a severe loss in terms of recognition accuracy. Based on these premises, we propose a method to build a higher resolution 3D face model from 3D data acquired by a low-resolution scanner. This face model is built using data acquired when a person passes in front of the scanner, without assuming any particular cooperation. The 3D data are registered and filtered by combining a model of the expected distribution of the acquisition error with a variant of the lowess method to remove outliers and build the final face model. The proposed approach is evaluated in terms of accuracy of face reconstruction and face recognition. © 2005-2012 IEEE.},
author_keywords={face recognition;  increased resolution;  Kinect depth camera;  locally weighted regression;  manifold estimation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chun201615693,
author={Chun, J. and Kim, W.},
title={3D face pose estimation by a robust real time tracking of facial features},
journal={Multimedia Tools and Applications},
year={2016},
volume={75},
number={23},
pages={15693-15708},
doi={10.1007/s11042-014-2356-9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911898412&doi=10.1007%2fs11042-014-2356-9&partnerID=40&md5=1ecee098802b2511153fd3c281923bbb},
affiliation={Department of Computer Science, Kyonggi University, San 94-6 Yiui-Dong, Suwon, Yeongtong-Gu, South Korea},
abstract={In this paper, we present a new 3D head pose estimating approach based on real-time facial feature tracking and facial feature recovering method which copes with the surrounding light variation and various occlusion. The major facial features are obtained by Haar-like feature detection along with AdaBoost learning from an input video image. The detected facial features are robustly tracked by optical flow with a template matching scheme which continuously compensates for losing track of the initially detected features in a sequence of input images. The head pose of an input face image can be obtained by evaluating 3D information of facial features from the detected 2D eye-points, nose and lip. From the experiments, the proposed method shows effectiveness in tracking and recovering facial features and produces reliable result in head pose estimation. © 2014, Springer Science+Business Media New York.},
author_keywords={AdaBoost learning algorithm;  Haar-like feature detection;  Head pose estimation;  Optical flow;  Template matching},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shah2016,
author={Shah, S.A.A. and Bennamoun, M. and Boussaid, F.},
title={Automatic 3D face landmark localization based on 3D vector field analysis},
journal={International Conference Image and Vision Computing New Zealand},
year={2016},
volume={2016-November},
doi={10.1109/IVCNZ.2015.7761526},
art_number={7761526},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006942604&doi=10.1109%2fIVCNZ.2015.7761526&partnerID=40&md5=5da7d80624be2691571e8fafab5b9dbf},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Perth, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, 35 Stirling Highway, Perth, WA, Australia},
abstract={In applications such as 3D face synthesis and animation, a prominent face landmark is required to enable 3D face normalization, pose correction, 3D face recognition and reconstruction. Due to variations in facial expressions, automatic 3D face landmark localization remains a challenge. Nose tip is one of the salient landmarks in a human face. In this paper, a novel nose tip localization technique is proposed. In the proposed approach, the rotation of the 3D vector field is analyzed for robust and efficient nose tip localization. The proposed technique has the following three characteristics: (1) it does not require any training; (2) it does not rely on any particular model; (3) it is very efficient, requiring an average time of only 1.9s for nose tip detection. We tested the proposed technique on BU3DFE and Shrec'10 datasets. Experimental results show that the proposed technique is robust to variations in facial expressions, achieving a 100% detection rate on these publicly available 3D face datasets. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kopinski20161,
author={Kopinski, T. and Sachara, F. and Handmann, U.},
title={A deep learning approach to mid-air gesture interaction for mobile devices from time-of-flight data},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={28-November-2016},
pages={1-9},
doi={10.1145/2994374.2994392},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007597226&doi=10.1145%2f2994374.2994392&partnerID=40&md5=b29268f05ce8a5bf281af85abe703dec},
affiliation={ENSTA ParisTech, 858 Blvd des Maréchaux, Palaiseau, France; Hochschule Ruhr West, Institut Informatik, Lützowstrasse 5, Bottrop, 46236, Germany},
abstract={This contribution presents a novel approach of utilizing Timeof-Flight (ToF) technology for mid-air hand gesture recognition on mobile devices. ToF sensors are capable of providing depth data at high frame rates independent of illumination making any kind of application possible for inand outdoor situations. This comes at the cost of precision regarding depth measurements and comparatively low lateral resolution. We present a novel feature generation technique based on a rasterization of the point clouds which realizes fixed-sized input making Deep Learning approaches applicable using Convolutional Neural Networks. In order to increase precision we introduce several methods to reduce noise and normalize the input to overcome difficulties in scaling. Backed by a large-scale database of about half a million data samples taken from different individuals our contribution shows how hand gesture recognition is realizable on commodity tablets in real-time at frame rates of up to 17Hz. A leave-one out cross-validation experiment demonstrates the feasibility of our approach with classification errors as low as 1,5% achieved persons unknown to the model. © 2016 ACM.},
author_keywords={Deep learning;  Mid-air gestures;  Object recognition},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li201693,
author={Li, B.Y.L. and Xue, M. and Mian, A. and Liu, W. and Krishna, A.},
title={Robust RGB-D face recognition using Kinect sensor},
journal={Neurocomputing},
year={2016},
volume={214},
pages={93-108},
doi={10.1016/j.neucom.2016.06.012},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977478475&doi=10.1016%2fj.neucom.2016.06.012&partnerID=40&md5=df18014ab4a68d40273fdbb72a7a85ae},
affiliation={Department of Computing, Curtin University, Kent Street, Perth, WA  6102, Australia; Dalian Key Lab of Digital Technology for National Culture, Dalian Minzu University, Liaoning, Dalian  116600, China; Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA  6009, Australia},
abstract={In this paper we propose a robust face recognition algorithm for low resolution RGB-D Kinect data. Many techniques are proposed for image preprocessing due to the noisy depth data. First, facial symmetry is exploited based on the 3D point cloud to obtain a canonical frontal view image irrespective of the initial pose and then depth data is converted to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are then used to project RGB to DCS (Discriminant Color Space) and normal maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust Sparse Coding method is proposed that codes the multiple channels (DCS or DNM) of a test image as a sparse combination of training samples with different pixel weighting. Weights are calculated dynamically in an iterative process to achieve robustness against variations in pose, illumination, facial expressions and disguise. In contrast to existing techniques, our multi-channel approach is more robust to variations. Reconstruction errors of the test image (DCS and DNM) are normalized and fused to decide its identity. The proposed algorithm is evaluated on four public databases. It achieves 98.4% identification rate on CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects. Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2 databases, the proposed algorithm achieves 97.6%, 95.6% and 95.2% identification rates respectively. To the best of our knowledge, these are the highest identification rates reported so far for the first three databases. © 2016 Elsevier B.V.},
author_keywords={3D face recognition;  Kinect;  Multi-channel discriminant transform;  Sparse coding},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo2016403,
author={Guo, Y. and Lei, Y. and Liu, L. and Wang, Y. and Bennamoun, M. and Sohel, F.},
title={EI3D: Expression-invariant 3D face recognition based on feature and shape matching},
journal={Pattern Recognition Letters},
year={2016},
volume={83},
pages={403-412},
doi={10.1016/j.patrec.2016.04.003},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966659227&doi=10.1016%2fj.patrec.2016.04.003&partnerID=40&md5=b09b19b7a5436b53278c02d001e93910},
affiliation={College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan  410073, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan  610065, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan  410073, China; College of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Computer Science and Software Engineering, The University of Western Australia, Perth, WA  6009, Australia; School of Engineering and Information Technology, Murdoch University, Perth, WA  6150, Australia},
abstract={This paper presents a local feature based shape matching algorithm for expression-invariant 3D face recognition. Each 3D face is first automatically detected from a raw 3D data and normalized to achieve pose invariance. The 3D face is then represented by a set of keypoints and their associated local feature descriptors to achieve robustness to expression variations. During face recognition, a probe face is compared against each gallery face using both local feature matching and 3D point cloud registration. The number of feature matches, the average distance of matched features, and the number of closest point pairs after registration are used to measure the similarity between two 3D faces. These similarity metrics are then fused to obtain the final results. The proposed algorithm has been tested on the FRGC v2 benchmark and a high recognition performance has been achieved. It obtained the state-of-the-art results by achieving an overall rank-1 identification rate of 97.0% and an average verification rate of 99.01% at 0.001 false acceptance rate for all faces with neutral and non-neutral expressions. Further, the robustness of our algorithm under different occlusions has been demonstrated on the Bosphorus dataset. © 2016 Elsevier B.V.},
author_keywords={3D face recognition;  Face identification;  Facial expression;  Keypoint detection;  Local feature;  Shape matching},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2016977,
author={Li, B.Y.L. and Mian, A.S. and Liu, W. and Krishna, A.},
title={Face recognition based on Kinect},
journal={Pattern Analysis and Applications},
year={2016},
volume={19},
number={4},
pages={977-987},
doi={10.1007/s10044-015-0456-4},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922423384&doi=10.1007%2fs10044-015-0456-4&partnerID=40&md5=9212597635e4d3b5fc8d8f899aa81089},
affiliation={Curtin University, GPO Box U1987, Perth, WA  6845, Australia; The University of Western Australia, 35 Stirling Highway, Crawley, WA  6009, Australia},
abstract={In this paper, we present a new algorithm that utilizes low-quality red, green, blue and depth (RGB-D) data from the Kinect sensor for face recognition under challenging conditions. This algorithm extracts multiple features and fuses them at the feature level. A Finer Feature Fusion technique is developed that removes redundant information and retains only the meaningful features for possible maximum class separability. We also introduce a new 3D face database acquired with the Kinect sensor which has released to the research community. This database contains over 5,000 facial images (RGB-D) of 52 individuals under varying pose, expression, illumination and occlusions. Under the first three variations and using only the noisy depth data, the proposed algorithm can achieve 72.5 % recognition rate which is significantly higher than the 41.9 % achieved by the baseline LDA method. Combined with the texture information, 91.3 % recognition rate has achieved under illumination, pose and expression variations. These results suggest the feasibility of low-cost 3D sensors for real-time face recognition. © 2015, Springer-Verlag London.},
author_keywords={3D face images;  Face recognition;  Gabor feature;  Kinect Sensor;  LDA},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yu20162212,
author={Yu, X. and Huang, J. and Zhang, S. and Metaxas, D.N.},
title={Face landmark fitting via optimized part mixtures and cascaded deformable model},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2016},
volume={38},
number={11},
pages={2212-2226},
doi={10.1109/TPAMI.2015.2509999},
art_number={7360185},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990933998&doi=10.1109%2fTPAMI.2015.2509999&partnerID=40&md5=a94fae75e001555e00a96c1e1a1d526a},
affiliation={Department of Computer Science, Rutgers UniversityNJ  08854, United States; Department of Computer Science and Engineering, University of Texas at Arlington, United States; Department of Computer Science, University of North Carolina at Charlotte, United States},
abstract={This paper addresses the problemof facial landmark localization and tracking from a single camera.We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. In initialization stage, we propose a group sparse optimized mixture model to automatically select the most salient facial landmarks. By introducing 3D face shape model, we apply procrustes analysis to provide pose-aware landmark initialization. In landmark localization stage, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework simultaneously handles face detection, pose-robust landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental databases and face-in-the-wild databases. The results reveal that our approach consistently outperforms state-of-the-art methods for face alignment and tracking. © 2015 IEEE.},
author_keywords={Deformable shape model;  Face landmark localization;  Face tracking;  Part based model},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bagga20162037,
author={Bagga, M. and Singh, B.},
title={Spoofing detection in face recognition: A review},
journal={Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016},
year={2016},
pages={2037-2042},
art_number={7724624},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997254313&partnerID=40&md5=4cf499800311a32991b0dd7ef48ece4c},
affiliation={Visvesvaraya Technological University, Belagavi, India; Punjab Technical University, BBSB, Engineering College, Fatehgarh Sahib, India},
abstract={In the recent days, the facial biometric system is widely used for the mobile payments and other surveillance systems. Its popularity is going to be increased because of its easiness to use and also it is user friendly. But the main problem in this system is its vulnerability to the spoof attacks made by 2D or 3D face masks or printed photographs. In order to guard against face spoofing, the anti-spoofing methods have been developed to do liveliness detection. In this paper, the different type of face spoofing attacks and the different techniques used for anti-spoofing are analyzed. © 2016 IEEE.},
author_keywords={2D and 3D attacks;  Biometrics;  Presentation attacks;  Recognition systems Spoofing attacks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sopiak201658,
author={Sopiak, D. and Oravec, M. and Pavlovicova, J. and Bukovcikova, Z. and Dittingerova, M. and Bilanska, A. and Novotna, M. and Gontkovic, J.},
title={Generating face images based on 3D morphable model},
journal={2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery, ICNC-FSKD 2016},
year={2016},
pages={58-62},
doi={10.1109/FSKD.2016.7603151},
art_number={7603151},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997751445&doi=10.1109%2fFSKD.2016.7603151&partnerID=40&md5=e4fa6607b3f3ed50fe86a0345c7ad36d},
affiliation={Faculty of Electrical Engineering and Information Technology, Slovak University of Technology, Bratislava, Bratislava, Slovakia},
abstract={In modern days the demand for biometrics increases rapidly. The world still needs to solve many problems and answer to lot of questions regarding to biometrics for creating better solutions for recognition and verification of objects. Biometrics has become really important topic of our security. Number of input samples per person affects recognition in modern algorithms used for face recognition. We can say that single-sample problem is one of the most challenging problems in face recognition. Most of face recognition algorithms requires at least two samples per person but it is very important to create a system where only one sample per person would achieve a good performance. This topic is especially related to passport and id photos because they include only one picture of person. In this paper, we explain process of reconstruction and generating 3D face model created from id or passport photo. We also present our results that shows influence of generated new samples on face recognition. © 2016 IEEE.},
author_keywords={biometrics;  face recognition;  morphable model},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lei2016994,
author={Lei, Y. and Feng, S. and Zhou, X. and Guo, Y.},
title={An efficient 3D partial face recognition approach with single sample},
journal={Proceedings of the 2016 IEEE 11th Conference on Industrial Electronics and Applications, ICIEA 2016},
year={2016},
pages={994-999},
doi={10.1109/ICIEA.2016.7603727},
art_number={7603727},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997335714&doi=10.1109%2fICIEA.2016.7603727&partnerID=40&md5=05e7f7f4b11d4caaaace67a1698a150c},
affiliation={College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan, 610065, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, 410073, China},
abstract={3D partial face recognition under missing parts, occlusions and data corruptions is a major challenge for the practical application of the techniques of 3D face recognition. Moreover, one individual can only provide one sample for training in most practical scenarios, and thus the face recognition with single sample problem is another highly challenging task. We propose an efficient framework for 3D partial face recognition with single sample addressing both of the two problems. First, we represent a facial scan with a set of keypoint based local geometrical descriptors, which gains sufficient robustness to partial facial data along with expression/pose variations. Then, a two-step modified collaborative representation classification scheme is proposed to address the single sample recognition problem. A class-based probability estimation is given during the first classification step, and the obtained result is then incorporated into the modified collaborative representation classification as a locality constraint to improve its classification performance. Extensive experiments on the Bosphorus and FRGC v2.0 datasets demonstrate the efficiency of the proposed approach when addressing the problem of 3D partial face recognition with single sample. © 2016 IEEE.},
author_keywords={3D facial representation;  3D partial face recognition;  collaborative representation;  locality constraint;  single sample problem},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yadav2016347,
author={Yadav, P.C. and Singh, H.V. and Patel, A.K. and Singh, A.},
title={A comparative analysis of different facial action tracking models and techniques},
journal={International Conference on Emerging Trends in Electrical, Electronics and Sustainable Energy Systems, ICETEESES 2016},
year={2016},
pages={347-349},
doi={10.1109/ICETEESES.2016.7581407},
art_number={7581407},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995514106&doi=10.1109%2fICETEESES.2016.7581407&partnerID=40&md5=f97e0fb29a875999fff7b6de4d7f5d72},
affiliation={Department of Electronics and Communication, KNIT Sultanpur, Sultanpur, Uttar Pradesh, India; Department of Electronics and Communication, University of Allahabad, Allahabad, Uttar Pradesh, India},
abstract={The tracking of facial activities from video is an important and challenging problem. Now a day, many computer vision techniques have been proposed to characterize the facial activities in the three levels (from local to global). First level is the bottom level, in which the facial feature tracking focuses on detecting and tracking of the prominent local landmarks surrounding facial components (e.g. mouth, eyebrow, etc), in second level the facial action units (AUs) characterize the specific behaviors of these local facial components (e.g. mouth open, eyebrow raiser, etc) and the third level is facial expression level, which represents subjects emotions (e.g. Surprise, Happy, Anger, etc.) and controls the global muscular movement of the whole face. Most of the existing methods focus on one or two levels of facial activities, and track (or recognize) them separately. In this paper, various facial action tracking models and techniques are compared in different conditions such as the performance of Active Facial Tracking for Fatigue Detection, Real Time 3D Face Pose Tracking from an Uncalibrated Camera, Simultaneous facial action tracking and expression recognition using a particle filter and Simultaneous Tracking and Facial Expression Recognition using Multiperson and Multiclass Autoregressive Models. © 2016 IEEE.},
author_keywords={Face and Facial Features;  Facial Expression;  Simultaneous Tracking and Recognition;  Stochastic Tracking and Fatigue Detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ma20161223,
author={Ma, M. and Peng, S. and Hu, X.},
title={A lighting robust fitting approach of 3D morphable model for face reconstruction},
journal={Visual Computer},
year={2016},
volume={32},
number={10},
pages={1223-1238},
doi={10.1007/s00371-015-1158-z},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944600705&doi=10.1007%2fs00371-015-1158-z&partnerID=40&md5=d62a2473f067e15d40c45609f8547cb9},
affiliation={Insititute of Automation, Chinese Academy of Sciences (CAS), 95 Zhongguancun East Road, Beijing, 100190, China},
abstract={Three-dimensional morphable model (3DMM) is a powerful tool for recovering 3D shape and texture from a single facial image. The success of 3DMM relies on two things: an effective optimization strategy and a realistic approach to synthesizing face images. However, most previous methods have focused on developing an optimization strategy under Phong’s synthesis approach. In this paper, we adopt a more realistic synthesis technique that fully considers illumination and reflectance in the 3DMM fitting process. Using the sphere harmonic illumination model (SHIM), our new synthesis approach can account for more lighting factors than Phong’s model. Spatially varying specular reflectance is also introduced into the synthesis process. Under SHIM, the cost function is nearly linear for all parameters, which simplifies the optimization. We apply our new optimization algorithm to determine the shape and texture parameters simultaneously. The accuracy of the recovered shape and texture can be improved significantly by considering the spatially varying specular reflectance. Hence, our algorithm produces an enhanced shape and texture compared with previous SHIM-based methods that recover shape from feature points. Although we use just a single input image in a profile pose, our approach gives plausible results. Experiments on a well-known image database show that, compared to state-of-the-art methods based on Phong’s model, the proposed approach enhances the robustness of the 3DMM fitting results under extreme lighting and profile pose. © 2015, Springer-Verlag Berlin Heidelberg.},
author_keywords={3D face;  Morphable model;  Spatially varying specular reflectance;  Sphere harmonic illumination},
document_type={Article},
source={Scopus},
}

@ARTICLE{Issa2016590,
author={Issa, H. and Issa, S. and Issa, M.},
title={New prototype of hybrid 3d-biometric facial recognition system},
journal={International Arab Journal of Information Technology},
year={2016},
volume={13},
number={5},
pages={590-594},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990062054&partnerID=40&md5=ef1d91a6c14e146bfea820fae7449e40},
affiliation={Department of Communications and Electronics Engineering, Isra University, Jordan; Department of Electronics and Information Engineering, Huazhong University of Science and Technology, China},
abstract={In the last decades, a lot of 3D face recognition techniques have been proposed. They can be divided into three parts, holistic matching techniques, feature-based techniques and hybrid techniques. In this paper, a hybrid technique is used, where, a prototype of a new hybrid face recognition technique depends on 3D face scan images are designed, simulated and implemented. Some geometric rules are used for analyzing and mapping the face. Image processing is used to get the twodimensional values of predetermined and specific facial points, software programming is used to perform a three-dimensional coordinates of the predetermined points and to calculate several geometric parameter ratios and relations. Neural network technique is used for processing the calculated geometric parameters and then performing facial recognition. The new design is not affected by variant pose, illumination and expression and has high accurate level compared with the 2D analysis. Moreover, the proposed algorithm is of higher performance than latest’s published biometric recognition algorithms in terms of cost, confidentiality of results, and availability of design tools. © 2016, Zarka Private Univ. All rights reserved.},
author_keywords={Face recognition;  Image processing;  Photo modeler software;  Probabilistic neural network},
document_type={Article},
source={Scopus},
}

@ARTICLE{Alashkar201621,
author={Alashkar, T. and Ben Amor, B. and Daoudi, M. and Berretti, S.},
title={A Grassmann framework for 4D facial shape analysis},
journal={Pattern Recognition},
year={2016},
volume={57},
pages={21-30},
doi={10.1016/j.patcog.2016.03.013},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961875504&doi=10.1016%2fj.patcog.2016.03.013&partnerID=40&md5=391b945aff70e66ce2d7ec3e6a746fac},
affiliation={Institute MinesTelecom/Telecom Lille, CRIStAL (UMR CNRS 9189), France; Department of Information Engineering, University of Florence, Italy},
abstract={In this paper, we investigate the contribution of dynamic evolution of 3D faces to identity recognition. To this end, we adopt a subspace representation of the flow of curvature-maps computed on 3D facial frames of a sequence, after normalizing their pose. Such representation allows us to embody the shape as well as its temporal evolution within the same subspace representation. Dictionary learning and sparse coding over the space of fixed-dimensional subspaces, called Grassmann manifold, have been used to perform face recognition. We have conducted extensive experiments on the BU-4DFE dataset. The obtained results of the proposed approach provide promising results. © 2016 Elsevier Ltd.},
author_keywords={4D face recognition;  Curvature-maps;  Dictionary learning;  Grassmann manifold;  Sparse coding},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bagchi201611059,
author={Bagchi, P. and Bhattacharjee, D. and Nasipuri, M.},
title={A robust analysis, detection and recognition of facial features in 2.5D images},
journal={Multimedia Tools and Applications},
year={2016},
volume={75},
number={18},
pages={11059-11096},
doi={10.1007/s11042-015-2835-7},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944707282&doi=10.1007%2fs11042-015-2835-7&partnerID=40&md5=a38a10b519d08b5fd08bad36e64e4543},
affiliation={Department of Computer Science and Engineering, RCC Institute of Information Technology, Beliaghata, Kolkata, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, India},
abstract={A robust technique for recognition of 3D faces which performs well with face images with various poses, expressions and occlusions. In this method, the face images represented in 3D mesh format are smoothed using trilinear interpolation and then converted to 2.5D image or range images. Nose-tip which is the most prominent feature on human face is detected first on the corner points selected by 3D Harris corner and curvedness at those corner points. K-Means clustering is applied to group those corner points in 2 groups. The cluster of points with larger curvedness values represents the possible locations of nose-tip. Nose-tip is finally localized using Mean-Gaussian curvature values of the prospective corner points in that cluster. Using the nose-tip location, other facial landmarks namely corners of the eyes and mouth are located and a facial graph is generated. The dimensionality of 2.5D feature space is that, depth values are stored at each (x, y) grid of the 2.5D image, so a 3D face image uses some function to map the depth value at any pixel position to the intensity with which that pixel will be displayed. Here finally extracted features for each subject is of dimensionality [1 × 21], taking into account the Euclidean distances in three dimensional form between each feature points detected automatically. Taking Euclidean distances between all pairs of landmark points as features, face images are classified using Multilayer Perceptron (MLP), as well as Support Vector Machines (SVM). Maximum recognition rates of 75 and 87.5 % have been obtained in case of Bosphorus Databases, 62.5 and 87.5 % in case of GavabDB databases, 75 and 87.5 % in case of Frav3D Databases by Multilayer Perceptron and Support Vector Machines respectively. © 2015, Springer Science+Business Media New York.},
author_keywords={2.5D image;  3D Harris corner detector;  Facial landmarks;  Range image},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ma20161983,
author={Ma, Z. and Fang, L. and Duan, J. and Xie, S. and Wang, Z.},
title={Personal identification based on finger vein and contour point clouds matching},
journal={2016 IEEE International Conference on Mechatronics and Automation, IEEE ICMA 2016},
year={2016},
pages={1983-1988},
doi={10.1109/ICMA.2016.7558870},
art_number={7558870},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991209152&doi=10.1109%2fICMA.2016.7558870&partnerID=40&md5=dca93b520578a8a2df7e476855495845},
affiliation={School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu, 611731, China; College of Management Science, Chengdu University of Technology, Chengdu, 610059, China},
abstract={Finger vein recognition has attracted increasing attention in biometric identification field. To solve the problems of lack of depth information in recognition based on 2D image and no enough obvious features used for recognition based on 3D point cloud, a novel approach of identifying individuals using 3D point clouds matching of finger vein and contour is proposed in this paper. A pair of vein images of a finger are captured under Near Infrared(NIR) light using our binocular vision device. The edge and vein contours of finger are extracted as features used for describe finger vein and then 3D point cloud of finger vein and contour is reconstructed though binocular vision technique. At last, personal identification based on matching results between template and reconstructed 3D point cloud using Iterative Closest Point(ICP) algorithm can be achieved. The experimental results show that proposed method can generate more discriminative features to represent 3D model of finger vein and contour so that the accuracy of matching is enhanced. © 2016 IEEE.},
author_keywords={3D reconstruction;  binocular vision;  finger vein;  near infrared(NIR);  points clouds matching},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Amolik2016237,
author={Amolik, A. and Ahamad, S.T. and Dey, S. and Manjula, R.},
title={3D face view generation from human drawn sketch: A review},
journal={2016 International Conference on Computation of Power, Energy, Information and Communication, ICCPEIC 2016},
year={2016},
pages={237-244},
doi={10.1109/ICCPEIC.2016.7557202},
art_number={7557202},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992630889&doi=10.1109%2fICCPEIC.2016.7557202&partnerID=40&md5=c016fb86fbf25d5b5b58421f8e3a778a},
affiliation={CSE, School of Computer Science and Engineering, VIT University, Vellore, Tamilnadu  632014, India},
abstract={From last few decades, generating a 3D face model from an human drawn sketch has caught the interest of many researchers in the area of image processing and face recognition. It has various applications in 3D cartoon modelling, police investigation and verification, and in Image Processing. Many techniques are there to generate 3D models from a sketch. 3D landmark estimation, 2D landmark detection, and synthesis of texture and surface with respect to 3-D morphable model are the steps, respectively, to generate the 3D face model. 3D face modelling using these steps has a higher rate of accuracy of identification of a person from her sketch and no proper photograph. In this piece of literature, we present a review on efficient technique that can be used to generate 3D face from sketch drawn by human. © 2016 IEEE.},
author_keywords={3D Modelling;  Active Shape Model;  Appearance Model;  Face Recognition;  MeshIK;  PCA(Principal Component Analysis)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2016,
author={Zhang, J. and Huang, D. and Wang, Y. and Sun, J.},
title={Lock3DFace: A large-scale database of low-cost Kinect 3D faces},
journal={2016 International Conference on Biometrics, ICB 2016},
year={2016},
doi={10.1109/ICB.2016.7550062},
art_number={7550062},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988432113&doi=10.1109%2fICB.2016.7550062&partnerID=40&md5=b25394bb9ffda22d92409c2cd0a74cfa},
affiliation={IRIP Lab, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China},
abstract={In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kim20163011,
author={Kim, D. and Choi, J. and Leksut, J.T. and Medioni, G.},
title={Accurate 3D face modeling and recognition from RGB-D stream in the presence of large pose changes},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2016},
volume={2016-August},
pages={3011-3015},
doi={10.1109/ICIP.2016.7532912},
art_number={7532912},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006789474&doi=10.1109%2fICIP.2016.7532912&partnerID=40&md5=132fb651cb89c0d50239c14c3a7104ae},
affiliation={Institute for Robotics and Intelligent Systems, University of Southern California, 3737 Watt way PHE 101, Los Angeles, CA  90089, United States},
abstract={We propose a 3D face modeling and recognition system using an RGB-D stream in the presence of large pose changes. In the previous work, all facial data points are registered with a reference to improve the accuracy of 3D face model from a low-resolution depth sequence. This registration often fails when applied to non-frontal faces. It causes inaccurate 3D face models and poor performance of matching. We address this problem by pre-aligning each input face ('frontalization') before the registration, which avoids registration failures. For each frame, our method estimates the 3D face pose, assesses the quality of data, segments the facial region, frontalizes it, and performs an accurate registration with the previous 3D model. The 3D-3D recognition system using accurate 3D models from our method outperforms other face recognition systems and shows 100% rank 1 recognition accuracy on a dataset with 30 subjects. © 2016 IEEE.},
author_keywords={3D Face Modeling;  3D Face Recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Peng20163932,
author={Peng, B. and Wang, W. and Dong, J. and Tan, T.},
title={Automatic detection of 3D lighting inconsistencies via a facial landmark based morphable model},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2016},
volume={2016-August},
pages={3932-3936},
doi={10.1109/ICIP.2016.7533097},
art_number={7533097},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006789881&doi=10.1109%2fICIP.2016.7533097&partnerID=40&md5=5744189c78634c921ade04eba46814c4},
affiliation={National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China},
abstract={Existing 3D lighting consistency based forensic methods have some practical problems. They usually require additional images and human labor to reconstruct the 3D face model for lighting estimation, and furthermore, they cannot deal with expressional faces effectively. These drawbacks make them unusable in many practical cases. In this paper, we propose a more practical 3D lighting based forensic method by incorporating a facial landmark based 3D morphable model to efficiently fit the face shape. We also introduce a residual error based algorithm to automatically exclude outliers in lighting estimation. Our proposed method is fully automatic and very efficient compared to previous ones. Also, it does not depend on additional images and has better performance for expressional faces. Experiments on a realistic face dataset with variational lighting conditions indicate the efficacy and superiority of our method. © 2016 IEEE.},
author_keywords={3D morphable model;  Face splicing;  Image forensics;  Lighting consistency},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu20163016,
author={Yu, X. and Gao, Y. and Zhou, J.},
title={3D face recognition under partial occlusions using radial strings},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2016},
volume={2016-August},
pages={3016-3020},
doi={10.1109/ICIP.2016.7532913},
art_number={7532913},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006713653&doi=10.1109%2fICIP.2016.7532913&partnerID=40&md5=dd468caaf8cc8593dad5e05e0c040b9c},
affiliation={School of Engineering, Griffith University, Nathan, QLD, Australia; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia},
abstract={3D face recognition with partial occlusions is a highly challenging problem. In this paper, we propose a novel radial string representation and matching approach to recognize 3D facial scans in the presence of partial occlusions. Here we encode 3D facial surfaces into an indexed collection of radial strings emanating from the nosetips and Dynamic Programming (DP) is then used to measure the similarity between two radial strings. In order to address the recognition problems with partial occlusions, a partial matching mechanism is established in our approach that effectively eliminates those occluded parts and finds the most discriminative parts during the matching process. Experimental results on the Bosphorus database demonstrate that the proposed approach yields superior performance on partially occluded data. © 2016 IEEE.},
author_keywords={3D face recognition;  Partial occlusions;  Radial string matching;  Structural recognition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nowak2016389,
author={Nowak, P.S. and Sankowski, W. and Krotewicz, P.},
title={3D face and hand scans acquisition system dedicated for Multimodal Biometric identification},
journal={Proceedings of the 23rd International Conference Mixed Design of Integrated Circuits and Systems, MIXDES 2016},
year={2016},
pages={389-393},
doi={10.1109/MIXDES.2016.7529772},
art_number={7529772},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992109556&doi=10.1109%2fMIXDES.2016.7529772&partnerID=40&md5=1e34ae62f025425d3f5bcf30c8b9a4ff},
affiliation={Department of Microelectronics and Computer Science, Lodz University of Technology, Lodz, Poland},
abstract={The aim of this paper is to present the data acquisition system built for the Multimodal Biometric System for Contactless Persons Identification. The system is capable of capturing 3D scans of face and hand. The hardware and software architecture of the acquisition system designed by the authors is described. The authors describe in detail the system calibration algorithm and present sample results. © 2016 Department of Microelectronics and Computer Science, Lodz University of Technology.},
author_keywords={3D scanner;  contactless identification;  multimodal biometrics;  structured light},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Samad2016522,
author={Samad, M.D. and Iftekharuddin, K.M.},
title={Frenet Frame-Based Generalized Space Curve Representation for Pose-Invariant Classification and Recognition of 3-D Face},
journal={IEEE Transactions on Human-Machine Systems},
year={2016},
volume={46},
number={4},
pages={522-533},
doi={10.1109/THMS.2016.2515602},
art_number={7412724},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959179169&doi=10.1109%2fTHMS.2016.2515602&partnerID=40&md5=fcd44fdaf17f8c95c76c56adebbdac00},
affiliation={Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA  23529, United States},
abstract={The state-of-the-art methods in classifying 3-D representation of the face involve challenges in extracting representative features directly from the large volume of facial data. These methods mostly ignore the effect of pose distortions on 3-D facial data and entail heavy computations as well as manual processing steps. This work proposes a novel Frenet frame-based generalized space curve representation method for 3-D pose-invariant face and facial expression recognition and classification. Three-dimensional facial curves are extracted from either frontal or synthetically posed 3-D facial data to derive the proposed Frenet frame-based features. A mathematical framework shows the proof of pose invariance property for the features. The effectiveness of the proposed method is evaluated in two recognition tasks: 3-D face recognition (3D-FR) and 3-D facial expression recognition (3D-FER) using benchmarked 3-D datasets. The proposed framework yields 96% rank-I recognition rate for 3D-FR and 91.4% area under ROC curves for six basic 3D-FER. The performance evaluation also shows that the proposed mathematical framework yields pose-invariant 3D-FR and 3D-FER for a wide range of pose angles. This pose invariance property of the Frenet frame-based features alleviates the need for an expensive 3-D face registration in the preprocessing step, which, in turn, enables a faster processing time. The evaluation results further suggest that the proposed method is not only computationally efficient and versatile, but also offers competitive performance when compared with the existing state-of-the-art methods reported for either 3D-FR or 3D-FER. © 2013 IEEE.},
author_keywords={3-D face classification;  Facial curves;  facial expression;  Frenet frame;  pose-invariant recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dong2016,
author={Dong, Y. and Wang, Y. and Yue, J. and Hu, Z.},
title={Real time 3D facial movement tracking using a monocular camera},
journal={Sensors (Switzerland)},
year={2016},
volume={16},
number={8},
doi={10.3390/s16081157},
art_number={1157},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979556449&doi=10.3390%2fs16081157&partnerID=40&md5=bf4e57a3924fb246feab613ef0a62047},
affiliation={School of Electronics and Information Engineering, Tongji University, Caoan Road 4800, Shanghai, 201804, China; Department of Electrical and Electronics, Kumamoto University, 2-39-1 Kurokami, Kumamoto shi, 8608555, Japan},
abstract={The paper proposes a robust framework for 3D facial movement tracking in real time using a monocular camera. It is designed to estimate the 3D face pose and local facial animation such as eyelid movement and mouth movement. The framework firstly utilizes the Discriminative Shape Regression method to locate the facial feature points on the 2D image and fuses the 2D data with a 3D face model using Extended Kalman Filter to yield 3D facial movement information. An alternating optimizing strategy is adopted to fit to different persons automatically. Experiments show that the proposed framework could track the 3D facial movement across various poses and illumination conditions. Given the real face scale the framework could track the eyelid with an error of 1 mm and mouth with an error of 2 mm. The tracking result is reliable for expression analysis or mental state inference. © 2016 by the authors; licensee MDPI, Basel, Switzerland.},
author_keywords={3D facial movement;  Eyelid;  Facial animation;  Facial feature points;  HCI},
document_type={Article},
source={Scopus},
}

@ARTICLE{Quan2016765,
author={Quan, W. and Matuszewski, B.J. and Shark, L.-K.},
title={Statistical shape modelling for expression-invariant face analysis and recognition},
journal={Pattern Analysis and Applications},
year={2016},
volume={19},
number={3},
pages={765-781},
doi={10.1007/s10044-014-0439-x},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922378470&doi=10.1007%2fs10044-014-0439-x&partnerID=40&md5=ecfc90e7e705f0e96425da2cb3863f4a},
affiliation={Robotics and Computer Vision Research Laboratory, ADSIP Research Centre, School of Computing, Engineering and Physical Sciences, University of Central Lancashire, CM124, C&T Building, Preston, Lancashire  PR1 2HE, United Kingdom; Robotics and Computer Vision Research Laboratory, ADSIP Research Centre, School of Computing, Engineering and Physical Sciences, University of Central Lancashire, CM149, C&T Building, Preston, Lancashire  PR1 2HE, United Kingdom; Robotics and Computer Vision Research Laboratory, ADSIP Research Centre, School of Computing, Engineering and Physical Sciences, University of Central Lancashire, CM147, C&T Building, Preston, Lancashire  PR1 2HE, United Kingdom},
abstract={Paper introduces a 3-D shape representation scheme for automatic face analysis and identification, and demonstrates its invariance to facial expression. The core of this scheme lies on the combination of statistical shape modelling and non-rigid deformation matching. While the former matches 3-D faces with facial expression, the latter provides a low-dimensional feature vector that controls the deformation of model for matching the shape of new input, thereby enabling robust identification of 3-D faces. The proposed scheme is also able to handle the pose variation without large part of missing data. To assist the establishment of dense point correspondences, a modified free-form-deformation based on B-spline warping is applied with the help of extracted landmarks. The hybrid iterative closest point method is introduced for matching the models and new data. The feasibility and effectiveness of the proposed method was investigated using standard publicly available Gavab and BU-3DFE datasets, which contain faces with expression and pose changes. The performance of the system was compared with that of nine benchmark approaches. The experimental results demonstrate that the proposed scheme provides a competitive solution for face recognition. © 2015, Springer-Verlag London.},
author_keywords={3-D face analysis and recognition;  Expression-invariant representation;  Shape matching and registration;  Statistical shape modelling},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Torkhani2016447,
author={Torkhani, G. and Ladgham, A. and Mansouri, M.N. and Sakly, A.},
title={Gabor-SVM applied to 3D-2D deformed mesh model},
journal={2nd International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2016},
year={2016},
pages={447-452},
doi={10.1109/ATSIP.2016.7523133},
art_number={7523133},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984607903&doi=10.1109%2fATSIP.2016.7523133&partnerID=40&md5=a94aa03e640ea81c789823dfdf4c35ed},
affiliation={CSR Research Unit, E e Laboratory, National School of Engineering, Ibn Eljazzar, Monastir, 5019, Tunisia; Electrical Departement, National School of Engineering, Ibn Eljazzar, Monastir, 5019, Tunisia},
abstract={We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate. © 2016 IEEE.},
author_keywords={3D face recognition;  deformed mesh model;  facial curvatures;  Gabor wavelet;  salient points;  SVM},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nozawa2016,
author={Nozawa, T. and Kato, T. and Savkin, P.A. and Nozawa, N. and Morishima, S.},
title={3D facial geometry reconstruction using patch database},
journal={SIGGRAPH 2016 - ACM SIGGRAPH 2016 Posters},
year={2016},
doi={10.1145/2945078.2945102},
art_number={a24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985995760&doi=10.1145%2f2945078.2945102&partnerID=40&md5=0e4b9210567d92e9fb986739b171cb44},
affiliation={Waseda University, Japan; Waseda Research Institute for Science and Engineering, Japan},
abstract={3D facial shape reconstruction in the wild environments is an important research task in the field of CG and CV. This is because it can be applied to a lot of products, such as 3DCG video games and face recognition. One of the most popular 3D facial shape reconstruction techniques is 3D Model-based approach. This approach approximates a facial shape by using 3D face model, which is calculated by principal component analysis. [Blanz and Vetter 1999] performed a 3D facial reconstruction by fitting points from facial feature points of an input of single facial image to vertex of template 3D facial model named 3D Morphable Model. This method can reconstruct a facial shape from a variety of images which include different lighting and face orientation, as long as facial feature points can be detected. However, representation quality of the result depends on the number of 3D model resolution.},
author_keywords={3D reconstruction;  Shape from X;  Texture synthesis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Azevedo20161,
author={Azevedo, P. and Dos Santos, T.O. and De Aguiar, E.},
title={An Augmented Reality Virtual Glasses Try-On System},
journal={Proceedings - 18th Symposium on Virtual and Augmented Reality, SVR 2016},
year={2016},
pages={1-9},
doi={10.1109/SVR.2016.12},
art_number={7517246},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986212349&doi=10.1109%2fSVR.2016.12&partnerID=40&md5=085b7be3ede076d7c43e14776ddf27f5},
affiliation={Departamento de Informãtica, UFES, Vitoria, Brazil; Departamento de Computacao e Eletronica, UFES, Sao Mateus, Brazil},
abstract={This paper presents a virtual try-on system to correctly visualize 3D objects (e.g., glasses) in the face of a given user. By capturing the image and depth information of a user through a low-cost RGB-D camera, we apply a face tracking technique to detect specific landmarks in the facial image. These landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3D facial morphable model that fits as good as possible to the user's head and face. At the end, we deform the chosen 3D objects from its rest shape to a deformed shape matching the specific facial shape of the user. The last step projects and renders the 3D object into the original image, with enhanced precision and in proper scale, showing the selected object in the user's face. We validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. Our results demonstrate that, by fitting a facial model to the user's face, the rendered virtual 3D objects look more realistic. © 2016 IEEE.},
author_keywords={Augmented reality;  Computer graphics;  Virtual reality},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zribi2016280,
author={Zribi, S. and Khadhraoui, T. and Benzarti, F. and Amiri, H.},
title={Automatic 3D face preprocessing},
journal={2016 IEEE/ACIS 14th International Conference on Software Engineering Research, Management and Applications, SERA 2016},
year={2016},
pages={280-284},
doi={10.1109/SERA.2016.7516157},
art_number={7516157},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983458659&doi=10.1109%2fSERA.2016.7516157&partnerID=40&md5=cbfca243b234f5dbf8c91979999d53bf},
affiliation={SITI Laboratory, National School of Engineers of Tunis (ENIT), Tunisia},
abstract={An efficient and above all cheap solutions, biometrics provide extensive information in access control applications. 3D mode provides great new opportunities in this sector in recent years. Firstly the paper discusses the formal work done in this domain discussing approach based on curvature calculation, alignment of surfaces, feature selection and facial curve including different techniques for 3D facial recognition data. The second part is about the steps required for the preprocessing phase of the 3D face data. Various experiments conducted and results obtained. © 2016 IEEE.},
author_keywords={3D face;  Delaunay triangulation;  Facial mask;  Median Filter;  Preprocessing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wu2016,
author={Wu, C. and Bradley, D. and Gross, M. and Beeler, T.},
title={An anatomically-constrained local deformation model for monocular face capture},
journal={ACM Transactions on Graphics},
year={2016},
volume={35},
number={4},
doi={10.1145/2897824.2925882},
art_number={a115},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979998440&doi=10.1145%2f2897824.2925882&partnerID=40&md5=6464d6134dfa09e64e44e29ed6b85152},
affiliation={Disney Research, United States; ETH Zurich, Switzerland},
abstract={We present a new anatomically-constrained local face model and fitting approach for tracking 3D faces from 2D motion data in very high quality. In contrast to traditional global face models, often built from a large set of blendshapes, we propose a local deformation model composed of many small subspaces spatially distributed over the face. Our local model offers far more flexibility and expressiveness than global blendshape models, even with a much smaller model size. This flexibility would typically come at the cost of reduced robustness, in particular during the under-constrained task of monocular reconstruction. However, a key contribution of this work is that we consider the face anatomy and introduce subspace skin thickness constraints into our model, which constrain the face to only valid expressions and helps counteract depth ambiguities in monocular tracking. Given our new model, we present a novel fitting optimization that allows 3D facial performance reconstruction from a single view at extremely high quality, far beyond previous fitting approaches. Our model is flexible, and can be applied also when only sparse motion data is available, for example with marker-based motion capture or even face posing from artistic sketches. Furthermore, by incorporating anatomical constraints we can automatically estimate the rigid motion of the skull, obtaining a rigid stabilization of the performance for free. We demonstrate our model and single-view fitting method on a number of examples, including, for the first time, extreme local skin deformation caused by external forces such as wind, captured from a single high-speed camera. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={Anatomical constraints;  Facial performance capture;  Local face model;  Monocular face tracking},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Andreu20163,
author={Andreu, Y. and Chiarugi, F. and Colantonio, S. and Giannakakis, G. and Giorgi, D. and Henriquez, P. and Kazantzaki, E. and Manousos, D. and Marias, K. and Matuszewski, B.J. and Pascali, M.A. and Pediaditis, M. and Raccichini, G. and Tsiknakis, M.},
title={Wize Mirror-a smart, multisensory cardio-metabolic risk monitoring system},
journal={Computer Vision and Image Understanding},
year={2016},
volume={148},
pages={3-22},
doi={10.1016/j.cviu.2016.03.018},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963813154&doi=10.1016%2fj.cviu.2016.03.018&partnerID=40&md5=f5ad3b569781bf18f9c8b19b9fc87e1e},
affiliation={Robotics and Computer Vision Research Laboratory, School of Computing Engineering and Physical Sciences, University of Central Lancashire, Preston, PR1 2HE, United Kingdom; Institute of Information Science and Technologies, National Research Council of Italy, Via G. Moruzzi 1, Pisa, 56124, Italy; Institute of Computer Science, Foundation for Research and Technology-Hellas (FORTH), N. Plastira 100, Vassilika Vouton, Heraklion, Crete, GR-700 13, Greece; Technological Educational Institute of Crete, Biomedical Informatics and EHealth Laboratory, Estavromenos, Heraklion, Crete, GR-71004, Greece},
abstract={In the recent years personal health monitoring systems have been gaining popularity, both as a result of the pull from the general population, keen to improve well-being and early detection of possibly serious health conditions and the push from the industry eager to translate the current significant progress in computer vision and machine learning into commercial products. One of such systems is the Wize Mirror, built as a result of the FP7 funded SEMEOTICONS (SEMEiotic Oriented Technology for Individuals CardiOmetabolic risk self-assessmeNt and Self-monitoring) project. The project aims to translate the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, multispectral images, and 3D scans of the face. The multisensory platform, being developed as the result of that project, in the form of a smart mirror, looks for signs related to cardio-metabolic risks. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. This paper is focused on the description of the part of that system, utilising computer vision and machine learning techniques to perform 3D morphological analysis of the face and recognition of psycho-somatic status both linked with cardio-metabolic risks. The paper describes the concepts, methods and the developed implementations as well as reports on the results obtained on both real and synthetic datasets. © 2016 The Authors. Published by Elsevier Inc.},
author_keywords={3D face detection;  3D morphometric analysis;  Multimodal data integration;  Psycho-somatic status recognition;  Tracking and reconstruction;  Unobtrusive health monitoring},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhen20161438,
author={Zhen, Q. and Huang, D. and Wang, Y. and Chen, L.},
title={Muscular Movement Model-Based Automatic 3D/4D Facial Expression Recognition},
journal={IEEE Transactions on Multimedia},
year={2016},
volume={18},
number={7},
pages={1438-1450},
doi={10.1109/TMM.2016.2557063},
art_number={7457243},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976531968&doi=10.1109%2fTMM.2016.2557063&partnerID=40&md5=a7d523dd13d895a284b1edd3bb64fd8c},
affiliation={Laboratory of Intelligent Recognition and Image Processing, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; LIRIS Laboratory, MI Department, Ecole Centrale de Lyon, Lyon, 69134, France},
abstract={Facial expression is an important channel for human nonverbal communication. This paper presents a novel and effective approach to automatic 3D/4D facial expression recognition based on the muscular movement model (MMM). In contrast to most of existing methods, the MMM deals with such an issue in the viewpoint of anatomy. It first automatically segments the input 3D face (frame) by localizing the corresponding points within each muscular region of the reference using iterative closest normal point. A set of features with multiple differential quantities, including {coordinate}, {normal,} and {shape\,index} values, are then extracted to describe the geometry deformation of each segmented region. Meanwhile, we analyze the importance of these muscular areas, and a score level fusion strategy is exploited to optimize their weights by the genetic algorithm in the learning step. The support vector machine and the hidden Markov model are finally used to predict the expression label in 3D and 4D, respectively. The experiments are conducted on the BU-3DFE and BU-4DFE databases, and the results achieved clearly demonstrate the effectiveness of the proposed method. © 1999-2012 IEEE.},
author_keywords={3D/4D facial expression recognition;  Muscle Movement Model;  shape representation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rai2016,
author={Rai, M.C.E. and Tortorici, C. and Al-Muhairi, H. and Safar, H.A. and Werghi, N.},
title={Landmarks detection on 3D face scans using local histogram descriptors},
journal={Proceedings of the 18th Mediterranean Electrotechnical Conference: Intelligent and Efficient Technologies and Services for the Citizen, MELECON 2016},
year={2016},
doi={10.1109/MELCON.2016.7495382},
art_number={7495382},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979224078&doi=10.1109%2fMELCON.2016.7495382&partnerID=40&md5=df67cb2bb374b10d64d71b697366ccaf},
affiliation={Electrical and Computer Engineering Departement, Khalifa University, United Arab Emirates},
abstract={In this work, we exploit 3D Constrained Local Model (CLM) for facial landmark detection. Our approach integrates the geometric information of 3D face scans. The fast increase demand of 3D data invite to develop 3D image processing methods for many applications and especially for automatic landmark detection. The new step in this paper is the introduction of mesh histogram of gradients (meshHOG) as local descriptors around every landmark location. The proposed work is evaluated on the publicly available Bosphorus database. A comparison with the other descriptors mesh LBP and mesh SIFT are also depicted. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhou2016717,
author={Zhou, J. and Tong, X. and Liu, Z. and Guo, B.},
title={3D cartoon face generation by local deformation mapping},
journal={Visual Computer},
year={2016},
volume={32},
number={6-8},
pages={717-727},
doi={10.1007/s00371-016-1265-5},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973649600&doi=10.1007%2fs00371-016-1265-5&partnerID=40&md5=a058f1631d67e4b37788167e5935116c},
affiliation={Tsinghua University, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Redmond, United States},
abstract={We present a data-driven method for automatically generating a 3D cartoon of a real 3D face. Given a sparse set of 3D real faces and their corresponding cartoon faces modeled by an artist, our method models the face in each subspace as the deformation of its nearby exemplars and learn a mapping between the deformations defined by the real faces and their cartoon counterparts. To reduce the exemplars needed for learning, we regress a collection of linear mappings defined locally in both face geometry and identity spaces and develop a progressive scheme for users to gradually add new exemplars for training. At runtime, our method first finds the nearby exemplars of an input real face and then constructs the result cartoon face from the corresponding cartoon faces of the nearby real face exemplars and the local deformations mapped from the real face subspace. Our method greatly simplifies the cartoon generation process by learning artistic styles from a sparse set of exemplars. We validate the efficiency and effectiveness of our method by applying it to faces of different facial features. Results demonstrate that our method not only preserves the artistic style of the exemplars, but also keeps the unique facial geometric features of different identities. © 2016, Springer-Verlag Berlin Heidelberg.},
author_keywords={3D cartoon generation;  Data-driven method;  Local deformation mapping},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Casas2016121,
author={Casas, D. and Feng, A. and Alexander, O. and Fyffe, G. and Debevec, P. and Ichikari, R. and Li, H. and Olszewski, K. and Suma, E. and Shapiro, A.},
title={Rapid photorealistic blendshape modeling from RGB-D sensors},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={23-25-May-2016},
pages={121-129},
doi={10.1145/2915926.2915936},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986267324&doi=10.1145%2f2915926.2915936&partnerID=40&md5=4e00f5f15196523125f31937c990585a},
affiliation={Institute for Creative Technologies, University of Southern California, United States; University of Southern California, United States},
abstract={Creating and animating realistic 3D human faces is an important element of virtual reality, video games, and other areas that involve interactive 3D graphics. In this paper, we propose a system to generate photorealistic 3D blendshape-based face models automatically using only a single consumer RGB-D sensor. The capture and processing requires no artistic expertise to operate, takes 15 seconds to capture and generate a single facial expression, and approximately 1 minute of processing time per expression to transform it into a blendshape model. Our main contributions include a complete end-To-end pipeline for capturing and generating photorealistic blendshape models automatically and a registration method that solves dense correspondences between two face scans by utilizing facial landmarks detection and optical flows. We demonstrate the effectiveness of the proposed method by capturing different human subjects with a variety of sensors and puppeteering their 3D faces with real-Time facial performance retargeting. The rapid nature of our method allows for just-in-Time construction of a digital face. To that end, we also integrated our pipeline with a virtual reality facial performance capture system that allows dynamic embodiment of the generated faces despite partial occlusion of the user's real face by the head-mounted display. © 2016 ACM.},
author_keywords={Animation;  Blendshapes;  Face modeling;  RGB-D},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Werghi2016964,
author={Werghi, N. and Tortorici, C. and Berretti, S. and Del Bimbo, A.},
title={Boosting 3D LBP-Based Face Recognition by Fusing Shape and Texture Descriptors on the Mesh},
journal={IEEE Transactions on Information Forensics and Security},
year={2016},
volume={11},
number={5},
pages={964-979},
doi={10.1109/TIFS.2016.2515505},
art_number={7373633},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963974403&doi=10.1109%2fTIFS.2016.2515505&partnerID=40&md5=376f43714248cb873d28dcb663ac1bb5},
affiliation={Electrical and Computer Engineering Department, Khalifa University, Abu Dhabi, 127788, United Arab Emirates; Department of Information Engineering, University of Florence, Florence, 50139, Italy},
abstract={In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBPs) on a mesh for 3D face recognition. Using a recently proposed framework, we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared with its depth-image counterpart, our approach is distinguished by the following features: 1) inherits the intrinsic advantages of mesh surface (e.g., preservation of the full geometry); 2) does not require normalization; and 3) can accommodate partial matching. In addition, it allows early level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data, also in comparison to the state-of-the-art solutions. © 2016 IEEE.},
author_keywords={3D face recognition;  feature and score fusion;  mesh-LBP},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ganguly2016275,
author={Ganguly, S. and Bhattachaijee, D. and Nasipuri, M.},
title={3D face recognition from complement component range face images},
journal={2015 IEEE International Conference on Computer Graphics, Vision and Information Security, CGVIS 2015},
year={2016},
pages={275-278},
doi={10.1109/CGVIS.2015.7449936},
art_number={7449936},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966632732&doi=10.1109%2fCGVIS.2015.7449936&partnerID=40&md5=ac5698f1d98fa8b29941ffc837f84a7b},
affiliation={Department of Computer Science and Engineering, Jadavpur University, Kolkata-32, India},
abstract={Face and facial attributes represent meaningful definition about a variety of information to discriminate an individual from others and for developing a computational model for automatic face recognition purpose. However, in this work, selection of relevant features from newly created face space is the pivotal contribution of the authors. Here, authors have demonstrated a new face space 'Complement Component' that have been used to extract the four basic components along X, and Y axes in four directions. Later, authors have experimented the discriminative attributes from these face spaces for recognition purpose. Here, comparison of the proposed method has been reported by examining its success on two well accepted 3D face databases, namely: Frav3D and Texas3D. In case of 2D face images, it does not contain depth like information i.e. Z-values in X-Y plane through intensity values. Therefore, it has not been undertaken during this investigation. © 2015 IEEE.},
author_keywords={3D face image;  Complement Component;  Face recognition;  range face image},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Marvadi2016,
author={Marvadi, D. and Paunwala, C. and Joshi, M. and Vora, A.},
title={Comparative analysis of 3D face recognition using 2D-PCA and 2D-LDA approaches},
journal={NUiCONE 2015 - 5th Nirma University International Conference on Engineering},
year={2016},
doi={10.1109/NUICONE.2015.7449603},
art_number={7449603},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966327409&doi=10.1109%2fNUICONE.2015.7449603&partnerID=40&md5=b5b3315314aa606ed6ef4b3af1a2625f},
affiliation={Electronics and Communication Dept., Chhotubhai Gopalbhai Patel Institute of Technology, Surat, India},
abstract={Even if, most of 2D face recognition approaches reached recognition rate more than 90% in controlled environment, current days face recognition systems degrade their performance in case of uncontrolled environment which includes pose variations, illumination variations, expression variations and ageing effect etc. Inclusion of 3D face analysis gives an age over 2D face recognition as they give vital informations such as 3D shape, texture and depth which improve discrimination power of an algorithm. In this paper, we have investigated different 3D face recognition approaches that are robust to changes in facial expressions and illumination variations. 2D-PCA and 2D-LDA approaches have been extended to 3D face recognition because they can directly work on 2D depth image matrices rather than 1D vectors without need for transformations before feature extraction. In turn, this reduces storage space and time required for computations. 2D depth image is extracted from 3D face model and nose region from depth mapped image has been detected as a reference point for cropping stage to convert model into a standard size. Two Dimensional Principal Component Analysis (2D-PCA) and Two Dimensional Linear Discriminant analysis (2D-LDA) are employed to obtain feature vectors globally compared to feature vectors obtained locally using PCA or LDA. Finally, euclidean distance classifier is applied for comparison of extracted features. A set of experiments on GavabDB 3D face database, which has 61 individuals in total, demonstrated that 3D face recognition using 2D-LDA method has achieved recognition accuracy of 93.3% and EER of 8.96% over database, which is higher compared to 2D-PCA. So, more optimized performance has been achieved using 2D-LDA for 3D face recognition analysis. © 2015 IEEE.},
author_keywords={2D-LDA;  2D-PCA;  EER;  Eigen-Surface;  Fisher-Surface;  LDA;  PCA},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liang2016890,
author={Liang, H. and Liang, R. and Song, M. and He, X.},
title={Coupled Dictionary Learning for the Detail-Enhanced Synthesis of 3-D Facial Expressions},
journal={IEEE Transactions on Cybernetics},
year={2016},
volume={46},
number={4},
pages={890-901},
doi={10.1109/TCYB.2015.2417211},
art_number={7083738},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927545850&doi=10.1109%2fTCYB.2015.2417211&partnerID=40&md5=700f60cf8278019546412d3ac70d7a70},
affiliation={Zhejiang University of Technology, Hangzhou, 310013, China; Microsoft Key Laboratory of Visual Perception, Zhejiang University, Hangzhou, 310058, China; State Key Laboratory of CAD and CG, Zhejiang University, Hangzhou, 310058, China},
abstract={The desire to reconstruct 3-D face models with expressions from 2-D face images fosters increasing interest in addressing the problem of face modeling. This task is important and challenging in the field of computer animation. Facial contours and wrinkles are essential to generate a face with a certain expression; however, these details are generally ignored or are not seriously considered in previous studies on face model reconstruction. Thus, we employ coupled radius basis function networks to derive an intermediate 3-D face model from a single 2-D face image. To optimize the 3-D face model further through landmarks, a coupled dictionary that is related to 3-D face models and their corresponding 3-D landmarks is learned from the given training set through local coordinate coding. Another coupled dictionary is then constructed to bridge the 2-D and 3-D landmarks for the transfer of vertices on the face model. As a result, the final 3-D face can be generated with the appropriate expression. In the testing phase, the 2-D input faces are converted into 3-D models that display different expressions. Experimental results indicate that the proposed approach to facial expression synthesis can obtain model details more effectively than previous methods can. © 2015 IEEE.},
author_keywords={Coupled dictionary;  expression synthesis;  landmark;  local coordinate coding (LCC)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lei2016218,
author={Lei, Y. and Guo, Y. and Hayat, M. and Bennamoun, M. and Zhou, X.},
title={A Two-Phase Weighted Collaborative Representation for 3D partial face recognition with single sample},
journal={Pattern Recognition},
year={2016},
volume={52},
pages={218-237},
doi={10.1016/j.patcog.2015.09.035},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947306090&doi=10.1016%2fj.patcog.2015.09.035&partnerID=40&md5=50ca73115b78f6746739042c1f293a0e},
affiliation={College of Electronics and Information Engineering, Sichuan University, Chengdu, Sichuan, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China; School of Computer Science and Software Engineering, University of Western Australia, Crawley, WA, Australia; IBM Research Australia, Carlton, VIC, Australia},
abstract={3D face recognition with the availability of only partial data (missing parts, occlusions and data corruptions) and single training sample is a highly challenging task. This paper presents an efficient 3D face recognition approach to address this challenge. We represent a facial scan with a set of local Keypoint-based Multiple Triangle Statistics (KMTS), which is robust to partial facial data, large facial expressions and pose variations. To address the single sample problem, we then propose a Two-Phase Weighted Collaborative Representation Classification (TPWCRC) framework. A class-based probability estimation is first calculated based on the extracted local descriptors as a prior knowledge. The resulting class-based probability estimation is then incorporated into the proposed classification framework as a locality constraint to further enhance its discriminating power. Experimental results on six challenging 3D facial datasets show that the proposed KMTS-TPWCRC framework achieves promising results for human face recognition with missing parts, occlusions, data corruptions, expressions and pose variations. © 2015 Elsevier Ltd. All rights reserved.},
author_keywords={3D face recognition;  3D representation;  Partial facial data;  Single sample problem;  Sparse representation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Danelakis2016174,
author={Danelakis, A. and Theoharis, T. and Pratikakis, I. and Perakis, P.},
title={An effective methodology for dynamic 3D facial expression retrieval},
journal={Pattern Recognition},
year={2016},
volume={52},
pages={174-185},
doi={10.1016/j.patcog.2015.10.012},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951748817&doi=10.1016%2fj.patcog.2015.10.012&partnerID=40&md5=a1fbf61adbf9e429129a352f95d656f9},
affiliation={Department of Informatics and Telecommunications, University of Athens, Greece; Department of Computer and Information Science, Norwegian University of Science and Technology, Norway; Department of Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece},
abstract={The problem of facial expression recognition in dynamic sequences of 3D face scans has received a significant amount of attention in the recent past whereas the problem of retrieval in this type of data has not. A novel retrieval methodology for such data is introduced in this paper. The proposed methodology automatically detects specific facial landmarks and uses them to create a descriptor. This descriptor is the concatenation of three sub-descriptors which capture topological as well as geometric information of the 3D face scans. The motivation behind the proposed hybrid facial expression descriptor is the fact that some facial expressions, like happiness and surprise, are characterized by obvious changes in the mouth topology while others, like anger, fear and sadness, produce geometric but no significant topological changes. The proposed retrieval scheme exploits the Dynamic Time Warping technique in order to compare descriptors corresponding to different 3D facial sequences. A detailed evaluation of the introduced retrieval scheme is presented showing that it outperforms previous state-of-the-art retrieval schemes. Experiments have been conducted using the six prototypical expressions of the standard dataset BU-4DFE and the eight prototypical expressions of the recently available dataset BP4D-Spontaneous. Finally, a majority voting scheme based on the retrieval results is used to achieve unsupervised dynamic 3D facial expression recognition. The achieved classification accuracy is comparable to the state-of-the-art supervised dynamic 3D facial expression recognition techniques. © 2015 Elsevier Ltd. All rights reserved.},
author_keywords={3D object retrieval;  4D facial expression recognition;  4D facial expression retrieval;  Dynamic 3D mesh sequence},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dornaika2016168,
author={Dornaika, F. and Chahla, C. and Khattar, F. and Abdallah, F. and Snoussi, H.},
title={Discriminant sparse label-sensitive embedding: Application to image-based face pose estimation},
journal={Engineering Applications of Artificial Intelligence},
year={2016},
volume={50},
pages={168-176},
doi={10.1016/j.engappai.2016.01.035},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960107217&doi=10.1016%2fj.engappai.2016.01.035&partnerID=40&md5=3b5b8d5cca4574a28d96bc8108d27a73},
affiliation={University of the Basque Country UPV/EHU, Manuel Lardizabal, 1, San-Sebastian, 20018, Spain; IKERBASQUE, Basque Foundation for Science, Bilbao, Spain; Lebanese University, Lebanon; University of Technology of Troyes, Troyes, France; University of Pau and Pays de l'Adour, Bayonne, France},
abstract={In this letter, the authors propose a new embedding scheme for image-based continuous face pose estimation. The main contributions are as follows. First, it is shown that the concept of label-sensitive Locality Preserving Projections, proposed for age estimation, can be used for model-less face pose estimation. Second, the authors propose a linear embedding by exploiting the connections between facial features and pose labels via a sparse coding scheme. The resulting technique is called Sparse Label sensitive Locality Preserving Projections (Sp-LsLPP). Third, for enhancing the discrimination between poses, the projections obtained by Sp-LsLPP are fed to a Discriminant Embedding that exploits the continuous labels. The resulting framework has less parameters compared to related works. It has been applied to the problem of model-less face yaw angle estimation (person independent 3D face pose estimation). It was tested on three databases: FacePix, Taiwan, and Columbia. It was conveniently compared with other linear and non-linear techniques. The experimental results confirm that the proposed framework can outperform, in general, the existing ones. © 2016 Elsevier Ltd. All rights reserved.},
author_keywords={Locality preserving projections;  Manifold learning;  Model-less face pose estimation;  Out-of-plan rotation;  Sparse coding},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Limonov201681,
author={Limonov, A. and Jeong, J.-Y. and Kim, M. and Kim, S. and Kim, Y.},
title={Human face 3D reconstruction with handheld single 2D camera on mobile devices},
journal={2016 IEEE International Conference on Consumer Electronics, ICCE 2016},
year={2016},
pages={81-82},
doi={10.1109/ICCE.2016.7430529},
art_number={7430529},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965140030&doi=10.1109%2fICCE.2016.7430529&partnerID=40&md5=3bd10b77483635c21b944f6c557c3526},
affiliation={Samsung Electronics Co. Ltd., Suwon, South Korea},
abstract={In this paper, we propose a live system for 3D reconstruction of human face using only single 2D camera without any 3D sensor. Lack of feature points and homogeneous skin color leads to low quality and success rate in conventional 3D reconstruction algorithms when applied to human face. Moreover, it requires difficult user interaction. To solve this problem, we adopt the facial shape and appearance model to our 3D reconstruction pipeline. This is the first approach which creates a complete 3D face model that can be directly used for 3D printing or virtual reality applications. Model can be acquired using modern smartphone in less than 40 seconds. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lagha2016,
author={Lagha, I. and Othman, A.},
title={Human face texture generation based on MPEG-4 standard},
journal={2015 5th International Conference on Information and Communication Technology and Accessibility, ICTA 2015},
year={2016},
doi={10.1109/ICTA.2015.7426891},
art_number={7426891},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988310674&doi=10.1109%2fICTA.2015.7426891&partnerID=40&md5=1ed60d9429b2b8f8342f9bc015895aff},
affiliation={Research Laboratory on Technologies of Information and Communication and Electrical Engineering, University of Tunis, Tunisia},
abstract={This paper is about a new method of generation texture for 3D face model. The 3D model is used as a signing avatar to help deaf people to interact easily. This approach is based on the MPEG-4 standard for facial detection points from two orthogonal photos representing the front and side view of a human head. The two photos are cut, combined and finally deformed corresponding to an UV-map to finally generate a texture which can be mapped on the 3D head. © 2015 IEEE.},
author_keywords={Human face;  MPEG-4;  Parametrization;  Texture mapping},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2016276,
author={Liu, J. and Zhang, Q. and Tang, C.},
title={Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information},
journal={Proceedings of 2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2015},
year={2016},
pages={276-284},
doi={10.1109/IAEAC.2015.7428562},
art_number={7428562},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966359479&doi=10.1109%2fIAEAC.2015.7428562&partnerID=40&md5=d07bdc62c3beb1c60304198eb193ba5c},
affiliation={College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China},
abstract={3D facial landmarks play a significant role in many 3D facial studies. Due to the complex geometry of high resolution 3D human face, landmark detection remains to be a challenge problem. This paper proposes a novel method to detect landmarks automatically for high resolution 3D faces without learning or training, solely using geometric information. For high resolution 3D faces, geodesic remeshing is used to reduce the vertices number, then the remeshed 3D faces are parameterized and interpolated on a regular grid, which enable us to compute the differential geometric features more efficiently and accurately. To detect landmarks, we extract global and local constraints for each landmark by considering relative positions of landmarks and differential geometric features, then landmarks are detected by combine these constraints. We evaluate our method and compare it with other methods which are mostly related to ours in a large scale publicly available 3D face data set, BJUT-3D face database. The experimental results show that our method is robust and effective, and achieves better performance than existing methods. © 2015 IEEE.},
author_keywords={3D faces;  differential geometric features;  geodesic remeshing;  geometric information;  landmarks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2016309,
author={Liu, J. and Zhang, Q. and Tang, C.},
title={CoMES: A novel method for robust nose tip detection in face range images},
journal={Proceedings of 2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2015},
year={2016},
pages={309-315},
doi={10.1109/IAEAC.2015.7428566},
art_number={7428566},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966378274&doi=10.1109%2fIAEAC.2015.7428566&partnerID=40&md5=fa04fdce67df4cdba620b187f71d84a7},
affiliation={College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China},
abstract={As the most distinct feature point in facial landmarks, nose tip plays a significant role in 3D facial studies. Successful detection of nose tip can facilitate many 3D facial studies tasks. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, need not training, can handle large rotations and occlusions. We first remove small isolated connected regions and noise from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, we compute multi-angle energy of each point, then we use hierarchical clustering method to cluster the points whose multi-angle energies are larger than a threshold value. In the largest cluster, we can find one point with the largest multi-angle energy. For all scales of the scale-space, we get a series of such points and apply hierarchical clustering again for these points, nose tip will have the largest multi-angle energy in the largest cluster. We evaluate our method in FRGC v2.0 3D face database and BOSPHORUS 3D face database. The experimental results verify the robustness of our method with a high nose tip detection rate. © 2015 IEEE.},
author_keywords={3D faces;  hierarchical clustering;  multi-angle energy;  nose tip;  range images;  scale-space},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Echeagaray-Patron2016843,
author={Echeagaray-Patron, B.A. and Miramontes-Jaramillo, D. and Kober, V.},
title={Conformal parameterization and curvature analysis for 3D facial recognition},
journal={Proceedings - 2015 International Conference on Computational Science and Computational Intelligence, CSCI 2015},
year={2016},
pages={843-844},
doi={10.1109/CSCI.2015.133},
art_number={7424213},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964412211&doi=10.1109%2fCSCI.2015.133&partnerID=40&md5=6a3fc9e7dac95a6f25aacc425aa40af5},
affiliation={Department of Computer Science, CICESE, Ensenada, B.C., 22860, Mexico; Department of Mathematics, Chelyabinsk State University, Russian Federation},
abstract={This work proposes a new algorithm for 3D face recognition. The algorithm uses 3D shape data without color or texture information and exploits local curvature information which is a measure with high discriminant capability and robust to deformations such as rotation and scaling. In order to reduce high dimensionality of typical face surfaces our approach uses a conformal parameterization, preserving angles of original faces and simplifies the correspondence problem. Experimental results are presented and discussed using CASIA and Gavab databases. © 2015 IEEE.},
author_keywords={3D face recognition;  Conformal parameterization;  Curvature analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wenhao2016333,
author={Wenhao, Z. and Smith, M.L. and Smith, L.N. and Farooq, A.},
title={Gender recognition from facial images: Two or three dimensions?},
journal={Journal of the Optical Society of America A: Optics and Image Science, and Vision},
year={2016},
volume={33},
number={3},
pages={333-344},
doi={10.1364/JOSAA.33.000333},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962138813&doi=10.1364%2fJOSAA.33.000333&partnerID=40&md5=3a8ae19c72c975c3cb011d23ca12d2f4},
affiliation={Center for Machine Vision, Bristol Robotics Laboratory, University of the West of England, T Block, Frenchay Campus Coldharbour Lane, Bristol, BS16 1QY, United Kingdom},
abstract={This paper seeks to compare encoded features from both two-dimensional (2D) and three-dimensional (3D) face images in order to achieve automatic gender recognition with high accuracy and robustness. The Fisher vector encoding method is employed to produce 2D, 3D, and fused features with escalated discriminative power. For 3D face analysis, a two-source photometric stereo (PS) method is introduced that enables 3D surface reconstructions with accurate details as well as desirable efficiency. Moreover, a 2D + 3D imaging device, taking the two-source PS method as its core, has been developed that can simultaneously gather color images for 2D evaluations and PS images for 3D analysis. This system inherits the superior reconstruction accuracy from the standard (three or more light) PS method but simplifies the reconstruction algorithm as well as the hardware design by only requiring two light sources. It also offers great potential for facilitating human computer interaction by being accurate, cheap, efficient, and nonintrusive. Ten types of low-level 2D and 3D features have been experimented with and encoded for Fisher vector gender recognition. Evaluations of the Fisher vector encoding method have been performed on the FERET database, Color FERET database, LFW database, and FRGCv2 database, yielding 97.7%, 98.0%, 92.5%, and 96.7% accuracy, respectively. In addition, the comparison of 2D and 3D features has been drawn from a self-collected dataset, which is constructed with the aid of the 2D + 3D imaging device in a series of data capture experiments. With a variety of experiments and evaluations, it can be proved that the Fisher vector encoding method outperforms most state-of-the-art gender recognition methods. It has also been observed that 3D features reconstructed by the two-source PS method are able to further boost the Fisher vector gender recognition performance, i.e., up to a 6% increase on the self-collected database. © 2016 Optical Society of America.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang201682,
author={Wang, J. and Xie, Q. and Xu, Y. and Zhou, L. and Ye, N.},
title={Cluttered indoor scene modeling via functional part-guided graph matching},
journal={Computer Aided Geometric Design},
year={2016},
volume={43},
pages={82-94},
doi={10.1016/j.cagd.2016.02.012},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960173025&doi=10.1016%2fj.cagd.2016.02.012&partnerID=40&md5=6a810baf82b6089cce5792c5905809c7},
affiliation={College of Mechanical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China},
abstract={We propose an automatic method for fast reconstruction of indoor scenes from raw point scans, which is a fairly challenging problem due to the restricted accessibility and the cluttered space for indoor environment. We first detect and remove points representing the ground, walls and ceiling from the input data and cluster the remaining points into different groups, referred to as sub-scenes. Our approach abstracts the sub-scenes with geometric primitives, and accordingly constructs the topology graphs with structural attributes based on the functional parts of objects (namely, anchors). To decompose sub-scenes into individual indoor objects, we devise an anchor-guided subgraph matching algorithm which leverages template graphs to partition the graphs into subgraphs (i.e., individual objects), which is capable of handling arbitrarily oriented objects within scenes. Subsequently, we present a data-driven approach to model individual objects, which is particularly formulated as a model instance recognition problem. A Randomized Decision Forest (RDF) is introduced to achieve robust recognition on decomposed indoor objects with raw point data. We further exploit template fitting to generate the geometrically faithful model to the input indoor scene. We visually and quantitatively evaluate the performance of our framework on a variety of synthetic and raw scans, which comprehensively demonstrates the efficiency and robustness of our reconstruction method on raw scanned point clouds, even in the presence of noise and heavy occlusions. © 2016 Elsevier B.V. All rights reserved.},
author_keywords={Cluttered indoor scenes;  Graph matching;  Raw point clouds;  Scene modeling},
document_type={Article},
source={Scopus},
}

@ARTICLE{Peng2016228,
author={Peng, W. and Xu, C. and Feng, Z.},
title={3D face modeling based on structure optimization and surface reconstruction with B-Spline},
journal={Neurocomputing},
year={2016},
volume={179},
pages={228-237},
doi={10.1016/j.neucom.2015.11.090},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955737027&doi=10.1016%2fj.neucom.2015.11.090&partnerID=40&md5=38e317e2fb1a0b0b79f8d01db6ccddec},
affiliation={School of Computer Science and Technology, Tianjin University, China; School of Computer Software, Tianjin University, China},
abstract={How to reconstruct 3D face model from wild photos is such a difficult issue that camera calibration is necessary and the images must be from video sequences. In this paper, a face reconstruction model with structure optimization is proposed to build 3D face surface with individual geometry and physical features reservation through wild face images directly and without camera calibration. Low rank and B-Spline are employed to estimate the aligned 2D structure, to calculate the depth information with SSIM, and to reconstruct the 3D face surface from control points and their space transformation. Furthermore, LFW and Bosphorus datasets, as well as Young-to-Aged samples, are introduced to verify the proposed approach and the experimental results demonstrate the feasibility and effectiveness even with different poses, expressions and age-variety. © 2015 Elsevier B.V.},
author_keywords={3D face reconstruction;  B-spline face;  Low rank;  SSIM;  Structure optimization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Goodarzi2016482,
author={Goodarzi, F. and Saripan, M.I.},
title={Real time face pose estimation using geometrical features},
journal={IEEE 2015 International Conference on Signal and Image Processing Applications, ICSIPA 2015 - Proceedings},
year={2016},
pages={482-487},
doi={10.1109/ICSIPA.2015.7412239},
art_number={7412239},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971673740&doi=10.1109%2fICSIPA.2015.7412239&partnerID=40&md5=73dad8ff5c977adc334d5cf02bcb4097},
affiliation={Department of Computer and Communications Eng., Faculty engineering, University Putra Malaysia, Serdang, Selangor Darul Ehsan, 43400 UPM, Malaysia},
abstract={3D Head pose estimation using a hybrid pose estimation method is discussed in this paper. The only equipment used is simple webcam that is available on most computers and laptops today. The hybrid method consists of tracking facial landmarks of face and using geometrical face pose estimation to compute distances to estimate 3D position of head. The pose estimation system works real time as it is ultimately will be used for 2D-3D face recognition system. Actual data show reasonable error for rotation along each axis (Yaw, Pitch and Roll) by using only few facial landmarks. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Danelakis2016257,
author={Danelakis, A. and Theoharis, T. and Pratikakis, I.},
title={A robust spatio-temporal scheme for dynamic 3D facial expression retrieval},
journal={Visual Computer},
year={2016},
volume={32},
number={2},
pages={257-269},
doi={10.1007/s00371-015-1142-7},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958843444&doi=10.1007%2fs00371-015-1142-7&partnerID=40&md5=f05e9730e895ef2993b6c66459b03112},
affiliation={Department of Informatics and Telecommunications, University of Athens, Athens, Greece; Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway; Department of Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, 67100, Greece},
abstract={The problem of facial expression recognition in dynamic sequences of 3D face scans has received a significant amount of attention in the recent past whereas the problem of retrieval in this type of data has not. A novel retrieval scheme for such data is introduced in this paper. It is the first spatio-temporal retrieval scheme ever used for retrieval in dynamic sequences of 3D face scans. The proposed scheme automatically detects specific facial landmarks and uses them to create a spatio-temporal descriptor. At first, geometric as well as topological information of the 3D face scans is captured by using the detected landmarks. In the sequel, the aforementioned spatial information is filtered by using wavelet transformation, resulting to our final spatio-temporal descriptor. Our descriptor is invariant to the number of the 3D face scans of a facial expression sequence. The proposed retrieval scheme exploits the Square of Euclidean distance in order to compare descriptors corresponding to different 3D facial sequences. A detailed evaluation of the introduced retrieval scheme is presented showing that it outperforms previous state-of-the-art retrieval schemes. Experiments have been conducted using the six prototypical expressions of the standard data set (Formula presented.). Finally, a majority voting methodology based on the retrieval results is used to achieve unsupervised dynamic 3D facial expression recognition. The achieved classification accuracy outperforms the state-of-the-art supervised dynamic 3D facial expression recognition techniques. © 2015, Springer-Verlag Berlin Heidelberg.},
author_keywords={3D Object retrieval;  Dynamic 3D mesh sequence;  Facial expressions;  Wavelet transformation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Moeini20161,
author={Moeini, A. and Faez, K. and Sadeghi, H. and Moeini, H.},
title={2D facial expression recognition via 3D reconstruction and feature fusion},
journal={Journal of Visual Communication and Image Representation},
year={2016},
volume={35},
pages={1-14},
doi={10.1016/j.jvcir.2015.11.006},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950162274&doi=10.1016%2fj.jvcir.2015.11.006&partnerID=40&md5=cc175747fc6e3ed4c7eae6a795bad1cb},
affiliation={Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran; Electrical Engineering Department, Semnan University, Semnan, Iran},
abstract={In this paper, a novel feature extraction method is proposed for facial expression recognition by extracting the feature from facial depth and 3D mesh alongside texture. Accordingly, the 3D Facial Expression Generic Elastic Model (3D FE-GEM) method is used to reconstruct an expression-invariant 3D model from the human face. Then, the texture, depth and mesh are extracted from the reconstructed face model. Afterwards, the Local Binary Pattern (LBP), proposed 3D High-Low Local Binary Pattern (3DH-LLBP) and Local Normal Binary Patterns (LNBPs) are applied to texture, depth and mesh of the face, respectively, to extract the feature from 2D images. Finally, the final feature vectors are generated through feature fusion and are classified by the Support Vector Machine (SVM). Convincing results are acquired for facial expression recognition on the CK+, CK, JAFFE and Bosphorus image databases compared to several state-of-the-art methods. © 2015 Elsevier Inc. All rights reserved.},
author_keywords={2D and 3D features;  3D local binary pattern;  Expression-invariant 3D face reconstruction;  Facial expression generic elastic model;  Facial expression recognition;  Feature fusion;  Local binary pattern;  Local normal binary pattern},
document_type={Article},
source={Scopus},
}

@ARTICLE{DeJong2016580,
author={De Jong, M.A. and Wollstein, A. and Ruff, C. and Dunaway, D. and Hysi, P. and Spector, T. and Liu, F. and Niessen, W. and Koudstaal, M.J. and Kayser, M. and Wolvius, E.B. and Böhringer, S.},
title={An automatic 3D facial landmarking algorithm using 2D gabor wavelets},
journal={IEEE Transactions on Image Processing},
year={2016},
volume={25},
number={2},
pages={580-588},
doi={10.1109/TIP.2015.2496183},
art_number={7312454},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009350723&doi=10.1109%2fTIP.2015.2496183&partnerID=40&md5=80c1d9b98cbf2089870b3220fa0bb632},
affiliation={Department of Oral and Maxillofacial Surgery Special Dental Care, and Orthodontics, Erasmus MC, University Medical Center Rotterdam, CE Rotterdam, 3015, Netherlands; Department of Medical Statistics and Bioinformatics, Leiden University Medical Center, ZC Leiden, 2333, Netherlands; Department of Forensic Molecular Biology, Erasmus MC, University Medical Center Rotterdam, CN Rotterdam, 3015, Netherlands; Section of Evolutionary Biology, Department of Biology II, University of Munich LMU, Martinsried, 82152, Germany; Department of Medical Physics, University College London Hospital, London, NW1 2BU, United Kingdom; Department of Twin Research and Genetic Epidemiology, Kings College London, London, SE1 7EH, United Kingdom; Department of Medical Informatics, Erasmus MC, University Medical Center Rotterdam, Rotterdam, 3015 CE, Netherlands; Faculty of Applied Sciences, Delft University of Technology, Delft, 2628 CJ, Netherlands; Craniofacial Unit, Great Ormond Street Hospital for Sick Children, London, WC1N 3JH, United Kingdom},
abstract={In this paper, we present a novel approach to automatic 3D facial landmarking using 2D Gabor wavelets. Our algorithm considers the face to be a surface and uses map projections to derive 2D features from raw data. Extracted features include texture, relief map, and transformations thereof. We extend an established 2D landmarking method for simultaneous evaluation of these data. The method is validated by performing landmarking experiments on two data sets using 21 landmarks and compared with an active shape model implementation. On average, landmarking error for our method was 1.9 mm, whereas the active shape model resulted in an average landmarking error of 2.3 mm. A second study investigating facial shape heritability in related individuals concludes that automatic landmarking is on par with manual landmarking for some landmarks. Our algorithm can be trained in 30 min to automatically landmark 3D facial data sets of any size, and allows for fast and robust landmarking of 3D faces. © 2015 IEEE.},
author_keywords={3D;  Algorithm;  Automatic landmarking;  Face;  Gabor filter;  Landmarking;  Surface data;  Wavelet},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ma2016679,
author={Ma, L. and Zheng, G. and Eitel, J.U.H. and Moskal, L.M. and He, W. and Huang, H.},
title={Improved Salient Feature-Based Approach for Automatically Separating Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar Point Cloud Data of Forest Canopies},
journal={IEEE Transactions on Geoscience and Remote Sensing},
year={2016},
volume={54},
number={2},
pages={679-696},
doi={10.1109/TGRS.2015.2459716},
art_number={7265032},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942023893&doi=10.1109%2fTGRS.2015.2459716&partnerID=40&md5=4e4b0a0fb0ce5cf0511c43bd0efc8427},
affiliation={International Institute for Earth System Science, Nanjing University, Nanjing, 210023, China; Reveley Geospatial Laboratory for Environmental Dynamics, College of Natural Resources, University of Idaho, Moscow, ID  83844, United States; Remote Sensing and Geospatial Analysis Laboratory, Precision Forestry Cooperative, School of Environment and Forest Science, University of Washington, Seattle, WA  98195, United States; State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, 100101, China},
abstract={Accurate separation of photosynthetic and nonphotosynthetic components in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a challenging but of key importance to understand the spatial distribution of the radiation regime, photosynthetic processes, and carbon and water exchanges of the forest canopy. The objective of this paper was to improve current methods for separating photosynthetic and nonphotosynthetic components in TLS data of forest canopies by adding two additional filters only based on its geometric information. By comparing the proposed approach with the eigenvalues plus color information-based method, we found that the proposed approach could effectively improve the overall producer's accuracy from 62.12% to 95.45%, and the overall classification producer's accuracy would increase from 84.28% to 97.80% as the forest leaf area index (LAI) decreases from 4.15 to 3.13. In addition, variations in tree species had negligible effects on the final classification accuracy, as shown by the overall producer's accuracy for coniferous (93.09%) and broadleaf (94.96%) trees. To remove quantitatively the effects of the woody materials in a forest canopy for improving TLS-based LAI estimates, we also computed the "woody-to-total area ratio" based on the classified linear class points from an individual tree. Automatic classification of the forest point cloud data set will facilitate the application of TLS on retrieving 3-D forest canopy structural parameters, including LAI and leaf and woody area ratios. © 2015 IEEE.},
author_keywords={Light detection and ranging (lidar);  pattern recognition;  point classification;  terrestrial laser scanning (TLS);  woody-to-total area ratio},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu2016,
author={Liu, J. and Zhang, Q. and Zhang, C. and Tang, C.},
title={Robust nose tip detection for face range images based on local features in scale-space},
journal={2015 International Conference on 3D Imaging, IC3D 2015 - Proceedings},
year={2016},
doi={10.1109/IC3D.2015.7391814},
art_number={7391814},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963541984&doi=10.1109%2fIC3D.2015.7391814&partnerID=40&md5=df7d12e321f564181753e5d93c034bad},
affiliation={National University of Defense Technology, College of Electronic Science and Engineering, Changsha, China},
abstract={Being the most distinct feature point in 3D facial landmarks, nose tip plays a significant role in 3D facial studies such as face detection, face recognition, facial features extraction, face alignment, etc. Successful detection of nose tip can facilitate many tasks of 3D facial studies. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, needs not training, can handle large rotations and occlusions. To reduce computational cost, we first remove small isolated regions from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, the Multi-angle Energy (ME) of each point is computed and sorted in descending order. Then the first m points in the descending order list are obtained and hierarchical clustering method is used to cluster these points. In the first h largest clusters, we can find one point with the largest ME. For all scales of the scale-space, we get a series of such points which are treated as nose tip candidates. For these candidates, we apply hierarchical clustering again. In the obtained largest cluster, we compute the mean value of ME. The ME of nose tip will be closest to the mean value. We evaluate our method in two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The experimental results verify the robustness of our method with a high nose tip detection rate. © 2015 IEEE.},
author_keywords={3D faces;  hierarchical clustering;  least square;  multi-angle energy;  normal;  nose tip;  range images;  robust smoothing;  scale-space;  sphere fitting},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhou2016109,
author={Zhou, W. and Chen, J.-X. and Wang, L.},
title={A RGB-D face recognition approach without confronting the camera},
journal={Proceedings of 2015 IEEE International Conference on Computer and Communications, ICCC 2015},
year={2016},
pages={109-114},
doi={10.1109/CompComm.2015.7387550},
art_number={7387550},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963938690&doi=10.1109%2fCompComm.2015.7387550&partnerID=40&md5=d5bfca843f857fa39877a526fa1c2108},
affiliation={Key Lab of Broadband Wireless Communication and Sensor Network Technology, Ministry of Education, Nanjing, 210003, China},
abstract={Face recognition research mainly focuses on traditional 2D color images, which is extremely susceptible to be affected by external factors such as various viewpoints and has limited recognition accuracy. In order to achieve improved recognition performance, as well as the 3D face holds more abundant information than 2D, we present a 3D human face recognition algorithm using the Microsoft's Kinect. The proposed approach integrates the depth data with the RGB data to generate 3D face raw data and then extracts feature points, identifies the target via a two-level cascade classifier. Also, we build a 3D-face database including 16 individuals captured exclusively using Kinect. The experimental results indicate that the introduced algorithm can not only achieve better recognition accuracy in comparison to existing 2D and 3D face recognition algorithms when the probe face is exactly in front of Kinect sensor, but also can increase 9.3% of recognition accuracy compared to the PCA-3D algorithm when it is not confronting the camera. © 2015 IEEE.},
author_keywords={3D face recognition;  classifier;  Kinect;  RGB-D images;  XML file},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang20161439,
author={Zhang, S. and Yu, H. and Dong, J. and Wang, T. and Ju, Z. and Liu, H.},
title={Automatic Reconstruction of Dense 3D Face Point Cloud with a Single Depth Image},
journal={Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015},
year={2016},
pages={1439-1444},
doi={10.1109/SMC.2015.255},
art_number={7379387},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964490778&doi=10.1109%2fSMC.2015.255&partnerID=40&md5=b3e1cafbcbcde3674270b25529af093d},
affiliation={Ocean University of China, Qingdao, China; University of Portsmouth, Portsmouth, United Kingdom},
abstract={Human face analysis is the basis for many other computer vision tasks, such as camera surveillance, entrance authorization and age estimation. With 3D face models, the vision task based on facial analysis can usually achieve a higher accuracy than the 2D cases since it provides more information with the additional dimension. However, most existing 3D face reconstruction methods suffer from complicated processing and high computation. This paper presents a novel method that simplifies the 3D face reconstruction process with only one shot of Kinect data. The output of the system is a high density of 3D face point cloud with smoother surface. This provides rich details of the human face for other computer vision tasks. Experiments with real world data show promising results using the proposed method. © 2015 IEEE.},
author_keywords={3D face region;  interpolation;  k-means;  Kinect;  RBF;  reconstruction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xia2016,
author={Xia, H. and Huang, T. and Chen, G.},
title={A method of extracting human facial feature points based on 3D laser scanning point cloud data},
journal={International Conference on Geoinformatics},
year={2016},
volume={2016-January},
doi={10.1109/GEOINFORMATICS.2015.7378673},
art_number={7378673},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962437313&doi=10.1109%2fGEOINFORMATICS.2015.7378673&partnerID=40&md5=5604340c250b337bda78d65c17e2cb5d},
affiliation={Faculty of Information Engineering, China University of Geosciences, Wuhan, China; Qianjiang Survey and Design Institute, Qianjiang, Hubei, China},
abstract={Three-dimensional body measurement is determined by measuring the size of each part of the body and body shape, in order to study the morphological characteristics of the human body, and provide basic information for human industrial design, ergonomics, engineering design, anthropological research, and medicine. This paper presents a three-dimensional point cloud data extraction method of facial feature points by doing nose points with examples, using this method can accurately locate the feature points, and there is no specific stations toward requirements in the process of scanning point cloud of the human body, it is convenient and quick. The results show that this method can meet the ergonomics and other features to quickly locate and measure the body size requirements. © 2015 IEEE.},
author_keywords={Measurement;  Nose point;  The face feature point;  Three-dimensional laser scanning;  Three-dimensional point cloud},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hu2016113,
author={Hu, X. and Liao, Q. and Peng, S.},
title={Video surveillance face recognition by more virtual training samples based on 3D modeling},
journal={Proceedings - International Conference on Natural Computation},
year={2016},
volume={2016-January},
pages={113-117},
doi={10.1109/ICNC.2015.7377975},
art_number={7377975},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960408687&doi=10.1109%2fICNC.2015.7377975&partnerID=40&md5=85ab8b4d6d0540e55a76c0b2e6c9ba7e},
affiliation={School of Mechanical and Electric Engineering, Guangzhou University, Guangzhou, China},
abstract={Video surveillance has been applied in more and more fields for security in last decade years, video-based face recognition therefore became an important task of an intelligent monitoring system. However, among these captured video faces there are many non-frontal faces. As a result, the state-of-art face algorithms would become worse when they were employed to recognize video faces. On the other hand, it was a common phenomenon especially at video monitoring field that only one training sample per person is gained from their identification card. The single sample per person (SSPP) results in effecting even not taking advantage of some fine algorithms such LDA. In order to effectively improve the correct recognition rate of multi-pose face recognition with a single frontal training sample, this paper proposed a face recognition algorithm based on 3D modeling. In the proposed algorithm, firstly a 2D frontal face with high-resolution was taken to build a 3D face model, and then several virtual faces with different poses were produced from the 3D face model. At last, both the original frontal face image and virtual face images were put into a gallery set. The algorithm was evaluated on SCface database using traditional PCA and LDA methods. The result showed that the proposed approach could effectively improve video face recognition rate and the correct recognition rate went up about 13% by LDA compared with traditional PCA. Therefore, the method that was proposed to create virtual looking down training samples was an effective algorithm and could be considered to apply in intelligent video monitoring system. © 2015 IEEE.},
author_keywords={3D modeling;  LDA;  PCA;  single training sample;  video surveillance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ganguly2016,
author={Ganguly, S. and Bhattacharjee, D. and Nasipuri, M.},
title={Decremental depth bunch based 3D face recognition from range image},
journal={IEEE Region 10 Annual International Conference, Proceedings/TENCON},
year={2016},
volume={2016-January},
doi={10.1109/TENCON.2015.7372755},
art_number={7372755},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962159083&doi=10.1109%2fTENCON.2015.7372755&partnerID=40&md5=22e866da77e8176ab874f3433272a714},
affiliation={Department of Computer Science and Engineering, Jadavpur University, Kolkata, India},
abstract={In this paper, a new technique, i.e. decremental depth bunches have been presented where two facial discriminating mechanisms have also been implemented for recognizing the individuals. Notably, based on variations of the depth values, different bunches of face regions (i.e. The small components) are extracted by differentiating the depth information that eventually describes detailed facial surface information. Now, from each bunches, statistical attributes as well as Hough peaks are encountered to initiate two feature vectors for feature-based as well as a holistic mechanism for classification by K-NN and Cosine distance respectively. The proposed mechanism is explicitly dependent on facial depth information that have been accomplished in range face images. Therefore, authors have considered two databases, namely: Frav3D and Bosphorus, that contains laser as well as structured light 3D scanner based procured 3D face images respectively. © 2015 IEEE.},
author_keywords={BIEM;  Cosine transform;  Depth bunch;  Hough peaks;  K-NN;  Laser scanner;  Range Image;  Statistical feature;  Structure light scanner},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Minga2016117,
author={Minga, Y. and Hong, X.},
title={A unified 3D face authentication framework based on robust localmesh SIFT feature},
journal={Neurocomputing},
year={2016},
volume={184},
pages={117-130},
doi={10.1016/j.neucom.2015.07.127},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975516142&doi=10.1016%2fj.neucom.2015.07.127&partnerID=40&md5=8ff10332acc80f01c907d3c49c1da439},
affiliation={Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Department of Computer Science and Engineering, University of Oulu, Finland},
abstract={In this paper, we design a unified 3D face authentication system for practical use. First, we propose afacial depth recovery method to construct a facial depth map from stereoscopic videos. It effectivelyutilize prior facial information and incorporate the visibility term to classify static and dynamic pixels forrobust depth estimation. Secondly, in order to make 3D face authentication more accurate and consistent,we present an intrinsic scale feature detection for interesting points on 3D facial mesh regions.Then, a novel feature descriptor is proposed, called Local Mesh Scale-Invariant Feature Transform(LMSIFT) to reflect the different face recognition abilities in different facial regions. Finally, the sparseoptimization problem of visual codebook is used to 3D face learning. We evaluate our approach onpublicly available 3D face databases and self-collected realistic scene databases. We also develop aninteractive education system to investigate its performance in practice, which demonstrates the highperformance of the proposed approach for accurate 3D face authentication. Compared with previouspopular approaches, our system has consistently better performance in terms of effectiveness, robustnessand universality. © 2015 Elsevier B.V..},
author_keywords={3D face authentication;  Depth estimation;  Facial region segmentation;  Interactive education platform;  Local Mesh Scale-Invariant Feature Transform(LMSIFT)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Krotewicz201665,
author={Krotewicz, P.},
title={Novel ear-assisted 3D face recognition under expression variations},
journal={International Journal of Biometrics},
year={2016},
volume={8},
number={1},
pages={65-81},
doi={10.1504/IJBM.2016.077148},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976902847&doi=10.1504%2fIJBM.2016.077148&partnerID=40&md5=887b19cd5da35df8a4cf29dba780a556},
affiliation={Department of Microelectronics and Computer Science, Lodz University of Technology, Lodz, Poland},
abstract={This paper concerns the novel region-based ear-assisted 3D face recognition based on iterative closest point (ICP) algorithm in face expression changing scenario. The proposed algorithm for 3D face biometric recognition was prepared and tested. As a first contribution, current state-of-the-art in the field of 3D face recognition is presented and the main approaches to the problem are briefly described. Furthermore, all the data processing steps: preprocessing, segmentation, feature extraction and feature comparison are described in detail. As a second contribution, the algorithm behaviour is scrutinised on the DMCSv1 database and the results in the form of DET curves are highlighted in this paper. Also, the comparison with results obtained by means of the algorithm neglecting ear regions is provided. It occurs that ear geometry information added to face as an auxiliary input to iterative closest point can greatly improve recognition results especially in the case of very strong facial expressions. Equal error rate does not exceed 6.25% on arbitrary data subset. In the last section, conclusions are formulated and plans for future work are presented. © 2016 Inderscience Enterprises Ltd.},
author_keywords={3D ear geometry;  3D face geometry;  Biometric identification;  Biometrics;  Contactless identification;  Expression variations;  Expression-invariant face recognition;  Feature-based system;  ICP;  Iterative closest point;  Region-based method},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Galbally2016199,
author={Galbally, J. and Satta, R.},
title={Biometric sensor interoperability: A case study in 3D face recognition},
journal={ICPRAM 2016 - Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods},
year={2016},
pages={199-204},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969988967&partnerID=40&md5=80af1e7c3fa3ef4e303e144e6927af47},
affiliation={European Commission - Joint Research Centre, IPSC, Via Enrico Fermi 2749, Ispra, 21027, Italy},
abstract={Biometric systems typically suffer a significant loss of performance when the acquisition sensor is changed between enrolment and authentication. Such a problem, commonly known as sensor interoperability, poses a serious challenge to the accuracy of matching algorithms. The present work addresses for the first time the sensor interoperability issue in 3D face recognition systems, analysing the performance of two popular and well known techniques for 3D facial authentication. For this purpose, a new gender-balanced database comprising 3D data of 26 subjects has been acquired using two devices belonging to the new generation of low-cost 3D sensors. The results show the high sensor-dependency of the tested systems and the need to develop matching algorithms robust to the variation in the sensor resolution. © Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={3D face database;  3D face recognition;  Interoperability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Song2016,
author={Song, D. and Luo, J. and Zi, C. and Tian, H.},
title={3D Face Recognition Using Anthropometric and Curvelet Features Fusion},
journal={Journal of Sensors},
year={2016},
volume={2016},
doi={10.1155/2016/6859364},
art_number={6859364},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954447014&doi=10.1155%2f2016%2f6859364&partnerID=40&md5=145d972d12039ce0cfdea217aaa57214},
affiliation={College of Electrical Engineering and Automation, Tianjin Polytechnic University, Tianjin, 300387, China; Key Laboratory of Advanced Electrical Engineering and Energy Technology, Tianjin, 300387, China; School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, Sydney, NSW  2522, Australia},
abstract={Curvelet transform can describe the signal by multiple scales, and multiple directions. In order to improve the performance of 3D face recognition algorithm, we proposed an Anthropometric and Curvelet features fusion-based algorithm for 3D face recognition (Anthropometric Curvelet Fusion Face Recognition, ACFFR). First, the eyes, nose, and mouth feature regions are extracted by the Anthropometric characteristics and curvature features of the human face. Second, Curvelet energy features of the facial feature regions at different scales and different directions are extracted by Curvelet transform. At last, Euclidean distance is used as the similarity between template and objectives. To verify the performance, the proposed algorithm is compared with Anthroface3D and Curveletface3D on the Texas 3D FR database. The experimental results have shown that the proposed algorithm performs well, with equal error rate of 1.75% and accuracy of 97.0%. The algorithm we proposed in this paper has better robustness to expression and light changes than Anthroface3D and Curveletface3D. © 2016 Dan Song et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bellil2016365,
author={Bellil, W. and Brahim, H. and Ben Amar, C.},
title={Gappy wavelet neural network for 3D occluded faces: detection and recognition},
journal={Multimedia Tools and Applications},
year={2016},
volume={75},
number={1},
pages={365-380},
doi={10.1007/s11042-014-2294-6},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953638822&doi=10.1007%2fs11042-014-2294-6&partnerID=40&md5=e5d7d42e44edd21d1de8b6e4cc7bdd48},
affiliation={REGIM: REsearch Groups on Intelligent Machines, University of Sfax, National Engineering School of Sfax (ENIS), Sfax, Tunisia},
abstract={The first handicap in 3D faces recognizing under unconstrained problem is the largest variability of the visual aspect when we use various sources. This great variability complicates the task of identifying persons from their 3D facial scans and it is the most reason that bring to face detection and recognition of the major problems in pattern recognition fields, biometrics and computer vision. We propose a new 3D face identification and recognition method based on Gappy Wavelet Neural Network (GWNN) that is able to provide better accuracy in the presence of facial occlusions. The proposed approach consists of three steps: the first step is face detection. The second step is to identify and remove occlusions. Occluded regions detection is done by considering that occlusions can be defined as local face deformations. These deformations are detected by a comparison between the input facial test wavelet coefficients and wavelet coefficients of generic face model formed by the mean data base faces. They are beneficial for neighborhood relationships between pixels rotation, dilation and translation invariant. Then, occluded regions are refined by removing wavelet coefficient above a certain threshold. Finally, the last stage of processing and retrieving is made based on wavelet neural network to recognize and to restore 3D occluded regions that gathers the most. The experimental results on this challenging database demonstrate that the proposed approach improves recognition rate performance from 93.57 to 99.45 % which represents a competitive result compared to the state of the art. © 2014, Springer Science+Business Media New York.},
author_keywords={3D face recognition; Wavelets;  Gappy data;  Occlusion detection;  Wavelet neural network},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vezzetti201624,
author={Vezzetti, E. and Tornincasa, S. and Marcolin, F. and Moos, S. and Violante, M.G. and Vicente, D.B. and Speranza, D. and Padula, F.},
title={3D human face analysis: Automatic expression recognition},
journal={Proceedings of the 12th IASTED International Conference on Biomedical Engineering, BioMed 2016},
year={2016},
pages={24-30},
doi={10.2316/P.2016.832-067},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015421390&doi=10.2316%2fP.2016.832-067&partnerID=40&md5=77c4f4a64cac89b59c02d9f7544acc5e},
affiliation={DIGEP, Politecnico di Torino, Corso Duca degli Abruzzi 24, Torino, 10129, Italy; DICeM, Università degli Studi di Cassino e Del Lazio Meridionale, Via Marconi 10, Cassino, FR, 03043, Italy; Department of Prenatal Diagnosis, ALTAMEDICA, Fetal Maternal Medical Centre, Rome, Italy},
abstract={A 3D automatic facial expression recognition procedure is presented in this work. The method is based on point-by-point mapping of seventeen Differential Geometry descriptors onto the probe facial depth map, which is then partitioned into seventy-nine regions. Then, features such as mean, median, mode, volumes, histograms are computed for each region and for each descriptor, to reach a varied large set of parameters representing the query face. Each set of parameters, given by a geometrical descriptor, a region, and a feature, form a trio, whose featuring numerical values are compared with appropriate thresholds, set via experimentation in a previous phase by processing a limited portion of the public facial Bosphorus database. This allows the identification of the emotion-based expression of the query 3D face among the six basic ones (anger, disgust, fear, joy, sadness, surprise). The algorithm was tested on the Bosphorus database and is suitable for applications in security, marketing, medical. The three-dimensional context has been preferred due to its invariance to different lightening/make-up/camouflage conditions.},
author_keywords={3D face;  Differential geometry;  Emotions;  Face expression recognition (FER);  Facial expression recognition;  Shape index},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Talandova20166373,
author={Talandova, H. and Kralik, L. and Adamek, M.},
title={Determination of the uncertainties and the physiological similarities of family members by using the biometric device the broadway 3D},
journal={International Journal of Applied Engineering Research},
year={2016},
volume={11},
number={9},
pages={6373-6375},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026785113&partnerID=40&md5=712cdc729bf70e487251043cd050117c},
affiliation={Security Engineering Department, Tomas Bata University in Zlin, Faculty of Applied Informatics, Zlin, Czech Republic},
abstract={The biometric identification by the face is one of the oldest biometric identification. With increasing progress has been using of identification by the face was implemented into area of security, where it provides a faster and more accurate identification. The 3D face reader uses for the identification of the person: eyes, mouth, nose, and in contrast to 2D readers also chin and cheeks. 3D face reader by Broadway manufacturer was used to measure the physiological similarities of family members. It is equipped with the 3D camera system, which uses the method of structured light scanning and saves the template into the 3D model of face. The obtained data were evaluated by software Turnstile Enrolment Application (TEA). The participants of the measurement were members of three different families. Each person was compared with the previously saved templates of other family members. On basis of this fact was evaluated the similarity of family members. © Research India Publications.},
author_keywords={Biometric identification;  Broadway 3D reader;  Face;  Similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ouamane2016129,
author={Ouamane, A. and Belahcene, M. and Benakcha, A. and Bourennane, S. and Taleb-Ahmed, A.},
title={Robust multimodal 2D and 3D face authentication using local feature fusion},
journal={Signal, Image and Video Processing},
year={2016},
volume={10},
number={1},
pages={129-137},
doi={10.1007/s11760-014-0712-x},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953840225&doi=10.1007%2fs11760-014-0712-x&partnerID=40&md5=c3fdfcf3431d7007c836ebe117c5b651},
affiliation={LMSE, Universite Mohamed Khider Biskra, Biskra, Algeria; LGEB, Universite Mohamed Khider Biskra, Biskra, Algeria; Institut Fresnel, Universite de Marseille, Marseille, France; LAMIH UMR CNRS 8201, Valenciennes, France},
abstract={In this work, we present a robust face authentication approach merging multiple descriptors and exploiting both 3D and 2D information. First, we correct the heads rotation in 3D by iterative closest point algorithm, followed by an efficient preprocessing phase. Then, we extract different features namely: multi-scale local binary patterns (MSLBP), novel statistical local features (SLF), Gabor wavelets, and scale invariant feature transform (SIFT). The principal component analysis followed by enhanced fisher linear discriminant model is used for dimensionality reduction and classification. Finally, fusion at the score level is carried out using two-class support vector machines. Extensive experiments are conducted on the CASIA 3D faces database. The evaluation of individual descriptors clearly showed the superiority of the proposed SLF features. In addition, applying the ($$\hbox {3D} + \hbox {2D}$$3D+2D) multimodal score level fusion, the best result is obtained by combining the SLF with the $$\hbox {MSLBP}+\hbox {SIFT}$$MSLBP+SIFT descriptor yielding in an equal error rate of 0.98 % and a recognition rate of $$\hbox {RR} = 97.22\,\%$$RR=97.22%. © 2014, Springer-Verlag London.},
author_keywords={Fusion;  Iterative closest point;  Multi-scale local binary patterns (MSLBP);  Scale invariant feature transform (SIFT);  Statistical local features (SLF)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Magruder2016,
author={Magruder, L.A. and Leigh, H.W. and Soderlund, A. and Clymer, B. and Baer, J. and Neuenschwander, A.L.},
title={Automated feature extraction for 3-dimensional point clouds},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9832},
doi={10.1117/12.2223845},
art_number={98320F},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991463555&doi=10.1117%2f12.2223845&partnerID=40&md5=5ca5c3d579c37e5705eaba5444dbe641},
affiliation={Applied Research Laboratories, University of Texas at Austin, P.O. Box 8029, Austin, TX  78713-8029, United States},
abstract={Light detection and ranging (LIDAR) technology offers the capability to rapidly capture high-resolution, 3-dimensional surface data with centimeter-level accuracy for a large variety of applications. Due to the foliage-penetrating properties of LIDAR systems, these geospatial data sets can detect ground surfaces beneath trees, enabling the production of highfidelity bare earth elevation models. Precise characterization of the ground surface allows for identification of terrain and non-terrain points within the point cloud, and facilitates further discernment between natural and man-made objects based solely on structural aspects and relative neighboring parameterizations. A framework is presented here for automated extraction of natural and man-made features that does not rely on coincident ortho-imagery or point RGB attributes. The TEXAS (Terrain EXtraction And Segmentation) algorithm is used first to generate a bare earth surface from a lidar survey, which is then used to classify points as terrain or non-terrain. Further classifications are assigned at the point level by leveraging local spatial information. Similarly classed points are then clustered together into regions to identify individual features. Descriptions of the spatial attributes of each region are generated, resulting in the identification of individual tree locations, forest extents, building footprints, and 3-dimensional building shapes, among others. Results of the fully-automated feature extraction algorithm are then compared to ground truth to assess completeness and accuracy of the methodology. © 2016 SPIE.},
author_keywords={3D point clouds;  bare earth;  feature extraction;  lidar},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhou2016749,
author={Zhou, K. and Gorte, B. and Zlatanova, S.},
title={Exploring regularities for improving façade reconstruction from point clouds},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2016},
volume={41},
pages={749-755},
doi={10.5194/isprsarchives-XLI-B5-749-2016},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979208513&doi=10.5194%2fisprsarchives-XLI-B5-749-2016&partnerID=40&md5=a8d6625f57dbc05bc73e2fa9b09c0ceb},
affiliation={Dept. of Geoscience and Remote Sensing, TU Delft, Netherlands; Dept. of Urbanism, 3D Geoinformation, TU Delft, Netherlands},
abstract={(Semi)- Automatic facade reconstruction from terrestrial LiDAR point clouds is often affected by both quality of point cloud itself and imperfectness of object recognition algorithms. In this paper, we employ regularities, which exist on façades, to mitigate these problems. For example, doors, windows and balconies often have orthogonal and parallel boundaries. Many windows are constructed with the same shape. They may be arranged at the same lines and distance intervals, so do different windows. By identifying regularities among objects with relatively poor quality, these can be applied to calibrate the objects and improve their quality. The paper focuses on the regularities among the windows, which is the majority of objects on the wall. Regularities are classified into three categories: Within an individual window, among similar windows and among different windows. Nine cases are specified as a reference for exploration. A hierarchical clustering method is employed to identify and apply regularities in a feature space, where regularities can be identified from clusters. To find the corresponding features in the nine cases of regularities, two phases are distinguished for similar and different windows. In the first phase, ICP (iterative closest points) is used to identify groups of similar windows. The registered points and a number of transformation matrices are used to identify and apply regularities among similar windows. In the second phase, features are extracted from the boundaries of the different windows. When applying regularities by relocating windows, the connections, called chains, established among the similar windows in the first phase are preserved. To test the performance of the algorithms, two datasets from terrestrial LiDAR point clouds are used. Both show good effects on the reconstructed model, while still matching with original point cloud, preventing over or under-regularization.},
author_keywords={Chain;  Features;  Hierarchical clustering;  ICP;  Regularities;  Terrestrial LiDAR point clouds;  Windows},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kadamen2016617,
author={Kadamen, J. and Sithole, G.},
title={Automatically determining scale within unstructured point clouds},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2016},
volume={41},
pages={617-624},
doi={10.5194/isprsarchives-XLI-B3-617-2016},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978036086&doi=10.5194%2fisprsarchives-XLI-B3-617-2016&partnerID=40&md5=e45f3c738a9989d2d6d38853dcf4e37d},
affiliation={Dept. of Geomatic Engineering, School of Architecture, University of Cape Town, Private Bag X3, Rondebosch, 7701, South Africa},
abstract={Three dimensional models obtained from imagery have an arbitrary scale and therefore have to be scaled. Automatically scaling these models requires the detection of objects in these models which can be computationally intensive. Real-Time object detection may pose problems for applications such as indoor navigation. This investigation poses the idea that relational cues, specifically height ratios, within indoor environments may offer an easier means to obtain scales for models created using imagery. The investigation aimed to show two things, (a) that the size of objects, especially the height off ground is consistent within an environment, and (b) that based on this consistency, objects can be identified and their general size used to scale a model. To test the idea a hypothesis is first tested on a terrestrial lidar scan of an indoor environment. Later as a proof of concept the same test is applied to a model created using imagery. The most notable finding was that the detection of objects can be more readily done by studying the ratio between the dimensions of objects that have their dimensions defined by human physiology. For example the dimensions of desks and chairs are related to the height of an average person. In the test, the difference between generalised and actual dimensions of objects were assessed. A maximum difference of 3:96% (2:93cm) was observed from automated scaling. By analysing the ratio between the heights (distance from the floor) of the tops of objects in a room, identification was also achieved.},
author_keywords={Monocular Camera;  Real-world Scale;  Simplified Object-Recognition;  Smart-phones;  Unstructured Point Cloud},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hammer2016,
author={Hammer, M. and Hebel, M. and Arens, M.},
title={Automated object detection and tracking with a flash LiDAR system},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9988},
doi={10.1117/12.2240640},
art_number={998803},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011029544&doi=10.1117%2f12.2240640&partnerID=40&md5=b07dd0e6e54d97fdabd5857cd0810c72},
affiliation={Fraunhofer Institute of Optronics, System Technologies, and Image Exploitation IOSB, Gutleuthausstr. 1, Ettlingen, 76275, Germany},
abstract={The detection of objects, or persons, is a common task in the fields of environment surveillance, object observation or danger defense. There are several approaches for automated detection with conventional imaging sensors as well as with LiDAR sensors, but for the latter the real-time detection is hampered by the scanning character and therefore by the data distortion of most LiDAR systems. The paper presents a solution for real-time data acquisition of a flash LiDAR sensor with synchronous raw data analysis, point cloud calculation, object detection, calculation of the next best view and steering of the pan-tilt head of the sensor. As a result the attention is always focused on the object, independent of the behavior of the object. Even for highly volatile and rapid changes in the direction of motion the object is kept in the field of view. The experimental setup used in this paper is realized with an elementary person detection algorithm in medium distances (20 m to 60 m) to show the efficiency of the system for objects with a high angular speed. It is easy to replace the detection part by any other object detection algorithm and thus it is easy to track nearly any object, for example a car or a boat or an UAV in various distances. © 2016 SPIE.},
author_keywords={3D object detection;  matrix LiDAR;  organized point cloud;  pan-tilt head;  person tracking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Böhm2016301,
author={Böhm, J. and Bredif, M. and Gierlinger, T. and Krämer, M. and Lindenbergh, R. and Liu, K. and Michel, F. and Sirmacek, B.},
title={The Iqmulus urban showcase: Automatic tree classification and identification in huge mobile mapping point clouds},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2016},
volume={41},
pages={301-307},
doi={10.5194/isprsarchives-XLI-B3-301-2016},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978159544&doi=10.5194%2fisprsarchives-XLI-B3-301-2016&partnerID=40&md5=2fbed8958deed902b55eef60e98a087e},
affiliation={Dept. of Civil, Environmental and Geomatic Engineering, University College London, United Kingdom; Universite Paris-Est, IGN, SRIG, MATIS, 73 avenue de Paris, Saint Mande, 94160, France; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Dept. of Geoscience and Remote Sensing, Delft University of Technology, Netherlands},
abstract={Current 3D data capturing as implemented on for example airborne or mobile laser scanning systems is able to efficiently sample the surface of a city by billions of unselective points during one working day. What is still difficult is to extract and visualize meaningful information hidden in these point clouds with the same efficiency. This is where the FP7 IQmulus project enters the scene. IQmulus is an interactive facility for processing and visualizing big spatial data. In this study the potential of IQmulus is demonstrated on a laser mobile mapping point cloud of 1 billion points sampling " 10 km of street environment in Toulouse, France. After the data is uploaded to the IQmulus Hadoop Distributed File System, a workflow is defined by the user consisting of retiling the data followed by a PCA driven local dimensionality analysis, which runs efficiently on the IQmulus cloud facility using a Spark implementation. Points scattering in 3 directions are clustered in the tree class, and are separated next into individual trees. Five hours of processing at the 12 node computing cluster results in the automatic identification of 4000+ urban trees. Visualization of the results in the IQmulus fat client helps users to appreciate the results, and developers to identify remaining flaws in the processing workflow.},
author_keywords={Big data;  Classification;  Cloud computing;  Mobile mapping;  Trees;  Web-based visualization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Macher2016667,
author={Macher, H. and Landes, T. and Grussenmeyer, P.},
title={Validation of point clouds segmentation algorithms through their application to several case studies for indoor building modelling},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2016},
volume={41},
pages={667-674},
doi={10.5194/isprsarchives-XLI-B5-667-2016},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979211453&doi=10.5194%2fisprsarchives-XLI-B5-667-2016&partnerID=40&md5=b571d5617308ef12d724c3da427f8bfc},
affiliation={Photogrammetry and Geomatics Group, ICube Laboratory UMR 7357, INSA, Strasbourg, France},
abstract={Laser scanners are widely used for the modelling of existing buildings and particularly in the creation process of as-built BIM (Building Information Modelling). However, the generation of as-built BIM from point clouds involves mainly manual steps and it is consequently time consuming and error-prone. Along the path to automation, a three steps segmentation approach has been developed. This approach is composed of two phases: A segmentation into sub-spaces namely floors and rooms and a plane segmentation combined with the identification of building elements. In order to assess and validate the developed approach, different case studies are considered. Indeed, it is essential to apply algorithms to several datasets and not to develop algorithms with a unique dataset which could influence the development with its particularities. Indoor point clouds of different types of buildings will be used as input for the developed algorithms, going from an individual house of almost one hundred square meters to larger buildings of several thousand square meters. Datasets provide various space configurations and present numerous different occluding objects as for example desks, computer equipments, home furnishings and even wine barrels. For each dataset, the results will be illustrated. The analysis of the results will provide an insight into the transferability of the developed approach for the indoor modelling of several types of buildings.},
author_keywords={3D modelling;  Building elements;  Datasets;  Indoor point clouds;  Laser scanning;  Space segmentation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Talandova2015,
author={Talandova, H. and Kralik, L. and Adamek, M.},
title={Determination of the physiological similarities of family members by using a broadway 3D biometric device},
journal={2015 International Conference on Logistics, Informatics and Service Science, LISS 2015},
year={2015},
doi={10.1109/LISS.2015.7369767},
art_number={7369767},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971654803&doi=10.1109%2fLISS.2015.7369767&partnerID=40&md5=058816a90db7e6a467b4861db15c66ea},
affiliation={Faculty of Applied Informatics, Tomas Bata University in Zlin, Zlin, Czech Republic},
abstract={The biometric identification by the face is one of the oldest biometric identification. With increasing progress in the area of identification by the face this technique was implemented into area of security, where it provides a faster and more accurate identification. The 3D face reader uses for the identification of the person: eyes, mouth, nose, and in contrast to 2D readers also chin and cheeks. 3D face reader by Broadway manufacturer was used to measure the physiological similarities of family members. It is equipped with the 3D camera system, which uses the method of structured light scanning and saves the template into the 3D model of face. The obtained data were evaluated by software Turnstile Enrolment Application (TEA). The participants of the measurement were members of three different families. Each person was compared with the previously saved templates of other family members. Using this method the similarity of family members was evalua00ted. © 2015 IEEE.},
author_keywords={Biometric identification;  Broadway 3D reader;  face;  similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Boukamcha2015,
author={Boukamcha, H. and Elhallek, M. and Atri, M. and Smach, F.},
title={3D face landmark auto detection},
journal={2015 World Symposium on Computer Networks and Information Security, WSCNIS 2015},
year={2015},
doi={10.1109/WSCNIS.2015.7368276},
art_number={7368276},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962430025&doi=10.1109%2fWSCNIS.2015.7368276&partnerID=40&md5=13ae6691c8afd0ea13bb469882ec0aea},
affiliation={University of Sciences of Monastir, Monastir, Tunisia; Université of Bourgogne, France},
abstract={This paper presents our methodology for Landmark Point detection to improve 3D face recognition in a presence of variant facial expression. The objective was to develop an automatic process for distinguishing and segmenting to be embedded in a 3D face recognition system using only 3D Point Distribution Model (PDM) as input. The approach used hydride method to extract this features from the surface curvature information. Landmark Localization is done on the segmented face via finding the change that decreases the deviation of the model from the mean profile. Face registering is achieved using previous anthropometric information and the localized landmarks. The results confirm that the method used is accurate and robust for the proposed application. © 2015 IEEE.},
author_keywords={3D Face;  Graph Matching;  Labelling;  Registration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu2015,
author={Wu, Y. and Xu, X. and Shah, S.K. and Kakadiaris, I.A.},
title={Towards fitting a 3D dense facial model to a 2D image: A landmark-free approach},
journal={2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems, BTAS 2015},
year={2015},
doi={10.1109/BTAS.2015.7358799},
art_number={7358799},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962911396&doi=10.1109%2fBTAS.2015.7358799&partnerID=40&md5=6b3d002c3c4ccce213150de367e4fa7e},
affiliation={Computational Biomedicine Lab, Department of Computer Science, University of Houston, Houston, TX, United States},
abstract={Head pose estimation helps to align a 3D face model to a 2D image, which is critical to research requiring dense 2D-to-2D or 3D-to-2D correspondence. Traditional pose estimation relies strongly on the accuracy of landmarks, so it is sensitive to missing or incorrect landmarks. In this paper, we propose a landmark-free approach to estimate the pose projection matrix. The method can be used to estimate this matrix in unconstrained scenarios and we demonstrate its effectiveness through multiple head pose estimation experiments. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dou2015,
author={Dou, P. and Zhang, L. and Wu, Y. and Shah, S.K. and Kakadiaris, I.A.},
title={Pose-robust face signature for multi-view face recognition},
journal={2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems, BTAS 2015},
year={2015},
doi={10.1109/BTAS.2015.7358788},
art_number={7358788},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962900920&doi=10.1109%2fBTAS.2015.7358788&partnerID=40&md5=b80000fb0790ba63c205feebd4e26bab},
affiliation={Computational Biomedicine Lab, Department of Computer Science, University of Houston, Houston, TX, United States},
abstract={Despite the great progress achieved in unconstrained face recognition, pose variations still remain a challenging and unsolved practical issue. We propose a novel framework for multi-view face recognition based on extracting and matching pose-robust face signatures from 2D images. Specifically, we propose an efficient method for monocular 3D face reconstruction, which is used to lift the 2D facial appearance to a canonical texture space and estimate the self-occlusion. On the lifted facial texture we then extract various local features, which are further enhanced by the occlusion encodings computed on the self-occlusion mask, resulting in a pose-robust face signature, a novel feature representation of the original 2D facial image. Extensive experiments on two public datasets demonstrate that our method not only simplifies the matching of multi-view 2D facial images by circumventing the requirement for pose-adaptive classifiers, but also achieves superior performance. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2015,
author={Chen, L. and Ferryman, J.},
title={Combining 3D and 2D for less constrained periocular recognition},
journal={2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems, BTAS 2015},
year={2015},
doi={10.1109/BTAS.2015.7358753},
art_number={7358753},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962888228&doi=10.1109%2fBTAS.2015.7358753&partnerID=40&md5=20a46a89c1a4eebb7ba03e56d9f8ff9a},
affiliation={Computational Vision Group, School of Systems Engineering, University of Reading, Whiteknights, Reading, RG6 6AY, United Kingdom},
abstract={Periocular recognition has recently become an active topic in biometrics. Typically it uses 2D image data of the periocular region. This paper is the first description of combining 3D shape structure with 2D texture. A simple and effective technique using iterative closest point (ICP) was applied for 3D periocular region matching. It proved its strength for relatively unconstrained eye region capture, and does not require any training. Local binary patterns (LBP) were applied for 2D image based periocular matching. The two modalities were combined at the score-level. This approach was evaluated using the Bosphorus 3D face database, which contains large variations in facial expressions, head poses and occlusions. The rank-1 accuracy achieved from the 3D data (80%) was better than that for 2D (58%), and the best accuracy (83%) was achieved by fusing the two types of data. This suggests that significant improvements to periocular recognition systems could be achieved using the 3D structure information that is now available from small and inexpensive sensors. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Samad2015337,
author={Samad, M.D. and Bobzien, J.L. and Harrington, J.W. and Iftekharuddin, K.M.},
title={Analysis of facial muscle activation in children with autism using 3D imaging},
journal={Proceedings - 2015 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2015},
year={2015},
pages={337-342},
doi={10.1109/BIBM.2015.7359704},
art_number={7359704},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962476943&doi=10.1109%2fBIBM.2015.7359704&partnerID=40&md5=6abc4dd3db8bcb16a12a0f75aa37895b},
affiliation={Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA  23529, United States; Department of Communication Disorder and Special Education, Old Dominion University, Norfolk, VA  23529, United States; Department of Pediatrics, Eastern Virginia Medical School, Norfolk, VA  23517, United States},
abstract={Autism Spectrum Disorder (ASD) impairs an individual's non-verbal skills including natural and contextual facial expressions. Such impairments may manifest as odd facial expressions (facial oddity) based on subjective evaluations of facial images. A few studies conducted on individuals with ASD have focused on the physiology of facial muscle usage by employing eletrophysiological sensors in response to visual stimuli. The sensors are placed directly on the face and may inhibit or limit the spontaneous facial response which may be too subtle for subjective human evaluations. This study uses a non-intrusive 3D facial imaging sensor that captures detailed geometric information of the face to facilitate quantification and detection of subtle changes in facial expression based on the physiology of facial muscle. A novel computer vision and data mining approach is developed from curve-based geometric feature of 3D facial data to discern the changes in the facial muscle actions. A pilot study is conducted with sixteen subjects (8 subjects with ASD and 8 typically-developing controls) where 3D facial images have been captured in response to visual stimuli involving 3D facial expressions. Statistical analyses reveal a significantly asymmetric facial muscle action in subjects with ASD compared to the typically-developing controls. This study demonstrates feasibility of using non-intrusive facial imaging sensor data in evaluating possible physiology-based impairments. © 2015 IEEE.},
author_keywords={3D Face;  Autism Spectrum Disorder;  Facial Expression Oddity;  Facial Muscle Activation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Moeini201583,
author={Moeini, A. and Faez, K. and Moeini, H.},
title={Unconstrained pose-invariant face recognition by a triplet collaborative dictionary matrix},
journal={Pattern Recognition Letters},
year={2015},
volume={68},
pages={83-89},
doi={10.1016/j.patrec.2015.08.012},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942575937&doi=10.1016%2fj.patrec.2015.08.012&partnerID=40&md5=a6695d9ad38152da17cfbb98fafe639d},
affiliation={Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran; Electrical Engineering Department, Semnan University, Semnan, Iran},
abstract={In this paper, a novel method is proposed for unconstrained pose-invariant face recognition from only an image in a gallery. A 3D face is initially reconstructed using only a 2D frontal image. Then, for each person in the gallery, a Triplet Collaborative Dictionary Matrix (TCDM) is created from all face poses by rotating the 3D reconstructed models and extracting features in rotated face. Each TCDM is subsequently rendered based on triplet angles of face poses. Finally, the classification is performed by Collaborative Representation Classification (CRC) with Regularized Least Square (RLS). Promising results were acquired to handle pose changes on the FERET, LFW and video face databases compared to state-of-the-art methods in pose-invariant face recognition. © 2015ElsevierB.V.Allrightsreserved.},
author_keywords={Collaborative dictionary matrix;  Collaborative representation;  Face synthesis;  Pose-invariant face recognition;  Real-world},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lv20153635,
author={Lv, S. and Da, F. and Deng, X.},
title={A 3D face recognition method using region-based extended local binary pattern},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2015},
volume={2015-December},
pages={3635-3639},
doi={10.1109/ICIP.2015.7351482},
art_number={7351482},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956668987&doi=10.1109%2fICIP.2015.7351482&partnerID=40&md5=2c0948e184ab3751c8c447b235e50e0a},
affiliation={School of Automation, Southeast University, Key Laboratory of Measurement and Control of CSE, Ministry of Education, Nanjing, 210096, China},
abstract={A 3D face recognition method using region-based extended local binary pattern (eLBP) is proposed. First, the depth image converted from the preprocessed 3D pointclouds is normalized. Then, different regions according to their distortions under facial expressions are extracted by binary masks and represented by the uniform pattern of extended LBP. Finally, sparse representation classifier (SRC) is adopted for classification on the single region. Feature-level and score-level fusion with weight-sparse representation classifier (W-SRC) are also tested and compared, and the latter has better performance. The experiments on FRGC v2.0 database demonstrate that the proposed method is robust and efficient. © 2015 IEEE.},
author_keywords={3D face recognition;  binary mask;  depth image;  extended local binary pattern;  weight-sparse representation classifier},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tortorici20152670,
author={Tortorici, C. and Werghi, N. and Berretti, S.},
title={Boosting 3D LBP-based face recognition by fusing shape and texture descriptors on the mesh},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2015},
volume={2015-December},
pages={2670-2674},
doi={10.1109/ICIP.2015.7351287},
art_number={7351287},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956639911&doi=10.1109%2fICIP.2015.7351287&partnerID=40&md5=6ea1b56b061b463d4fc6e850dc9c85e6},
affiliation={Khalifa University, Abu Dhabi, United Arab Emirates; University of Florence, Florence, Italy},
abstract={In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBP) for 3D face recognition. Using the framework proposed in [1], we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared to its depth-image counterpart, our approach is distinguished by the following features: a) inherits the intrinsic advantages of mesh surface; b) does not require normalization; c) can accommodate partial matching. In addition, it allows early-level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data. © 2015 IEEE.},
author_keywords={3D face recognition;  fusion;  mesh-LBP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Batabyal20153270,
author={Batabyal, T. and Vaccari, A. and Acton, S.T.},
title={UGraSP: A unified framework for activity recognition and person identification using graph signal processing},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2015},
volume={2015-December},
pages={3270-3274},
doi={10.1109/ICIP.2015.7351408},
art_number={7351408},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956689217&doi=10.1109%2fICIP.2015.7351408&partnerID=40&md5=5599f2511cd81114fb4aae00789b74d4},
affiliation={Virginia Image and Video Analysis Laboratory, Department of Electrical Engineering, University of Virginia, P.O. Box 400743, Charlottesville, VA  22904, United States},
abstract={With the growing availability and wide distribution of low-cost, high-performance 3D imaging sensors, the image analysis community has witnessed an increased demand for solutions to the challenges of activity recognition and person identification. We propose an integrated framework, based on graph signal processing, that simultaneously performs both tasks using a single set of features. The novelty of our approach is based on the fact that the set of features used for activity recognition accommodates person identification without additional computation. The analysis is based on the extracted structure-invariant graph (skeleton). The Laplacian of the skeleton is used both to identify the person and recognize the performed activity. While person identification is achieved directly from the analysis of the Laplacian, activity recognition is obtained after transformation, into the graph spectral domain, of the vectorized form of the skeletal joints 3D coordinates. Feature vectors for activity recognition are then derived, in this domain, from the covariance matrices evaluated over fixed-length sequential video segments. Both classification tasks are implemented using linear support vector machines (SVM). When applied to real activity datasets, our approach shows an improved performance over the existing state-of-the-art. © 2015 IEEE.},
author_keywords={activity Recognition;  Adjacency Matrix;  Graph Fourier Transform;  Graph Signal Processing;  Laplacian;  Person Identification;  Point cloud datasets},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gálai2015,
author={Gálai, B. and Benedek, C.},
title={Feature selection for Lidar-based gait recognition},
journal={2015 International Workshop on Computational Intelligence for Multimedia Understanding, IWCIM 2015},
year={2015},
doi={10.1109/IWCIM.2015.7347076},
art_number={7347076},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962873425&doi=10.1109%2fIWCIM.2015.7347076&partnerID=40&md5=39be328f36f8e80a79539086d1ad61e5},
affiliation={Institute for Computer Science and Control, Kende u. 13-17, H-1111 Budapest, Hungary},
abstract={In this paper, we present a performance analysis of various descriptors suited to human gait analysis in Rotating Multi-Beam (RMB) Lidar measurement sequences. The gait descriptors for training and recognition are observed and extracted in realistic outdoor surveillance scenarios, where multiple pedestrians walk concurrently in the field of interest, their trajectories often intersect, while occlusions or background noise may affects the observation. For the Lidar scenes, we compared the modifications of five approaches proposed originally for optical cameras or Kinect measurements. Our results confirmed that efficient person re-identification can be achieved using a single Lidar sensor, even if it produces sparse point clouds. © 2015 IEEE.},
author_keywords={Gait recognition;  Lidar},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yue2015508,
author={Yue, L. and Shen, T.-Z. and Zhang, C. and Zhao, S.-Y. and Du, B.-Z.},
title={Radial-curve-based facial expression recognition},
journal={Journal of Beijing Institute of Technology (English Edition)},
year={2015},
volume={24},
number={4},
pages={508-512},
doi={10.15918/j.jbit1004-0579.201524.0412},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957809484&doi=10.15918%2fj.jbit1004-0579.201524.0412&partnerID=40&md5=4c2775af67311346b70321b4ed6b5401},
affiliation={School of Information and Electronics, Beijing Institute of Technology, Beijing, 100081, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081, China},
abstract={A fully automatic facial-expression recognition (FER) system on 3D expression mesh models was proposed. The system didn't need human interaction from the feature extraction stage till the facial expression classification stage. The features extracted from a 3D expression mesh model were a bunch of radial facial curves to represent the spatial deformation of the geometry features on human face. Each facial curve was a surface line on the 3D face mesh model, begun from the nose tip and ended at the boundary of the previously trimmed 3D face points cloud. Then Euclid distance was employed to calculate the difference between facial curves extracted from the neutral face and each face with different expressions of one person as feature. By employing support vector machine (SVM) as classifier, the experimental results on the well-known 3D-BUFE dataset indicate that the proposed system could better classify the six prototypical facial expressions than state-of-art algorithms. © 2015 Beijing Institute of Technology.},
author_keywords={Euclidean distance;  Facial expression;  Radial curve;  Support vector machine (SVM)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Juefei-Xu20154780,
author={Juefei-Xu, F. and Luu, K. and Savvides, M.},
title={Spartans: Single-sample periocular-based alignment-robust recognition technique applied to non-frontal scenarios},
journal={IEEE Transactions on Image Processing},
year={2015},
volume={24},
number={12},
pages={4780-4795},
doi={10.1109/TIP.2015.2468173},
art_number={7194796},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942426071&doi=10.1109%2fTIP.2015.2468173&partnerID=40&md5=5c4e7251f3f476056056f0adec2a3e4d},
affiliation={CyLab Biometrics Center, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA  15213, United States},
abstract={In this paper, we investigate a single-sample periocular-based alignment-robust face recognition technique that is pose-tolerant under unconstrained face matching scenarios. Our Spartans framework starts by utilizing one single sample per subject class, and generate new face images under a wide range of 3D rotations using the 3D generic elastic model which is both accurate and computationally economic. Then, we focus on the periocular region where the most stable and discriminant features on human faces are retained, and marginalize out the regions beyond the periocular region since they are more susceptible to expression variations and occlusions. A novel facial descriptor, high-dimensional Walsh local binary patterns, is uniformly sampled on facial images with robustness toward alignment. During the learning stage, subject-dependent advanced correlation filters are learned for pose-tolerant non-linear subspace modeling in kernel feature space followed by a coupled max-pooling mechanism which further improve the performance. Given any unconstrained unseen face image, the Spartans can produce a highly discriminative matching score, thus achieving high verification rate. We have evaluated our method on the challenging Labeled Faces in the Wild database and solidly outperformed the state-of-the-art algorithms under four evaluation protocols with a high accuracy of 89.69%, a top score among image-restricted and unsupervised protocols. The advancement of Spartans is also proven in the Face Recognition Grand Challenge and Multi-PIE databases. In addition, our learning method based on advanced correlation filters is much more effective, in terms of learning subject-dependent pose-tolerant subspaces, compared with many well-established subspace methods in both linear and non-linear cases. © 2015 IEEE.},
author_keywords={advanced correlation filters;  alignment robustness;  Periocular-based recognition;  pose tolerance;  singlesample 3D face synthesis;  unconstrained face recognition},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wagner2015,
author={Wagner, D. and Kalischewski, K. and Velten, J. and Kummert, A.},
title={Detection of ascending and descending stairways by surface normal vectors},
journal={2015 IEEE 9th International Workshop on Multidimensional (nD) Systems, nDS 2015},
year={2015},
doi={10.1109/NDS.2015.7332646},
art_number={7332646},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971393508&doi=10.1109%2fNDS.2015.7332646&partnerID=40&md5=09fdb02ff56f88bbedc27be033b1ac76},
affiliation={Faculty of Electrical, Information and Media Engineering, University of Wuppertal, Wuppertal, D-42119, Germany},
abstract={Persons with walking frames are often limited with respect to visual acuity and they are depending on assistance. To give them the possibility to move free and autonomous in their environment an intelligent assistance system is proposed which can be mounted on a walking frame is used to observe the scene in walking direction, and obstacles have to be detected. Especially, stairways represent a high risk of injury if a collapse occurs. For stairway recognition, an algorithm is proposed which estimates normal vectors by using a covariance matrix and this makes it possible to segment the point cloud data provided by the Kinect sensor. The calculation of surface normal vectors of these regions helps to detect ascending and descending stairways. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li201583,
author={Li, H. and Ding, H. and Huang, D. and Wang, Y. and Zhao, X. and Morvan, J.-M. and Chen, L.},
title={An efficient multimodal 2D + 3D feature-based approach to automatic facial expression recognition},
journal={Computer Vision and Image Understanding},
year={2015},
volume={140},
pages={83-92},
doi={10.1016/j.cviu.2015.07.005},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941738207&doi=10.1016%2fj.cviu.2015.07.005&partnerID=40&md5=29b6f6afe28865c0c4433387ebc04ce9},
affiliation={School of Mathematics and Statistics, Xi'An Jiaotong University, Xi'an, China; Ecole Centrale de Lyon, LIRIS UMR5205, Lyon, France; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; School of Management, Xi'An Jiaotong University, Xi'an, China; Université Lyon 1, Institut Camille Jordan, Lyon, France; GMSV Research Center, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia},
abstract={We present a fully automatic multimodal 2D + 3D feature-based facial expression recognition approach and demonstrate its performance on the BU-3DFE database. Our approach combines multi-order gradient-based local texture and shape descriptors in order to achieve efficiency and robustness. First, a large set of fiducial facial landmarks of 2D face images along with their 3D face scans are localized using a novel algorithm namely incremental Parallel Cascade of Linear Regression (iPar-CLR). Then, a novel Histogram of Second Order Gradients (HSOG) based local image descriptor in conjunction with the widely used first-order gradient based SIFT descriptor are used to describe the local texture around each 2D landmark. Similarly, the local geometry around each 3D landmark is described by two novel local shape descriptors constructed using the first-order and the second-order surface differential geometry quantities, i.e., Histogram of mesh Gradients (meshHOG) and Histogram of mesh Shape index (curvature quantization, meshHOS). Finally, the Support Vector Machine (SVM) based recognition results of all 2D and 3D descriptors are fused at both feature-level and score-level to further improve the accuracy. Comprehensive experimental results demonstrate that there exist impressive complementary characteristics between the 2D and 3D descriptors. We use the BU-3DFE benchmark to compare our approach to the state-of-the-art ones. Our multimodal feature-based approach outperforms the others by achieving an average recognition accuracy of 86.32%. Moreover, a good generalization ability is shown on the Bosphorus database. © 2015 Elsevier Inc. All rights reserved.},
author_keywords={Facial expression recognition;  Local shape descriptor;  Local texture descriptor;  Multimodal fusion},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ghiass201525,
author={Ghiass, R.S. and Arandjelovic, O. and Laurendeau, D.},
title={Highly accurate and fully automatic head pose estimation from a low quality consumer-level RGB-D sensor},
journal={HCMC 2015 - Proceedings of the 2nd Workshop on Computational Models of Social Interactions: Human-Computer-Media Communication, co-located with ACM MM 2015},
year={2015},
pages={25-34},
doi={10.1145/2810397.2810401},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964388743&doi=10.1145%2f2810397.2810401&partnerID=40&md5=4a1ae22bfe2e6f774ff7b6768f8410fc},
affiliation={Département de Génie Électrique et de Génie Informatique, Université Laval Québec, Canada; School of Computer Science, University of St. Andrews, United Kingdom},
abstract={In this paper we describe a novel algorithm for head pose estimation from low-quality RGB-D data acquired using a consumer-level device such as Microsoft Kinect. We focus our attention on the wellknown challenges in the processing of depth point-clouds which include spurious data, noise, and missing data caused by occlusion. Our algorithm performs pose estimation by fitting a 3D morphable model which explicitly includes pose parameters. Several important novelties are described. (i) We propose a method for automatic removal of the majority of spurious depth data which uses facial feature detection in the associated RGB image. By back-projecting the corresponding image loci and intersecting them with the 3D point-cloud we construct the facial features plane used to crop the point-cloud. (ii) Both high convergence speed and high fitting accuracy are achieved by formulating the fitting objective function to include both point-to-point and point-to-plane point-cloud matching terms. (iii) The effect of misleading point-cloud matches caused by noisy or missing data is reduced by using the Tukey biweight function as a robust statistic and by employing a re-weighting scheme for different terms in the fitting objective function. (iv) Lastly, the proposed algorithm is evaluated on the standard benchmark Biwi Kinect Head Pose Database on which it is shown to outperform substantially the current state-of-the-art, achieving more than a 20-fold reduction in error estimates of all three Euler angles i.e. yaw, pitch, and roll. A thorough analysis of the results is used both to gain full insight into the behaviour of the described algorithm as well as to highlight important methodological issues which future authors should consider in the evaluation of pose estimation algorithms. © 2015 ACM.},
author_keywords={Depth;  Kinect;  Point cloud;  Robust statistic;  Scanner},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lee20158821,
author={Lee, Y.-H. and Kim, C.G. and Kim, Y. and Whangbo, T.K.},
title={Facial landmarks detection using improved active shape model on android platform},
journal={Multimedia Tools and Applications},
year={2015},
volume={74},
number={20},
pages={8821-8830},
doi={10.1007/s11042-013-1565-y},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941993984&doi=10.1007%2fs11042-013-1565-y&partnerID=40&md5=6563d6c9dcc5bcfa32cb7951bffddc1b},
affiliation={Department of Applied Computer Engineering, Dankook University, 152, Jukgeon-ro, Suji-gu, Yongin-si, Gyeonggi-do  448-701, South Korea; Department of Computer Science, Namseoul University, 91 Daehak-ro, Seongwhaneub, Seobuk-gu, Cheonan, Chungnam, South Korea; Department of Electronic Engineering, Dankook University, 119 Dandae-ro, Dongnam-gu, Cheonan, Chungnam, South Korea; Department of Interactive Media, Gachon University, 1342 Seongnam-daero, Sujeong-gu, Seongnam-si, Gyeonggi-do, South Korea},
abstract={Detection of facial feature is fundamental for applications such as security, biometrics, 3D face modeling and personal authentication. Active Shape Model (ASM) is one of the most popular local texture models for face detection. This paper presents an issue related to face detection based on ASM, and proposes an efficient extraction algorithm for facial landmarks suitable for use on mobile devices. We modifies the original ASM to improve its performance with three changes; (1) Improving the initialization model using the center of the eyes by using a feature map of color information, (2) Constructing modified model definition and fitting more landmarks than the classical ASM, and (3) Extending and building a 2-D profile model for detecting faces in input image. The proposed method is evaluated on dataset containing over 700 images of faces, and experimental results reveal that the proposed algorithm exhibited a significant improvement of over 10.2 % in average success ratio, compared to the classic ASM, clearly outperforming on success rate and computing time. © 2013, Springer Science+Business Media New York.},
author_keywords={Active shape model (ASM);  Face analysis;  Facial feature points;  Facial landmarks},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Park2015239,
author={Park, B.-H. and Oh, S.-Y.},
title={Facial Expression Training System using Bilinear Shape Model},
journal={HAI 2015 - Proceedings of the 3rd International Conference on Human-Agent Interaction},
year={2015},
pages={239-241},
doi={10.1145/2814940.2814985},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962812240&doi=10.1145%2f2814940.2814985&partnerID=40&md5=b0a381250690a7c70eec5b9c77492523},
affiliation={POSTECH, San 31, Hyoja-dong, Nam-gu Pohang, 790-784, South Korea},
abstract={We introduce a facial expression training system using the bilinear shape model which helps people to practice making a facial expression. The user face on the camera preview screen is reconstructed into a 3D face model and the model is transformed to a blend shape model which represents the facial expressions. This way, the system can precisely analyze the facial expression of users. With a target 3D face model appearing on the screen, the 3D face model changes its facial expression, which leads the user to change his facial expression to become look like same. The system recognizes whether the facial expression of the user is the same as the one of 3D face models. As the system gives the various missions to users to change his facial expression, they can practice facial expressions. It can be used for Bell's palsy patients who need face rehabilitation exercise or those who need to practice unique facial expressions such as stewardess smile or facial mimicry. © 2015 ACM.},
author_keywords={Augmented Reality;  Computer Vision;  Facial Expression;  Game;  Machine Learning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Polewski201510,
author={Polewski, P. and Yao, W. and Heurich, M. and Krzystek, P. and Stilla, U.},
title={Active learning approach to detecting standing dead trees from ALS point clouds combined with aerial infrared imagery},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2015},
volume={2015-October},
pages={10-18},
doi={10.1109/CVPRW.2015.7301378},
art_number={7301378},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951994655&doi=10.1109%2fCVPRW.2015.7301378&partnerID=40&md5=07f6c8777959af0595ddf51a20db7f94},
affiliation={Hochschule München, München, 80333, Germany; Bavarian Forest National Park, Grafenau, 94481, Germany; Technische Universität München, München, 80333, Germany},
abstract={Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89% with less than 100 training examples, which corresponds to 10% of the pre-selected data pool. © 2015 IEEE.},
author_keywords={Entropy;  Feature extraction;  Image segmentation;  Logistics;  Three-dimensional displays;  Training;  Vegetation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Patil2015393,
author={Patil, H. and Kothari, A. and Bhurchandi, K.},
title={3-D face recognition: features, databases, algorithms and challenges},
journal={Artificial Intelligence Review},
year={2015},
volume={44},
number={3},
pages={393-441},
doi={10.1007/s10462-015-9431-0},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941419993&doi=10.1007%2fs10462-015-9431-0&partnerID=40&md5=c90b8821f623bcb40665d0010a316ef6},
affiliation={Visvesvaraya National Institute of Technology, Nagpur, India},
abstract={Face recognition is being widely accepted as a biometric technique because of its non-intrusive nature. Despite extensive research on 2-D face recognition, it suffers from poor recognition rate due to pose, illumination, expression, ageing, makeup variations and occlusions. In recent years, the research focus has shifted toward face recognition using 3-D facial surface and shape which represent more discriminating features by the virtue of increased dimensionality. This paper presents an extensive survey of recent 3-D face recognition techniques in terms of feature detection, classifiers as well as published algorithms that address expression and occlusion variation challenges followed by our critical comments on the published work. It also summarizes remarkable 3-D face databases and their features used for performance evaluation. Finally we suggest vital steps of a robust 3-D face recognition system based on the surveyed work and identify a few possible directions for research in this area. © 2015, Springer Science+Business Media Dordrecht.},
author_keywords={3-D Face databases;  3-D faces;  Biometrics;  Classifiers;  Face matching;  Face recognition;  Feature extraction},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hassner20154295,
author={Hassner, T. and Harel, S. and Paz, E. and Enbar, R.},
title={Effective face frontalization in unconstrained images},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2015},
volume={07-12-June-2015},
pages={4295-4304},
doi={10.1109/CVPR.2015.7299058},
art_number={7299058},
note={cited By 203},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942466590&doi=10.1109%2fCVPR.2015.7299058&partnerID=40&md5=2a8796db149f5cb27f13805aaf9327bf},
affiliation={Open University of Israel, Ra'anana, 4353701, Israel; Adience, Tel-Aviv, Israel},
abstract={'Frontalization' is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. We observe that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, we explore the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces. We show that this leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gilani20154639,
author={Gilani, S.Z. and Shafait, F. and Mian, A.},
title={Shape-based automatic detection of a large number of 3D facial landmarks},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2015},
volume={07-12-June-2015},
pages={4639-4648},
doi={10.1109/CVPR.2015.7299095},
art_number={7299095},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959230238&doi=10.1109%2fCVPR.2015.7299095&partnerID=40&md5=f8501e146cd433e762260b5d717f9377},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, Australia},
abstract={We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morphologically significant. The proposed algorithm evolves level set curves with adaptive geometric speed functions to automatically extract effective seed points for dense correspondence. Correspondences are established by minimizing the bending energy between patches around seed points of given faces to those of a reference face. Given its hierarchical structure, our algorithm is capable of establishing thousands of correspondences between a large number of faces. Finally, a morphable model based on the dense corresponding points is fitted to an unseen query face for transfer of correspondences and hence automatic detection of landmarks. The proposed algorithm can detect any number of pre-defined landmarks including subtle landmarks that are even difficult to detect manually. Extensive experimental comparison on two benchmark databases containing 6, 507 scans shows that our algorithm outperforms six state of the art algorithms. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu20153013,
author={Liu, W. and Ji, R. and Li, S.},
title={Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2015},
volume={07-12-June-2015},
pages={3013-3021},
doi={10.1109/CVPR.2015.7298920},
art_number={7298920},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959255873&doi=10.1109%2fCVPR.2015.7298920&partnerID=40&md5=3fca9fb092063d91c91379555c56b582},
affiliation={Dep. of Cognitive Science, School of Info. Science and Eng., Xiamen University, China; Fujian Key Lab for Brain-Like Intelligent Systems, China},
abstract={Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Starke201595,
author={Starke, S. and Hendrich, N. and Bistry, H. and Zhang, J.},
title={Fast and robust detection and tracking of multiple persons on RGB-D data fusing spatio-temporal information},
journal={IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},
year={2015},
volume={2015-October},
pages={95-101},
doi={10.1109/MFI.2015.7295752},
art_number={7295752},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951861796&doi=10.1109%2fMFI.2015.7295752&partnerID=40&md5=28e5e4a059d76aa228a15fa375d4fcf9},
affiliation={Dept. of Informatics, University of Hamburg, Germany},
abstract={In this paper, we present an efficient and adaptive method for detecting and tracking multiple persons while providing real-time capability and high robustness to outlier noise. Given an RGB-D image data sequence, our algorithm combines two independent approaches for person detection. First, a cluster-based segmentation and classification on RGB-D point clouds and second a face detection on RGB images, where each method itself is post-processed by spatio-temporal filtering for tracking and sensitivity purposes. Our analysis and experimental results prove that the combined approach performs significantly better than the individual solutions and greatly reduces the number of false positives in situations where one detector fails. © 2015 IEEE.},
author_keywords={Computer Vision;  Face Detection;  Information Fusion;  Mobile Robotics;  Pattern Recognition;  Person Detection;  RGB-D Data;  Tracking},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kankare2015161,
author={Kankare, V. and Liang, X. and Vastaranta, M. and Yu, X. and Holopainen, M. and Hyyppä, J.},
title={Diameter distribution estimation with laser scanning based multisource single tree inventory},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2015},
volume={108},
pages={161-171},
doi={10.1016/j.isprsjprs.2015.07.007},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939156385&doi=10.1016%2fj.isprsjprs.2015.07.007&partnerID=40&md5=d1a540071f1e3615a2a7c3bb45322600},
affiliation={University of Helsinki, Department of Forest Sciences, Finland; Centre of Excellence in Laser Scanning Research, Finnish Geodetic Institute, Masala, FI-02431, Finland; Finnish Geodetic Institute, Department of Remote Sensing and Photogrammetry, Finland},
abstract={Tree detection and tree species recognition are bottlenecks of the airborne remote sensing-based single tree inventories. The effect of these factors in forest attribute estimation can be reduced if airborne measurements are aided with tree mapping information that is collected from the ground. The main objective here was to demonstrate the use of terrestrial laser scanning-derived (TLS) tree maps in aiding airborne laser scanning-based (ALS) single tree inventory (multisource single tree inventory, MS-STI) and its capability in predicting diameter distribution in various forest conditions. Automatic measurement of TLS point clouds provided the tree maps and the required reference information from the tree attributes. The study area was located in Evo, Finland, and the reference data was acquired from 27 different sample plots with varying forest conditions. The workflow of MS-STI included: (1) creation of automatic tree map from TLS point clouds, (2) automatic diameter at breast height (DBH) measurement from TLS point clouds, (3) individual tree detection (ITD) based on ALS, (4) matching the ITD segments to the field-measured reference, (5) ALS point cloud metric extraction from the single tree segments and (6) DBH estimation based on the derived metrics. MS-STI proved to be accurate and efficient method for DBH estimation and predicting diameter distribution. The overall accuracy (root mean squared error, RMSE) of the DBH was 36.9. mm. Results showed that the DBH accuracy decreased if the tree density (trees/ha) increased. The highest accuracies were found in old-growth forests (tree densities less than 500 stems/ha). MS-STI resulted in the best accuracies regarding Norway spruce (. Picea abies (L.) H. Karst.)-dominated forests (RMSE of 29.9. mm). Diameter distributions were predicted with low error indices, thereby resulting in a good fit compared to the reference. Based on the results, diameter distribution estimation with MS-STI is highly dependent on the forest structure and the accuracy of the tree maps that are used. The most important development step in the future for the MS-STI and automatic measurements of the TLS point cloud is to develop tree species recognition methods and further develop tree detection techniques. The possibility of using MLS or harvester data as a basis for the required tree maps should also be assessed in the future. © 2015 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author_keywords={ALS;  Diameter distribution;  Multisource;  Remote sensing;  TLS},
document_type={Article},
source={Scopus},
}

@ARTICLE{Moeini2015,
author={Moeini, A. and Faez, K. and Moeini, H.},
title={Face recognition across makeup and plastic surgery from real-world images},
journal={Journal of Electronic Imaging},
year={2015},
volume={24},
number={5},
doi={10.1117/1.JEI.24.5.053028},
art_number={053028},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946137315&doi=10.1117%2f1.JEI.24.5.053028&partnerID=40&md5=eb4607189d3e9ec16ca8381fe3208d44},
affiliation={Amirkabir University of Technology, Department of Electrical Engineering, Hafez Avenue, Tehran, 15914, Iran; Semnan University, Department of Electrical Engineering, Daneshgah Avenue, Semnan, 35131, Iran},
abstract={A study for feature extraction is proposed to handle the problem of facial appearance changes including facial makeup and plastic surgery in face recognition. To extend a face recognition method robust to facial appearance changes, features are individually extracted from facial depth on which facial makeup and plastic surgery have no effect. Then facial depth features are added to facial texture features to perform feature extraction. Accordingly, a three-dimensional (3-D) face is reconstructed from only a single two-dimensional (2-D) frontal image in real-world scenarios. Then the facial depth is extracted from the reconstructed model. Afterward, the dual-tree complex wavelet transform (DT-CWT) is applied to both texture and reconstructed depth images to extract the feature vectors. Finally, the final feature vectors are generated by combining 2-D and 3-D feature vectors, and are then classified by adopting the support vector machine. Promising results have been achieved for makeup-invariant face recognition on two available image databases including YouTube makeup and virtual makeup, and plastic surgery-invariant face recognition on a plastic surgery face database is compared to several state-of-the-art feature extraction methods. Several real-world scenarios are also planned to evaluate the performance of the proposed method on a combination of these three databases with 1102 subjects. © 2015 SPIE and IS&T.},
author_keywords={face recognition;  facial makeup;  facial plastic surgery;  facial three-dimensional depth reconstruction;  real world},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bentaieb2015,
author={Bentaieb, S. and Ouamri, A. and Keche, M.},
title={Nose tip localization on a three dimensional face across pose, expressions and occlusions variations in a Riemannian context},
journal={3rd International Conference on Control, Engineering and Information Technology, CEIT 2015},
year={2015},
doi={10.1109/CEIT.2015.7233159},
art_number={7233159},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960156406&doi=10.1109%2fCEIT.2015.7233159&partnerID=40&md5=2bd7ab45e70081a32aa5fe81b4bb16f0},
affiliation={Signals and Images Laboratory, University of USTOMB, Oran, Algeria},
abstract={Nose tip localization is an important step for registration, preprocessing and recognition of 3D face data. In this paper, we propose a new approach for the nose tip detection that is robust to pose and expression variations and in presence of occlusions. From a rotated 3D face, we extract facial curves that are matched to a profile curve model. An optimal matching using the Riemannian geometry, based on the Elastic Shape Analysis is performed to obtain the accurate nose tip. The proposed method requires no training and can locate the nose tip in less than 6 seconds. Experiments are performed on the Bosphorus database. Quantitative analysis and comparison with the ground truth locations are provided. The results confirm that our approach achieves 97.68% with error no larger than 12 mm and 98.19% within 20 mm. © 2015 IEEE.},
author_keywords={Nose Tip Localization;  Shape Analysis;  SRVF},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kopinski2015336,
author={Kopinski, T. and Magand, S. and Gepperth, A. and Handmann, U.},
title={A light-weight real-time applicable hand gesture recognition system for automotive applications},
journal={IEEE Intelligent Vehicles Symposium, Proceedings},
year={2015},
volume={2015-August},
pages={336-342},
doi={10.1109/IVS.2015.7225708},
art_number={7225708},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949810838&doi=10.1109%2fIVS.2015.7225708&partnerID=40&md5=4fd7e4d7ee4b106cf99188a969570b3e},
affiliation={Department of Informatics, University Ruhr West, Germany; ENSTA ParisTech, Germany},
abstract={We present a novel approach for improved hand-gesture recognition by a single time-of-flight(ToF) sensor in an automotive environment. As the sensor's lateral resolution is comparatively low, we employ a learning approach comprising multiple processing steps, including PCA-based cropping, the computation of robust point cloud descriptors and training of a Multilayer perceptron (MLP) on a large database of samples. A sophisticated temporal fusion technique boosts the overall robustness of recognition by taking into account data coming from previous classification steps. Overall results are very satisfactory when evaluated on a large benchmark set of ten different hand poses, especially when it comes to generalization on previously unknown persons. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Deng20155509,
author={Deng, X. and Da, F. and Shao, H.},
title={Expression-robust 3D face recognition using region-based multiscale wavelet feature fusion},
journal={Journal of Computational Information Systems},
year={2015},
volume={11},
number={15},
pages={5509-5517},
doi={10.12733/jcis14953},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950271521&doi=10.12733%2fjcis14953&partnerID=40&md5=da8d93edfc030808fb309b097e409e94},
affiliation={Department of Automation, Southeast University, Nanjing, 210096, China; Key Laboratory of Measurement and Control for Complex System of Ministry of Education, Southeast University, Nanjing, 210096, China},
abstract={In order to eliminate the impact of facial expressions and improve the efficiency of calculation, this paper proposes a novel expression-robust 3D face recognition algorithm using region-based feature fusion technique based on multiscale wavelet transformations. The discrete wavelet transformation is applied to extract frequency component features of geometric image based on the semi-rigid face region as well as the non-rigid face region in order to reduce the influence from the facial expression using the Coherent Point Drift non-rigid point set registration. The dimensionality reduction methods are utilized to promote the computational efficiency, and the experimental results show that our algorithm outperforms state-of-the-art methods based on FRGC v2.0. Copyright © 2015 Binary Information Press.},
author_keywords={3D face recognition;  Dimensionality reduction;  Multiscale wavelet transform;  Non-rigid point set registration},
document_type={Article},
source={Scopus},
}

@ARTICLE{Vicente20152014,
author={Vicente, F. and Huang, Z. and Xiong, X. and De La Torre, F. and Zhang, W. and Levi, D.},
title={Driver Gaze Tracking and Eyes off the Road Detection System},
journal={IEEE Transactions on Intelligent Transportation Systems},
year={2015},
volume={16},
number={4},
pages={2014-2027},
doi={10.1109/TITS.2015.2396031},
art_number={7053946},
note={cited By 71},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027940377&doi=10.1109%2fTITS.2015.2396031&partnerID=40&md5=c7484bbebd4b8ab4ab6aa69e20d74204},
affiliation={Carnegie Mellon University, Pittsburgh, PA  15213-3815, United States; General Motors Company, Warren, MI  48090, United States; General Motors Company, Herzliya, 46733, Israel},
abstract={Distracted driving is one of the main causes of vehicle collisions in the United States. Passively monitoring a driver's activities constitutes the basis of an automobile safety system that can potentially reduce the number of accidents by estimating the driver's focus of attention. This paper proposes an inexpensive vision-based system to accurately detect Eyes Off the Road (EOR). The system has three main components: 1) robust facial feature tracking; 2) head pose and gaze estimation; and 3) 3-D geometric reasoning to detect EOR. From the video stream of a camera installed on the steering wheel column, our system tracks facial features from the driver's face. Using the tracked landmarks and a 3-D face model, the system computes head pose and gaze direction. The head pose estimation algorithm is robust to nonrigid face deformations due to changes in expressions. Finally, using a 3-D geometric analysis, the system reliably detects EOR. © 2000-2011 IEEE.},
author_keywords={Driver monitoring system;  eyes off the road detection;  gaze estimation;  Head pose estimation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang20151,
author={Wang, C. and Cho, Y.K. and Kim, C.},
title={Automatic BIM component extraction from point clouds of existing buildings for sustainability applications},
journal={Automation in Construction},
year={2015},
volume={56},
pages={1-13},
doi={10.1016/j.autcon.2015.04.001},
note={cited By 58},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928163215&doi=10.1016%2fj.autcon.2015.04.001&partnerID=40&md5=7d2fa77c5aa1a0ff811ffd97ed438e3f},
affiliation={School of Civil and Environmental Engineering, Georgia Institute of Technology, 790 Atlantic Drive, Atlanta, GA  30332-0355, United States; Department of Architectural Engineering, School of Engineering, Chung-Ang University, 221 Heuksuk-dong, Dongjak-gu, Seoul, 156-756, South Korea},
abstract={Building information models (BIMs) are increasingly being applied throughout a building's lifecycle for various applications, such as progressive construction monitoring and defect detection, building renovation, energy simulation, and building system analysis in the Architectural, Engineering, Construction, and Facility Management (AEC/FM) domains. In conventional approaches, as-is BIM is primarily manually created from point clouds, which is labor-intensive, costly, and time consuming. This paper proposes a method for automatically extracting building geometries from unorganized point clouds. The collected raw data undergo data downsizing, boundary detection, and building component categorization, resulting in the building components being recognized as individual objects and their visualization as polygons. The results of tests conducted on three collected as-is building data to validate the technical feasibility and evaluate the performance of the proposed method indicate that it can simplify and accelerate the as-is building model from the point cloud creation process. © 2015 Elsevier B.V.F.},
author_keywords={As-is model;  BIM;  Energy simulation;  Laser scanner;  Object recognition;  Point cloud},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Casas2015,
author={Casas, D. and Alexander, O. and Feng, A.W. and Fyffe, G. and Ichikari, R. and Debevec, P. and Wang, R. and Suma, E. and Shapiro, A.},
title={Blendshapes from commodity RGB-D sensors},
journal={ACM SIGGRAPH 2015 Talks, SIGGRAPH 2015},
year={2015},
doi={10.1145/2775280.2792540},
art_number={a33},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957944664&doi=10.1145%2f2775280.2792540&partnerID=40&md5=a6145b3a3a02581e6511e8db8eed5d93},
affiliation={Institute for Creative Technologies, United States; University of Southern California, United States},
abstract={Creating and animating a realistic 3D human face is an important task in computer graphics. The capability of capturing the 3D face of a human subject and reanimate it quickly will find many applications in games, training simulations, and interactive 3D graphics. We demonstrate a system to capture photorealistic 3D faces and generate the blendshape models automatically using only a single commodity RGB-D sensor. Our method can rapidly generate a set of expressive facial poses from a single depth sensor, such as a Microsoft Kinect version 1, and requires no artistic expertise in order to process those scans. The system takes only a matter of seconds to capture and produce a 3D facial pose and only requires a few minutes of processing time to transform it into a blendshape-compatible model. Our main contributions include an end-to-end pipeline for capturing and generating face blendshape models automatically, and a registration method that solves dense correspondences between two face scans by utilizing facial landmarks detection and optical flows. We demonstrate the effectiveness of the proposed method by capturing different human subjects and puppeteering their 3D faces in an animation system with real-time facial performance retargeting.},
author_keywords={Blendshapes;  Depth sensors;  Face animation;  RGB-D},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nozawa2015,
author={Nozawa, N. and Kuwahara, D. and Morishima, S.},
title={3D face reconstruction from a single non-frontal face image},
journal={ACM SIGGRAPH 2015 Posters, SIGGRAPH 2015},
year={2015},
doi={10.1145/2787626.2792634},
art_number={a57},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959330987&doi=10.1145%2f2787626.2792634&partnerID=40&md5=d3534ef5921756d0b1719e1260e7ba9d},
affiliation={Waseda University, Japan; Waseda Research Institute for Science and Engineering, Japan},
abstract={A reconstruction of a human face shape from a single image is an important theme for criminal investigation such as recognition of suspected people from surveillance cameras with only a few frames. It is, however, still difficult to recover a face shape from a non-frontal face image. Method using shading cues on a face depends on the lighting circumstance and cannot be adapted to images in which shadows occurs, for example [Kemelmacher et al. 2011]. On the other hand, [Blanz et al. 2004] reconstructed a shape by 3D Morphable Model (3DMM) only with facial feature points. This method, however, requires the pose-wise correspondences of vertices in the model to feature points of input image because a face contour cannot be seen when the facial direction is not the front. In this paper, we propose a method which can reconstruct a facial shape from a non-frontal face image only with a single general correspondence table. Our method searches for the correspondences of points on a facial contour in the iterative reconstruction process, and makes the reconstruction simple and stable.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu2015,
author={Wu, Z. and Li, J. and Hu, J. and Deng, W.},
title={Pose-invariant face recognition using 3D multi-depth generic elastic models},
journal={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
year={2015},
doi={10.1109/FG.2015.7163148},
art_number={7163148},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944936819&doi=10.1109%2fFG.2015.7163148&partnerID=40&md5=278ecaac5b5e8f327f22655de729b82a},
affiliation={School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China},
abstract={In order to handle pose variation problem in face recognition, Generic Elastic Models (GEMs) was proposed as a low computational and efficient 3D face modeling method, which generates 3D face model from single frontal face image by elastically deforming a generic 3D depth map based on 2D observations of the input face image. In this paper, we extend GEMs to Multi-Depth GEMs (MD-GEMs) by utilizing multiple generic depth maps which merely vary in depth linearly in process of 3D face modeling, taking the assumption that face depth variation across individuals can be modeled by a linear transformation of generic depth map. Multiple 3D models are generated for each input frontal face. In recognition, the galleries are the 3D models constructed from the frontal face of each ID while the probe is a non-frontal face. The pose of input non-frontal face is estimated by a linear regression method and 3D models in the constructed database are rotated and rendered at the estimated pose. Corresponding 2D images are synthesized after 2D projection. After face alignment, the distances between the input image and synthesized images are calculated by a normalized correlation measure and thus the corresponding identity in the database is matched. Experiments on Multi-PIE verify the effectiveness of MD-GEMs on handling pose variation problem in face recognition. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jeni2015,
author={Jeni, L.A. and Girard, J.M. and Cohn, J.F. and Kanade, T.},
title={Real-time dense 3D face alignment from 2D video with automatic facial action unit coding},
journal={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
year={2015},
doi={10.1109/FG.2015.7163165},
art_number={7163165},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944930280&doi=10.1109%2fFG.2015.7163165&partnerID=40&md5=96ab861ac1cfd10b951e2c232f2dd041},
affiliation={Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Department of Psychology, University of Pittsburgh, Pittsburgh, PA, United States},
abstract={Face alignment is the problem of automatically locating detailed facial landmarks across different subjects, illuminations, and viewpoints. Previous methods can be divided into two broad categories. 2D-based methods locate a relatively small number of 2D fiducial points in real time while 3D-based methods fit a high-resolution 3D model offline at a much higher computational cost. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jeni2015,
author={Jeni, L.A. and Cohn, J.F. and Kanade, T.},
title={Dense 3D face alignment from 2D videos in real-time},
journal={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
year={2015},
doi={10.1109/FG.2015.7163142},
art_number={7163142},
note={cited By 73},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944909690&doi=10.1109%2fFG.2015.7163142&partnerID=40&md5=2a8673e53dfb91aa54f6657865da9fdb},
affiliation={Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Department of Psychology, University of Pittsburgh, Pittsburgh, PA, United States},
abstract={To enable real-time, person-independent 3D registration from 2D video, we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60 degrees. From a single 2D image of a person's face, a dense 3D shape is registered in real time for each frame. The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression. The algorithm first estimates the location of a dense set of markers and their visibility, then reconstructs face shapes by fitting a part-based 3D model. Because no assumptions are required about illumination or surface properties, the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video. The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction. Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video. The software is available online at http://zface.org. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheng2015,
author={Cheng, S. and Marras, I. and Zafeiriou, S. and Pantic, M.},
title={Active nonrigid ICP algorithm},
journal={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
year={2015},
doi={10.1109/FG.2015.7163161},
art_number={7163161},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944909111&doi=10.1109%2fFG.2015.7163161&partnerID=40&md5=485b3afd7b2a80b7654565e4be5c3615},
affiliation={Department of Computing, Imperial College London, United Kingdom; EEMCS, University of Twente, Netherlands},
abstract={The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15-20 years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yang2015,
author={Yang, X. and Huang, D. and Wang, Y. and Chen, L.},
title={Automatic 3D facial expression recognition using geometric scattering representation},
journal={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
year={2015},
doi={10.1109/FG.2015.7163090},
art_number={7163090},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944929596&doi=10.1109%2fFG.2015.7163090&partnerID=40&md5=a37d5ac157cc57929768c4e43e84b371},
affiliation={IRIP Lab, Sino-French Engineering School, Beihang University, Beijing, 100191, China; IRIP Lab, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Department of Mathematics and Computer Science, Ecole Centrale de Lyon, CNRS, Lyon, 69134, France},
abstract={Facial Expression Recognition (FER) is one of the most active topics in the domain of computer vision and pattern recognition, and it has received increasing attention for its wide application potentials as well as attractive scientific challenges. In this paper, we present a novel method to automatic 3D FER based on geometric scattering representation. A set of maps of shape features in terms of multiple order differential quantities, i.e. the Normal Maps (NOM) and the Shape Index Maps (SIM), are first jointly adopted to comprehensively describe geometry attributes of the facial surface. The scattering operator is then introduced to further highlight expression related cues on these maps, thereby constructing geometric scattering representations of 3D faces for classification. The scattering descriptor not only encodes distinct local shape changes of various expressions as by several milestone descriptors, such as SIFT, HOG, etc., but also captures subtle information hidden in high frequencies, which is quite crucial to better distinguish expressions that are easily confused. We evaluate the proposed approach on the BU-3DFE database, and the performance is up to 84.8% and 82.7% with two commonly used protocols respectively which is superior to the state of the art ones. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shirke20151582,
author={Shirke, V.D. and Gawande, U.},
title={Generation of 3D face views from artist drawn sketch: A review},
journal={2015 International Conference on Industrial Instrumentation and Control, ICIC 2015},
year={2015},
pages={1582-1586},
doi={10.1109/IIC.2015.7151002},
art_number={7151002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941139457&doi=10.1109%2fIIC.2015.7151002&partnerID=40&md5=0f98f2980ecc5b4a653c815f1befa802},
affiliation={Department of Computer Technology, YCCE, Nagpur, Maharashtra, India},
abstract={Approach to construct 3D face model from an artist drawn sketch is an area of interest in image processing from last few decades. It has various application like police investigation, 3D cartoon modeling. From an individual's sketch it is possible to construct 3D face model using various techniques. To construct 3D face views, from the individuals sketch steps required are 2D landmark detection, 3D landmark estimation, surface and texture synthesis with reference to 3D morphable model. This system is beneficial for the purpose of increasing the identification accuracy of the persons whose photographs are not available. In this paper, we tend to surveyed different techniques to construct 3D face views from artist drawn sketch. © 2015 IEEE.},
author_keywords={Active appearance Model;  Active Shape Model;  MeshIK;  Principal Component Analysis;  Shape from Shading},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schumacher201549,
author={Schumacher, M. and Piotraschke, M. and Blanz, V.},
title={Hallucination of facial details from degraded images using 3D face models},
journal={Image and Vision Computing},
year={2015},
volume={40},
pages={49-64},
doi={10.1016/j.imavis.2015.06.004},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935466420&doi=10.1016%2fj.imavis.2015.06.004&partnerID=40&md5=27b647c8dcd456f6e54155f0ee0a2657},
affiliation={Institute for Vision and Graphics, University of Siegen, Germany},
abstract={The goals of this paper are: (1) to enhance the quality of images of faces, (2) to enable 3D Morphable Models (3DMMs) to cope with severely degraded images, and (3) to reconstruct textured 3D faces with details that are not in the input images. Details that are lost in the input images due to blur, low resolution or occlusions, are filled in by the 3DMM and an additional texture enhancement algorithm that adds high-resolution details from example faces. By leveraging class-specific knowledge, this restoration process goes beyond what general image operations such as deblurring or inpainting can achieve. The benefit of the 3DMM for image restoration is that it can be applied to any pose and illumination, unlike image-based methods. However, it is only with the new fitting algorithm that 3DMMs can produce realistic faces from severely degraded images. The new method includes the blurring or downsampling operator explicitly into the analysis-by-synthesis algorithm. © 2015 Elsevier B.V. All rights reserved.},
author_keywords={3D models;  Face hallucination;  Faces;  Model-based deblurring;  Occlusions},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ganguly2015,
author={Ganguly, S. and Bhattacharjee, D. and Nasipuri, M.},
title={Register-My-Face: A tool to register three-dimensional face images},
journal={Journal of Electronic Imaging},
year={2015},
volume={24},
number={4},
doi={10.1117/1.JEI.24.4.043007},
art_number={043007},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938875249&doi=10.1117%2f1.JEI.24.4.043007&partnerID=40&md5=7914e95c86f641146856d21584357f7a},
affiliation={Jadavpur University, Department of Computer Science and Engineering, 188, Raja Subodh Chandra Mullick Road, Jadavpur, Kolkata, West Bengal, 700032, India},
abstract={Efficient registration across pose is the most challenging research area for accurate recognition of human face images. Authors have discussed a tool that has been developed for registration of face images across poses using the nose tip of the face images. The nose tip has been considered here because of its stability in situations such as variations in pose, expression, etc. The aim of this investigation is to develop a face registration tool called "Register-My-Face" with a working methodology in all the three directions, namely yaw, pitch, and roll. This tool has been developed for three-dimensional (3-D) face registration, which is inspired by analyzing the "depth values" of face range images. The registration of the face is done using a geometrical technique which is based on computing the corresponding rotation in three orthogonal directions. The advantages of the designed tool are that it does not need any training phase for accurate detection of the nose tip, and this method can handle large pose variations, including 90 deg pose variations about the Y-axis in both the positive and negative directions. The method that has been followed to develop this tool is also independent of facial expression, occlusion and illumination variations. Moreover, it quickly detects the nose tip because it does not need to process the entire face surface, but only requires the isolated nose region. The tool has been integrated with three different databases; GavabDB, Bosphorus, and Frav3D, and the investigation highlights the robustness of the tool. Additionally, for exploring the performance of the tool, a SIMULINK model for hardware interface is also developed with a discrete solver and is tested on two different configuration setups and executed in two different execution modes with two simulation stop timings 10.0 and 1.0. This model can proceed according to the algorithm with a minimum of 6.640 s to register an unregistered raw 3-D face scan input image from the Frav3D database. Accuracies of the nose region of 98.87%, 94.44%, and 98.08% for the Frav3D, GavabDB, and Bosphorus databases, respectively, are observed. For nose tip detection, the success rates are 98.91% for the Frav3D database, 98.74% for the GavabDB database, and 96.03% for the Bosphorus database. Based on the success rate of nose tip detection, the registration process is implemented on three databases. Registration accuracy, computed between a neutral and the registered range face image for the Frav3D database is 87.5% for GavabDB and 89.87% for Bosphorus database, and the rate of success is 70.23%. © 2015 SPIE and IS&T.},
author_keywords={3-D face image depth value;  face registration;  nose tip detection;  Otsu method;  range image;  SIMULINK model},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li20153739,
author={Li, K. and Zeng, D. and Zhang, J. and Lin, R. and Gao, L. and Liao, X.},
title={A real sense 3D face reconstruction system based on multi-view stereo vision},
journal={Journal of Information and Computational Science},
year={2015},
volume={12},
number={10},
pages={3739-3753},
doi={10.12733/jics20106046},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937578970&doi=10.12733%2fjics20106046&partnerID=40&md5=7130d5d45220a0ddafde996ead128596},
affiliation={Key Laboratory for NeuroInformation of Ministry of Education, University of Electronic Science and Technology of China, Chengdu, 611731, China; School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, 611731, China},
abstract={This paper proposed a system for a real 3D facial reconstruction method based on multi-view stereo vision generated using an orthographic 3D model. Multi-view stereopsis is an effective technology for expanding perspective and reducing noise. The presented multi-view stereo vision is implemented as a multi-view stereo vision using calibration, stereo matching, combining, reconstruction and texture mapping procedures. This paper makes a systemic contribution and two technical contributions. Its systemic contribution is that it demonstrates a state-of-the-art passive stereo vision system for meaningful orthographic 3D facial reconstruction. We have tested our method on various subjects, including two actors and an artificial plastic human head model. The primary technical contribution is an algorithm that combines stereo vision data based on clustering, which includes the following procedures: outlier detection, filling and data combining. The second contribution is a multi-view calibration method, which results in an orthographic position model. An orthographic position is an important requirement and is useful in various medical fields, e. g., maxillofacial surgery research. In addition, we describe a cluster sampling algorithm for undersampling point clouds. The presented sampling algorithm keeps data on an average distribution. A quantitative evaluation proves that the system is an effective, low-cost and high-resolution solution to reconstructing a 3D face model without markings or structured light. 1548-7741/Copyright © 2015 Binary Information Press},
author_keywords={3D face reconstruction;  Multi-view stereo vision;  Orthographisch 3D model},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mejia201526,
author={Mejia, S. and Romero, M.},
title={A Survey on 3D Face Modeling},
journal={Proceedings - 2014 IEEE International Conference on Mechatronics, Electronics, and Automotive Engineering, ICMEAE 2014},
year={2015},
pages={26-31},
doi={10.1109/ICMEAE.2014.31},
art_number={7120841},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946121780&doi=10.1109%2fICMEAE.2014.31&partnerID=40&md5=d6e8fc3834ed7e3da28edf181caf6ff8},
affiliation={Universidad Autónoma Del Estado de México, Mexico},
abstract={In the advance of science, different disciplines are interested to study the human body, among of them we have the facial anthropometry, which studies the human face's dimension and proportions by detecting what is called anthropometric landmarks over the facial surface. Such facial landmarks can be detected by using either direct or indirect techniques. Within a 3D face image it is generally know that the most distinctive facial landmarks are the inner-corners and the tip of the nose, although their appearance varies among human races. Hence, most face processing application generally relay on a face model for comparison and analysis based on such anthropometric landmarks. This paper presents a survey on 3D face modeling, which in the best of our knowledge is missed in relate literature and it is necessary to propose a new 3D facial model, and such model is a primary step in any face processing application. In particular, we are aim to produce a useful 3D face model for research in key areas based on Mexican anthropometric analysis. © 2014 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kong20151801,
author={Kong, S.G. and Mbouna, R.O.},
title={Head Pose Estimation from a 2D Face Image Using 3D Face Morphing With Depth Parameters},
journal={IEEE Transactions on Image Processing},
year={2015},
volume={24},
number={6},
pages={1801-1808},
doi={10.1109/TIP.2015.2405483},
art_number={7045610},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927144375&doi=10.1109%2fTIP.2015.2405483&partnerID=40&md5=2810e3d32d62996553fd0e0e7065f9ba},
affiliation={Department of Computer Engineering, Sejong University, Seoul, South Korea; Department of Electrical and Computer Engineering, Temple University, Philadelphia, PA  19122, United States},
abstract={This paper presents estimation of head pose angles from a single 2D face image using a 3D face model morphed from a reference face model. A reference model refers to a 3D face of a person of the same ethnicity and gender as the query subject. The proposed scheme minimizes the disparity between the two sets of prominent facial features on the query face image and the corresponding points on the 3D face model to estimate the head pose angles. The 3D face model used is morphed from a reference model to be more specific to the query face in terms of the depth error at the feature points. The morphing process produces a 3D face model more specific to the query image when multiple 2D face images of the query subject are available for training. The proposed morphing process is computationally efficient since the depth of a 3D face model is adjusted by a scalar depth parameter at feature points. Optimal depth parameters are found by minimizing the disparity between the 2D features of the query face image and the corresponding features on the morphed 3D model projected onto 2D space. The proposed head pose estimation technique was evaluated on two benchmarking databases: 1) the USF Human-ID database for depth estimation and 2) the Pointing'04 database for head pose estimation. Experiment results demonstrate that head pose estimation errors in nodding and shaking angles are as low as 7.93° and 4.65° on average for a single 2D input face image. © 1992-2012 IEEE.},
author_keywords={3D face model;  depth estimation;  feature disparity minimization;  Head pose estimation;  morphing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20151817,
author={Zhang, C. and Gu, Y.-Z. and Hu, K.-L. and Wang, Y.-G.},
title={Face recognition using SIFT features under 3D meshes},
journal={Journal of Central South University},
year={2015},
volume={22},
number={5},
pages={1817-1825},
doi={10.1007/s11771-015-2700-x},
art_number={2700},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930008100&doi=10.1007%2fs11771-015-2700-x&partnerID=40&md5=569ac008f1ef201b7fcbffd9e0faa163},
affiliation={Key Laboratory of Wireless Sensor Network & Communication, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai, 201800, China; Department of Computer Science and Engineering, Shaoxing University, Shaoxing, 312000, China},
abstract={Expression, occlusion, and pose variations are three main challenges for 3D face recognition. A novel method is presented to address 3D face recognition using scale-invariant feature transform (SIFT) features on 3D meshes. After preprocessing, shape index extrema on the 3D facial surface are selected as keypoints in the difference scale space and the unstable keypoints are removed after two screening steps. Then, a local coordinate system for each keypoint is established by principal component analysis (PCA). Next, two local geometric features are extracted around each keypoint through the local coordinate system. Additionally, the features are augmented by the symmetrization according to the approximate left-right symmetry in human face. The proposed method is evaluated on the Bosphorus, BU-3DFE, and Gavab databases, respectively. Good results are achieved on these three datasets. As a result, the proposed method proves robust to facial expression variations, partial external occlusions and large pose changes. © 2015, Central South University Press and Springer-Verlag Berlin Heidelberg.},
author_keywords={3D face recognition;  3D meshes;  expression;  large pose changes;  occlusion;  scale-invariant feature transform (SIFT)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Moeini2015969,
author={Moeini, A. and Moeini, H.},
title={Real-world and rapid face recognition toward pose and expression variations via feature library matrix},
journal={IEEE Transactions on Information Forensics and Security},
year={2015},
volume={10},
number={5},
pages={969-984},
doi={10.1109/TIFS.2015.2393553},
art_number={7012060},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927138590&doi=10.1109%2fTIFS.2015.2393553&partnerID=40&md5=856fc589830e5721774c0a6ea40d60cb},
affiliation={Department of Electrical Engineering, Amirkabir University of Technology, Tehran, 79085, Iran; Department of Electrical Engineering, Semnan University, Semnan, 35195-163, Iran},
abstract={In this paper, a novel method for face recognition under pose and expression variations is proposed from only a single image in the gallery. A 3D probabilistic facial expression recognition generic elastic model is proposed to reconstruct a 3D model from real-world human face using only a single 2D frontal image with/without facial expressions. Then, a feature library matrix (FLM) is generated for each subject in the gallery from all face poses by rotating the 3D reconstructed models and extracting features in the rotated face pose. Therefore, each FLM is subsequently rendered for each subject in the gallery based on triplet angles of face poses. In addition, before matching the FLM, an initial estimate of triplet angles is obtained from the face pose in probe images using an automatic head pose estimation approach. Then, an array of the FLM is selected for each subject based on the estimated triplet angles. Finally, the selected arrays from FLMs are compared with extracted features from the probe image by iterative scoring classification using the support vector machine. Convincing results are acquired to handle pose and expression changes on the Bosphorus, Face Recognition Technology (FERET), Carnegie Mellon University-Pose, Illumination, and Expression (CMU-PIE), and Labeled Faces in the Wild (LFW) face databases compared with several state-of-the-art methods in pose-invariant face recognition. The proposed method not only demonstrates an excellent performance by obtaining high accuracy on all four databases but also outperforms other approaches realistically. © 2015 IEEE.},
author_keywords={3D face reconstruction;  feature library matrix;  iterative scoring classification;  Pose-invariant face recognition;  probabilistic facial expression recognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Azazi20153056,
author={Azazi, A. and Lebai Lutfi, S. and Venkat, I. and Fernández-Martínez, F.},
title={Towards a robust affect recognition: Automatic facial expression recognition in 3D faces},
journal={Expert Systems with Applications},
year={2015},
volume={42},
number={6},
pages={3056-3066},
doi={10.1016/j.eswa.2014.10.042},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919798976&doi=10.1016%2fj.eswa.2014.10.042&partnerID=40&md5=8be63441c1bcd18a8ce47846384e2d27},
affiliation={School of Computer Sciences, Universiti Sains Malaysia, Minden, Pulau Pinang, 11800, Malaysia; Departamento de Teoría de la Señal y Comunicaciones, Universidad Carlos III de Madrid, Madrid, Spain},
abstract={Facial expressions are a powerful tool that communicates a person's emotional state and subsequently his/her intentions. Compared to 2D face images, 3D face images offer more granular cues that are not available in the 2D images. However, one major setback of 3D faces is that they impose a higher dimensionality than 2D faces. In this paper, we attempt to address this problem by proposing a fully automatic 3D facial expression recognition model that tackles the high dimensionality problem in a twofold solution. First, we transform the 3D faces into the 2D plane using conformal mapping. Second, we propose a Differential Evolution (DE) based optimization algorithm to select the optimal facial feature set and the classifier parameters simultaneously. The optimal features are selected from a pool of Speed Up Robust Features (SURF) descriptors of all the prospective facial points. The proposed model yielded an average recognition accuracy of 79% using the Bosphorus database and 79.36% using the BU-3DFE database. In addition, we exploit the facial muscular movements to enhance the probability estimation (PE) of Support Vector Machine (SVM). Joint application of feature selection with the proposed enhanced PE (EPE) yielded an average recognition accuracy of 84% using the Bosphorus database and 85.81% using the BU-3DFE database, which is statistically significantly better (at p<0.01 and p<0.001, respectively) if compared to the individual exploit of the optimal features only. © 2014 Elsevier Ltd. All rights reserved.},
author_keywords={3D Facial expression recognition;  Action units;  Conformal mapping;  Differential Evolution;  Probability estimation;  Speed Up Robust Features;  Support vector machines},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Beumier20152291,
author={Beumier, C.},
title={Design of coded structured light pattern for 3D facial surface capture},
journal={European Signal Processing Conference},
year={2015},
volume={06-10-September-2004},
pages={2291-2294},
art_number={7079884},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979948896&partnerID=40&md5=b09f05fe749fab292462602ba77b999e},
affiliation={Signal and Image Centre, Royal Military Academy, Avenue de la Renaissance, 30, Brussels, B-1000, Belgium},
abstract={In the context of 3D face recognition, facial surfaces are advantageously captured by a structured light acquisition system, which is typically quick, low cost and uses off-the-shelve components. The light pattern projected, a key aspect of the structured light approach, makes the major difference between developed systems. In most of them, elements of the light pattern must be identified by a property such as element thickness or colour. We present in this paper the design of projected patterns that led to the realisation of three 3D acquisition prototypes. © 2004 EUSIPCO.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Elaiwat20151235,
author={Elaiwat, S. and Bennamoun, M. and Boussaid, F. and El-Sallam, A.},
title={A Curvelet-based approach for textured 3D face recognition},
journal={Pattern Recognition},
year={2015},
volume={48},
number={4},
pages={1235-1246},
doi={10.1016/j.patcog.2014.10.013},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920743471&doi=10.1016%2fj.patcog.2014.10.013&partnerID=40&md5=dde3d88e6412c83672a1a30bbf49b936},
affiliation={School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia; School of Sport Science, Exercise and Health, University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia},
abstract={In this paper, we present a fully automated multimodal Curvelet-based approach for textured 3D face recognition. The proposed approach relies on a novel multimodal keypoint detector capable of repeatably identifying keypoints on textured 3D face surfaces. Unique local surface descriptors are then constructed around each detected keypoint by integrating Curvelet elements of different orientations, resulting in highly descriptive rotation invariant features. Unlike previously reported Curvelet-based face recognition algorithms which extract global features from textured faces only, our algorithm extracts both texture and 3D local features. In addition, this is achieved across a number of frequency bands to achieve robust and accurate recognition under varying illumination conditions and facial expressions. The proposed algorithm was evaluated using three well-known and challenging datasets, namely FRGC v2, BU-3DFE and Bosphorus datasets. Reported results show superior performance compared to prior art, with 99.2%, 95.1% and 91% verification rates at 0.001 FAR for FRGC v2, BU-3DFE and Bosphorus datasets, respectively. © 2014 Elsevier Ltd. All rights reserved.},
author_keywords={Digital Curvelet transform;  Face recognition;  Keypoint detection;  Local features},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Suwajanakorn20153952,
author={Suwajanakorn, S. and Seitz, S.M. and Kemelmacher-Shlizerman, I.},
title={What makes tom hanks look like tom hanks},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2015},
volume={2015 International Conference on Computer Vision, ICCV 2015},
pages={3952-3960},
doi={10.1109/ICCV.2015.450},
art_number={7410807},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973890906&doi=10.1109%2fICCV.2015.450&partnerID=40&md5=21ed3393d125cd8cda738c0d24394d4f},
affiliation={Department of Computer Science and Engineering, University of Washington, United States},
abstract={We reconstruct a controllable model of a person from a large photo collection that captures his or her persona, i.e., physical appearance and behavior. The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned. Moreover, we show the ability to drive or puppeteer the captured person B using any other video of a different person A. In this scenario, B acts out the role of person A, but retains his/her own personality and character. Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem. We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tulyakov201528,
author={Tulyakov, S. and Vieriu, R.-L. and Sangineto, E. and Sebe, N.},
title={FaceCept3D: Real Time 3D Face Tracking and Analysis},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2015},
volume={2015-February},
pages={28-33},
doi={10.1109/ICCVW.2015.13},
art_number={7406362},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962007100&doi=10.1109%2fICCVW.2015.13&partnerID=40&md5=8c99411902fcebb040af2e1d81fa5c97},
affiliation={Department of Information Engineering and Computer Science, University of Trento, Italy},
abstract={We present an open source cross platform technology for 3D face tracking and analysis. It contains a full stack of components for complete face understanding: detection, head pose tracking, facial expression and action units recognition. Given a depth sensor, one can combine FaceCept3D modules to fulfill a specific application scenario. Key advantages of the technology include real time processing speed and ability to handle extreme head pose variations. Possible application areas of the technology range from human computer interaction to active aging, where precise and real-time analysis is required. The technology is available to community. © 2015 IEEE.},
author_keywords={Face;  Face recognition;  Feature extraction;  Iterative closest point algorithm;  Magnetic heads;  Three-dimensional displays},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li201575,
author={Li, Y. and Wang, Y. and Wang, B. and Sui, L.},
title={Nose tip detection on three-dimensional faces using pose-invariant differential surface features},
journal={IET Computer Vision},
year={2015},
volume={9},
number={1},
pages={75-84},
doi={10.1049/iet-cvi.2014.0070},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988269675&doi=10.1049%2fiet-cvi.2014.0070&partnerID=40&md5=9b44b1684d50589e30f8aa93abe2e4b3},
affiliation={School of Computer Science and Engineering, Xi'an University of Technology, 5 South Jinhua Road, Xi'an, 710048, China},
abstract={Three-dimensional (3D) facial data offer the potential to overcome the difficulties caused by the variation of head pose and illumination in 2D face recognition. In 3D face recognition, localisation of nose tip is essential to face normalisation, face registration and pose correction etc. Most of the existing methods of nose tip detection on 3D face deal mainly with frontal or near-frontal poses or are rotation sensitive. Many of them are training-based or model-based. In this study, a novel method of nose tip detection is proposed. Using pose-invariant differential surface features - high-order and low-order curvatures, it can detect nose tip on 3D faces under various poses automatically and accurately. Moreover, it does not require training and does not depend on any particular model. Experimental results on GavabDB verify the robustness and accuracy of the proposed method. © The Institution of Engineering and Technology 2015.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Srinivasan2015,
author={Srinivasan, A. and Balamurugan, V.},
title={Occlusion detection and image restoration in 3D face image},
journal={IEEE Region 10 Annual International Conference, Proceedings/TENCON},
year={2015},
volume={2015-January},
doi={10.1109/TENCON.2014.7022477},
art_number={7022477},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940493685&doi=10.1109%2fTENCON.2014.7022477&partnerID=40&md5=59b85a1c61b18d769cde95762474d1c0},
affiliation={Department of Information Technology, MNM Jain Engineering College, Anna University, Chennai, Tamil Nadu, India; Department of Computer Science and Engineering, Chandy College of Engineering, Anna University, Thoothukudi, Tamil Nadu, India},
abstract={Face recognition system has emerged as an important field in case of surveillance systems. Since three-dimensional imaging systems have reached a notable growth, we consider the 3D image for face recognition. Occlusion (extraneous objects that hinder face recognition, e.g., scarf, glass, beard etc.,) is one of the greatest challenges in face recognition systems. Other issues are illumination, pose, scale etc., an innovative three dimensional occlusion detection and restoration strategy for the recognition of three dimensional faces partially occluded by unforeseen objects is presented. Normalization provides orientation of the image to frontal view since we require frontal position for face recognition. An efficient method is used for detection of occlusions, which specifies the missing information in the occluded face. A restoration method then eliminates occlusion and renders a restored facial image. It exploits the information provided by the non-occluded part of the face to recover the original face. Restored faces are then applied to a suitable face recognition system. The proposed system will provide better accuracy to eliminate the occlusion and restored facial information method is independent of the face recognition method. © 2014 IEEE.},
author_keywords={3D Face Restoration;  Face recognition;  Normalization;  Occlusion},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hou2015,
author={Hou, G. and Liu, F. and Sun, Z.},
title={Computer vision research with new imaging technology},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9813},
doi={10.1117/12.2205630},
art_number={981303},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958212210&doi=10.1117%2f12.2205630&partnerID=40&md5=13e3557d5d6972c5eaad8f140118a8ad},
affiliation={Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, China; College of Engineering and Information Technology, University of Chinese, Academy of Sciences, China},
abstract={Light field imaging is capable of capturing dense multi-view 2D images in one snapshot, which record both intensity values and directions of rays simultaneously. As an emerging 3D device, the light field camera has been widely used in digital refocusing, depth estimation, stereoscopic display, etc. Traditional multi-view stereo (MVS) methods only perform well on strongly texture surfaces, but the depth map contains numerous holes and large ambiguities on textureless or low-textured regions. In this paper, we exploit the light field imaging technology on 3D face modeling in computer vision. Based on a 3D morphable model, we estimate the pose parameters from facial feature points. Then the depth map is estimated through the epipolar plane images (EPIs) method. At last, the high quality 3D face model is exactly recovered via the fusing strategy. We evaluate the effectiveness and robustness on face images captured by a light field camera with different poses. © 2015 SPIE.},
author_keywords={3D face model;  depth estimation;  EPIs;  Light field imaging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pawar2015,
author={Pawar, A.A. and Patil, N.N.},
title={Recognition of 3-D faces with missing parts and line scratch removal using new technique},
journal={2015 International Conference on Pervasive Computing: Advance Communication Technology and Application for Society, ICPC 2015},
year={2015},
doi={10.1109/PERVASIVE.2015.7087053},
art_number={7087053},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929245161&doi=10.1109%2fPERVASIVE.2015.7087053&partnerID=40&md5=974956336a63dc1414bbfaacd2ad4d82},
affiliation={Department of Computer Engineering, SES's R. C. Patel Institute of Technology, Shirpur, MS, India},
abstract={This paper presents an implementation of face recognition, which is a very important task of human face identification. Line scratch detection in images is a highly challenging situation because of various characteristics of this defect. Few characteristics are considered with the different texture and geometry of images. We propose a useful algorithm for frame-by-frame line scratch detection in face image which deals with 3D approach and a filtering of detection. The temporary filtering algorithm can be used to remove false detection due to thin vertical structures by detecting the scratches on an image. Experimental evaluation can be detecting the lines and scratches on a face image and they used to solve this difficult approach. Our method is used with missing parts in an image. Three-dimensional face recognition is an extended method of facial recognition is considered according with the geometry and texture of a face. It has been elaborated that 3D face recognition methods can provide high accuracy as well as high detection with a comparison of 2D recognition. 3D avoids such mismatch effect of 2D face recognition algorithms. Additionally, most 3D scanners achieve both a 3D mesh and the texture of a face image. This allows combining the output of pure 3D matches with the more traditional algorithms of 2D face recognition, thus producing better performance. © 2015 IEEE.},
author_keywords={3D Images;  Adaptive detection;  Face mask;  Hough transforms;  ICP algorithm;  Line scratches;  Missing parts;  RANSAC;  SIFT},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gong2015,
author={Gong, X. and Fu, Z. and Li, X. and Feng, L.},
title={A two-stage estimation method for depth estimation of facial landmarks},
journal={2015 IEEE International Conference on Identity, Security and Behavior Analysis, ISBA 2015},
year={2015},
doi={10.1109/ISBA.2015.7126355},
art_number={7126355},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942531988&doi=10.1109%2fISBA.2015.7126355&partnerID=40&md5=f4518cb4d35925f77c360342195850d2},
affiliation={School of Information Science and Technology, Southwest Jiaotong University, China; Jincheng College of Sichuan University, Chengdu, 611731, China; College of Computer Science, Sichuan Normal University, Chengdu, 610066, China},
abstract={To address the problem of 3D face modeling based on a set of landmarks on images, the traditional feature-based morphable model, using face class-specific information, makes direct use of these 2D points to infer a dense 3D face surface. However, the unknown depth of landmarks degrades accuracy considerably. A promising solution is to predict the depth of landmarks at first. Bases on this idea, a two-stage estimation method is proposed to compute the depth value of landmarks from two images. And then, the estimated 3D landmarks are applied to a deformation algorithm to make a precise 3D dense facial shape. Test results on synthesized images with known ground-truth show that the proposed two-stage estimation method can obtain landmarks' depth both effectively and efficiently, and further that the reconstructed accuracy is greatly enhanced with the estimated 3D landmarks. Reconstruction results of real-world photos are rather realistic. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Echeagaray-Patrón2015,
author={Echeagaray-Patrón, B.A. and Kober, V.},
title={3D face recognition based on matching of facial surfaces},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9598},
doi={10.1117/12.2186695},
art_number={95980V},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951325808&doi=10.1117%2f12.2186695&partnerID=40&md5=cb5a757e08c6a66683fc0706f09faf95},
affiliation={Department of Computer Science, CICESE, Ensenada, B.C.22860, Mexico; Department of Mathematics, Chelyabinsk State University, Russian Federation},
abstract={Face recognition is an important task in pattern recognition and computer vision. In this work a method for 3D face recognition in the presence of facial expression and poses variations is proposed. The method uses 3D shape data without color or texture information. A new matching algorithm based on conformal mapping of original facial surfaces onto a Riemannian manifold followed by comparison of conformal and isometric invariants computed in the manifold is suggested. Experimental results are presented using common 3D face databases that contain significant amount of expression and pose variations. © 2015 SPIE.},
author_keywords={3D face recognition;  3D facial shape analysis;  Conformal mapping},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo2015,
author={Guo, Z. and Liu, S. and Wang, Y. and Lei, T.},
title={Learning deformation model for expression-robust 3D face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9817},
doi={10.1117/12.2228002},
art_number={98170O},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028295582&doi=10.1117%2f12.2228002&partnerID=40&md5=42fcb3ae1a1a8506c08003ac3fd73bc1},
affiliation={School of Electronics and Information, Northwestern Polytechnical University, Xi'an, 710072, China; School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, 730070, China},
abstract={Expression change is the major cause of local plastic deformation of the facial surface. The intra-class differences with large expression change somehow are larger than the inter-class differences as it's difficult to distinguish the same individual with facial expression change. In this paper, an expression-robust 3D face recognition method is proposed by learning expression deformation model. The expression of the individuals on the training set is modeled by principal component analysis, the main components are retained to construct the facial deformation model. For the test 3D face, the shape difference between the test and the neutral face in training set is used for reconstructing the expression change by the constructed deformation model. The reconstruction residual error is used for face recognition. The average recognition rate on GavabDB and self-built database reaches 85.1% and 83%, respectively, which shows strong robustness for expression changes. © 2015 SPIE.},
author_keywords={3D face recognition;  Facial deformation model;  Principal component analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qu2015,
author={Qu, C. and Monari, E. and Schuchert, T. and Beyerer, J.},
title={Realistic texture extraction for 3D face models robust to self-occlusion},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9405},
doi={10.1117/12.2079924},
art_number={94050P},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926658310&doi=10.1117%2f12.2079924&partnerID=40&md5=651c0ff57313fc39c7c0f6b0a3543a30},
affiliation={Vision and Fusion Laboratory, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe, 76131, Germany; Fraunhofer Institute of Optronics, System Technologies and Image Exploitation, Fraunhoferstr. 1, Karlsruhe, 76131, Germany},
abstract={In the context of face modeling, probably the most well-known approach to represent 3D faces is the 3D Morphable Model (3DMM). When 3DMM is fitted to a 2D image, the shape as well as the texture and illumination parameters are simultaneously estimated. However, if real facial texture is needed, texture extraction from the 2D image is necessary. This paper addresses the possible problems in texture extraction of a single image caused by self-occlusion. Unlike common approaches that leverage the symmetric property of the face by mirroring the visible facial part, which is sensitive to inhomogeneous illumination, this work first generates a virtual texture map for the skin area iteratively by averaging the color of neighbored vertices. Although this step creates unrealistic, overly smoothed texture, illumination stays constant between the real and virtual texture. In the second pass, the mirrored texture is gradually blended with the real or generated texture according to the visibility. This scheme ensures a gentle handling of illumination and yet yields realistic texture. Because the blending area only relates to non-informative area, main facial features still have unique appearance in different face halves. Evaluation results reveal realistic rendering in novel poses robust to challenging illumination conditions and small registration errors.},
author_keywords={3D face model;  Self-occlusion;  Texture extraction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Luo2015,
author={Luo, J. and Geng, S.Z. and Xiao, Z.X. and Xiu, C.B.},
title={A review of recent advances in 3d face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9443},
doi={10.1117/12.2178750},
art_number={944303},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925431201&doi=10.1117%2f12.2178750&partnerID=40&md5=f8e793b50759be140f6e5bf45667a121},
affiliation={Key Laboratory of Advanced of Electrical Engineering and Energy Technology, Tianjin Polytechnic University, Tianjin, 300387, China; College of Electrical Engineering and Automation, Tianjin Polytechnic University, Tianjin, 300387, China},
abstract={Face recognition based on machine vision has achieved great advances and been widely used in the various fields. However, there are some challenges on the face recognition, such as facial pose, variations in illumination, and facial expression. So, this paper gives the recent advances in 3D face recognition. 3D face recognition approaches are categorized into four groups: minutiae approach, space transform approach, geometric features approach, model approach. Several typical approaches are compared in detail, including feature extraction, recognition algorithm, and the performance of the algorithm. Finally, this paper summarized the challenge existing in 3D face recognition and the future trend. This paper aims to help the researches majoring on face recognition. © 2015 SPIE.},
author_keywords={3D face recognition;  geometric features;  minutiae approach.;  space transform},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ming201514,
author={Ming, Y.},
title={Robust regional bounding spherical descriptor for 3D face recognition and emotion analysis},
journal={Image and Vision Computing},
year={2015},
volume={35},
pages={14-22},
doi={10.1016/j.imavis.2014.12.003},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921669860&doi=10.1016%2fj.imavis.2014.12.003&partnerID=40&md5=fe785e43fe879b32d41aadd4abc61953},
affiliation={School of Electronic Engineering, Beijing Key Laboratory of Work Safety Intelligent Monitoring, Beijing University of Posts and Telecommunications, Beijing, 100876, China},
abstract={3D face recognition and emotion analysis play important roles in many fields of communication and edutainment. An effective facial descriptor, with higher discriminating capability for face recognition and higher descriptiveness for facial emotion analysis, is a challenging issue. However, in the practical applications, the descriptiveness and discrimination are independent and contradictory to each other. 3D facial data provide a promising way to balance these two aspects. In this paper, a robust regional bounding spherical descriptor (RBSR) is proposed to facilitate 3D face recognition and emotion analysis. In our framework, we first segment a group of regions on each 3D facial point cloud by shape index and spherical bands on the human face. Then the corresponding facial areas are projected to regional bounding spheres to obtain our regional descriptor. Finally, a regional and global regression mapping (RGRM) technique is employed to the weighted regional descriptor for boosting the classification accuracy. Three largest available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the performance comparison and the experimental results show a consistently better performance for 3D face recognition and emotion analysis. © 2015 Elsevier B.V. All rights reserved.},
author_keywords={3D face recognition;  Emotion analysis;  Kullback-Leiber divergence (KLD);  Regional and global regression;  Regional bounding spherical descriptor},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Quan201545,
author={Quan, W. and Matuszewski, B.J. and Shark, L.-K.},
title={3-D shape matching for face analysis and recognition},
journal={ICPRAM 2015 - 4th International Conference on Pattern Recognition Applications and Methods, Proceedings},
year={2015},
volume={2},
pages={45-52},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938888045&partnerID=40&md5=232ea318938db81af84e5520771102aa},
affiliation={Robotics and Computer Vision Research Laboratory, Applied Digital Signal and Image Processing (ADSIP) Research Centre, University of Central Lancashire, Preston, PR1 2HE, United Kingdom},
abstract={The aims of this paper are to introduce a 3-D shape matching scheme for automatic face recognition and to demonstrate its invariance to pose and facial expressions. The core of this scheme lies on the combination of non-rigid deformation registration and statistical shape modelling. While the former matches 3-D faces regardless of facial expression variations, the latter provides a low-dimensional feature vector that describes the deformation after the shape matching process, thereby enabling robust identification of 3-D faces. In order to assist establishment of accurate dense point correspondences, an isometric embedding shape representation is introduced, which is able to transform 3-D faces to a canonical form that retains the intrinsic geometric structure and achieve shape alignment of 3-D faces independent from individual's facial expression. The feasibility and effectiveness of the proposed method was investigated using standard publicly available Gavab and BU-3DFE databases, which contain faces expressions and pose variations. The performance of the system was compared with the existing benchmark approaches and it demonstrates that the proposed scheme provides a competitive solution for the face recognition task with real-world practicality.},
author_keywords={Face recognition;  Isometric embedding representation;  Non-rigid deformation registration;  Shape matching and modelling},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2015193,
author={Li, J. and Long, S. and Zeng, D. and Zhao, Q.},
title={Example-based 3D face reconstruction from uncalibrated frontal and profile images},
journal={Proceedings of 2015 International Conference on Biometrics, ICB 2015},
year={2015},
pages={193-200},
doi={10.1109/ICB.2015.7139051},
art_number={7139051},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943257826&doi=10.1109%2fICB.2015.7139051&partnerID=40&md5=60413dd9023d53b49181fde374ae159b},
affiliation={College of Computer Science, Sichuan University, Chengdu, 610065, China},
abstract={Reconstructing 3D face models from multiple uncalibrated 2D face images is usually done by using a single reference 3D face model or some gender/ethnicity-specific 3D face models. However, different persons, even those of the same gender or ethnicity, usually have significantly different faces in terms of their overall appearance, which forms the base of person recognition using faces. Consequently, existing 3D reference model based methods have limited capability of reconstructing 3D face models for a large variety of persons. In this paper, we propose to explore a reservoir of diverse reference models to improve the 3D face reconstruction performance. Specifically, we convert the face reconstruction problem into a multi-label segmentation problem. Its energy function is formulated from different cues, including 1) similarity between the desired output and the initial model, 2) color consistency between different views, 3) smoothness constraint on adjacent pixels, and 4) model consistency within local neighborhood. Experimental results on challenging datasets demonstrate that the proposed algorithm is capable of recovering high quality face models in both qualitative and quantitative evaluations. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20151535,
author={Zhang, J. and Tao, D. and Bian, X. and Zhan, X.},
title={Monocular face reconstruction with global and local shape constraints},
journal={Neurocomputing},
year={2015},
volume={149},
number={PC},
pages={1535-1543},
doi={10.1016/j.neucom.2014.08.039},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912062432&doi=10.1016%2fj.neucom.2014.08.039&partnerID=40&md5=628bb2b11b98a273cadaff74052ae33c},
affiliation={School of Science and Technology, Zhejiang International Studies University, Hangzhou, 310012, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; The Chinese University of Hong Kong, Hong Kong, China},
abstract={To reconstruct 3D face from single monocular image, this paper proposes an approach which comprises three steps. First, a set of 3D facial features is recovered from 2D features extracted from the image. The features are recovered by solving equations derived from a regularized scaled orthogonal projection. The regularization is achieved by a global shape constraint exploiting a prior reference 3D facial shape. Second, we warp a high-resolution reference 3D face, using both recovered 3D features and local shape constraint at each model points. Last, realistic 3D face is obtained through texture synthesis. Compared with existing approach, the proposed feature recovery method has higher accuracy, and it is robust to facial pose variation appeared on the given image. Moreover, the model warping method based on local shape constraints can warp a high-resolution reference 3D face using few 3D features more reasonably and accurately. The proposed approach generates realistic 3D face with impressive visual effect. © 2014 Elsevier B.V.},
author_keywords={3D reconstruction;  Digital entertainment;  Global shape constraint;  Local shape constraint;  Reference 3D face},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tang2015466,
author={Tang, Y. and Sun, X. and Huang, D. and Morvan, J.-M. and Wang, Y. and Chen, L.},
title={3D face recognition with asymptotic cones based principal curvatures},
journal={Proceedings of 2015 International Conference on Biometrics, ICB 2015},
year={2015},
pages={466-472},
doi={10.1109/ICB.2015.7139111},
art_number={7139111},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943279293&doi=10.1109%2fICB.2015.7139111&partnerID=40&md5=bd52f57fa0f04d14b00dab8a5d4e64e2},
affiliation={Université de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, Lyon, 69134, France; King Abdullah University of Science and Technology, V.C.C. Research Center, Thuwal, 23955-6900, Saudi Arabia; IRIP, School of Computer Science and Engineering, Beihang Universtiy, Beijing, 100191, China; Université de Lyon, CNRS, Université Claude Bernard Lyon 1, ICJ UMR 5208, Villeurbanne, F-69622, France},
abstract={The classical curvatures of smooth surfaces (Gaussian, mean and principal curvatures) have been widely used in 3D face recognition (FR). However, facial surfaces resulting from 3D sensors are discrete meshes. In this paper, we present a general framework and define three principal curvatures on discrete surfaces for the purpose of 3D FR. These principal curvatures are derived from the construction of asymptotic cones associated to any Borel subset of the discrete surface. They describe the local geometry of the underlying mesh. First two of them correspond to the classical principal curvatures in the smooth case. We isolate the third principal curvature that carries out meaningful geometric shape information. The three principal curvatures in different Borel subsets scales give multi-scale local facial surface descriptors. We combine the proposed principal curvatures with the LNP-based facial descriptor and SRC for recognition. The identification and verification experiments demonstrate the practicability and accuracy of the third principal curvature and the fusion of multi-scale Borel subset descriptors on 3D face from FRGC v2.0. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2015192,
author={Wang, X. and Ruan, Q. and Jin, Y. and An, G.},
title={3D face recognition using closest point coordinates and spherical vector norms},
journal={IET Conference Publications},
year={2015},
volume={2015},
number={CP681},
pages={192-196},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983122121&partnerID=40&md5=4a26023e54f7fb05eaa6d68668c88d60},
affiliation={Institution of Information Science, Beijing Jiaotong University, Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, 100044, China},
abstract={In this paper, we introduce a new feature named spherical vector norms for 3D face recognition. The proposed feature is efficient, insensitive to facial expression and contains discriminatory information of 3D face. The feature extraction method is firstly finding a set of the points with the closest distance to the standard face, denoted as closest point coordinates, and then extracting the spherical vector norms of these points. This paper combines point coordinates and spherical vector norms for improving recognition. Finally this approach is finished by Linear Discriminant Analysis (LDA) and Nearest Neighbor classifier. We have performed different experiments on the Face Recognition Grand Challenge database. It achieves the verification rate of 97.11% on All vs. All experiment at 0.1% FAR and 96.64% verification rate on Neutral vs. Expression experiment.},
author_keywords={3D face recognition;  Linear discriminant analysis;  Spherical vector norms},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20153357,
author={Zhang, C. and Gu, Y. and Wang, Y. and Li, F. and Zhan, Y. and Pi, J. and Qu, L.},
title={Adaptive multiple regions matching for 3D face recognition under expression and pose variations},
journal={Journal of Computational Information Systems},
year={2015},
volume={11},
number={9},
pages={3357-3369},
doi={10.12733/jcis14297},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938320850&doi=10.12733%2fjcis14297&partnerID=40&md5=af76d7b96bb6eca2db8bc3e1babc780a},
affiliation={Key Laboratory of Wireless Sensor Network & Communication, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai, 200050, China; University of Chinese Academy of Sciences, Beijing, 100049, China; Shanghai Internet of Things Co., Ltd., Shanghai, 201800, China},
abstract={Expression and pose variations are two major challenges for 3D face recognition. This paper presents a method to cope with these two challenges by fusing the matching results of adaptive multiple regions on the 3D face. First, one approach is proposed for pose correction of 3D face based on three landmark points: nose tip, nasion, and subnasale. Then multiple regions are adaptively chosen from the facial surface, which include nose, left and right eye-forehead regions, left and right cheeks, and mouth-chin region. Next, a least trimmed square Hausdorff distance method is applied for region matching. Moreover, to obtain a better overall performance, several score-level and rank-level fusion schemes are used to fuse the contribution of each region. The proposed approach is evaluated on the Bosphorus and the BU-3DFE databases, and yields good results. The study shows that the proposed algorithm is robust to expression and pose changes. ©, 2015, Binary Information Press. All right reserved.},
author_keywords={3D face recognition;  Expression;  Pose correction;  Pose rotation;  Region matching},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Belghini2015317,
author={Belghini, N. and Ezghari, S. and Zahi, A.},
title={3D face recognition using facial curves, sparse random projection and fuzzy similarity measure},
journal={Colloquium in Information Science and Technology, CIST},
year={2015},
volume={2015-January},
number={January},
pages={317-322},
doi={10.1109/CIST.2014.7016639},
art_number={7016639},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938075627&doi=10.1109%2fCIST.2014.7016639&partnerID=40&md5=bca04cb8b40640821dbbc277a0cf6e71},
affiliation={System Intelligent and Application Laboratory (SIA) FST, Fez, Morocco},
abstract={In this paper, we propose a fuzzy similarity based classification approach for 3D face recognition. In the feature extraction method, we exploit curve concept to represent the 3D facial data, two types of curves was considered: depth-level and depth-radial curves. As the dimension of the obtained features is high, the problem 'curse of dimensionality' appears. To solve this problem, the Random Projection (RP) method was used. The proposed classifier performs Fuzzification operation using triangular membership functions for input data and ordered weighted averaging operators to measure similarity. Experiment was conducted using vrml files from 3D Database considering only one training sample per person. The obtained results are very promising for depth-level and depth-radial curves, besides the recognition rates are higher than 98%. © 2014 IEEE.},
author_keywords={3D face recognition;  facial curves;  fuzzy logic;  OWA operator;  similarity measure;  sparse random projection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ratyal2015241,
author={Ratyal, N.I. and Taj, I.A. and Bajwa, U.I. and Sajid, M.},
title={3D face recognition based on pose and expression invariant alignment},
journal={Computers and Electrical Engineering},
year={2015},
volume={46},
pages={241-255},
doi={10.1016/j.compeleceng.2015.06.007},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931030581&doi=10.1016%2fj.compeleceng.2015.06.007&partnerID=40&md5=a95f41db3d492d423152cf1d2907956a},
affiliation={Vision and Pattern Recognition Systems Research Group, Mohammad Ali Jinnah University, Islamabad, Pakistan; Department of Computer Science, COMSATS Institute of Information Technology, Lahore, Pakistan},
abstract={In this paper we present a novel pose and expression invariant approach for 3D face registration based on intrinsic coordinate system characterized by nose tip, horizontal nose plane and vertical symmetry plane of the face. It is observed that distance of nose tip from 3D scanner is reduced after pose correction which is presented as a quantifying heuristic for proposed registration scheme. In addition, motivated by the fact that a single classifier cannot be generally efficient against all face regions, a two tier ensemble classifier based 3D face recognition approach is presented which employs Principal Component Analysis (PCA) for feature extraction and Mahalanobis Cosine (MahCos) matching score for classification of facial regions with weighted Borda Count (WBC) based combination and a re-ranking stage. The performance of proposed approach is corroborated by extensive experiments performed on two databases: GavabDB and FRGC v2.0, confirming effectiveness of fusion strategies to improve performance. © 2015 Elsevier Ltd. All rights reserved.},
author_keywords={3D face recognition;  3D registration;  Ensemble classifier;  Fusion;  Intrinsic coordinate system},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Naveen20151537,
author={Naveen, S. and Moni, R.S.},
title={Multimodal face recognition system using spectral transformation of 2D texture feature and statistical processing of face range images},
journal={Procedia Computer Science},
year={2015},
volume={46},
pages={1537-1545},
doi={10.1016/j.procs.2015.02.078},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931407168&doi=10.1016%2fj.procs.2015.02.078&partnerID=40&md5=76b4162cf883c24cb44c108b0dd1fe2f},
affiliation={Department of ECE, LBS Institute of Technology for Women, Trivandrum, Kerala  695012, India; Department of ECE, Marian Engineering College, Trivandrum, Kerala  695582, India},
abstract={3D Face recognition has been an area of interest for the past few decades in pattern recognition. This paper focuses on problems of person identification using 3D Face data. Here unregistered Face data, i.e. both texture and depth is fed to classifier in spectral representations of data. 2D Discrete Fourier Transform (DFT) is used for spectral representation. Fusion of scores improves the recognition accuracy significantly since use of depth information alone in spectral representation was not sufficient to increase accuracy. Statistical method seems to degrade performance of system when applied to texture data and was effective for depth data. © 2015 The Authors.},
author_keywords={CDF;  Depth map;  Point cloud;  Pose correction;  Rotation invariance;  Spectral transformations;  Texture map and principal component analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Majeed2015373,
author={Majeed, R. and Beiji, Z. and Hatem, H.},
title={Nose tip detection in 3D face image based on maximum intensity algorithm},
journal={International Journal of Multimedia and Ubiquitous Engineering},
year={2015},
volume={10},
number={5},
pages={373-382},
doi={10.14257/ijmue.2015.10.5.35},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932107713&doi=10.14257%2fijmue.2015.10.5.35&partnerID=40&md5=d426923498038e74777ea5086e124eec},
affiliation={School of Information Science and Engineering, Central South University, Changsha, 410083, China; Department of Computer Science, College of Sciences, Baghdad University, Iraq, China},
abstract={In order to increase the ability to track face movements with large head rotations, a 3D shape model is used in the system. In this paper, we present a robust nose tip detection method in 3D facial image that handling facial expression and hair occlusion. The 3D face smoothed by weighted median filter, the holes are filled by linear interpolation during the re-sampling phase and the 3D Gaussian filter is used to remove noise. Since the database, mesh contains unimportant parts like neck, shoulder, clothes and hair that can also change the overall appearance of a face. We propose a 3D mask to cut the face and crop useful part, which helps us to achieve sufficient accuracy for noise detection. Nose localization is one of the most significant tasks of any facial classification system, compared to other facial landmarks. We proceed to detect the point N from the front image following the assumption that the relevant point has the highest value of the Z axis. In this framework, the face model is determined from a frontal 3D face image. In this experiment, the performance rate is improved from 90.1% to 98.3%. As indicated by the experimental result, the proposed binary mask with maximum intensity method provides a significant improvement in performance of nose tip detection, also it success in different facial expression. © 2015 SERSC.},
author_keywords={3D face detection;  3D facial image;  3D mask;  Denoising;  Nose tip detection},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chouchane2015,
author={Chouchane, A. and Belahcene, M. and Ouamane, A. and Bourennane, S.},
title={3D face recognition based on histograms of local descriptors},
journal={2014 4th International Conference on Image Processing Theory, Tools and Applications, IPTA 2014},
year={2015},
doi={10.1109/IPTA.2014.7001925},
art_number={7001925},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921721830&doi=10.1109%2fIPTA.2014.7001925&partnerID=40&md5=4bbf52130cd2484d05cce4611c376687},
affiliation={LMSE, University of Biskra, Biskra, R.P.07000, Algeria; Centre de Développement des Technologies Avancées, ASM Alger, Algeria; Institut Fresnel, UMR CNRS 7249, Ecole Centrale Marseille, France},
abstract={Face recognition in an uncontrolled condition such as illumination and expression variations is a challenging task. Local descriptor is one of the most efficient methods used to deal with these problems. In this paper, we present an automatic 3D face recognition approach based on three local descriptors, local phase quantization (LPQ), Three-Patch Local Binary Patterns (TPLBP) and Four-Patch Local Binary Patterns (TPLBP). Facial images are passing through one of the three descriptors and divided into sub-regions or rectangular blocks. The histogram of each sub-region is extracted and concatenated into a single feature vector. PCA (Principal Component Analysis) and EFM (Enhanced Fisher linear discriminant Model) are used to reduce the dimensionality of the resulting feature vectors. Finally, these vectors are sent to the classification step, when we use two methods; SVM (Support Victor Machine) and similarity measures. CASIA 3D face database is introduced to experimental evaluation. The experimental results illustrate a high recognition performance of the proposed approach. © 2014 IEEE.},
author_keywords={3D face recognition;  FPLBP;  Local phase quantization;  Locale descriptors;  Support vector machines;  TPLBP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lagorio2015,
author={Lagorio, A. and Cadoni, M. and Grosso, E. and Tistarelli, M.},
title={A 3D algorithm for unsupervised face identification},
journal={3rd International Workshop on Biometrics and Forensics, IWBF 2015},
year={2015},
doi={10.1109/IWBF.2015.7110239},
art_number={7110239},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936101701&doi=10.1109%2fIWBF.2015.7110239&partnerID=40&md5=65b942c6e6bd6ec99d853a5a632cd56f},
affiliation={VisionLab - Computer Vision Laboratory, United Kingdom},
abstract={With the increasing availability of low-cost 3D data acquisition devices, the use of 3D face data for the recognition of individuals is becoming more appealing and computationally feasible. This paper proposes a completely automatic algorithm for face registration and matching. The algorithm is based on the extraction of stable 3D facial features characterizing the face and the subsequent construction of a signature manifold. The facial features are extracted by performing a continuous-to-discrete scale-space analysis. Registration is driven from the matching of triplets of feature points and the registration error is computed as shape matching score. Conversely to most techniques in the literature, a major advantage of the proposed method is that no data pre-processing is required. Therefore all presented results have been obtained exclusively from the raw data available from the 3D acquisition device. The method has been tested on the Bosphorus 3D face database and the performances compared to the ICP baseline algorithm. Even in presence of noise in the data, the algorithm proved to be very robust and reported identification performances which are aligned to the current state of the art, but without requiring any pre-processing of the raw data. © 2015 IEEE.},
author_keywords={3D Face recognition;  Face recognition},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chouchane201550,
author={Chouchane, A. and Belahcene, M. and Bourennane, S.},
title={3D and 2D face recognition using integral projection curves based depth and intensity images},
journal={International Journal of Intelligent Systems Technologies and Applications},
year={2015},
volume={14},
number={1},
pages={50-69},
doi={10.1504/IJISTA.2015.072219},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964744543&doi=10.1504%2fIJISTA.2015.072219&partnerID=40&md5=e5008a255876319ef73de7c015360bc8},
affiliation={Faculty of Science and Technology, Department of Electrical Engineering, University of Mohamed Khider, Biskra, BP 145 RP, Biskra, 07000, Algeria; Institut Fresnel, UMR CNRS 7249, Ecole Centrale Marseille, France},
abstract={This paper presents an automatic face recognition system in the presence of illumination, expressions and pose variations based on depth and intensity information. At first, the registration of 3D faces is achieved using iterative closest point (ICP). Nose tip point must be located using Maximum Intensity Method. This point usually has the largest depth value; however there is a problem with some unnecessary data such as: shoulders, hair, neck and parts of clothes; to cope with this issue, we propose the integral projection curves (IPC)-based facial area segmentation to extract the facial area. After that, the combined method principal component analysis (PCA) with enhanced fisher model (EFM) is used to obtain the feature matrix vectors. Finally, the classification is performed using distance measurement and support vector machine (SVM). The experiments are implemented on two face databases CASIA3D and GavabDB; our results show that the proposed method achieves a high recognition performance. Copyright © 2015 Inderscience Enterprises Ltd.},
author_keywords={2D and 3D face recognition;  EFM;  Enhanced fisher model;  IPC-based facial area segmentation;  Nose tip;  PCA;  Principal component analysis;  Support vector machine;  SVM},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liang2015406,
author={Liang, R. and Shen, W. and Li, X.-X. and Wang, H.},
title={Bayesian multi-distribution-based discriminative feature extraction for 3D face recognition},
journal={Information Sciences},
year={2015},
volume={320},
pages={406-417},
doi={10.1016/j.ins.2015.03.063},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937521792&doi=10.1016%2fj.ins.2015.03.063&partnerID=40&md5=ea6a6994deb813d32a7a1d39602f0d94},
affiliation={College of Information Engineering, Zhejiang University of TechnologyHangzhou, China},
abstract={Due to the difficulties associated with the collection of 3D samples, 3D face recognition technologies often have to work with smaller than desirable sample sizes. With the aim of enlarging the training number for each subject, we divide each training image into several patches. However, this immediately introduces two further problems for 3D models: high computational cost and dispersive features caused by the divided 3D image patches. We therefore first map 3D face images into 2D depth images, which greatly reduces the dimension of the samples. Though the depth images retain most of the robust features of 3D images, such as pose and illumination invariance, they lose many discriminative features of the original 3D samples. In this study, we propose a Bayesian learning framework to extract the discriminative features from the depth images. Specifically, we concentrate the features of the intra-class patches to a mean feature by maximizing the multivariate Gaussian likelihood function, and, simultaneously, enlarge the distances between the inter-class mean features by maximizing the exponential priori distribution of the mean features. For classification, we use the nearest neighbor classifier combined with the Mahalanobis distance to calculate the distance between the features of the test image and items in the training set. Experiments on two widely-used 3D face databases demonstrate the efficiency and accuracy of our proposed method compared to relevant state-of-the-art methods. © 2015 Elsevier Inc. All rights reserved.},
author_keywords={3D face recognition;  Bayesian learning;  Depth image;  Single training sample per person},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yu2015,
author={Yu, H. and Liu, H.},
title={Facial pose estimation via dense and sparse representation},
journal={IEEE SSCI 2014: 2014 IEEE Symposium Series on Computational Intelligence - RiiSS 2014: 2014 IEEE Symposium on Robotic Intelligence in Informationally Structured Space, Proceedings},
year={2015},
doi={10.1109/RIISS.2014.7009177},
art_number={7009177},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922591541&doi=10.1109%2fRIISS.2014.7009177&partnerID=40&md5=9a8e923143007069c93c512b48956a03},
affiliation={University of Portsmouth, Portsmouth, PO1 2DJ, United Kingdom},
abstract={Facial pose estimation is an important part for facial analysis such as face and facial expression recognition. In most existing methods, facial features are essential for facial pose estimation. However, occluded key features and uncontrolled illumination of face images make the facial feature detection vulnerable. In this paper, we propose methods for facial pose estimation via dense reconstruction and sparse representation but avoid localizing facial features. The Sparse Representation Classifier (SRC) method has achieved successful results in face recognition. In this paper, we explore SRC in pose estimation. Sparse representation learns a dictionary of base functions, so each input pose can be approximated by a linear combination of just a sparse subset of the bases. The experiment conducted on the CMU Multiple face database has shown the effectiveness of the proposed method. © 2014 IEEE.},
author_keywords={3D face;  human face;  linear regression;  pose analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Said2015,
author={Said, S. and Jemai, O. and Zaied, M. and Ben Amar, C.},
title={3D fast wavelet network model-assisted 3D face recognition},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9875},
doi={10.1117/12.2228368},
art_number={98750E},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958206236&doi=10.1117%2f12.2228368&partnerID=40&md5=e95e90acc2067f0a5a38e384b37d87d2},
affiliation={REsearch Groups in Intelligent Machines (REGIM-Lab), University of Sfax, National Engineering School of Sfax, BP 1173, Sfax, 3038, Tunisia},
abstract={In last years, the emergence of 3D shape in face recognition is due to its robustness to pose and illumination changes. These attractive benefits are not all the challenges to achieve satisfactory recognition rate. Other challenges such as facial expressions and computing time of matching algorithms remain to be explored. In this context, we propose our 3D face recognition approach using 3D wavelet networks. Our approach contains two stages: learning stage and recognition stage. For the training we propose a novel algorithm based on 3D fast wavelet transform. From 3D coordinates of the face (x,y,z), we proceed to voxelization to get a 3D volume which will be decomposed by 3D fast wavelet transform and modeled after that with a wavelet network, then their associated weights are considered as vector features to represent each training face. For the recognition stage, an unknown identity face is projected on all the training WN to obtain a new vector features after every projection. A similarity score is computed between the old and the obtained vector features. To show the efficiency of our approach, experimental results were performed on all the FRGC v.2 benchmark. © 2015 SPIE.},
author_keywords={3D face recognition;  Fast wavelet transform;  Wavelet network},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{ElRai2015144,
author={El Rai, M.C. and Werghi, N. and Tortorici, C. and Al-Muhairi, H. and Al Safar, H.},
title={Mesh LBP features for 3D constrained local model},
journal={2015 International Conference on Information and Communication Technology Research, ICTRC 2015},
year={2015},
pages={144-147},
doi={10.1109/ICTRC.2015.7156442},
art_number={7156442},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944128208&doi=10.1109%2fICTRC.2015.7156442&partnerID=40&md5=f31b9796bfb499a038cffd35fa3bf121},
affiliation={Khalifa University of Science, Technology and Research, Electrical and Computer Engineering Department, United Arab Emirates},
abstract={We propose an automatic facial landmarks detection in 3D mesh manifold. The method is based on 3D Constrained Local Model (CLM) which learns both global variations in 3D face scan and local ones around every vertex landmark. Differently from the other approaches of CLM, our contribution is a full 3D mesh. The framework exploits the intrinsic 3D features around the mesh vertices by utilizing histogram-based mesh Local Binary Patterns (mesh-LBP). The experiments are conducted on publicly available 3D face scans Bosphorus database. © 2015 IEEE.},
author_keywords={3D Facial landmarks;  Constrained local model;  Histograms-based mesh LBP;  Landmarks detection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sibbing20159,
author={Sibbing, D. and Kobbelt, L.},
title={Data driven 3D face tracking based on a facial deformation model},
journal={VMV 2015 - Vision, Modeling and Visualization},
year={2015},
pages={9-16},
doi={10.2312/vmv.2015125},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018329328&doi=10.2312%2fvmv.2015125&partnerID=40&md5=7f879795487cd2b31b323608dc0a286a},
affiliation={RWTH Aachen University, Germany},
abstract={We introduce a new markerless 3D face tracking approach for 2D video streams captured by a single consumer grade camera. Our approach is based on tracking 2D features in the video and matching them with the projection of the corresponding feature points of a deformable 3D model. By this we estimate the initial shape and pose of the face. To make the tracking and reconstruction more robust we add a smoothness prior for pose changes as well as for deformations of the faces. Our major contribution lies in the formulation of the smooth deformation prior which we derive from a large database of previously captured facial animations showing different (dynamic) facial expressions of a fairly large number of subjects. We split these animation sequences into snippets of fixed length which we use to predict the facial motion based on previous frames. In order to keep the deformation model compact and independent from the individual physiognomy, we represent it by deformation gradients (instead of vertex positions) and apply a principal component analysis in deformation gradient space to extract the major modes of facial deformation. Since the facial deformation is optimized during tracking, it is particularly easy to apply them to other physiognomies and thereby re-target the facial expressions. We demonstrate the effectiveness of our technique on a number of examples. © 2015 The Eurographics Association.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qu2015139,
author={Qu, C. and Herrmann, C. and Monari, E. and Schuchert, T. and Beyerer, J.},
title={3D vs. 2D: On the Importance of Registration for Hallucinating Faces under Unconstrained Poses},
journal={Proceedings -2015 12th Conference on Computer and Robot Vision, CRV 2015},
year={2015},
pages={139-146},
doi={10.1109/CRV.2015.26},
art_number={7158332},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943144222&doi=10.1109%2fCRV.2015.26&partnerID=40&md5=14ed73c33bdc3b35422e6bb3bb4a6f03},
affiliation={Vision and Fusion Laboratory, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Fraunhofer Institute of Optronics, System Technologies and Image Exploitation (Fraunhofer IOSB), Karlsruhe, Germany},
abstract={Face Hallucination (FH) differs from generic single-image super-resolution (SR) algorithms in its specific domain of application. By exploiting the common structures of human faces, magnification of lower resolution images can be achieved. Despite the growing interest in recent years, considerably less attention is paid to a crucial step in FH-registration of facial images. In this work, registration techniques employed in the literature are first summarized and the importance of using well-aligned training and test images is demonstrated. A novel method to inversely map the high-resolution (HR) 3D training texture to the low-resolution (LR) 2D test image in arbitrary poses is then presented, which prevents information loss in LR images and is thus beneficial to SR. The effectiveness of our 3D approach is evaluated on the Multi-PIE and the PUT face databases. Superior qualitative and quantitative FH results to the state-of-the-art methods in all tested poses prove the necessity of accurate registration in FH. The merit of 3D FH in generating super-resolved frontal faces is also verified, revealing 30% improvement in face recognition over the 2D approach under 30° of yaw rotation on the Multi-PIE dataset. © 2015 IEEE.},
author_keywords={3D face modeling;  3D morphable model;  Face hallucination;  image registration;  super-resolution;  texture extraction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tseng2015,
author={Tseng, C.-Y. and Wang, I.-J. and Chu, C.-H.},
title={Product personalization using 3D parametric face models: An example of the eyeglass frame design},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2015},
volume={1B-2015},
doi={10.1115/DETC2015-47065},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979052826&doi=10.1115%2fDETC2015-47065&partnerID=40&md5=10bfc8ef027be99960e2e0ad957b5b78},
affiliation={Department of Industrial Engineering and Engineering Management, National Tsing Hua University, Hsinchu City, Taiwan},
abstract={Personalized design enhances the values added by a product or service by satisfying individual customer requirements. It has become a trend in consumer product development nowadays. This paper proposes a method for design personalization of the eyeglasses frame using anthropometric data. Three-dimensional face models were constructed using non-contact scanning devices. Principal Component Analysis (PCA) was applied to reduce the data complexity while preserving sufficient data variance. Kriging based parametric models correlate the mesh point coordinates of a face model to a set of feature parameters. The correlation allows synthesizing and controlling 3D facial geometry approximating to individual users with given parameter values. Rendering the synthesized geometry with human face images generates realistic face models. These models not only allow adjusting the frame design in real-time, but also evaluating whether or how the design style fits individual face characteristics. This study enhances the practical values of 3D anthropometric data by realizing the concept of human-centric design. © Copyright 2015 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Böer2015299,
author={Böer, G. and Hahmann, F. and Buhr, I. and Essig, H. and Schramm, H.},
title={Detection of facial landmarks in 3D face scans using the discriminative generalized hough transform (DGHT)},
journal={Informatik aktuell},
year={2015},
pages={299-304},
doi={10.1007/978-3-662-46224-9_52},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012215360&doi=10.1007%2f978-3-662-46224-9_52&partnerID=40&md5=3b2ad1354317200c24de231f3f2bfcf5},
affiliation={Institute of Applied Computer Science, University of Applied Sciences Kiel, Germany; Hannover Medical School, Germany; University Hospital Zurich, Switzerland; Faculty of Engineering, University of Kiel, Germany},
abstract={This paper presents the Discriminative Generalized Hough Transform (DGHT) as a technique to localize landmarks in 3D face scans. While the DGHT has been successfully used for the detection of landmarks in 2D and 3D images this work extends the framework to be used with triangle meshes for the first time. Instead of edge features and their respective gradient direction, the relative positions and orientations of the mesh faces are utilized to describe the geometric structures which are relevant for the detection of a specific landmark. Implementing a coarse-to-fine strategy at first a decimated version of the mesh is used to locate the global region of the point of interest, followed by more detailed localizations on higher resolution meshes. The utilized shape models are created in an automated, discriminative training process which assigns individual weights to the single model points, aiming at an increased localization rate. The technique has been applied to detect 38 anthropometric facial landmarks on 99 3D face scans. With an average error of 1.9mm, the most accurate detection was performed for the right alare, the average error when considering all landmarks amounts to 5.1 mm. © Springer-Verlag Berlin Heidelberg 2015.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Andreu-Cabedo2015,
author={Andreu-Cabedo, Y. and Castellano, P. and Colantonio, S. and Coppini, G. and Favilla, R. and Germanese, D. and Giannakakis, G. and Giorgi, D. and Larsson, M. and Marraccini, P. and Martinelli, M. and Matuszewski, B. and Milanic, M. and Pascali, M. and Pediaditis, M. and Raccichini, G. and Randeberg, L. and Salvetti, O. and Stromberg, T.},
title={Mirror mirror on the wall⋯ An intelligent multisensory mirror for well-being self-assessment},
journal={Proceedings - IEEE International Conference on Multimedia and Expo},
year={2015},
volume={2015-August},
doi={10.1109/ICME.2015.7177468},
art_number={7177468},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946038300&doi=10.1109%2fICME.2015.7177468&partnerID=40&md5=e74b7c1779d66dd9a05849e23fa7c077},
affiliation={University of Central Lancashire, Preston, United Kingdom; National Research Council of Italy, Pisa, Italy; Linköping University, Linköping, Sweden; Institute of Computer Science, Foundation for Research and Technology, Heraklion, Greece; Norvegian University of Science and Technology, Trondheim, Norway},
abstract={The face reveals the healthy status of an individual, through a combination of physical signs and facial expressions. The project SEMEOTICONS is translating the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, images, and 3D scans of the face. SEMEOTICONS is developing a multisensory platform, in the form of a smart mirror, looking for signs related to cardio-metabolic risk. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. Building the multisensory mirror requires addressing significant scientific and technological challenges, from touch-less data acquisition, to real-time processing and integration of multimodal data. © 2015 IEEE.},
author_keywords={3D face detection and tracking;  3D morphometric analysis;  breath analysis;  Cardio-metabolic risk;  multimodal data integration;  multispectral imaging;  psycho-somatic status recognition;  unobtrusive health monitoring},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Svoboda2015452,
author={Svoboda, J. and Bronstein, M.M. and Drahansky, M.},
title={Contactless biometric hand geometry recognition using a low-cost 3D camera},
journal={Proceedings of 2015 International Conference on Biometrics, ICB 2015},
year={2015},
pages={452-457},
doi={10.1109/ICB.2015.7139109},
art_number={7139109},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943329089&doi=10.1109%2fICB.2015.7139109&partnerID=40&md5=088b980bbb44eef1cfc61d3c229e1b06},
affiliation={Faculty of Informatics, Universita della Svizzera Italiana, Lugano, Switzerland; Faculty of Information Technology, Brno University of Technology, Brno, Czech Republic},
abstract={In the past decade, the interest in using 3D data for biometric person authentication has increased significantly, propelled by the availability of affordable 3D sensors. The adoption of 3D features has been especially successful in face recognition applications, leading to several commercial 3D face recognition products. In other biometric modalities such as hand recognition, several studies have shown the potential advantage of using 3D geometric information, however, no commercial-grade systems are currently available. In this paper, we present a contactless 3D hand recognition system based on the novel Intel RealSense camera, the first mass-produced embeddable 3D sensor. The small form factor and low cost make this sensor especially appealing for commercial biometric applications, however, they come at the price of lower resolution compared to more expensive 3D scanners used in previous research. We analyze the robustness of several existing 2D and 3D features that can be extracted from the images captured by the RealSense camera and study the use of metric learning for their fusion. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li201589,
author={Li, X. and Ruan, Q. and An, G. and Jin, Y. and Zhao, R.},
title={Multiple strategies to enhance automatic 3D facial expression recognition},
journal={Neurocomputing},
year={2015},
volume={161},
pages={89-98},
doi={10.1016/j.neucom.2015.02.063},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929048022&doi=10.1016%2fj.neucom.2015.02.063&partnerID=40&md5=564417aac016e7330021937c4890cec0},
affiliation={Institute of Information Science, Beijing Jiaotong University, Beijing, 100044, China; Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, 100044, China},
abstract={The research on 3D facial expression recognition has attracted numbers of interests due to its superiority to 2D data and it has been greatly promoted in recent years. However, its performance needs to be further improved and its data structure needs to be further analyzed to keep its automation well as the mesh structure of 3D face models cannot be applied directly to algebraic operations. This paper addresses these problems with multiple strategies, so that 3D facial expression recognition can be automatically implemented and its performance is subsequently enhanced. Firstly, an image-like-structure is proposed to represent the 3D face models, so that algebraic operations can be directly applied to analyze 3D data. Based on this image-like-structure, the strategies of irregular division schemes and the entropy weighted blocks are employed to improve the recognition accuracy. The former aims to keep the integrity of local structure; the latter is employed to emphasize the contribution of different facial regions. Both of them can be separately or jointly, utilized to facial feature descriptors. With the remarkable experimental results based on LBP and LTP, we can conclude that these strategies are available to promote the performance of automatic 3D facial expression recognition, which draws a promising direction for automatic 3D facial expression recognition. © 2015 Elsevier B.V.},
author_keywords={Automatic 3D facial expression recognition;  Block weighted strategy;  Image-like-structure;  Irregular division;  Multiple strategies},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2015732,
author={Li, D. and Lam, K.-M.},
title={Design and learn distinctive features from pore-scale facial keypoints},
journal={Pattern Recognition},
year={2015},
volume={48},
number={3},
pages={732-745},
doi={10.1016/j.patcog.2014.09.026},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916623300&doi=10.1016%2fj.patcog.2014.09.026&partnerID=40&md5=6208ed5c0007d45fdca2b5e12b229462},
affiliation={Centre for Signal Processing, Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Hong Kong},
abstract={Establishing correct correspondences between two faces with different viewpoints has played an important role in 3D face reconstruction and other computer-vision applications. Usually, face images are considered to lack sufficient distinctive features to establish a large number of correspondences on uncalibrated images. In this paper, we investigate pore-scale facial features, which are formed from pores, fine wrinkles, and hair. These features have many characteristics that make them suitable for matching facial images under different variations. Using both biological observation and computer-vision consideration, a new framework is devised for pore-scale facial-feature extraction and matching. The matching difficulty under various skin appearances of different subjects and imaging distortion is also analyzed. For further improving the matching performance and tackling distortions such as varying illuminations and unfocused blurring, a pore-to-pore correspondences dataset is established for training a more distinctive and compact descriptor. Experiments are conducted on a face database containing 105 subjects, and the results prove that the pore-scale features are highly distinctive; face images with a minimum resolution of 600×700 (0.4 mega) pixels contain sufficient details to perform a reliable matching in different poses. Generally, our algorithm can establish between 500 and 2000 correct correspondences on a pair of uncalibrated face images of the same person. Furthermore, the proposed methods can be applied to face recognition, 3D reconstruction, etc. © 2014 Elsevier Ltd. All rights reserved.},
author_keywords={Face matching;  Face recognition;  Face reconstruction;  Feature extraction;  Pore-scale facial feature},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xia2015746,
author={Xia, B. and Ben Amor, B. and Drira, H. and Daoudi, M. and Ballihi, L.},
title={Combining face averageness and symmetry for 3D-based gender classification},
journal={Pattern Recognition},
year={2015},
volume={48},
number={3},
pages={746-758},
doi={10.1016/j.patcog.2014.09.021},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916613241&doi=10.1016%2fj.patcog.2014.09.021&partnerID=40&md5=0253ea2e68cb8c21c444aab8203bfccd},
affiliation={University Lille1 - Sciences and Technology, France; Institut Mines-Télécom/Télécom Lille, France; LIFL (UMR CNRS 8022), France},
abstract={Although human face averageness and symmetry are valuable clues in social perception (such as attractiveness, masculinity/femininity, and healthy/ sick), in the literature of facial attribute recognition, little consideration has been given to them. In this work, we propose to study the morphological differences between male and female faces by analyzing the averageness and symmetry of their 3D shapes. In particular, we address the following questions: (i) is there any relationship between gender and face averageness/symmetry? and (ii) if this relationship exists, which specific areas on the face are involved? To this end, we propose first to capture densely both the face shape averageness (AVE) and symmetry (SYM) using our Dense Scalar Field (DSF), which denotes the shooting directions of geodesics between facial shapes. Then, we explore such representations by using classical machine learning techniques, the Feature Selection (FS) methods and Random Forest (RF) classification algorithm. Experiments conducted on the FRGCv2 dataset show that a significant relationship exists between gender and facial averageness/symmetry when achieving a classification rate of 93.7% on the 466 earliest scans of subjects (mainly neutral) and 92.4% on the whole FRGCv2 dataset (including facial expressions). © 2014 Elsevier Ltd. All rights reserved.},
author_keywords={3D Face;  Dense scalar field;  Face averageness;  Face symmetry;  Feature selection;  Gender classification;  Random Forest},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bolkart2015100,
author={Bolkart, T. and Wuhrer, S.},
title={3D faces in motion: Fully automatic registration and statistical analysis},
journal={Computer Vision and Image Understanding},
year={2015},
volume={131},
pages={100-115},
doi={10.1016/j.cviu.2014.06.013},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918820441&doi=10.1016%2fj.cviu.2014.06.013&partnerID=40&md5=e670c914333a5b8a1c85ad8664292632},
affiliation={Saarland University, Saarbrücken, Germany},
abstract={This paper presents a representation of 3D facial motion sequences that allows performing statistical analysis of 3D face shapes in motion. The resulting statistical analysis is applied to automatically generate realistic facial animations and to recognize dynamic facial expressions. To perform statistical analysis of 3D facial shapes in motion over different subjects and different motion sequences, a large database of motion sequences needs to be brought in full correspondence. Existing algorithms that compute correspondences between 3D facial motion sequences either require manual input or suffer from instabilities caused by drift. For large databases, algorithms that require manual interaction are not practical. We propose an approach to robustly compute correspondences between a large set of facial motion sequences in a fully automatic way using a multilinear model as statistical prior. In order to register the motion sequences, a good initialization is needed. We obtain this initialization by introducing a landmark prediction method for 3D motion sequences based on Markov Random Fields. Using this motion sequence registration, we find a compact representation of each motion sequence consisting of one vector of coefficients for identity and a high dimensional curve for expression. Based on this representation, we synthesize new motion sequences and perform expression recognition. We show experimentally that the obtained registration is of high quality, where 56% of all vertices are at distance at most 1 mm from the input data, and that our synthesized motion sequences look realistic. © 2014 Elsevier Inc. All rights reserved.},
author_keywords={Motion sequence registration;  Statistical analysis;  Statistical model fitting;  Statistical shape space},
document_type={Article},
source={Scopus},
}

@ARTICLE{Berssenbrügge201539,
author={Berssenbrügge, P. and Lingemann-Koch, M. and Abeler, A. and Runte, C. and Jung, S. and Kleinheinz, J. and Denz, C. and Dirksen, D.},
title={Measuring facial symmetry: A perception-based approach using 3D shape and color},
journal={Biomedizinische Technik},
year={2015},
volume={60},
number={1},
pages={39-47},
doi={10.1515/bmt-2014-0024},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925359029&doi=10.1515%2fbmt-2014-0024&partnerID=40&md5=ce7851ea3409282a880c96db81f6d4da},
affiliation={Department of Prosthetic Dentistry and Biomaterials, University of Münster, Albert-Schweitzer-Campus 1, Building W30, Münster, 48149, Germany; Department of Maxillofacial Surgery, University of Münster, Albert-Schweitzer-Campus 1, Building W30, Münster, 48149, Germany; Institute of Applied Physics, University of Münster, Corrensstrasse 2, Münster, 48149, Germany},
abstract={Objective: Facial symmetry is an important factor affecting esthetics. Thus, its restoration is an essential task in maxillofacial surgery. The aim of this study is to develop an objective measure of facial asymmetry by a novel approach where both the shape and the color are taken into account and to validate its correlation with perception. Methods: Optical three-dimensional (3D) face scans of 30 healthy adults are performed. Face-specific asymmetry indices are calculated by quantifying color differences as well as spatial distances between 3D data of a face and its mirrored copy. Subjective ratings of symmetry and attractiveness of the faces by 100 subjects are used to validate these indices. Results: The symmetry ratings show significant correlations with color and geometric asymmetry indices. The attractiveness ratings correlate only weakly with both indices. However, the product of the indices exhibits significant correlations with both attractiveness and symmetry ratings. Conclusion: The presented combined asymmetry index comprising shape and coloring turned out to reflect subjective perception of both facial symmetry and attractiveness. It thus promises to be a valid objective measure for facial esthetics, which could contribute, e.g., to the evaluation of surgical methods as well as to the design of craniofacial prostheses. © 2015, WDG. All rights reserved.},
author_keywords={Color asymmetry;  Correlation of measurements and subjective ratings;  Facial attractiveness;  Facial symmetry;  Optical 3D data acquisition;  Perception},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wahab20151042,
author={Wahab, W. and Ridwan, M. and Kusumoputro, B.},
title={Design and implementation of an automatic face-image data acquisition system using IP based multi camera},
journal={International Journal of Technology},
year={2015},
volume={6},
number={6},
pages={1042-1049},
doi={10.14716/ijtech.v6i6.1848},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954155041&doi=10.14716%2fijtech.v6i6.1848&partnerID=40&md5=56b7375b3caf0fb41203d8036a10e9c3},
affiliation={Department of Electrical Engineering, Faculty of Engineering, Universitas Indonesia, Kampus Baru UI Depok, Depok, 16424, Indonesia; Informatics Engineering Study Program, Polytechnic Kampar, Jl. Tengku Muhammad Km.2, Bangkinang, Riau, 28461, Indonesia},
abstract={Current research trends in 3D Face recognition system requires a special hardware for fast capturing face image data from multi angle view. To support this research, we had designed and implemented an automatic image data acquisition system using multi-camera for capturing facial images from 5° different angle views, which spanned horizontally from 180° from left to right, and vertically from horizontal up to 70° above the face. The system was designed using 30 IP cameras that were mounted on two rigid steel arms that had the form of three quarter of a circle, the two steel arms formed the angle of 90° to each other. At each arm, 15 IP cameras were mounted with 5° spacing vertically to each others. This arm was driven by a DC motor which was controlled by a microcontroller and supervised directly by a laptop computer along with the data acquisition activities. The software for capturing images was designed using C# GUI programming language. The system had been working in good condition and image-data were saved in JPEG format. Time duration of capturing images data for one object face expression with 30 times capturing for the whole angle views, was only 3 minutes 44.5 seconds with total number of 16,650 images collected. The delay time between two cameras capturing was less than 1 sec. This project is aimed to support the 3D face recognition research in the department. © IJTech 2015.},
author_keywords={Data acquisition system;  Image data base;  Instrumentation;  IP camera;  Microcontroller},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2015297,
author={Li, X. and Ruan, Q. and Jin, Y. and An, G. and Zhao, R.},
title={Fully automatic 3D facial expression recognition using polytypic multi-block local binary patterns},
journal={Signal Processing},
year={2015},
volume={108},
pages={297-308},
doi={10.1016/j.sigpro.2014.09.033},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908273471&doi=10.1016%2fj.sigpro.2014.09.033&partnerID=40&md5=854ba111ed61286ffdfa7400709b96b6},
affiliation={Institute of Information Science, Beijing Jiaotong University, Beijing, 100044, China; Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, 100044, China},
abstract={3D facial expression recognition has been greatly promoted for overcoming the inherent drawbacks of 2D facial expression recognition and has achieved superior recognition accuracy to the 2D. In this paper, a novel holistic, full-automatic approach for 3D facial expression recognition is proposed. First, 3D face models are represented in 2D-image-like structure which makes it possible to take advantage of the wealth of 2D methods to analyze 3D models. Then an enhanced facial representation, namely polytypic multi-block local binary patterns (P-MLBP), is proposed. The P-MLBP involves both the feature-based irregular divisions to depict the facial expressions accurately and the fusion of depth and texture information of 3D models to enhance the facial feature. Based on the BU-3DFE database, three kinds of classifiers are employed to conduct 3D facial expression recognition for evaluation. Their experimental results outperform the state of the art and show the effectiveness of P-MLBP for 3D facial expression recognition. Therefore, the proposed strategy is validated for 3D facial expression recognition; and its simplicity opens a promising direction for fully automatic 3D facial expression recognition. © 2014 Elsevier B.V.},
author_keywords={3D facial expression recognition;  Automatic data normalization;  Feature fusion;  Feature-based irregular divisions;  P-MLBP},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pushparani201533938,
author={Pushparani, M. and Indumathi, T.},
title={Human authentication by matching 3d skull with face image using Scca},
journal={International Journal of Applied Engineering Research},
year={2015},
volume={10},
number={14},
pages={33938-33948},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940062487&partnerID=40&md5=94160469b47077f0f6b1aa95ce98f35e},
affiliation={Dept. of Computer science, Mother Teresa Women’s University, India},
abstract={In networked society the people perform many e-commerce activities in daily life. In such applications personal identification is critically important. Biometric identifiers are replacing traditional identifiers, as it is difficult to steal, replace, forget or transfer them. It is possible that, a 2D-image based facial recognition system can be easily spoofed with simple tricks and some poorly-designed systems have even been shown to be fooled by the imposters. Spoofing with photograph or video is one of the most common manners to circumvent a face recognition system. It becomes easier to spoof in these biometric systems with the aid of fake biometric; it further reduces the reliability and security of biometric system. In this paper, face spoof attack many biometric, applying skull identification. We will be exploring the techniques that are more secure and reliable. In skull identification, nearly all of the methods depend on accurate extraction and representation of the relationship between the skull and face. However, it is very difficult to extract this complex relationship. Because this work aims to identify human face is from skull. This paper proposes a skull identification method that matches a skull with enrolled faces, in which the mapping between the skull and face is obtained using enhance canonical correlation coefficient analysis with scale invariant feature transform (SIFT). Here a statistic method is adopted to estimate outlook from subclass of skull-face database using Principle component analysis and Linear discriminant analysis (LDA). In order to improve the accuracy of the result, we select the suitable organ (eyes, nose and mouth) for the statistic result based on anatomy principle from the database and achieve the organ and face integration to build the final outlook, a method to build a joint statistical 3D model of the skull and face is presented. The Second approach for enhancing the matching performance of AAM is to AAM itself, by proposing a novel fitting algorithm or enhancing the existing fitting algorithms. In this proposed a fast AAM using enhance canonical correlation coefficient analysis (ECCCA), which has modeled the relation between differences of the image and the model parameter for improving the convergence speed of fitting algorithm. We propose to identify an skull through using a correlation measure between the 3D skull and 3D face in terms of the morphology, and measure the correlation using Enhance canonical correlation coefficient analysis (ECCCA).We use the 3D skull data as the probe and 3D face geometric data as the gallery, and match the skull with enrolled 3D faces by the correlation measure between the probe and the gallery. The Third approach for scale invariant feature transform (SIFT) bundles a feature detector and a feature descriptor. The detector extracts from an image a number of frames (attributed regions) in a way which is consistent with (some) variations of the illumination, viewpoint and other viewing conditions. © Research India Publications.},
author_keywords={Active appearance modal (AAM);  Enhance canonical correlation coefficient analysis (ECCCA);  Linear discriminant analysis(LDA);  Principle component analysis (PCA);  Scale invariant feature transform (SIFT);  Security;  Skull identification},
document_type={Article},
source={Scopus},
}

@ARTICLE{Duan2015674,
author={Duan, F. and Huang, D. and Tian, Y. and Lu, K. and Wu, Z. and Zhou, M.},
title={3D face reconstruction from skull by regression modeling in shape parameter spaces},
journal={Neurocomputing},
year={2015},
volume={151},
number={P2},
pages={674-682},
doi={10.1016/j.neucom.2014.04.089},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919479161&doi=10.1016%2fj.neucom.2014.04.089&partnerID=40&md5=5ca5558ce2e3a8cb365ced144f3b67d3},
affiliation={College of Information Science and Technology, Beijing Normal University, Beijing, 100875, China; College of Navigation and Aerospace Engineering, The PLA Information Engineering University, Zhengzhou, 450002, China; College of Engineering and Information Technology, University of Chinese Academy of Sciences, Beijing, 100049, China},
abstract={Craniofacial reconstruction is to estimate a person[U+05F3]s face model from the skull. It can be applied in many fields such as forensic medicine, face animation. In this article, a regression modeling based method for craniofacial reconstruction is proposed, in which a statistical shape model is built for skulls and faces, respectively, and the relationship between them is extracted in the shape parameter spaces through partial least squares regression (PLSR). Craniofacial reconstruction is realized by using the relationship and the face statistical shape model. To better represent craniofacial shape variations and boost the reconstruction, both the skull and face are divided into five corresponding feature regions, and a mapping from each skull region to the corresponding face region is established. For an unknown skull, the five face regions are obtained through the five mappings, and the face is recovered by stitching the five face regions. The attributes such as age and body mass index (BMI) can be added into the mappings to achieve the face reconstruction with different attributes. Compared with other statistical learning based methods in literature, the proposed method more directly and reasonably reflects the relationship that the face shape is determined by the skull and influenced by some attributes. In addition, the proposed method does not need to locate landmarks, whose quantity and accuracy can highly affect the reconstruction. Experimental results validate the proposed method. © 2014 Elsevier B.V.},
author_keywords={Craniofacial reconstruction;  PLSR;  Statistical shape model},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abdelmalik20151,
author={Abdelmalik, O. and Mébarka, B. and Abdelhamid, B. and Mohamed, B. and Abdelmalik, T.A.},
title={Identification of faces by multimodal information fusion of depth and color},
journal={Journal of Electrical Engineering},
year={2015},
volume={15},
number={3},
pages={1-8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952907399&partnerID=40&md5=3b48061d17a217a61cdbe2faa9d8fc66},
affiliation={LMSE, Université Mohamed Khider Biskra, Algeria; LGB, Université Mohamed Khider Biskra, Algeria; LAMIH, Université de Valenciennes, France},
abstract={Recognition of traditional optical faces of color images or intensity presents many challenges, such as variations in lighting, pose and expression. In fact, the human face not only generates 2D texture information, but also 3D shape information. In this paper, we examine what information the contributions of depth and color to make facial recognition when the variation in lighting and expression are taken into account. We present three methods of feature extraction based on reduction of one-dimensional space: the Linear Discriminant Analysis (LDA), Enhanced Fisher Linear Discriminant Model (EFM) and the Direct LDA (DLDA). A theoretical presentation of these approaches and their applications on the depth images and color is made. It is also a comparative study on information fusion of depth and color for both levels: characteristics and scores to select the most effective features and robust and thus build a strong classifier. The concatenation of feature vectors and fusion of the pixels of the image depth and color: the average, the product, the minimum and maximum are used in the case of fusion characteristics. For the merger to level scores, we used the fuzzy Sugeno integral and Choquet and support vector machines (SVM). The experiments are performed on the database CASIA 3D Face, complex data sets with variations, including variations in lighting, expression and longstanding failures between two scans. The experimental results show the promising performance of the proposed system. Note that in our system, all processes are performed automatically.},
author_keywords={Color images;  Depth images;  Fusion;  Reduce space},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Balaban2015,
author={Balaban, S.},
title={Deep learning and face recognition: The state of the art},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9457},
doi={10.1117/12.2181526},
art_number={94570B},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948695714&doi=10.1117%2f12.2181526&partnerID=40&md5=4fface00f7dff75414e41e75454d8bdd},
affiliation={Lambda Labs, 536 Hawthorne Ave., Palo Alto, CA  94301, United States},
abstract={Deep Neural Networks (DNNs) have established themselves as a dominant technique in machine learning. DNNs have been top performers on a wide variety of tasks including image classification, speech recognition, and face recognition.1-3 Convolutional neural networks (CNNs) have been used in nearly all of the top performing methods on the Labeled Faces in the Wild (LFW) dataset.3-6 In this talk and accompanying paper, I attempt to provide a review and summary of the deep learning techniques used in the state-of-the-art. In addition, I highlight the need for both larger and more challenging public datasets to benchmark these systems. Despite the ability of DNNs and autoencoders to perform unsupervised feature learning, modern facial recognition pipelines still require domain specific engineering in the form of re-alignment. For example, in Facebook's recent DeepFace paper, a 3D «frontalization» step lies at the beginning of the pipeline. This step creates a 3D face model for the incoming image and then uses a series of affine transformations of the fiducial points to «frontalize» the image. This step enables the DeepFace system to use a neural network architecture with locally connected layers without weight sharing as opposed to standard convolutional layers.6 Deep learning techniques combined with large datasets have allowed research groups to surpass human level performance on the LFW dataset.3, 5 The high accuracy (99.63% for FaceNet at the time of publishing) and utilization of outside data (hundreds of millions of images in the case of Google's FaceNet) suggest that current face verification benchmarks such as LFW may not be challenging enough, nor provide enough data, for current techniques.3, 5 There exist a variety of organizations with mobile photo sharing applications that would be capable of releasing a very large scale and highly diverse dataset of facial images captured on mobile devices. Such an «ImageNet for Face Recognition» would likely receive a warm welcome from researchers and practitioners alike. © 2015 SPIE.},
author_keywords={biometrics;  Deep Learning;  Face identification;  Face verification;  Facial Recognition;  Feature Learning;  Representation Learning},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Paul201563,
author={Paul, O.I. and Lu, Y.},
title={Three-dimensional (3D) facial recognition and prediction},
journal={International Journal of Electrical Engineering},
year={2015},
volume={22},
number={2},
pages={63-72},
doi={10.6329/CIEE.2015.2.04},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948449021&doi=10.6329%2fCIEE.2015.2.04&partnerID=40&md5=bf2146992f03c3829486315d16f3e9a6},
affiliation={NUAA, Aeronautical Engineering, and Mechanical and Electrical Engineering, United States; Nanjing University of Aeronautics and Astronautics, China},
abstract={This paper provides solution to the problem in identifying humans from their three dimensional facial characteristics. For this reason a standard 3D facial recognition system was built and used in this research work. The system which also consist of several other systems were divided into simpler form for proper analysis and better performances. The sub-system consist of: registration, representation of faces, extraction of discriminative features, and fusion of matchers. For each of the sub-system, this paper evaluates the state of the art methods, and also propose new and better ones. This research uses generic face model which speeds up the correspondence establishment process. In facial representation schemes, implementation of diverse range of approaches such as point clouds, curvature-based descriptors, and range images were implored. While various feature extraction methods were used to determine the discriminative facial features. An in-depth analysis of decision-level fusion algorithms was perform. In addition to the evaluation of baseline fusion methods, we propose to use two novel fusion schemes where the first one employs a confidence-aided combination approach, and the second one implements a two-level serial integration method. Recognition simulations performed on the 3DRMA and the FRGC databases show that: generic face template-based rigid registration of faces is better than the non-rigid variant, principal curvature directions and surface normals have better discriminative power, representing faces using local patch descriptors can both reduce the feature dimensionality and improve the identification rate, and confidence-assisted fusion rules and serial two-stage fusion schemes have a potential to improve the accuracy when compared to other decision-level fusion rules.},
author_keywords={Face representation;  Facial recognition;  Feature extraction;  Registration;  Three-dimensional},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shahzad2015205,
author={Shahzad, M. and Schmitt, M. and Zhu, X.X.},
title={Segmentation and crown parameter extraction of individual trees in an airborne TomoSAR point cloud},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2015},
volume={40},
number={3W2},
pages={205-209},
doi={10.5194/isprsarchives-XL-3-W2-205-2015},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925345206&doi=10.5194%2fisprsarchives-XL-3-W2-205-2015&partnerID=40&md5=efc09ad19931100ccbf038f9280a60e3},
affiliation={Helmholtz Young Investigators Group SiPEO, Technische Universitaet Muenchen (TUM), Munich, Germany},
abstract={The analysis of individual trees is an important field of research in the forest remote sensing community. While the current state-of-theart mostly focuses on the exploitation of optical imagery and airborne LiDAR data, modern SAR sensors have not yet met the interest of the research community in that regard. This paper describes how several critical parameters of individual deciduous trees can be extraced from airborne multi-aspect TomoSAR point clouds: First, the point cloud is segmented by unsupervised mean shift clustering. Then ellipsoid models are fitted to the points of each cluster. Finally, from these 3D ellipsoids the geometrical tree parameters location, height and crown radius are extracted. Evaluation with respect to a manually derived reference dataset prove that almost 86% of all trees are localized, thus providing a promising perspective for further research towards individual tree recognition from SAR data.},
author_keywords={3D reconstruction;  Forested areas;  Multi-aspect;  Synthetic Aperture Radar (SAR);  TomoSAR point clouds;  Trees},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pang201522,
author={Pang, G. and Qiu, R. and Huang, J. and You, S. and Neumann, U.},
title={Automatic 3D industrial point cloud modeling and recognition},
journal={Proceedings of the 14th IAPR International Conference on Machine Vision Applications, MVA 2015},
year={2015},
pages={22-25},
doi={10.1109/MVA.2015.7153124},
art_number={7153124},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941209093&doi=10.1109%2fMVA.2015.7153124&partnerID=40&md5=f590ed90b19406b37c0de5a9cc87311f},
affiliation={University of Southern California, United States},
abstract={3D modeling of point clouds is an important but time-consuming process, inspiring extensive research in automatic methods. Prior efforts focus on primitive geometry, street structures or indoor objects, but industrial data has rarely been pursued. Our work presents a method for automatic modeling and recognition of 3D industrial site point clouds, dividing the task into 3 separate sub-problems: pipe modeling, plane classification, and object recognition. The results are integrated to obtain the complete model, revealing some issues during the integration, solved by utilizing information gained from each individual process. Experiments show that the presented method automatically models large and complex industrial scenes with a quality that outperforms leading commercial modeling software and is comparable to professional hand-made models. © 2015 MVA organization.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang20151991,
author={Zhang, H. and Reardon, C. and Zhang, C. and Parker, L.E.},
title={Adaptive human-centered representation for activity recognition of multiple individuals from 3D point cloud sequences},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
year={2015},
volume={2015-June},
number={June},
pages={1991-1998},
doi={10.1109/ICRA.2015.7139459},
art_number={7139459},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938240196&doi=10.1109%2fICRA.2015.7139459&partnerID=40&md5=1487a4e32626d513e665613bc533caf3},
affiliation={Department of Electrical Engineering and Computer Science, Colorado School of Mines, Golden, CO  80401, United States; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN  37996, United States},
abstract={Activity recognition of multi-individuals (ARMI) within a group, which is essential to practical human-centered robotics applications such as childhood education, is a particularly challenging and previously not well studied problem. We present a novel adaptive human-centered (AdHuC) representation based on local spatio-temporal features (LST) to address ARMI in a sequence of 3D point clouds. Our human-centered detector constructs affiliation regions to associate LST features with humans by mining depth data and using a cascade of rejectors to localize humans in 3D space. Then, features are detected within each affiliation region, which avoids extracting irrelevant features from dynamic background clutter and addresses moving cameras on mobile robots. Our feature descriptor is able to adapt its support region to linear perspective view variations and encode multi-channel information (i.e., color and depth) to construct the final representation. Empirical studies validate that the AdHuC representation obtains promising performance on ARMI using a Meka humanoid robot to play multi-people Simon Says games. Experiments on benchmark datasets further demonstrate that our adaptive human-centered representation outperforms previous approaches for activity recognition from color-depth data. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vetrivel2015261,
author={Vetrivel, A. and Gerke, M. and Kerle, N. and Vosselman, G.},
title={Segmentation of UAV-based images incorporating 3D point cloud information},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2015},
volume={40},
number={3W2},
pages={261-268},
doi={10.5194/isprsarchives-XL-3-W2-261-2015},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925356664&doi=10.5194%2fisprsarchives-XL-3-W2-261-2015&partnerID=40&md5=c38ec216cf579315b4749832a1e0bcd2},
affiliation={University of Twente, Faculty of Geo-Information Science and Earth Observation (ITC), P.O. Box 217, Enschede, 7500 AE, Netherlands},
abstract={Numerous applications related to urban scene analysis demand automatic recognition of buildings and distinct sub-elements. For example, if LiDAR data is available, only 3D information could be leveraged for the segmentation. However, this poses several risks, for instance, the in-plane objects cannot be distinguished from their surroundings. On the other hand, if only image based segmentation is performed, the geometric features (e.g., normal orientation, planarity) are not readily available. This renders the task of detecting the distinct sub-elements of the building with similar radiometric characteristic infeasible. In this paper the individual sub-elements of buildings are recognized through sub-segmentation of the building using geometric and radiometric characteristics jointly. 3D points generated from Unmanned Aerial Vehicle (UAV) images are used for inferring the geometric characteristics of roofs and facades of the building. However, the image-based 3D points are noisy, error prone and often contain gaps. Hence the segmentation in 3D space is not appropriate. Therefore, we propose to perform segmentation in image space using geometric features from the 3D point cloud along with the radiometric features. The initial detection of buildings in 3D point cloud is followed by the segmentation in image space using the region growing approach by utilizing various radiometric and 3D point cloud features. The developed method was tested using two data sets obtained with UAV images with a ground resolution of around 1-2 cm. The developed method accurately segmented most of the building elements when compared to the plane-based segmentation using 3D point cloud alone.},
author_keywords={3D point cloud;  Building detection;  Image segmentation;  Region growing;  Texture;  UAV images},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xiong2015,
author={Xiong, H. and Xu, J. and Xu, C. and Pan, M.},
title={Parsing optical scanned 3D data by Bayesian inference},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9675},
doi={10.1117/12.2202969},
art_number={967532},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963628533&doi=10.1117%2f12.2202969&partnerID=40&md5=51c0402133ae343fb47f22d8c7db33e6},
affiliation={Faculty of Electromechanical Engineering, Guangdong University of Technology, Guangzhou Higher Education Mega Center, No. 100 Waihuan Xi Road, Panyu District, Guangzhou, China},
abstract={Optical devices are always used to digitize complex objects to get their shapes in form of point clouds. The results have no semantic meaning about the objects, and tedious process is indispensable to segment the scanned data to get meanings. The reason for a person to perceive an object correctly is the usage of knowledge, so Bayesian inference is used to the goal. A probabilistic And-Or-Graph is used as a unified framework of representation, learning, and recognition for a large number of object categories, and a probabilistic model defined on this And-Or-Graph is learned from a relatively small training set per category. Given a set of 3D scanned data, the Bayesian inference constructs a most probable interpretation of the object, and a semantic segment is obtained from the part decomposition. Some examples are given to explain the method. © Copyright 2015 SPIE.},
author_keywords={And-Or-Graph;  Bayesian inference;  point clouds process;  Semantic segment},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Linder20153039,
author={Linder, T. and Wehner, S. and Arras, K.O.},
title={Real-time full-body human gender recognition in (RGB)-D data},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
year={2015},
volume={2015-June},
number={June},
pages={3039-3045},
doi={10.1109/ICRA.2015.7139616},
art_number={7139616},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938232217&doi=10.1109%2fICRA.2015.7139616&partnerID=40&md5=cf0a657f513cc5cf2cb4d0468743ff5b},
affiliation={Social Robotics Lab, Dept. of Computer Science, University of Freiburg, Germany},
abstract={Understanding social context is an important skill for robots that share a space with humans. In this paper, we address the problem of recognizing gender, a key piece of information when interacting with people and understanding human social relations and rules. Unlike previous work which typically considered faces or frontal body views in image data, we address the problem of recognizing gender in RGB-D data from side and back views as well. We present a large, gender-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the Kinect v1 and Kinect v2 sensor. We then learn and compare several classifiers on the Kinect v2 data using a HOG baseline, two state-of-the-art deep-learning methods, and a recent tessellation-based learning approach. Originally developed for person detection in 3D data, the latter is able to learn the best selection, location and scale of a set of simple point cloud features. We show that for gender recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 Hz. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2015153,
author={Wang, Y. and Zhu, X.X.},
title={Semantic interpretation of INSAR estimates using optical images with application to urban infrastructure monitoring},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2015},
volume={40},
number={3W3},
pages={153-160},
doi={10.5194/isprsarchives-XL-3-W3-153-2015},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959367648&doi=10.5194%2fisprsarchives-XL-3-W3-153-2015&partnerID=40&md5=36125be2919898081d31202890b60daf},
affiliation={Helmholtz Young Investigators Group, SiPEO, Technische Universität München, Arcisstraße 21, Munich, 80333, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Oberpfaffenhofen, Weßling, 82234, Germany},
abstract={Synthetic aperture radar interferometry (InSAR) has been an established method for long term large area monitoring. Since the launch of meter-resolution spaceborne SAR sensors, the InSAR community has shown that even individual buildings can be monitored in high level of detail. However, the current deformation analysis still remains at a primitive stage of pixel-wise motion parameter inversion and manual identification of the regions of interest. We are aiming at developing an automatic urban infrastructure monitoring approach by combining InSAR and the semantics derived from optical images, so that the deformation analysis can be done systematically in the semantic/object level. This paper explains how we transfer the semantic meaning derived from optical image to the InSAR point clouds, and hence different semantic classes in the InSAR point cloud can be automatically extracted and monitored. Examples on bridges and railway monitoring are demonstrated.},
author_keywords={Bridge monitoring;  InSAR;  Optical INSAR fusion;  Railway monitoring;  SAR;  Semantic classification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kashani2015,
author={Kashani, A.G. and Crawford, P.S. and Biswas, S.K. and Graettinger, A.J. and Grau, D.},
title={Automated tornado damage assessment and wind speed estimation based on terrestrial laser scanning},
journal={Journal of Computing in Civil Engineering},
year={2015},
volume={29},
number={3},
doi={10.1061/(ASCE)CP.1943-5487.0000389},
art_number={04014051},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928017564&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000389&partnerID=40&md5=edb6429622d1d18832fa35e1295ba633},
affiliation={Dept. of Civil, Construction and Environmental Engineering, Univ. of Alabama, P.O. Box 870205, Tuscaloosa, AL  35487-0205, United States; School of Sustainable Engineering and the Built Environment, Arizona State Univ., Tempe, AZ  85287-1404, United States},
abstract={There are more than 1,000 tornadoes in the United States each year, yet engineers do not typically design for tornadoes because of insufficient information about wind loads. Collecting building-level damage data in the aftermath of tornadoes can improve the understanding of tornado winds, but these data are difficult to collect because of safety, time, and access constraints. This study presents and tests an automated geographic information system (GIS) method using postevent point cloud data collected by terrestrial scanners and preevent aerial images to calculate the percentage of roof and wall damage and estimate wind speeds at an individual building scale. Simulations determined that for typical point cloud density (>25∈∈points/m2), a GIS raster cell size of 40-50 cm resulted in less than 10% error in damaged roof and wall detection. Data collected after recent tornadoes were used to correlate wind speed estimates and the percent of detected damage. The developed method estimated wind speeds from damage data collected after the 2011 Tuscaloosa, AL tornado at finer scales than the typical large-scale assessments done by reconnaissance engineers. © 2014 American Society of Civil Engineers.},
author_keywords={Assessment;  Automatic identification systems;  Damage;  Geographic information systems (GIS);  Laser scanning;  Natural disasters;  Three-dimensional analysis;  Wind speed},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Varney2015,
author={Varney, N.M. and Asari, V.K.},
title={Volumetrie features for object region classification in 3D LiDAR point clouds},
journal={Proceedings - Applied Imagery Pattern Recognition Workshop},
year={2015},
volume={2015-February},
number={February},
doi={10.1109/AIPR.2014.7041941},
art_number={7041941},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937960702&doi=10.1109%2fAIPR.2014.7041941&partnerID=40&md5=ea1da5516de97b2a378ad09755e02a9a},
affiliation={Department of Electrical and Computer Engineering, University of Dayton, Dayton, OH  45469, United States},
abstract={LiDAR data is a set of geo-spatially located points which contain (X, Y, Z) location and intensity data. This paper presents the extraction of a novel set of volume and texture-based features from segmented point clouds. First, the data is segmented into individual object regions using an automatic seeded region growing technique. Then, these object regions are normalized to a N × N × N voxel space, where each voxel contains information about the location and density of points within that voxel. A set of volumetric features are extracted to represent the object region; these features include: 3D form factor, rotation invariant local binary pattern (RILBP), fill, stretch, corrugation, contour, plainness and relative variance. The form factor, fill, and stretch provide a series of meaningful relationships between the volume, surface area, and shape of the object. RILBP provides a textural description from the height variation of the LiDAR data. The corrugation, contour, and plainness are extracted by 3D Eigen analysis of the object volume to describe the details of the object's surface. Relative variance provides an illustration of the distribution of points throughout the object. The new feature set is robust, and scale and rotation invariant for object region classification. The performance of the proposed feature extraction technique has been evaluated on a set of segmented and voxelized point cloud objects in a subset of the aerial LiDAR data from Surrey, British Columbia, which was available through the Open Data Program. The volumetric features, when used as an input to an SVM classifier, correctly classified the object regions with an accuracy of 97.5 %, with a focus on identifying five classes: ground, vegetation, buildings, vehicles, and barriers. © 2014 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Varney2015,
author={Varney, N.M. and Asari, V.K.},
title={Volume component analysis for classification of LiDAR data},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9477},
number={January},
doi={10.1117/12.2179268},
art_number={94770F},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937231577&doi=10.1117%2f12.2179268&partnerID=40&md5=f90b0a608ec94d6fb687c0271123252b},
affiliation={University of Dayton, 300 College Park, Dayton, OH, United States},
abstract={One of the most difficult challenges of working with LiDAR data is the large amount of data points that are produced. Analysing these large data sets is an extremely time consuming process. For this reason, automatic perception of LiDAR scenes is a growing area of research. Currently, most LiDAR feature extraction relies on geometrical features specific to the point cloud of interest. These geometrical features are scene-specific, and often rely on the scale and orientation of the object for classification. This paper proposes a robust method for reduced dimensionality feature extraction of 3D objects using a volume component analysis (VCA) approach.1 This VCA approach is based on principal component analysis (PCA). PCA is a method of reduced feature extraction that computes a covariance matrix from the original input vector. The eigenvectors corresponding to the largest eigenvalues of the covariance matrix are used to describe an image. Block-based PCA is an adapted method for feature extraction in facial images because PCA, when performed in local areas of the image, can extract more significant features than can be extracted when the entire image is considered. The image space is split into several of these blocks, and PCA is computed individually for each block. This VCA proposes that a LiDAR point cloud can be represented as a series of voxels whose values correspond to the point density within that relative location. From this voxelized space, block-based PCA is used to analyze sections of the space where the sections, when combined, will represent features of the entire 3-D object. These features are then used as the input to a support vector machine which is trained to identify four classes of objects, vegetation, vehicles, buildings and barriers with an overall accuracy of 93.8%. © 2015 SPIE.},
author_keywords={automatic perception;  LiDAR;  scene understanding;  VCA;  volume component analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cai201540,
author={Cai, J. and Moen, C.D.},
title={Automated buckling mode identification of thin-walled structures from 3D finite element mode shapes or point clouds},
journal={Structural Stability Research Council Annual Stability Conference 2015, SSRC 2015},
year={2015},
pages={40-57},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942412894&partnerID=40&md5=2ac8f762df454deb4113e4bd28ca94cd},
affiliation={Civil and Environmental Engineering, Virginia Tech, United States},
abstract={A Generalized Beam Theory (GBT) approach is derived that performs automated, quantitative modal decomposition of thin-walled members with an open cross-section. The technique extracts modal amplitudes and modal participation factors from any 3D displacement field, for example from finite element analysis or point clouds measured in the lab during a test to collapse. Thin-walled members exhibit deformation that can be represented as combinations of cross-sectional and global buckling modes. It is useful to quantitatively decompose these modes for strength prediction and design code development. Conventionally, buckling mode participation has been determined by visual inspection. This process is subjective and tedious since the person conducting the inspection is often dealing with many models or experiments. Taking advantage of GBT kinematics, the proposed method distinguishes itself by using only the GBT cross-section deformation modes instead of member-wise basis functions. The method is by nature applicable to different boundary and loading conditions without recalculation of basis functions. The mechanics are formulated to show that the method is supported by GBT kinematic assumptions, which ensures its general applicability. The approach is implemented in a Graphical User Interface (GUI) that accepts a thin-walled member 3D displacement field as input and then calculates modal participation factors, i.e., for member local, distortional, and global (Euler) buckling. Copyright © 2015 by the Structural Stability Research Council.},
document_type={Conference Paper},
source={Scopus},
}
