@ARTICLE{7412724,
author={M. D. {Samad} and K. M. {Iftekharuddin}},
journal={IEEE Transactions on Human-Machine Systems},
title={Frenet Frame-Based Generalized Space Curve Representation for Pose-Invariant Classification and Recognition of 3-D Face},
year={2016},
volume={46},
number={4},
pages={522-533},
abstract={The state-of-the-art methods in classifying 3-D representation of the face involve challenges in extracting representative features directly from the large volume of facial data. These methods mostly ignore the effect of pose distortions on 3-D facial data and entail heavy computations as well as manual processing steps. This work proposes a novel Frenet frame-based generalized space curve representation method for 3-D pose-invariant face and facial expression recognition and classification. Three-dimensional facial curves are extracted from either frontal or synthetically posed 3-D facial data to derive the proposed Frenet frame-based features. A mathematical framework shows the proof of pose invariance property for the features. The effectiveness of the proposed method is evaluated in two recognition tasks: 3-D face recognition (3D-FR) and 3-D facial expression recognition (3D-FER) using benchmarked 3-D datasets. The proposed framework yields 96% rank-I recognition rate for 3D-FR and 91.4% area under ROC curves for six basic 3D-FER. The performance evaluation also shows that the proposed mathematical framework yields pose-invariant 3D-FR and 3D-FER for a wide range of pose angles. This pose invariance property of the Frenet frame-based features alleviates the need for an expensive 3-D face registration in the preprocessing step, which, in turn, enables a faster processing time. The evaluation results further suggest that the proposed method is not only computationally efficient and versatile, but also offers competitive performance when compared with the existing state-of-the-art methods reported for either 3D-FR or 3D-FER.},
keywords={computational geometry;emotion recognition;face recognition;feature extraction;image classification;image representation;pose estimation;Frenet frame-based generalized space curve representation;3D face representation;feature extraction;3D pose-invariant facial expression recognition;3D pose-invariant facial expression classification;three-dimensional facial curve extraction;frontal-posed 3D facial data;synthetically-posed 3D facial data;mathematical framework;3D-FR;benchmarked 3D datasets;rank-I recognition rate;area-under ROC curves;performance evaluation;pose angles;pose invariance property;3D face registration;3D pose-invariant face recognition;3D pose-invariant face classification;Face;Feature extraction;Face recognition;Nose;Data mining;Distortion;Manuals;Facial curves;facial expression;Frenet frame;pose-invariant recognition;3-D face classification},
doi={10.1109/THMS.2016.2515602},
ISSN={2168-2291},
month={Aug},}
@INPROCEEDINGS{7130344,
author={K. {Yurtkan} and H. {Soyel} and H. {Demirel}},
booktitle={2015 23nd Signal Processing and Communications Applications Conference (SIU)},
title={Entropy driven feature selection for facial expression recognition based on 3-D facial feature distances},
year={2015},
volume={},
number={},
pages={2322-2325},
abstract={Facial expressions contain a lot of information about the feelings of a human. It plays an important role in humancomputer interaction. In this paper, entropy based feature selection method applied to 3D facial feature distances is presented for a facial expression recognition system classifying the expressions into 6 basic classes based on 3-Dimensional (3D) face geometry. Our previous work on entropy based feature selection has been improved by employing 3D feature distances between the 83 points on the face as facial features. 3D distances are more robust to rotations of the face and involve more accurate information than 3D feature positions that are used in our previous work. Entropy is applied in order to rank the feature distances for feature selection. The system is tested on BU-3DFE database in person independent manner and provides encouraging recognition rates.},
keywords={computational geometry;emotion recognition;face recognition;feature extraction;feature selection;human computer interaction;entropy driven feature selection method;facial expression recognition system;3D facial feature distances;human-computer interaction;3D face geometry;expression classification;BU-3DFE database;Transform coding;Face recognition;Three-dimensional displays;Face;Entropy;Facial features;Computers;Facial expression analysis;facial expression recognition;feature selection;face biometrics;entropy},
doi={10.1109/SIU.2015.7130344},
ISSN={2165-0608},
month={May},}
@INPROCEEDINGS{7087053,
author={A. A. {Pawar} and N. N. {Patil}},
booktitle={2015 International Conference on Pervasive Computing (ICPC)},
title={Recognition of 3-D faces with missing parts and line scratch removal using new technique},
year={2015},
volume={},
number={},
pages={1-5},
abstract={This paper presents an implementation of face recognition, which is a very important task of human face identification. Line scratch detection in images is a highly challenging situation because of various characteristics of this defect. Few characteristics are considered with the different texture and geometry of images. We propose a useful algorithm for frame-by-frame line scratch detection in face image which deals with 3D approach and a filtering of detection. The temporary filtering algorithm can be used to remove false detection due to thin vertical structures by detecting the scratches on an image. Experimental evaluation can be detecting the lines and scratches on a face image and they used to solve this difficult approach. Our method is used with missing parts in an image. Three-dimensional face recognition is an extended method of facial recognition is considered according with the geometry and texture of a face. It has been elaborated that 3D face recognition methods can provide high accuracy as well as high detection with a comparison of 2D recognition. 3D avoids such mismatch effect of 2D face recognition algorithms. Additionally, most 3D scanners achieve both a 3D mesh and the texture of a face image. This allows combining the output of pure 3D matches with the more traditional algorithms of 2D face recognition, thus producing better performance.},
keywords={computational geometry;face recognition;filtering theory;image matching;image texture;mesh generation;object detection;3D face recognition;missing parts;line scratch removal;human face identification;frame-by-frame line scratch detection;image texture;image geometry;detection filtering;false detection removal;3D scanners;3D mesh;pure 3D matches;Face recognition;Three-dimensional displays;Transforms;Filtering;Image recognition;Noise;Films;3D Images;Adaptive detection;Face mask;Hough transforms;ICP algorithm;Line scratches;Missing parts;RANSAC;SIFT},
doi={10.1109/PERVASIVE.2015.7087053},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7581407,
author={P. C. {Yadav} and H. V. {Singh} and A. K. {Patel} and A. {Singh}},
booktitle={2016 International Conference on Emerging Trends in Electrical Electronics Sustainable Energy Systems (ICETEESES)},
title={A comparative analysis of different facial action tracking models and techniques},
year={2016},
volume={},
number={},
pages={347-349},
abstract={The tracking of facial activities from video is an important and challenging problem. Now a day, many computer vision techniques have been proposed to characterize the facial activities in the three levels (from local to global). First level is the bottom level, in which the facial feature tracking focuses on detecting and tracking of the prominent local landmarks surrounding facial components (e.g. mouth, eyebrow, etc), in second level the facial action units (AUs) characterize the specific behaviors of these local facial components (e.g. mouth open, eyebrow raiser, etc) and the third level is facial expression level, which represents subjects emotions (e.g. Surprise, Happy, Anger, etc.) and controls the global muscular movement of the whole face. Most of the existing methods focus on one or two levels of facial activities, and track (or recognize) them separately. In this paper, various facial action tracking models and techniques are compared in different conditions such as the performance of Active Facial Tracking for Fatigue Detection, Real Time 3D Face Pose Tracking from an Uncalibrated Camera, Simultaneous facial action tracking and expression recognition using a particle filter and Simultaneous Tracking and Facial Expression Recognition using Multiperson and Multiclass Autoregressive Models.},
keywords={face recognition;particle filtering (numerical methods);pose estimation;tracking;comparative analysis;facial action tracking models;facial action tracking techniques;facial activities tracking;facial action units;active facial tracking;fatigue detection;real time 3d face pose tracking;uncalibrated camera;facial action tracking;particle filter;facial expression recognition;multiclass autoregressive models;Face;Solid modeling;Three-dimensional displays;Face recognition;Computational modeling;Tracking;Facial features;Simultaneous Tracking and Recognition;Face and Facial Features;Facial Expression;Stochastic Tracking and Fatigue Detection},
doi={10.1109/ICETEESES.2016.7581407},
ISSN={},
month={March},}
@INPROCEEDINGS{7532912,
author={D. {Kim} and J. {Choi} and J. T. {Leksut} and G. {Medioni}},
booktitle={2016 IEEE International Conference on Image Processing (ICIP)},
title={Accurate 3D face modeling and recognition from RGB-D stream in the presence of large pose changes},
year={2016},
volume={},
number={},
pages={3011-3015},
abstract={We propose a 3D face modeling and recognition system using an RGB-D stream in the presence of large pose changes. In the previous work, all facial data points are registered with a reference to improve the accuracy of 3D face model from a low-resolution depth sequence. This registration often fails when applied to non-frontal faces. It causes inaccurate 3D face models and poor performance of matching. We address this problem by pre-aligning each input face (`frontalization') before the registration, which avoids registration failures. For each frame, our method estimates the 3D face pose, assesses the quality of data, segments the facial region, frontalizes it, and performs an accurate registration with the previous 3D model. The 3D-3D recognition system using accurate 3D models from our method outperforms other face recognition systems and shows 100% rank 1 recognition accuracy on a dataset with 30 subjects.},
keywords={face recognition;image colour analysis;image matching;image registration;image resolution;image sequences;pose estimation;3D face modeling;3D face recognition;RGB-D stream;face pose changes;facial data points registration;low-resolution depth sequence;nonfrontal faces;image matching;frontalization;3D face pose estimation;3D-3D recognition system;Decision support systems;Erbium;Indexes;Voltage control;3D Face Recognition;3D Face Modeling},
doi={10.1109/ICIP.2016.7532912},
ISSN={2381-8549},
month={Sep.},}
@ARTICLE{8376028,
author={S. {Kang} and J. {Lee} and K. {Bong} and C. {Kim} and Y. {Kim} and H. {Yoo}},
journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
title={Low-Power Scalable 3-D Face Frontalization Processor for CNN-Based Face Recognition in Mobile Devices},
year={2018},
volume={8},
number={4},
pages={873-883},
abstract={A low-power scalable 3-D face frontalization processor is proposed for accurate face recognition in mobile devices. In spite of recent improvement in face recognition accuracy mainly from convolutional neural networks (CNNs), their performance is limited to face images with frontal view. For face recognition with human-level accuracy in real-life environment, in which most of the face images are captured from arbitrary angles, 3-D face frontalization is essential as a preprocessing stage for CNN-based face recognition algorithms. The proposed face frontalization processor shows scalability in two aspects: image resolution and accuracy. For low-power consumption and scalability, the processor proposes three features: 1) scalable processing element (PE) architecture with workload adaptation; 2) accuracy scalable regression weight quantization to reduce the external memory access (EMA) down to 81.3%; and 3) pipelined memory-level zero-skipping to further reduce the EMA by 98.4% without any latency overhead. From the proposed EMA reduction features, the EMA is reduced by 99.7% with little accuracy degradation in face frontalization results. The proposed face frontalization processor is implemented in 65-nm CMOS process, and it shows 4.73 frames/s) throughput. Moreover, power consumption of the implemented face frontalization processor is 0.53 mW, which is suitable for applications on mobile devices.},
keywords={CMOS integrated circuits;face recognition;image resolution;low-power electronics;neural nets;CMOS process;EMA reduction features;external memory access;scalable processing element architecture;face frontalization processor;3-D face frontalization processor;face recognition accuracy;human-level accuracy;face images;CNN-based face recognition algorithms;image resolution;low-power consumption;mobile devices;Face recognition;Facial features;Feature extraction;Mobile handsets;Computer architecture;Image resolution;Low power electronics;Accuracy scalability;convolutional neural network;face recognition;low-power processor;3-D face frontalization},
doi={10.1109/JETCAS.2018.2845663},
ISSN={2156-3357},
month={Dec},}
@INPROCEEDINGS{7449936,
author={S. {Ganguly} and D. {Bhattachaijee} and M. {Nasipuri}},
booktitle={2015 IEEE International Conference on Computer Graphics, Vision and Information Security (CGVIS)},
title={3D face recognition from complement component range face images},
year={2015},
volume={},
number={},
pages={275-278},
abstract={Face and facial attributes represent meaningful definition about a variety of information to discriminate an individual from others and for developing a computational model for automatic face recognition purpose. However, in this work, selection of relevant features from newly created face space is the pivotal contribution of the authors. Here, authors have demonstrated a new face space `Complement Component' that have been used to extract the four basic components along X, and Y axes in four directions. Later, authors have experimented the discriminative attributes from these face spaces for recognition purpose. Here, comparison of the proposed method has been reported by examining its success on two well accepted 3D face databases, namely: Frav3D and Texas3D. In case of 2D face images, it does not contain depth like information i.e. Z-values in X-Y plane through intensity values. Therefore, it has not been undertaken during this investigation.},
keywords={face recognition;feature extraction;visual databases;automatic 3D face recognition;complement component range face images;facial attributes;computational model;feature selection;face space;X-axis;Y-axis;3D face databases;Frav3D database;Texas3D database;2D face images;Z-values;X-Y plane;intensity values;Face;Face recognition;Feature extraction;Three-dimensional displays;Databases;Image recognition;Data mining;Face recognition;3D face image;range face image;Complement Component},
doi={10.1109/CGVIS.2015.7449936},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7532913,
author={X. {Yu} and Y. {Gao} and J. {Zhou}},
booktitle={2016 IEEE International Conference on Image Processing (ICIP)},
title={3D face recognition under partial occlusions using radial strings},
year={2016},
volume={},
number={},
pages={3016-3020},
abstract={3D face recognition with partial occlusions is a highly challenging problem. In this paper, we propose a novel radial string representation and matching approach to recognize 3D facial scans in the presence of partial occlusions. Here we encode 3D facial surfaces into an indexed collection of radial strings emanating from the nosetips and Dynamic Programming (DP) is then used to measure the similarity between two radial strings. In order to address the recognition problems with partial occlusions, a partial matching mechanism is established in our approach that effectively eliminates those occluded parts and finds the most discriminative parts during the matching process. Experimental results on the Bosphorus database demonstrate that the proposed approach yields superior performance on partially occluded data.},
keywords={dynamic programming;face recognition;image matching;image representation;stereo image processing;3D face recognition;partial occlusions;radial string representation;radial string matching approach;3D facial scans;3D facial surfaces;dynamic programming;partial matching mechanism;Bosphorus database;Decision support systems;Pattern analysis;Biometrics (access control);Handheld computers;Indexes;Pattern recognition;Computer graphics;3D face recognition;radial string matching;structural recognition;partial occlusions},
doi={10.1109/ICIP.2016.7532913},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{7960597,
author={Ö. F. {Söylemez} and B. {Ergen} and N. H. {Söylemez}},
booktitle={2017 25th Signal Processing and Communications Applications Conference (SIU)},
title={A 3D facial expression recognition system based on SVM classifier using distance based features},
year={2017},
volume={},
number={},
pages={1-3},
abstract={In this study, an SVM-based system is proposed for the classification of facial expressions that are represented in 3D. Distance based features are used as a feature vector, which are determined by the distances between the different key points on the image. Study was conducted on a subset (Happy, sadness, surprise) of Bosphorus 3D Face Database. 9 different fiducial points are used to calculate a total of 5 distance features. SVM classification was performed with K-fold cross validation thus mean classification performance of different training and test clusters were determined. %85 success rate has achieved as a result of the expression analysis performed on the 3D facial scans.},
keywords={face recognition;feature extraction;image classification;pattern clustering;support vector machines;3D facial expression recognition system;SVM classifier;distance-based features;feature vector;Bosphorus 3D face database;K-fold cross validation;classification performance;test clusters;training clusters;3D facial scans;support vector machines;Three-dimensional displays;Support vector machines;Face recognition;Databases;Face;Facial features;Computational modeling;Facial expression recognition;Distance based Features;Support vector machines},
doi={10.1109/SIU.2017.7960597},
ISSN={},
month={May},}
@INPROCEEDINGS{8373868,
author={A. {Jan} and H. {Ding} and H. {Meng} and L. {Chen} and H. {Li}},
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
title={Accurate Facial Parts Localization and Deep Learning for 3D Facial Expression Recognition},
year={2018},
volume={},
number={},
pages={466-472},
abstract={Meaningful facial parts can convey key cues for both facial action unit detection and expression prediction. Textured 3D face scan can provide both detailed 3D geometric shape and 2D texture appearance cues of the face which are beneficial for Facial Expression Recognition (FER). However, accurate facial parts extraction as well as their fusion are challenging tasks. In this paper, a novel system for 3D FER is designed based on accurate facial parts extraction and deep feature fusion of facial parts. Experiments are conducted on the BU-3DFE database, demonstrating the effectiveness of combing different facial parts, texture and depth cues and reporting the state-of-the-art results in comparison with all existing methods under the same setting.},
keywords={emotion recognition;face recognition;feature extraction;image fusion;image texture;facial action unit detection;2D texture appearance cues;deep feature fusion;BU-3DFE database;depth cues;3D facial expression recognition;facial parts localization;facial parts extraction;deep learning;facial expression prediction;textured 3D face scan;3D geometric shape;Face;Three-dimensional displays;Two dimensional displays;Feature extraction;Face recognition;Shape;Task analysis;Affective Computing;Facial expression recognition;Human-Computer Interaction},
doi={10.1109/FG.2018.00075},
ISSN={},
month={May},}
@INPROCEEDINGS{7910452,
author={S. G. {Gunanto} and M. {Hariadi} and E. M. {Yuniarno}},
booktitle={2016 2nd International Conference of Industrial, Mechanical, Electrical, and Chemical Engineering (ICIMECE)},
title={Computer facial animation with synthesize marker on 3D faces surface},
year={2016},
volume={},
number={},
pages={260-263},
abstract={An animated character has its own characteristics and behaviour. The animator needs to be skilled enough to make a complex animation, especially on making face expression. a Well defined facial expression can represent the emotional condition and make the animation more expressing the mood. But most facial animation is done by manually, frame-by-frame. It is very time-consuming. This research proposed a combination methods for handling the facial animation based on marker location and face surface from the 3D character. The motion data captured based on the location and movement of the marker then implemented on 3D face model to generate the motion guidance. As a guidance, this marker data role as a centroid of a vertex cluster. The cluster provided by implementing segmentation fp-NN Clustering method based on surface and can visualize the deformation using linear blend skinning methods. The result from this research shows that this system can automatically generate facial animations based on the marker data and the surface segmentation. The visualization of deformation arranges accordingly to the motion captured data and organized sequentially.},
keywords={computer animation;data visualisation;face recognition;image segmentation;neural nets;pattern clustering;computer facial animation;synthesize marker;3D face surface;facial expression;3D character;motion data capture;motion guidance;vertex cluster;segmentation fp-NN Clustering method;linear blend skinning methods;Facial animation;Three-dimensional displays;Solid modeling;Motion segmentation;Surface treatment;Interpolation;facial animation;feature marker;surface},
doi={10.1109/ICIMECE.2016.7910452},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8629150,
author={B. {Kamanditya} and R. P. {Kuswara} and M. A. {Nugroho} and B. {Kusumoputro}},
booktitle={2018 IEEE 5th International Conference on Engineering Technologies and Applied Sciences (ICETAS)},
title={Convolution Neural Network for Pose Estimation of Noisy Three-Dimensional Face Images},
year={2018},
volume={},
number={},
pages={1-5},
abstract={From limited two-dimensional recognition, facial recognition has now been developed to be able to recognize three-dimensional face images, which usually involves process of face pose estimation. As the conventional artificial neural networks has shown low recognition rate to this problem, Convolution Neural Network have been the most potential classifier to determine the pose estimation of a three-dimensional face images. Convolution operation is expected to minimize the effect of distortion and disorientation of the object, and able to efficiently reduce the required parameters. Results show that the CNN system could estimate the pose position of the 3D face images with high recognition rate, however, this recognition rate decline significantly for the noisy buried face images, showing the CNN still need improvement to deal with noisy environments.},
keywords={convolutional neural nets;face recognition;feature extraction;image classification;pose estimation;Convolution Neural Network;pose estimation;noisy three-dimensional face images;two-dimensional recognition;facial recognition;low recognition rate;pose position;3D face images;high recognition rate;recognition rate decline;noisy buried face images;convolution operation;CNN system;Face;Face recognition;Convolution;Feature extraction;Image recognition;Noise measurement;Databases;head pose estimation;convolutional neural network;image noises;hyperparameter evaluation;face recognition},
doi={10.1109/ICETAS.2018.8629150},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8571963,
author={S. {Yamada} and H. {Lu} and J. K. {Tan} and H. {Kim} and N. {Kimura} and T. {Okawachi} and E. {Nozoe} and N. {Nakamura}},
booktitle={2018 18th International Conference on Control, Automation and Systems (ICCAS)},
title={Extraction of Median Plane from Facial 3D Point Cloud Based on Symmetry Analysis Using ICP Algorithm},
year={2018},
volume={},
number={},
pages={1347-1350},
abstract={Cleft lip is a kind of congenital facial morphological abnormality. In the clinical field of cleft lip, it is necessary to analyze symmetric shape. However, there is no method to analyze the cleft lip technique based on symmetrical viewpoints. On the other hand, in our previous method to find a symmetric axis using a 2D image, since the middle line is extracted only from the front view of the face moire image. There was a problem that low accuracy was obtained by slight rotation of the face and it was not possible to consider 3D information. In this paper, we propose a method to extract the median plane of the face by analyzing based on bilateral symmetry by using 3D point cloud on the face of front. By extracting the median plane, we believe that not only surgical assistance of doctor be possible but also become a clue to development of simulation software which is the end goal.},
keywords={biomechanics;face recognition;feature extraction;image reconstruction;medical image processing;surgery;symmetric shape;clinical field;congenital facial morphological abnormality;ICP algorithm;symmetry analysis;facial 3D point cloud;bilateral symmetry;median plane;problem that low accuracy;face moire image;middle line;symmetric axis;symmetrical viewpoints;cleft lip technique;Three-dimensional displays;Lips;Face;Surgery;Iterative closest point algorithm;Two dimensional displays;Nose;Cleft lip;ICP algorithm;3D point cloud;Point Cloud Library;Facial median plane;Symmetry analysis.},
doi={},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7368276,
author={H. {Boukamcha} and M. {Elhallek} and M. {Atri} and F. {Smach}},
booktitle={2015 World Symposium on Computer Networks and Information Security (WSCNIS)},
title={3D face landmark auto detection},
year={2015},
volume={},
number={},
pages={1-6},
abstract={This paper presents our methodology for Landmark Point detection to improve 3D face recognition in a presence of variant facial expression. The objective was to develop an automatic process for distinguishing and segmenting to be embedded in a 3D face recognition system using only 3D Point Distribution Model (PDM) as input. The approach used hydride method to extract this features from the surface curvature information. Landmark Localization is done on the segmented face via finding the change that decreases the deviation of the model from the mean profile. Face registering is achieved using previous anthropometric information and the localized landmarks. The results confirm that the method used is accurate and robust for the proposed application.},
keywords={computer graphics;face recognition;3D face landmark auto detection;landmark point detection;variant facial expression;automatic process;3D face recognition system;3D point distribution model;PDM;hydride method;surface curvature information;landmark localization;face registering;anthropometric information;localized landmarks;Nose;Face;Three-dimensional displays;Databases;Face recognition;Facial features;3D Face;Labelling;Graph Matching;Registration},
doi={10.1109/WSCNIS.2015.7368276},
ISSN={},
month={Sep.},}
@ARTICLE{7944639,
author={H. {Li} and J. {Sun} and Z. {Xu} and L. {Chen}},
journal={IEEE Transactions on Multimedia},
title={Multimodal 2D+3D Facial Expression Recognition With Deep Fusion Convolutional Neural Network},
year={2017},
volume={19},
number={12},
pages={2816-2831},
abstract={This paper presents a novel and efficient deep fusion convolutional neural network (DF-CNN) for multimodal 2D+3D facial expression recognition (FER). DF-CNN comprises a feature extraction subnet, a feature fusion subnet, and a softmax layer. In particular, each textured three-dimensional (3D) face scan is represented as six types of 2D facial attribute maps (i.e., geometry map, three normal maps, curvature map, and texture map), all of which are jointly fed into DF-CNN for feature learning and fusion learning, resulting in a highly concentrated facial representation (32-dimensional). Expression prediction is performed by two ways: 1) learning linear support vector machine classifiers using the 32-dimensional fused deep features, or 2) directly performing softmax prediction using the six-dimensional expression probability vectors. Different from existing 3D FER methods, DF-CNN combines feature learning and fusion learning into a single end-to-end training framework. To demonstrate the effectiveness of DF-CNN, we conducted comprehensive experiments to compare the performance of DFCNN with handcrafted features, pre-trained deep features, finetuned deep features, and state-of-the-art methods on three 3D face datasets (i.e., BU-3DFE Subset I, BU-3DFE Subset II, and Bosphorus Subset). In all cases, DF-CNN consistently achieved the best results. To the best of our knowledge, this is the first work of introducing deep CNN to 3D FER and deep learning-based featurelevel fusion for multimodal 2D+3D FER.},
keywords={emotion recognition;face recognition;feature extraction;image fusion;image representation;image texture;learning (artificial intelligence);neural nets;probability;multimodal 2D+3D facial expression recognition;deep fusion convolutional neural network;facial representation;DF-CNN;featurelevel fusion;deep learning;six-dimensional expression probability vectors;fusion learning;feature learning;2D facial attribute maps;three-dimensional face scan;feature fusion subnet;feature extraction subnet;FER;Three-dimensional displays;Convolutional neural networks;Two dimensional displays;Face recognition;Feature extraction;Shape;Deep fusion convolutional neural network (DF-CNN);facial expression recognition (FER);multimodal;textured three-dimensional (3D) face scan},
doi={10.1109/TMM.2017.2713408},
ISSN={1520-9210},
month={Dec},}
@ARTICLE{7457243,
author={Q. {Zhen} and D. {Huang} and Y. {Wang} and L. {Chen}},
journal={IEEE Transactions on Multimedia},
title={Muscular Movement Model-Based Automatic 3D/4D Facial Expression Recognition},
year={2016},
volume={18},
number={7},
pages={1438-1450},
abstract={Facial expression is an important channel for human nonverbal communication. This paper presents a novel and effective approach to automatic 3D/4D facial expression recognition based on the muscular movement model (MMM). In contrast to most of existing methods, the MMM deals with such an issue in the viewpoint of anatomy. It first automatically segments the input 3D face (frame) by localizing the corresponding points within each muscular region of the reference using iterative closest normal point. A set of features with multiple differential quantities, including coordinate, normal, values, are then extracted to describe the geometry deformation of each segmented region. Meanwhile, we analyze the importance of these muscular areas, and a score level fusion strategy is exploited to optimize their weights by the genetic algorithm in the learning step. The support vector machine and the hidden Markov model are finally used to predict the expression label in 3D and 4D, respectively. The experiments are conducted on the BU-3DFE and BU-4DFE databases, and the results achieved clearly demonstrate the effectiveness of the proposed method.},
keywords={emotion recognition;face recognition;feature extraction;genetic algorithms;hidden Markov models;image fusion;image segmentation;iterative methods;learning (artificial intelligence);support vector machines;muscular movement model;automatic 3D/4D facial expression recognition;human nonverbal communication;MMM;face segmentation;iterative closest normal point;feature extraction;score level fusion strategy;genetic algorithm;support vector machine;learning step;hidden Markov model;BU-3DFE database;BU-4DFE database;Three-dimensional displays;Face;Hidden Markov models;Face recognition;Feature extraction;Shape;Support vector machines;3D/4D facial expression recognition;Muscle Movement Model;shape representation;3D/4D facial expression recognition;muscle movement model (MMM);shape representation},
doi={10.1109/TMM.2016.2557063},
ISSN={1520-9210},
month={July},}
@INPROCEEDINGS{7457820,
author={J. {Manceau} and C. {Soladié} and R. {Séguier}},
booktitle={2015 Visual Communications and Image Processing (VCIP)},
title={3D facial clone based on depth patches},
year={2015},
volume={},
number={},
pages={1-4},
abstract={3D face clones can be used in many areas such as Human-Computer Interaction and as preprocessing in applications, such as emotion analysis. However, such clones should be structured and the model facial shape accurately while keeping the attributes of individuals. A structured mesh is a mesh with a known semantic and topological structure. We use a face model designed from a database of 3D face examples. These global models can produce structured clones but they do not often retain the specifics of the analyzed person. Indeed, methods using models are very dependent on their databases. In our technique, we use an RGB-D sensor to get the attributes of individuals and a 3D Morphable Face Model to mark facial shape. We reverse the process classically used: we first perform fitting and then data fusion. For each depth frame, we retain the suitable data parts called Patches. This selection is performed using a distance error and the direction of the normal vectors. Depending on the location, we merge either sensor data or 3D Morphable Face Model data. We compare our method with state of the art fitting processes. The qualitative and quantitative tests show that our results are more accurate than an current fitting method and our clone has both the attributes of the person and the shape of the face well modeled.},
keywords={face recognition;human computer interaction;image resolution;image sensors;sensor fusion;vectors;3D facial clone;depth patches;human-computer interaction;model facial shape;structured mesh;known semantic structure;topological structure;3D face examples;RGB-D sensor;3D morphable face model;data fusion;distance error;normal vectors;Face;Cloning;Three-dimensional displays;Cameras;Solid modeling;Databases;Mouth;Structured mesh;Facial clone;Depth patches;Patches detection;Patches fusion},
doi={10.1109/VCIP.2015.7457820},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7516157,
author={S. {Zribi} and T. {Khadhraoui} and F. {Benzarti} and H. {Amiri}},
booktitle={2016 IEEE 14th International Conference on Software Engineering Research, Management and Applications (SERA)},
title={Automatic 3D face preprocessing},
year={2016},
volume={},
number={},
pages={280-284},
abstract={An efficient and above all cheap solutions, biometrics provide extensive information in access control applications. 3D mode provides great new opportunities in this sector in recent years. Firstly the paper discusses the formal work done in this domain discussing approach based on curvature calculation, alignment of surfaces, feature selection and facial curve including different techniques for 3D facial recognition data. The second part is about the steps required for the preprocessing phase of the 3D face data. Various experiments conducted and results obtained.},
keywords={face recognition;solid modelling;3D face preprocessing;biometrics;access control applications;domain discussing approach;curvature calculation;surface alignment;feature selection;facial curve;3D facial recognition data;Face;Three-dimensional displays;Face recognition;Feature extraction;Surface treatment;Surface morphology;Data mining;Preprocessing;3D face;Delaunay triangulation;Median Filter;Facial mask},
doi={10.1109/SERA.2016.7516157},
ISSN={},
month={June},}
@INPROCEEDINGS{7890167,
author={J. {Yun} and J. {Lee} and D. {Han} and J. {Ju} and J. {Kim}},
booktitle={2017 19th International Conference on Advanced Communication Technology (ICACT)},
title={Cost-efficient 3D face reconstruction from a single 2D image},
year={2017},
volume={},
number={},
pages={629-632},
abstract={We propose a three-dimensional (3D) face-modelling method from a single two-dimensional (2D) face image using a gallery of 2D face images and their corresponding 3D face models. Unlike existing methods, which require human effort, we provide a simple way to reconstruct 3D face models without user interaction. Our main approach is based on the idea that a particular coefficient that linearly combines vectors of 2D face images and outputs a vector that approximates the input image vector in terms of the vector norm can be reused in 3D models. Therefore, the pair of a 2D image and its 3D model plays an important role in our algorithm. Using the FaceGen software allows us to avoid the employed in previous works procedure whereby the 3D model is generated in a labor-intensive, expensive way. As a result, we are able to easily establish our 2D and 3D database. In this paper, we present a method for adopting the coefficients in 3D models and demonstrate the results of our algorithm.},
keywords={face recognition;image reconstruction;vectors;cost-efficient 3D face reconstruction;single 2D image;three-dimensional face-modelling method;single two-dimensional face image;2D face images;image reconstruction;3D face models;particular coefficient;image vector;FaceGen software;Face;Three-dimensional displays;Two dimensional displays;Solid modeling;Image reconstruction;Shape;Manuals;Facial modelling;3D face reconstruction;2d face fitting;face landmark matching;linear combination},
doi={10.23919/ICACT.2017.7890167},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8054955,
author={M. {Jazouli} and A. {Majda} and A. {Zarghili}},
booktitle={2017 Intelligent Systems and Computer Vision (ISCV)},
title={A $P recognizer for automatic facial emotion recognition using Kinect sensor},
year={2017},
volume={},
number={},
pages={1-5},
abstract={Autism is a developmental disorder involving qualitative impairments in social interaction. One source of those impairments are difficulties with facial expressions of emotion. Autistic people often have difficulty to recognize or to understand other people's emotions and feelings, or expressing their own. This work proposes a method to automatically recognize seven basic emotions among autistic children in real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and Neutral. The method uses the Microsoft Kinect sensor to track and identify points of interest from the 3D face model and it is based on the $P point-cloud recognizer to identify multi-stroke emotions as point-clouds. The experimental results show that our system can achieve above 94.28% recognition rate. Our study provides a novel clinical tool to help children with autism to assisting doctors in operating rooms.},
keywords={emotion recognition;face recognition;feature extraction;multistroke emotions;point-clouds;recognition rate;autism;$P recognizer;automatic facial emotion recognition;developmental disorder;qualitative impairments;social interaction;autistic people;autistic children;Microsoft Kinect sensor;3D face model;$P point-cloud recognizer;Face recognition;Emotion recognition;Face;Three-dimensional displays;Autism;Support vector machines;Algorithm design and analysis;ASD;Autism;emotion;face expression;Kinect;$P Recognizer},
doi={10.1109/ISACV.2017.8054955},
ISSN={},
month={April},}
@INPROCEEDINGS{7976644,
author={S. {Sghaier} and C. {Souani} and H. {Faeidh} and K. {Besbes}},
booktitle={2016 Global Summit on Computer Information Technology (GSCIT)},
title={Novel Technique for 3D Face Segmentation and Landmarking},
year={2016},
volume={},
number={},
pages={27-31},
abstract={In this paper we propose a new technique to detect and extract any part that we need from a 3D face. The regions of interest of our approach are the eyes and the nose. Moreover, we are interested in the extraction of the salient landmarks in 3D face. Therefore, in this paper a new method is proposed to detect the nose tip and eye corners from a three-dimensional face range image. Our original technique allows fully automated processing, treating incomplete and noisy input data, and automatically rejecting non-facial areas. Besides, it is robust against holes in 3D image and insensitive to facial expressions. Besides, it is stable against translation and rotation of the face, and it is suitable for different resolutions of images. All the experiments have been performed on the GAVAB and FRAV 3D databases. After applying the proposed method to the 3D face, experimental results show that it is comparable to state-of-the-art methods in terms of its accuracy and flexibility.},
keywords={face recognition;image segmentation;visual databases;3D face segmentation;landmark extraction;nose tip detection;eye corner detection;three-dimensional face range image;automatic nonfacial area rejection;3D image;facial expressions;image resolutions;GAVAB 3D database;FRAV 3D database;Face;Three-dimensional displays;Nose;Information technology;Noise measurement;Robustness;Image resolution;3D face;segmentation;region of interest;anthropometric;landmarks},
doi={10.1109/GSCIT.2016.17},
ISSN={},
month={July},}
@INPROCEEDINGS{7351408,
author={T. {Batabyal} and A. {Vaccari} and S. T. {Acton}},
booktitle={2015 IEEE International Conference on Image Processing (ICIP)},
title={UGraSP: A unified framework for activity recognition and person identification using graph signal processing},
year={2015},
volume={},
number={},
pages={3270-3274},
abstract={With the growing availability and wide distribution of low-cost, high-performance 3D imaging sensors, the image analysis community has witnessed an increased demand for solutions to the challenges of activity recognition and person identification. We propose an integrated framework, based on graph signal processing, that simultaneously performs both tasks using a single set of features. The novelty of our approach is based on the fact that the set of features used for activity recognition accommodates person identification without additional computation. The analysis is based on the extracted structure-invariant graph (skeleton). The Laplacian of the skeleton is used both to identify the person and recognize the performed activity. While person identification is achieved directly from the analysis of the Laplacian, activity recognition is obtained after transformation, into the graph spectral domain, of the vectorized form of the skeletal joints 3D coordinates. Feature vectors for activity recognition are then derived, in this domain, from the covariance matrices evaluated over fixed-length sequential video segments. Both classification tasks are implemented using linear support vector machines (SVM). When applied to real activity datasets, our approach shows an improved performance over the existing state-of-the-art.},
keywords={covariance matrices;feature extraction;graph theory;image classification;object detection;object recognition;support vector machines;video signal processing;UGraSP;unified framework;image analysis community;activity recognition;person identification;integrated framework;graph signal processing;feature tasks;structure-invariant graph extraction;graph skeleton;skeleton Laplacian;graph spectral domain;vectorized form;skeletal joints 3D coordinates;feature vectors;covariance matrices;fixed-length sequential video segments;classification tasks;linear support vector machines;SVM;real activity datasets;performance improvement;Laplace equations;Skeleton;Three-dimensional displays;Motion segmentation;Image recognition;Sensors;Support vector machines;Laplacian;Adjacency Matrix;Graph Signal Processing;Graph Fourier Transform;activity Recognition;Person Identification;Point cloud datasets},
doi={10.1109/ICIP.2015.7351408},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8272703,
author={H. {Li} and J. {Sun} and L. {Chen}},
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)},
title={Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition},
year={2017},
volume={},
number={},
pages={234-242},
abstract={This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01%, 97.60%, and 96.13% on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition.},
keywords={face recognition;feature extraction;geometry;image classification;image representation;pose estimation;location-sensitive sparse representation;expression-robust 3D face recognition approach;given raw 3D facial surfaces;3D face pre-processing pipeline;face region cropping;normalized 3D facial surface;facial surface normal components;deep face net;facial surface normals;different facial locations;location sensitive sparse representation classifier;different 3D faces;different normal components;BU-3DFE databases;massive 2D face images;Three-dimensional displays;Face;Face recognition;Solid modeling;Two dimensional displays;Shape;Deformable models},
doi={10.1109/BTAS.2017.8272703},
ISSN={2474-9699},
month={Oct},}
@INPROCEEDINGS{8578301,
author={S. {Zulqarnain Gilani} and A. {Mian}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition},
year={2018},
volume={},
number={},
pages={1896-1905},
abstract={Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem.},
keywords={convolutional neural nets;face recognition;learning (artificial intelligence);stereo image processing;3D face datasets;open world face recognition problem;3D facial scans;3D face identities;3D face recognition;facial images;deep learning;large test datasets;deep CNN model;Three-dimensional displays;Face recognition;Face;Two dimensional displays;Training;Solid modeling;Shape},
doi={10.1109/CVPR.2018.00203},
ISSN={2575-7075},
month={June},}
@INPROCEEDINGS{8578839,
author={J. {Deng} and S. {Cheng} and N. {Xue} and Y. {Zhou} and S. {Zafeiriou}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={UV-GAN: Adversarial Facial UV Map Completion for Pose-Invariant Face Recognition},
year={2018},
volume={},
number={},
pages={7093-7102},
abstract={Recently proposed robust 3D face alignment methods establish either dense or sparse correspondence between a 3D face model and a 2D facial image. The use of these methods presents new challenges as well as opportunities for facial texture analysis. In particular, by sampling the image using the fitted model, a facial UV can be created. Unfortunately, due to self-occlusion, such a UV map is always incomplete. In this paper, we propose a framework for training Deep Convolutional Neural Network (DCNN) to complete the facial UV map extracted from in-the-wild images. To this end, we first gather complete UV maps by fitting a 3D Morphable Model (3DMM) to various multiview image and video datasets, as well as leveraging on a new 3D dataset with over 3,000 identities. Second, we devise a meticulously designed architecture that combines local and global adversarial DCNNs to learn an identity-preserving facial UV completion model. We demonstrate that by attaching the completed UV to the fitted mesh and generating instances of arbitrary poses, we can increase pose variations for training deep face recognition/verification models, and minimise pose discrepancy during testing, which lead to better performance. Experiments on both controlled and in-the-wild UV datasets prove the effectiveness of our adversarial UV completion model. We achieve state-of-the-art verification accuracy, 94.05%, under the CFP frontal-profile protocol only by combining pose augmentation during training and pose discrepancy reduction during testing. We will release the first in-the-wild UV dataset (we refer as WildUV) that comprises of complete facial UV maps from 1,892 identities for research purposes.},
keywords={convolution;face recognition;feedforward neural nets;image texture;UV-GAN;adversarial facial UV map completion;pose-invariant face recognition;robust 3D face alignment methods;dense correspondence;sparse correspondence;3D face model;2D facial image;facial texture analysis;3DMM;multiview image;video datasets;local adversarial DCNNs;global adversarial DCNNs;identity-preserving facial UV completion model;fitted mesh;arbitrary poses;in-the-wild UV dataset;deep convolutional neural network;3D morphable model;facial UV maps;deep face recognition/verification models;Face;Three-dimensional displays;Face recognition;Two dimensional displays;Shape;Training;Generators},
doi={10.1109/CVPR.2018.00741},
ISSN={2575-7075},
month={June},}
@INPROCEEDINGS{7424213,
author={B. A. {Echeagaray-Patron} and D. {Miramontes-Jaramillo} and V. {Kober}},
booktitle={2015 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={Conformal Parameterization and Curvature Analysis for 3D Facial Recognition},
year={2015},
volume={},
number={},
pages={843-844},
abstract={This work proposes a new algorithm for 3D face recognition. The algorithm uses 3D shape data without color or texture information and exploits local curvature information which is a measure with high discriminant capability and robust to deformations such as rotation and scaling. In order to reduce high dimensionality of typical face surfaces our approach uses a conformal parameterization, preserving angles of original faces and simplifies the correspondence problem. Experimental results are presented and discussed using CASIA and Gavab databases.},
keywords={face recognition;visual databases;3D facial recognition;3D shape data;local curvature information;discriminant capability;dimensionality reduction;face surfaces;conformal parameterization;original face angle preservation;CASIA databases;Gavab databases;curvature analysis;Three-dimensional displays;Face;Face recognition;Training;Databases;Algorithm design and analysis;Shape;3D face recognition;curvature analysis;conformal parameterization},
doi={10.1109/CSCI.2015.133},
ISSN={},
month={Dec},}
@ARTICLE{8500093,
author={S. {Moschoglou} and E. {Ververas} and Y. {Panagakis} and M. A. {Nicolaou} and S. {Zafeiriou}},
journal={IEEE Journal of Selected Topics in Signal Processing},
title={Multi-Attribute Robust Component Analysis for Facial UV Maps},
year={2018},
volume={12},
number={6},
pages={1324-1337},
abstract={The collection of large-scale three-dimensional (3-D) face models has led to significant progress in the field of 3-D face alignment “in-the-wild,” with several methods being proposed toward establishing sparse or dense 3-D correspondences between a given 2-D facial image and a 3-D face model. Utilizing 3-D face alignment improves 2-D face alignment in many ways, such as alleviating issues with artifacts and warping effects in texture images. However, the utilization of 3-D face models introduces a new set of challenges for researchers. Since facial images are commonly captured in arbitrary recording conditions, a considerable amount of missing information and gross outliers is observed (e.g., due to self-occlusion, subjects wearing eye-glasses, and so on). To this end, in this paper we propose the Multi-Attribute Robust Component Analysis (MA-RCA), a novel technique that is suitable for facial UV maps containing a considerable amount of missing information and outliers, while additionally, elegantly incorporates knowledge from various available attributes, such as age and identity. We evaluate the proposed method on problems such as UV denoising, UV completion, facial expression synthesis, and age progression, where MA-RCA outperforms compared techniques.},
keywords={face recognition;image texture;pose estimation;multiattribute robust component analysis;facial images;2-D face alignment;3-D face model;2-D facial image;3-D face alignment;face models;facial UV maps;Three-dimensional displays;Face;Two dimensional displays;Training;Signal processing algorithms;Principal component analysis;Image denoising;Robust component analysis;low rank;sparsity;facial UV maps},
doi={10.1109/JSTSP.2018.2877108},
ISSN={1932-4553},
month={Dec},}
@INPROCEEDINGS{8451742,
author={S. {Wang} and X. {Shen} and J. {Liu}},
booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
title={Dense Optical Flow Variation Based 3D Face Reconstruction from Monocular Video},
year={2018},
volume={},
number={},
pages={2665-2669},
abstract={This paper presents a method for reconstructing 3D face expressions from monocular video sequences. Unlike previous approaches we don't require any prior face models, nor a large collection of images with diverse variation of poses and illuminations. Instead, we leverage a monocular video sequence without any restrictions. We formulate the 3D face reconstruction as an energy minimization problem integrated with dense optical flow variation, as rigid as possible(ARAP) constraint, spatial and temporal constraints. This paper offers the first dense optical flow variational approach to the problem of 3D reconstruction of non-rigid face expressions from a monocular video. Dense optical flow variation cost substitutes for photo consistency cost to enhance the reconstruction of exaggerated expressions. A generic 3D face template mesh and a simple 3D warping algorithm allow us to reconstruct a true 3D face mesh, relax the constraints of diverse views or illuminations and also avoid the dependency of the quality of prior face models, such as the facial expressions or face races varieties limitation. Finally, we use a per-pixel shape-from-shading(SFS) algorithm to estimate the fine-scale geometry details such as wrinkles to further improve the reconstruction fidelity. Given unconstrained monocular RGB videos, our method reconstructs wrinkle-level 3D face model, without the need for any prior models or diverse capture conditions.},
keywords={emotion recognition;face recognition;image reconstruction;image sequences;3D face reconstruction;3D face expressions;monocular video sequence;prior face models;diverse variation;temporal constraints;dense optical flow variational approach;nonrigid face expressions;generic 3D face template mesh;simple 3D warping algorithm;3D face mesh;facial expressions;reconstruction fidelity;wrinkle-level 3D face model;monocular RGB videos;dense optical flow variation;Three-dimensional displays;Face;Image reconstruction;Optical imaging;Solid modeling;Two dimensional displays;Integrated optics;3D face reconstruction;dense optical flow;monocular video},
doi={10.1109/ICIP.2018.8451742},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{7233159,
author={S. {Bentaieb} and A. {Ouamri} and M. {Keche}},
booktitle={2015 3rd International Conference on Control, Engineering Information Technology (CEIT)},
title={Nose tip localization on a three dimensional face across pose, expressions and occlusions variations in a Riemannian context},
year={2015},
volume={},
number={},
pages={1-5},
abstract={Nose tip localization is an important step for registration, preprocessing and recognition of 3D face data. In this paper, we propose a new approach for the nose tip detection that is robust to pose and expression variations and in presence of occlusions. From a rotated 3D face, we extract facial curves that are matched to a profile curve model. An optimal matching using the Riemannian geometry, based on the Elastic Shape Analysis is performed to obtain the accurate nose tip. The proposed method requires no training and can locate the nose tip in less than 6 seconds. Experiments are performed on the Bosphorus database. Quantitative analysis and comparison with the ground truth locations are provided. The results confirm that our approach achieves 97.68% with error no larger than 12 mm and 98.19% within 20 mm.},
keywords={computational geometry;face recognition;feature extraction;image matching;image registration;pose estimation;nose tip localization;three-dimensional face;occlusion variations;3D face data registration;3D face data recognition;3D face data preprocessing;nose tip detection;pose variations;expression variations;rotated 3D face;facial curve extraction;profile curve model;optimal image matching;Riemannian geometry;elastic shape analysis;Bosphorus database;quantitative analysis;ground truth locations;Nose;Face;Three-dimensional displays;Shape;Face recognition;Databases;Robustness;Nose Tip Localization;Shape Analysis;SRVF},
doi={10.1109/CEIT.2015.7233159},
ISSN={},
month={May},}
@INPROCEEDINGS{7378673,
author={ and and },
booktitle={2015 23rd International Conference on Geoinformatics},
title={A method of extracting human facial feature points based on 3D laser scanning point cloud data},
year={2015},
volume={},
number={},
pages={1-3},
abstract={Three-dimensional body measurement is determined by measuring the size of each part of the body and body shape, in order to study the morphological characteristics of the human body, and provide basic information for human industrial design, ergonomics, engineering design, anthropological research, and medicine. This paper presents a three-dimensional point cloud data extraction method of facial feature points by doing nose points with examples, using this method can accurately locate the feature points, and there is no specific stations toward requirements in the process of scanning point cloud of the human body, it is convenient and quick. The results show that this method can meet the ergonomics and other features to quickly locate and measure the body size requirements.},
keywords={computer graphics;face recognition;feature extraction;optical scanners;human facial feature point extraction;3D laser scanning point cloud data;three-dimensional body measurement;morphological characteristics;human body;human industrial design;ergonomics;engineering design;anthropological research;medicine;three-dimensional point cloud data extraction method;nose point;Atmospheric modeling;Ergonomics;Biomedical imaging;The face feature point;Nose point;Measurement;Three-dimensional point cloud;Three-dimensional laser scanning},
doi={10.1109/GEOINFORMATICS.2015.7378673},
ISSN={2161-024X},
month={June},}
@INPROCEEDINGS{8075548,
author={T. {Frikha} and F. {Chaabane} and B. {Said} and H. {Drira} and M. {Abid} and C. {Ben Amar} and L. {Lille}},
booktitle={2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)},
title={Embedded approach for a Riemannian-based framework of analyzing 3D faces},
year={2017},
volume={},
number={},
pages={1-5},
abstract={Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform.},
keywords={biometrics (access control);embedded systems;face recognition;feature extraction;field programmable gate arrays;hardware-software codesign;software architecture;stereo image processing;tensors;biometric facial recognition system;embedded systems;data completion;full face recognition;hardware architecture;adaptation approach;face detection;Riemannian-based framework;3D face analysis;multimedia embedded applications;face expressions recognition;mixed hardware software architecture;codesign technique;FPGA platform;Computer architecture;Shape;Face recognition;Multimedia communication;Three-dimensional displays;Embedded systems;Measurement;Facial analysis;face detection;Facial expressions;3D face recognition;embedded architecture;elastic analysis algorithm;Riemann geometry;Curve analysis},
doi={10.1109/ATSIP.2017.8075548},
ISSN={},
month={May},}
@ARTICLE{8408720,
author={R. S. {Siqueira} and G. R. {Alexandre} and J. M. {Soares} and G. A. P. {Thé}},
journal={IEEE Robotics and Automation Letters},
title={Triaxial Slicing for 3-D Face Recognition From Adapted Rotational Invariants Spatial Moments and Minimal Keypoints Dependence},
year={2018},
volume={3},
number={4},
pages={3513-3520},
abstract={This letter presents a multiple slicing model for threedimensional (3-D) images of human face, using the frontal, sagittal, and transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2-D spatial moments of Hu and 3-D spatial invariant rotation moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs nonneutral ROC I experiment, applying linear discriminant analysis as classifier and more than one sample for training, achieved 98.7% of verification rate at 0.1% of false acceptance rate. By using the support vector machine as classifier the rank1 experiment recognition rates of 99% and 95.4% have been achieved for a neutral vs neutral and for a neutral vs nonneutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when anger and disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity.},
keywords={face recognition;feature extraction;image classification;support vector machines;verification rate;false acceptance rate;support vector machine;rank1 experiment recognition rates;Bosphorus Database;3-D face recognition;multiple slicing model;human face;orthogonal planes;nose tip;facial recognition;3-D spatial invariant rotation moments;neutral vs nonneutral ROC;linear discriminant analysis;adapted rotational invariant spatial moments;minimal keypoint dependence;Three-dimensional displays;Face;Feature extraction;Nose;Two dimensional displays;Robustness;Iterative closest point algorithm;Computer vision for automation;recognition;surveillance systems},
doi={10.1109/LRA.2018.2854295},
ISSN={2377-3766},
month={Oct},}
@INPROCEEDINGS{8578645,
author={F. {Liu} and R. {Zhu} and D. {Zeng} and Q. {Zhao} and X. {Liu}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition},
year={2018},
volume={},
number={},
pages={5216-5225},
abstract={This paper proposes an encoder-decoder network to disentangle shape features during 3D face reconstruction from single 2D images, such that the tasks of reconstructing accurate 3D face shapes and learning discriminative shape features for face recognition can be accomplished simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. To construct training data we develop a method for fitting 3D morphable model (3DMM) to multiple 2D images of a subject. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy.},
keywords={face recognition;feature extraction;image classification;image reconstruction;image representation;learning (artificial intelligence);solid modelling;stereo image processing;discriminative shape features;3DMM capacity;MICC database;BU3DFE database;LFW database;YTF database;face recognition accuracy;3D face reconstruction accuracy;3D morphable model;3D face shape reconstruction error;face identification error;composite 3D face shape model;dense 3D face shapes;3D face reconstruction methods;accurate 3D face shapes;single 2D images;disentangle shape features;joint face reconstruction;Face;Three-dimensional displays;Shape;Image reconstruction;Face recognition;Two dimensional displays;Decoding},
doi={10.1109/CVPR.2018.00547},
ISSN={2575-7075},
month={June},}
@INPROCEEDINGS{8272691,
author={D. {Kim} and M. {Hernandez} and J. {Choi} and G. {Medioni}},
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)},
title={Deep 3D face identification},
year={2017},
volume={},
number={},
pages={133-142},
abstract={We propose a novel 3D face recognition algorithm using a deep convolutional neural network (DCNN) and a 3D face expression augmentation technique. The performance of 2D face recognition algorithms has significantly increased by leveraging the representational power of deep neural networks and the use of large-scale labeled training data. In this paper, we show that transfer learning from a CNN trained on 2D face images can effectively work for 3D face recognition by fine-tuning the CNN with an extremely small number of 3D facial scans. We also propose a 3D face expression augmentation technique which synthesizes a number of different facial expressions from a single 3D face scan. Our proposed method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets without using hand-crafted features. The 3D face identification using our deep features also scales well for large databases.},
keywords={convolution;face recognition;feedforward neural nets;image representation;learning (artificial intelligence);deep 3D face identification;novel 3D face recognition algorithm;deep convolutional neural network;3D face expression augmentation technique;2D face recognition algorithms;large-scale labeled training data;2D face images;3D facial scans;single 3D face scan;BU-3DFE;3D-TEC datasets;deep features;CNN;DCNN;transfer learning;Three-dimensional displays;Face;Two dimensional displays;Face recognition;Feature extraction;Solid modeling;Shape},
doi={10.1109/BTAS.2017.8272691},
ISSN={2474-9699},
month={Oct},}
@INPROCEEDINGS{8369655,
author={M. {Hada} and R. {Yamada} and S. {Akamatsu}},
booktitle={2018 International Workshop on Advanced Image Technology (IWAIT)},
title={How does the transformation of an avatar face giving a favorable impression affect human recognition of the face?},
year={2018},
volume={},
number={},
pages={1-3},
abstract={We investigated how different appearances in the favorable impressions of 3D avatar faces affect face-recognition performances by humans. We conducted an encoding and testing experiment using synthesized facial images and artificially manipulated the strength of the perceived impressions in three different dimensions. We also subjectively assessed the favorability of the synthesized faces that were used as visual stimuli in face-recognition tests and found that facial transformation, which decreased the favorability impressions, generally deteriorates human face-recognition performance.},
keywords={avatars;face recognition;image recognition;facial transformation;human face-recognition performance;3D avatar faces;face-recognition performances;synthesized facial images;face-recognition tests;visual stimuli;Face;Face recognition;Three-dimensional displays;Avatars;Image coding;Solid modeling;Analysis of variance;Social impression of face;morphable 3D face model;facial impression manipulation;Thurston's paired comparison;face memory},
doi={10.1109/IWAIT.2018.8369655},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8089906,
author={L. {Fangmin} and C. {Ke} and L. {Xinhua}},
booktitle={2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA)},
title={3D Face Reconstruction Based on Convolutional Neural Network},
year={2017},
volume={},
number={},
pages={71-74},
abstract={Fast and robust 3D reconstruction of facial geometric structure from a single image is a challenging task with numerous applications, but there exist two problems when applied "in the wild": the 3D estimates are unstable for different photos of the same subject; the 3D estimates are over-regularized and generic. In response, a robust method for regressing discriminative 3D morphable face models(3DMM) is described to support face recognition and 3D mask printing. Combining the local data sets with the public data sets, improving the exiting 3DMM fitting method and then using a convolutional neural network(CNN) to improve reconstruction effect. The ground truth 3D faces of the CNN are the pooled 3DMM parameters extracted from the photos of the same subject. Using CNN to regress 3DMM shape and texture parameters directly from an input photo and offering a method for generating huge numbers of labeled examples. There are two key points of the paper: one is the training data generation for the model training; the other is the training of 3D reconstruction model. Experimental results and analysis show that this method costs much less time than traditional methods of 3D face modeling, and it is improved for different races on photos with any angles than the existing methods based on deep learning, and the system has better robustness.},
keywords={face recognition;image morphing;image reconstruction;image texture;learning (artificial intelligence);neural nets;regression analysis;shape recognition;3D face reconstruction;facial geometric structure;robust method;face recognition;3D mask printing;local data sets;public data sets;reconstruction effect;texture parameters;training data generation;3D reconstruction model;3D face modeling;convolutional neural network;discriminative 3D morphable face models;3DMM fitting method;CNN;Face;Three-dimensional displays;Solid modeling;Image reconstruction;Shape;Robustness;Data models;3D face reconstruction;convolutional neural network(CNN);3DMM;shape;texture},
doi={10.1109/ICICTA.2017.23},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7163165,
author={L. A. {Jeni} and J. M. {Girard} and J. F. {Cohn} and T. {Kanade}},
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
title={Real-time dense 3D face alignment from 2D video with automatic facial action unit coding},
year={2015},
volume={1},
number={},
pages={1-1},
abstract={Face alignment is the problem of automatically locating detailed facial landmarks across different subjects, illuminations, and viewpoints. Previous methods can be divided into two broad categories. 2D-based methods locate a relatively small number of 2D fiducial points in real time while 3D-based methods fit a high-resolution 3D model offline at a much higher computational cost.},
keywords={face recognition;image coding;image resolution;lighting;video signal processing;high-resolution 3D model;3D-based methods;2D fiducial points;2D-based methods;viewpoints;illuminations;facial landmarks;face alignment;automatic facial action unit coding;2D video;real-time dense 3D face alignment;Three-dimensional displays;Face;Gold;Real-time systems;Conferences;Streaming media;Encoding},
doi={10.1109/FG.2015.7163165},
ISSN={},
month={May},}
@INPROCEEDINGS{7372755,
author={S. {Ganguly} and D. {Bhattacharjee} and M. {Nasipuri}},
booktitle={TENCON 2015 - 2015 IEEE Region 10 Conference},
title={Decremental depth bunch based 3D face recognition from range image},
year={2015},
volume={},
number={},
pages={1-4},
abstract={In this paper, a new technique, i.e. decremental depth bunches have been presented where two facial discriminating mechanisms have also been implemented for recognizing the individuals. Notably, based on variations of the depth values, different bunches of face regions (i.e. the small components) are extracted by differentiating the depth information that eventually describes detailed facial surface information. Now, from each bunches, statistical attributes as well as Hough peaks are encountered to initiate two feature vectors for feature-based as well as a holistic mechanism for classification by K-NN and Cosine distance respectively. The proposed mechanism is explicitly dependent on facial depth information that have been accomplished in range face images. Therefore, authors have considered two databases, namely: Frav3D and Bosphorus, that contains laser as well as structured light 3D scanner based procured 3D face images respectively.},
keywords={face recognition;feature extraction;image classification;image scanners;optical scanners;decremental depth bunch;3D face recognition;range image;facial discriminating mechanism;face region extraction;facial surface information;statistical attributes;Hough peaks;feature vector;holistic mechanism;K-NN classification;Cosine distance;Frav3D database;Bosphorus database;light 3D scanner;laser scanner;Face recognition;Three-dimensional displays;Image restoration;Image edge detection;Databases;Uniform resource locators;Range Image;Depth bunch;Hough peaks;Statistical feature;Cosine transform;K-NN;Laser scanner;Structure light scanner;BIEM},
doi={10.1109/TENCON.2015.7372755},
ISSN={2159-3450},
month={Nov},}
@ARTICLE{7012060,
author={A. {Moeini} and H. {Moeini}},
journal={IEEE Transactions on Information Forensics and Security},
title={Real-World and Rapid Face Recognition Toward Pose and Expression Variations via Feature Library Matrix},
year={2015},
volume={10},
number={5},
pages={969-984},
abstract={In this paper, a novel method for face recognition under pose and expression variations is proposed from only a single image in the gallery. A 3D probabilistic facial expression recognition generic elastic model is proposed to reconstruct a 3D model from real-world human face using only a single 2D frontal image with/without facial expressions. Then, a feature library matrix (FLM) is generated for each subject in the gallery from all face poses by rotating the 3D reconstructed models and extracting features in the rotated face pose. Therefore, each FLM is subsequently rendered for each subject in the gallery based on triplet angles of face poses. In addition, before matching the FLM, an initial estimate of triplet angles is obtained from the face pose in probe images using an automatic head pose estimation approach. Then, an array of the FLM is selected for each subject based on the estimated triplet angles. Finally, the selected arrays from FLMs are compared with extracted features from the probe image by iterative scoring classification using the support vector machine. Convincing results are acquired to handle pose and expression changes on the Bosphorus, Face Recognition Technology (FERET), Carnegie Mellon University-Pose, Illumination, and Expression (CMU-PIE), and Labeled Faces in the Wild (LFW) face databases compared with several state-of-the-art methods in pose-invariant face recognition. The proposed method not only demonstrates an excellent performance by obtaining high accuracy on all four databases but also outperforms other approaches realistically.},
keywords={face recognition;feature extraction;image classification;image reconstruction;matrix algebra;support vector machines;pose-invariant face recognition;face databases;LFW;labeled faces in the wild;CMU-PIE;FERET;face recognition technology;support vector machine;iterative scoring classification;feature extraction;rotated face pose;FLM;single 2D frontal image;real-world human face;3D model reconstruction;3D probabilistic facial expression recognition generic elastic model;feature library matrix;pose variations;expression variations;rapid face recognition;Face;Three-dimensional displays;Solid modeling;Face recognition;Hidden Markov models;Image reconstruction;Feature extraction;Pose-invariant face recognition;3D face reconstruction;iterative scoring classification;Pose-invariant face recognition;probabilistic facial expression recognition;3D face reconstruction;feature library matrix;iterative scoring classification},
doi={10.1109/TIFS.2015.2393553},
ISSN={1556-6013},
month={May},}
@ARTICLE{8219732,
author={C. {Sagonas} and E. {Ververas} and Y. {Panagakis} and S. {Zafeiriou}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Recovering Joint and Individual Components in Facial Data},
year={2018},
volume={40},
number={11},
pages={2668-2681},
abstract={A set of images depicting faces with different expressions or in various ages consists of components that are shared across all images (i.e.,jointcomponents) imparting to the depicted object the properties of human faces as well asindividualcomponents that are related to different expressions or age groups. Discovering the common (joint) and individual components in facial images is crucial for applications such as facial expression transfer and age progression. The problem is rather challenging when dealing with images captured in unconstrained conditions in the presence of sparse non-Gaussian errors of large magnitude (i.e., sparse gross errors or outliers) and contain missing data. In this paper, we investigate the use of a method recently introduced in statistics, the so-called Joint and Individual Variance Explained (JIVE) method, for the robust recovery of joint and individual components in visual facial data consisting of an arbitrary number of views. Since the JIVE is not robust to sparse gross errors, we propose alternatives, which are (1) robust to sparse gross, non-Gaussian noise, (2) able to automatically find the individual components rank, and (3) can handle missing data. We demonstrate the effectiveness of the proposed methods to several computer vision applications, namely facial expression synthesis and 2D and 3D face age progression `in-the-wild'.},
keywords={computer vision;face recognition;Gaussian noise;facial expression synthesis;3D face age progression;joint components;facial images;facial expression transfer;sparse gross errors;visual facial data;joint and individual variance explained method;computer vision;Matrix decomposition;Robustness;Sparse matrices;Visualization;Data mining;Minimization;Computer vision;Low-rank;sparsity;facial expression synthesis;face age progression;joint and individual components},
doi={10.1109/TPAMI.2017.2784421},
ISSN={0162-8828},
month={Nov},}
@INPROCEEDINGS{8389776,
author={G. {Geetha} and M. {Safa} and C. {Fancy} and K. {Chittal}},
booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)},
title={3D face recognition using Hadoop},
year={2017},
volume={},
number={},
pages={1882-1885},
abstract={Face Recognition is one of the biometric technique to vestige the given faces. We present a 3D face recognition method using Hadoop to recognize 3D faces under varying expressions, lighting and different poses to overcome the challenges of the 2D face recognition. In this paper, a threshold facial region of an image is detected and preprocessing is done based through the image excellence. If the selected face is frontal face with good lighting, extract the prerequisite features and do the necessary comparison steps to recognize the faces. In case, if the selected face is in bad lighting, then perform histogram equalization and normalization to increase the contrast. Different Poses and expressions are the very challenging zones which require surplus pre-processing to improve the performance of the face recognition system. Hence an enhanced normalization method called 3D Morphable Model are used as a pre- processing technique to create a frontal view from a non-frontal view and also merge images with different views in to a single frontal view. Next to diminish the number of features used for recognition process; we emulate the linear discriminant analysis method for further classification. Eventually, we used an open-source Hadoop Image Processing Interface (HIPI) to act as an interface for MapReduce technology for recognition.},
keywords={biometrics (access control);face recognition;feature extraction;parallel processing;frontal face;biometric technique;3D face recognition method;histogram equalization;surplus preprocessing;enhanced normalization method;3D morphable model;single frontal view;nonfrontal view;linear discriminant analysis method;open-source Hadoop image processing interface;HIPI;MapReduce technology;Face;Face recognition;Three-dimensional displays;Lighting;Feature extraction;Solid modeling;Hadoop;Image Processing;Map Reduce;Linear Discriminant analysis},
doi={10.1109/ICECDS.2017.8389776},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8296673,
author={T. {Wu} and F. {Zhou} and Q. {Liao}},
booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
title={Real-time 3D face reconstruction from one single image by displacement mapping},
year={2017},
volume={},
number={},
pages={2204-2208},
abstract={In this paper, we present a fast and robust method to reconstruct a plausible three-dimension (3D) face from one single frontal face image. In training phase, we classify the faces into several groups based on the facial structures and propose to learn a mapping, known as the displacement mapping (DM) in this paper, for each group. DM relates two displacements: One displacements, denoted as 2D displacements, represent the differences between the positions of feature points on the 2D training faces and those on the reference 2D face that has been pre-defined for the corresponding group; another displacements, denoted as 3D displacements, are the differences between the positions of vertices on the reconstructed 3D face and those on the reference 3D face that is also pre-defined. During the reconstruction phase, we first classify the input face as one of the groups and calculate the 2D displacements. Then we take advantage of the 2D displacements and the learned DM to estimate the 3D displacements. Subsequently, 3D displacements can be used to obtain the precise 3D face by shifting the 3D reference face. Experiments on Basel face model (BFM) database as well as some real-world 2D face images demonstrate the effectiveness and efficiency of the proposed method, in comparison with some state-of-arts methods.},
keywords={face recognition;image reconstruction;facial structures;real-time 3D face reconstruction;plausible three-dimension face;reference 3D face;reconstructed 3D face;2D training faces;single frontal face image;displacement mapping;real-world 2D face images;Basel face model database;3D reference face;precise 3D face;Face;Three-dimensional displays;Two dimensional displays;Training;Image reconstruction;Solid modeling;Nose;3D face reconstruction;feature points;displacement mapping;reference model},
doi={10.1109/ICIP.2017.8296673},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{8518089,
author={S. {Li} and L. {Su} and Y. {Liu} and Z. {He}},
booktitle={IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium},
title={Segmentation of Individual Trees Based on a Point Cloud Clustering Method Using Airborne Lidar Data},
year={2018},
volume={},
number={},
pages={7520-7523},
abstract={The objective of this paper was to develop a new algorithm to segment individual trees directly by using the three-dimensional space characteristic of airborne light detection and ranging point cloud data. The local maximum method was used in the initial segmentation and the error identification tree exclusion. On the basis of the point cloud spatial distribution of individual trees and the adjacent relationship with the other trees, a point cloud clustering method was developed to decide the points belonging to the individual trees. This algorithm was tested by 6 forest plots in the Genhe forestry reserve. The results showed that this algorithm could segment individual trees quickly and accurately, and the overall accuracy of this algorithm was 96.3%.},
keywords={forestry;geophysical image processing;image segmentation;optical radar;pattern clustering;remote sensing by laser beam;vegetation mapping;segment individual trees;point cloud clustering method;airborne lidar data;three-dimensional space characteristic;airborne light detection;ranging point cloud data;local maximum method;initial segmentation;error identification tree exclusion;point cloud spatial distribution;Genhe forestry reserve;Vegetation;Three-dimensional displays;Forestry;Laser radar;Clustering algorithms;Remote sensing;Lasers;LiDAR;segmentation;tree;clustering},
doi={10.1109/IGARSS.2018.8518089},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{8346387,
author={H. M. R. {Afzal} and S. {Luo} and M. K. {Afzal}},
booktitle={2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)},
title={Reconstruction of 3D facial image using a single 2D image},
year={2018},
volume={},
number={},
pages={1-5},
abstract={In many scenarios related to image processing, 3D face reconstruction is considered an essential part. A robust and fast method of 3D face reconstruction from a single 2D image is presented. This method consists of three main steps including extraction of features from single image, calculating the depth of image and adjustment of a 3D model on the direction of Z-axis. In the first step, the features of image are extracted by using supervised descent method (SDM). Using SDM, face regions like facial components (eyes, nose lips) and face contours are detected. Second step consists of depth prediction by implementation of multivariate Gaussian distribution. Finally, 3D face is constructed with the help of features and the depth information and 3D database. The proposed method has been verified by conducting several experiments depicted in evaluation section. Our method is robust in nature and gives good results even using a single image, comparing to other methods that use multiple images for reconstruction of 3D images.},
keywords={face recognition;feature extraction;Gaussian distribution;gradient methods;image reconstruction;stereo image processing;3D face reconstruction;supervised descent method;SDM;face regions;3D model;facial components;face contours;depth prediction;multivariate Gaussian distribution;image processing;single 2D image;3D facial image;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Shape;Two dimensional displays;Solid modeling;3D face reconstruction;features extraction;Gaussain distribution;facial modeling},
doi={10.1109/ICOMET.2018.8346387},
ISSN={},
month={March},}
@INPROCEEDINGS{8248353,
author={Z. {Cheng} and T. {Shi} and W. {Cui} and Y. {Dong} and X. {Fang}},
booktitle={2017 4th International Conference on Systems and Informatics (ICSAI)},
title={3D face recognition based on kinect depth data},
year={2017},
volume={},
number={},
pages={555-559},
abstract={In this paper, a contour map human facial recognition algorithm is proposed to implement the three-dimensional (3D) face recognition with the Kinect Xbox One. Since the scale of 3D depth data collected from Kinect is tremendous, the face recognition process cannot be handled in real time. To improve the speed and accuracy of the recognition process, the proposed algorithm turns the 3D depth data to the two-dimensional (2D) contour map. Furthermore, due to the 3D depth data obtained by Kinect, there is no need of expensive, ponderous and slow 3D scanners. Ten male and female subjects were involved in the validation experiment and the results verify that the proposed algorithm was feasible for face recognition. In addition, compared with other methods, Eigenface, Local Binary Patterns (LBP) and Linear Discriminant Analysis (LDA), the proposed algorithm has the better security and reliability.},
keywords={face recognition;feature extraction;kinect depth data;contour map human facial recognition algorithm;three-dimensional face recognition;Kinect Xbox;3D depth data;face recognition process;two-dimensional contour map;Face recognition;Three-dimensional displays;Face;Algorithm design and analysis;Two dimensional displays;Lighting;Fingerprint recognition;face recognition;Kinect;3D depth data;2D contour map},
doi={10.1109/ICSAI.2017.8248353},
ISSN={},
month={Nov},}
@ARTICLE{7194796,
author={F. {Juefei-Xu} and K. {Luu} and M. {Savvides}},
journal={IEEE Transactions on Image Processing},
title={Spartans: Single-Sample Periocular-Based Alignment-Robust Recognition Technique Applied to Non-Frontal Scenarios},
year={2015},
volume={24},
number={12},
pages={4780-4795},
abstract={In this paper, we investigate a single-sample periocular-based alignment-robust face recognition technique that is pose-tolerant under unconstrained face matching scenarios. Our Spartans framework starts by utilizing one single sample per subject class, and generate new face images under a wide range of 3D rotations using the 3D generic elastic model which is both accurate and computationally economic. Then, we focus on the periocular region where the most stable and discriminant features on human faces are retained, and marginalize out the regions beyond the periocular region since they are more susceptible to expression variations and occlusions. A novel facial descriptor, high-dimensional Walsh local binary patterns, is uniformly sampled on facial images with robustness toward alignment. During the learning stage, subject-dependent advanced correlation filters are learned for pose-tolerant non-linear subspace modeling in kernel feature space followed by a coupled max-pooling mechanism which further improve the performance. Given any unconstrained unseen face image, the Spartans can produce a highly discriminative matching score, thus achieving high verification rate. We have evaluated our method on the challenging Labeled Faces in the Wild database and solidly outperformed the state-of-the-art algorithms under four evaluation protocols with a high accuracy of 89.69%, a top score among image-restricted and unsupervised protocols. The advancement of Spartans is also proven in the Face Recognition Grand Challenge and Multi-PIE databases. In addition, our learning method based on advanced correlation filters is much more effective, in terms of learning subject-dependent pose-tolerant subspaces, compared with many well-established subspace methods in both linear and non-linear cases.},
keywords={correlation methods;emotion recognition;face recognition;feature extraction;filtering theory;image matching;learning (artificial intelligence);pose estimation;solid modelling;Spartans framework;single-sample periocular-based alignment-robust face recognition technique;nonfrontal scenarios;unconstrained face matching scenarios;face image generation;3D rotations;3D generic elastic model;discriminant features;expression variations;occlusions;facial descriptor;high-dimensional Walsh local binary patterns;subject-dependent advanced correlation filters;learning stage;pose-tolerant nonlinear subspace modeling;kernel feature space;coupled max-pooling mechanism;Face Recognition Grand Challenge database;multi PIE database;Face;Feature extraction;Three-dimensional displays;Kernel;Face recognition;Databases;Solid modeling;Periocular-based recognition;pose tolerance;alignment robustness;unconstrained face recognition;singlesample 3D face synthesis;advanced correlation filters;Periocular-based recognition;pose tolerance;alignment robustness;unconstrained face recognition;singlesample 3D face synthesis;advanced correlation filters;Algorithms;Face;Humans;Image Processing, Computer-Assisted;ROC Curve},
doi={10.1109/TIP.2015.2468173},
ISSN={1057-7149},
month={Dec},}
@INPROCEEDINGS{7449603,
author={D. {Marvadi} and C. {Paunwala} and M. {Joshi} and A. {Vora}},
booktitle={2015 5th Nirma University International Conference on Engineering (NUiCONE)},
title={Comparative analysis of 3D face recognition using 2D-PCA and 2D-LDA approaches},
year={2015},
volume={},
number={},
pages={1-5},
abstract={Even if, most of 2D face recognition approaches reached recognition rate more than 90% in controlled environment, current days face recognition systems degrade their performance in case of uncontrolled environment which includes pose variations, illumination variations, expression variations and ageing effect etc. Inclusion of 3D face analysis gives an age over 2D face recognition as they give vital informations such as 3D shape, texture and depth which improve discrimination power of an algorithm. In this paper, we have investigated different 3D face recognition approaches that are robust to changes in facial expressions and illumination variations. 2D-PCA and 2D-LDA approaches have been extended to 3D face recognition because they can directly work on 2D depth image matrices rather than 1D vectors without need for transformations before feature extraction. In turn, this reduces storage space and time required for computations. 2D depth image is extracted from 3D face model and nose region from depth mapped image has been detected as a reference point for cropping stage to convert model into a standard size. Two Dimensional Principal Component Analysis (2D-PCA) and Two Dimensional Linear Discriminant analysis (2D-LDA) are employed to obtain feature vectors globally compared to feature vectors obtained locally using PCA or LDA. Finally, euclidean distance classifier is applied for comparison of extracted features. A set of experiments on GavabDB 3D face database, which has 61 individuals in total, demonstrated that 3D face recognition using 2D-LDA method has achieved recognition accuracy of 93.3% and EER of 8.96% over database, which is higher compared to 2D-PCA. So, more optimized performance has been achieved using 2D-LDA for 3D face recognition analysis.},
keywords={face recognition;feature extraction;image classification;principal component analysis;3D face recognition;2D-PCA approaches;2D-LDA approaches;pose variations;illumination variations;expression variations;ageing effect;3D face analysis;2D face recognition;3D shape;3D texture;3D depth;facial expressions;2D depth image matrices;feature extraction;nose region;cropping stage;two dimensional principal component analysis;two dimensional linear discriminant analysis;feature vectors;Euclidean distance classifier;GavabDB 3D face database;Three-dimensional displays;Face recognition;Databases;Face;Principal component analysis;Nose;Covariance matrices;Eigen-Surface;Fisher-Surface;PCA;2D-PCA;LDA;2D-LDA;EER},
doi={10.1109/NUICONE.2015.7449603},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8457967,
author={A. {ElSayed} and A. {Mahmood} and T. {Sobh}},
booktitle={2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)},
title={Effect of Super Resolution on High Dimensional Features for Unsupervised Face Recognition in the Wild},
year={2017},
volume={},
number={},
pages={1-5},
abstract={Majority of the face recognition algorithms use query faces captured from uncontrolled, in the wild, environment. Because of cameras' limited capabilities, it is common for these captured facial images to be blurred or low resolution. Super resolution algorithms are therefore crucial in improving the resolution of such images especially when the image size is small and enlargement is required. This paper aims to demonstrate the effect of one of the state-of-the-art algorithms in the field of image super resolution. To demonstrate the functionality of the algorithm, various before and after 3D face alignment cases are provided using the images from the Labeled Faces in the Wild (lfw) dataset. Resulting images are subject to test on a closed set recognition protocol using unsupervised algorithms with high dimensional extracted features. The inclusion of super resolution algorithm resulted in significant improvement in recognition rate over recently reported results obtained from unsupervised algorithms on the same dataset.},
keywords={face recognition;feature extraction;image resolution;unsupervised learning;image super resolution;3D face alignment cases;Labeled Faces;Wild dataset;closed set recognition protocol;unsupervised algorithms;high dimensional extracted features;super resolution algorithm;unsupervised face recognition;face recognition algorithms;query faces;image size;Face;Image resolution;Face recognition;Feature extraction;Signal resolution;Histograms;Protocols;Super-Resolution;high dimensional features;unsupervised learning;face recognition;label faces in the wild (lfw)},
doi={10.1109/AIPR.2017.8457967},
ISSN={2332-5615},
month={Oct},}
@ARTICLE{7045610,
author={S. G. {Kong} and R. O. {Mbouna}},
journal={IEEE Transactions on Image Processing},
title={Head Pose Estimation From a 2D Face Image Using 3D Face Morphing With Depth Parameters},
year={2015},
volume={24},
number={6},
pages={1801-1808},
abstract={This paper presents estimation of head pose angles from a single 2D face image using a 3D face model morphed from a reference face model. A reference model refers to a 3D face of a person of the same ethnicity and gender as the query subject. The proposed scheme minimizes the disparity between the two sets of prominent facial features on the query face image and the corresponding points on the 3D face model to estimate the head pose angles. The 3D face model used is morphed from a reference model to be more specific to the query face in terms of the depth error at the feature points. The morphing process produces a 3D face model more specific to the query image when multiple 2D face images of the query subject are available for training. The proposed morphing process is computationally efficient since the depth of a 3D face model is adjusted by a scalar depth parameter at feature points. Optimal depth parameters are found by minimizing the disparity between the 2D features of the query face image and the corresponding features on the morphed 3D model projected onto 2D space. The proposed head pose estimation technique was evaluated on two benchmarking databases: 1) the USF Human-ID database for depth estimation and 2) the Pointing'04 database for head pose estimation. Experiment results demonstrate that head pose estimation errors in nodding and shaking angles are as low as 7.93° and 4.65° on average for a single 2D input face image.},
keywords={estimation theory;face recognition;image morphing;pose estimation;solid modelling;2D face image;3D face morphing;3D face model;reference face model;reference model;query subject;facial feature;query face image;head pose angle;morphing process;query image;optimal depth parameter;head pose estimation technique;benchmarking database;USF human-ID database;depth estimation;Face;Solid modeling;Estimation;Computational modeling;Feature extraction;Databases;Head pose estimation;3-D face model;Feature disparity minimization;Head pose estimation;3D face model;morphing;feature disparity minimization;depth estimation},
doi={10.1109/TIP.2015.2405483},
ISSN={1057-7149},
month={June},}
@INPROCEEDINGS{7827274,
author={L. {Han} and Q. {Xiao} and S. {Wang}},
booktitle={2016 23rd International Conference on Mechatronics and Machine Vision in Practice (M2VIP)},
title={3D face reconstruction with global and local constraints in double spaces},
year={2016},
volume={},
number={},
pages={1-5},
abstract={In this paper, we proposed a new 3D face reconstruction approach to reconstruct the 3D face from a single 2D image with arbitrary pose. The proposed approach enhanced the 3D morphable model by introducing a new local face shape constraint in another space. Then, the face can be constrained in double spaces: the global face shape space and the local face structure space. The advantages of the proposed approach are: 1) it explores the global and local constraints in the double spaces; 2) it reconstructs the 3D face from a single face image with arbitrary pose. The approach is validated on BJUT_3D face database, and the 3D facial expressions are generated on FacewareHouse and FaceScurb database.},
keywords={face recognition;image reconstruction;shape recognition;solid modelling;visual databases;3D face reconstruction;global constraints;local constraints;double spaces;2D image;arbitrary pose;3D morphable model;local face shape constraint;global face shape space;local face structure space;BJUT_3D face database;3D facial expressions;FacewareHouse;FaceScurb database;Face;Three-dimensional displays;Image reconstruction;Two dimensional displays;Shape;Solid modeling;Training;3D face reconstruction;face shape space;face structure space;global constraint;local constraint},
doi={10.1109/M2VIP.2016.7827274},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7823997,
author={S. {Naveen} and K. P. {Rugmini} and R. S. {Moni}},
booktitle={2016 International Conference on Communication Systems and Networks (ComNet)},
title={3D face reconstruction by pose correction, patch cloning and texture wrapping},
year={2016},
volume={},
number={},
pages={112-116},
abstract={Face is being considered as one of the most commonly used biometric modality. The inaccuracy in two dimensional face recognition systems is mainly due to pose variations, occlusions, illumination etc. Among this, changes in illumination condition do not affect 3D face recognition systems. But pose variation drastically changes the appearance of face images. To solve the problems with depth map and texture images corrupted by head pose variations and the occlusions generated due to these pose variations, a reconstruction method is proposed which consist of three stages. In the first stage, the pose correction is done by Iterative Closest Point (ICP) algorithm and in the second stage the occluded region of the face is reconstructed by a resurfacing method called patch cloning. It is followed by the wrapping of reconstructed depth map by its texture to generate a 3D model. The statistical error between the original face and the reconstructed face is also evaluated. In this work, facial symmetry is used as a prior knowledge. Experiments are done with the FRAV3D database.},
keywords={biometrics (access control);face recognition;image reconstruction;image texture;iterative methods;lighting;pose estimation;solid modelling;3D face reconstruction;pose correction;patch cloning;biometric modality;two dimensional face recognition systems;illumination condition;texture images;head pose variations;iterative closest point;ICP algorithm;resurfacing method;reconstructed depth map texture wrapping;3D model;statistical error evaluation;facial symmetry;FRAV3D database;Face;Three-dimensional displays;Image reconstruction;Face recognition;Iterative closest point algorithm;Cloning;Solid modeling;Face recognition;Face Resurfacing;Pose Correction;ICP algorithm;Patch Cloning},
doi={10.1109/CSN.2016.7823997},
ISSN={},
month={July},}
@INPROCEEDINGS{8267812,
author={A. {Sulistiyono} and A. K. P. {Atmani} and S. G. {Gunanto} and },
booktitle={2017 International Conference on Smart Cities, Automation Intelligent Computing Systems (ICON-SONICS)},
title={Toward to an exaggeration engine for facial animation: Evaluating the difference of RBF implementation in expression-marker transfer},
year={2017},
volume={},
number={},
pages={1-5},
abstract={The human face has a unique shape and size, as well as a 3D character face model. During this process of animated facial expression of 3D virtual characters are mostly still done manually by moving the rig in each frame. The more characters used, the more production costs incurred. The absence of a cheap facial motion transfer system is also one of the reasons why not many studios are using motion capture technology in Indonesia. This study will evaluate the implementation of radial basis function (RBF) as a method of marker-transfer used as a reference to rig movement in the facial animation system. Testing is done by performing variations of radial function, namely: Gaussian, Inverse Quadratic, Inverse Multiquadric, and Multiquadric. The value of epsilon used is 0.01. The experimental results show that the range of feature point shifts generated by RBF Gaussian, Inverse Multiquadric, Inverse Quadratic, and Multiquadric have the same pattern with the difference in the distance of the marker rigging point shift on the target 3D model. The farthest range of differences is generated by RBF Gaussian. The resulting maximum range difference can reach 92.37% and a minimum of 2.47% compared to other methods. This appears on the Gaussian chart to have a sharper pattern. As with the concept of exaggeration, the Gaussian method which has the maximum range is the right method to apply the exaggeration principle in 3D face-expression animation.},
keywords={computer animation;face recognition;Gaussian processes;image capture;radial basis function networks;inverse quadratic;inverse multiquadric;RBF Gaussian method;multiquadric;feature point shift range;facial motion transfer system;radial basis function method;3D face-expression animation;marker rigging point shift;facial animation system;motion capture technology;3D virtual characters;3D character face model;human face;exaggeration engine;Face;Three-dimensional displays;Solid modeling;Facial animation;Two dimensional displays;Electronic mail;facial animation;exaggeration;RBF},
doi={10.1109/ICON-SONICS.2017.8267812},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7846562,
author={J. {Yin} and X. {Yang}},
booktitle={2016 International Conference on Audio, Language and Image Processing (ICALIP)},
title={3D facial reconstruction of based on OpenCV and DirectX},
year={2016},
volume={},
number={},
pages={341-344},
abstract={Face as a biometric identification in computer vision is an important medium, in areas such as video surveillance, animation games, security anti-terrorist has a very wide range of applications, creating vivid, strong visibility of 3d face model, now has become a challenging in the field of computer vision is one of the important topics. At first, this paper used the zhongxing-micro ZC301P cameras to build a binocular stereo vision system for recording images. After the camera calibration and binocular calibration, the three-dimensional data of facial images were extracted using the functions of OpenCV computer vision library, and then 3d face model were reconstructed preliminary by DirectX. According the reconstruction process, the human face three-dimensional reconstruction software was designed and developed. The paper laid the foundation for the next step work that is to obtain more clear and strong visibility of 3d face.},
keywords={calibration;cameras;computer vision;face recognition;image reconstruction;stereo image processing;visibility;human face three-dimensional reconstruction software;DirectX;OpenCV function;facial image three-dimensional data extraction;binocular calibration;camera calibration;binocular stereo vision system;zhongxing-micro ZC301P camera;3D face model;computer vision;biometric identification;3D facial reconstruction;Three-dimensional displays;Image reconstruction;Solid modeling;Face;Cameras;Computational modeling;Computer vision;Computer vision;3d face model;OpenCV;DirectX},
doi={10.1109/ICALIP.2016.7846562},
ISSN={},
month={July},}
@INPROCEEDINGS{7153124,
author={G. {Pang} and R. {Qiu} and J. {Huang} and S. {You} and U. {Neumann}},
booktitle={2015 14th IAPR International Conference on Machine Vision Applications (MVA)},
title={Automatic 3D industrial point cloud modeling and recognition},
year={2015},
volume={},
number={},
pages={22-25},
abstract={3D modeling of point clouds is an important but time-consuming process, inspiring extensive research in automatic methods. Prior efforts focus on primitive geometry, street structures or indoor objects, but industrial data has rarely been pursued. Our work presents a method for automatic modeling and recognition of 3D industrial site point clouds, dividing the task into 3 separate sub-problems: pipe modeling, plane classification, and object recognition. The results are integrated to obtain the complete model, revealing some issues during the integration, solved by utilizing information gained from each individual process. Experiments show that the presented method automatically models large and complex industrial scenes with a quality that outperforms leading commercial modeling software and is comparable to professional hand-made models.},
keywords={image classification;mechanical engineering computing;object recognition;pipes;automatic 3D industrial point cloud modeling;automatic 3D industrial point cloud recognition;primitive geometry;street structures;indoor objects;plane classification;pipe modeling;object recognition;Three-dimensional displays;Solid modeling;Data models;Object recognition;Libraries;Software;Surface treatment},
doi={10.1109/MVA.2015.7153124},
ISSN={},
month={May},}
@INPROCEEDINGS{8095315,
author={F. M. Z. {Heravi} and A. {Nait-Ali}},
booktitle={2017 2nd International Conference on Bio-engineering for Smart Technologies (BioSMART)},
title={A 3D dynamic shape model to simulate rejuvenation ageing trajectory of 3D face images},
year={2017},
volume={},
number={},
pages={1-5},
abstract={It is not only interesting to predict how an individual of a relatively young age will look in the future but also to reconstruct the facial appearance in the past during childhood. It can be even more desirable when different circumstances, behavior and lifestyle and their impacts on the facial shape appearance as a consequence are taken into account. Such may be applicable for many practical reasons in healthcare, forensics psychology, missing people and children, etc. This paper presents the 3D Face Time Machine Matrix (FT2M), a 3D Dynamic Shape Model which is a fusion of two models of ageing and rejuvenation with facial shape variations due to lifestyle and behavioral factors. This dynamic model is learned from a database of three dimensional facial images which is built by ten individual age groups between 3 to 75 years old. 3D facial aging modeling is a complex process since it affects both the shape and texture of the face. We propose a Dynamic face model to transform the given input face to his youthful or adulthood appearance by taking into account his lifestyle and behavioral traits and the probable changes may occur in perceptible appearance by altering its shape and texture simultaneously.},
keywords={face recognition;image texture;shape recognition;3D dynamic shape model;facial appearance;facial shape appearance;facial shape variations;behavioral factors;dynamic model;individual age groups;3D facial aging modeling;behavioral traits;face time machine matrix;3D face time machine matrix;FT2M;three dimensional facial images;dynamic face model;age regression;Rejuvenation;Ageing Trajectory;Face;Three-dimensional displays;Shape;Aging;Solid modeling;Image reconstruction;Fats;Face Time Machine Matrix (FT2M);3D Facial Modeling;Shape;Perception},
doi={10.1109/BIOSMART.2017.8095315},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8272733,
author={P. {Dou} and I. A. {Kakadiaris}},
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)},
title={Multi-view 3D face reconstruction with deep recurrent neural networks},
year={2017},
volume={},
number={},
pages={483-492},
abstract={Image-based 3D face reconstruction has great potential in different areas, such as facial recognition, facial analysis, and facial animation. Due to the variations in image quality, single-image-based 3D face reconstruction might not be sufficient to accurately reconstruct a 3D face. To overcome this limitation, multi-view 3D face reconstruction uses multiple images of the same subject and aggregates complementary information for better accuracy. Though theoretically appealing, there are multiple challenges in practice. Among these challenges, the most significant is that it is difficult to establish coherent and accurate correspondence among a set of images, especially when these images are captured in different conditions. In this paper, we propose a method, Deep Recurrent 3D FAce Reconstruction (DRFAR), to solve the task ofmulti-view 3D face reconstruction using a subspace representation of the 3D facial shape and a deep recurrent neural network that consists of both a deep con-volutional neural network (DCNN) and a recurrent neural network (RNN). The DCNN disentangles the facial identity and the facial expression components for each single image independently, while the RNN fuses identity-related features from the DCNN and aggregates the identity specific contextual information, or the identity signal, from the whole set of images to predict the facial identity parameter, which is robust to variations in image quality and is consistent over the whole set of images. Through extensive experiments, we evaluate our proposed method and demonstrate its superiority over existing methods.},
keywords={computer animation;convolution;face recognition;feedforward neural nets;image reconstruction;recurrent neural nets;stereo image processing;3D facial shape;deep recurrent neural network;facial expression components;single image;facial identity parameter;image quality;multiview 3D face reconstruction;deep recurrent neural networks Image;facial recognition;facial analysis;facial animation;single-image;multiple images;Deep Recurrent 3D FAce Reconstruction;deep convolutional neural network;DRFAR;DCNN;Three-dimensional displays;Face;Image reconstruction;Solid modeling;Recurrent neural networks;Shape;Two dimensional displays},
doi={10.1109/BTAS.2017.8272733},
ISSN={2474-9699},
month={Oct},}
@INPROCEEDINGS{7163090,
author={X. {Yang} and D. {Huang} and Y. {Wang} and L. {Chen}},
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
title={Automatic 3D facial expression recognition using geometric scattering representation},
year={2015},
volume={1},
number={},
pages={1-6},
abstract={Facial Expression Recognition (FER) is one of the most active topics in the domain of computer vision and pattern recognition, and it has received increasing attention for its wide application potentials as well as attractive scientific challenges. In this paper, we present a novel method to automatic 3D FER based on geometric scattering representation. A set of maps of shape features in terms of multiple order differential quantities, i.e. the Normal Maps (NOM) and the Shape Index Maps (SIM), are first jointly adopted to comprehensively describe geometry attributes of the facial surface. The scattering operator is then introduced to further highlight expression related cues on these maps, thereby constructing geometric scattering representations of 3D faces for classification. The scattering descriptor not only encodes distinct local shape changes of various expressions as by several milestone descriptors, such as SIFT, HOG, etc., but also captures subtle information hidden in high frequencies, which is quite crucial to better distinguish expressions that are easily confused. We evaluate the proposed approach on the BU-3DFE database, and the performance is up to 84.8% and 82.7% with two commonly used protocols respectively which is superior to the state of the art ones.},
keywords={emotion recognition;face recognition;image classification;image representation;shape recognition;automatic 3D facial expression recognition;automatic 3D FER;BU-3DFE database;local shape changes;3D face classification;scattering operator;facial surface geometry attributes;SIM;shape index maps;NOM;normal maps;multiple order differential quantities;shape feature maps;geometric scattering representation;Three-dimensional displays;Shape;Scattering;Indexes;Support vector machines;Solid modeling;Feature extraction},
doi={10.1109/FG.2015.7163090},
ISSN={},
month={May},}
@INPROCEEDINGS{8296795,
author={S. {Soltanpour} and Q. M. J. {Wu}},
booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
title={High-order local normal derivative pattern (LNDP) for 3D face recognition},
year={2017},
volume={},
number={},
pages={2811-2815},
abstract={This paper proposes a novel descriptor based on the local derivative pattern (LDP) for 3D face recognition. Compared to the local binary pattern (LBP), LDP can capture more detailed information by encoding directional pattern features. It is based on the local derivative variations that extract high-order local information. We propose a novel discriminative facial shape descriptor, local normal derivative pattern (LNDP) that extracts LDP from the surface normal. Using surface normal, the orientation of a surface at each point is determined as a first-order surface differential. Three normal component images are extracted by estimating three components of normal vectors in x, y, and z channels. Each normal component is divided into several patches and encoded using LDP. The final descriptor is created by concatenating histograms of the LNDP on each patch. Experimental results on two famous 3D face databases, FRGC v2.0 and Bosphorus illustrate the effectiveness of the proposed descriptor.},
keywords={face recognition;feature extraction;image capture;stereo image processing;vectors;3D face recognition;local binary pattern;LDP;detailed information;directional pattern features;local derivative variations;high-order local information;LNDP;surface normal;first-order surface;normal component images;normal vectors;famous 3D face databases;high-order local normal derivative pattern;local derivative pattern;discriminative facial shape descriptor;Three-dimensional displays;Face;Histograms;Feature extraction;Face recognition;Data mining;Databases;3D face;surface normal;local derivative pattern;high-order local pattern},
doi={10.1109/ICIP.2017.8296795},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{8227850,
author={H. {Hu} and S. A. A. {Shah} and M. {Bennamoun} and M. {Molton}},
booktitle={TENCON 2017 - 2017 IEEE Region 10 Conference},
title={2D and 3D face recognition using convolutional neural network},
year={2017},
volume={},
number={},
pages={133-132},
abstract={Face recognition remains a challenge today as recognition performance is strongly affected by variability such as illumination, expressions and poses. In this work we apply Convolutional Neural Networks (CNNs) on the challenging task of both 2D and 3D face recognition. We constructed two CNN models, namely CNN-1 (two convolutional layers) and CNN-2 (one convolutional layer) for testing on 2D and 3D dataset. A comprehensive parametric study of two CNN models on face recognition is represented in which different combinations of activation function, learning rate and filter size are investigated. We find that CNN-2 has a better accuracy performance on both 2D and 3D face recognition. Our experimental results show that an accuracy of 85.15% was accomplished using CNN-2 on depth images with FRGCv2.0 dataset (4950 images with 557 objectives). An accuracy of 95% was achieved using CNN-2 on 2D raw image with the AT&amp;T dataset (400 images with 40 objectives). The results indicate that the proposed CNN model is capable to handle complex information from facial images in different dimensions. These results provide valuable insights into further application of CNN on 3D face recognition.},
keywords={face recognition;feature extraction;feedforward neural nets;learning (artificial intelligence);convolutional neural network;recognition performance;CNN model;CNN-1;CNN-2;2D raw image;2D face recognition;3D face recognition;convolutional layers;2D dataset;3D dataset;activation function;learning rate;filter size;depth images;FRGCv2.0 dataset;AT&T dataset;facial images;Face recognition;Two dimensional displays;Three-dimensional displays;Face;Training;Solid modeling;Feature extraction;Face Recognition;Convolutional Neural Networks;Depth Image},
doi={10.1109/TENCON.2017.8227850},
ISSN={2159-3450},
month={Nov},}
@INPROCEEDINGS{7130256,
author={N. C. {Camgöz} and B. {Gökberk} and L. {Akarun}},
booktitle={2015 23nd Signal Processing and Communications Applications Conference (SIU)},
title={Facial landmark localization in depth images using Supervised Descent Method},
year={2015},
volume={},
number={},
pages={1997-2000},
abstract={This paper proposes using the state of the art 2D facial landmark localization method, Supervised Descent Method (SDM), for facial landmark localization in 3D depth images. The proposed method was evaluated on frontal faces with no occlusion from the Bosphorus 3D Face Database. In the experiments, in which 2D features were used to train SDM, the proposed approach achieved state-of-the-art performance for several landmarks over the currently available 3D facial landmark localization methods.},
keywords={face recognition;object detection;visual databases;supervised descent method;localization 2D facial landmark localization method;SDM;3D depth images;frontal faces;Bosphorus 3D face database;3D facial landmark localization methods;Three-dimensional displays;Computer vision;Histograms;Principal component analysis;Databases;Pattern recognition;Conferences;supervised descent method;3D facial landmark localization;face depth images},
doi={10.1109/SIU.2015.7130256},
ISSN={2165-0608},
month={May},}
@ARTICLE{8443382,
author={F. {Mokhayeri} and E. {Granger} and G. {Bilodeau}},
journal={IEEE Transactions on Information Forensics and Security},
title={Domain-Specific Face Synthesis for Video Face Recognition From a Single Sample Per Person},
year={2019},
volume={14},
number={3},
pages={757-772},
abstract={In video surveillance, face recognition (FR) systems are employed to detect individuals of interest appearing over a distributed network of cameras. The performance of still-to-video FR systems can decline significantly because faces captured in unconstrained operational domain (OD) over multiple video cameras have a different underlying data distribution compared to faces captured under controlled conditions in the enrollment domain with a still camera. This is particularly true when individuals are enrolled to the system using a single reference still. To improve the robustness of these systems, it is possible to augment the reference set by generating synthetic faces based on the original still. However, without the knowledge of the OD, many synthetic images must be generated to account for all possible capture conditions. FR systems may, therefore, require complex implementations and yield lower accuracy when training on many less relevant images. This paper introduces an algorithm for domain-specific face synthesis (DSFS) that exploits the representative intra-class variation information available from the OD. Prior to operation (during camera calibration), a compact set of faces from unknown persons appearing in the OD is selected through affinity propagation clustering in the captured condition space (defined by pose and illumination estimation). The domain-specific variations of these face images are then projected onto the reference still of each individual by integrating an image-based face relighting technique inside the 3-D reconstruction framework. A compact set of synthetic faces is generated that resemble individuals of interest under the capture conditions relevant to the OD. In a particular implementation based on sparse representation classification, the synthetic faces generated with the DSFS are employed to form a cross-domain dictionary that accounts for structured sparsity, where the dictionary blocks combine the original and synthetic faces of each individual. Experimental results obtained with videos from the Chokepoint and COX-S2V data sets reveal that augmenting the reference gallery set of still-to-video FR systems using the proposed DSFS approach can provide a significantly higher level of accuracy compared with the state-of-the-art approaches, with only a moderate increase in its computational complexity.},
keywords={computational complexity;face recognition;image classification;image representation;pattern clustering;video cameras;video signal processing;video surveillance;video surveillance;face recognition systems;still-to-video FR systems;unconstrained operational domain;OD;multiple video cameras;enrollment domain;single reference;synthetic faces;synthetic images;camera calibration;compact set;captured condition space;domain-specific variations;face images;image-based face relighting technique;cross-domain dictionary;reference gallery set;single sample per person;video face recognition;domain-specific face synthesis;representative intraclass variation information;Face;Robustness;Lighting;Dictionaries;Cameras;Face recognition;Training;Face recognition;single sample per person;face synthesis;3D face reconstruction;illumination transferring;sparse representation-based classification;video surveillance},
doi={10.1109/TIFS.2018.2866295},
ISSN={1556-6013},
month={March},}
@INPROCEEDINGS{7428562,
author={J. {Liu} and Q. {Zhang} and C. {Tang}},
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
title={Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information},
year={2015},
volume={},
number={},
pages={276-284},
abstract={3D facial landmarks play a significant role in many 3D facial studies. Due to the complex geometry of high resolution 3D human face, landmark detection remains to be a challenge problem. This paper proposes a novel method to detect landmarks automatically for high resolution 3D faces without learning or training, solely using geometric information. For high resolution 3D faces, geodesic remeshing is used to reduce the vertices number, then the remeshed 3D faces are parameterized and interpolated on a regular grid, which enable us to compute the differential geometric features more efficiently and accurately. To detect landmarks, we extract global and local constraints for each landmark by considering relative positions of landmarks and differential geometric features, then landmarks are detected by combine these constraints. We evaluate our method and compare it with other methods which are mostly related to ours in a large scale publicly available 3D face data set, BJUT-3D face database. The experimental results show that our method is robust and effective, and achieves better performance than existing methods.},
keywords={face recognition;feature extraction;image resolution;interpolation;mesh generation;object detection;automatic landmark detection;high resolution nonrigid 3D faces;geometric information;3D facial landmarks;geodesic remeshing;vertices number;parameterization;interpolation;regular grid;differential geometric features;global constraints extraction;local constraints extraction;Three-dimensional displays;Indexes;Shape;Training;Feature extraction;Facial animation;Mesh generation;3D faces;landmarks;geometric information;geodesic remeshing;differential geometric features},
doi={10.1109/IAEAC.2015.7428562},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8361546,
author={R. {Ahdid} and K. {Taifi} and S. {Said} and M. {Fakir} and B. {Manaut}},
booktitle={2017 14th International Conference on Computer Graphics, Imaging and Visualization},
title={Automatic Face Recognition System Using Iso-Geodesic Curves in Riemanian Manifold},
year={2017},
volume={},
number={},
pages={73-78},
abstract={In this paper, we present an automatic 3D face recognition system. This system is based on the representation of human faces surfaces as collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two facial surfaces, we compute a geodesic distance between a pair of facial curves using a Riemannian geometry. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008).},
keywords={differential geometry;face recognition;feature extraction;support vector machines;facial curves;geodesic distance;facial surfaces;3D Fast Marching algorithm;human faces surfaces;automatic 3D face recognition system;riemanian manifold;Iso-Geodesic Curves;automatic face recognition system;Three-dimensional displays;Face;Face recognition;Nose;Databases;Shape;Support vector machines;3D face recognition;facial surfaces;Riemannian geometry;iso-geodesic curves;geodesic distance},
doi={10.1109/CGiV.2017.25},
ISSN={},
month={May},}
@INPROCEEDINGS{7803052,
author={R. {Caesar} and and S. G. {Gunanto}},
booktitle={2016 1st International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)},
title={An automatic 3D face model segmentation for acquiring weight motion area},
year={2016},
volume={},
number={},
pages={81-86},
abstract={Inside facial animation works there is an animator that need to be skilled enough to produce detailed animation, so the facial animation can be smooth when doing facial expressions. Every animated character requires special handling based on the characteristics of the size and location of the bone. This process, where every face model need special handling were time consuming and tedious work. For that issue this research propose method for using motion capture marker data in 3D face model for automatically segment weight motion area based on the feature point. Marker data that came from motion capture of human model will be used to represent a centroid of vertex cluster that forming expressions in animated character. The data grouping process will be spherical coordinate result calculation between feature point and vertices using modified nearest neighbor algorithm. The result obtained in this research will show the weight motion area that generated automatically from the feature point based on nearest neighbor algorithm in a 3D face model.},
keywords={computer animation;face recognition;image capture;image motion analysis;image representation;nearest neighbor algorithm;character animation;vertex cluster;centroid representation;motion capture marker data;weight motion area acquisition;3D face model segmentation;Face;Three-dimensional displays;Solid modeling;Facial animation;Data models;Information technology;Information systems;facial animation;segmentation;weight motion area;nearest neighbor;feature point},
doi={10.1109/ICITISEE.2016.7803052},
ISSN={},
month={Aug},}
@ARTICLE{7467565,
author={M. {Emambakhsh} and A. {Evans}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Nasal Patches and Curves for Expression-Robust 3D Face Recognition},
year={2017},
volume={39},
number={5},
pages={995-1007},
abstract={The potential of the nasal region for expression robust 3D face recognition is thoroughly investigated by a novel five-step algorithm. First, the nose tip location is coarsely detected and the face is segmented, aligned and the nasal region cropped. Then, a very accurate and consistent nasal landmarking algorithm detects seven keypoints on the nasal region. In the third step, a feature extraction algorithm based on the surface normals of Gabor-wavelet filtered depth maps is utilised and, then, a set of spherical patches and curves are localised over the nasal region to provide the feature descriptors. The last step applies a genetic algorithm-based feature selector to detect the most stable patches and curves over different facial expressions. The algorithm provides the highest reported nasal region-based recognition ranks on the FRGC, Bosphorus and BU-3DFE datasets. The results are comparable with, and in many cases better than, many state-of-the-art 3D face recognition algorithms, which use the whole facial domain. The proposed method does not rely on sophisticated alignment or denoising steps, is very robust when only one sample per subject is used in the gallery, and does not require a training step for the landmarking algorithm.},
keywords={face recognition;feature extraction;feature selection;Gabor filters;genetic algorithms;image segmentation;stereo image processing;wavelet transforms;nasal patches;expression-robust 3D face recognition;five-step algorithm;nose tip location detection;face segmentation;nasal landmarking algorithm;feature extraction;Gabor-wavelet filtered depth maps;genetic algorithm-based feature selector;nasal region-based recognition;FRGC datasets;Bosphorus datasets;BU-3DFE datasets;Three-dimensional displays;Face recognition;Face;Nose;Robustness;Feature extraction;Algorithm design and analysis;Face recognition;facial landmarking;nose region;feature selection;Gabor wavelets;surface normals},
doi={10.1109/TPAMI.2016.2565473},
ISSN={0162-8828},
month={May},}
@INPROCEEDINGS{7961795,
author={Y. {Tang} and L. {Chen}},
booktitle={2017 12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)},
title={3D Facial Geometric Attributes Based Anti-Spoofing Approach against Mask Attacks},
year={2017},
volume={},
number={},
pages={589-595},
abstract={3D scanning and 3D printing techniques, as the technical impetus of 3D face recognition, also boost unconsciously the security threat against it from the spoofing attacks via manufactured mask. In order to improve the robustness of 3D face recognition system, several countermeasures against mask attacks based on photometric features have been reported in recent years. However, the anti-spoofing approach involving 3D meshed face scan and the related 3D facial features have not been studied yet. For filling this gap, in this paper, we propose to exploit the anti-spoofing performance of geometric attributes based 3D facial description. It synthesises the advantages of the selected geometric attributes, named principal curvature measures, and the meshSIFT-based feature descriptor. Specifically, the estimation of geometric attributes is coherent to the property of discrete surface, and the feature related to them can accurately describe the shape of facial surface. These characteristics are beneficial to discovering the geometry-based dissimilarity between genuine face and fraud mask. In the experiment part, the baselines of verification and anti-spoofing performance are evaluated on the Morpho database. Furthermore, for simulating a real-world scenario and testing the corresponding anti-spoofing performance, the size of genuine face set is massively extended by uniting the Morpho database and the FRGC v2.0 database to increase the ratio of genuine faces to fraud masks. The evaluation results prove that the proposed 3D face verification system can guarantee competitive verification accuracy for genuine faces and promising anti-spoofing performance against mask attacks.},
keywords={face recognition;feature extraction;mesh generation;transforms;3D facial geometric attributes;anti-spoofing approach;mask attacks;3D scanning;3D printing;3D face recognition;security threat;photometric features;3D meshed face scan;principal curvature measures;mesh SIFT-based feature descriptor;Morpho database;FRGC v2.0 database;3D face verification system;Three-dimensional displays;Face;Face recognition;Databases;Facial features;Shape;Estimation},
doi={10.1109/FG.2017.74},
ISSN={},
month={May},}
@INPROCEEDINGS{8099647,
author={P. {Dou} and S. K. {Shah} and I. A. {Kakadiaris}},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={End-to-End 3D Face Reconstruction with Deep Neural Networks},
year={2017},
volume={},
number={},
pages={1503-1512},
abstract={Monocular 3D facial shape reconstruction from a single 2D facial image has been an active research area due to its wide applications. Inspired by the success of deep neural networks (DNN), we propose a DNN-based approach for End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different from recent works that reconstruct and refine the 3D face in an iterative manner using both an RGB image and an initial 3D facial shape rendering, our DNN model is end-to-end, and thus the complicated 3D rendering process can be avoided. Moreover, we integrate in the DNN architecture two components, namely a multi-task loss function and a fusion convolutional neural network (CNN) to improve facial expression reconstruction. With the multi-task loss function, 3D face reconstruction is divided into neutral 3D facial shape reconstruction and expressive 3D facial shape reconstruction. The neutral 3D facial shape is class-specific. Therefore, higher layer features are useful. In comparison, the expressive 3D facial shape favors lower or intermediate layer features. With the fusion-CNN, features from different intermediate layers are fused and transformed for predicting the 3D expressive facial shape. Through extensive experiments, we demonstrate the superiority of our end-to-end framework in improving the accuracy of 3D face reconstruction.},
keywords={convolution;face recognition;image colour analysis;image fusion;image reconstruction;neural nets;rendering (computer graphics);End-to-End 3D FAce Reconstruction;UH-E2FAR;single 2D image;RGB image;initial 3D facial shape rendering;DNN model;DNN architecture two components;multitask loss function;fusion convolutional neural network;facial expression reconstruction;neutral 3D facial shape reconstruction;expressive 3D facial shape reconstruction;expressive 3D facial shape favors;3D expressive facial shape;end-to-end framework;deep neural networks;monocular 3D facial shape reconstruction;single 2D facial image;3D rendering process;iterative manner;Three-dimensional displays;Shape;Face;Two dimensional displays;Image reconstruction;Solid modeling;Neural networks},
doi={10.1109/CVPR.2017.164},
ISSN={1063-6919},
month={July},}
@INPROCEEDINGS{7924793,
author={ and and },
booktitle={2016 2nd IEEE International Conference on Computer and Communications (ICCC)},
title={Generation of person-specific 3D model based on single photograph},
year={2016},
volume={},
number={},
pages={704-707},
abstract={At present, a person-specific 3D model generation technology has been an active research topic for scientists in the field of computer vision and computer graphics. The paper proposes a 3D modeling method based on single photograph, by analysis of individual difference like geometric features which present the shape and locations of facial components. An extension of the basic Active Appearance Model is proposed to localize some facial feature points in the frontal image and all these feature points are aligned to fit the generic 3D face model to a specialized one to reflect the given person's face, being both more robust and faster. Then an optimized texture mapping of cylindrical projection and model adjustment technologies are presented, to synthesize personal virtual face realistically. This method rectifies these previous defects, such as the difficulty in gaining access to acquiring the accurate data of a 3D face model, slowly manual location methods, incomplete facial information mining. The approach gives a significant improvement in both the reliability and the overall accuracy of 3D modeling.},
keywords={computer graphics;computer vision;data mining;face recognition;image texture;optimisation;stereo image processing;person-specific 3D model;single photograph;computer vision;computer graphics;facial component locations;facial component shape;generic 3D face model;personal virtual face synthesis;incomplete facial information mining;Computational modeling;Deformable models;Analytical models;Robustness;generic face model;active appearance model;texture mapping;model adjustment},
doi={10.1109/CompComm.2016.7924793},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8524581,
author={R. {Reji} and P. {SojanLal}},
booktitle={2017 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)},
title={Region Based 3D Face Recognition},
year={2017},
volume={},
number={},
pages={1-6},
abstract={This paper focuses on a region based methodology for expression in sensitive 3D face recognition process. Considering facial regions that are comparatively unchanging during expressions, results shows that using fifteen sub regions on the face can attain high 3D face recognition. We use a modified face recognition algorithm along with hierarchical contour based image registration for finding the similarity score. Our method operates in two modes: verification mode and confirmation mode. Crop 100 mm of frontal face region, apply preprocessing and automatically detect nose tip, translate the face image to origin and crop fifteen sub regions. The cropped sub regions are defined by cuboids which occupy more volumetric data, Nose Tip is the most projecting point of the face with the highest value along Z-axis so consider it as origin. The modified face recognition algorithm reduces the effects caused by facial expressions and artifacts. Finally a Hierarchical contour based image registration technique is applied which yields better results. The approach is applied on Bosphorus 3D datasets and achieved a verification rate of 95.3% at 0.1% false acceptance rate. In the identification scenario 99.3% rank one recognition is achieved.},
keywords={face recognition;image registration;frontal face region;nose tip;face image;facial expressions;Hierarchical contour based image registration technique;Bosphorus 3D datasets;face recognition algorithm;size 100.0 mm;Face;Nose;Face recognition;Three-dimensional displays;Probes;Image registration;Biometrics;MFRA;Rank based Score;Contour based image registration;3D face recognition},
doi={10.1109/ICCIC.2017.8524581},
ISSN={2473-943X},
month={Dec},}
@INPROCEEDINGS{8578635,
author={S. {Cheng} and I. {Kotsia} and M. {Pantic} and S. {Zafeiriou}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications},
year={2018},
volume={},
number={},
pages={5117-5126},
abstract={The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.},
keywords={biometrics (access control);computer vision;emotion recognition;face recognition;image capture;image resolution;stereo image processing;visual databases;high-resolution 3D faces;4DFAB;facial behaviour;facial expression recognition;behavioural biometrics;computer vision applications;automatic face analysis;scale visual databases;face recognition;Databases;Three-dimensional displays;Face;Face recognition;Cameras;Two dimensional displays;Task analysis},
doi={10.1109/CVPR.2018.00537},
ISSN={2575-7075},
month={June},}
@INPROCEEDINGS{7110239,
author={A. {Lagorio} and M. {Cadoni} and E. {Grosso} and M. {Tistarelli}},
booktitle={3rd International Workshop on Biometrics and Forensics (IWBF 2015)},
title={A 3D algorithm for unsupervised face identification},
year={2015},
volume={},
number={},
pages={1-7},
abstract={With the increasing availability of low-cost 3D data acquisition devices, the use of 3D face data for the recognition of individuals is becoming more appealing and computationally feasible. This paper proposes a completely automatic algorithm for face registration and matching. The algorithm is based on the extraction of stable 3D facial features characterizing the face and the subsequent construction of a signature manifold. The facial features are extracted by performing a continuous-to-discrete scale-space analysis. Registration is driven from the matching of triplets of feature points and the registration error is computed as shape matching score. Conversely to most techniques in the literature, a major advantage of the proposed method is that no data pre-processing is required. Therefore all presented results have been obtained exclusively from the raw data available from the 3D acquisition device. The method has been tested on the Bosphorus 3D face database and the performances compared to the ICP baseline algorithm. Even in presence of noise in the data, the algorithm proved to be very robust and reported identification performances which are aligned to the current state of the art, but without requiring any pre-processing of the raw data.},
keywords={data acquisition;face recognition;feature extraction;image matching;image registration;3D algorithm;unsupervised face identification;low-cost 3D data acquisition device;automatic algorithm;face registration;face matching;3D facial feature;signature manifold;facial feature extraction;continuous-to-discrete scale-space analysis;registration error;shape matching score;3D acquisition device;Bosphorus 3D face database;ICP baseline algorithm;Face;Three-dimensional displays;Feature extraction;Databases;Iterative closest point algorithm;Face recognition;Noise;Face recognition;3D Face recognition},
doi={10.1109/IWBF.2015.7110239},
ISSN={},
month={March},}
@INPROCEEDINGS{8470411,
author={K. {Dutta} and D. {Bhattacharjee} and M. {Nasipuri}},
booktitle={2018 Fifth International Conference on Emerging Applications of Information Technology (EAIT)},
title={TR-LBP: A modified Local Binary Pattem-based technique for 3D face recognition},
year={2018},
volume={},
number={},
pages={1-4},
abstract={In this paper, a novel technique has been introduced for 3D face recognition based on the modified local binary pattern extracted from a 3D range image. The new LBP technique is applied to shape index of 3D facial surface data. The novelty of this technique illustrates that a modified local binary pattern termed as Triangular Local Binary Pattern (TR-LBP), which gives new texture representation of 3D facial surface for improvement of facial texture classification performance compared to other variants of LBP. In this paper, authors have also described the TR-LBP technique by extending it on 2D intensity image of same subjects. Entropy-based feature extraction is used for feature vector creation. Further, KNN is used for calculating classification accuracy on two popular 3D face databases: Frav3D and Bosphorous. Here the classifications results are compared with other two existing LBP techniques applied to range images and shape index (SI) form of range image respectively.},
keywords={entropy;face recognition;feature extraction;image classification;image representation;image texture;nearest neighbour methods;3D facial surface data;Triangular Local Binary Pattern;facial texture classification performance;TR-LBP technique;entropy-based feature extraction;3D face recognition;3D range image;3D face databases;modified local binary pattem-based technique;shape index;texture representation;2D intensity image;feature vector creation;KNN;Frav3D;Bosphorous;Three-dimensional displays;Shape;Face;Indexes;Face recognition;Two dimensional displays;Entropy;Range Image;Shape Index;TR-LBP;2D Intensity Image;Entropy-based feature.},
doi={10.1109/EAIT.2018.8470411},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7853992,
author={R. {Amin} and A. F. {Shams} and S. M. M. {Rahman} and D. {Hatzinakos}},
booktitle={2016 9th International Conference on Electrical and Computer Engineering (ICECE)},
title={Evaluation of discrimination power of facial parts from 3D point cloud data},
year={2016},
volume={},
number={},
pages={602-605},
abstract={Feature selection from facial regions is a well-known approach to increase the performance of 2D image-based face recognition systems. In case of 3D modality, the approach of region-based feature selection for face recognition is relatively new. In this context, this paper presents an approach to evaluate the discrimination power of different regions of a 3D facial surface for its potential use in face recognition systems. We propose the use of weighted average of unit normal vector on the facial surface as the feature for region-based face recognition from 3D point cloud data (PCD). The iterative closest point algorithm is employed for the registration of segmented regions of facial point clouds. A metric based on angular distance between normals is introduced to indicate the similarity between two surfaces of same facial region. Finally, the intra class correlation based discrimination score is formulated to find out the key facial regions such as the eyes, nose, and mouth that are significant while recognizing a person with facial surface PCD.},
keywords={computational geometry;correlation methods;face recognition;feature selection;image registration;image segmentation;discrimination power evaluation;facial parts;2D image-based face recognition systems;3D modality;region-based feature selection;3D facial surface;3D point cloud data;3D PCD;iterative closest point algorithm;segmented region registration;angular distance;intra class correlation;discrimination score;Three-dimensional displays;Face;Face recognition;Measurement;Two dimensional displays;Nose;Databases},
doi={10.1109/ICECE.2016.7853992},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7984642,
author={ and and and and },
booktitle={2017 2nd International Conference on Image, Vision and Computing (ICIVC)},
title={Design and implementation of high resolution face image acquisition system under low illumination based on the open source computer vision library},
year={2017},
volume={},
number={},
pages={680-683},
abstract={Super-resolution face image acquisition system is indispensable in people's life. Under the condition of low illumination, the illumination environment difference is too big or the light is insufficient, which leads to the traditional image acquisition system can not collect the high quality face image, and the limitation is poor. Based on open source computer vision library (OpenCV) in the C++ environment configuration, the use of Three Dimension (3D) face recognition technology algorithm, design a set of low illumination conditions of the super resolution face image acquisition system. Experiments show that the design scheme with real-time focusing speed), fast (single acquisition 0.05 seconds), accurate (facial recognition rate of 99.3%) etc. characteristics, be able to fully meet the needs of low illumination conditions for super-resolution of face image acquisition.},
keywords={computer vision;face recognition;image capture;image resolution;high resolution face image acquisition system;open source computer vision library;superresolution face image acquisition system;illumination environment difference;high quality face image;OpenCV;C++ environment configuration;3D face recognition technology algorithm;low illumination condition;real-time focusing speed;facial recognition rate;Image recognition;Face recognition;Image resolution;Face;Three-dimensional displays;Libraries;Lead;facial recognition technology 3D algorithm;OpenCV;C++ environment},
doi={10.1109/ICIVC.2017.7984642},
ISSN={},
month={June},}
@INPROCEEDINGS{7877838,
author={S. {An} and Q. {Ruan}},
booktitle={2016 IEEE 13th International Conference on Signal Processing (ICSP)},
title={3D facial expression recognition algorithm using local threshold binary pattern and histogram of oriented gradient},
year={2016},
volume={},
number={},
pages={265-270},
abstract={Facial expression which carries rich information of body behavior is the leading carrier of human affective and the symbol of intelligence. The main purpose of this paper is to recognize 3D human facial expression. The research in this paper includes the expression feature extraction algorithm and fusion with different kinds of feature. To contain more local texture feature information, we proposed a new feature of 3D facial expression named Local Threshold Binary Pattern (LTBP) which based on Local Binary Pattern (LBP). We calculate the difference of gray value standard between neighboring pixels and the center pixel as a threshold to binary instead of the traditional LBP operation which only comparison of size between neighboring pixels and the center pixel. After we get the LTBP feature, we fuse the LTBP and HOG (Histogram of Oriented Gradient) features to get multi-feature fusion for 3D facial expression recognition. Our algorithm of 3D facial expression recognition comprises three steps: (1) extracting two sets of feature vectors and establishing the correlation criterion function between the two sets of feature vectors; (2) solving the two sets canonical projective vectors and extracting their canonical correlation features by the framework of canonical correlation analysis algorithm; (3) doing feature fusion for classification by using proposed strategy. We have performed comprehensive experiments on the BU-3DFE database which is presently the largest available 3D face database. We have achieved verification rates of more than 90% for the 3D facial expression recognition.},
keywords={correlation methods;emotion recognition;face recognition;feature extraction;image classification;image fusion;image resolution;image texture;vectors;3D facial expression recognition algorithm;local threshold binary pattern;histogram-of-oriented gradient;body behavior;expression feature extraction algorithm;expression feature fusion algorithm;local texture feature information;LTBP;gray value standard;neighboring pixels;center pixel;LTBP feature fusion;HOG feature fusion;multifeature fusion;feature vectors;correlation criterion function;canonical projective vectors;canonical correlation analysis algorithm;canonical correlation feature extraction;BU-3DFE database;3D face database;Feature extraction;Face recognition;Three-dimensional displays;Correlation;Histograms;Algorithm design and analysis;Standards;Local Threshold Binary Pattern (LTBP);multi-feature fusion;3D expression recognition;canonical correlation analysis},
doi={10.1109/ICSP.2016.7877838},
ISSN={2164-5221},
month={Nov},}
@INPROCEEDINGS{7351482,
author={S. {Lv} and F. {Da} and X. {Deng}},
booktitle={2015 IEEE International Conference on Image Processing (ICIP)},
title={A 3D face recognition method using region-based extended local binary pattern},
year={2015},
volume={},
number={},
pages={3635-3639},
abstract={A 3D face recognition method using region-based extended local binary pattern (eLBP) is proposed. First, the depth image converted from the preprocessed 3D pointclouds is normalized. Then, different regions according to their distortions under facial expressions are extracted by binary masks and represented by the uniform pattern of extended LBP. Finally, sparse representation classifier (SRC) is adopted for classification on the single region. Feature-level and score-level fusion with weight-sparse representation classifier (W-SRC) are also tested and compared, and the latter has better performance. The experiments on FRGC v2.0 database demonstrate that the proposed method is robust and efficient.},
keywords={computer graphics;face recognition;3D face recognition method;region-based extended local binary pattern;eLBP;depth image;preprocessed 3D pointclouds;facial expressions;binary masks;uniform pattern;extended LBP;feature-level fusion;score-level fusion;weight-sparse representation classifier;W-SRC;FRGC v2.0 database;Face;Three-dimensional displays;Feature extraction;Face recognition;Mathematical model;Nose;Mouth;3D face recognition;extended local binary pattern;depth image;binary mask;weight-sparse representation classifier},
doi={10.1109/ICIP.2015.7351482},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7785119,
author={H. X. {Pham} and V. {Pavlovic}},
booktitle={2016 Fourth International Conference on 3D Vision (3DV)},
title={Robust Real-Time 3D Face Tracking from RGBD Videos under Extreme Pose, Depth, and Expression Variation},
year={2016},
volume={},
number={},
pages={441-449},
abstract={We introduce a novel end-to-end real-time pose-robust 3D face tracking framework from RGBD videos, which is capable of tracking head pose and facial actions simultaneously in unconstrained environment without intervention or pre-calibration from a user. In particular, we emphasize tracking the head pose from profile to profile and improving tracking performance in challenging instances, where the tracked subject is at a considerably large distance from the camera and the quality of data deteriorates severely. To achieve these goals, the tracker is guided by an efficient multi-view 3D shape regressor, trained upon generic RGB datasets, which is able to predict model parameters despite large head rotations or tracking range. Specifically, the shape regressor is made aware of the head pose by inferring the possibility of particular facial landmarks being visible through a joint regression-classification local random forest framework, and piecewise linear regression models effectively map visibility features into shape parameters. In addition, the regressor is combined with a joint 2D+3D optimization that sparsely exploits depth information to further refine shape parameters to maintain tracking accuracy over time. The result is a robust on-line RGBD 3D face tracker that can model extreme head poses and facial expressions accurately in challenging scenes, which are demonstrated in our extensive experiments.},
keywords={cameras;emotion recognition;face recognition;image classification;learning (artificial intelligence);object tracking;optimisation;regression analysis;video signal processing;RGBD videos;extreme pose;depth information;facial expressions;robust online RGBD 3D face tracker;joint 2D+3D optimization;shape parameters;map visibility features;piecewise linear regression models;joint regression-classification local random forest framework;facial landmarks;tracking range;head rotations;model parameter prediction;generic RGB datasets;multiview 3D shape regressor;camera;unconstrained environment;facial actions;head pose tracking;end-to-end real-time pose-robust 3D face tracking framework;expression variation;Three-dimensional displays;Face;Shape;Solid modeling;Robustness;Tracking;Two dimensional displays;3D face tracking;blendshape;multi-view;pose-robust},
doi={10.1109/3DV.2016.54},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7835606,
author={F. {Majid Zadeh Heravi} and M. {Eslahi} and E. {Farazdaghi} and A. {Nait-Ali}},
booktitle={2016 International Conference on Bio-engineering for Smart Technologies (BioSMART)},
title={A Morphable Model to simulate rejuvenation trajectory of 3D face images: Preliminary results},
year={2016},
volume={},
number={},
pages={1-4},
abstract={Facial Aging (FA) process as a natural phenomenon has been of great interest to many researchers as well as for studies in the field of gerontology and private sector firms like airports and police departments. As opposed to previous studies which focused on predicting the facial appearance changes of the individuals, this paper details the backward aging trajectory of face images in 3 dimensional environment. This model is learned from a database of three dimensional facial images which is built by five individual age groups between 3 to 35 years old. 3D facial aging modeling is a complex process since it affects both the shape and texture of the face. We propose the Statistical Rejuvenation Model (SRM), a morphable face model to transform the given adult face to its youthful appearance by changing its shape and texture simultaneously.},
keywords={face recognition;image texture;solid modelling;statistical analysis;morphable face model;rejuvenation trajectory simulation;3D face images;facial aging process;gerontology;private sector;airports;police departments;backward aging trajectory;3D facial aging modeling;statistical rejuvenation model;SRM;Face;Three-dimensional displays;Solid modeling;Computational modeling;Aging;Principal component analysis;Geometry;UV Texture Map;Textural model;Geometrical model;Wireframe;Mesh;SRM;3D Facial Modeling},
doi={10.1109/BIOSMART.2016.7835606},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7899971,
author={Q. {Zhen} and D. {Huang} and Y. {Wang} and H. {Drira} and B. B. {Amor} and M. {Daoudi}},
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)},
title={Magnifying subtle facial motions for 4D Expression Recognition},
year={2016},
volume={},
number={},
pages={2252-2257},
abstract={In this paper, we propose an effective approach for automatic 4D Facial Expression Recognition (FER). The flow of 3D facial scans is first modeled to capture spatial deformations based on the recently-developed Riemannian approach, namely Dense Scalar Fields (DSF), where registration and comparison of neighboring 3D face frames are jointly led. The deformations are then fed into a temporal filtering based magnification step to amplify the slight facial actions over time. The proposed method allows revealing subtle (hidden) deformations which enhances the performance in classification. We evaluate our approach on the BU-4DFE dataset, and the state-of-art accuracy up to 94.18% is achieved, which is superior to the top one so far reported, clearly demonstrating its effectiveness.},
keywords={emotion recognition;face recognition;image filtering;image registration;automatic 4D facial expression recognition;FER;3D facial scans;dense scalar fields;DSF;3D face frames;temporal filtering based magnification step;BU-4DFE dataset;Riemannian approach;spatial deformations;Three-dimensional displays;Face;Shape;Hidden Markov models;Electronic mail;Geometry;Measurement},
doi={10.1109/ICPR.2016.7899971},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7577570,
author={S. G. {Gunanto} and M. {Hariadi} and E. M. {Yuniarno}},
booktitle={2016 4th International Conference on Cyber and IT Service Management},
title={Feature-points nearest neighbor clustering on 3D face models},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Defining motion area on the face of 3D virtual character starts with the mapping of skeleton movement. Every animated character requires special handling based on the characteristics of the size and location of the bone to support producing facial expressions correctly. This process is often done specifically for each face model to be used. This research tried to use a marker-based motion capture data as a reference for the automation of generating clusters adaptively in the face of 3D characters. Each vertex which forming expression on the faces of the 3D models selected as centroids of cluster and representation a motion area whose numbers will correspond with the number of feature-point markers of motion capture data. Clustering process is done with the synthesis of modified nearest neighbor approach with the feature-point value. The results obtained were able to demonstrate a clustering process for generating motion area in a variety of 3D face model.},
keywords={face recognition;image capture;image motion analysis;image representation;pattern clustering;feature-point nearest neighbor clustering;3D virtual character;skeleton movement;animated character;facial expressions;marker-based motion capture data;motion area;feature-point markers;feature-point value;3D face model;Face;Three-dimensional displays;Solid modeling;Adaptation models;Data models;Training data;Automation;feature-point;clustering;3D face models},
doi={10.1109/CITSM.2016.7577570},
ISSN={},
month={April},}
@INPROCEEDINGS{8122665,
author={S. {Soltanpour} and Q. M. J. {Wu}},
booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
title={Multiscale depth local derivative pattern for sparse representation based 3D face recognition},
year={2017},
volume={},
number={},
pages={560-565},
abstract={3D face recognition is a popular research area due to its vast application in biometrics and security. Local feature-based methods gain importance in the recent years due to their robustness under degradation conditions. In this paper, a novel high-order local pattern descriptor in combination with sparse representation based classifier (SRC) is proposed for expression robust 3D face recognition. 3D point clouds are converted to depth maps after preprocessing. Multi-directional derivatives are applied in spatial space to encode the depth maps based on the local derivative pattern (LDP) scheme. Directional pattern features are calculated according to local derivative variations. Since LDP computes spatial relationship of neighbors in a local region, it extracts distinct information from the depth map. Multiscale depth-LDP is presented as a novel descriptor for 3D face recognition. The descriptor is employed along with the SRC to increase the range data distinctiveness. A histogram on the derivative pattern creates a spatial feature descriptor that represents the distinctive micro-patterns from 3D data. We evaluate the proposed algorithm on two famous 3D face databases, FRGC v2.0 and Bosphorus. The experimental results demonstrate that the proposed approach achieves acceptable performance under facial expression.},
keywords={emotion recognition;face recognition;feature extraction;image representation;expression robust 3D face recognition;3D point clouds;multidirectional derivatives;local derivative pattern scheme;directional pattern features;local derivative variations;multiscale depth-LDP;spatial feature descriptor;distinctive micropatterns;multiscale depth local derivative pattern;sparse representation-based classifier;SRC;3D face databases;FRGC v2.0;Bosphorus;Three-dimensional displays;Face;Face recognition;Feature extraction;Databases;Two dimensional displays;Shape},
doi={10.1109/SMC.2017.8122665},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8237378,
author={A. {Bulat} and G. {Tzimiropoulos}},
booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
title={How Far are We from Solving the 2D 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)},
year={2017},
volume={},
number={},
pages={1021-1030},
abstract={This paper investigates how far a very deep neural network is from attaining close to saturating performance on existing 2D and 3D face alignment datasets. To this end, we make the following 5 contributions: (a) we construct, for the first time, a very strong baseline by combining a state-of-the-art architecture for landmark localization with a state-of-the-art residual block, train it on a very large yet synthetically expanded 2D facial landmark dataset and finally evaluate it on all other 2D facial landmark datasets. (b)We create a guided by 2D landmarks network which converts 2D landmark annotations to 3D and unifies all existing datasets, leading to the creation of LS3D-W, the largest and most challenging 3D facial landmark dataset to date (~230,000 images). (c) Following that, we train a neural network for 3D face alignment and evaluate it on the newly introduced LS3D-W. (d) We further look into the effect of all “traditional” factors affecting face alignment performance like large pose, initialization and resolution, and introduce a “new” one, namely the size of the network. (e) We show that both 2D and 3D face alignment networks achieve performance of remarkable accuracy which is probably close to saturating the datasets used. Training and testing code as well as the dataset can be downloaded from https://www.adrianbulat.com/face-alignment/.},
keywords={face recognition;neural nets;stereo image processing;deep neural network;saturating performance;3D face alignment datasets;state-of-the-art architecture;landmark localization;state-of-the-art residual block;2D facial landmark datasets;2D landmarks network;2D landmark annotations;face alignment performance;3D face alignment networks;alignment problem;3D facial landmarks;LS3D-W;3D facial landmark dataset;Face;Three-dimensional displays;Two dimensional displays;Training;Testing;Videos;Computer vision},
doi={10.1109/ICCV.2017.116},
ISSN={2380-7504},
month={Oct},}
@INPROCEEDINGS{8546220,
author={H. {Zhang} and Q. {Li} and Z. {Sun}},
booktitle={2018 24th International Conference on Pattern Recognition (ICPR)},
title={Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark Localization},
year={2018},
volume={},
number={},
pages={2202-2208},
abstract={3D face shape is more expressive and viewpoint-consistent than its 2D counterpart. However, 3D facial landmark localization in a single image is challenging due to the ambiguous nature of landmarks under 3D perspective. Existing approaches typically adopt a suboptimal two-step strategy, performing 2D landmark localization followed by depth estimation. In this paper, we propose the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial landmark localization, addressing it more effectively in an end-to-end fashion. First, a compact volumetric representation is proposed to encode the per-voxel likelihood of positions being the 3D landmarks. The dimensionality of such a representation is fixed regardless of the number of target landmarks, so that the curse of dimensionality could be avoided. Then, a stacked hourglass network is adopted to estimate the volumetric representation from coarse to fine, followed by a 3D convolution network that takes the estimated volume as input and regresses 3D coordinates of the face shape. In this way, the 3D structural constraints between landmarks could be learned by the neural network in a more efficient manner. Moreover, the proposed pipeline enables end-to-end training and improves the robustness and accuracy of 3D facial landmark localization. The effectiveness of our approach is validated on the 3DFAW and AFLW2000-3D datasets. Experimental results show that the proposed method achieves state-of-the-art performance in comparison with existing methods.},
keywords={face recognition;image representation;learning (artificial intelligence);neural nets;pose estimation;regression analysis;joint voxel and coordinate regression method;3D face shape;3D facial landmark localization;suboptimal two-step strategy;per-voxel likelihood encoding;stacked hourglass network;compact volumetric representation estimation;3DFAW dataset;depth estimation;curse of dimensionality;AFLW2000-3D datasets;3D structural constraints;3D convolution network;target landmarks;2D landmark localization;accurate 3D facial landmark localization;Three-dimensional displays;Two dimensional displays;Face;Shape;Heating systems;Convolution;Pose estimation},
doi={10.1109/ICPR.2018.8546220},
ISSN={1051-4651},
month={Aug},}
@INPROCEEDINGS{8373857,
author={J. {Deng} and Y. {Zhou} and S. {Cheng} and S. {Zaferiou}},
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
title={Cascade Multi-View Hourglass Model for Robust 3D Face Alignment},
year={2018},
volume={},
number={},
pages={399-403},
abstract={Estimating the 3D facial landmarks from a 2D image remains a challenging problem. Even though state-of-the-art 2D alignment methods are able to predict accurate landmarks for semi-frontal faces, the majority of them fail to provide semantically consistent landmarks for profile faces. A de facto solution to this problem is through 3D face alignment that preserves correspondence across different poses. In this paper, we proposed a Cascade Multi-view Hourglass Model for 3D face alignment, where the first Hourglass model is explored to jointly predict semi-frontal and profile 2D facial landmarks, after removing spatial transformations, another Hourglass model is employed to estimate the 3D facial shapes. To improve the capacity without sacrificing the computational complexity, the original residual bottleneck block in the Hourglass model is replaced by a parallel, multi-scale inception-resnet block. Extensive experiments on two challenging 3D face alignment datasets, AFLW2000-3D and Menpo-3D, show the robustness of the proposed method under continuous pose changes.},
keywords={face recognition;pose estimation;3D facial landmarks;semifrontal faces;semantically consistent landmarks;profile 2D facial landmarks;3D facial shapes;multiscale inception-resnet block;AFLW2000-3D;Menpo-3D;cascade multiview hourglass model;robust 3D face alignment dataset;de facto solution;computational complexity;Three-dimensional displays;Face;Two dimensional displays;Solid modeling;Computational modeling;Shape;Training;Multiview;3D Face Alignment;Cascade Hourglass},
doi={10.1109/FG.2018.00064},
ISSN={},
month={May},}
@INPROCEEDINGS{7139111,
author={Y. {Tang} and X. {Sun} and D. {Huang} and J. {Morvan} and Y. {Wang} and L. {Chen}},
booktitle={2015 International Conference on Biometrics (ICB)},
title={3D face recognition with asymptotic cones based principal curvatures},
year={2015},
volume={},
number={},
pages={466-472},
abstract={The classical curvatures of smooth surfaces (Gaussian, mean and principal curvatures) have been widely used in 3D face recognition (FR). However, facial surfaces resulting from 3D sensors are discrete meshes. In this paper, we present a general framework and define three principal curvatures on discrete surfaces for the purpose of 3D FR. These principal curvatures are derived from the construction of asymptotic cones associated to any Borel subset of the discrete surface. They describe the local geometry of the underlying mesh. First two of them correspond to the classical principal curvatures in the smooth case. We isolate the third principal curvature that carries out meaningful geometric shape information. The three principal curvatures in different Borel subsets scales give multi-scale local facial surface descriptors. We combine the proposed principal curvatures with the LNP-based facial descriptor and SRC for recognition. The identification and verification experiments demonstrate the practicability and accuracy of the third principal curvature and the fusion of multi-scale Borel subset descriptors on 3D face from FRGC v2.0.},
keywords={face recognition;geometry;set theory;3D face recognition;asymptotic cones;principal curvatures;smooth surfaces;facial surfaces;3D sensors;geometric shape information;Borel subsets;Face;Three-dimensional displays;Face recognition;Histograms;Shape;Eigenvalues and eigenfunctions;Iris recognition},
doi={10.1109/ICB.2015.7139111},
ISSN={2376-4201},
month={May},}
@INPROCEEDINGS{7761526,
author={S. A. A. {Shah} and M. {Bennamoun} and F. {Boussaid}},
booktitle={2015 International Conference on Image and Vision Computing New Zealand (IVCNZ)},
title={Automatic 3D face landmark localization based on 3D vector field analysis},
year={2015},
volume={},
number={},
pages={1-6},
abstract={In applications such as 3D face synthesis and animation, a prominent face landmark is required to enable 3D face normalization, pose correction, 3D face recognition and reconstruction. Due to variations in facial expressions, automatic 3D face landmark localization remains a challenge. Nose tip is one of the salient landmarks in a human face. In this paper, a novel nose tip localization technique is proposed. In the proposed approach, the rotation of the 3D vector field is analyzed for robust and efficient nose tip localization. The proposed technique has the following three characteristics: (1) it does not require any training; (2) it does not rely on any particular model; (3) it is very efficient, requiring an average time of only 1.9s for nose tip detection. We tested the proposed technique on BU3DFE and Shrec'10 datasets. Experimental results show that the proposed technique is robust to variations in facial expressions, achieving a 100% detection rate on these publicly available 3D face datasets.},
keywords={computer animation;face recognition;image reconstruction;automatic 3D face landmark localization;3D vector field analysis;3D face synthesis;animation;3D face normalization;pose correction;3D face recognition;3D face reconstruction;facial expressions;nose tip localization;nose tip detection;3D face datasets;Nose;Three-dimensional displays;Face;Shape;Support vector machines;Two dimensional displays;Training},
doi={10.1109/IVCNZ.2015.7761526},
ISSN={2151-2205},
month={Nov},}
@INPROCEEDINGS{8373915,
author={W. {Tian} and F. {Liu} and Q. {Zhao}},
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
title={Landmark-Based 3D Face Reconstruction from an Arbitrary Number of Unconstrained Images},
year={2018},
volume={},
number={},
pages={774-779},
abstract={In this paper, we propose a novel method for reconstructing 3D faces from 2D images. The method is characterized in three aspects. (i) It utilizes only geometric cues in the input images, i.e., 2D facial landmarks. (ii) It works for an arbitrary number of unconstrained images, both single and multiple images. (iii) It can effectively exploit complementary information in multiple images of varying poses and expressions. The method is implemented based on cascaded regression in shape space. We have evaluated the method on three databases and observed from the experimental results that (i) the reconstruction error is reduced as more images of different poses are used, (ii) the proposed method can obtain comparable reconstruction results by using state-of-the-art automated methods to detect the 2D landmarks, and (iii) the proposed method is robust to variations in facial expressions and image qualities.},
keywords={face recognition;image reconstruction;regression analysis;stereo image processing;cascaded regression;image qualities;facial expressions;reconstruction error;2D facial landmarks;geometric cues;unconstrained images;3D face reconstruction;Conferences;Face;Gesture recognition;3D face reconstruction;unconstrained images;landmark based;cascade regression},
doi={10.1109/FG.2018.00122},
ISSN={},
month={May},}
@ARTICLE{7373633,
author={N. {Werghi} and C. {Tortorici} and S. {Berretti} and A. {Del Bimbo}},
journal={IEEE Transactions on Information Forensics and Security},
title={Boosting 3D LBP-Based Face Recognition by Fusing Shape and Texture Descriptors on the Mesh},
year={2016},
volume={11},
number={5},
pages={964-979},
abstract={In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBPs) on a mesh for 3D face recognition. Using a recently proposed framework, we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared with its depth-image counterpart, our approach is distinguished by the following features: 1) inherits the intrinsic advantages of mesh surface (e.g., preservation of the full geometry); 2) does not require normalization; and 3) can accommodate partial matching. In addition, it allows early level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data, also in comparison to the state-of-the-art solutions.},
keywords={face recognition;feature extraction;image fusion;image matching;image texture;3D LBP-based face recognition;local binary patterns;shape descriptor;texture descriptor;LBP fusion;face mesh surface;facial surface;depth-image counterpart;partial matching;BU-3DFE database;Bosphorus database;Face;Three-dimensional displays;Face recognition;Shape;Geometry;Histograms;Boosting;mesh-LBP;feature and score fusion;3D face recognition;Mesh-LBP;feature and score fusion;3D face recognition},
doi={10.1109/TIFS.2016.2515505},
ISSN={1556-6013},
month={May},}
@INPROCEEDINGS{8100063,
author={J. {Booth} and E. {Antonakos} and S. {Ploumpis} and G. {Trigeorgis} and Y. {Panagakis} and S. {Zafeiriou}},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={3D Face Morphable Models "In-the-Wild"},
year={2017},
volume={},
number={},
pages={5464-5473},
abstract={3D Morphable Models (3DMMs) are powerful statistical models of 3D facial shape and texture, and among the state-of-the-art methods for reconstructing facial shape from single images. With the advent of new 3D sensors, many 3D facial datasets have been collected containing both neutral as well as expressive faces. However, all datasets are captured under controlled conditions. Thus, even though powerful 3D facial shape models can be learnt from such data, it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions (in-the-wild). In this paper, we propose the first, to the best of our knowledge, in-the-wild 3DMM by combining a powerful statistical model of facial shape, which describes both identity and expression, with an in-the-wild texture model. We show that the employment of such an in-the-wild texture model greatly simplifies the fitting procedure, because there is no need to optimise with regards to the illumination parameters. Furthermore, we propose a new fast algorithm for fitting the 3DMM in arbitrary images. Finally, we have captured the first 3D facial database with relatively unconstrained conditions and report quantitative evaluations with state-of-the-art performance. Complementary qualitative reconstruction results are demonstrated on standard in-the-wild facial databases.},
keywords={emotion recognition;face recognition;image morphing;image reconstruction;image sampling;image texture;solid modelling;statistical analysis;facial shape;state-of-the-art methods;3D facial datasets;expressive faces;statistical texture models;in-the-wild 3DMM;in-the-wild texture model;3D facial database;in-the-wild facial databases;3D face morphable models;Three-dimensional displays;Shape;Solid modeling;Cameras;Face;Image reconstruction;Data models},
doi={10.1109/CVPR.2017.580},
ISSN={1063-6919},
month={July},}
@ARTICLE{7792560,
author={P. {Huber} and P. {Kopp} and W. {Christmas} and M. {Rätsch} and J. {Kittler}},
journal={IEEE Signal Processing Letters},
title={Real-Time 3D Face Fitting and Texture Fusion on In-the-Wild Videos},
year={2017},
volume={24},
number={4},
pages={437-441},
abstract={We present a fully automatic approach to real-time 3D face reconstruction from monocular in-the-wild videos. With the use of a cascaded-regressor-based face tracking and a 3D morphable face model shape fitting, we obtain a semidense 3D face shape. We further use the texture information from multiple frames to build a holistic 3D face representation from the video footage. Our system is able to capture facial expressions and does not require any person-specific training. We demonstrate the robustness of our approach on the challenging 300 Videos in the Wild (300-VW) dataset. Our real-time fitting framework is available as an open-source library at http://4dface.org.},
keywords={face recognition;image fusion;image reconstruction;image representation;image texture;video signal processing;real-time 3D face fitting;texture fusion;real-time 3D face reconstruction;monocular in-the-wild videos;cascaded-regressor-based face tracking;3D morphable face model shape fitting;semidense 3D face shape;texture information;holistic 3D face representation;video footage;person-specific training;open-source library;Face;Three-dimensional displays;Shape;Solid modeling;Two dimensional displays;Videos;Cameras;Face model fitting;3D Morphable Face Model;3D face reconstruction;open source software;real-time},
doi={10.1109/LSP.2016.2643284},
ISSN={1070-9908},
month={April},}
@INPROCEEDINGS{7797090,
author={S. Z. {Gilani} and A. {Mian}},
booktitle={2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
title={Towards Large-Scale 3D Face Recognition},
year={2016},
volume={},
number={},
pages={1-8},
abstract={3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases.},
keywords={convolution;face recognition;feature extraction;image matching;learning (artificial intelligence);solid modelling;large-scale 3D face recognition;fully convolutional deep network;FCDN;seed points extraction;level set curves;single curvature dependent adaptive speed;dense correspondence;training set;surface matching;morphable model fitting;landmark localization results;FRGCv2 dataset;Bosphorus dataset;query face recognition;Three-dimensional displays;Face;Face recognition;Solid modeling;Databases;Two dimensional displays;Robustness},
doi={10.1109/DICTA.2016.7797090},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7471958,
author={T. {Wu} and F. {Zhou} and Q. {Liao}},
booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A fast 3D face reconstruction method from a single image using adjustable model},
year={2016},
volume={},
number={},
pages={1656-1660},
abstract={In this paper, we propose a fast and robust method which uses only a single frontal face image as input to reconstruct a plausible 3D face. Our method mainly consists of three stages: feature point detection, model adaptation in X-Y plane and model adjustment on Z-axis direction. At first stage, we detect some face regions such as face contour and facial components automatically. In these regions, we extract several feature points which can generally describe the structure of face. Subsequently, we apply several deformation processes and optimization procedures on an adjustable 3D face model in the X-Y plane based on these feature points. Finally, we present a method of insertion to obtain a dense and smooth model. Experimental results demonstrate the effectiveness and efficiency of our method as well as the robust adaptation to the complex imaging condition.},
keywords={face recognition;image reconstruction;optimisation;fast 3D face reconstruction method;adjustable model;single frontal face image;feature point detection;Z-axis direction;X-Y plane;deformation processes;optimization procedures;Face;Nose;Mouth;Image reconstruction;Three-dimensional displays;Feature extraction;Adaptation models;3D face reconstruction;adjustable model;model adaption;insertion},
doi={10.1109/ICASSP.2016.7471958},
ISSN={2379-190X},
month={March},}
@INPROCEEDINGS{7821896,
author={ and and S. G. {Gunanto}},
booktitle={2016 6th International Annual Engineering Seminar (InAES)},
title={2D to 3D space transformation for facial animation based on marker data},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Computer facial animation aims to create an animated character expression as natural as possible as well as human facial expressions. Using the data marker catches facial motion capture, will be determined the location of the feature points of 3D face models to follow the motion of the marker points of human faces. To overcome the morphological differences between the face of the source with the character's face, then applied with radial basis retargeting process mapping so that the character's face can still display the natural expression. Using the data marker 2D, Radial Basis Function (RBF) transformation was applied to determine the position of the feature points on the 3D face models. RBF space transformation has good ability in determining the appropriate facial motion marker points on a human face to the character's face. Motion that occurs in 3D face models is scaled according to the relative scale between the source and the target.},
keywords={computer animation;face recognition;feature extraction;image capture;motion estimation;radial basis function networks;transforms;3D space transformation;2D space transformation;computer facial animation;animated character expression;human facial expressions;data marker;facial motion capture;3D face models;morphological differences;character face;radial basis retargeting process mapping;natural expression;radial basis function transformation;RBF space transformation;Face;Three-dimensional displays;Solid modeling;Two dimensional displays;Facial animation;Data models;facial animation;radial basis function;marker data},
doi={10.1109/INAES.2016.7821896},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8035340,
author={L. {Han} and Q. {Xiao} and X. {Liang}},
booktitle={2017 International Conference on Computer, Information and Telecommunication Systems (CITS)},
title={3D face reconstruction based on progressive cascade regression},
year={2017},
volume={},
number={},
pages={297-301},
abstract={In order to better learn the distributions of 2D and 3D faces and the mapping between them with limited training samples, a new 3D face reconstruction method based on progressive cascade regression is proposed. Firstly, it learns the mapping between 2D and 3D facial landmarks to estimate the initial 3D facial landmarks with a coupled space learning method. Secondly, a deformed space is constructed with the difference between the estimated initial landmarks and the ground truth of training samples; and more accurate 3D facial landmarks are reconstructed by modifying the initial 3D ones with shape compensations which are calculated by minimizing an objective function. Finally, the realistic 3D faces are reconstructed by a method that is based on a simple sparse regulation and shape deformation. The results on BJUT 3D face database demonstrate the effectiveness of the proposed method. In addition, compared with some typical methods, the method can get better subjective and objective results, especially in details.},
keywords={face recognition;image reconstruction;learning (artificial intelligence);regression analysis;solid modelling;3D face reconstruction;progressive cascade regression;training samples;facial landmark mapping;coupled space learning;deformed space construction;sparse regulation;shape deformation;Three-dimensional displays;Face;Training;Two dimensional displays;Image reconstruction;Solid modeling;Shape},
doi={10.1109/CITS.2017.8035340},
ISSN={},
month={July},}
@INPROCEEDINGS{7899989,
author={D. {Kim} and J. {Choi} and J. T. {Leksut} and G. {Medioni}},
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)},
title={Expression invariant 3D face modeling from an RGB-D video},
year={2016},
volume={},
number={},
pages={2362-2367},
abstract={We aim to reconstruct an accurate neutral 3D face model from an RGB-D video in the presence of extreme expression changes. Since each depth frame, taken by a low-cost sensor, is noisy, point clouds from multiple frames can be registered and aggregated to build an accurate 3D model. However, direct aggregation of multiple data produces erroneous results in natural interaction (e.g., talking and showing expressions). We propose to analyze facial expression from an RGB frame and neutralize the corresponding 3D point cloud if needed. We first estimate the person's expression by fitting blendshape coefficients using 2D facial landmarks for each frame and calculate an expression deformity (expression score). With the estimated expression score, we determine whether an input face is neutral or non-neutral. If the face is non-neutral, we proceed to neutralize the expression of the 3D point cloud in that frame. To neutralize the 3D point cloud of a face, we deform our generic 3D face model by applying the estimated blendshape coefficients, find displacement vectors from the deformed generic face to a neutral generic face, and apply the displacement vectors to the input 3D point cloud. After preprocessing frames in a video, we rank frames based on the expression scores and register the ranked frames into a single 3D model. Our system produces a neutral 3D face model in the presence of extreme expression changes even when neutral faces do not exist in the video.},
keywords={data aggregation;image reconstruction;image registration;video signal processing;expression invariant 3D face modeling;RGB-D video;depth frame;noisy point clouds;low-cost sensor;multiple data direct aggregation;3D point cloud;blendshape coefficients;2D facial landmarks;displacement vectors;neutral generic face;video preprocessing frames;frame ranking;person expression estimation;Three-dimensional displays;Face;Solid modeling;Two dimensional displays;Deformable models;Computational modeling;Iterative closest point algorithm},
doi={10.1109/ICPR.2016.7899989},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7523133,
author={G. {Torkhani} and A. {Ladgham} and M. N. {Mansouri} and A. {Sakly}},
booktitle={2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)},
title={Gabor-SVM applied to 3D-2D deformed mesh model},
year={2016},
volume={},
number={},
pages={447-452},
abstract={We propose a robust method for 3D face recognition using 3D to 2D modeling and facial curvatures detection. The 3D-2D algorithm permits to transform 3D images into 3D triangular mesh, then the mesh model is deformed and fitted to the 2D space in order to obtain a 2D smoother mesh. Then, we apply Gabor wavelets to the deformed model in order to exploit surface curves in the detection of salient face features. The classification of the final Gabor facial model is performed using the support vector machines (SVM). To demonstrate the quality of our technique, we give some experiments using the 3D AJMAL faces database. The experimental results prove that the proposed method is able to give a good recognition quality and a high accuracy rate.},
keywords={face recognition;feature extraction;Gabor filters;mesh generation;support vector machines;visual databases;wavelet transforms;3D-2D deformed mesh model;Gabor-SVM;3D face recognition;2D modeling;3D modeling;facial curvatures detection;3D image transformation;3D triangular mesh;2D space;2D smoother mesh;Gabor wavelets;surface curves;salient face feature detection;Gabor facial model;support vector machines;3D AJMAL face database;Three-dimensional displays;Solid modeling;Face recognition;Face;Deformable models;Feature extraction;Databases;3D face recognition;salient points;deformed mesh model;facial curvatures;Gabor wavelet;SVM},
doi={10.1109/ATSIP.2016.7523133},
ISSN={},
month={March},}
@INPROCEEDINGS{8373908,
author={A. {Morales} and G. {Piella} and O. {Martínez} and F. M. {Sukno}},
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
title={A Quantitative Comparison of Methods for 3D Face Reconstruction from 2D Images},
year={2018},
volume={},
number={},
pages={731-738},
abstract={In the past years, many studies have highlighted the relation between deviations from normal facial morphology (dysmorphology) and some genetic and mental disorders. Recent advances in methods for reconstructing the 3D geometry of the face from 2D images opens new possibilities for dysmorphology research without the need for specialized 3D imaging equipment. However, it is unclear whether these methods could reconstruct the facial geometry with the required accuracy. In this paper we present a comparative study of some of the most relevant approaches for 3D face reconstruction from 2D images, including photometric-stereo, deep learning and 3D Morphable Model fitting. We address the comparison in qualitatively and quantitatively terms using a public database consisting of 2D images and 3D scans from 100 people. Interestingly, we find that some methods produce quite noisy reconstructions that do not seem realistic, whereas others look more natural. However, the latter do not seem to adequately capture the geometric variability that exists between different subjects and produce reconstructions that look always very similar across individuals, thus questioning their fidelity.},
keywords={face recognition;image reconstruction;stereo image processing;photometric-stereo;deep learning;3D Morphable Model fitting;noisy reconstructions;normal facial morphology;genetic disorders;mental disorders;dysmorphology research;specialized 3D imaging equipment;facial geometry;3D face reconstruction;Three-dimensional displays;Face;Two dimensional displays;Image reconstruction;Solid modeling;Geometry;Machine learning;3D face reconstruction;craniofacial geometry;photometric stereo;3D Morphable Model;deep learning},
doi={10.1109/FG.2018.00115},
ISSN={},
month={May},}
@INPROCEEDINGS{7471957,
author={X. {Hu} and Y. {Wang} and F. {Zhu} and C. {Pan}},
booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning-based fully 3D face reconstruction from a single image},
year={2016},
volume={},
number={},
pages={1651-1655},
abstract={This paper presents an algorithm for fully reconstructing a 3D face from a single image. This task is still highly challenging as most current methods only care about the frontal face, ignoring side face, such as the neck, ears etc. In our algorithm, to get the more detailed texture, we deal with the shape reconstruction and texture recovery respectively. For shape, we estimate the deformation of the 3D model by a set of feature points. For texture, due to the similar facial structure, we divide the full texture into patches and show how sparse learning model can be used to fully recover the texture of the 3D face. Extensive experiment results on the CMU-PIE database and images downloaded from the Internet demonstrate that our method outperforms the state-of-the-art methods.},
keywords={face recognition;image reconstruction;image texture;learning (artificial intelligence);learning-based fully 3D face reconstruction;image texture recovery;shape reconstruction;deformation estimation;feature point set;facial structure;3D model;sparse learning model;CMU-PIE database;Internet;Face;Three-dimensional displays;Image reconstruction;Solid modeling;Shape;Databases;Computational modeling;3D Morphable model;face alignmen-t;deform transfer;full reconstruction;sparse learning},
doi={10.1109/ICASSP.2016.7471957},
ISSN={2379-190X},
month={March},}
@ARTICLE{7312454,
author={M. A. {de Jong} and A. {Wollstein} and C. {Ruff} and D. {Dunaway} and P. {Hysi} and T. {Spector} and F. {Liu} and W. {Niessen} and M. J. {Koudstaal} and M. {Kayser} and E. B. {Wolvius} and S. {Böhringer}},
journal={IEEE Transactions on Image Processing},
title={An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets},
year={2016},
volume={25},
number={2},
pages={580-588},
abstract={In this paper, we present a novel approach to automatic 3D facial landmarking using 2D Gabor wavelets. Our algorithm considers the face to be a surface and uses map projections to derive 2D features from raw data. Extracted features include texture, relief map, and transformations thereof. We extend an established 2D landmarking method for simultaneous evaluation of these data. The method is validated by performing landmarking experiments on two data sets using 21 landmarks and compared with an active shape model implementation. On average, landmarking error for our method was 1.9 mm, whereas the active shape model resulted in an average landmarking error of 2.3 mm. A second study investigating facial shape heritability in related individuals concludes that automatic landmarking is on par with manual landmarking for some landmarks. Our algorithm can be trained in 30 min to automatically landmark 3D facial data sets of any size, and allows for fast and robust landmarking of 3D faces.},
keywords={face recognition;feature extraction;Gabor filters;wavelet transforms;automatic 3D facial landmarking algorithm;2D Gabor wavelets;map projections;feature extraction;data sets;active shape model;landmarking error;facial shape heritability;automatic landmarking;Three-dimensional displays;Face;Ellipsoids;Accuracy;Training;Solid modeling;Nose;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Gabor filter;wavelet;automatic landmarking;landmarking;surface data;3D;face;algorithm;Algorithms;Anatomic Landmarks;Face;Humans;Imaging, Three-Dimensional;Pattern Recognition, Automated;Wavelet Analysis},
doi={10.1109/TIP.2015.2496183},
ISSN={1057-7149},
month={Feb},}
@INPROCEEDINGS{8673465,
author={M. {Sayed} and Z. {Bhatti} and I. A. {Ismaili}},
booktitle={2019 2nd International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)},
title={Proposed Model for Facial Animation using Covariance Matrix and Mahalanobis Distance Algorithms},
year={2019},
volume={},
number={},
pages={1-5},
abstract={In this research work, an automated 3D face expression generation technique is presented, which is extracted from real life video of face motion. The face expression is extracted from real human face using Covariance Matrix for detecting face marker points and Mahalanobis Distance to calculate the distance of each marker points within frames. The technique of tracking points on face uses markers placed on key positions of face muscles, then by getting its position from all frames of pre-recoded face video using the distance algorithm the movement of each face muscle is detected and measured. The face muscles are marked with particular tracking markers that are detected and tracked by the system. This tracking occurs by using color segmentation using Huffman Transform, where we detect color of points and track the location and distance of each tracker points. The original and translated position values of each marker points are obtained and recorded in text file in vector values. The tracked values will be transferred in a 3D Animation software like MAYA and applied on a pre-Rigged 3D model of Human face. The 3D face will be rigged using joints to emulate the face muscle behavior.},
keywords={computer animation;face recognition;image colour analysis;image segmentation;solid modelling;face expression;human face;face marker points;tracking points;face uses markers;pre-recoded face video;particular tracking markers;tracker points;tracked values;face muscle behavior;Mahalanobis Distance algorithms;face motion;covariance matrix;facial animation;real life video;color segmentation;Huffman Transform;3D face;pre-Rigged 3D model;3D Animation software;Face;Covariance matrices;Three-dimensional displays;Tracking;Facial animation;Computational modeling;Covariance Matrix;Mahalanobis Distance;Face Tracking;Expression},
doi={10.1109/ICOMET.2019.8673465},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8237768,
author={R. {Yu} and S. {Saito} and H. {Li} and D. {Ceylan} and H. {Li}},
booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
title={Learning Dense Facial Correspondences in Unconstrained Images},
year={2017},
volume={},
number={},
pages={4733-4742},
abstract={We present a minimalists but effective neural network that computes dense facial correspondences in highly unconstrained RGB images. Our network learns a per-pixel flow and a matchability mask between 2D input photographs of a person and the projection of a textured 3D face model. To train such a network, we generate a massive dataset of synthetic faces with dense labels using renderings of a morphable face model with variations in pose, expressions, lighting, and occlusions. We found that a training refinement using real photographs is required to drastically improve the ability to handle real images. When combined with a facial detection and 3D face fitting step, we show that our approach outperforms the state-of-the-art face alignment methods in terms of accuracy and speed. By directly estimating dense correspondences, we do not rely on the full visibility of sparse facial landmarks and are not limited to the model space of regression-based approaches. We also assess our method on video frames and demonstrate successful per-frame processing under extreme pose variations, occlusions, and lighting conditions. Compared to existing 3D facial tracking techniques, our fitting does not rely on previous frames or frontal facial initialization and is robust to imperfect face detections.},
keywords={face recognition;image texture;learning (artificial intelligence);neural nets;regression analysis;unconstrained images;minimalists;effective neural network;computes dense facial correspondences;highly unconstrained RGB images;per-pixel flow;2D input photographs;textured 3D face model;massive dataset;synthetic faces;dense labels;morphable face model;facial detection;3D face fitting step;state-of-the-art face;sparse facial landmarks;extreme pose variations;lighting conditions;existing 3D facial tracking techniques;frontal facial initialization;imperfect face detections;Face;Three-dimensional displays;Two dimensional displays;Solid modeling;Decoding;Rendering (computer graphics);Estimation},
doi={10.1109/ICCV.2017.506},
ISSN={2380-7504},
month={Oct},}
@INPROCEEDINGS{7163142,
author={L. A. {Jeni} and J. F. {Cohn} and T. {Kanade}},
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
title={Dense 3D face alignment from 2D videos in real-time},
year={2015},
volume={1},
number={},
pages={1-8},
abstract={To enable real-time, person-independent 3D registration from 2D video, we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60 degrees. From a single 2D image of a person's face, a dense 3D shape is registered in real time for each frame. The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression. The algorithm first estimates the location of a dense set of markers and their visibility, then reconstructs face shapes by fitting a part-based 3D model. Because no assumptions are required about illumination or surface properties, the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video. The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction. Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video. The software is available online at http://zface.org.},
keywords={face recognition;image reconstruction;image registration;regression analysis;multiview reconstruction;3D reconstruction;part-based 3D model;high-resolution 3D face-scans;fast cascade regression framework;facial landmarks;3D cascade regression approach;person-independent 3D registration;2D videos;dense 3D face alignment;Three-dimensional displays;Face;Solid modeling;Shape;Videos;Image reconstruction;Training},
doi={10.1109/FG.2015.7163142},
ISSN={},
month={May},}
@ARTICLE{8382306,
author={J. {Neves} and H. {Proença}},
journal={IEEE Transactions on Information Forensics and Security},
title={“A Leopard Cannot Change Its Spots”: Improving Face Recognition Using 3D-Based Caricatures},
year={2019},
volume={14},
number={1},
pages={151-161},
abstract={Caricatures refer to a representation of a person, in which the distinctive features are deliberately exaggerated, with several studies showing that humans perform better at recognizing people from caricatures than using original images. Inspired by this observation, this paper introduces the first fully automated caricature-based face recognition approach capable of working with data acquired in the wild. Our approach leverages the 3D face structure from a single 2D image and compares it with a reference model for obtaining a compact representation of face features deviations. This descriptor is subsequently deformed using a “measure locally, weight globally” strategy to resemble the caricature drawing process. The deformed deviations are incorporated in the 3D model using the Laplacian mesh deformation algorithm, and the 2D face caricature image is obtained by projecting the deformed model in the original camera view. To demonstrate the advantages of caricature-based face recognition, we train the VGG-face network from scratch using either original face images (baseline) or caricatured images and use these models for extracting face descriptors from the LFW, IJB-A, and MegaFace data sets. The experiments show an increase in the recognition accuracy when using caricatures rather than original images. Moreover, our approach achieves competitive results with the state-of-the-art face recognition methods, even without explicitly tuning the network for any of the evaluation sets.},
keywords={cameras;face recognition;feature extraction;image representation;fully automated caricature-based face recognition approach;3D face structure;single 2D image;reference model;compact representation;caricature drawing process;deformed deviations;Laplacian mesh deformation algorithm;2D face caricature image;deformed model;original camera view;VGG-face network;caricatured images;face descriptors;recognition accuracy;3D-based caricatures;face feature deviations;measure locally-weight globally strategy;3D model;face descriptor extraction;MegaFace data sets;IJB-A dataset;LFW dataset;Face;Face recognition;Three-dimensional displays;Two dimensional displays;Solid modeling;Deformable models;Strain;Face recognition;3D caricature generation;caricature-based face recognition;facial feature analysis},
doi={10.1109/TIFS.2018.2846617},
ISSN={1556-6013},
month={Jan},}
@INPROCEEDINGS{7550062,
author={ and D. {Huang} and Y. {Wang} and J. {Sun}},
booktitle={2016 International Conference on Biometrics (ICB)},
title={Lock3DFace: A large-scale database of low-cost Kinect 3D faces},
year={2016},
volume={},
number={},
pages={1-8},
abstract={In this paper, we present a large-scale database consisting of low cost Kinect 3D face videos, namely Lock3DFace, for 3D face analysis, particularly for 3D Face Recognition (FR). To the best of our knowledge, Lock3DFace is currently the largest low cost 3D face database for public academic use. The 3D samples are highly noisy and contain a diversity of variations in expression, pose, occlusion, time lapse, and their corresponding texture and near infrared channels have changes in lighting condition and radiation intensity, allowing for evaluating FR methods in complex situations. Furthermore, based on Lock3DFace, we design the standard experimental protocol for low-cost 3D FR, and give the baseline performance of individual subsets belonging to different scenarios for fair comparison in the future.},
keywords={emotion recognition;face recognition;image texture;pose estimation;video databases;video signal processing;large-scale database;low-cost Kinect 3D face videos;Lock3DFace database;3D face analysis;3D face recognition;3D FR;expression analysis;pose analysis;occlusion analysis;time lapse analysis;texture analysis;near infrared channels;lighting condition;radiation intensity;Three-dimensional displays;Databases;Videos;Solid modeling;Two dimensional displays;Lighting;Sensors},
doi={10.1109/ICB.2016.7550062},
ISSN={},
month={June},}
@INPROCEEDINGS{8296396,
author={S. {Hong} and W. {Im} and J. {Ryu} and H. S. {Yang}},
booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
title={SSPP-DAN: Deep domain adaptation network for face recognition with single sample per person},
year={2017},
volume={},
number={},
pages={825-829},
abstract={Real-world face recognition using a single sample per person (SSPP) is a challenging task. The problem is exacerbated if the conditions under which the gallery image and the probe set are captured are completely different. To address these issues from the perspective of domain adaptation, we introduce an SSPP domain adaptation network (SSPP-DAN). In the proposed approach, domain adaptation, feature extraction, and classification are performed jointly using a deep architecture with domain-adversarial training. However, the SSPP characteristic of one training sample per class is insufficient to train the deep architecture. To overcome this shortage, we generate synthetic images with varying poses using a 3D face model. Experimental evaluations using a realistic SSPP dataset show that deep domain adaptation and image synthesis complement each other and dramatically improve accuracy. Experiments on a benchmark dataset using the proposed approach show state-of-the-art performance.},
keywords={face recognition;feature extraction;image classification;image sampling;learning (artificial intelligence);SSPP-DAN;domain-adversarial training;3D face model;image synthesis complement;face recognition;single sample per person domain adaptation network;feature extraction;image classification;image sampling;Training;Face;Feature extraction;Face recognition;Image generation;Probes;Surveillance;SSPP face recognition;Domain adaptation;Image synthesis;SSPP-DAN;Surveillance camera},
doi={10.1109/ICIP.2017.8296396},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{8579077,
author={J. {Li} and B. M. {Chen} and G. H. {Lee}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={SO-Net: Self-Organizing Network for Point Cloud Analysis},
year={2018},
volume={},
number={},
pages={9397-9406},
abstract={This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website1.},
keywords={computer vision;feature extraction;learning (artificial intelligence);nearest neighbour methods;self-organising feature maps;deep learning;orderless point clouds;SO-Net models;spatial distribution;individual points;SOM nodes;input point cloud;single feature vector;point cloud reconstruction;part segmentation;shape retrieval;point cloud recognition networks;point cloud analysis;permutation invariant architecture;self-organizing map;self-organizing network;point-to-node k nearest neighbor search;SO-Net;hierarchical feature extraction;Three-dimensional displays;Feature extraction;Training;Shape;Two dimensional displays;Graphical models},
doi={10.1109/CVPR.2018.00979},
ISSN={2575-7075},
month={June},}
@ARTICLE{7083738,
author={H. {Liang} and R. {Liang} and M. {Song} and X. {He}},
journal={IEEE Transactions on Cybernetics},
title={Coupled Dictionary Learning for the Detail-Enhanced Synthesis of 3-D Facial Expressions},
year={2016},
volume={46},
number={4},
pages={890-901},
abstract={The desire to reconstruct 3-D face models with expressions from 2-D face images fosters increasing interest in addressing the problem of face modeling. This task is important and challenging in the field of computer animation. Facial contours and wrinkles are essential to generate a face with a certain expression; however, these details are generally ignored or are not seriously considered in previous studies on face model reconstruction. Thus, we employ coupled radius basis function networks to derive an intermediate 3-D face model from a single 2-D face image. To optimize the 3-D face model further through landmarks, a coupled dictionary that is related to 3-D face models and their corresponding 3-D landmarks is learned from the given training set through local coordinate coding. Another coupled dictionary is then constructed to bridge the 2-D and 3-D landmarks for the transfer of vertices on the face model. As a result, the final 3-D face can be generated with the appropriate expression. In the testing phase, the 2-D input faces are converted into 3-D models that display different expressions. Experimental results indicate that the proposed approach to facial expression synthesis can obtain model details more effectively than previous methods can.},
keywords={computer animation;face recognition;image reconstruction;learning (artificial intelligence);coupled dictionary learning;detail enhanced synthesis;3D facial expressions;3D face models;2D face images fosters;computer animation;face model reconstruction;coupled radius basis function networks;Three-dimensional displays;Solid modeling;Image reconstruction;Shape analysis;Encoding;Coupled dictionary;expression synthesis;landmark;local coordinate coding (LCC);Coupled dictionary;expression synthesis;landmark;local coordinate coding (LCC);Algorithms;Face;Facial Expression;Humans;Imaging, Three-Dimensional;Machine Learning},
doi={10.1109/TCYB.2015.2417211},
ISSN={2168-2267},
month={April},}
@INPROCEEDINGS{8575249,
author={G. J. {Hsu} and W. {Huang} and J. {Kang}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
title={Hierarchical Network for Facial Palsy Detection},
year={2018},
volume={},
number={},
pages={693-6936},
abstract={We propose the Hierarchical Detection Network (HDN) for the detection of facial palsy syndrome. This can be the first deep-learning based approach for the facial palsy detection. The proposed HDN consists of three component networks, the first detects faces, the second detects the landmarks on the detected faces, and the third detects the local palsy regions. The first and the third component networks are built on the Darknet framework, but with fewer layers of convolutions for shorter processing speed. The second component network employs the latest 3D face alignment network for locating the landmarks. The first component network employs a Na × Na grid over the overall input image, while the third component network employs a Nb × Nb grid over each detected face, making the HDN capable of efficiently locating the affected palsy regions. As previous approaches were evaluated on proprietary databases, we have collected 32 videos from YouTube and made the first public database for facial palsy study. To enhance the robustness against expression variations, we include the CK+ facial expression database in the training and testing phases. We show that the HDN does not just detect the local palsy regions, but also captures the frequency of the intensity, enabling the video-to-description diagnosis of the syndrome. Experiments show that the proposed approach offers an accurate and efficient solution for facial palsy detection/diagnosis.},
keywords={face recognition;learning (artificial intelligence);medical image processing;video signal processing;syndrome video-to-description diagnosis;3D face alignment network;facial palsy detection;facial palsy diagnosis;hierarchical detection network;face detection;facial palsy syndrome;HDN;facial expression database;facial palsy study;affected palsy regions;component network;local palsy regions;Databases;Feature extraction;Benchmark testing;Detectors;Face detection;Fans;Videos},
doi={10.1109/CVPRW.2018.00100},
ISSN={2160-7516},
month={June},}
@INPROCEEDINGS{7557202,
author={A. {Amolik} and S. T. {Ahamad} and S. {Dey} and },
booktitle={2016 International Conference on Computation of Power, Energy Information and Commuincation (ICCPEIC)},
title={3D face view generation from human drawn sketch: A review},
year={2016},
volume={},
number={},
pages={237-244},
abstract={From last few decades, generating a 3D face model from an human drawn sketch has caught the interest of many researchers in the area of image processing and face recognition. It has various applications in 3D cartoon modelling, police investigation and verification, and in Image Processing. Many techniques are there to generate 3D models from a sketch. 3D landmark estimation, 2D landmark detection, and synthesis of texture and surface with respect to 3-D morphable model are the steps, respectively, to generate the 3D face model. 3D face modelling using these steps has a higher rate of accuracy of identification of a person from her sketch and no proper photograph. In this piece of literature, we present a review on efficient technique that can be used to generate 3D face from sketch drawn by human.},
keywords={face recognition;3D face view generation;human drawn sketch;3D face model;image processing;face recognition;3D cartoon modelling;police investigation;police verification;3D landmark estimation;2D landmark detection;texture synthesis;3-D morphable model;identification accuracy;Three-dimensional displays;Face;Two dimensional displays;Databases;Solid modeling;Active appearance model;Active Shape Model;PCA(Principal Component Analysis);MeshIK;3D Modelling;Appearance Model;Face Recognition},
doi={10.1109/ICCPEIC.2016.7557202},
ISSN={},
month={April},}
@INPROCEEDINGS{8687254,
author={T. {Terada} and Y. {Chen} and R. {Kimura}},
booktitle={2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
title={3D Facial Landmark Detection Using Deep Convolutional Neural Networks},
year={2018},
volume={},
number={},
pages={390-393},
abstract={Various applications have been developed in facial research to aid the recognition of personal attributes, analysis of race, and personal authentication for the security industry and other research fields. Thus, it is possible to identify the differences in facial shape based on place of birth or country. The present study analyzes facial shape using scanned 3D facial images and investigates ways to extract facial landmarks from the 3D facial images. The detection of the facial landmark requires the normalization of the facial scale and position among in the 3D image data to analyze the facial shape. Therefore, it is difficult to obtain accurate facial landmarks from 3D facial images. Our method decomposes the task into the following three parts: (a) conversion of data from the 3D facial image to a 2D image, (b) extraction of facial landmarks from the 3D image using Convolutional Neural Network (CNN), (c) inversion of the identified facial landmarks from 2D to 3D images. In experiments, we compared the accuracy of this model for facial landmark detection.},
keywords={component;landmarks detection;3d facial image;point cloud;facial analysis;cnn},
doi={10.1109/FSKD.2018.8687254},
ISSN={},
month={July},}
@INPROCEEDINGS{7877955,
author={G. {Liang} and Z. {Shu} and J. {Jianguo}},
booktitle={2016 IEEE 13th International Conference on Signal Processing (ICSP)},
title={Fusing deep convolutional network with SFM for 3D face reconstruction},
year={2016},
volume={},
number={},
pages={873-878},
abstract={3D face reconstruction has long been a Research Focus and application hotsopt, in the field of 3D reconstruction from 2D face image sequences, the reconstruction result is impeded by the problems of the occlusion and illumination or pose variations frequently. Different from 3D morphable model fitting algorithm, the reconstruction method based on face feature points puts forward higher requirements to the robustness and accuracy. This paper propose a reconstruction method of fusing deep convolutional network for facial feature points extraction and factorization for SFM(Structure from Motion), we investigated the way of solving accurate sparse face structure matrix by importing a modified matrix, and the possibility of improving the realistic of 3D face reconstruction result by registering the sparse face structure and the general 3D face model. To reconstruct final dense face model, we use thin plate spline interpolation. The reconstruction result based on our method proved to be efficient and robust.},
keywords={face recognition;image fusion;image reconstruction;fusing deep convolutional network;SFM;3D face reconstruction;illumination;3D morphable model fitting algorithm;structure from motion;facial feature points extraction;factorization;Face;Three-dimensional displays;Image reconstruction;Sparse matrices;Solid modeling;Shape;Feature extraction},
doi={10.1109/ICSP.2016.7877955},
ISSN={2164-5221},
month={Nov},}
@INPROCEEDINGS{7603727,
author={Y. {Lei} and S. {Feng} and X. {Zhou} and Y. {Guo}},
booktitle={2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)},
title={An efficient 3D partial face recognition approach with single sample},
year={2016},
volume={},
number={},
pages={994-999},
abstract={3D partial face recognition under missing parts, occlusions and data corruptions is a major challenge for the practical application of the techniques of 3D face recognition. Moreover, one individual can only provide one sample for training in most practical scenarios, and thus the face recognition with single sample problem is another highly challenging task. We propose an efficient framework for 3D partial face recognition with single sample addressing both of the two problems. First, we represent a facial scan with a set of keypoint based local geometrical descriptors, which gains sufficient robustness to partial facial data along with expression/pose variations. Then, a two-step modified collaborative representation classification scheme is proposed to address the single sample recognition problem. A class-based probability estimation is given during the first classification step, and the obtained result is then incorporated into the modified collaborative representation classification as a locality constraint to improve its classification performance. Extensive experiments on the Bosphorus and FRGC v2.0 datasets demonstrate the efficiency of the proposed approach when addressing the problem of 3D partial face recognition with single sample.},
keywords={face recognition;image classification;image representation;probability;3D partial face recognition;data corruptions;facial scan;local geometrical descriptors;two-step modified collaborative representation classification scheme;single sample recognition problem;class-based probability estimation;FRGC v2.0 datasets;Bosphorus datasets;Three-dimensional displays;Face;Face recognition;Training;Feature extraction;Estimation;Collaboration;3D partial face recognition;3D facial representation;locality constraint;collaborative representation;single sample problem},
doi={10.1109/ICIEA.2016.7603727},
ISSN={2158-2297},
month={June},}
@INPROCEEDINGS{8590014,
author={C. {Lv} and Z. {Wu} and X. {Wang} and D. {Zhang} and X. {Liu} and M. {Zhou}},
booktitle={2018 International Conference on Cyberworlds (CW)},
title={Facial Expression Editing in Face Sketch Using Shape Space Theory},
year={2018},
volume={},
number={},
pages={33-40},
abstract={Facial expression editing in face sketch is an important and challenging problem in computer vision community as facial animation and modeling. For criminal investigation and portrait drawing, automatic expression editing tools for face sketch improve work efficiency obviously and reduce professional requirements for users. In this paper, we propose a novel method for facial expression editing in face sketch using shape space theory. The new facial expressions in the sketch images can be regenerated automatically. The method includes two components: 1) face sketch modeling; 2) expression editing. The face sketch modeling constructs 3D face sketch data from 3D facial database to match the 2D face sketch. Using facial landmarks, the "shape" of the face sketch is represented in shape space. The shape space is a manifold space which removes the rigid transform group. In shape space, the accurate 3D face sketch model is obtained which is consistent to the original 2D face sketch. For expression editing, we change the parameters of 3D face sketch model in the shape space to obtain new expressions. The expression transfer in 3D face sketch model can be mapped into the 2D face sketch. The advantages of our method are: full-automatic in modeling process; no requirements of drawing skills to user and friendly interaction; robustness to head poses and different scales. In experiments, we use the 3D facial database, FaceWareHouse, to construct the 3D face sketch model and use face sketch images from database: CUHK Face sketch Database (CUFS) to show the performance of expression editing. Experimental results demonstrate that our method can effectively edit facial expressions in face sketch with high consistency and fidelity.},
keywords={computer vision;emotion recognition;face recognition;image matching;pose estimation;visual databases;facial expression editing;CUHK face sketch database;face sketch modelling;3D facial database;2D face sketch matching;shape space theory;Face;Shape;Three-dimensional displays;Solid modeling;Space vehicles;Mathematical model;Two dimensional displays;face sketch modeling;face sketch editing;shape space;facial landmarks model},
doi={10.1109/CW.2018.00019},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7899906,
author={H. X. {Pham} and V. {Pavlovic} and and },
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)},
title={Robust real-time performance-driven 3D face tracking},
year={2016},
volume={},
number={},
pages={1851-1856},
abstract={We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker capable of handling a wide range of target scene depths, whose performances are demonstrated in our extensive experiments better than those of the state-of-the-arts.},
keywords={cameras;computational geometry;face recognition;image colour analysis;object tracking;optimisation;solid modelling;stereo image processing;video signal processing;3D face tracking;RGBD video streams;head pose tracking;facial actions;cameras;point cloud;3D shape regressor;joint 2D+3D optimization;3D model reconstruction;Three-dimensional displays;Shape;Face;Training;Two dimensional displays;Optimization;Solid modeling},
doi={10.1109/ICPR.2016.7899906},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7377975,
author={X. {Hu} and and S. {Peng}},
booktitle={2015 11th International Conference on Natural Computation (ICNC)},
title={Video surveillance face recognition by more virtual training samples based on 3D modeling},
year={2015},
volume={},
number={},
pages={113-117},
abstract={Video surveillance has been applied in more and more fields for security in last decade years, video-based face recognition therefore became an important task of an intelligent monitoring system. However, among these captured video faces there are many non-frontal faces. As a result, the state-of-art face algorithms would become worse when they were employed to recognize video faces. On the other hand, it was a common phenomenon especially at video monitoring field that only one training sample per person is gained from their identification card. The single sample per person (SSPP) results in effecting even not taking advantage of some fine algorithms such LDA. In order to effectively improve the correct recognition rate of multi-pose face recognition with a single frontal training sample, this paper proposed a face recognition algorithm based on 3D modeling. In the proposed algorithm, firstly a 2D frontal face with high-resolution was taken to build a 3D face model, and then several virtual faces with different poses were produced from the 3D face model. At last, both the original frontal face image and virtual face images were put into a gallery set. The algorithm was evaluated on SCface database using traditional PCA and LDA methods. The result showed that the proposed approach could effectively improve video face recognition rate and the correct recognition rate went up about 13% by LDA compared with traditional PCA. Therefore, the method that was proposed to create virtual looking down training samples was an effective algorithm and could be considered to apply in intelligent video monitoring system.},
keywords={face recognition;principal component analysis;video surveillance;video surveillance;video-based face recognition;captured video faces;video monitoring field;identification card;single sample per person results;SSPP results;multi-pose face recognition;single frontal training sample;2D frontal face;3D face model;original frontal face image;virtual face images;gallery set;SCface database;PCA methods;LDA methods;virtual looking down training samples;intelligent video monitoring system;Face;Three-dimensional displays;Face recognition;Cameras;Training;Solid modeling;Monitoring;3D modeling;single training sample;video surveillance;PCA;LDA},
doi={10.1109/ICNC.2015.7377975},
ISSN={2157-9563},
month={Aug},}
@INPROCEEDINGS{7780900,
author={T. {Bolkart} and S. {Wuhrer}},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={A Robust Multilinear Model Learning Framework for 3D Faces},
year={2016},
volume={},
number={},
pages={4911-4919},
abstract={Multilinear models are widely used to represent the statistical variations of 3D human faces as they decouple shape changes due to identity and expression. Existing methods to learn a multilinear face model degrade if not every person is captured in every expression, if face scans are noisy or partially occluded, if expressions are erroneously labeled, or if the vertex correspondence is inaccurate. These limitations impose requirements on the training data that disqualify large amounts of available 3D face data from being usable to learn a multilinear model. To overcome this, we introduce the first framework to robustly learn a multilinear model from 3D face databases with missing data, corrupt data, wrong semantic correspondence, and inaccurate vertex correspondence. To achieve this robustness to erroneous training data, our framework jointly learns a multilinear model and fixes the data. We evaluate our framework on two publicly available 3D face databases, and show that our framework achieves a data completion accuracy that is comparable to state-of-the-art tensor completion methods. Our method reconstructs corrupt data more accurately than state-of-the-art methods, and improves the quality of the learned model significantly for erroneously labeled expressions.},
keywords={computer graphics;face recognition;learning (artificial intelligence);visual databases;robust multilinear model learning framework;3D faces;statistical variations;3D human faces;multilinear face model;face scans;3D face databases;missing data;corrupt data;semantic correspondence;inaccurate vertex correspondence;erroneous training data;data completion accuracy;tensor completion methods;Data models;Computational modeling;Three-dimensional displays;Solid modeling;Semantics;Robustness;Shape},
doi={10.1109/CVPR.2016.531},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{8326087,
author={S. {Lim} and B. {Hwang} and S. {Yoon} and J. S. {Choi} and C. {Park}},
booktitle={2018 IEEE International Conference on Consumer Electronics (ICCE)},
title={Automatic 3D face component analysis technique},
year={2018},
volume={},
number={},
pages={1-2},
abstract={This paper proposes a technique to perform segmentation for the meaningful regions that part of the face captured by 3D scanners or 3D sensors, automatically. Each part recognition of the scanned face is vital for the 3D applications such as modeling, animation and 3D printing. We transfer the template model labeled with the meaningful part to the scanned face model to find the corresponding part of each meaningful part of the template model. This technique can be used to the several applications such as 3D face modeling, facial animation, virtual facial surgery and 3D printing.},
keywords={face recognition;image segmentation;part recognition;modeling animation;template model;scanned face model;automatic 3D face component analysis technique;image segmentation;Conferences;Consumer electronics},
doi={10.1109/ICCE.2018.8326087},
ISSN={2158-4001},
month={Jan},}
@INPROCEEDINGS{8538385,
author={A. {Kusnadi} and and R. {Winantyo} and I. Z. {Pane}},
booktitle={2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)},
title={Evaluation of Feature Detectors on Repeatability Quality of Facial Keypoints In Triangulation Method},
year={2018},
volume={},
number={},
pages={1-4},
abstract={This study derived from a research focusing on 3D face recognition using ToF camera. But the system can't be used outdoors, because of a backlight. To solve this problem, a commercial digital single-lens reflex (DSLR) camera will be used. It can be approached y solving the stereo-view reconstruction problem for each pair of consecutive images. To reconstruct an object, projection matrix estimation from 2D point correspondences will be needed. The accuracy of 3D reconstruction is highly dependent on the corresponding points of 2D data projections from images to other images. In this research, The detectors are Harris-Stephens, SURF, FAST, Minimum Eigenvalue, and BRISK have been tested and analyzed through black box test. To evaluate feature detectors performance, the repeatability score for a given pair of images is computed. To do that it can use recall and precision. The best detector is the Harris Stephens detector because it has the best F-measure values of 0.46.},
keywords={cameras;computer vision;eigenvalues and eigenfunctions;face recognition;feature extraction;image reconstruction;matrix algebra;stereo image processing;minimum eigenvalue;facial keypoints;repeatability quality;Harris Stephens detector;repeatability score;feature detectors performance;black box test;2D data projections;corresponding points;2D point correspondences;projection matrix estimation;consecutive images;stereo-view reconstruction problem;DSLR;single-lens reflex camera;ToF camera;3D face recognition;triangulation method;Detectors;Cameras;Feature extraction;Three-dimensional displays;Eigenvalues and eigenfunctions;Face;Informatics;feature detectiont;keypoint;triangulation method;repeatability;recall;precision, F-measure},
doi={10.1109/ICSCEE.2018.8538385},
ISSN={},
month={July},}
@INPROCEEDINGS{7797014,
author={X. {Yu} and Y. {Gao} and J. {Zhou}},
booktitle={2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
title={Boosting Radial Strings for 3D Face Recognition with Expressions and Occlusions},
year={2016},
volume={},
number={},
pages={1-6},
abstract={In this paper, we present a new radial string representation and matching approach for 3D face recognition under expression variations and partial occlusions. The radial strings are an indexed collection of strings emanating from the nose tip of a face scan. The matching between two radial strings is conducted through a dynamic programming process, in which a partial matching mechanism is established to effectively find those un-occluded substrings. Moreover, the most discriminative and stable radial strings are selected optimally by the well-known AdaBoost algorithm to achieve a composite classifier for 3D face recognition under facial expression changes. Experimental results on the GavabDB and the Bosphorus databases show that the proposed approach achieves promising results for human face recognition with expressions and occlusions.},
keywords={dynamic programming;face recognition;image classification;image matching;image representation;learning (artificial intelligence);3D face recognition;radial string representation;expression variations;partial occlusions;dynamic programming;partial matching mechanism;AdaBoost algorithm;composite classifier;GavabDB databases;Bosphorus databases;Three-dimensional displays;Face recognition;Face;Cost function;Nose;Classification algorithms;Shape},
doi={10.1109/DICTA.2016.7797014},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8578512,
author={A. T. {Tran} and T. {Hassner} and I. {Masi} and E. {Paz} and Y. {Nirkin} and G. {Medioni}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={Extreme 3D Face Reconstruction: Seeing Through Occlusions},
year={2018},
volume={},
number={},
pages={3935-3944},
abstract={Existing single view, 3D face reconstruction methods can produce beautifully detailed 3D results, but typically only for near frontal, unobstructed viewpoints. We describe a system designed to provide detailed 3D reconstructions of faces viewed under extreme conditions, out of plane rotations, and occlusions. Motivated by the concept of bump mapping, we propose a layered approach which decouples estimation of a global shape from its mid-level details (e.g., wrinkles). We estimate a coarse 3D face shape which acts as a foundation and then separately layer this foundation with details represented by a bump map. We show how a deep convolutional encoder-decoder can be used to estimate such bump maps. We further show how this approach naturally extends to generate plausible details for occluded facial regions. We test our approach and its components extensively, quantitatively demonstrating the invariance of our estimated facial details. We further provide numerous qualitative examples showing that our method produces detailed 3D face shapes in viewing conditions where existing state of the art often break down.},
keywords={face recognition;image coding;image reconstruction;bump mapping;coarse 3D face shape;deep convolutional encoder-decoder;3D face shapes;unobstructed viewpoints;frontal viewpoints;3D face reconstruction methods;occlusions;Face;Shape;Three-dimensional displays;Image reconstruction;Estimation;Robustness;Strain},
doi={10.1109/CVPR.2018.00414},
ISSN={2575-7075},
month={June},}
@INPROCEEDINGS{8578865,
author={L. {Tran} and X. {Liu}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={Nonlinear 3D Face Morphable Model},
year={2018},
volume={},
number={},
pages={7346-7355},
abstract={As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.},
keywords={decoding;image coding;image reconstruction;image representation;image scanners;image texture;learning (artificial intelligence);network coding;statistical analysis;3D facial shape;image synthesis;nonlinear 3DMM model;statistical model;nonlinear 3D face morphable model;3D facial image texture;associated 3D face image scanning;2D face imaging;image representation;PCA basis functions;decoding;original input face reconstruction;network encoder;Face;Three-dimensional displays;Shape;Solid modeling;Two dimensional displays;Image reconstruction;Analytical models},
doi={10.1109/CVPR.2018.00767},
ISSN={2575-7075},
month={June},}
@INPROCEEDINGS{7603151,
author={D. {Sopiak} and M. {Oravec} and J. {Pavlovičová} and Z. {Bukovčíková} and M. {Dittingerová} and A. {Biľanská} and M. {Novotná} and J. {Gontkovič}},
booktitle={2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
title={Generating face images based on 3D morphable model},
year={2016},
volume={},
number={},
pages={58-62},
abstract={In modern days the demand for biometrics increases rapidly. The world still needs to solve many problems and answer to lot of questions regarding to biometrics for creating better solutions for recognition and verification of objects. Biometrics has become really important topic of our security. Number of input samples per person affects recognition in modern algorithms used for face recognition. We can say that single-sample problem is one of the most challenging problems in face recognition. Most of face recognition algorithms requires at least two samples per person but it is very important to create a system where only one sample per person would achieve a good performance. This topic is especially related to passport and id photos because they include only one picture of person. In this paper, we explain process of reconstruction and generating 3D face model created from id or passport photo. We also present our results that shows influence of generated new samples on face recognition.},
keywords={face recognition;face images;3D morphable model;object verification;object recognition;face recognition algorithms;id photos;passport photos;3D face model;Face;Three-dimensional displays;Solid modeling;Shape;Face recognition;Histograms;biometrics;face recognition;morphable model},
doi={10.1109/FSKD.2016.7603151},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8634657,
author={S. A. {Ali Shah} and M. {Bennamoun} and M. {Molton}},
booktitle={2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)},
title={A Fully Automatic Framework for Prediction of 3D Facial Rejuvenation},
year={2018},
volume={},
number={},
pages={1-6},
abstract={How will I look afterwards? is a common question asked by the patients undergoing a cosmetic procedure. Cosmetic practitioners at present can only offer subjective and descriptive replies. This subjective prediction is a serious concern for patients undergoing cosmetic treatment and therefore necessitates the development of automatic techniques for facial quantification. This paper proposes a novel machine learning approach to quantify and predict the outcome of 3D facial rejuvenation prior to actual cosmetic procedure. The facial rejuvenation prediction results are achieved by estimating the dermal filler volume in 3D faces. This involves estimation of structural changes in 3D face images and to learn underlying structural mapping. Our preliminary experimental results show that the proposed model achieves superior prediction accuracy on real world dataset compared to baseline methods. The computational time analysis shows that the proposed technique is very efficient (at test time) which makes it suitable for real time applications.},
keywords={cosmetics;face recognition;learning (artificial intelligence);superior prediction accuracy;fully automatic framework;3D facial rejuvenation;common question;cosmetic practitioners;cosmetic treatment;automatic techniques;facial quantification;actual cosmetic procedure;facial rejuvenation prediction results;3D face images;Three-dimensional displays;Prediction algorithms;Solid modeling;Predictive models;Training;Machine learning;Australia;Facial rejuvenation;Predictive Modeling;Machine Learning},
doi={10.1109/IVCNZ.2018.8634657},
ISSN={2151-2205},
month={Nov},}
@INPROCEEDINGS{8256378,
author={H. {Li} and Y. {Li} and W. {Liu} and H. {Dong}},
booktitle={2017 International Conference on Behavioral, Economic, Socio-cultural Computing (BESC)},
title={Coarse-to-fine facial landmarks localization based on convolutional feature},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Accurate facial landmarks localization (FLL) plays an important role in face recognition, face tracking and 3D face reconstruction. It can be formulated as a regression problem, which outputs facial landmarks positions from the detected face image. Deep constitutional neural network (CNN) has achieved great success in vision tasks, but it is insignificant to use it directly. In this paper, instead of adopting CNN model straightforwardly, we combine different convolutional features with extreme machine learning (ELM) in a cascade framework to achieve accurate FLL. Specifically, we extract globally and spatially convolutional feature in the first stage for containing better localization property by training deep CNN, which takes the whole face region as input and concatenates lower layers with higher layers. Then, we extract locally and correlatedly convolutional feature in the following stages for preserving shape constraint by building multi-objective CNN, which inputs local patches centered at the current landmarks and concatenates independent subnetwork of each landmark together. Moreover, the regressor embedded in CNN is replaced by the robust ELM for accurate shape regression. Extensive experiments demonstrate that our method performs better in challenging datasets.},
keywords={face recognition;feature extraction;image reconstruction;learning (artificial intelligence);neural nets;object tracking;regression analysis;convolutional feature;face recognition;face tracking;3D face reconstruction;deep constitutional neural network;extreme machine learning;localization property;FLL;coarse-to-fine facial landmark localization;facial landmark localization;face image detection;multiobjective deep CNN model;shape preservation;shape regression problem;Face;Shape;Feature extraction;Frequency locked loops;Robustness;Training;facial landmark localization;convolutional feature;cascade framework;extreme machine learning;unconstrained environment},
doi={10.1109/BESC.2017.8256378},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7369767,
author={H. {Talandova} and L. {Kralik} and M. {Adamek}},
booktitle={2015 International Conference on Logistics, Informatics and Service Sciences (LISS)},
title={Determination of the physiological similarities of family members by using a broadway 3D biometric device},
year={2015},
volume={},
number={},
pages={1-4},
abstract={The biometric identification by the face is one of the oldest biometric identification. With increasing progress in the area of identification by the face this technique was implemented into area of security, where it provides a faster and more accurate identification. The 3D face reader uses for the identification of the person: eyes, mouth, nose, and in contrast to 2D readers also chin and cheeks. 3D face reader by Broadway manufacturer was used to measure the physiological similarities of family members. It is equipped with the 3D camera system, which uses the method of structured light scanning and saves the template into the 3D model of face. The obtained data were evaluated by software Turnstile Enrolment Application (TEA). The participants of the measurement were members of three different families. Each person was compared with the previously saved templates of other family members. Using this method the similarity of family members was evalua00ted.},
keywords={biometrics (access control);face recognition;physiological similarity;family member;broadway 3D biometric device;biometric identification;3D face reader;2D reader;Broadway manufacturer;3D camera system;structured light scanning;3D model;software turnstile enrolment application;TEA;Face;Three-dimensional displays;Security;Shape;Atmospheric measurements;Particle measurements;Object recognition;Biometric identification;face;Broadway 3D reader;similarity},
doi={10.1109/LISS.2015.7369767},
ISSN={},
month={July},}
@INPROCEEDINGS{7899769,
author={G. {Tian} and T. {Mori} and Y. {Okuda}},
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)},
title={Spoofing detection for embedded face recognition system using a low cost stereo camera},
year={2016},
volume={},
number={},
pages={1017-1022},
abstract={Spoofing detection is essential for practical face recognition system. Based on the fact that genuine face has special geometric curvatures across surface, this paper brings forward an ultra-fast yet accurate spoofing detection approach using a low-cost stereo camera. To obtain curvatures, the three dimensional shapes of selected facial landmarks are analyzed, by fitting point cloud around each landmark to a specific partial face surface. Spoofing detection is then performed by evaluating curvatures of each landmark and integrating them together. Experiments verify that the approach is able to detect spoofed faces in printed photographs without or with various bending at FAR equal to 0.00%. Meanwhile, genuine faces have a trivial opportunity to be falsely rejected: FRR is 0.59% for near frontal faces and less than 5% for faces with large varying poses. Detection time is 51 milliseconds when executed on a single processor [1] running at a clock frequency of 266M Hz, this makes the detection very suitable for embedded face recognition system.},
keywords={face recognition;stereo image processing;spoofing detection;embedded face recognition system;low cost stereo camera;facial landmark 3D shapes;frequency 266 MHz;Face;Three-dimensional displays;Nose;Face recognition;Cameras;Surface fitting;Fitting;spoof detection;point cloud;surface fitting;stereo vision},
doi={10.1109/ICPR.2016.7899769},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7780741,
author={M. {Piotraschke} and V. {Blanz}},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Automated 3D Face Reconstruction from Multiple Images Using Quality Measures},
year={2016},
volume={},
number={},
pages={3418-3427},
abstract={Automated 3D reconstruction of faces from images is challenging if the image material is difficult in terms of pose, lighting, occlusions and facial expressions, and if the initial 2D feature positions are inaccurate or unreliable. We propose a method that reconstructs individual 3D shapes from multiple single images of one person, judges their quality and then combines the best of all results. This is done separately for different regions of the face. The core element of this algorithm and the focus of our paper is a quality measure that judges a reconstruction without information about the true shape. We evaluate different quality measures, develop a method for combining results, and present a complete processing pipeline for automated reconstruction.},
keywords={face recognition;feature extraction;image reconstruction;shape recognition;automated 3D face reconstruction;quality measures;pose;lighting;occlusions;facial expressions;2D feature positions;3D shape reconstruction;face regions;Face;Three-dimensional displays;Image reconstruction;Shape;Two dimensional displays;Solid modeling;Feature extraction},
doi={10.1109/CVPR.2016.372},
ISSN={1063-6919},
month={June},}
@ARTICLE{8642338,
author={S. A. A. {Shah} and M. {Bennamoun} and M. K. {Molton}},
journal={IEEE Access},
title={Machine Learning Approaches for Prediction of Facial Rejuvenation Using Real and Synthetic Data},
year={2019},
volume={7},
number={},
pages={23779-23787},
abstract={This paper proposes a novel machine learning approaches to predict the outcome of facial rejuvenation prior to a cosmetic procedure. This is achieved by estimating the required amount of dermal filler volume that needs to be applied on the face by learning the underlying structural mapping from the pretreatment and posttreatment 3D face images. We develop and train our proposed deep neural network, called Rejuv3DNet, designed specifically to predict the dermal filler volume. We also propose the kernel regression (KR)-based model to validate and improve our volume estimation results using regression. Our other contributions include the development of the first 3D face cosmetic dataset, which consists of real-world pretreatment and posttreatment 3D face images and a novel technique for the generation of synthetic cosmetic treatment 3D face images. Our experimental results show that the proposed Rejuv3DNet and the KR model achieve 62.5% and 66.67%, respectively, on real-world data, while these techniques achieve a prediction accuracy of 75.2% and 89.5%, and 77.2% and 90.1% on our two different synthetic datasets. Our proposed techniques have been found to be computationally efficient, achieving near real-time prediction performance. The reported accuracies are our preliminary results for proof of concept, which can be improved with more data. The proposed approach has the potential for further investigation in the cosmetic surgery domain.},
keywords={cosmetics;face recognition;learning (artificial intelligence);neural nets;regression analysis;surgery;cosmetic procedure;dermal filler volume;deep neural network;kernel regression-based model;volume estimation results;real-world pretreatment;KR model;real-world data;prediction accuracy;real-time prediction performance;cosmetic surgery domain;Rejuv3DNet model;synthetic cosmetic treatment;3D face cosmetic dataset;3D face images;facial rejuvenation prediction;synthetic datasets;structural mapping;machine learning;Face;Three-dimensional displays;Solid modeling;Predictive models;Training;Machine learning;Australia;Deep learning;deep neural network;facial analysis;regression},
doi={10.1109/ACCESS.2019.2899379},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8296814,
author={C. {Gou} and Y. {Wu} and F. {Wang} and Q. {Ji}},
booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
title={Coupled cascade regression for simultaneous facial landmark detection and head pose estimation},
year={2017},
volume={},
number={},
pages={2906-2910},
abstract={Current approaches for facial landmark detection and head pose estimation first perform landmark detection, followed by fitting 3D face model or regression model to estimate head pose. Different from the existing methods, in this paper, we propose a unified method, called Coupled Cascade Regression (CCR), for simultaneous facial landmark detection and head pose estimation. At each cascade level, two separate regressors are learned to update the landmark locations and 3D face model parameters based on the local appearance features, respectively. Since 2D facial landmark locations and head pose parameters are related, we further apply the projection model to refine the prediction results in each cascade iteration and make them consistent. As a result, CCR can leverage both the learning methods and the projection model to simultaneously perform facial landmark detection and pose estimation to enhance the performances of both tasks. Experimental results on 300-W and BU datasets indicate that our proposed CCR method outperforms many conventional methods both for landmark detection and head pose estimation.},
keywords={face recognition;image enhancement;iterative methods;pose estimation;regression analysis;solid modelling;stereo image processing;regression model;cascade level;3D face model parameters;2D facial landmark locations;projection model;cascade iteration;learning methods;CCR method;facial landmark detection;Coupled Cascade Regression;head pose estimation;Solid modeling;Three-dimensional displays;Face;Pose estimation;Two dimensional displays;Training;Facial landmark detection;head pose estimation;coupled cascade regression},
doi={10.1109/ICIP.2017.8296814},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{8099972,
author={L. {Sheng} and J. {Cai} and T. {Cham} and V. {Pavlovic} and K. N. {Ngan}},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={A Generative Model for Depth-Based Robust 3D Facial Pose Tracking},
year={2017},
volume={},
number={},
pages={4598-4607},
abstract={We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face models visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.},
keywords={face recognition;object tracking;pose estimation;ray tracing;statistical analysis;generative model;arbitrary facial expression variations;generative framework;face models visibility;depth-based robust 3D facial pose tracking;unconstrained scenarios;face model adaptation;statistical 3D face model;uncertainty prediction;distribution prediction;ray visibility constraint;Biwi datasets;ICT-3DHP datasets;Face;Three-dimensional displays;Solid modeling;Adaptation models;Robustness;Shape;Probabilistic logic},
doi={10.1109/CVPR.2017.489},
ISSN={1063-6919},
month={July},}
@INPROCEEDINGS{8237379,
author={A. S. {Jackson} and A. {Bulat} and V. {Argyriou} and G. {Tzimiropoulos}},
booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
title={Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression},
year={2017},
volume={},
number={},
pages={1031-1039},
abstract={3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Code and models will be made available at http://aaronsplace.co.uk.},
keywords={computer vision;face recognition;image reconstruction;neural nets;pose estimation;regression analysis;facial expressions;reconstruction quality;facial landmark localization;single 2D image;volumetric representation;direct regression;simple CNN architecture;3D Morphable Model;nonvisible parts;whole 3D facial geometry;arbitrary facial poses;single 2D facial image;CNN works;Convolutional Neural Network;general these methods;nonuniform illumination;dense correspondence;methodological challenges;multiple facial images;fundamental Computer Vision problem;direct volumetric CNN regression;single image;pose 3D;Three-dimensional displays;Face;Two dimensional displays;Image reconstruction;Geometry;Shape;Optimization},
doi={10.1109/ICCV.2017.117},
ISSN={2380-7504},
month={Oct},}
@INPROCEEDINGS{7358788,
author={P. {Dou} and L. {Zhang} and Y. {Wu} and S. K. {Shah} and I. A. {Kakadiaris}},
booktitle={2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)},
title={Pose-robust face signature for multi-view face recognition},
year={2015},
volume={},
number={},
pages={1-8},
abstract={Despite the great progress achieved in unconstrained face recognition, pose variations still remain a challenging and unsolved practical issue. We propose a novel framework for multi-view face recognition based on extracting and matching pose-robust face signatures from 2D images. Specifically, we propose an efficient method for monocular 3D face reconstruction, which is used to lift the 2D facial appearance to a canonical texture space and estimate the self-occlusion. On the lifted facial texture we then extract various local features, which are further enhanced by the occlusion encodings computed on the self-occlusion mask, resulting in a pose-robust face signature, a novel feature representation of the original 2D facial image. Extensive experiments on two public datasets demonstrate that our method not only simplifies the matching of multi-view 2D facial images by circumventing the requirement for pose-adaptive classifiers, but also achieves superior performance.},
keywords={face recognition;feature extraction;image matching;image reconstruction;image texture;pose estimation;multiview face recognition;unconstrained face recognition;pose variation;pose-robust face signature extraction;pose-robust face signature matching;monocular 3D face reconstruction;2D facial appearance;canonical texture space;self-occlusion estimation;facial texture;feature extraction;occlusion encoding;self-occlusion mask;feature representation;multiview 2D facial image matching;Image reconstruction;Handheld computers;Three-dimensional displays},
doi={10.1109/BTAS.2015.7358788},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8354126,
author={M. {Kowalski} and Z. {Nasarzewski} and G. {Galinski} and P. {Garbat}},
booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
title={HoloFace: Augmenting Human-to-Human Interactions on HoloLens},
year={2018},
volume={},
number={},
pages={141-149},
abstract={We present HoloFace, an open-source framework for face alignment, head pose estimation and facial attribute retrieval for Microsoft HoloLens. HoloFace implements two state-of-the-art face alignment methods which can be used interchangeably: one running locally and one running on a remote backend. Head pose estimation is accomplished by fitting a deformable 3D model to the landmarks localized using face alignment. The head pose provides both the rotation of the head and a position in the world space. The parameters of the fitted 3D face model provide estimates of facial attributes such as mouth opening or smile. Together the above information can be used to augment the faces of people seen by the HoloLens user, and thus their interaction. Potential usage scenarios include facial recognition, emotion recognition, eye gaze tracking and many others. We demonstrate the capabilities of our framework by augmenting the faces of people seen through the HoloLens with various objects and animations.},
keywords={emotion recognition;face recognition;pose estimation;solid modelling;human-to-human interactions;open-source framework;facial attribute retrieval;Microsoft HoloLens;remote backend;deformable 3D model;fitted 3D face model;facial attributes;mouth opening;HoloLens user;facial recognition;face alignment methods;HoloFace framework;Three-dimensional displays;Head;Facial animation;Cameras;Headphones;Solid modeling;Magnetic heads},
doi={10.1109/WACV.2018.00022},
ISSN={},
month={March},}
@INPROCEEDINGS{8125644,
author={A. V. {Kumar} and V. V. R. {Prasad} and K. M. {Bhurchandi} and V. R. {Satpute} and L. {Pious} and S. {Kar}},
booktitle={2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)},
title={Dense reconstruction of 3D human face using 5 images and no reference model},
year={2017},
volume={},
number={},
pages={1185-1190},
abstract={3D reconstruction of human faces is of high importance in applications like forensics, facial features based human tracking and realistic reconstruction of human faces in virtual reality applications. The published algorithms so far require either a large number of views of a human face or a 3D facial reference model. This paper proposes a technique for 3D human face reconstruction using only five views without any reference model unlike most of the contemporary facial reconstruction techniques. Face localization is done followed by facial feature point extraction in the facial images. The features are tracked in the other images using the Kanade-Lucas-Tomasi (KLT) object tracking algorithm. 3D location of each feature is calculated in all the images using triangulation and a 3D point cloud is formed. The Point cloud is processed to remove outliers and holes. The 3D face is then formed by interpolating and meshing the point cloud. This computationally efficient and low cost technique outperforms commercial and online available software and published algorithms.},
keywords={face recognition;feature extraction;image reconstruction;object tracking;solid modelling;virtual reality;dense reconstruction;facial features;human tracking;3D facial reference model;contemporary facial reconstruction techniques;face localization;facial feature point extraction;facial images;point cloud;3D human face;Three-dimensional displays;Face;Image reconstruction;Feature extraction;Skin;Image color analysis;Cameras;3D Reconstruction;meshing;point cloud;tracking;triangulation},
doi={10.1109/CoDIT.2017.8125644},
ISSN={},
month={April},}
@INPROCEEDINGS{7351287,
author={C. {Tortorici} and N. {Werghi} and S. {Berretti}},
booktitle={2015 IEEE International Conference on Image Processing (ICIP)},
title={Boosting 3D LBP-based face recognition by fusing shape and texture descriptors on the mesh},
year={2015},
volume={},
number={},
pages={2670-2674},
abstract={In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBP) for 3D face recognition. Using the framework proposed in [1], we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared to its depth-image counterpart, our approach is distinguished by the following features: a) inherits the intrinsic advantages of mesh surface; b) does not require normalization; c) can accommodate partial matching. In addition, it allows early-level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data.},
keywords={face recognition;image fusion;image matching;image texture;3D LBP-based face recognition;shape descriptor fusion;texture descriptor fusion;local binary patterns;face mesh surface;global description;partial description;partial matching;early-level fusion;texture modality;shape modality;BU-3DFE database;Bosphorus database;facial expressions;Face;Shape;Histograms;Three-dimensional displays;Face recognition;Databases;Manifolds;mesh-LBP;fusion;3D face recognition},
doi={10.1109/ICIP.2015.7351287},
ISSN={},
month={Sep.},}
@ARTICLE{7932891,
author={C. {Ferrari} and G. {Lisanti} and S. {Berretti} and A. D. {Bimbo}},
journal={IEEE Transactions on Multimedia},
title={A Dictionary Learning-Based 3D Morphable Shape Model},
year={2017},
volume={19},
number={12},
pages={2666-2679},
abstract={Face analysis from 2D images and videos is a central task in many multimedia applications. Methods developed to this end perform either face recognition or facial expression recognition, and in both cases results are negatively influenced by variations in pose, illumination, and resolution of the face. Such variations have a lower impact on 3D face data, which has given the way to the idea of using a 3D morphable model as an intermediate tool to enhance face analysis on 2D data. In this paper, we propose a new approach for constructing a 3D morphable shape model (called DL-3DMM) and show our solution can reach the accuracy of deformation required in applications where fine details of the face are concerned. For constructing the model, we start from a set of 3D face scans with large variability in terms of ethnicity and expressions. Across these training scans, we compute a point-topoint dense alignment, which is accurate also in the presence of topological variations of the face. The DL-3DMM is constructed by learning a dictionary of basis components on the aligned scans. The model is then fitted to 2D target faces using an efficient regularized ridge-regression guided by 2D/3D facial landmark correspondences in order to generate pose-normalized face images. Comparison between the DL-3DMM and the standard PCA-based 3DMM demonstrates that in general a lower reconstruction error can be obtained with our solution. Application to action unit detection and emotion recognition from 2D images and videos shows competitive results with state of the art methods on two benchmark datasets.},
keywords={emotion recognition;face recognition;image enhancement;image morphing;image reconstruction;learning (artificial intelligence);multimedia systems;principal component analysis;regression analysis;solid modelling;video signal processing;dictionary learning;3D morphable shape model;face recognition;facial expression recognition;3D morphable model;DL-3DMM;pose-normalized face images;multimedia;face analysis enhancement;regularized ridge-regression;2D images;2D videos;Three-dimensional displays;Face recognition;Shape;Machine learning;Emotion recognition;Action unit detection;dictionary learning;dense correspondence;emotion recognition;3D morphable model},
doi={10.1109/TMM.2017.2707341},
ISSN={1520-9210},
month={Dec},}
@INPROCEEDINGS{8272746,
author={N. {Srinivas} and R. {Tokola} and A. {Mikkilineni} and I. {Nookaew} and M. {Leuze} and C. {Boehnen}},
booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)},
title={DNA2FACE: An approach to correlating 3D facial structure and DNA},
year={2017},
volume={},
number={},
pages={590-598},
abstract={In this paper we introduce the concept of correlating genetic variations in an individual's specific genetic code (DNA) and facial morphology. This is the first step in the research effort to estimate facial appearance from DNA samples, which is gaining momentum within intelligence, law enforcement and national security communities. The dataset for the study consisting of genetic data and 3D facial scans (phenotype) data was obtained through the FaceBase Consortium. The proposed approach has three main steps: phenotype feature extraction from 3D face images, genotype feature extraction from a DNA sample, and genome-wide association analysis to determine genetic variations that contribute to facial structure and appearance. Results indicate that there exist significant correlations between genetic information and facial structure. We have identified 30 single nucleotide polymorphisms (SNPs), i.e. genetic variations, that significantly contribute to facial structure and appearance. We conclude with a preliminary attempt at facial reconstruction from the genetic data and emphasize on the complexity of the problem and the challenges encountered.},
keywords={biology computing;DNA;face recognition;feature extraction;genetics;genomics;molecular biophysics;national security;polymorphism;DNA2FACE;correlating 3D;facial structure;genetic variations;facial appearance;DNA sample;genetic data;3D facial scans data;phenotype feature extraction;3D face images;genotype feature extraction;genetic information;facial reconstruction;Face;DNA;Three-dimensional displays;Genomics;Bioinformatics;Morphology},
doi={10.1109/BTAS.2017.8272746},
ISSN={2474-9699},
month={Oct},}
@INPROCEEDINGS{7533097,
author={B. {Peng} and W. {Wang} and J. {Dong} and T. {Tan}},
booktitle={2016 IEEE International Conference on Image Processing (ICIP)},
title={Automatic detection of 3D lighting inconsistencies via a facial landmark based morphable model},
year={2016},
volume={},
number={},
pages={3932-3936},
abstract={Existing 3D lighting consistency based forensic methods have some practical problems. They usually require additional images and human labor to reconstruct the 3D face model for lighting estimation, and furthermore, they cannot deal with expressional faces effectively. These drawbacks make them unusable in many practical cases. In this paper, we propose a more practical 3D lighting based forensic method by incorporating a facial landmark based 3D morphable model to efficiently fit the face shape. We also introduce a residual error based algorithm to automatically exclude outliers in lighting estimation. Our proposed method is fully automatic and very efficient compared to previous ones. Also, it does not depend on additional images and has better performance for expressional faces. Experiments on a realistic face dataset with variational lighting conditions indicate the efficacy and superiority of our method.},
keywords={face recognition;image forensics;image reconstruction;3D lighting inconsistencies;automatic detection;facial landmark based morphable model;human labor;3D lighting consistency based forensic methods;expressional faces;face reconstruction;realistic face dataset;variational lighting conditions;Face;Three-dimensional displays;Lighting;Shape;Solid modeling;Forgery;Estimation;Image forensics;face splicing;lighting consistency;3D morphable model},
doi={10.1109/ICIP.2016.7533097},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{7428566,
author={J. {Liu} and Q. {Zhang} and C. {Tang}},
booktitle={2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
title={CoMES: A novel method for robust nose tip detection in face range images},
year={2015},
volume={},
number={},
pages={309-315},
abstract={As the most distinct feature point in facial landmarks, nose tip plays a significant role in 3D facial studies. Successful detection of nose tip can facilitate many 3D facial studies tasks. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, need not training, can handle large rotations and occlusions. We first remove small isolated connected regions and noise from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, we compute multi-angle energy of each point, then we use hierarchical clustering method to cluster the points whose multi-angle energies are larger than a threshold value. In the largest cluster, we can find one point with the largest multi-angle energy. For all scales of the scale-space, we get a series of such points and apply hierarchical clustering again for these points, nose tip will have the largest multi-angle energy in the largest cluster. We evaluate our method in FRGC v2.0 3D face database and BOSPHORUS 3D face database. The experimental results verify the robustness of our method with a high nose tip detection rate.},
keywords={edge detection;face recognition;pattern clustering;solid modelling;CoMES;robust nose tip detection;face range images;facial landmark;3D facial studies;scale-space;multiangle energy;hierarchical clustering method;FRGC v2.0 3D face database;BOSPHORUS 3D face database;Nose;Face;Three-dimensional displays;Training;Robustness;Feature extraction;Smoothing methods;nose tip;3D faces;scale-space;multi-angle energy;hierarchical clustering;range images},
doi={10.1109/IAEAC.2015.7428566},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8661454,
author={A. {Camarena-Ibarrola} and M. {Castro-Coria} and K. {Figueroa}},
booktitle={2018 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)},
title={Cloud Point Matching for Text-Independent Speaker Identification},
year={2018},
volume={},
number={},
pages={1-6},
abstract={In Text-Independent speaker identification, the individual that produced some captured speech signal has to be identified without his collaboration, he might not even know that he is being the subject of an identification process. The system could not ask the individual to utter some specific word or phrase, which is precisely what is done in Text-Dependent speaker recognition. Text-Independent speaker identification is far more complicated since we cannot simply measure the similarity of an utterance of a word or phrase to another utterance made by the same speaker of the same word or phrase in which case we could use the dynamics of the speech signal. In this paper we search in the speech signal looking for voiced speech segments and estimate its first three formants, so we end up with a three-dimensional point cloud for each speaker of the collection of known speakers. To identify a speaker we have to measure the similarity of a point-cloud from an unknown speaker to the point-clouds that belong to known speakers, we do that by searching for local structures in the cloud in a way that is highly scalable and robust. We performed tests with both a collection of our own in Spanish and with the English Language Speech Database for Speaker Recognition (ELSDSR) from the Technical University of Denmark achieving results that improve recent published work with ELSDSR.},
keywords={speaker recognition;speech processing;cloud point matching;captured speech signal;three-dimensional point cloud;text-independent speaker identification;text-dependent speaker recognition;English Language Speech Database for Speaker Recognition;Technical University of Denmark;Three-dimensional displays;Speaker recognition;Feature extraction;Correlation;Mel frequency cepstral coefficient;Speech processing;Databases},
doi={10.1109/ROPEC.2018.8661454},
ISSN={2573-0770},
month={Nov},}
@INPROCEEDINGS{7163161,
author={S. {Cheng} and I. {Marras} and S. {Zafeiriou} and M. {Pantic}},
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
title={Active nonrigid ICP algorithm},
year={2015},
volume={1},
number={},
pages={1-8},
abstract={The problem of fitting a 3D facial model to a 3D mesh has received a lot of attention the past 15-20 years. The majority of the techniques fit a general model consisting of a simple parameterisable surface or a mean 3D facial shape. The drawback of this approach is that is rather difficult to describe the non-rigid aspect of the face using just a single facial model. One way to capture the 3D facial deformations is by means of a statistical 3D model of the face or its parts. This is particularly evident when we want to capture the deformations of the mouth region. Even though statistical models of face are generally applied for modelling facial intensity, there are few approaches that fit a statistical model of 3D faces. In this paper, in order to capture and describe the non-rigid nature of facial surfaces we build a part-based statistical model of the 3D facial surface and we combine it with non-rigid iterative closest point algorithms. We show that the proposed algorithm largely outperforms state-of-the-art algorithms for 3D face fitting and alignment especially when it comes to the description of the mouth region.},
keywords={face recognition;statistical analysis;facial surfaces;iterative closest point algorithm;nonrigid iterative closest point algorithms;3D face fitting;mouth region;statistical 3D model;3D facial deformation;single facial model;3D facial shape;3D mesh;3D facial model;active nonrigid ICP algorithm;Face;Three-dimensional displays;Iterative closest point algorithm;Solid modeling;Deformable models;Shape;Mouth},
doi={10.1109/FG.2015.7163161},
ISSN={},
month={May},}
@INPROCEEDINGS{8095327,
author={S. {Bentaieb} and A. {Ouamri} and M. {Keche} and A. {Nait-Ali}},
booktitle={2017 2nd International Conference on Bio-engineering for Smart Technologies (BioSMART)},
title={Gender classification from 3D face images using multi-task sparse representation over reduced dictionary},
year={2017},
volume={},
number={},
pages={1-4},
abstract={In this paper, we address the problem of gender classification based on facial images. The Speeded Up Robust Feature (SURF) algorithm descriptors are used as features to built dictionaries and a multi-task Sparse Representation Classification (SRC) is used as classifier to determine the gender of an individual face. Our approach uses smaller and compact dictionaries by removing the redundant atoms from the constructed ones. The feasibility of using the SURF on the shape index map for gender classification is demonstrated through experimental investigation conducted on FRGCv2 dataset. The proposed approach achieves 91.04±1.19% of correct gender classification rate using only 5% of the size of the dictionary and 97.83 ± 0.76% is obtained using 23% of the size of the dictionary.},
keywords={face recognition;feature extraction;image classification;image representation;shape recognition;3D face images;reduced dictionary;speeded up robust feature algorithm descriptors;SURF algorithm descriptors;multitask SRC;shape index map;gender classification rate;multitask Sparse Representation Classification;Dictionaries;Face;Three-dimensional displays;Shape;Indexes;Training;Nose},
doi={10.1109/BIOSMART.2017.8095327},
ISSN={},
month={Aug},}
@ARTICLE{7524772,
author={C. {Benedek} and B. {Gálai} and B. {Nagy} and Z. {Jankó}},
journal={IEEE Transactions on Circuits and Systems for Video Technology},
title={Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance System},
year={2018},
volume={28},
number={1},
pages={101-113},
abstract={This paper presents new approaches for gait and activity analysis based on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed algorithms are embedded into an integrated 4D vision and visualization system, which is able to analyze and interactively display real scenarios in natural outdoor environments with walking pedestrians. The main focus of the investigations is gait-based person reidentification during tracking and recognition of specific activity patterns, such as bending, waving, making phone calls, and checking the time looking at wristwatches. The descriptors for training and recognition are observed and extracted from realistic outdoor surveillance scenarios, where multiple pedestrians are walking in the field of interest following possibly intersecting trajectories; thus, the observations might often be affected by occlusions or background noise. Since there is no public database available for such scenarios, we created and published a new Lidar-based outdoor gait and activity data set on our website that contains point cloud sequences of 28 different persons extracted and aggregated from 35-min-long measurements. The presented results confirm that both efficient gait-based identification and activity recognition are achievable in the sparse point clouds of a single RMB Lidar sensor. After extracting the people trajectories, we synthesized a free-viewpoint video, in which moving avatar models follow the trajectories of the observed pedestrians in real time, ensuring that the leg movements of the animated avatars are synchronized with the real gait cycles observed in the Lidar stream.},
keywords={gait analysis;image motion analysis;image recognition;optical radar;gait analysis;activity recognition;4D surveillance system;walking pedestrians;person reidentification;point cloud sequences;sparse point clouds;gait cycles;Lidar stream;multibeam Lidar sensor;RMB Lidar sensor;Laser radar;Three-dimensional displays;Trajectory;Surveillance;Legged locomotion;Databases;Visualization;4D reconstruction;activity recognition;gait recognition;multibeam Lidar},
doi={10.1109/TCSVT.2016.2595331},
ISSN={1051-8215},
month={Jan},}
@INPROCEEDINGS{8265511,
author={H. A. {Le} and I. A. {Kakadiaris}},
booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
title={UHDB31: A Dataset for Better Understanding Face Recognition Across Pose and Illumination Variation},
year={2017},
volume={},
number={},
pages={2555-2563},
abstract={Face datasets are a fundamental tool to analyze the performance of face recognition algorithms. However, the accuracy achieved on current benchmark datasets is saturated. Although multiple face datasets have been published recently, they only focus on the number of samples and lack diversity on facial appearance factors, such as pose and illumination. In addition, while 3D data have been demonstrated improved face recognition accuracy by a significant margin, only a few 3D face datasets provide high quality 2D and 3D data. In this paper, we introduce a new and challenging dataset, called UHDB31, which not only allows direct measurement of the influence of pose, illumination, and resolution on face recognition but also facilitates different experimental configurations with both 2D and 3D data. We conduct a series of experiments with various face recognition algorithms and point out how far they are from solving the face recognition problem under pose, illumination, and resolution variation. The dataset is publicly available and free for research use1.},
keywords={face recognition;pose estimation;face recognition algorithms;multiple face datasets;facial appearance factors;face recognition accuracy;pose illumination;face recognition problem;benchmark datasets;UHDB31;illumination variation;3D face datasets;3D data;Three-dimensional displays;Face;Face recognition;Two dimensional displays;Lighting;Image resolution;Cameras},
doi={10.1109/ICCVW.2017.300},
ISSN={2473-9944},
month={Oct},}
@INPROCEEDINGS{8265506,
author={D. {Crispell} and M. {Bazik}},
booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
title={Pix2Face: Direct 3D Face Model Estimation},
year={2017},
volume={},
number={},
pages={2512-2518},
abstract={An efficient, fully automatic method for 3D face shape and pose estimation in unconstrained 2D imagery is presented. The proposed method jointly estimates a dense set of 3D landmarks and facial geometry using a single pass of a modified version of the popular "U-Net" neural network architecture. Additionally, we propose a method for directly estimating a set of 3D Morphable Model (3DMM) parameters, using the estimated 3D landmarks and geometry as constraints in a simple linear system. Qualitative modeling results are presented, as well as quantitative evaluation of predicted 3D face landmarks in unconstrained video sequences.},
keywords={face recognition;image sequences;neural net architecture;pose estimation;solid modelling;video signal processing;qualitative modeling;U-Net neural network architecture;unconstrained video sequences;facial geometry;dense set;unconstrained 2D imagery;3D face shape;fully automatic method;direct 3D face Model estimation;pix2face;predicted 3D face landmarks;3DMM;3D Morphable Model parameters;Three-dimensional displays;Face;Solid modeling;Geometry;Shape;Estimation;Cameras},
doi={10.1109/ICCVW.2017.295},
ISSN={2473-9944},
month={Oct},}
@INPROCEEDINGS{7126355,
author={X. {Gong} and Z. {Fu} and X. {Li} and L. {Feng}},
booktitle={IEEE International Conference on Identity, Security and Behavior Analysis (ISBA 2015)},
title={A two-stage estimation method for depth estimation of facial landmarks},
year={2015},
volume={},
number={},
pages={1-6},
abstract={To address the problem of 3D face modeling based on a set of landmarks on images, the traditional feature-based morphable model, using face class-specific information, makes direct use of these 2D points to infer a dense 3D face surface. However, the unknown depth of landmarks degrades accuracy considerably. A promising solution is to predict the depth of landmarks at first. Bases on this idea, a two-stage estimation method is proposed to compute the depth value of landmarks from two images. And then, the estimated 3D landmarks are applied to a deformation algorithm to make a precise 3D dense facial shape. Test results on synthesized images with known ground-truth show that the proposed two-stage estimation method can obtain landmarks' depth both effectively and efficiently, and further that the reconstructed accuracy is greatly enhanced with the estimated 3D landmarks. Reconstruction results of real-world photos are rather realistic.},
keywords={face recognition;image reconstruction;two-stage estimation method;depth estimation;facial landmarks;3D face modeling;feature-based morphable model;face class-specific information;dense 3D face surface;deformation algorithm;precise 3D dense facial shape;synthesized images;3D landmarks;image reconstruction;Face;Three-dimensional displays;Estimation;Solid modeling;Shape;Image reconstruction;Computational modeling},
doi={10.1109/ISBA.2015.7126355},
ISSN={},
month={March},}
@INPROCEEDINGS{8373832,
author={G. {Zhang} and H. {Han} and S. {Shan} and X. {Song} and X. {Chen}},
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
title={Face Alignment across Large Pose via MT-CNN Based 3D Shape Reconstruction},
year={2018},
volume={},
number={},
pages={210-217},
abstract={Face alignment plays an important role for robust face recognition and analysis applications in the wild. While a number of face alignment methods are available, large-pose face alignment remains a very challenging problem due to the ambiguity of facial keypoints in 2D face images. Recent attempts to solve this problem via 3D model fitting show more robustness against large poses and 2D ambiguity, but their accuracy and speed are still limited. We propose a 3D reconstruction based method to quickly and accurately detect 2D facial landmarks and estimate their visibilities. By designing a cascaded multi-task CNN model, we can efficiently reconstruct the 3D face shape, together with pose estimation as an auxiliary task. Finally, the landmarks on 3D shape are projected to the 2D face image to get the 2D landmarks and their visibilities. Experimental results on the challenging 300W-LP, AFLW2000-3D, and AFLW databases show that the proposed approach can be comparable with the state-of-the-art methods and is able to run in real time (32ms per image) on 3.4 GHz CPU.},
keywords={face recognition;image reconstruction;neural nets;pose estimation;shape recognition;stereo image processing;3D face shape;pose estimation;2D face image;AFLW2000-3D;MT-CNN based 3D shape reconstruction;robust face recognition;face alignment methods;large-pose face alignment;facial keypoints;2D facial landmarks;cascaded multitask CNN model;time 32.0 ms;frequency 3.4 GHz;Face;Three-dimensional displays;Shape;Two dimensional displays;Task analysis;Image reconstruction;Principal component analysis;Landmark detection;3D shape regression;multitask CNN;large-pose face alignment},
doi={10.1109/FG.2018.00039},
ISSN={},
month={May},}
@INPROCEEDINGS{7406362,
author={S. {Tulyakov} and R. {Vieriu} and E. {Sangineto} and N. {Sebe}},
booktitle={2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
title={FaceCept3D: Real Time 3D Face Tracking and Analysis},
year={2015},
volume={},
number={},
pages={28-33},
abstract={We present an open source cross platform technology for 3D face tracking and analysis. It contains a full stack of components for complete face understanding: detection, head pose tracking, facial expression and action units recognition. Given a depth sensor, one can combine FaceCept3D modules to fulfill a specific application scenario. Key advantages of the technology include real time processing speed and ability to handle extreme head pose variations. Possible application areas of the technology range from human computer interaction to active aging, where precise and real-time analysis is required. The technology is available to community.},
keywords={face recognition;human computer interaction;pose estimation;real-time systems;real time 3D face tracking;open source cross platform technology;complete face understanding;head pose tracking;facial expression;action units recognition;depth sensor;FaceCept3D modules;extreme head pose variations;human computer interaction;active aging;HCI;Face;Three-dimensional displays;Face recognition;Feature extraction;Magnetic heads;Iterative closest point algorithm},
doi={10.1109/ICCVW.2015.13},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8265400,
author={Y. {Liu} and A. {Jourabloo} and W. {Ren} and X. {Liu}},
booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
title={Dense Face Alignment},
year={2017},
volume={},
number={},
pages={1619-1628},
abstract={Face alignment is a classic problem in the computer vision field. Previous works mostly focus on sparse alignment with a limited number of facial landmark points, i.e., facial landmark detection. In this paper, for the first time, we aim at providing a very dense 3D alignment for large-pose face images. To achieve this, we train a CNN to estimate the 3D face shape, which not only aligns limited facial landmarks but also fits face contours and SIFT feature points. Moreover, we also address the bottleneck of training CNN with multiple datasets, due to different landmark markups on different datasets, such as 5, 34, 68. Experimental results show our method not only provides high-quality, dense 3D face fitting but also outperforms the state-of-the-art facial landmark detection methods on challenging datasets. Our model can run at real time during testing and it's available at http:///cvlab.cse.msu.edu/project-pifa.html.},
keywords={computer vision;convolution;face recognition;feature extraction;feedforward neural nets;shape recognition;stereo image processing;dense face alignment;computer vision field;sparse alignment;facial landmark points;dense 3D alignment;face images;3D face shape;facial landmarks;face contours;training CNN;multiple datasets;dense 3D face fitting;facial landmark detection methods;challenging datasets;landmark markups;SIFT feature points;Face;Three-dimensional displays;Shape;Solid modeling;Two dimensional displays;Labeling;Training},
doi={10.1109/ICCVW.2017.190},
ISSN={2473-9944},
month={Oct},}
@INPROCEEDINGS{7780824,
author={J. {Roth} and Y. {Tong} and X. {Liu}},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Adaptive 3D Face Reconstruction from Unconstrained Photo Collections},
year={2016},
volume={},
number={},
pages={4197-4206},
abstract={Given a collection of "in-the-wild" face images captured under a variety of unknown pose, expression, and illumination conditions, this paper presents a method for reconstructing a 3D face surface model of an individual along with albedo information. Motivated by the success of recent face reconstruction techniques on large photo collections, we extend prior work to adapt to low quality photo collections with fewer images. We achieve this by fitting a 3D Morphable Model to form a personalized template and developing a novel photometric stereo formulation, under a coarse-to-fine scheme. Superior experimental results are reported on synthetic and real-world photo collections.},
keywords={face recognition;image capture;image reconstruction;stereo image processing;unconstrained photo collections;adaptive 3D face reconstruction;in-the-wild face image capture;unknown pose;illumination conditions;3D face surface reconstruction;albedo information;low quality photo collections;3D morphable model;personalized template;photometric stereo formulation;coarse-to-fine scheme;synthetic photo collections;real-world photo collections;Face;Image reconstruction;Three-dimensional displays;Surface reconstruction;Solid modeling;Two dimensional displays;Computational modeling},
doi={10.1109/CVPR.2016.455},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{8014991,
author={S. {Xiao} and J. {Li} and Y. {Chen} and Z. {Wang} and J. {Feng} and S. {Yan} and A. {Kassim}},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
title={3D-Assisted Coarse-to-Fine Extreme-Pose Facial Landmark Detection},
year={2017},
volume={},
number={},
pages={2060-2068},
abstract={We propose a novel 3D-assisted coarse-to-fine extreme-pose facial landmark detection system in this work. For a given face image, our system first refines the face bounding box with landmark locations inferred from a 3D face model generated by a Recurrent 3D Regressor at coarse level. Another R3R is then employed to fit a 3D face model onto the 2D face image cropped with the refined bounding box at fine-scale. 2D landmark locations inferred from the fitted 3D face are further adjusted with the popular 2D regression method, i.e. LBF. The 3D-assisted coarse-to-fine strategy and the 2D adjustment process explicitly ensure both the robustness to extreme face poses and bounding box disturbance and the accuracy towards pixel-level landmark displacement. Extensive experiments on the Menpo Challenge test sets demonstrate the superior performance of our system.},
keywords={face recognition;object detection;solid modelling;3D-assisted coarse-to-fine extreme-pose facial landmark detection;face image;3D face model;recurrent 3D regressor;2D face image;bounding box;2D landmark locations;2D adjustment process;Menpo Challenge test sets;face pose;Face;Three-dimensional displays;Two dimensional displays;Solid modeling;Feature extraction;Detectors;Robustness},
doi={10.1109/CVPRW.2017.257},
ISSN={2160-7516},
month={July},}
@INPROCEEDINGS{7961793,
author={T. {Hosoi}},
booktitle={2017 12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)},
title={Head Pose and Expression Transfer Using Facial Status Score},
year={2017},
volume={},
number={},
pages={573-580},
abstract={We propose a method to transfer both head poseand facial expression of a source person in a video to the faceof a target person in an output video. Our method models theentire 2D frame instead of the 3D face, and it generates outputresults using a status score, which includes the relative facialstatus about the head pose and expression in a frame. From thetarget video, the learning process obtains frame features neededfor moving to each frame from the neutral frame for all frames,and generates the basis of these features via principal componentanalysis (PCA). Then, it learns to generate these features from agiven status score sequentially. In the transfer process, it obtainsa status score from a source frame of the video and generatesthe features from the given status score. Then, it generates theoutput frame using the reconstructed features. An output videois generated by repeating these steps for each source frame.Our method generates output results on the trajectory of thetarget video by using the advantage of PCA. Therefore, in theoutput results generated by our methods, both head pose andexpression are transferred correctly while the non-face regionsof the frames are supported. Finally, we experimentally comparethe effectiveness of our method and conventional methods.},
keywords={face recognition;learning (artificial intelligence);pose estimation;principal component analysis;stereo image processing;video signal processing;head pose;expression transfer;facial status score;output video;3D face;learning process;principal component analysis;PCA;Face;Shape;Three-dimensional displays;Solid modeling;Principal component analysis;Deformable models},
doi={10.1109/FG.2017.142},
ISSN={},
month={May},}
@INPROCEEDINGS{8250221,
author={F. H. d. B. {Zavan} and N. {Gasparin} and J. C. {Batista} and L. P. e. {Silva} and V. {Albiero} and O. R. P. {Bellon} and L. {Silva}},
booktitle={2017 30th SIBGRAPI Conference on Graphics, Patterns and Images Tutorials (SIBGRAPI-T)},
title={Face Analysis in the Wild},
year={2017},
volume={},
number={},
pages={9-16},
abstract={With the global demand for extra security systems, and the growing of human-machine interaction, facial analysis in unconstrained environments (in the wild) became a hot-topic in recent computer vision research.Unconstrained environments include surveillance footage, social media photos and live broadcasts.This type of images and videos include no control over illumination, position, size, occlusion, and facial expressions. Successful facial processing methods for controlled scenarios are unable to pledge with challenging circumstances. Consequently, methods tailored for handling those situations are indispensable for the face analysis research progress. This work presents a comprehensive review of state-of-the-art methods, drawing attention to the complications derived from in the wild scenarios and the behavior differences when applied to the controlled images.The main topics to be covered are: (1) face detection; (2) facial image quality; (3) head pose estimation; (4) face alignment; (5) 3D face reconstruction; (6) gender and age estimation; (7) facial expressions and emotions; and (8) face recognition. Finally, available code and applications for in the wild face analysis are presented,followed by a discussion on future directions.},
keywords={computer vision;face recognition;image reconstruction;pose estimation;video surveillance;age estimation;wild face analysis;global demand;extra security systems;human-machine interaction;facial analysis;surveillance footage;social media photos;live broadcasts;videos;occlusion;gender estimation;facial expressions;face recognition;computer vision research;face detection;facial image quality;face alignment;facial processing methods;3D face reconstruction;Face;Three-dimensional displays;Face detection;Estimation;Face recognition;face analysis;face processing},
doi={10.1109/SIBGRAPI-T.2017.11},
ISSN={2474-0705},
month={Oct},}
@ARTICLE{7776921,
author={J. {Roth} and Y. {Tong} and X. {Liu}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Adaptive 3D Face Reconstruction from Unconstrained Photo Collections},
year={2017},
volume={39},
number={11},
pages={2127-2141},
abstract={Given a photo collection of “unconstrained” face images of one individual captured under a variety of unknown pose, expression, and illumination conditions, this paper presents a method for reconstructing a 3D face surface model of the individual along with albedo information. Unlike prior work on face reconstruction that requires large photo collections, we formulate an approach to adapt to photo collections with a high diversity in both the number of images and the image quality. To achieve this, we incorporate prior knowledge about face shape by fitting a 3D morphable model to form a personalized template, following by using a novel photometric stereo formulation to complete the fine details, under a coarse-to-fine scheme. Our scheme incorporates a structural similarity-based local selection step to help identify a common expression for reconstruction while discarding occluded portions of faces. The evaluation of reconstruction performance is through a novel quality measure, in the absence of ground truth 3D scans. Superior large-scale experimental results are reported on synthetic, Internet, and personal photo collections.},
keywords={face recognition;image reconstruction;shape recognition;stereo image processing;unconstrained photo collections;photo collection;3D face surface model;image quality;face shape;ground truth 3D scans;personal photo collections;structural similarity-based local selection step;coarse-to-fine scheme;photometric stereo formulation;3D morphable model fitting;albedo information;unconstrained face images;adaptive 3D face reconstruction;Image reconstruction;Face;Three-dimensional displays;Lighting;Solid modeling;Surface reconstruction;Adaptation models;Face reconstruction;photometric stereo;unconstrained},
doi={10.1109/TPAMI.2016.2636829},
ISSN={0162-8828},
month={Nov},}
@INPROCEEDINGS{7785121,
author={E. {Richardson} and M. {Sela} and R. {Kimmel}},
booktitle={2016 Fourth International Conference on 3D Vision (3DV)},
title={3D Face Reconstruction by Learning from Synthetic Data},
year={2016},
volume={},
number={},
pages={460-469},
abstract={Fast and robust three-dimensional reconstruction of facial geometric structure from a single image is a challenging task with numerous applications. Here, we introduce a learning-based approach for reconstructing a three-dimensional face from a single image. Recent face recovery methods rely on accurate localization of key characteristic points. In contrast, the proposed approach is based on a Convolutional-Neural-Network (CNN) which extracts the face geometry directly from its image. Although such deep architectures outperform other models in complex computer vision problems, training them properly requires a large dataset of annotated examples. In the case of three-dimensional faces, currently, there are no large volume data sets, while acquiring such big-data is a tedious task. As an alternative, we propose to generate random, yet nearly photo-realistic, facial images for which the geometric form is known. The suggested model successfully recovers facial shapes from real images, even for faces with extreme expressions and under various lighting conditions.},
keywords={face recognition;image reconstruction;learning (artificial intelligence);neural nets;3D face reconstruction;synthetic data;learning-based approach;face recovery methods;convolutional-neural-network;CNN;face geometry;deep architectures;complex computer vision problems;geometric form;Face;Geometry;Image reconstruction;Three-dimensional displays;Lighting;Training;Shape;3D morphable model;convolutional networks;shape from shading},
doi={10.1109/3DV.2016.56},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8634685,
author={S. A. {Ali Shah} and M. {Bennamoun} and M. {Molton}},
booktitle={2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)},
title={A Training-Free Mesh Upsampling and Morphing Technique for 3D Face Rejuvenation},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Mesh upsampling and morphing is a challenging problem due to the irregularity and sparseness of the 3D data. Unlike 2D grid of pixels, 3D points do not have any regular structure and spatial order. In this paper, we present an efficient mesh upsampling and morphing technique. The proposed technique does not not require training and does not rely on any particular upsampling model. The key idea is to select and process each mesh triangle based on a heuristic criteria to define the 3D coordinate of a new point. An interactive mesh morphing technique is also introduced to test the effectiveness of the mesh upsampling algorithm. We perform quantitative and qualitative analysis to evaluate the performance of our proposed technique. Our empirical results show that our upsampled points have better uniformity and are located closer to the underlying surfaces. The computational time analysis demonstrates that the proposed technique is very efficient. The average mesh upsampling time is only 2.11sec which makes the proposed technique suitable for real time applications. To further demonstrate the effectiveness of our proposed technique, we evaluate it for a novel task of facial rejuvenation prediction and report our preliminary results in this paper.},
keywords={face recognition;image morphing;interactive systems;mesh generation;stereo image processing;3D face rejuvenation;interactive mesh morphing technique;mesh upsampling algorithm;quantitative analysis;qualitative analysis;computational time analysis;average mesh upsampling time;free mesh upsampling training;Three-dimensional displays;Face;Task analysis;Prediction algorithms;Australia;Two dimensional displays;Geometry;Mesh Upsampling;Morphing},
doi={10.1109/IVCNZ.2018.8634685},
ISSN={2151-2205},
month={Nov},}
@ARTICLE{8618459,
author={S. {Soltanpour} and Q. M. J. {Wu}},
journal={IEEE Transactions on Image Processing},
title={Weighted Extreme Sparse Classifier and Local Derivative Pattern for 3D Face Recognition},
year={2019},
volume={28},
number={6},
pages={3020-3033},
abstract={A novel weighted hybrid classifier and a high-order, local normal derivative pattern descriptor are proposed for 3D face recognition. The local derivative pattern (LDP) captures the detailed information based on the local derivative variation in different directions. The LDP is computed on three normal maps in $x$ -, $y$ -, and $z$ -directions and on different scales. The surface normal captures the orientation of a surface at each point of 3D data. More informative local shape information is extracted using the surface normal, as compared to depth. The $n$ th-order LDP on the surface normal is proposed to encode the more detailed features from the $(n-1)$ th-order’s local derivative direction variations. An extreme learning machine (ELM)-based autoencoder, using a multilayer network structure, is employed to select more discriminant features and to provide a faster training speed. A weighted hybrid framework is proposed to handle facial challenges using a combination of the ELM and the sparse representation classifier (SRC). The advantage of speed for the ELM and the accuracy for the SRC in a weighted scheme is used to enhance the performance of the recognition system. Experimental results regarding four famous 3D face databases illustrate the generalization and effectiveness of the proposed method in terms of both computational cost and recognition accuracy.},
keywords={Three-dimensional displays;Face recognition;Feature extraction;Face;Shape;Computational efficiency;Task analysis;Face recognition;local derivative pattern;surface normals;sparse representation;extreme learning machine;weighted classifier},
doi={10.1109/TIP.2019.2893524},
ISSN={1057-7149},
month={June},}
@INPROCEEDINGS{7049881,
author={T. D. {Ngo} and T. D. {Bui}},
booktitle={The 2015 IEEE RIVF International Conference on Computing Communication Technologies - Research, Innovation, and Vision for Future (RIVF)},
title={A Vietnamese 3D taking face for embodied conversational agents},
year={2015},
volume={},
number={},
pages={94-99},
abstract={Conversational agents are receiving significant attention from multi-agent and human computer interaction research societies. Many techniques have been developed to enable these agents to behave in a human-like manner. In order to do so, they are simulated with similar communicative channels as humans. Moreover, they are also simulated with emotion and personality. In this work, we focus on issue of expressing emotions for embodied-agents. We present a three dimensional face with ability to speak emotional Vietnamese speech and naturally express emotions while talking. Our face can represent lip movements during emotionally pronouncing Vietnamese words, and at the same time it can show emotional facial expressions while speaking. The face's architecture consists of three parts: Vietnamese Emotional Speech Synthesis module, Emotions to Facial Expressions module, and Combination module which creates lip movements when pronouncing Vietnamese emotional speech and combines these movements with emotional facial expressions. We have tested the face in the football supporter domain in order to confirm its naturalness. The face is simulated as the face of a football supporter agent which experiences emotions and expresses emotional expressions in his voice as well as on his face.},
keywords={emotion recognition;human computer interaction;multi-agent systems;natural language processing;signal representation;speech synthesis;sport;Vietnamese 3D talking face;embodied conversational agents;multiagent;human computer interaction research societies;communicative channels;personality;three dimensional face;lip movement representation;emotional facial expressions;Vietnamese emotional speech synthesis module;emotion-facial expressions module;combination module;football supporter domain;football supporter agent;Face;Speech;Acoustics;Three-dimensional displays;Lips;Databases;Feature extraction;Conversational Agent;Vietnamese 3D Talking Face;Emotional Speech;Emotional Facial Expression},
doi={10.1109/RIVF.2015.7049881},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8099733,
author={S. {Saito} and L. {Wei} and L. {Hu} and K. {Nagano} and H. {Li}},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Photorealistic Facial Texture Inference Using Deep Neural Networks},
year={2017},
volume={},
number={},
pages={2326-2335},
abstract={We present a data-driven inference method that can synthesize a photorealistic texture map of a complete 3D face model given a partial 2D view of a person in the wild. After an initial estimation of shape and low-frequency albedo, we compute a high-frequency partial texture map, without the shading component, of the visible face area. To extract the fine appearance details from this incomplete input, we introduce a multi-scale detail analysis technique based on mid-layer feature correlations extracted from a deep convolutional neural network. We demonstrate that fitting a convex combination of feature correlations from a high-resolution face database can yield a semantically plausible facial detail description of the entire face. A complete and photorealistic texture map can then be synthesized by iteratively optimizing for the reconstructed feature correlations. Using these high-resolution textures and a commercial rendering framework, we can produce high-fidelity 3D renderings that are visually comparable to those obtained with state-of-the-art multi-view face capture systems. We demonstrate successful face reconstructions from a wide range of low resolution input images, including those of historical figures. In addition to extensive evaluations, we validate the realism of our results using a crowdsourced user study.},
keywords={face recognition;feature extraction;image reconstruction;image resolution;image texture;neural nets;rendering (computer graphics);photorealistic facial texture inference;deep neural networks;photorealistic texture map;multiscale detail analysis technique;mid-layer feature correlations;deep convolutional neural network;convex combination;high-resolution face database;high-resolution textures;high-fidelity 3D renderings;3D face model;face reconstructions;Face;Three-dimensional displays;Correlation;Feature extraction;Neural networks;Rendering (computer graphics);Shape},
doi={10.1109/CVPR.2017.250},
ISSN={1063-6919},
month={July},}
@INPROCEEDINGS{7139109,
author={J. {Svoboda} and M. M. {Bronstein} and M. {Drahansky}},
booktitle={2015 International Conference on Biometrics (ICB)},
title={Contactless biometric hand geometry recognition using a low-cost 3D camera},
year={2015},
volume={},
number={},
pages={452-457},
abstract={In the past decade, the interest in using 3D data for biometric person authentication has increased significantly, propelled by the availability of affordable 3D sensors. The adoption of 3D features has been especially successful in face recognition applications, leading to several commercial 3D face recognition products. In other biometric modalities such as hand recognition, several studies have shown the potential advantage of using 3D geometric information, however, no commercial-grade systems are currently available. In this paper, we present a contactless 3D hand recognition system based on the novel Intel RealSense camera, the first mass-produced embeddable 3D sensor. The small form factor and low cost make this sensor especially appealing for commercial biometric applications, however, they come at the price of lower resolution compared to more expensive 3D scanners used in previous research. We analyze the robustness of several existing 2D and 3D features that can be extracted from the images captured by the RealSense camera and study the use of metric learning for their fusion.},
keywords={cameras;feature extraction;image capture;image fusion;image resolution;learning (artificial intelligence);palmprint recognition;contactless biometric hand geometry recognition;low-cost 3D camera;biometric person authentication;3D face recognition;3D geometric information;contactless 3D hand recognition system;Intel RealSense camera;embeddable 3D sensor;image resolution;2D feature extraction;3D feature extraction;image capture;metric learning;image fusion;Three-dimensional displays;Feature extraction;Sensors;Cameras;Measurement;Biometrics (access control);Wrist},
doi={10.1109/ICB.2015.7139109},
ISSN={2376-4201},
month={May},}
@INPROCEEDINGS{8265398,
author={F. {Chang} and A. T. {Tran} and T. {Hassner} and I. {Masi} and R. {Nevatia} and G. {Medioni}},
booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
title={FacePoseNet: Making a Case for Landmark-Free Face Alignment},
year={2017},
volume={},
number={},
pages={1599-1608},
abstract={We show how a simple convolutional neural network (CNN) can be trained to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose, directly from image intensities. We further explain how this FacePoseNet (FPN) can be used to align faces in 2D and 3D as an alternative to explicit facial landmark detection for these tasks. We claim that in many cases the standard means of measuring landmark detector accuracy can be misleading when comparing different face alignments. Instead, we compare our FPN with existing methods by evaluating how they affect face recognition accuracy on the IJB-A and IJB-B benchmarks: using the same recognition pipeline, but varying the face alignment method. Our results show that (a) better landmark detection accuracy measured on the 300W benchmark does not necessarily imply better face recognition accuracy. (b) Our FPN provides superior 2D and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces at a small fraction of the computational cost of comparably accurate landmark detectors. For many purposes, FPN is thus a far faster and far more accurate face alignment method than using facial landmark detectors.},
keywords={face recognition;feedforward neural nets;learning (artificial intelligence);object detection;pose estimation;regression analysis;recognition pipeline;landmark detection accuracy;face recognition accuracy;3D face alignment;facial landmark detectors;FacePoseNet;landmark-free face;freedom 3D head;image intensities;landmark detector accuracy;convolutional neural network training;facial landmark detection;regression;face alignment method;Face;Three-dimensional displays;Detectors;Two dimensional displays;Benchmark testing;Face recognition;Shape},
doi={10.1109/ICCVW.2017.188},
ISSN={2473-9944},
month={Oct},}
@INPROCEEDINGS{7852696,
author={F. {Li} and C. {Lai} and S. {Jin} and Y. {Peng}},
booktitle={2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
title={Automatic calibration of 3D human faces},
year={2016},
volume={},
number={},
pages={135-139},
abstract={This paper presents a method for correcting the posture of 3D face with unknown position and posture based on the standard face pose. We define a standard posture of face which is not affected by the head gesture. Then the algorithms of principal component analysis and iterative closest point are used to achieve the preliminary adjustment of the target model. For the precise calibration, we use the method of depth values iterative searching to find the point of nose precisely, and propose an algorithm of searching along a straight line to find the ridge line of nose. Ultimately we determine the difference between the posture of the target model and standard posture to obtain accurate calculation of the target model posture.},
keywords={calibration;computer graphics;face recognition;principal component analysis;automatic calibration;3D human faces;standard face pose;standard posture;head gesture;principal component analysis;iterative closest point;precise calibration;iterative searching;target model posture;Three-dimensional displays;Calibration;Nose;Standards;Iterative closest point algorithm;Solid modeling;Principal component analysis;3D Faces;Posture Correction;Face Calibration;Facial Feature Points},
doi={10.1109/CISP-BMEI.2016.7852696},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7151002,
author={V. D. {Shirke} and U. {Gawande}},
booktitle={2015 International Conference on Industrial Instrumentation and Control (ICIC)},
title={Generation of 3D face views from artist drawn sketch: A review},
year={2015},
volume={},
number={},
pages={1582-1586},
abstract={Approach to construct 3D face model from an artist drawn sketch is an area of interest in image processing from last few decades. It has various application like police investigation, 3D cartoon modeling. From an individual's sketch it is possible to construct 3D face model using various techniques. To construct 3D face views, from the individuals sketch steps required are 2D landmark detection, 3D landmark estimation, surface and texture synthesis with reference to 3D morphable model. This system is beneficial for the purpose of increasing the identification accuracy of the persons whose photographs are not available. In this paper, we tend to surveyed different techniques to construct 3D face views from artist drawn sketch.},
keywords={image texture;object detection;solid modelling;3D face views generation;3D face model;artist drawn sketch;image processing;2D landmark detection;3D landmark estimation;surface synthesis;texture synthesis;3D morphable model;Three-dimensional displays;Face;Face recognition;Computational modeling;Surface texture;Solid modeling;Analytical models;MeshIK;Principal Component Analysis;Active appearance Model;Active Shape Model;Shape from Shading},
doi={10.1109/IIC.2015.7151002},
ISSN={},
month={May},}
@INPROCEEDINGS{7358753,
author={L. {Chen} and J. {Ferryman}},
booktitle={2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)},
title={Combining 3D and 2D for less constrained periocular recognition},
year={2015},
volume={},
number={},
pages={1-6},
abstract={Periocular recognition has recently become an active topic in biometrics. Typically it uses 2D image data of the periocular region. This paper is the first description of combining 3D shape structure with 2D texture. A simple and effective technique using iterative closest point (ICP) was applied for 3D periocular region matching. It proved its strength for relatively unconstrained eye region capture, and does not require any training. Local binary patterns (LBP) were applied for 2D image based periocular matching. The two modalities were combined at the score-level. This approach was evaluated using the Bosphorus 3D face database, which contains large variations in facial expressions, head poses and occlusions. The rank-1 accuracy achieved from the 3D data (80%) was better than that for 2D (58%), and the best accuracy (83%) was achieved by fusing the two types of data. This suggests that significant improvements to periocular recognition systems could be achieved using the 3D structure information that is now available from small and inexpensive sensors.},
keywords={face recognition;image matching;image texture;iterative methods;rendering (computer graphics);periocular recognition;biometrics;2D image data;3D shape structure;2D texture;iterative closest point;ICP;eye region capture;local binary pattern;periocular matching;Bosphorus 3D face database;facial expression;head poses;occlusion;Three-dimensional displays;Face;Face recognition;Iterative closest point algorithm;Probes;Databases},
doi={10.1109/BTAS.2015.7358753},
ISSN={},
month={Sep.},}
@ARTICLE{8024025,
author={C. {Ye} and X. {Qian}},
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
title={3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired},
year={2018},
volume={26},
number={2},
pages={441-450},
abstract={This paper presents a 3-D object recognition method and its implementation on a robotic navigation aid to allow real-time detection of indoor structural objects for the navigation of a blind person. The method segments a point cloud into numerous planar patches and extracts their inter-plane relationships (IPRs).Based on the existing IPRs of the object models, the method defines six high level features (HLFs) and determines the HLFs for each patch. A Gaussian-mixture-model-based plane classifier is then devised to classify each planar patch into one belonging to a particular object model. Finally, a recursive plane clustering procedure is used to cluster the classified planes into the model objects. As the proposed method uses geometric context to detect an object, it is robust to the object's visual appearance change. As a result, it is ideal for detecting structural objects (e.g., stairways, doorways, and so on). In addition, it has high scalability and parallelism. The method is also capable of detecting some indoor non-structural objects. Experimental results demonstrate that the proposed method has a high success rate in object recognition.},
keywords={feature extraction;Gaussian processes;geometry;handicapped aids;image segmentation;mobile robots;object recognition;path planning;pattern clustering;robot vision;nonstructural objects;robotic navigation aid;real-time detection;indoor structural objects;numerous planar patches;object models;HLFs;Gaussian-mixture-model;plane classifier;planar patch;particular object model;recursive plane clustering procedure;point cloud;3D-object recognition;interplane relationships;Object recognition;Three-dimensional displays;Feature extraction;Visualization;Navigation;Cameras;RNA;Robotic navigation aid;visually impaired;3D object recognition;geometric context;Gaussian mixture model},
doi={10.1109/TNSRE.2017.2748419},
ISSN={1534-4320},
month={Feb},}
@INPROCEEDINGS{7552934,
author={C. {Wei} and Y. F. {Wang}},
booktitle={2016 IEEE International Conference on Multimedia and Expo (ICME)},
title={With one look: 3D face shape estimation from a single snapshot},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Estimating the 3D shape information of a face from a single image is a challenging task, especially when the input image is captured under unconstrained scenarios (e.g., variations of pose, illumination, expression, or even disguise). Previous approaches to this problem typically require careful initialization, registration, or segmentation of the face image regions. With the objective to match the detected landmarks of the input image with those of a set of reference 3D models, we propose a non-negative least squares (NNLS) based algorithm for joint pose and shape estimation. With the additional imposed pose regularization, our method is able to perform person-specific shape estimation, while the camera pose can be simultaneously recovered. We show that our method is robust, effective, and computationally feasible. Moreover, it would perform favorably against existing approaches to 3D shape estimation from a single unconstrained image.},
keywords={face recognition;image matching;image registration;image segmentation;least squares approximations;object detection;pose estimation;shape recognition;3D face shape estimation;single snapshot;3D shape information estimation;face image region initialization;face image region registration;face image region segmentation;landmark detection;image matching;nonnegative least squares;NNLS based algorithm;pose estimation;pose regularization;person-specific shape estimation;camera pose;unconstrained image;Shape;Three-dimensional displays;Face;Solid modeling;Pose estimation;Cameras;Shape estimation;pose estimation},
doi={10.1109/ICME.2016.7552934},
ISSN={1945-788X},
month={July},}
@INPROCEEDINGS{8695341,
author={P. {Liu} and Y. {Yu} and Y. {Zhou} and S. {Du}},
booktitle={2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
title={Single View 3D Face Reconstruction with Landmark Updating},
year={2019},
volume={},
number={},
pages={403-408},
abstract={3D face reconstruction is a long-term and challenging problem, which has wide application scenarios, such as animation, recognition. Inspired by recent works in face landmark marching, we develop a method for 3D face reconstruction using a novel landmark updating optimization strategy. Our method is also based on minimization of the landmark projection error for calculation speed. To achieve more accuracy, contour landmarks and self-occluded landmarks need to be updated. For contour landmarks, the detected landmarks on 2D image are to updated by ?nding a nearest position on contour curve. For self-occluded landmarks, we render the model onto image plane, extract the edge of projected area, and generate new correspondence landmarks according to the edge pixels. We test our method quantitatively with ground truth data, and provide qualitative example reconstructions, show that our method performs better with experiments on MICC dataset.},
keywords={Face;Three-dimensional displays;Solid modeling;Shape;Image reconstruction;Image edge detection;Geometry;Face reconstruction;3D Morphable Model;Landmark update;Self-occluded;Facial feature point},
doi={10.1109/MIPR.2019.00082},
ISSN={},
month={March},}
@INPROCEEDINGS{7299095,
author={S. Z. {Gilani} and F. {Shafait} and A. {Mian}},
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Shape-based automatic detection of a large number of 3D facial landmarks},
year={2015},
volume={},
number={},
pages={4639-4648},
abstract={We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morphologically significant. The proposed algorithm evolves level set curves with adaptive geometric speed functions to automatically extract effective seed points for dense correspondence. Correspondences are established by minimizing the bending energy between patches around seed points of given faces to those of a reference face. Given its hierarchical structure, our algorithm is capable of establishing thousands of correspondences between a large number of faces. Finally, a morphable model based on the dense corresponding points is fitted to an unseen query face for transfer of correspondences and hence automatic detection of landmarks. The proposed algorithm can detect any number of pre-defined landmarks including subtle landmarks that are even difficult to detect manually. Extensive experimental comparison on two benchmark databases containing 6, 507 scans shows that our algorithm outperforms six state of the art algorithms.},
keywords={face recognition;feature extraction;geometry;minimisation;shape recognition;shape-based automatic detection;3D facial landmark;geometric speed function;bending energy minimization;Three-dimensional displays;Shape;Databases;Level set;Algorithm design and analysis;Feature extraction;Mathematical model},
doi={10.1109/CVPR.2015.7299095},
ISSN={1063-6919},
month={June},}
@ARTICLE{7847427,
author={P. {Henriquez} and B. J. {Matuszewski} and Y. {Andreu-Cabedo} and L. {Bastiani} and S. {Colantonio} and G. {Coppini} and M. {D’Acunto} and R. {Favilla} and D. {Germanese} and D. {Giorgi} and P. {Marraccini} and M. {Martinelli} and M. {Morales} and M. A. {Pascali} and M. {Righi} and O. {Salvetti} and M. {Larsson} and T. {Strömberg} and L. {Randeberg} and A. {Bjorgan} and G. {Giannakakis} and M. {Pediaditis} and F. {Chiarugi} and E. {Christinaki} and K. {Marias} and M. {Tsiknakis}},
journal={IEEE Transactions on Multimedia},
title={Mirror Mirror on the Wall... An Unobtrusive Intelligent Multisensory Mirror for Well-Being Status Self-Assessment and Visualization},
year={2017},
volume={19},
number={7},
pages={1467-1481},
abstract={A person's well-being status is reflected by their face through a combination of facial expressions and physical signs. The SEMEOTICONS project translates the semeiotic code of the human face into measurements and computational descriptors that are automatically extracted from images, videos, and three-dimensional scans of the face. SEMEOTICONS developed a multisensory platform in the form of a smart mirror to identify signs related to cardio-metabolic risk. The aim was to enable users to self-monitor their well-being status over time and guide them to improve their lifestyle. Significant scientific and technological challenges have been addressed to build the multisensory mirror, from touchless data acquisition, to real-time processing and integration of multimodal data.},
keywords={data acquisition;data integration;emotion recognition;face recognition;intelligent sensors;mirrors;sensor fusion;unobtrusive intelligent multisensory mirror;well-being status self-assessment;visualization;face reflection;facial expressions;physical signs;SEMEOTICONS project;human face semeiotic code;computational descriptors;image automatic extraction;three-dimensional facial scans;smart mirror;cardio-metabolic risk;touchless data acquisition;multimodal data integration;Mirrors;Three-dimensional displays;Sensors;Face;Cameras;Face detection;Videos;Cardio-metabolic risk;unobtrusive well-being monitoring;multimodal data integration;3D face detection and tracking;3D morphometric analysis;psychosomatic status recognition;multispectral imaging;breath analysis},
doi={10.1109/TMM.2017.2666545},
ISSN={1520-9210},
month={July},}
@INPROCEEDINGS{7724624,
author={M. {Bagga} and B. {Singh}},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={Spoofing detection in face recognition: A review},
year={2016},
volume={},
number={},
pages={2037-2042},
abstract={In the recent days, the facial biometric system is widely used for the mobile payments and other surveillance systems. Its popularity is going to be increased because of its easiness to use and also it is user friendly. But the main problem in this system is its vulnerability to the spoof attacks made by 2D or 3D face masks or printed photographs. In order to guard against face spoofing, the anti-spoofing methods have been developed to do liveliness detection. In this paper, the different type of face spoofing attacks and the different techniques used for anti-spoofing are analyzed.},
keywords={authorisation;biometrics (access control);face recognition;face spoofing detection;face recognition;facial biometric system;mobile payments;surveillance systems;spoof attacks vulnerability;2D face masks;3D face masks;printed photographs;antispoofing;liveliness detection;face spoofing attacks;Handheld computers;Decision support systems;Conferences;Biometrics;Presentation attacks;Recognition systems Spoofing attacks;2D and 3D attacks},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{8265505,
author={S. {Zafeiriou} and G. G. {Chrysos} and A. {Roussos} and E. {Ververas} and J. {Deng} and G. {Trigeorgis}},
booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
title={The 3D Menpo Facial Landmark Tracking Challenge},
year={2017},
volume={},
number={},
pages={2503-2511},
abstract={Recently, deformable face alignment is synonymous to the task of locating a set of 2D sparse landmarks in intensity images. Currently, discriminatively trained Deep Convolutional Neural Networks (DCNNs) are the state-of-the-art in the task of face alignment. DCNNs exploit large amount of high quality annotations that emerged the last few years. Nevertheless, the provided 2D annotations rarely capture the 3D structure of the face (this is especially evident in the facial boundary). That is, the annotations neither provide an estimate of the depth nor correspond to the 2D projections of the 3D facial structure. This paper summarises our efforts to develop (a) a very large database suitable to be used to train 3D face alignment algorithms in images captured "in-the-wild" and (b) to train and evaluate new methods for 3D face landmark tracking. Finally, we report the results of the first challenge in 3D face tracking "in-the-wild".},
keywords={face recognition;feature extraction;neural nets;DCNNs;high quality annotations;facial boundary;3D facial structure;3D menpo facial landmark tracking challenge;deformable face alignment;2D sparse landmarks;intensity images;Deep Convolutional Neural Networks;state-of-the-art;Three-dimensional displays;Face;Two dimensional displays;Shape;Solid modeling;Videos;Cameras},
doi={10.1109/ICCVW.2017.16},
ISSN={2473-9944},
month={Oct},}
@INPROCEEDINGS{8575738,
author={G. {Meyer} and M. {Do}},
booktitle={2018 15th Conference on Computer and Robot Vision (CRV)},
title={Real-Time 3D Face Verification with a Consumer Depth Camera},
year={2018},
volume={},
number={},
pages={71-79},
abstract={We present a system for accurate real-time 3D face verification using a low-quality consumer depth camera. To verify the identity of a subject, we built a high-quality reference model offline by fitting a 3D morphable model to a sequence of low-quality depth images. At runtime, we compare the similarity between the reference model and a single depth image by aligning the model to the image and measuring differences between every point on the two facial surfaces. The model and the image will not match exactly due to sensor noise, occlusions, as well as changes in expression, hairstyle, and eye-wear; therefore, we leverage a data driven approach to determine whether or not the model and the image match. We train a random decision forest to verify the identity of a subject where the point-to-point distances between the reference model and the depth image are used as input features to the classifier. Our approach runs in real-time and is designed to continuously authenticate a user as he/she uses his/her device. In addition, our proposed method outperforms existing 2D and 3D face verification methods on a benchmark data set.},
keywords={cameras;face recognition;image classification;image matching;stereo image processing;real-time 3D face verification;low-quality consumer depth camera;high-quality reference model offline;3D morphable model;low-quality depth images;image match;point-to-point distances;sensor noise;2D face verification methods;Face;Three-dimensional displays;Solid modeling;Computational modeling;Two dimensional displays;Cameras;Face Verification;Face Recognition;Depth Cameras;3D Computer Vision},
doi={10.1109/CRV.2018.00020},
ISSN={},
month={May},}
@INPROCEEDINGS{8578972,
author={K. {Genova} and F. {Cole} and A. {Maschinot} and A. {Sarna} and D. {Vlasic} and W. T. {Freeman}},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={Unsupervised Training for 3D Morphable Model Regression},
year={2018},
volume={},
number={},
pages={8377-8386},
abstract={We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results.},
keywords={face recognition;regression analysis;rendering (computer graphics);unsupervised training;morphable model;regression network;unlabeled photographs;training loss;facial recognition network;differentiable renderer;predicted 3D face;multiview identity loss;loopback loss;output distribution;batch distribution loss;network fooling effects;Training;Three-dimensional displays;Shape;Lighting;Solid modeling;Face recognition;Computational modeling},
doi={10.1109/CVPR.2018.00874},
ISSN={2575-7075},
month={June},}
@INPROCEEDINGS{7430529,
author={A. {Limonov} and J. {Jeong} and M. {Kim} and S. {Kim} and Y. {Kim}},
booktitle={2016 IEEE International Conference on Consumer Electronics (ICCE)},
title={Human face 3D reconstruction with handheld single 2D camera on mobile devices},
year={2016},
volume={},
number={},
pages={81-82},
abstract={In this paper, we propose a live system for 3D reconstruction of human face using only single 2D camera without any 3D sensor. Lack of feature points and homogeneous skin color leads to low quality and success rate in conventional 3D reconstruction algorithms when applied to human face. Moreover, it requires difficult user interaction. To solve this problem, we adopt the facial shape and appearance model to our 3D reconstruction pipeline. This is the first approach which creates a complete 3D face model that can be directly used for 3D printing or virtual reality applications. Model can be acquired using modern smartphone in less than 40 seconds.},
keywords={cameras;face recognition;image colour analysis;image reconstruction;mobile handsets;shape recognition;human face 3D reconstruction;handheld single-2D camera;mobile devices;feature points;homogeneous skin color;user interaction;facial shape model;facial appearance model;3D reconstruction pipeline;3D printing;virtual reality;smart phone;Three-dimensional displays;Face;Cameras;Solid modeling;Shape;Mobile handsets;Facial features},
doi={10.1109/ICCE.2016.7430529},
ISSN={2158-4001},
month={Jan},}
@INPROCEEDINGS{8314888,
author={G. {Torkhani} and A. {Ladgham} and A. {Sakly}},
booktitle={2017 18th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering (STA)},
title={3D Gabor-Edge filters applied to face depth images},
year={2017},
volume={},
number={},
pages={578-582},
abstract={This manuscript introduces a novel 3D face authentication system inspired from the advantageous capacities of Gabor-Edge filters. The approach studies 3D face difficulties such as expression variety, different rotations and exposure to illuminations. The proposed systems starts by preprocessing the 3D face images to resolve acquisition problems. Then, a filtering process is performed by implanting our 3D Gabor-Edge technique extended based on the classic 3D Gabor masks. The next step is to achieve the classification of facial features from the edge saliency by the artificial Neural Network Classifier (NNC). The evaluation of the adopted system is achieved by exporting common datasets from GavabDB database. Experimental results are reported to prove the high accuracy rates of our method compared to the recent researches in the same biometric field.},
keywords={biometrics (access control);edge detection;face recognition;feature extraction;Gabor filters;image classification;neural nets;3D face difficulties;3D face images;3D Gabor-Edge technique;classic 3D Gabor masks;edge saliency;Gabor-edge filters;3D face authentication system;acquisition problems;facial feature classification;artificial neural network classifier;GavabDB database;biometric field;Three-dimensional displays;Face;Feature extraction;Authentication;Gabor filters;Face recognition;face authentication;Gabor filtering;3D images;saliency},
doi={10.1109/STA.2017.8314888},
ISSN={2573-539X},
month={Dec},}
@INPROCEEDINGS{7986860,
author={S. {Graßhof} and H. {Ackermann} and F. {Kuhnke} and J. {Ostermann} and S. S. {Brandt}},
booktitle={2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)},
title={Projective structure from facial motion},
year={2017},
volume={},
number={},
pages={298-301},
abstract={Nonrigid Structure-From-Motion is a well-known approach to estimate time-varying 3D structures from 2D input image sequences. For challenging problems such as the reconstruction of human faces, state-of-the-art approaches estimate statistical shape spaces from training data. It is common practice to use orthographic or weak-perspective camera models to map 3D to 2D points. We propose to use a projective camera model combined with a multilinear tensor-based face model, enabling approximation of a dense 3D face surface by sparse 2D landmarks. Using a projective camera is beneficial, as it is able to handle perspective projections and particular camera motions which are critical for affine models. We show how the nonlinearity of the projective model can be linearized so that its parameters can be estimated by an alternating-least-squares approach. This enables simple and fast estimation of the model parameters. The effectiveness of the proposed algorithm is demonstrated using challenging real image data.},
keywords={face recognition;image reconstruction;image sequences;least squares approximations;projective structure;facial motion;nonrigid structure-from-motion;time-varying 3D structures;2D input image sequences;image reconstruction;sparse 2D landmarks;alternating-least-squares approach;Cameras;Three-dimensional displays;Solid modeling;Shape;Two dimensional displays;Tensile stress;Image reconstruction},
doi={10.23919/MVA.2017.7986860},
ISSN={},
month={May},}
@INPROCEEDINGS{7299058,
author={T. {Hassner} and S. {Harel} and E. {Paz} and R. {Enbar}},
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Effective face frontalization in unconstrained images},
year={2015},
volume={},
number={},
pages={4295-4304},
abstract={“Frontalization” is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. We observe that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, we explore the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces. We show that this leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation.},
keywords={approximation theory;face recognition;face frontalization;unconstrained images;frontal facing view synthesis;unconstrained photos;face recognition systems;3D surface;approximation;gender estimation;Face;Three-dimensional displays;Facial features;Face recognition;Shape;Solid modeling;Approximation methods},
doi={10.1109/CVPR.2015.7299058},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{7139051,
author={J. {Li} and S. {Long} and D. {Zeng} and Q. {Zhao}},
booktitle={2015 International Conference on Biometrics (ICB)},
title={Example-based 3D face reconstruction from uncalibrated frontal and profile images},
year={2015},
volume={},
number={},
pages={193-200},
abstract={Reconstructing 3D face models from multiple uncalibrated 2D face images is usually done by using a single reference 3D face model or some gender/ethnicity-specific 3D face models. However, different persons, even those of the same gender or ethnicity, usually have significantly different faces in terms of their overall appearance, which forms the base of person recognition using faces. Consequently, existing 3D reference model based methods have limited capability of reconstructing 3D face models for a large variety of persons. In this paper, we propose to explore a reservoir of diverse reference models to improve the 3D face reconstruction performance. Specifically, we convert the face reconstruction problem into a multi-label segmentation problem. Its energy function is formulated from different cues, including 1) similarity between the desired output and the initial model, 2) color consistency between different views, 3) smoothness constraint on adjacent pixels, and 4) model consistency within local neighborhood. Experimental results on challenging datasets demonstrate that the proposed algorithm is capable of recovering high quality face models in both qualitative and quantitative evaluations.},
keywords={face recognition;image reconstruction;image segmentation;minimisation;3D face model reconstruction;frontal image;profile image;multilabel segmentation problem;energy function minimization;qualitative evaluation;quantitative evaluation;Face;Three-dimensional displays;Solid modeling;Image reconstruction;Biological system modeling;Databases;Optimization},
doi={10.1109/ICB.2015.7139051},
ISSN={2376-4201},
month={May},}
@INPROCEEDINGS{8373916,
author={Z. {Feng} and P. {Huber} and J. {Kittler} and P. {Hancock} and X. {Wu} and Q. {Zhao} and P. {Koppen} and M. {Raetsch}},
booktitle={2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
title={Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild},
year={2018},
volume={},
number={},
pages={780-786},
abstract={This paper investigates the evaluation of dense 3D face reconstruction from a single 2D image in the wild. To this end, we organise a competition that provides a new benchmark dataset that contains 2000 2D facial images of 135 subjects as well as their 3D ground truth face scans. In contrast to previous competitions or challenges, the aim of this new benchmark dataset is to evaluate the accuracy of a 3D dense face reconstruction algorithm using real, accurate and high-resolution 3D ground truth face scans. In addition to the dataset, we provide a standard protocol as well as a Python script for the evaluation. Last, we report the results obtained by three state-of-the-art 3D face reconstruction systems on the new benchmark dataset. The competition is organised along with the 2018 13th IEEE Conference on Automatic Face &amp; Gesture Recognition.},
keywords={face recognition;gesture recognition;image reconstruction;solid modelling;2D Face images;dense 3D face reconstruction;single 2D image;benchmark dataset;3D dense face reconstruction algorithm;high-resolution 3D ground truth face scans;Automatic Face & Gesture Recognition;Python script;standard protocol;Face;Three-dimensional displays;Two dimensional displays;Image reconstruction;Benchmark testing;Protocols;Nose;3D Dense Face Reconstruction;Evaluation;3D Morphable Face Model},
doi={10.1109/FG.2018.00123},
ISSN={},
month={May},}
@INPROCEEDINGS{7391814,
author={ and and and },
booktitle={2015 International Conference on 3D Imaging (IC3D)},
title={Robust nose tip detection for face range images based on local features in scale-space},
year={2015},
volume={},
number={},
pages={1-8},
abstract={Being the most distinct feature point in 3D facial landmarks, nose tip plays a significant role in 3D facial studies such as face detection, face recognition, facial features extraction, face alignment, etc. Successful detection of nose tip can facilitate many tasks of 3D facial studies. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, needs not training, can handle large rotations and occlusions. To reduce computational cost, we first remove small isolated regions from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, the Multi-angle Energy (ME) of each point is computed and sorted in descending order. Then the first m points in the descending order list are obtained and hierarchical clustering method is used to cluster these points. In the first h largest clusters, we can find one point with the largest ME. For all scales of the scale-space, we get a series of such points which are treated as nose tip candidates. For these candidates, we apply hierarchical clustering again. In the obtained largest cluster, we compute the mean value of ME. The ME of nose tip will be closest to the mean value. We evaluate our method in two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The experimental results verify the robustness of our method with a high nose tip detection rate.},
keywords={face recognition;feature extraction;object detection;pattern clustering;nose tip detection;face range images;local features;3D facial landmarks;3D facial studies;multiangle energy;ME;range image smoothing;hierarchical clustering method;FRGC database;BOSPHORUS database;Nose;Three-dimensional displays;Face;Robustness;Training;Feature extraction;Smoothing methods;nose tip;3D faces;range images;robust smoothing;normal;scale-space;multi-angle energy;sphere fitting;least square;hierarchical clustering},
doi={10.1109/IC3D.2015.7391814},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7495382,
author={M. C. E. {Rai} and C. {Tortorici} and H. {Al-Muhairi} and H. A. {Safar} and N. {Werghi}},
booktitle={2016 18th Mediterranean Electrotechnical Conference (MELECON)},
title={Landmarks detection on 3D face scans using local histogram descriptors},
year={2016},
volume={},
number={},
pages={1-5},
abstract={In this work, we exploit 3D Constrained Local Model (CLM) for facial landmark detection. Our approach integrates the geometric information of 3D face scans. The fast increase demand of 3D data invite to develop 3D image processing methods for many applications and especially for automatic landmark detection. The new step in this paper is the introduction of mesh histogram of gradients (meshHOG) as local descriptors around every landmark location. The proposed work is evaluated on the publicly available Bosphorus database. A comparison with the other descriptors mesh LBP and mesh SIFT are also depicted.},
keywords={computational geometry;face recognition;mesh generation;object detection;facial landmark detection;3D face scans;local histogram descriptors;3D constrained local model;geometric information;3D image processing methods;mesh histogram-of-gradients;meshHOG;local descriptors;publicly available Bosphorus database;Three-dimensional displays;Face;Histograms;Solid modeling;Shape;Deformable models;Mathematical model},
doi={10.1109/MELCON.2016.7495382},
ISSN={2158-8481},
month={April},}
@ARTICLE{8458443,
author={Y. {Cha} and T. {Price} and Z. {Wei} and X. {Lu} and N. {Rewkowski} and R. {Chabra} and Z. {Qin} and H. {Kim} and Z. {Su} and Y. {Liu} and A. {Ilie} and A. {State} and Z. {Xu} and J. {Frahm} and H. {Fuchs}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Towards Fully Mobile 3D Face, Body, and Environment Capture Using Only Head-worn Cameras},
year={2018},
volume={24},
number={11},
pages={2993-3004},
abstract={We propose a new approach for 3D reconstruction of dynamic indoor and outdoor scenes in everyday environments, leveraging only cameras worn by a user. This approach allows 3D reconstruction of experiences at any location and virtual tours from anywhere. The key innovation of the proposed ego-centric reconstruction system is to capture the wearer's body pose and facial expression from near-body views, e.g. cameras on the user's glasses, and to capture the surrounding environment using outward-facing views. The main challenge of the ego-centric reconstruction, however, is the poor coverage of the near-body views - that is, the user's body and face are observed from vantage points that are convenient for wear but inconvenient for capture. To overcome these challenges, we propose a parametric-model-based approach to user motion estimation. This approach utilizes convolutional neural networks (CNNs) for near-view body pose estimation, and we introduce a CNN-based approach for facial expression estimation that combines audio and video. For each time-point during capture, the intermediate model-based reconstructions from these systems are used to re-target a high-fidelity pre-scanned model of the user. We demonstrate that the proposed self-sufficient, head-worn capture system is capable of reconstructing the wearer's movements and their surrounding environment in both indoor and outdoor situations without any additional views. As a proof of concept, we show how the resulting 3D-plus-time reconstruction can be immersively experienced within a virtual reality system (e.g., the HTC Vive). We expect that the size of the proposed egocentric capture-and-reconstruction system will eventually be reduced to fit within future AR glasses, and will be widely useful for immersive 3D telepresence, virtual tours, and general use-anywhere 3D content creation.},
keywords={cameras;computer vision;convolution;face recognition;feedforward neural nets;image reconstruction;mobile computing;motion estimation;pose estimation;solid modelling;virtual reality;environment capture;head-worn cameras;dynamic indoor scenes;virtual tours;ego-centric reconstruction system;near-body views;parametric-model-based approach;user motion estimation;convolutional neural networks;CNN-based approach;facial expression estimation;intermediate model-based reconstructions;high-fidelity pre-scanned model;head-worn capture system;3D-plus-time reconstruction;virtual reality system;immersive 3D telepresence;use-anywhere 3D content creation;capture-and-reconstruction system;fully mobile 3D face capture;AR glasses;HTC Vive;near-view body pose estimation;Cameras;Three-dimensional displays;Image reconstruction;Face;Solid modeling;Pose estimation;Deformable models;Telepresence;Ego-centric Vision;Motion Capture;Convolutional Neural Networks},
doi={10.1109/TVCG.2018.2868527},
ISSN={1077-2626},
month={Nov},}
@INPROCEEDINGS{8658722,
author={Y. {Xing} and R. {Tewari} and P. {Mendonca}},
booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
title={A Self-Supervised Bootstrap Method for Single-Image 3D Face Reconstruction},
year={2019},
volume={},
number={},
pages={1014-1023},
abstract={State-of-the-art methods for 3D reconstruction of faces from a single image require 2D-3D pairs of ground-truth data for supervision. Such data is costly to acquire, and most datasets available in the literature are restricted to pairs for which the input 2D images depict faces in a near fronto-parallel pose. Therefore, many data-driven methods for single-image 3D facial reconstruction perform poorly on profile and near-profile faces. We propose a method to improve the performance of single-image 3D facial reconstruction networks by utilizing the network to synthesize its own training data for fine-tuning, comprising: (i) single-image 3D reconstruction of faces in near-frontal images without ground-truth 3D shape; (ii) application of a rigid-body transformation to the reconstructed face model; (iii) rendering of the face model from new viewpoints; and (iv) use of the rendered image and corresponding 3D reconstruction as additional data for supervised fine-tuning. The new 2D-3D pairs thus produced have the same high-quality observed for near fronto-parallel reconstructions, thereby nudging the network towards more uniform performance as a function of the viewing angle of input faces. Application of the proposed technique to the fine-tuning of a state-of-the-art single-image 3D-reconstruction network for faces demonstrates the usefulness of the method, with particularly significant gains for profile or near-profile views.},
keywords={face recognition;image reconstruction;rendering (computer graphics);statistical analysis;self-supervised bootstrap method;single-image 3D face reconstruction;ground-truth data;input 2D images;data-driven methods;near-profile faces;single-image 3D facial reconstruction networks;single-image 3D reconstruction;near-frontal images;ground-truth 3D shape;reconstructed face model;rendered image;fronto-parallel reconstructions;near fronto-parallel pose;2D-3D pairs;rigid-body transformation;supervised fine-tuning;Three-dimensional displays;Face;Image reconstruction;Solid modeling;Data models;Training;Two dimensional displays},
doi={10.1109/WACV.2019.00113},
ISSN={1550-5790},
month={Jan},}
@INPROCEEDINGS{8122755,
author={O. F. {Osman} and R. M. I. {Elbashir} and I. E. {Abbass} and C. {Kendrick} and M. {Goyal} and M. H. {Yap}},
booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
title={Automated assessment of facial wrinkling: A case study on the effect of smoking},
year={2017},
volume={},
number={},
pages={1081-1086},
abstract={Facial wrinkle is one of the most prominent biological changes that accompanying the natural aging process. However, there are some external factors contributing to premature wrinkles development, such as sun exposure and smoking. Clinical studies have shown that heavy smoking causes premature wrinkles development. However, there is no computerised system that can automatically assess the facial wrinkles on the whole face. This study investigates the effect of smoking on facial wrinkling using a social habit face dataset and an automated computerised computer vision algorithm. The wrinkles pattern represented in the intensity of 0-255 was first extracted using a modified Hybrid Hessian Filter. The face was divided into ten predefined regions, where the wrinkles in each region was extracted. Then the statistical analysis was performed to analyse which region is effected mainly by smoking. The result showed that the density of wrinkles for smokers in two regions around the mouth was significantly higher than the non-smokers, at p-value of 0.05. Other regions are inconclusive due to lack of large-scale dataset. Finally, the wrinkle was visually compared between smoker and non-smoker faces by generating a generic 3D face model.},
keywords={ageing;computer vision;face recognition;image filtering;statistical analysis;automated computerised computer vision algorithm;facial wrinkling;heavy smoking;wrinkle pattern;premature wrinkle development;natural aging process;social habit face dataset;hybrid hessian filter;statistical analysis;generic 3D face model;Face;Skin;Mouth;Visualization;Aging;Filtering algorithms;Solid modeling},
doi={10.1109/SMC.2017.8122755},
ISSN={},
month={Oct},}
@ARTICLE{7265032,
author={L. {Ma} and G. {Zheng} and J. U. H. {Eitel} and L. M. {Moskal} and W. {He} and H. {Huang}},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Improved Salient Feature-Based Approach for Automatically Separating Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar Point Cloud Data of Forest Canopies},
year={2016},
volume={54},
number={2},
pages={679-696},
abstract={Accurate separation of photosynthetic and nonphotosynthetic components in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a challenging but of key importance to understand the spatial distribution of the radiation regime, photosynthetic processes, and carbon and water exchanges of the forest canopy. The objective of this paper was to improve current methods for separating photosynthetic and nonphotosynthetic components in TLS data of forest canopies by adding two additional filters only based on its geometric information. By comparing the proposed approach with the eigenvalues plus color information-based method, we found that the proposed approach could effectively improve the overall producer's accuracy from 62.12% to 95.45%, and the overall classification producer's accuracy would increase from 84.28% to 97.80% as the forest leaf area index (LAI) decreases from 4.15 to 3.13. In addition, variations in tree species had negligible effects on the final classification accuracy, as shown by the overall producer's accuracy for coniferous (93.09%) and broadleaf (94.96%) trees. To remove quantitatively the effects of the woody materials in a forest canopy for improving TLS-based LAI estimates, we also computed the “woody-to-total area ratio” based on the classified linear class points from an individual tree. Automatic classification of the forest point cloud data set will facilitate the application of TLS on retrieving 3-D forest canopy structural parameters, including LAI and leaf and woody area ratios.},
keywords={forestry;optical radar;photosynthesis;remote sensing by radar;salient feature-based approach;photosynthetic components;nonphotosynthetic components;terrestrial lidar point cloud data;forest canopies;TLS data;geometric information;eigenvalues;information-based method;forest leaf area index;Three-dimensional displays;Vegetation;Laser radar;Eigenvalues and eigenfunctions;Accuracy;Surface emitting lasers;Probability density function;Light detection and ranging (lidar);pattern recognition;point classification;terrestrial laser scanning (TLS);woody-to-total area ratio;Light detection and ranging (lidar);pattern recognition;point classification;terrestrial laser scanning (TLS);woody-to-total area ratio},
doi={10.1109/TGRS.2015.2459716},
ISSN={0196-2892},
month={Feb},}
@INPROCEEDINGS{7406406,
author={N. C. {Camgöz} and V. {truc} and B. {Gokberk} and L. {Akarun} and A. A. {Kindiroglu}},
booktitle={2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
title={Facial Landmark Localization in Depth Images Using Supervised Ridge Descent},
year={2015},
volume={},
number={},
pages={378-383},
abstract={Supervised Descent Method (SDM) has proven successful in many computer vision applications such as face alignment, tracking and camera calibration. Recent studies which used SDM, achieved state of the-art performance on facial landmark localization in depth images [4]. In this study, we propose to use ridge regression instead of least squares regression for learning the SDM, and to change feature sizes in each iteration, effectively turning the landmark search into a coarse to fine process. We apply the proposed method to facial landmark localization on the Bosphorus 3D Face Database, using frontal depth images with no occlusion. Experimental results confirm that both ridge regression and using adaptive feature sizes improve the localization accuracy considerably.},
keywords={computer vision;face recognition;image sensors;iterative methods;learning (artificial intelligence);object tracking;regression analysis;facial landmark localization;depth images;supervised ridge descent;supervised descent method;SDM;computer vision applications;face alignment;face tracking;camera calibration;ridge regression;adaptive feature sizes;Bosphorus 3D face database;Face;Shape;Three-dimensional displays;Nose;Databases;Feature extraction;Training},
doi={10.1109/ICCVW.2015.57},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7351660,
author={R. J. {Arteaga} and S. J. {Ruuth}},
booktitle={2015 IEEE International Conference on Image Processing (ICIP)},
title={Laplace-Beltrami spectra for shape comparison of surfaces in 3D using the closest point method},
year={2015},
volume={},
number={},
pages={4511-4515},
abstract={The need to compare separate objects arises in a wide range of applications. In one approach for comparing objects, `Shape-DNA' is constructed to give a numerical fingerprint representing an individual object. Shape-DNA is a cropped set of eigenvalues of the Laplace-Beltrami operator for the surface of the object. In this paper, we compute the Shape-DNA of surfaces using the closest point method. Our approach may be applied to a variety of surface representations including triangulations, point clouds and certain analytical shapes. A 2D multidimensional scaling plot illustrates that similar objects form groups based on the Shape-DNAs. Our method has the benefit that it may be applied to surfaces defined by dense point clouds without requiring the construction of point connectivity.},
keywords={DNA;eigenvalues and eigenfunctions;fingerprint identification;image representation;Laplace transforms;shape recognition;Laplace-Beltrami spectra;shape comparison;closest point method;shape-DNA;numerical fingerprint representation;eigenvalues;surface representations;point clouds;2D multidimensional scaling plot;Shape;Three-dimensional displays;Eigenvalues and eigenfunctions;Standards;Interpolation;Rabbits;shape comparison;Laplace-Beltrami spectra;closest point method;point cloud},
doi={10.1109/ICIP.2015.7351660},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7139616,
author={T. {Linder} and S. {Wehner} and K. O. {Arras}},
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
title={Real-time full-body human gender recognition in (RGB)-D data},
year={2015},
volume={},
number={},
pages={3039-3045},
abstract={Understanding social context is an important skill for robots that share a space with humans. In this paper, we address the problem of recognizing gender, a key piece of information when interacting with people and understanding human social relations and rules. Unlike previous work which typically considered faces or frontal body views in image data, we address the problem of recognizing gender in RGB-D data from side and back views as well. We present a large, gender-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the Kinect v1 and Kinect v2 sensor. We then learn and compare several classifiers on the Kinect v2 data using a HOG baseline, two state-of-the-art deep-learning methods, and a recent tessellation-based learning approach. Originally developed for person detection in 3D data, the latter is able to learn the best selection, location and scale of a set of simple point cloud features. We show that for gender recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 Hz.},
keywords={human-robot interaction;image classification;image sensors;learning (artificial intelligence);object detection;object recognition;real-time full-body human gender recognition;RGB-D data;human social relations;human social rules;image data;multiperspective RGB-D dataset;gender-balanced RGB-D dataset;annotated RGB-D dataset;Kinect v2 sensor;Kinect v1 sensor;HOG baseline;deep-learning methods;tessellation-based learning approach;person detection;point cloud features;Three-dimensional displays;Robot sensing systems;Training;Legged locomotion;Accuracy;Support vector machines},
doi={10.1109/ICRA.2015.7139616},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7517246,
author={P. {Azevedo} and T. O. {Dos Santos} and E. {De Aguiar}},
booktitle={2016 XVIII Symposium on Virtual and Augmented Reality (SVR)},
title={An Augmented Reality Virtual Glasses Try-On System},
year={2016},
volume={},
number={},
pages={1-9},
abstract={This paper presents a virtual try-on system to correctly visualize 3D objects (e.g., glasses) in the face of a given user. By capturing the image and depth information of a user through a low-cost RGB-D camera, we apply a face tracking technique to detect specific landmarks in the facial image. These landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3D facial morphable model that fits as good as possible to the user's head and face. At the end, we deform the chosen 3D objects from its rest shape to a deformed shape matching the specific facial shape of the user. The last step projects and renders the 3D object into the original image, with enhanced precision and in proper scale, showing the selected object in the user's face. We validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. Our results demonstrate that, by fitting a facial model to the user's face, the rendered virtual 3D objects look more realistic.},
keywords={augmented reality;face recognition;image capture;image colour analysis;image matching;image reconstruction;image sensors;rendering (computer graphics);shape recognition;augmented reality virtual glass try-on system;3D object visualization;image capturing;depth information;low-cost RGB-D camera;facial image;point cloud reconstruction;3D facial morphable model;deformed shape matching;virtual 3D object rendering;Three-dimensional displays;Face;Solid modeling;Glass;Cameras;Shape;Augmented reality;Virtual reality;Computer graphics},
doi={10.1109/SVR.2016.12},
ISSN={},
month={June},}
@INPROCEEDINGS{8500537,
author={L. {Shi} and J. {Yue} and Y. {Dong} and M. {Lin} and S. {wang} and R. {Shen} and Z. {Chang}},
booktitle={2018 IEEE Intelligent Vehicles Symposium (IV)},
title={CG Benefited Driver Facial Landmark Localization Across Large Rotation},
year={2018},
volume={},
number={},
pages={1995-2002},
abstract={Facial landmark localization is a crucial initial step Driver Inattention Monitoring. The aim of this paper is to localize driver facial landmarks across large rotation, say [-90°, +90°] in yaw rotation, to cope with real driving conditions. The paper proposes a flexible pipe-line for creating automatically labeled face image to supply wanted dataset. The benefits of CG (Computer Graphics) techniques such as 3D face modelling and morphing, photorealistic rendering and ground truth generation are utilized. To the best of our knowledge this is the first time to combine CG rendering and automatic ground truth labelling techniques with face landmark localization algorithms. The effectiveness of the CG rendered data is proved by cross validation with Multi-PIE dataset. Landmark localization across large rotation is obtained by a system simply integrating the off the-shelves algorithms and trained with the CG rendered data. The experiments of the implemented system on Multi-PIE and real persons show that it could localize facial landmarks across large rotation accurately and in real time.},
keywords={driver information systems;face recognition;rendering (computer graphics);road safety;driver facial landmark localization;driver facial landmarks;yaw rotation;driving conditions;flexible pipe-line;face image;CG techniques;photorealistic rendering;ground truth generation;automatic ground truth;face landmark localization algorithms;MultiPIE dataset;initial step driver inattention monitoring;Cameras;Face;Lighting;Three-dimensional displays;Rendering (computer graphics);Shape;Solid modeling},
doi={10.1109/IVS.2018.8500537},
ISSN={1931-0587},
month={June},}
@ARTICLE{7360185,
author={X. {Yu} and J. {Huang} and S. {Zhang} and D. N. {Metaxas}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Face Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Model},
year={2016},
volume={38},
number={11},
pages={2212-2226},
abstract={This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. In initialization stage, we propose a group sparse optimized mixture model to automatically select the most salient facial landmarks. By introducing 3D face shape model, we apply procrustes analysis to provide pose-aware landmark initialization. In landmark localization stage, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework simultaneously handles face detection, pose-robust landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental databases and face-in-the-wild databases. The results reveal that our approach consistently outperforms state-of-the-art methods for face alignment and tracking.},
keywords={face recognition;image sensors;pose estimation;shape recognition;face landmark fitting;optimized part mixtures;cascaded deformable model;facial landmark localization;facial landmark tracking;single camera;cascaded deformable shape;head pose variations;3D face shape model;pose-aware landmark;mean-shift local search;pose-robust landmark localization;face detection;laboratory environmental databases;face alignment;face tracking;Face;Shape;Deformable models;Detectors;Three-dimensional displays;Face detection;Databases;Face landmark localization;face tracking;deformable shape model;part based model},
doi={10.1109/TPAMI.2015.2509999},
ISSN={0162-8828},
month={Nov},}
@INPROCEEDINGS{7899697,
author={G. {Pang} and U. {Neumann}},
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)},
title={3D point cloud object detection with multi-view convolutional neural network},
year={2016},
volume={},
number={},
pages={585-590},
abstract={Efficient detection of three dimensional (3D) objects in point clouds is a challenging problem. Performing 3D descriptor matching or 3D scanning-window search with detector are both time-consuming due to the 3-dimensional complexity. One solution is to project 3D point cloud into 2D images and thus transform the 3D detection problem into 2D space, but projection at multiple viewpoints and rotations produce a large amount of 2D detection tasks, which limit the performance and complexity of the 2D detection algorithm choice. We propose to use convolutional neural network (CNN) for the 2D detection task, because it can handle all viewpoints and rotations for the same class of object together, as well as predicting multiple classes of objects with the same network, without the need for individual detector for each object class. We further improve the detection efficiency by concatenating two extra levels of early rejection networks with binary outputs before the multi-class detection network. Experiments show that our method has competitive overall performance with at least one-order of magnitude speed-up comparing with latest 3D point cloud detection methods.},
keywords={neural nets;object detection;multiclass detection network;early rejection networks;CNN;convolutional neural network;2D detection algorithm;2D space;2D images;3-dimensional complexity;3D scanning-window search;3D descriptor matching;three dimensional object detection;multiview convolutional neural network;3D point cloud object detection;Three-dimensional displays;Two dimensional displays;Object detection;Training;Detectors;Complexity theory;Search problems},
doi={10.1109/ICPR.2016.7899697},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7412239,
author={F. {Goodarzi} and M. I. {Saripan}},
booktitle={2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)},
title={Real time face pose estimation using geometrical features},
year={2015},
volume={},
number={},
pages={482-487},
abstract={3D Head pose estimation using a hybrid pose estimation method is discussed in this paper. The only equipment used is simple webcam that is available on most computers and laptops today. The hybrid method consists of tracking facial landmarks of face and using geometrical face pose estimation to compute distances to estimate 3D position of head. The pose estimation system works real time as it is ultimately will be used for 2D-3D face recognition system. Actual data show reasonable error for rotation along each axis (Yaw, Pitch and Roll) by using only few facial landmarks.},
keywords={pose estimation;stereo image processing;real time face pose estimation;geometrical features;3D head pose estimation;hybrid pose estimation;webcam;computers;laptops;facial landmarks;geometrical face pose estimation;pose estimation system;2D-3D face recognition system;Face;Detectors;Facial features;Three-dimensional displays;Face recognition},
doi={10.1109/ICSIPA.2015.7412239},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7988861,
author={A. T. {Angonese} and P. F. {Ferreira Rosa}},
booktitle={2017 International Conference on Military Technologies (ICMT)},
title={Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform},
year={2017},
volume={},
number={},
pages={779-786},
abstract={This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.},
keywords={mobile robots;neural nets;object detection;object recognition;operating systems (computers);path planning;robot vision;SLAM (robots);multiple people detection;multiple people identification system;dynamic simultaneous localization and mapping system;autonomous mobile robotic platform;robot exploration;robot navigation;Pioneer 3DX robot;RGBD camera;Sick Lms200 sensor;robot operating system;SLAM;HOG detector;histogram of oriented gradients;deep convolutional neural network;CNN;MatLab MatConvNet library;Simultaneous localization and mapping;Libraries;Computer architecture;Feature extraction;Operating systems;People Detection;HOG;Deep Learning;CNN;Simultaneous Localization and Mapping (SLAM);robot operating system ROS},
doi={10.1109/MILTECHS.2017.7988861},
ISSN={},
month={May},}
@INPROCEEDINGS{7358799,
author={Y. {Wu} and X. {Xu} and S. K. {Shah} and I. A. {Kakadiaris}},
booktitle={2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)},
title={Towards fitting a 3D dense facial model to a 2D image: A landmark-free approach},
year={2015},
volume={},
number={},
pages={1-8},
abstract={Head pose estimation helps to align a 3D face model to a 2D image, which is critical to research requiring dense 2D-to-2D or 3D-to-2D correspondence. Traditional pose estimation relies strongly on the accuracy of landmarks, so it is sensitive to missing or incorrect landmarks. In this paper, we propose a landmark-free approach to estimate the pose projection matrix. The method can be used to estimate this matrix in unconstrained scenarios and we demonstrate its effectiveness through multiple head pose estimation experiments.},
keywords={face recognition;matrix algebra;pose estimation;3D dense facial model;landmark-free approach;3D face model;pose projection matrix;unconstrained scenario;multiple head pose estimation experiment;Three-dimensional displays;Solid modeling;Face;Matrix decomposition;Training},
doi={10.1109/BTAS.2015.7358799},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8237443,
author={S. {Xiao} and J. {Feng} and L. {Liu} and X. {Nie} and W. {Wang} and S. {Yan} and A. {Kassim}},
booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
title={Recurrent 3D-2D Dual Learning for Large-Pose Facial Landmark Detection},
year={2017},
volume={},
number={},
pages={1642-1651},
abstract={Despite remarkable progress of face analysis techniques, detecting landmarks on large-pose faces is still difficult due to self-occlusion, subtle landmark difference and incomplete information. To address these challenging issues, we introduce a novel recurrent 3D-2D dual learning model that alternatively performs 2D-based 3D face model refinement and 3D-to-2D projection based 2D landmark refinement to reliably reason about self-occluded landmarks, precisely capture the subtle landmark displacement and accurately detect landmarks even in presence of extremely large poses. The proposed model presents the first loop-closed learning framework that effectively exploits the informative feedback from the 3D-2D learning and its dual 2D-3D refinement tasks in a recurrent manner. Benefiting from these two mutual-boosting steps, our proposed model demonstrates appealing robustness to large poses (up to profile pose) and outstanding ability to capture fine-scale landmark displacement compared with existing 3D models. It achieves new state-of-the-art on the challenging AFLW benchmark. Moreover, our proposed model introduces a new architectural design that economically utilizes intermediate features and achieves 4× faster speed than its deep learning based counterparts.},
keywords={face recognition;feature extraction;learning (artificial intelligence);pose estimation;Recurrent 3D-2D Dual Learning;large-pose facial landmark detection;face analysis techniques;faces;subtle landmark difference;incomplete information;3D face model refinement;2D landmark refinement;self-occluded landmarks;subtle landmark displacement;poses;informative feedback;2D-3D refinement tasks;fine-scale landmark displacement;deep learning based counterparts;3D models;Three-dimensional displays;Face;Solid modeling;Two dimensional displays;Feature extraction;Robustness;Predictive models},
doi={10.1109/ICCV.2017.181},
ISSN={2380-7504},
month={Oct},}
@INPROCEEDINGS{7158332,
author={C. {Qu} and C. {Herrmann} and E. {Monari} and T. {Schuchert} and J. {Beyerer}},
booktitle={2015 12th Conference on Computer and Robot Vision},
title={3D vs. 2D: On the Importance of Registration for Hallucinating Faces Under Unconstrained Poses},
year={2015},
volume={},
number={},
pages={139-146},
abstract={Face Hallucination (FH) differs from generic single-image super-resolution (SR) algorithms in its specific domain of application. By exploiting the common structures of human faces, magnification of lower resolution images can be achieved. Despite the growing interest in recent years, considerably less attention is paid to a crucial step in FH -- registration of facial images. In this work, registration techniques employed in the literature are first summarized and the importance of using well-aligned training and test images is demonstrated. A novel method to inversely map the high-resolution (HR) 3D training texture to the low-resolution (LR) 2D test image in arbitrary poses is then presented, which prevents information loss in LR images and is thus beneficial to SR. The effectiveness of our 3D approach is evaluated on the Multi-PIE and the PUT face databases. Superior qualitative and quantitative FH results to the state-of-the-art methods in all tested poses prove the necessity of accurate registration in FH. The merit of 3D FH in generating super-resolved frontal faces is also verified, revealing 30% improvement in face recognition over the 2D approach under 30° of yaw rotation on the Multi-PIE dataset.},
keywords={face recognition;image registration;image resolution;image texture;pose estimation;face hallucination;unconstrained poses;generic single-image super-resolution algorithms;facial image registration;high-resolution 3D training texture;low-resolution test image;information loss prevention;multi PIE face database;PUT face database;face recognition;yaw rotation;Three-dimensional displays;Training;Shape;Mouth;Face recognition;Image resolution;Training data;Face hallucination;super-resolution;image registration;3D morphable model;3D face modeling;texture extraction},
doi={10.1109/CRV.2015.26},
ISSN={},
month={June},}
@INPROCEEDINGS{7156442,
author={M. C. E. {Rai} and N. {Werghi} and C. {Tortorici} and H. {Al-Muhairi} and H. A. {Safar}},
booktitle={2015 International Conference on Information and Communication Technology Research (ICTRC)},
title={Mesh LBP features for 3D constrained local model},
year={2015},
volume={},
number={},
pages={144-147},
abstract={We propose an automatic facial landmarks detection in 3D mesh manifold. The method is based on 3D Constrained Local Model (CLM) which learns both global variations in 3D face scan and local ones around every vertex landmark. Differently from the other approaches of CLM, our contribution is a full 3D mesh. The framework exploits the intrinsic 3D features around the mesh vertices by utilizing histogram-based mesh Local Binary Patterns (mesh-LBP). The experiments are conducted on publicly available 3D face scans Bosphorus database.},
keywords={face recognition;feature extraction;mesh generation;object detection;mesh LBP features;automatic facial landmarks detection;3D mesh manifold;3D constrained local model;3D CLM;3D face scan;vertex landmark;full 3D mesh;3D features;mesh vertices;histogram-based mesh local binary patterns;mesh-LBP;Bosphorus database;Three-dimensional displays;Solid modeling;Face;Shape;Histograms;Computational modeling;Training;Constrained local model;Histograms-based mesh LBP;3D Facial landmarks;Landmarks detection},
doi={10.1109/ICTRC.2015.7156442},
ISSN={},
month={May},}
@INPROCEEDINGS{7284882,
author={E. {Bondi} and P. {Pala} and S. {Berretti} and A. {Del Bimbo}},
booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
title={Reconstructing high-resolution face models from Kinect depth sequences acquired in uncooperative contexts},
year={2015},
volume={07},
number={},
pages={1-6},
abstract={Performing face recognition across 3D scans of different resolution is now attracting an increasing interest thanks to the introduction of a new generation of depth cameras, capable of acquiring color/depth images over time. However, these devices have still a much lower resolution than the 3D high-resolution scanners typically used for face recognition applications. If data are acquired without user cooperation, the problem is even more challenging and the gap of resolution between probe and gallery scans can yield to a severe loss in terms of recognition accuracy. Based on these premises, we propose a method to build a higher-resolution 3D face model from 3D data acquired by a low-resolution scanner. This face model is built using data acquired when a person passes in front of the scanner, following an uncooperative protocol. To perform non-rigid registration of point sets and account for deformation of the face during the acquisition process, the Coherent Point Drift (CPD) method is used. Registered 3D data are filtered through a variant of the lowess method to remove outliers and build the final face model. The proposed approach is evaluated in terms of accuracy of face reconstruction and face recognition.},
keywords={face recognition;image filtering;image reconstruction;image registration;image resolution;image sequences;interactive systems;user interfaces;3D data filtering;CPD method;coherent point drift;point set nonrigid registration;user cooperation;face recognition;Kinect depth sequence;high-resolution face model reconstruction;Three-dimensional displays;Face;Solid modeling;Computational modeling;Image reconstruction;Cameras;Face recognition},
doi={10.1109/FG.2015.7284882},
ISSN={},
month={May},}
@INPROCEEDINGS{7780392,
author={X. {Zhu} and Z. {Lei} and X. {Liu} and H. {Shi} and S. Z. {Li}},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Face Alignment Across Large Poses: A 3D Solution},
year={2016},
volume={},
number={},
pages={146-155},
abstract={Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45), lacking the ability to align faces in large poses up to 90. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods.},
keywords={face recognition;feature extraction;neural nets;pose estimation;solid modelling;CNN;convolutional neural network;3D face model;3DDFA;3D dense face alignment;facial pixel extraction;large poses;Face;Three-dimensional displays;Computational modeling;Solid modeling;Shape;Two dimensional displays;Feature extraction},
doi={10.1109/CVPR.2016.23},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{7177468,
author={Y. {Andreu-Cabedo} and P. {Castellano} and S. {Colantonio} and G. {Coppini} and R. {Favilla} and D. {Germanese} and G. {Giannakakis} and D. {Giorgi} and M. {Larsson} and P. {Marraccini} and M. {Martinelli} and B. {Matuszewski} and M. {Milanic} and M. {Pascali} and M. {Pediaditis} and G. {Raccichini} and L. {Randeberg} and O. {Salvetti} and T. {Stromberg}},
booktitle={2015 IEEE International Conference on Multimedia and Expo (ICME)},
title={Mirror mirror on the wall #x2026; An intelligent multisensory mirror for well-being self-assessment},
year={2015},
volume={},
number={},
pages={1-6},
abstract={The face reveals the healthy status of an individual, through a combination of physical signs and facial expressions. The project SEMEOTICONS is translating the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, images, and 3D scans of the face. SEMEOTICONS is developing a multisensory platform, in the form of a smart mirror, looking for signs related to cardio-metabolic risk. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. Building the multisensory mirror requires addressing significant scientific and technological challenges, from touch-less data acquisition, to real-time processing and integration of multimodal data.},
keywords={data integration;face recognition;feature extraction;medical computing;mirrors;intelligent multisensory mirror;facial expression;Semeoticons;smart mirror;user guidance;cardio-metabolic risk;touch-less data acquisition;multimodal data integration;semeiotic code;computational descriptors;Face;Three-dimensional displays;Mirrors;Skin;Indexes;Monitoring;Biomedical monitoring;Cardio-metabolic risk;unobtrusive health monitoring;3D face detection and tracking;3D morphometric analysis;multispectral imaging;breath analysis;psycho-somatic status recognition;multimodal data integration},
doi={10.1109/ICME.2015.7177468},
ISSN={1945-7871},
month={June},}
@INPROCEEDINGS{7550083,
author={W. P. {Koppen} and W. J. {Christmas} and D. J. M. {Crouch} and W. F. {Bodmer} and J. V. {Kittler}},
booktitle={2016 International Conference on Biometrics (ICB)},
title={Extending non-negative matrix factorisation to 3D registered data},
year={2016},
volume={},
number={},
pages={1-8},
abstract={The use of non-negative matrix factorisation (NMF) on 2D face images has been shown to result in sparse feature vectors that encode for local patches on the face, and thus provides a statistically justified approach to learning parts from wholes. However successful on 2D images, the method has so far not been extended to 3D images. The main reason for this is that 3D space is a continuum and so it is not apparent how to represent 3D coordinates in a non-negative fashion. This work compares different non-negative representations for spatial coordinates, and demonstrates that not all non-negative representations are suitable. We analyse the representational properties that make NMF a successful method to learn sparse 3D facial features. Using our proposed representation, the factorisation results in sparse and interpretable facial features.},
keywords={face recognition;feature extraction;image registration;image representation;learning (artificial intelligence);matrix decomposition;nonnegative matrix factorization;NMF;3D face image registration;representational property;3D facial feature learning;Face;Three-dimensional displays;Shape;Principal component analysis;Two dimensional displays;Facial features;Encoding},
doi={10.1109/ICB.2016.7550083},
ISSN={},
month={June},}
@INPROCEEDINGS{8100068,
author={W. {Peng} and Z. {Feng} and C. {Xu} and Y. {Su}},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Parametric T-Spline Face Morphable Model for Detailed Fitting in Shape Subspace},
year={2017},
volume={},
number={},
pages={5515-5523},
abstract={Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration for the synthesis of 3D faces by assuming that faces are in a linear class. However, the human face is in a nonlinear manifold, and a new test are always not in the pre-learnt subspace accurately because of the disparity brought by ethnicity, age, gender, etc. In the paper, we propose a parametric T-spline morphable model (T-splineMM) for 3D face representation, which has great advantages of fitting data from an unknown source accurately. In the model, we describe a face by C^2 T-spline surface, and divide the face surface into several shape units (SUs), according to facial action coding system (FACS), on T-mesh instead of on the surface directly. A fitting algorithm is proposed to optimize coefficients of T-spline control point components along pre-learnt identity and expression subspaces, as well as to optimize the details in refinement progress. As any pre-learnt subspace is not complete to handle the variety and details of faces and expressions, it covers a limited span of morphing. SUs division and detail refinement make the model fitting the facial muscle deformation in a larger span of morphing subspace. We conduct experiments on face scan data, kinect data as well as the space-time data to test the performance of detail fitting, robustness to missing data and noise, and to demonstrate the effectiveness of our model. Convincing results are illustrated to demonstrate the effectiveness of our model compared with the popular methods.},
keywords={face recognition;feature extraction;image morphing;image reconstruction;mesh generation;muscle;solid modelling;splines (mathematics);shape units;facial action coding system;fitting algorithm;T-spline control point components;expression subspaces;face scan data;parametric T-spline face morphable model;shape subspace pre-learnt subspace;human face;parametric T-spline morphable model for 3D;T-spline surface;face surface;kinect data;space-time data;Face;Three-dimensional displays;Shape;Solid modeling;Lips;Splines (mathematics);Computational modeling},
doi={10.1109/CVPR.2017.585},
ISSN={1063-6919},
month={July},}
@ARTICLE{7544592,
author={E. {Bondi} and P. {Pala} and S. {Berretti} and A. {Del Bimbo}},
journal={IEEE Transactions on Information Forensics and Security},
title={Reconstructing High-Resolution Face Models From Kinect Depth Sequences},
year={2016},
volume={11},
number={12},
pages={2843-2853},
abstract={Performing face recognition across 3D scans with different resolution is now attracting an increasing interest thanks to the introduction of a new generation of depth cameras, capable of acquiring color/depth images over time. In fact, these devices acquire and provide depth data with much lower resolution compared with the 3D high-resolution scanners typically used for face recognition applications. If data are acquired without user cooperation, the problem is even more challenging, and the gap of resolution between probe and gallery scans can yield to a severe loss in terms of recognition accuracy. Based on these premises, we propose a method to build a higher resolution 3D face model from 3D data acquired by a low-resolution scanner. This face model is built using data acquired when a person passes in front of the scanner, without assuming any particular cooperation. The 3D data are registered and filtered by combining a model of the expected distribution of the acquisition error with a variant of the lowess method to remove outliers and build the final face model. The proposed approach is evaluated in terms of accuracy of face reconstruction and face recognition.},
keywords={cameras;face recognition;image colour analysis;image filtering;image reconstruction;image resolution;image scanners;image sensors;image sequences;regression analysis;3D data registration;low-resolution scanner;gallery scans;3D high-resolution scanners;color images;depth images;depth cameras;3D scans;face recognition;Kinect depth sequences;high-resolution face model reconstruction;Face;Three-dimensional displays;Cameras;Solid modeling;Image reconstruction;Face recognition;Data models;Kinect depth camera;increased resolution;manifold estimation;locally weighted regression;face recognition},
doi={10.1109/TIFS.2016.2601059},
ISSN={1556-6013},
month={Dec},}
@INPROCEEDINGS{7961749,
author={L. {Wang} and X. {Yu} and D. N. {Metaxas}},
booktitle={2017 12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)},
title={A Coupled Encoder-Decoder Network for Joint Face Detection and Landmark Localization},
year={2017},
volume={},
number={},
pages={251-257},
abstract={Face detection and landmark localization have been extensively investigated and are the prerequisite for many face applications, such as face recognition and 3D face reconstruction. Most existing methods achieve success on only one of the two problems. In this paper, we propose a coupled encoder-decoder network to jointly detect faces and localize facial key points. The encoder and decoder generate response maps for facial landmark localization. Moreover, we observe that the intermediate feature maps from the encoder and decoder have strong power in describing facial regions, which motivates us to build a unified framework by coupling the feature maps for multi-scale cascaded face detection. Experiments on face detection show strongly competitive results against the existing methods on two public benchmarks. The landmark localization further shows consistently better accuracy than state-of-the-arts on three face-in-the-wild databases.},
keywords={decoding;encoding;face recognition;feature extraction;network theory (graphs);encoder-decoder network;face detection;landmark localization;facial key points localization;response maps;feature maps;Face;Face detection;Feature extraction;Decoding;Training;Neural networks;Proposals},
doi={10.1109/FG.2017.40},
ISSN={},
month={May},}
@INPROCEEDINGS{7830326,
author={J. {Li} and Y. {Zhang} and P. {Xu} and S. {Lan} and S. {Li}},
booktitle={2016 9th International Symposium on Computational Intelligence and Design (ISCID)},
title={3D Personalized Face Modeling Based on KINECT2},
year={2016},
volume={1},
number={},
pages={197-201},
abstract={As a challenging topic in computer graphics and computer vision, 3D face modeling has been applied in various fields including film and animation production, game development as well as medical analysis. In this paper, we mainly aims at face personalized modeling optimization based on KINECT2. The optimization of the scheme proposed is the mesh deformation based on the differential coordinates constrained by face++ feature points and facial muscle function, namely firstly by extracting the location of feature points on the color image and depth map to obtain the feature information of the personalized face, then aligning the intermediate results of the KINECT2 modeling to the feature information. It is good to keep the local grid differential property, at the same time. In this paper, the automatic UV mapping is realized by means of planar differential mesh deformation, which improves the accuracy and efficiency of manual adjustment of UV coordinates in Maya. The experimental results show that the optimization results are more fit in with the individual face.},
keywords={computer vision;face recognition;feature extraction;image colour analysis;mesh generation;optimisation;solid modelling;manual adjustment efficiency;accuracy improvement;planar differential mesh deformation;automatic UV mapping;local grid differential property;KINECT2 modeling;depth map;color image;feature points location extraction;facial muscle function;face++ feature points;differential coordinates;face personalized modeling optimization;computer vision;computer graphics;3D personalized face modeling;Handheld computers;Decision support systems;Computational intelligence;Erbium;3D face modeling;KINECT2;Mesh deformation;The differential coordinates},
doi={10.1109/ISCID.2016.1052},
ISSN={2473-3547},
month={Dec},}
@ARTICLE{7053946,
author={F. {Vicente} and Z. {Huang} and X. {Xiong} and F. {De la Torre} and W. {Zhang} and D. {Levi}},
journal={IEEE Transactions on Intelligent Transportation Systems},
title={Driver Gaze Tracking and Eyes Off the Road Detection System},
year={2015},
volume={16},
number={4},
pages={2014-2027},
abstract={Distracted driving is one of the main causes of vehicle collisions in the United States. Passively monitoring a driver's activities constitutes the basis of an automobile safety system that can potentially reduce the number of accidents by estimating the driver's focus of attention. This paper proposes an inexpensive vision-based system to accurately detect Eyes Off the Road (EOR). The system has three main components: 1) robust facial feature tracking; 2) head pose and gaze estimation; and 3) 3-D geometric reasoning to detect EOR. From the video stream of a camera installed on the steering wheel column, our system tracks facial features from the driver's face. Using the tracked landmarks and a 3-D face model, the system computes head pose and gaze direction. The head pose estimation algorithm is robust to nonrigid face deformations due to changes in expressions. Finally, using a 3-D geometric analysis, the system reliably detects EOR.},
keywords={driver information systems;face recognition;feature extraction;gaze tracking;geometry;object detection;pose estimation;road accidents;road safety;road traffic;facial expressions;driver-dependent calibration;3D geometric analysis;nonrigid face deformations;head pose estimation algorithm;gaze direction;steering wheel column;video stream;3D geometric reasoning;gaze estimation;robust facial feature tracking;EOR detection;eyes off the road detection system;inexpensive vision-based system;driver attention focus;automobile safety system;passively driver activity monitoring;United States;vehicle collisions;distracted driving;driver gaze tracking;Vehicles;Estimation;Cameras;Face;Three-dimensional displays;Magnetic heads;Driver monitoring system;eyes off the road detection;gaze estimation;head pose estimation;Driver monitoring system;eyes off the road detection;gaze estimation;head pose estimation},
doi={10.1109/TITS.2015.2396031},
ISSN={1524-9050},
month={Aug},}
@INPROCEEDINGS{7780913,
author={C. {Zhang} and W. A. P. {Smith} and A. {Dessein} and N. {Pears} and H. {Dai}},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Functional Faces: Groupwise Dense Correspondence Using Functional Maps},
year={2016},
volume={},
number={},
pages={5033-5041},
abstract={In this paper we present a method for computing dense correspondence between a set of 3D face meshes using functional maps. The functional maps paradigm brings with it a number of advantages for face correspondence. First, it allows us to combine various notions of correspondence. We do so by proposing a number of face-specific functions, suited to either within-or between-subject correspondence. Second, we propose a groupwise variant of the method allowing us to compute cycle-consistent functional maps between all faces in a training set. Since functional maps are of much lower dimension than point-to-point correspondences, this is feasible even when the input meshes are very high resolution. Finally, we show how a functional map provides a geometric constraint that can be used to filter feature matches between non-rigidly deforming surfaces.},
keywords={image filtering;image matching;image resolution;groupwise dense correspondence;3D face meshes;cycle-consistent functional maps;point-to-point correspondences;resolution;feature matches filter;Shape;Optimization;Facial animation;Three-dimensional displays;Linear programming;Approximation algorithms;Computational modeling},
doi={10.1109/CVPR.2016.544},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{7426891,
author={I. {Lagha} and A. {Othman}},
booktitle={2015 5th International Conference on Information Communication Technology and Accessibility (ICTA)},
title={Human face texture generation based on MPEG-4 standard},
year={2015},
volume={},
number={},
pages={1-5},
abstract={This paper is about a new method of generation texture for 3D face model. The 3D model is used as a signing avatar to help deaf people to interact easily. This approach is based on the MPEG-4 standard for facial detection points from two orthogonal photos representing the front and side view of a human head. The two photos are cut, combined and finally deformed corresponding to an UV-map to finally generate a texture which can be mapped on the 3D head.},
keywords={avatars;face recognition;solid modelling;human face texture generation;MPEG-4 standard;3D model;facial detection points;UV-map;Deformable models;Face;Face recognition;Image recognition;Parametrization;MPEG-4;Texture mapping;Human face},
doi={10.1109/ICTA.2015.7426891},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7410807,
author={S. {Suwajanakorn} and S. M. {Seitz} and I. {Kemelmacher-Shlizerman}},
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)},
title={What Makes Tom Hanks Look Like Tom Hanks},
year={2015},
volume={},
number={},
pages={3952-3960},
abstract={We reconstruct a controllable model of a person from a large photo collection that captures his or her persona, i.e., physical appearance and behavior. The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned. Moreover, we show the ability to drive or puppeteer the captured person B using any other video of a different person A. In this scenario, B acts out the role of person A, but retains his/her own personality and character. Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem. We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video.},
keywords={face recognition;image reconstruction;image texture;controllable person model reconstruction;photo collection;unstructured photo collections;B acts;3D face reconstruction;face tracking;face alignment;multitexture modeling;Internet imagery;Internet video;Three-dimensional displays;Shape;Face;Videos;Solid modeling;Image reconstruction;Deformable models},
doi={10.1109/ICCV.2015.450},
ISSN={2380-7504},
month={Dec},}
@ARTICLE{8309989,
author={H. {Zhao} and X. {Jin} and X. {Huang} and M. {Chai} and K. {Zhou}},
journal={IEEE Computer Graphics and Applications},
title={Parametric Reshaping of Portrait Images for Weight-change},
year={2018},
volume={38},
number={1},
pages={77-90},
abstract={We present an easy-to-use parametric image retouching method for thinning or fattening a face in a single portrait image while maintaining a close similarity to the source image. First, our method reconstructs a 3D face from the input face image using a morphable model. Second, according to the linear regression equation derived from the depth statistics of the soft tissue in the face and the user-set parameters of weight-change degree, we calculate the new positions of the feature points. The Laplacian deformation method is then used for non-feature points in the 3D face model. Our model-based reshaping process can achieve globally consistent editing effects without noticeable artifacts. We seamlessly blend the reshaped face region with the background using image retargeting method based on mesh parametrization. The effectiveness of our algorithm is demonstrated by experiments and user study.},
keywords={computer graphics;face recognition;image reconstruction;mesh generation;regression analysis;solid modelling;parametric reshaping;portrait images;easy-to-use parametric image retouching method;source image;input face image;morphable model;linear regression equation;depth statistics;soft tissue;weight-change degree;feature points;Laplacian deformation method;nonfeature points;reshaping process;globally consistent editing effects;reshaped face region;image retargeting method;mesh parametrization;3D face model;Rendering (computer graphics);Three-dimensional displays;Solid modeling;Mathematical model;Two dimensional displays;Computational modeling;Linear regression;Applications;facial reshaping;portrait retouching;image retargeting;computer graphics;picture/image generation},
doi={10.1109/MCG.2018.011461529},
ISSN={0272-1716},
month={Jan},}
@INPROCEEDINGS{8123040,
author={N. {Mohsin} and S. {Payandeh}},
booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
title={Localization and identification of body extremities based on data from multiple depth sensors},
year={2017},
volume={},
number={},
pages={2736-2741},
abstract={This paper explores the novel use of multiple depth sensors to overcome occlusions and improve localization and tracking of body extremities. The usage of data from only depth sensors not only overcomes visual challenges associated with RGB sensors under low illumination, but also protects the identity of surveyed person with high confidentiality. For integrating depth information from multiple sources, the paper presents first an overview of a novel calibration method for multiple depth sensors. In case of occlusion of any fiducial point in the primary sensor's depth image, co-ordinates of the point can be obtained from the frame of other sensors using the calibration parameters. To localize salient body parts such as hands, head and feet, a surface triangular mesh is applied on generated 3D point cloud from the primary sensor. The geodesic extrema from the mesh coincide with body extremities. The body extremities can be identified based on those relative geodesic distances between the extremities. Once the body parts are labelled, a portion of body can be targeted and evaluated for specific gait analysis and visualization. For the performance evaluation, our calibration method has fared well in comparison to other available techniques. Also, our proposed localization of salient body parts is able to successfully tag the specific body part i.e. the head region.},
keywords={calibration;data visualisation;feature extraction;gait analysis;image motion analysis;image sensors;mesh generation;object detection;sensor fusion;RGB sensors;salient body parts;depth information;body extremities localization;body extremities identification;multiple depth sensor data;body extremities tracking;illumination;identity confidentiality;specific body part tagging;fiducial point occlusion;primary sensor depth image;3D point cloud;mesh geodesic extrema;relative geodesic distances;gait analysis;visualization;Sensors;Three-dimensional displays;Calibration;Extremities;Image sensors;Image segmentation;Cameras;multiple depth sensors calibration;body extremities localization;Kinect II;geodesic distances},
doi={10.1109/SMC.2017.8123040},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8397011,
author={S. {Chen} and M. {Dong} and J. {Le} and S. {Barbat}},
booktitle={2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
title={Understanding Human Aging Patterns from a Machine Perspective},
year={2018},
volume={},
number={},
pages={226-231},
abstract={Recent research shows that the aging patterns deeply learned from large-scale data lead to significant performance improvement on age estimation. However, the insight about why and how deep learning models achieved superior performance is inadequate. In this paper, we propose to analyze, visualize and understand the deep aging patterns. We first train a series of convolutional neural networks for age estimation, and then illustrate the learning outcomes using feature maps, activation histograms, and deconvolution. We also develop a visualization method that can compare the facial appearance and track its changes at different ages through the mapping between 2D images and a 3D face template. Our framework provides an innovative way to understand human facial aging process from a machine perspective.},
keywords={data visualisation;face recognition;feedforward neural nets;learning (artificial intelligence);machine perspective;large-scale data;deep learning models;deep aging patterns;convolutional neural networks;visualization method;human facial aging process;3D face template;2D images;Aging;Estimation;Face;Three-dimensional displays;Visualization;Deconvolution;Feature extraction;Age Estimation;Convolutional Neural Networks;Visualization},
doi={10.1109/MIPR.2018.00055},
ISSN={},
month={April},}
@ARTICLE{8103313,
author={S. {Yoon} and J. {Lewis} and T. {Rhee}},
journal={IEEE Computer Graphics and Applications},
title={Blending Face Details: Synthesizing a Face Using Multiscale Face Models},
year={2017},
volume={37},
number={6},
pages={65-75},
abstract={In this article, we present a novel approach to synthesize a new 3D face model using weighted blending of multiscale details across different faces including human (see Figure 1a) and nonhuman characters (see Figures 1b and 1c). Our approach decomposes face models into component scales, with a correspondence of salient facial features across faces. Specifically, we create a MFM that hierarchically represents the face's spatial details. A 3D face mesh is parameterized into 2D parameter space and decomposed into a base surface and multiscale continuous displacement maps (CDMs). Each MFM represents face details from coarse to fine scales while providing full correspondences across CDMs.},
keywords={computer graphics;computer graphics;MFM;CDMs;multiscale continuous displacement maps;2D parameter space;3D face mesh;salient facial features;nonhuman characters;weighted multiscale detail blending;face details;multiscale face models;Face recognition;Computational modeling;Solid modeling;Three-dimensional displays;Semantics;Shape analysis;Splines (mathematics);computer graphics;face modeling;multiscale face model;continuous displacement maps;blendshapes;multilevel b-spline;parameterization},
doi={10.1109/MCG.2017.4031069},
ISSN={0272-1716},
month={November},}
@INPROCEEDINGS{8688162,
author={F. {Cao} and F. {Yan} and Y. {Gu} and C. {Ding} and Y. {Zhuang} and W. {Wang}},
booktitle={2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)},
title={A Novel Image Model of Point Clouds and its Application in Place Recognition},
year={2018},
volume={},
number={},
pages={79-83},
abstract={In this paper, we present a novel panoramic image model for scattered point clouds, and apply it to the problem of place recognition. We project a point cloud onto a sphere, and then the sphere is divided into a set of individual grids by longitudes and latitudes. Each grid is regard as a pixel and its value is computed using the geometrical relationship among the points in the grid and its neighbors. For convenience, the sphere is transferred into a flat. Since point clouds are converted to 2D images, we use ORB features and bag of words technique to solve place recognition problem. Our experimental results show that our image model is a more universal one and achieve a good performance in place recognition in both accuracy and efficiency.},
keywords={},
doi={10.1109/CYBER.2018.8688162},
ISSN={2379-7711},
month={July},}
@INPROCEEDINGS{7533187,
author={L. {Ding} and A. {Elliethy} and E. {Freedenberg} and S. A. {Wolf-Johnson} and J. {Romphf} and P. {Christensen} and G. {Sharma}},
booktitle={2016 IEEE International Conference on Image Processing (ICIP)},
title={Comparative analysis of homologous buildings using range imaging},
year={2016},
volume={},
number={},
pages={4378-4382},
abstract={This paper reports on a novel application of computer vision and image processing technologies to an interdisciplinary project in architectural history that seeks to help identify and visualize differences between homologous buildings constructed to a common template design. By identifying the mutations in homologous buildings, we assist humanists in giving voice to the contributions of the myriad additional “authors” for these buildings beyond their primary designers. We develop a framework for comparing 3D point cloud representations of homologous buildings captured using lidar: focusing on identifying similarities and differences, both among 3D scans of different buildings and between the 3D scans and the design specifications of architectural drawings. The framework addresses global and local alignment for highlighting gross differences as well as differences in individual structural elements and provides methods for readily highlighting the differences via suitable visualizations. The framework is demonstrated on pairs of homologous buildings selected from the Canadian and Ottoman rail networks. Results demonstrate the utility of the framework confirming differences already apparent to the humanist researchers and also revealing new differences that were not previously observed.},
keywords={buildings (structures);design engineering;optical radar;radar imaging;structural engineering computing;Ottoman rail networks;Canadian rail networks;structural elements;local alignment;global alignment;architectural drawings;design specifications;3D scans;lidar;3D point cloud representations;mutation identification;range imaging;homologous buildings;Laser radar;Heating;Three-dimensional displays;architectural biometrics;building difference visualization;point cloud comparison;range imaging;visual big data analytics},
doi={10.1109/ICIP.2016.7533187},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{7881713,
author={ and and and and X. {Fu}},
booktitle={2017 IEEE International Conference on Big Data and Smart Computing (BigComp)},
title={Head pose-free eye gaze prediction for driver attention study},
year={2017},
volume={},
number={},
pages={42-46},
abstract={Driver's gaze direction is an indicator of driver state and plays a significantly role in driving safety. Traditional gaze zone estimation methods based on eye model have disadvantages due to the vulnerability under large head movement. Different from these methods, an appearance-based head pose-free eye gaze prediction method is proposed in this paper, for driver gaze zone estimation under free head movement. To achieve this goal, a gaze zone classifier is trained with head vectors and eye image features by random forest. The head vector is calculated by Pose from Orthography and Scaling with ITerations (POSIT) where a 3D face model is combined with facial landmark detection. And the eye image features are derived from eye images which extracted through eye region localization. These features are presented as the combination of sparse coefficients by sparse encoding with eye image dictionary, having good potential to carry information of the eye images. Experimental results show that the proposed method is applicable in real driving environment.},
keywords={driver information systems;face recognition;feature extraction;gaze tracking;image classification;iterative methods;learning (artificial intelligence);object detection;pose estimation;solid modelling;vectors;appearance-based head pose-free eye gaze prediction method;driver attention study;driver state indicator;driving safety;eye model;free head movement;driver gaze zone estimation;gaze zone classifier;head vector calculation;random forest;pose-from-orthography-and-scaling-with-iterations;POSIT;3D face model;facial landmark detection;region localization;eye image feature extraction;sparse coefficients;sparse encoding;eye image dictionary;Dictionaries;Estimation;Feature extraction;Magnetic heads;Face;Training;Driver state;Head pose-free;Random forest;Gaze zone;Dictionary learning},
doi={10.1109/BIGCOMP.2017.7881713},
ISSN={2375-9356},
month={Feb},}
@INPROCEEDINGS{7785124,
author={F. {Maninchedda} and C. {Häne} and M. R. {Oswald} and M. {Pollefeys}},
booktitle={2016 Fourth International Conference on 3D Vision (3DV)},
title={Face Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization},
year={2016},
volume={},
number={},
pages={489-498},
abstract={We present a system which is able to reconstruct human faces on mobile devices with only on-device processing using the sensors which are typically built into a current commodity smart phone. Such technology can for example be used for facial authentication purposes or as a fast preview for further post-processing. Our method uses recently proposed techniques which compute depth maps by passive multi-view stereo directly on the device. We propose an efficient method which recovers the geometry of the face from the typically noisy point cloud. First, we show that we can safely restrict the reconstruction to a 2.5D height map representation. Therefore we then propose a novel low dimensional height map shape model for faces which can be fitted to the input data efficiently even on a mobile phone. In order to be able to represent instance specific shape details, such as moles, we augment the reconstruction from the shape model with a distance map which can be regularized efficiently. We thoroughly evaluate our approach on synthetic and real data, thereby we use both high resolution depth data acquired using high quality multi-view stereo and depth data directly computed on mobile phones.},
keywords={face recognition;image reconstruction;image resolution;stereo image processing;face reconstruction;mobile devices;height map shape model;fast regularization;facial authentication;depth maps;passive multiview stereo;face geometry;noisy point cloud;low dimensional height map shape model;high resolution depth data;high quality multiview stereo;Face;Computational modeling;Cameras;Three-dimensional displays;Shape;Solid modeling;Image reconstruction},
doi={10.1109/3DV.2016.59},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7301378,
author={P. {Polewski} and W. {Yao} and M. {Heurich} and P. {Krzystek} and U. {Stilla}},
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
title={Active learning approach to detecting standing dead trees from ALS point clouds combined with aerial infrared imagery},
year={2015},
volume={},
number={},
pages={10-18},
abstract={Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89% with less than 100 training examples, which corresponds to 10% of the pre-selected data pool.},
keywords={environmental factors;feature extraction;forestry;image classification;image colour analysis;infrared imaging;learning (artificial intelligence);matrix algebra;object detection;vegetation;active learning approach;standing dead tree detection;ALS point cloud;forest process;forest science;environmental science;aerial color infrared imagery;feature extraction;greedy approximation;kernelized feature matrix;error reduction;semisupervised classifier;Bavarian Forest National Park;Vegetation;Three-dimensional displays;Entropy;Training;Image segmentation;Logistics;Feature extraction},
doi={10.1109/CVPRW.2015.7301378},
ISSN={2160-7516},
month={June},}
@INPROCEEDINGS{8347107,
author={C. {Qu} and J. {Metzler} and E. {Monari}},
booktitle={2018 IEEE Winter Applications of Computer Vision Workshops (WACVW)},
title={ivisX: An Integrated Video Investigation Suite for Forensic Applications},
year={2018},
volume={},
number={},
pages={9-17},
abstract={Video data from surveillance cameras are nowadays an important instrument for investigating crimes and identifying the identity of an offender. The analysis of the mass data acquired from numerous cameras poses enormous challenges to police investigation authorities. Supporting softwares and video management tools currently on the market focus either on elaborate visualization and editing of video data, specific image processing or video content analysis tasks. As a result, such a scattered system landscape further exacerbates the complexity and difficulty of a timely analysis of the available data. This work presents our unified framework ivisX, which is an integrated suite to simplify the entire workflow of video data investigation. The algorithmic backbone of ivisX is built upon an effective content-based search algorithm using region covariance for low-resolution (LR) data and a novel 3D face super-resolution (FSR) approach, which can generate high-resolution (HR) 3D face models to render high-quality facial composites with a single blurred and pixelated face image of the LR domain. Moreover, ivisX has a modular design, which allows for flexible incorporation of various extensions ranging from processing and display of video data from multiple cameras to analysis and documentation of the results into a powerful integrated toolkit to assist forensic investigation.},
keywords={cameras;face recognition;image resolution;solid modelling;low-resolution data;integrated video investigation suite;surveillance cameras;video management tools;specific image processing;timely analysis;unified framework ivisX;video data investigation;3D face super-resolution approach;Cameras;Forensics;Face;Tools;Surveillance;Documentation;Three-dimensional displays},
doi={10.1109/WACVW.2018.00007},
ISSN={},
month={March},}
@INPROCEEDINGS{7298920,
author={ and R. {Ji} and },
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery},
year={2015},
volume={},
number={},
pages={3013-3021},
abstract={Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.},
keywords={Boltzmann machines;feature extraction;image colour analysis;image matching;learning (artificial intelligence);object detection;3D object detection;bimodal deep Boltzmann machines;RGBD imagery;3D scenes;point clouds;3D training data labeling;cross-modality cues;cross-modality deep learning framework;3D scene object detection;cross-modality feature learning;3D detection window;3D point cloud;exemplar shape matching;3D CAD models;2D positive samples;R-CNNs;raw feature extraction;RGB domains;depth domains;RMRC dataset;bimodal based deep feature learning framework;Three-dimensional displays;Solid modeling;Training;Frequency modulation;Joints;Detectors;Feature extraction},
doi={10.1109/CVPR.2015.7298920},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{8453724,
author={S. {Sihvo} and P. {Virjonen} and P. {Nevalainen} and J. {Heikkonen}},
booktitle={2018 Baltic Geodetic Congress (BGC Geomatics)},
title={Tree Detection around Forest Harvester Based on Onboard LiDAR Measurements},
year={2018},
volume={},
number={},
pages={364-367},
abstract={This paper proposes a new approach for the detection of tree locations around forest machines producing a situational model based on on-site terrestrial LiDAR data collected during harvesting operation. A triangularized ground model is used to planarize the point cloud in order to simplify the tree detection. The planarized ground makes the vertical cutting of the point cloud systematical. Tree stem lines detected from individual trees at individual scan views are used to guide the final alignment into global coordinates. The setup is numerically efficient and does not rely on any positioning and orientation system (POS) based e.g. on an inertial measurement unit (IMU) or global navigation satellite system (GNSS) or wheel rotation counter on the autonomous vehicle.},
keywords={optical radar;satellite navigation;tree detection;forest harvester;onboard LiDAR measurements;tree locations;forest machines;situational model;on-site terrestrial LiDAR data;triangularized ground model;planarized ground;tree stem lines;wheel rotation counter;Vegetation;Three-dimensional displays;Laser radar;Forestry;Semiconductor device modeling;Global navigation satellite system;Planarization;object recognition;collision avoidance;autonomous vehicles forestry},
doi={10.1109/BGC-Geomatics.2018.00075},
ISSN={},
month={June},}
@ARTICLE{7879309,
author={H. {Zhang} and C. {Ye}},
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
title={An Indoor Wayfinding System Based on Geometric Features Aided Graph SLAM for the Visually Impaired},
year={2017},
volume={25},
number={9},
pages={1592-1604},
abstract={This paper presents a 6-degree of freedom (DOF) pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired. The PE method involves two-graph simultaneous localization and mapping (SLAM) processes to reduce the accumulative pose error of the device. In the first step, the floor plane is extracted from the 3-D camera's point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch, and Zerrors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y, and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a visually impaired person in an indoor environment. The system uses the estimated pose and floor plan to locate the device user in a building and guides the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.},
keywords={biomedical engineering;geometry;indoor navigation;speech;vision defects;indoor wayfinding system;geometric features aided graph SLAM;visually impaired;pose estimation method;two-graph simultaneous localization;mapping processes;landmark node;yaw errors;computational time;speech interface;indoor navigation task;Simultaneous localization and mapping;RNA;Cameras;Three-dimensional displays;Navigation;Floors;Indoor environments;Blind navigation;wayfinding;robotic navigation aid;pose estimation;graph SLAM;3-D camera;Canes;Dependent Ambulation;Equipment Design;Equipment Failure Analysis;Humans;Imaging, Three-Dimensional;Patient Identification Systems;Reproducibility of Results;Self-Help Devices;Sensitivity and Specificity;Spatial Navigation;Treatment Outcome;User-Computer Interface;Visually Impaired Persons;Wireless Technology},
doi={10.1109/TNSRE.2017.2682265},
ISSN={1534-4320},
month={Sep.},}
@INPROCEEDINGS{7780547,
author={T. {Hackel} and J. D. {Wegner} and K. {Schindler}},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Contour Detection in Unstructured 3D Point Clouds},
year={2016},
volume={},
number={},
pages={1610-1618},
abstract={We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds &gt; 107 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.},
keywords={edge detection;feature extraction;graph theory;image classification;image coding;image representation;learning (artificial intelligence);object detection;contour detection;unstructured 3D point clouds;surface orientation;large-scale outdoor point clouds;high-quality surface;solid models;unstructured-inhomogeneous point clouds;contour extraction;two-stage discriminative learning problem;contour score;binary classifier;feature extraction;contour scores;higher-order MRF;graph cliques;Canny-style edge detection;image representation;Three-dimensional displays;Feature extraction;Solid modeling;Image edge detection;Surface reconstruction;Surface topography;Solids},
doi={10.1109/CVPR.2016.178},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{8569716,
author={T. {Akita} and Y. {Yamauchi} and H. {Fujiyoshi}},
booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
title={Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye Cameras},
year={2018},
volume={},
number={},
pages={1103-1108},
abstract={Recently, automated emergency brake systems for pedestrian have been commercialized. However, they cannot detect crossing pedestrians when turning at intersections because the field of view is not wide enough. Thus, we propose to utilize a surround view camera system becoming popular by making it into stereo vision which is robust for the pedestrian recognition. However, conventional stereo camera technologies cannot be applied due to fisheye cameras and uncalibrated camera poses. Thus we have created the new method to absorb difference of the pedestrian appearance between cameras by machine learning for the stereo vision. The method of stereo matching between image patches in each camera image was designed by combining D-Brief and NCC with SVM. Good generalization performance was achieved by it compared with individual conventional algorithms. Furthermore, feature amounts of the point cloud reconstructed by the stereo pairs are utilized with Random Forest to discriminate pedestrians. The algorithm was evaluated for the actual camera images of crossing pedestrians at various intersections, and 96.0% of pedestrian tracking rate with high position detection accuracy was achieved. They were compared with Faster R-CNN as the best pattern recognition technique, and our proposed method indicated better detection performance.},
keywords={cameras;image matching;image reconstruction;learning (artificial intelligence);object detection;object tracking;pedestrians;stereo image processing;support vector machines;traffic engineering computing;machine learning-based stereo vision algorithm;surround view fisheye cameras;automated emergency brake systems;crossing pedestrians;surround view camera system;pedestrian recognition;uncalibrated camera;pedestrian appearance;stereo matching;image patches;camera image;stereo pairs;pedestrian tracking rate;high position detection accuracy;detection performance;stereo camera technologies;D-Brief;SVM;random forest;Cameras;Turning;Distortion;Machine learning algorithms;Machine learning;Accidents;Feature extraction},
doi={10.1109/ITSC.2018.8569716},
ISSN={2153-0017},
month={Nov},}

