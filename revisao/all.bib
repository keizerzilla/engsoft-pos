@article{ISI:000399118600008,
abstract = {The vulnerability of face recognition systems to presentation attacks
(also known as direct attacks or spoof attacks) has received a great
deal of interest from the biometric community. The rapid evolution of
face recognition systems into real-time applications has raised new
concerns about their ability to resist presentation attacks,
particularly in unattended application scenarios such as automated
border control. The goal of a presentation attack is to subvert the face
recognition system by presenting a facial biometric artifact. Popular
face biometric artifacts include a printed photo, the electronic display
of a facial photo, replaying video using an electronic display, and 3D
face masks. These have demonstrated a high security risk for
state-of-the-art face recognition systems. However, several presentation
attack detection (PAD) algorithms (also known as countermeasures or
antispoofing methods) have been proposed that can automatically detect
and mitigate such targeted attacks. The goal of this survey is to
present a systematic overview of the existing work on face presentation
attack detection that has been carried out. This paper describes the
various aspects of face presentation attacks, including different types
of face artifacts, state-of-the-art PAD algorithms and an overview of
the respective research labs working in this domain, vulnerability
assessments and performance evaluation metrics, the outcomes of
competitions, the availability of public databases for benchmarking new
PAD algorithms in a reproducible manner, and finally a summary of the
relevant international standardization in this field. Furthermore, we
discuss the open challenges and future work that need to be addressed in
this evolving field of biometrics.},
author = {Ramachandra, Raghavendra and Busch, Christoph},
doi = {10.1145/3038924},
issn = {0360-0300},
journal = {ACM COMPUTING SURVEYS},
keywords = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
number = {1},
title = {{Presentation Attack Detection Methods for Face Recognition Systems: A Comprehensive Survey}},
volume = {50},
year = {2017}
}
@inproceedings{ISI:000460471100001,
abstract = {This paper presents an efficient 3D face recognition method to handle
facial expression. The proposed method uses the Surfaces Empirical Mode
Decomposition (SEMD), facial curves and local shape descriptor in a
matching process to overcome the distortions caused by expressions in
faces. The basic idea is that, the face is presented at different scales
by SEMD. Then the isometric invariant features on each scale are
extracted. After that, the geometric information is obtained on the 3D
surface in terms of radial and level facial curves. Finally, the feature
vectors on each scale are associated with their corresponding geometric
information. The presented method is validated on GavabDB database
resulting a rank 1 recognition rate (RR) of 98.9{\%} for all faces with
neutral and non-neutral expressions. This result outperforms other 3D
expression-invariant face recognition methods on the same database.},
annote = {2nd Mediterranean Conference on Pattern Recognition and Artificial
Intelligence (MedPRAI), Ibn Tofail Univ, Rabat, MOROCCO, MAR 27-28, 2018},
author = {Abbad, Abdelghafour and Abbad, Khalid and Tairi, Hamid},
booktitle = {PROCEEDINGS OF THE 2ND MEDITERRANEAN CONFERENCE ON PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE (MEDPRAI-2018)},
doi = {10.1145/3177148.3180087},
isbn = {978-1-4503-5290-1},
keywords = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Bahria Univ; Univ Larbi Tebessi Tebessa; OCP; ENSIAS; Int Assoc Pattern Recognit},
pages = {1--6},
title = {{3D face recognition in the presence of facial expressions based on empirical mode decomposition}},
year = {2018}
}
@inproceedings{ISI:000455343100004,
abstract = {RGB-D cameras, such as the Microsoft Kinect, provide us with the 3D
information, color and depth, associated with the scene. Interactive 3D
Tele-Immersion (i3DTI) systems use such RGB-D cameras to capture the
person present in the scene in order to collaborate with other remote
users and interact with the virtual objects present in the environment.
Using a single camera, it becomes difficult to estimate an accurate
skeletal pose and complete 3D model of the person, especially when the
person is not in the complete view of the camera. With multiple cameras,
even with partial views, it is possible to get a more accurate estimate
of the skeleton of the person leading to a better and complete 3D model.
In this paper, we present a real-time skeletal pose identification
approach that leverages on the inaccurate skeletons of the individual
Kinects, and provides a combined optimized skeleton. We estimate the
Probability of an Accurate Joint (PAJ) for each joint from all of the
Kinect skeletons. We determine the correct direction of the person and
assign the correct joint sides for each skeleton. We then use a greedy
consensus approach to combine the highly probable and accurate joints to
estimate the combined skeleton. Using the individual skeletons, we
segment the point clouds from all the cameras. We use the already
computed PAJ values to obtain the Probability of an Accurate Bone (PAB).
The individual point clouds are then combined one segment after another
using the calculated PAB values. The generated combined point cloud is a
complete and accurate 3D representation of the person present in the
scene. We validate our estimated skeleton against two well-known methods
by computing the error distance between the best view Kinect skeleton
and the estimated skeleton. An exhaustive analysis is performed by using
around 500000 skeletal frames in total, captured using 7 users and 7
cameras. Visual analysis is performed by checking whether the estimated
skeleton is completely present within the human model. We also develop a
3D Holo-Bubble game to showcase the real-time performance of the
combined skeleton and point cloud. Our results show that our method
performs better than the state-of-the-art approaches that use multiple
Kinects, in terms of objective error, visual quality and real-time user
performance.},
annote = {9th ACM Multimedia Systems Conference (MMSys), Centrum Wiskunde {\&}
Informatica Amsterdam, Amsterdam, NETHERLANDS, JUN 12-15, 2018},
author = {Desai, Kevin and Prabhakaran, Balakrishnan and Raghuraman, Suraj},
booktitle = {PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18)},
doi = {10.1145/3204949.3204958},
isbn = {978-1-4503-5192-8},
keywords = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Assoc Comp Machinery; ACM SIGMM; ACM SIGOPS; ACM SIGMOBILE; ACM SIGCOMM},
pages = {40--51},
title = {{Combining Skeletal Poses for 3D Human Model Generation using Multiple Kinects}},
year = {2018}
}
@inproceedings{ISI:000457913100097,
abstract = {In this paper, we present our latest progress in Emotion Recognition
techniques, which combines acoustic features and facial features in both
non-temporal and temporal mode. This paper presents the details of our
techniques used in the Audio-Video Emotion Recognition subtask in the
2018 Emotion Recognition in the Wild (EmotiW) Challenge. After the
multimodal results fusion, our final accuracy in Acted Facial Expression
in Wild (AFEW) test dataset achieves 61.87{\%}, which is 1.53{\%} higher
than the best results last year. Such improvements prove the
effectiveness of our methods.},
annote = {20th ACM International Conference on Multimodal Interaction (ICMI),
Boulder, CO, OCT 16-20, 2018},
author = {Liu, Chuanhe and Tang, Tianhao and Lv, Kui and Wang, Minghao},
booktitle = {ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION},
doi = {10.1145/3242969.3264989},
isbn = {978-1-4503-5692-3},
keywords = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Assoc Comp Machinery; Assoc Comp Machinery SIGCHI; Openstream; Microsoft; Univ Colorado Boulder, Inst Cognit Sci; audEERING},
pages = {630--634},
title = {{Multi-Feature Based Emotion Recognition for Video Clips}},
year = {2018}
}
@article{ISI:000433517100003,
abstract = {We propose a deformation-based representation for analyzing expressions
fromthree-dimensional (3D) faces. A point cloud of a 3D face is
decomposed into an ordered deformable set of curves that start from a
fixed point. Subsequently, a mapping function is defined to identify the
set of curves with an element of a high-dimensional matrix Lie group,
specifically the direct product of SE(3). Representing 3D faces as an
element of a high-dimensional Lie group has two main advantages. First,
using the group structure, facial expressions can be decoupled from a
neutral face. Second, an underlying non-linear facial expression
manifold can be captured with the Lie group and mapped to a linear
space, Lie algebra of the group. This opens up the possibility of
classifying facial expressions with linear models without compromising
the underlying manifold. Alternatively, linear combinations of
linearised facial expressions can be mapped back from the Lie algebra to
the Lie group. The approach is tested on the Binghamton University 3D
Facial Expression (BU-3DFE) and the Bosphorus datasets. The results show
that the proposed approach performed comparably, on the BU-3DFE dataset,
without using features or extensive landmark points.},
author = {Demisse, Girum G and Aouada, Djamila and Ottersten, Bjorn},
doi = {10.1145/3176649},
issn = {1551-6857},
journal = {ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS},
keywords = {revisao{\_}acm,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}acm,revisao{\_}webofscience},
number = {1, S},
title = {{Deformation-Based 3D Facial Expression Representation}},
volume = {14},
year = {2018}
}
@article{ISI:000458017400007,
abstract = {Making machines understand human expressions enables various useful
applications in human-machine interaction. In this article, we present a
novel facial expression recognition approach with 3D Mesh Convolutional
Neural Networks (3DMCNN) and a visual analytics-guided 3DMCNN design and
optimization scheme. From an RGBD camera, we first reconstruct a 3D face
model of a subject with facial expressions and then compute the
geometric properties of the surface. Instead of using regular
Convolutional Neural Networks (CNNs) to learn intensities of the facial
images, we convolve the geometric properties on the surface of the 3D
model using 3DMCNN. We design a geodesic distance-based convolution
method to overcome the difficulties raised from the irregular sampling
of the face surface mesh. We further present interactive visual
analytics for the purpose of designing and modifying the networks to
analyze the learned features and cluster similar nodes in 3DMCNN. By
removing low-activity nodes in the network, the performance of the
network is greatly improved. We compare our method with the regular
CNN-based method by interactively visualizing each layer of the networks
and analyze the effectiveness of our method by studying representative
cases. Testing on public datasets, our method achieves a higher
recognition accuracy than traditional image-based CNN and other 3D CNNs.
The proposed framework, including 3DMCNN and interactive visual
analytics of the CNN, can be extended to other applications.},
address = {New York, NY, USA},
author = {Jin, Hai and Lian, Yuanfeng and Hua, Jing},
doi = {10.1145/3200572},
issn = {2157-6904},
journal = {ACM Trans. Intell. Syst. Technol.},
keywords = {3D mesh convolutional neural networks,Facial expression analysis,revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience,visual analysis},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
number = {1, SI},
pages = {7:1----7:22},
publisher = {ACM},
title = {{Learning Facial Expressions with 3D Mesh Convolutional Neural Network}},
url = {http://doi.acm.org/10.1145/3200572},
volume = {10},
year = {2018}
}
@inproceedings{7823997,
abstract = {Face is being considered as one of the most commonly used biometric modality. The inaccuracy in two dimensional face recognition systems is mainly due to pose variations, occlusions, illumination etc. Among this, changes in illumination condition do not affect 3D face recognition systems. But pose variation drastically changes the appearance of face images. To solve the problems with depth map and texture images corrupted by head pose variations and the occlusions generated due to these pose variations, a reconstruction method is proposed which consist of three stages. In the first stage, the pose correction is done by Iterative Closest Point (ICP) algorithm and in the second stage the occluded region of the face is reconstructed by a resurfacing method called patch cloning. It is followed by the wrapping of reconstructed depth map by its texture to generate a 3D model. The statistical error between the original face and the reconstructed face is also evaluated. In this work, facial symmetry is used as a prior knowledge. Experiments are done with the FRAV3D database.},
annote = {From Duplicate 1 (3D face reconstruction by pose correction, patch cloning and texture wrapping - Naveen, S; Rugmini, K P; Moni, R S)

cited By 0

From Duplicate 2 (3D face reconstruction by pose correction, patch cloning and texture wrapping - Naveen, S; Rugmini, K P; Moni, R S)

26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Naveen, S and Rugmini, K P and Moni, R S},
booktitle = {2016 International Conference on Communication Systems and Networks (ComNet)},
doi = {10.1109/CSN.2016.7823997},
isbn = {978-1-5090-3349-2},
keywords = {biometrics (access control),estela,etapa1,face recognition,id308,ieeexplore,image,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {estela,etapa1,id308,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
pages = {112--116},
publisher = {IEEE},
title = {{3D face reconstruction by pose correction, patch cloning and texture wrapping}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014911379{\&}doi=10.1109{\%}2FCSN.2016.7823997{\&}partnerID=40{\&}md5=d83e42a8548567d921eea89900252c10 http://ieeexplore.ieee.org/document/7823997/},
year = {2016}
}
@inproceedings{Zhen20172252,
abstract = {In this paper, we propose an effective approach for automatic 4D Facial Expression Recognition (FER). The flow of 3D facial scans is first modeled to capture spatial deformations based on the recently-developed Riemannian approach, namely Dense Scalar Fields (DSF), where registration and comparison of neighboring 3D face frames are jointly led. The deformations are then fed into a temporal filtering based magnification step to amplify the slight facial actions over time. The proposed method allows revealing subtle (hidden) deformations which enhances the performance in classification. We evaluate our approach on the BU-4DFE dataset, and the state-of-art accuracy up to 94.18{\%} is achieved, which is superior to the top one so far reported, clearly demonstrating its effectiveness. {\textcopyright} 2016 IEEE.},
annote = {From Duplicate 1 (Magnifying subtle facial motions for 4D Expression Recognition - Zhen, Q; Huang, D; Wang, Y; Drira, H; Amor, B B; Daoudi, M)

cited By 0

From Duplicate 2 (Magnifying subtle facial motions for 4D Expression Recognition - Zhen, Q; Huang, D; Wang, Y; Drira, H; Amor, B B; Daoudi, M)

09/05/2018 Em processo de sele{\c{c}}{\~{a}}o (Etapa 1)
09/05/2018 Exclu{\'{i}}do (Etapa 1)},
author = {Zhen, Q and Huang, D and Wang, Y and Drira, H and Amor, B B and Daoudi, M},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7899971},
keywords = {artur,emotion recognition,etapa1,face recognition,id458,ieeexplore,image filteri,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {artur,etapa1,id458,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
pages = {2252--2257},
title = {{Magnifying subtle facial motions for 4D Expression Recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019103923{\&}doi=10.1109{\%}2FICPR.2016.7899971{\&}partnerID=40{\&}md5=692231105b2782590c4b2fa195758038},
year = {2017}
}
@inproceedings{Kim2018133,
abstract = {We propose a novel 3D face recognition algorithm using a deep convolutional neural network (DCNN) and a 3D face expression augmentation technique. The performance of 2D face recognition algorithms has significantly increased by leveraging the representational power of deep neural networks and the use of large-scale labeled training data. In this paper, we show that transfer learning from a CNN trained on 2D face images can effectively work for 3D face recognition by fine-tuning the CNN with an extremely small number of 3D facial scans. We also propose a 3D face expression augmentation technique which synthesizes a number of different facial expressions from a single 3D face scan. Our proposed method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets without using hand-crafted features. The 3D face identification using our deep features also scales well for large databases. {\textcopyright} 2017 IEEE.},
annote = {From Duplicate 1 (Deep 3D face identification - Kim, Donghyun; Hernandez, Matthias; Choi, Jongmoo; Medioni, Gerard)

22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)

From Duplicate 2 (Deep 3D face identification - Kim, D; Hernandez, M; Choi, J; Medioni, G)

cited By 0},
author = {Kim, Donghyun and Hernandez, Matthias and Choi, Jongmoo and Medioni, Gerard},
booktitle = {IEEE International Joint Conference on Biometrics, IJCB 2017},
doi = {10.1109/BTAS.2017.8272691},
isbn = {978-1-5386-1124-1},
keywords = {convolution,etapa1,face recognition,feedforward neural ne,gil,id145,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,gil,id145,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {oct},
pages = {133--142},
publisher = {IEEE},
title = {{Deep 3D face identification}},
url = {http://ieeexplore.ieee.org/document/8272691/ https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046247920{\&}doi=10.1109{\%}2FBTAS.2017.8272691{\&}partnerID=40{\&}md5=9660021125b86249b9768912252c19ff},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Shah2016,
abstract = {In applications such as 3D face synthesis and animation, a prominent face landmark is required to enable 3D face normalization, pose correction, 3D face recognition and reconstruction. Due to variations in facial expressions, automatic 3D face landmark localization remains a challenge. Nose tip is one of the salient landmarks in a human face. In this paper, a novel nose tip localization technique is proposed. In the proposed approach, the rotation of the 3D vector field is analyzed for robust and efficient nose tip localization. The proposed technique has the following three characteristics: (1) it does not require any training; (2) it does not rely on any particular model; (3) it is very efficient, requiring an average time of only 1.9s for nose tip detection. We tested the proposed technique on BU3DFE and Shrec'10 datasets. Experimental results show that the proposed technique is robust to variations in facial expressions, achieving a 100{\%} detection rate on these publicly available 3D face datasets. {\textcopyright} 2015 IEEE.},
annote = {From Duplicate 1 (Automatic 3D face landmark localization based on 3D vector field analysis - Shah, Syed Afaq Ali; Bennamoun, Mohammed; Boussaid, Farid)

22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)

From Duplicate 2 (Automatic 3D face landmark localization based on 3D vector field analysis - Shah, S A A; Bennamoun, M; Boussaid, F)

cited By 0},
author = {Shah, Syed Afaq Ali and Bennamoun, Mohammed and Boussaid, Farid},
booktitle = {International Conference Image and Vision Computing New Zealand},
doi = {10.1109/IVCNZ.2015.7761526},
isbn = {978-1-5090-0357-0},
keywords = {computer animation,etapa1,face recognition,gil,id139,ieeexplore,image reconstr,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,gil,id139,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
pages = {1--6},
publisher = {IEEE},
title = {{Automatic 3D face landmark localization based on 3D vector field analysis}},
url = {http://ieeexplore.ieee.org/document/7761526/ https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006942604{\&}doi=10.1109{\%}2FIVCNZ.2015.7761526{\&}partnerID=40{\&}md5=5da7d80624be2691571e8fafab5b9dbf},
volume = {2016-Novem},
year = {2016}
}
@inproceedings{ISI:000435270800012,
abstract = {In this paper, we present an automatic 3D face recognition system. This
system is based on the representation of human faces surfaces as
collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching
algorithm. To compare two facial surfaces, we compute a geodesic
distance between a pair of facial curves using a Riemannian geometry. In
the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor
(KNN) and Support Vector Machines (SVM). To test this method and
evaluate its performance, a simulation series of experiments were
performed on 3D Shape REtrieval Contest 2008 database (SHREC2008).},
annote = {14th International Conference on Computer Graphics, Imaging and
Visualization (CGiV), Marrakesh, MOROCCO, MAY 23-25, 2017},
author = {Ahdid, Rachid and Taifi, Khaddouj and Said, Said and Fakir, Mohamed and Manaut, Bouzid},
booktitle = {2017 14TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS, IMAGING AND VISUALIZATION (CGIV 2017)},
doi = {10.1109/CGiV.2017.25},
editor = {{Banissi, E and Sarfraz, M and Zeroual, A}},
isbn = {978-1-5386-0852-4},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
pages = {73--78},
series = {International Conference on Computer Graphics Imaging and Visualization},
title = {{Automatic Face Recognition System using Iso-Geodesic Curves in Riemanian Manifold}},
year = {2017}
}
@inproceedings{ISI:000426951900005,
abstract = {It is not only interesting to predict how an individual of a relatively
young age will look in the future but also to reconstruct the facial
appearance in the past during childhood. It can be even more desirable
when different circumstances, behavior and lifestyle and their impacts
on the facial shape appearance as a consequence are taken into account.
Such may be applicable for many practical reasons in healthcare,
forensics psychology, missing people and children, etc. This paper
presents the 3D Face Time Machine Matrix (FT2M), a 3D Dynamic Shape
Model which is a fusion of two models of ageing and rejuvenation with
facial shape variations due to lifestyle and behavioral factors. This
dynamic model is learned from a database of three dimensional facial
images which is built by ten individual age groups between 3 to 75 years
old. 3D facial aging modeling is a complex process since it affects both
the shape and texture of the face. We propose a Dynamic face model to
transform the given input face to his youthful or adulthood appearance
by taking into account his lifestyle and behavioral traits and the
probable changes may occur in perceptible appearance by altering its
shape and texture simultaneously.},
annote = {2nd International Conference on Bio-engineering for Smart Technologies
(BioSMART), Paris, FRANCE, AUG 30-SEP 01, 2017},
author = {{Majid Zadeh Heravi}, Farnaz and Nait-Ali, Amine},
booktitle = {2017 2ND INTERNATIONAL CONFERENCE ON BIO-ENGINEERING FOR SMART TECHNOLOGIES (BIOSMART)},
editor = {{Naitali, A}},
isbn = {978-1-5386-0706-0},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {IEEE; IEEE France Sect; IEEE France Sect EMBS Chapterie; Univ Paris Creteil Val Marne; Lab Images Signaux Systeme Intelligents; ESME; Inst Mines Telecom, Telecom SudParis},
title = {{A 3D Dynamic Shape Model to Simulate Rejuvenation {\&} Ageing Trajectory of 3D Face Images}},
year = {2017}
}
@inproceedings{ISI:000402657200006,
abstract = {Using of 3D images for the identification was in a field of the interest
of many researchers which developed a few methods offering good results.
However, there are few techniques exploiting the 3D asymmetry amongst
these methods. We propose fast algorithm for rough extraction face
asymmetry that is used to 3D face recognition with hidden Markov models.
This paper presents conception of fast method for determine 3D face
asymmetry. The research results indicate that face recognition with 3D
face asymmetry may be used in biometrics systems.},
annote = {8th International Conference on Image Processing and Communications
(IP{\&}C), UTP Univ Technol {\&} Sci, Inst Telecommunicat {\&} Comp Sci,
Bydgoszcz, POLAND, SEP 07-09, 2016},
author = {Bobulski, Janusz},
booktitle = {IMAGE PROCESSING AND COMMUNICATIONS CHALLENGES 8},
doi = {10.1007/978-3-319-47274-4_6},
editor = {{Choras, RS}},
isbn = {978-3-319-47274-4; 978-3-319-47273-7},
issn = {2194-5357},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {UTP Univ Technol {\&} Sci},
pages = {53--60},
series = {Advances in Intelligent Systems and Computing},
title = {{Face Recognition with 3D Face Asymmetry}},
volume = {525},
year = {2017}
}
@inproceedings{ISI:000380393900012,
abstract = {Facial Expression Recognition (FER) is one of the most active topics in
the domain of computer vision and pattern recognition, and it has
received increasing attention for its wide application potentials as
well as attractive scientific challenges. In this paper, we present a
novel method to automatic 3D FER based on geometric scattering
representation. A set of maps of shape features in terms of multiple
order differential quantities, i.e. the Normal Maps (NOM) and the Shape
Index Maps (SIM), are first jointly adopted to comprehensively describe
geometry attributes of the facial surface. The scattering operator is
then introduced to further highlight expression related cues on these
maps, thereby constructing geometric scattering representations of 3D
faces for classification. The scattering descriptor not only encodes
distinct local shape changes of various expressions as by several
milestone descriptors, such as SIFT, HOG, etc., but also captures subtle
information hidden in high frequencies, which is quite crucial to better
distinguish expressions that are easily confused. We evaluate the
proposed approach on the BU-3DFE database, and the performance is up to
84.8{\%} and 82.7{\%} with two commonly used protocols respectively which is
superior to the state of the art ones.},
annote = {IEEE 11th International Conference and Workshops on Automatic Face and
Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015},
author = {Yang, Xudong and Huang, Di and Wang, Yunhong and Chen, Liming},
booktitle = {2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG), VOL. 2},
isbn = {978-1-4799-6026-2},
issn = {2326-5396},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {IEEE Comp Soc; IEEE Biometric Council},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{Automatic 3D Facial Expression Recognition using Geometric Scattering Representation}},
year = {2015}
}
@inproceedings{ISI:000380516600012,
abstract = {Reconstructing 3D face models from multiple uncalibrated 2D face images
is usually done by using a single reference 3D face model or some
gender/ethnicity-specific 3D face models. However, different persons,
even those of the same gender or ethnicity, usually have significantly
different faces in terms of their overall appearance, which forms the
base of person recognition using faces. Consequently, existing 3D
reference model based methods have limited capability of reconstructing
3D face models for a large variety of persons. In this paper, we propose
to explore a reservoir of diverse reference models to improve the 3D
face reconstruction performance. Specifically, we convert the face
reconstruction problem into a multi-label segmentation problem. Its
energy function is formulated from different cues, including 1)
similarity between the desired output and the initial model, 2) color
consistency between different views, 3) smoothness constraint on
adjacent pixels, and 4) model consistency within local neighborhood.
Experimental results on challenging datasets demonstrate that the
proposed algorithm is capable of recovering high quality face models in
both qualitative and quantitative evaluations.},
annote = {International Conference on Biometrics (ICB), Phuket, THAILAND, MAY
19-22, 2015},
author = {Li, Jing and Long, Shuqin and Zeng, Dan and Zhao, Qijun},
booktitle = {2015 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)},
isbn = {978-1-4799-7824-3},
issn = {2376-4201},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {SIEW-SNGIEM AWARD; Kasetsart Univ; CHANWANICH; ST Elect; SAFRAN Morpho},
pages = {193--200},
series = {International Conference on Biometrics},
title = {{Example-Based 3D Face Reconstruction from Uncalibrated Frontal and Profile Images}},
year = {2015}
}
@inproceedings{ISI:000366513400025,
abstract = {Face recognition is an important task in pattern recognition and
computer vision. In this work a method for 3D face recognition in the
presence of facial expression and poses variations is proposed. The
method uses 3D shape data without color or texture information. A new
matching algorithm based on conformal mapping of original facial
surfaces onto a Riemannian manifold followed by comparison of conformal
and isometric invariants computed in the manifold is suggested.
Experimental results are presented using common 3D face databases that
contain significant amount of expression and pose variations.},
annote = {9th Conference of Optics and Photonics for Information Processing, San
Diego, CA, AUG 10-12, 2015},
author = {{Adriana Echeagaray-Patron}, Beatriz and Kober, Vitaly},
booktitle = {OPTICS AND PHOTONICS FOR INFORMATION PROCESSING IX},
doi = {10.1117/12.2186695},
editor = {{Awwal, AAS and Iftekharuddin, KM and Matin, MA and Vazquez, MG and Marquez, A}},
isbn = {978-1-62841-764-7},
issn = {0277-786X},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{3D face recognition based on matching of facial surfaces}},
volume = {9598},
year = {2015}
}
@article{ISI:000354988900026,
abstract = {Expression, occlusion, and pose variations are three main challenges for 3D face recognition. A novel method is presented to address 3D face recognition using scale-invariant feature transform (SIFT) features on 3D meshes. After preprocessing, shape index extrema on the 3D facial surface are selected as keypoints in the difference scale space and the unstable keypoints are removed after two screening steps. Then, a local coordinate system for each keypoint is established by principal component analysis (PCA). Next, two local geometric features are extracted around each keypoint through the local coordinate system. Additionally, the features are augmented by the symmetrization according to the approximate left-right symmetry in human face. The proposed method is evaluated on the Bosphorus, BU-3DFE, and Gavab databases, respectively. Good results are achieved on these three datasets. As a result, the proposed method proves robust to facial expression variations, partial external occlusions and large pose changes. {\textcopyright} 2015, Central South University Press and Springer-Verlag Berlin Heidelberg.},
annote = {From Duplicate 2 (Face recognition using SIFT features under 3D meshes - Zhang, C; Gu, Y.-Z.; Hu, K.-L.; Wang, Y.-G.)

cited By 5},
author = {Zhang, C and Gu, Y.-Z. and Hu, K.-L. and Wang, Y.-G. and Cheng, Zhang and Yu-zhang, Gu and Ke-li, Hu and Ying-guan, Wang},
doi = {10.1007/s11771-015-2700-x},
issn = {2095-2899},
journal = {JOURNAL OF CENTRAL SOUTH UNIVERSITY},
keywords = {,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {5},
pages = {1817--1825},
title = {{Face recognition using SIFT features under 3D meshes}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930008100{\&}doi=10.1007{\%}2Fs11771-015-2700-x{\&}partnerID=40{\&}md5=569ac008f1ef201b7fcbffd9e0faa163},
volume = {22},
year = {2015}
}
@article{Echeagaray-Patrón2017648,
abstract = {Face recognition is one of the most rapidly developing areas of image
processing and computer vision. In this work, a new method for face
recognition and identification using 3D facial surfaces is proposed. The
method is invariant to facial expression and pose variations in the
scene. The method uses 3D shape data without color or texture
information. The method is based on conformal mapping of original facial
surfaces onto a Riemannian manifold, followed by comparison of conformal
and isometric invariants computed in this manifold. Computer results are
presented using known 3D face databases that contain significant amount
of expression and pose variations.},
annote = {From Duplicate 1 (A method of face recognition using 3D facial surfaces - Echeagaray-Patr{\'{o}}n, B A; Kober, V I; Karnaukhov, V N; Kuznetsov, V V)

cited By 18},
author = {Echeagaray-Patron, B A and Kober, V I and Karnaukhov, V N and Kuznetsov, V V and Echeagaray-Patr{\'{o}}n, B A and Kober, V I and Karnaukhov, V N and Kuznetsov, V V},
doi = {10.1134/S1064226917060067},
issn = {1064-2269},
journal = {Journal of Communications Technology and Electronics},
keywords = {,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jun},
number = {6},
pages = {648--652},
title = {{A Method of Face Recognition Using 3D Facial Surfaces}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024129138{\&}doi=10.1134{\%}2FS1064226917060067{\&}partnerID=40{\&}md5=d4b2a551be78c542e1d7b64631adf914},
volume = {62},
year = {2017}
}
@article{Schönborn2017160,
abstract = {We present a novel fully probabilistic method to interpret a single face image with the 3D Morphable Model. The new method is based on Bayesian inference and makes use of unreliable image-based information. Rather than searching a single optimal solution, we infer the posterior distribution of the model parameters given the target image. The method is a stochastic sampling algorithm with a propose-and-verify architecture based on the Metropolis–Hastings algorithm. The stochastic method can robustly integrate unreliable information and therefore does not rely on feed-forward initialization. The integrative concept is based on two ideas, a separation of proposal moves and their verification with the model (Data-Driven Markov Chain Monte Carlo), and filtering with the Metropolis acceptance rule. It does not need gradients and is less prone to local optima than standard fitters. We also introduce a new collective likelihood which models the average difference between the model and the target image rather than individual pixel differences. The average value shows a natural tendency towards a normal distribution, even when the individual pixel-wise difference is not Gaussian. We employ the new fitting method to calculate posterior models of 3D face reconstructions from single real-world images. A direct application of the algorithm with the 3D Morphable Model leads us to a fully automatic face recognition system with competitive performance on the Multi-PIE database without any database adaptation. {\textcopyright} 2016, Springer Science+Business Media New York.},
annote = {From Duplicate 1 (Markov Chain Monte Carlo for Automated Face Image Analysis - Sch{\"{o}}nborn, S; Egger, B; Morel-Forster, A; Vetter, T)

cited By 6},
author = {Schonborn, Sandro and Egger, Bernhard and Morel-Forster, Andreas and Vetter, Thomas and Sch{\"{o}}nborn, S and Egger, Bernhard and Morel-Forster, Andreas and Vetter, Thomas},
doi = {10.1007/s11263-016-0967-5},
issn = {0920-5691},
journal = {INTERNATIONAL JOURNAL OF COMPUTER VISION},
keywords = {,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jun},
number = {2},
pages = {160--183},
title = {{Markov Chain Monte Carlo for Automated Face Image Analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028240219{\&}doi=10.1007{\%}2Fs11263-016-0967-5{\&}partnerID=40{\&}md5=77d362cf06ebb4646cd34c71dcf3a2e2},
volume = {123},
year = {2017}
}
@inproceedings{ISI:000392743800047,
abstract = {Current 3D data capturing as implemented on for example airborne or mobile laser scanning systems is able to efficiently sample the surface of a city by billions of unselective points during one working day. What is still difficult is to extract and visualize meaningful information hidden in these point clouds with the same efficiency. This is where the FP7 IQmulus project enters the scene. IQmulus is an interactive facility for processing and visualizing big spatial data. In this study the potential of IQmulus is demonstrated on a laser mobile mapping point cloud of 1 billion points sampling " 10 km of street environment in Toulouse, France. After the data is uploaded to the IQmulus Hadoop Distributed File System, a workflow is defined by the user consisting of retiling the data followed by a PCA driven local dimensionality analysis, which runs efficiently on the IQmulus cloud facility using a Spark implementation. Points scattering in 3 directions are clustered in the tree class, and are separated next into individual trees. Five hours of processing at the 12 node computing cluster results in the automatic identification of 4000+ urban trees. Visualization of the results in the IQmulus fat client helps users to appreciate the results, and developers to identify remaining flaws in the processing workflow.},
annote = {From Duplicate 1 (The Iqmulus urban showcase: Automatic tree classification and identification in huge mobile mapping point clouds - B{\"{o}}hm, J; Bredif, M; Gierlinger, T; Kr{\"{a}}mer, M; Lindenbergh, R; Liu, K; Michel, F; Sirmacek, B)

cited By 3

From Duplicate 2 (THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS - Boehm, J; Bredif, M; Gierlinger, T; Kraemer, M; Lindenbergh, R; Liu, K; Michel, F; Sirmacek, B)

23rd Congress of the
International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
Prague, CZECH REPUBLIC, JUL 12-19, 2016},
author = {Boehm, J and Bredif, M and Gierlinger, T and Kraemer, M and Lindenbergh, R and Liu, K and Michel, F and Sirmacek, B and B{\"{o}}hm, J and Bredif, M and Gierlinger, T and Kr{\"{a}}mer, M and Lindenbergh, R and Liu, K and Michel, F and Sirmacek, B},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprsarchives-XLI-B3-301-2016},
editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
issn = {2194-9034},
keywords = {,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {B3},
organization = {Int Soc Photogrammetry {\&} Remote Sensing},
pages = {301--307},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{THE IQMULUS URBAN SHOWCASE: AUTOMATIC TREE CLASSIFICATION AND IDENTIFICATION IN HUGE MOBILE MAPPING POINT CLOUDS}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978159544{\&}doi=10.5194{\%}2Fisprsarchives-XLI-B3-301-2016{\&}partnerID=40{\&}md5=2fbed8958deed902b55eef60e98a087e},
volume = {41},
year = {2016}
}
@article{ISI:000410465400003,
abstract = {In this paper, we introduce a novel, automatic method for 3D face
recognition. A new feature called a spherical vector norms map of a 3D
face is created using the normal vector of each point. This feature
contains more detailed information than the original depth image in
regions such as the eyes and nose. For certain flat areas of 3D face,
such as the forehead and cheeks, this map could increase the
distinguishability of different points. In addition, this feature is
robust to facial expression due to an adjustment that is made in the
mouth region. Then, the facial representations, which are based on
Histograms of Oriented Gradients, are extracted from the spherical
vector norms map and the original depth image. A new partitioning
strategy is proposed to produce the histogram of eight patches of a
given image, in which all of the pixels are binned based on the
magnitude and direction of their gradients. In this study, SVNs map and
depth image are represented compactly with two histograms of oriented
gradients; this approach is completed by Linear Discriminant Analysis
and a Nearest Neighbor classifier.},
author = {Wang, Xue-Qiao and Yuan, Jia-Zheng and Li, Qing},
doi = {10.6688/JISE.2017.33.5.3},
issn = {1016-2364},
journal = {JOURNAL OF INFORMATION SCIENCE AND ENGINEERING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {5},
pages = {1141--1161},
title = {{3D Face Recognition Using Spherical Vector Norms Map}},
volume = {33},
year = {2017}
}
@article{ISI:000406111700008,
abstract = {BackgroundOur objective is to help clinicians detect the facial effects
of prenatal alcohol exposure by developing computer-based tools for
screening facial form.
MethodsAll 415 individuals considered were evaluated by expert
dysmorphologists and categorized as (i) healthy control (HC), (ii) fetal
alcohol syndrome (FAS), or (iii) heavily prenatally alcohol exposed (HE)
but not clinically diagnosable as FAS; 3D facial photographs were used
to build models of facial form to support discrimination studies.
Surface curvature-based delineations of facial form were introduced.
Results(i) Facial growth in FAS, HE, and control subgroups is similar in
both cohorts. (ii) Cohort consistency of agreement between clinical
diagnosis and HC-FAS facial form classification is lower for midline
facial regions and higher for nonmidline regions. (iii) Specific HC-FAS
differences within and between the cohorts include: for HC, a smoother
philtrum in Cape Coloured individuals; for FAS, a smoother philtrum in
Caucasians; for control-FAS philtrum difference, greater homogeneity in
Caucasians; for control-FAS face difference, greater homogeneity in Cape
Coloured individuals. (iv) Curvature changes in facial profile induced
by prenatal alcohol exposure are more homogeneous and greater in Cape
Coloureds than in Caucasians. (v) The Caucasian HE subset divides into
clusters with control-like and FAS-like facial dysmorphism. The Cape
Coloured HE subset is similarly divided for nonmidline facial regions
but not clearly for midline structures. (vi) The Cape Coloured HE subset
with control-like facial dysmorphism shows orbital hypertelorism.
ConclusionsFacial curvature assists the recognition of the effects of
prenatal alcohol exposure and helps explain why different facial regions
result in inconsistent control-FAS discrimination rates in disparate
ethnic groups. Heavy prenatal alcohol exposure can give rise to orbital
hypertelorism, supporting a long-standing suggestion that prenatal
alcohol exposure at a particular time causes increased separation of the
brain hemispheres with a concomitant increase in orbital separation.},
author = {Suttie, Michael and Wetherill, Leah and Jacobson, Sandra W and Jacobson, Joseph L and Hoyme, H Eugene and Sowell, Elizabeth R and Coles, Claire and Wozniak, Jeffrey R and Riley, Edward P and Jones, Kenneth L and Foroud, Tatiana and Hammond, Peter and CIFASD},
doi = {10.1111/acer.13429},
issn = {0145-6008},
journal = {ALCOHOLISM-CLINICAL AND EXPERIMENTAL RESEARCH},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {8},
pages = {1471--1483},
title = {{Facial Curvature Detects and Explicates Ethnic Differences in Effects of Prenatal Alcohol Exposure}},
volume = {41},
year = {2017}
}
@article{ISI:000461160000001,
abstract = {This paper proposes a novel machine learning approaches to predict the
outcome of facial rejuvenation prior to a cosmetic procedure. This is
achieved by estimating the required amount of dermal filler volume that
needs to be applied on the face by learning the underlying structural
mapping from the pretreatment and posttreatment 3D face images. We
develop and train our proposed deep neural network, called Rejuv3DNet,
designed specifically to predict the dermal filler volume. We also
propose the kernel regression (KR)-based model to validate and improve
our volume estimation results using regression. Our other contributions
include the development of the first 3D face cosmetic dataset, which
consists of real-world pretreatment and posttreatment 3D face images and
a novel technique for the generation of synthetic cosmetic treatment 3D
face images. Our experimental results show that the proposed Rejuv3DNet
and the KR model achieve 62.5{\%} and 66.67{\%}, respectively, on real-world
data, while these techniques achieve a prediction accuracy of 75.2{\%} and
89.5{\%}, and 77.2{\%} and 90.1{\%} on our two different synthetic datasets.
Our proposed techniques have been found to be computationally efficient,
achieving near real-time prediction performance. The reported accuracies
are our preliminary results for proof of concept, which can be improved
with more data. The proposed approach has the potential for further
investigation in the cosmetic surgery domain.},
author = {Shah, Syed Afaq Ali and Bennamoun, Mohammed and Molton, Michael K},
doi = {10.1109/ACCESS.2019.2899379},
issn = {2169-3536},
journal = {IEEE ACCESS},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
pages = {23779--23787},
title = {{Machine Learning Approaches for Prediction of Facial Rejuvenation Using Real and Synthetic Data}},
volume = {7},
year = {2019}
}
@inproceedings{ISI:000388373401159,
abstract = {This paper presents an algorithm for fully reconstructing a 3D face from
a single image. This task is still highly challenging as most current
methods only care about the frontal face, ignoring side face, such as
the neck, ears etc. In our algorithm, to get the more detailed texture,
we deal with the shape reconstruction and texture recovery respectively.
For shape, we estimate the deformation of the 3D model by a set of
feature points. For texture, due to the similar facial structure, we
divide the full texture into patches and show how sparse learning model
can be used to fully recover the texture of the 3D face. Extensive
experiment results on the CMU-PIE database and images downloaded from
the Internet demonstrate that our method outperforms the
state-of-the-art methods.},
annote = {IEEE International Conference on Acoustics, Speech, and Signal
Processing, Shanghai, PEOPLES R CHINA, MAR 20-25, 2016},
author = {Hu, Xiaoping and Wang, Ying and Zhu, Feiyun and Pan, Chunhong},
booktitle = {2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING PROCEEDINGS},
isbn = {978-1-4799-9988-0},
issn = {1520-6149},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers Signal Proc Soc},
pages = {1651--1655},
series = {International Conference on Acoustics Speech and Signal Processing ICASSP},
title = {{LEARNING-BASED FULLY 3D FACE RECONSTRUCTION FROM A SINGLE IMAGE}},
year = {2016}
}
@article{ISI:000410870200008,
abstract = {Humans present clear demographic traits which allow their peers to
recognize their gender and ethnic groups as well as estimate their age.
Abundant literature has investigated the problem of automated gender,
ethnicity and age recognition from facial images. However, despite the
co-existence of these traits, most of the studies have addressed them
separately, very little attention has been given to their correlations.
In this work, we address the problem of joint demographic estimation and
investigate the correlation through the morphological differences in 3D
facial shapes. To this end, a set of facial features are extracted to
capture the 3D shape differences among the demographic groups. Then, a
correlation-based feature selection is applied to highlight salient
features and remove redundancy. These features are later fed to Random
Forest for gender and ethnicity classification, and age estimation.
Extensive experiments conducted on FRGCv2 dataset, under
Expression-Dependent and Expression-Independent settings, demonstrate
the effectiveness of the proposed approaches for the three traits, and
also show the accuracy improvement when considering their correlations.
To the best of our knowledge, this is the first study exploring the
correlations of these facial soft-biometric traits using 3D faces. This
is also the first work which studies the problem of age estimation from
3D Faces.(1) (C) 2017 Elsevier B.V. All rights reserved.},
author = {Xia, Baiqiang and {Ben Amor}, Boulbaba and Daoudi, Mohamed},
doi = {10.1016/j.imavis.2017.06.004},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {90--102},
title = {{= Joint gender, ethnicity and age estimation from 3D faces An experimental illustration of their correlations}},
volume = {64},
year = {2017}
}
@inproceedings{ISI:000380516600072,
abstract = {The classical curvatures of smooth surfaces (Gaussian, mean and
principal curvatures) have been widely used in 3D face recognition (FR).
However, facial surfaces resulting from 3D sensors are discrete meshes.
In this paper, we present a general framework and define three principal
curvatures on discrete surfaces for the purpose of 3D FR. These
principal curvatures are derived from the construction of asymptotic
cones associated to any Borel subset of the discrete surface. They
describe the local geometry of the underlying mesh. First two of them
correspond to the classical principal curvatures in the smooth case. We
isolate the third principal curvature that carries out meaningful
geometric shape information. The three principal curvatures in different
Borel subsets scales give multi-scale local facial surface descriptors.
We combine the proposed principal curvatures with the LNP-based facial
descriptor and SRC for recognition. The identification and verification
experiments demonstrate the practicability and accuracy of the third
principal curvature and the fusion of multi-scale Borel subset
descriptors on 3D face from FRGC v2.0.},
annote = {International Conference on Biometrics (ICB), Phuket, THAILAND, MAY
19-22, 2015},
author = {Tang, Yinhang and Sun, Xiang and Huang, Di and Morvan, Jean-Marie and Wang, Yunhong and Chen, Liming},
booktitle = {2015 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)},
isbn = {978-1-4799-7824-3},
issn = {2376-4201},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {SIEW-SNGIEM AWARD; Kasetsart Univ; CHANWANICH; ST Elect; SAFRAN Morpho},
pages = {466--472},
series = {International Conference on Biometrics},
title = {{3D Face Recognition with Asymptotic Cones based Principal Curvatures}},
year = {2015}
}
@article{ISI:000370290900001,
abstract = {Curvelet transform can describe the signal by multiple scales, and
multiple directions. In order to improve the performance of 3D face
recognition algorithm, we proposed an Anthropometric and Curvelet
features fusion-based algorithm for 3D face recognition (Anthropometric
Curvelet Fusion Face Recognition, ACFFR). First, the eyes, nose, and
mouth feature regions are extracted by the Anthropometric
characteristics and curvature features of the human face. Second,
Curvelet energy features of the facial feature regions at different
scales and different directions are extracted by Curvelet transform. At
last, Euclidean distance is used as the similarity between template and
objectives. To verify the performance, the proposed algorithm is
compared with Anthroface3D and Curveletface3D on the Texas 3D FR
database. The experimental results have shown that the proposed
algorithm performs well, with equal error rate of 1.75{\%} and accuracy of
97.0{\%}. The algorithm we proposed in this paper has better robustness to
expression and light changes than Anthroface3D and Curveletface3D.},
author = {Song, Dan and Luo, Jing and Zi, Chunyuan and Tian, Huixin},
doi = {10.1155/2016/6859364},
issn = {1687-725X},
journal = {JOURNAL OF SENSORS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
title = {{3D Face Recognition Using Anthropometric and Curvelet Features Fusion}},
year = {2016}
}
@inproceedings{ISI:000454996700113,
abstract = {This paper investigates the evaluation of dense 3D face reconstruction
from a single 2D image in the wild. To this end, we organise a
competition that provides a new benchmark dataset that contains 2000 2D
facial images of 135 subjects as well as their 3D ground truth face
scans. In contrast to previous competitions or challenges, the aim of
this new benchmark dataset is to evaluate the accuracy of a 3D dense
face reconstruction algorithm using real, accurate and high-resolution
3D ground truth face scans. In addition to the dataset, we provide a
standard protocol as well as a Python script for the evaluation. Last,
we report the results obtained by three state-of-the-art 3D face
reconstruction systems on the new benchmark dataset. The competition is
organised along with the 2018 13th IEEE Conference on Automatic Face {\&}
Gesture Recognition.},
annote = {13th IEEE International Conference on Automatic Face {\&} Gesture
Recognition (FG), Xi an, PEOPLES R CHINA, MAY 15-19, 2018},
author = {Feng, Zhen-Hua and Huber, Patrik and Kittler, Josef and Hancock, Peter and Wu, Xiao-Jun and Zhao, Qijun and Koppen, Paul and Raetsch, Matthias},
booktitle = {PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE {\&} GESTURE RECOGNITION (FG 2018)},
doi = {10.1109/FG.2018.00123},
isbn = {978-1-5386-2335-0},
issn = {2326-5396},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE Comp Soc; IEEE Biometr Council},
pages = {780--786},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild}},
year = {2018}
}
@article{ISI:000369689500019,
abstract = {Current research trends in 3D Face recognition system requires a special
hardware for fast capturing face image data from multi angle view. To
support this research, we had designed and implemented an automatic
image data acquisition system using multi-camera for capturing facial
images from 5 degrees different angle views, which spanned horizontally
from 180 degrees from left to right, and vertically from horizontal up
to 70 degrees above the face. The system was designed using 30 IP
cameras that were mounted on two rigid steel arms that had the form of
three quarter of a circle, the two steel arms formed the angle of 90
degrees to each other. At each arm, 15 IP cameras were mounted with 5
degrees spacing vertically to each others. This arm was driven by a DC
motor which was controlled by a microcontroller and supervised directly
by a laptop computer along with the data acquisition activities. The
software for capturing images was designed using C{\#} GUI programming
language. The system had been working in good condition and image-data
were saved in JPEG format. Time duration of capturing images data for
one object face expression with 30 times capturing for the whole angle
views, was only 3 minutes 44.5 seconds with total number of 16,650
images collected. The delay time between two cameras capturing was less
than 1 sec. This project is aimed to support the 3D face recognition
research in the department},
author = {Wahab, Wahidin and Ridwan, M and Kusumoputro, Benyamin},
issn = {2086-9614},
journal = {INTERNATIONAL JOURNAL OF TECHNOLOGY},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {6},
pages = {1042--1049},
title = {{DESIGN AND IMPLEMENTATION OF AN AUTOMATIC FACE-IMAGE DATA ACQUISITION SYSTEM USING IP BASED MULTI CAMERA}},
volume = {6},
year = {2015}
}
@inproceedings{ISI:000460476900026,
abstract = {From limited two-dimensional recognition, facial recognition has now
been developed to be able to recognize three-dimensional face images,
which usually involves process of face pose estimation. As the
conventional artificial neural networks has shown low recognition rate
to this problem, Convolution Neural Network have been the most potential
classifier to determine the pose estimation of a three-dimensional face
images. Convolution operation is expected to minimize the effect of
distortion and disorientation of the object, and able to efficiently
reduce the required parameters. Results show that the CNN system could
estimate the pose position of the 3D face images with high recognition
rate, however, this recognition rate decline significantly for the noisy
buried face images, showing the CNN still need improvement to deal with
noisy environments.},
annote = {5th IEEE International Conference on Engineering Technologies and
Applied Sciences (IEEE ICETAS), Bangkok, THAILAND, NOV 22-23, 2018},
author = {Kamanditya, Bharindra and Kuswara, Randy Pangestu and Nugroho, Muhammad Adi and Kusumoputro, Benyamin},
booktitle = {2018 5TH IEEE INTERNATIONAL CONFERENCE ON ENGINEERING TECHNOLOGIES AND APPLIED SCIENCES (IEEE ICETAS)},
isbn = {978-1-5386-7966-1},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IEEE IIUM Student Branch; ETSS Management},
title = {{Convolution Neural Network for Pose Estimation of Noisy Three-Dimensional Face Images}},
year = {2018}
}
@article{Hu:2017:SVF:3054851.3054911,
abstract = {Video surveillance has attracted more and more interests in the last decade, video-based Face Recognition (FR) therefore became an important task. However, the surveillance videos include many vague non-frontal faces especially the view of faces looking down and up. As a result, most FR algorithms would perform worse when they were applied in surveillance videos. On the other hand, it was common at video monitoring field that only Single training Sample Per Person (SSPP) is available from their identification card. In order to effectively improve FR for both the SSPP problem and the low-quality problem, this paper proposed an approach to synthesis face images-based on 3D face modeling and blurring. In the proposed algorithm, firstly a 2D frontal face with high-resolution was used to build a 3D face model, then several virtual faces with different poses were synthesized from the 3D model, and finally some degraded face images were constructed from the original and the virtual faces through blurring process. At last multiple face images could be chosen from frontal, virtual and degraded faces to build a training set. Both SCface and LFW databases were employed to evaluate the proposed algorithm by using PCA, FLDA, scale invariant feature transform, compressive sensing and deep learning. The results on both datasets showed that the performance of these methods could be improved when virtual faces were generated to train the classifiers. Furthermore, in SCface database the average recognition rates increased up to 10{\%}, 16.62{\%}, 13.03{\%}, 19.44{\%} and 23.28{\%} respectively for the above-mentioned methods when virtual view and blurred faces were taken to train their classifiers. Experimental results indicated that the proposed method for generating more train samples was effective and could be considered to be applied in intelligent video monitoring system.},
address = {Amsterdam, The Netherlands, The Netherlands},
annote = {05/06/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
05/06/2018 Exclu{\'{i}}do (etapa 1)},
author = {Hu, Xiao and Peng, Shaohu and Wang, Li and Yang, Zhao and Li, Zhaowen},
doi = {10.1016/j.neucom.2016.12.059},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Compressive sensing,Deep learning,Scale invariant feature transform,Single training sample per person,Video surveillance,acm,estela,etapa1,id509,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {acm,estela,etapa1,id509,revisao{\_}scopus,revisao{\_}webofscience},
month = {apr},
number = {C},
pages = {46--58},
publisher = {Elsevier Science Publishers B. V.},
title = {{Surveillance video face recognition with single sample per person based on 3D modeling and blurring}},
url = {https://doi.org/10.1016/j.neucom.2016.12.059 http://linkinghub.elsevier.com/retrieve/pii/S0925231217300012},
volume = {235},
year = {2017}
}
@inproceedings{7368276,
abstract = {This paper presents our methodology for Landmark Point detection to improve 3D face recognition in a presence of variant facial expression. The objective was to develop an automatic process for distinguishing and segmenting to be embedded in a 3D face recognition system using only 3D Point Distribution Model (PDM) as input. The approach used hydride method to extract this features from the surface curvature information. Landmark Localization is done on the segmented face via finding the change that decreases the deviation of the model from the mean profile. Face registering is achieved using previous anthropometric information and the localized landmarks. The results confirm that the method used is accurate and robust for the proposed application.},
annote = {29/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
29/04 Exclu{\'{i}}do (etapa 1)},
author = {Boukamcha, H and Elhallek, M and Atri, M and Smach, F},
booktitle = {2015 World Symposium on Computer Networks and Information Security (WSCNIS)},
doi = {10.1109/WSCNIS.2015.7368276},
keywords = {3D face landmar,computer graphics,etapa1,face recognition,id394,ieeexplore,poly,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,id394,ieeexplore,poly,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
pages = {1--6},
title = {{3D face landmark auto detection}},
year = {2015}
}
@inproceedings{ISI:000450395900012,
abstract = {Face recognition undergoes three stages of full reliance on artificial,
human-computer interaction, and machine automatic recognition, free from
the initial recognition of a single frontal grayscale image to the
development of three-dimensional face recognition on the basis of
research on multi-pose face recognition with the realization of dynamic
face recognition as a carrier, and achieves certain results. In China,
although face recognition technology started late, it has developed
rapidly and received strong support from the country. At present, many
universities and research institutes in China have very good research
foundations in the field of image processing and pattern recognition,
and have actively carried out basic research on human biometric
recognition technology including face recognition, and have achieved
good results. Research results show that in the near future, China's
research in this field will enter the world's advanced ranks. Face
recognition is one of the most classical problems in the field of
identification. At present, there are many solutions and good
experimental results obtained. However, a general-purpose face
recognition system that can be practically applied and used for
arbitrary backgrounds and arbitrary gestures has not yet appeared.
Therefore, face recognition goes into the application field to improve
the effectiveness of the extracted facial features and classifier
optimization, and etc.},
annote = {5th International Conference on Electrical {\&} Electronics Engineering
and Computer Science (ICEEECS), Beijing, PEOPLES R CHINA, JUN 29-30,
2018},
author = {Tu, Min},
booktitle = {2018 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL {\&} ELECTRONICS ENGINEERING AND COMPUTER SCIENCE (ICEEECS 2018)},
editor = {{Wu, A}},
isbn = {978-1-912407-04-0},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {49--52},
title = {{Research on Face Recognition Technology Based on Computer Neural Network}},
year = {2018}
}
@article{ISI:000449578500012,
abstract = {In recent years, information technology is developing continuously and
set off a burst of artificial intelligence boom in the field of science.
The development of advanced technologies such as unmanned driving and AI
chips, is the extensive application of artificial intelligence.
Face-related technologies have a wide range of applications because of
intuitive results and good concealment. Since 3D face information can
provide more comprehensive facial information than 2D face information,
and it can solve many difficulties that cannot be solved in 2D face
recognition. Therefore, more and more researchers have studied 3D face
recognition in recent years. Under the new circumstances, the research
on face are experiencing all kinds of challenges. With the tireless of
many scientists, the new technology is also making a constant progress,
and in the development of many technologies it still maintained its
leading position. In this paper, we simply sort out the present
development process of facial correlation technology, and the general
evolution of this technology is outlined. Finally, the practical
significance of this technology development is briefly discussed. (C)
2018 Published by Elsevier Inc.},
author = {Fei, Hongyan and Tu, Bing and Chen, Ququ and He, Danbing and Zhou, Chengle and Peng, Yishu},
doi = {10.1016/j.jvcir.2018.09.012},
issn = {1047-3203},
journal = {JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {139--143},
title = {{An overview of face-related technologies}},
volume = {56},
year = {2018}
}
@inproceedings{ISI:000456303900024,
abstract = {This paper focuses on a region based methodology for expression in
sensitive 3D face recognition process. Considering facial regions that
are comparatively unchanging during expressions, results shows that
using fifteen sub regions on the face can attain high 3D face
recognition. We use a modified face recognition algorithm along with
hierarchical contour based image registration for finding the similarity
score. Our method operates in two modes: verification mode and
confirmation mode. Crop 100 mm of frontal face region, apply
preprocessing and automatically detect nose tip, translate the face
image to origin and crop fifteen sub regions. The cropped sub regions
are defined by cuboids which occupy more volumetric data, Nose Tip is
the most projecting point of the face with the highest value along
Z-axis so consider it as origin. The modified face recognition algorithm
reduces the effects caused by facial expressions and artifacts. Finally
a Hierarchical contour based image registration technique is applied
which yields better results. The approach is applied on Bosphorus 3D
datasets and achieved a verification rate of 95.3{\%} at 0.1{\%} false
acceptance rate. In the identification scenario 99.3{\%} rank one
recognition is achieved.},
annote = {8th IEEE International Conference on Computational Intelligence and
Computing Research (IEEE ICCIC), Tamilnadu Coll Engn, Coimbatore, INDIA,
DEC 14-16, 2017},
author = {Reji, R and SojanLal, P},
booktitle = {2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND COMPUTING RESEARCH (ICCIC)},
editor = {{Krishnan, N and Karthikeyan, M}},
isbn = {978-1-5090-6621-6},
issn = {2471-7851},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IEEE PODHIGAI; IEEE SIPCICOM},
pages = {144--149},
series = {IEEE International Conference on Computational Intelligence and Computing Research},
title = {{Region Based 3D Face Recognition}},
year = {2017}
}
@inproceedings{ISI:000406771302059,
abstract = {We aim to reconstruct an accurate neutral 3D face model from an RGB-D
video in the presence of extreme expression changes. Since each depth
frame, taken by a low-cost sensor, is noisy, point clouds from multiple
frames can be registered and aggregated to build an accurate 3D model.
However, direct aggregation of multiple data produces erroneous results
in natural interaction (e.g., talking and showing expressions). We
propose to analyze facial expression from an RGB frame and neutralize
the corresponding 3D point cloud if needed. We first estimate the
person's expression by fitting blend-shape coefficients using 2D facial
landmarks for each frame and calculate an expression deformity
(expression score). With the estimated expression score, we determine
whether an input face is neutral or non-neutral. If the face is
non-neutral, we proceed to neutralize the expression of the 3D point
cloud in that frame. To neutralize the 3D point cloud of a face, we
deform our generic 3D face model by applying the estimated blendshape
coefficients, find displacement vectors from the deformed generic face
to a neutral generic face, and apply the displacement vectors to the
input 3D point cloud. After preprocessing frames in a video, we rank
frames based on the expression scores and register the ranked frames
into a single 3D model. Our system produces a neutral 3D face model in
the presence of extreme expression changes even when neutral faces do
not exist in the video.},
annote = {23rd International Conference on Pattern Recognition (ICPR), Mexican
Assoc Comp Vis Robot {\&} Neural Comp, Cancun, MEXICO, DEC 04-08, 2016},
author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni, Gerard},
booktitle = {2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)},
isbn = {978-1-5090-4847-2},
issn = {1051-4651},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm; Elsevier; IBM Res; INTEL; CONACYT},
pages = {2362--2367},
series = {International Conference on Pattern Recognition},
title = {{Expression Invariant 3D Face Modeling from an RGB-D Video}},
year = {2016}
}
@article{ISI:000457037300020,
abstract = {Delineation of individual deciduous trees with Light Detection and
Ranging (LiDAR) data has long been sought for accurate forest inventory
in temperate forests. Previous attempts mainly focused on high-density
LiDAR data to obtain reliable delineation results, which may have
limited applications due to the high cost and low availability of such
data. Here, the feasibility of individual deciduous tree delineation
with low-density LiDAR data was examined using a point-density-based
algorithm. First a high-resolution point density model (PDM) was
developed from low-density LiDAR point cloud to locate individual trees
through the horizontal spatial distribution of LiDAR points. Then,
individual tree crowns and associated attributes were delineated with a
2D marker-controlled watershed segmentation. Additionally, the PDM-based
approach was compared with a conventional canopy height model (CHM)
based delineation. The results demonstrated that the PDM-based approach
produced an 89{\%} detection accuracy to identify deciduous trees in our
study area. The tree attributes derived from the PDM-based algorithm
explained 81{\%} and 83{\%} of tree height and crown width variations of
forest stands, respectively. The conventional CHM-based tree attributes,
on the other hand, could explain only 71{\%} and 66{\%} of tree height and
crown width, respectively. Our results suggest that the application of
the PDM-based individual tree identification in deciduous forests with
low-density LiDAR data is feasible and has relatively high accuracy to
predict tree height and crown width, which are highly desired in
large-scale forest inventory and analysis.},
author = {Shao, Gang and Shao, Guofan and Fei, Songlin},
doi = {10.1080/01431161.2018.1513664},
issn = {0143-1161},
journal = {INTERNATIONAL JOURNAL OF REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {346--363},
title = {{Delineation of individual deciduous trees in plantations with low-density LiDAR data}},
volume = {40},
year = {2019}
}
@article{ISI:000424092300038,
abstract = {A plethora of information contained in full-waveform (FW) Light
Detection and Ranging (LiDAR) data offers prospects for characterizing
vegetation structures. This study aims to investigate the capacity of FW
LiDAR data alone for tree species identification through the integration
of waveform metrics with machine learning methods and Bayesian
inference. Specifically, we first conducted automatic tree segmentation
based on the waveform-based canopy height model (CHM) using three
approaches including TreeVaW, watershed algorithms and the combination
of TreeVaW and watershed (TW) algorithms. Subsequently, the Random
forests (RF) and Conditional inference forests (CF) models were employed
to identify important tree-level waveform metrics derived from three
distinct sources, such as raw waveforms, composite waveforms, the
waveform-based point cloud and the combined variables from these three
sources. Further, we discriminated tree (gray pine, blue oak, interior
live oak) and shrub species through the RF, CF and Bayesian multinomial
logistic regression (BMLR) using important waveform metrics identified
in this study. Results of the tree segmentation demonstrated that the TW
algorithms outperformed other algorithms for delineating individual tree
crowns. The CF model overcomes waveform metrics selection bias caused by
the RF model which favors correlated metrics and enhances the accuracy
of subsequent classification. We also found that composite waveforms are
more informative than raw waveforms and waveform-based point cloud for
characterizing tree species in our study area. Both classical machine
learning methods (the RF and CF) and the BMLR generated satisfactory
average overall accuracy (74{\%} for the RF, 77{\%} for the CF and 81{\%} for
the BMLR) and the BMLR slightly outperformed the other two methods.
However, these three methods suffered from low individual classification
accuracy for the blue oak which is prone to being misclassified as the
interior live oak due to the similar characteristics of blue oak and
interior live oak. Uncertainty estimates from the BMLR method compensate
for this downside by providing classification results in a probabilistic
sense and rendering users with more confidence in interpreting and
applying classification results to real-world tasks such as forest
inventory. Overall, this study recommends the CF method for feature
selection and suggests that BMLR could be a superior alternative to
classical machining learning methods.},
author = {Zhou, Tan and Popescu, Sorin C and Lawing, A Michelle and Eriksson, Marian and Strimbu, Bogdan M and Buerkner, Paul C},
doi = {10.3390/rs10010039},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jan},
number = {1},
title = {{Bayesian and Classical Machine Learning Methods: A Comparison for Tree Species Classification with LiDAR Waveform Signatures}},
volume = {10},
year = {2018}
}
@inproceedings{ISI:000418371405064,
abstract = {Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration
for the synthesis of 3D faces by assuming that faces are in a linear
class. However, the human face is in a nonlinear manifold, and a new
test are always not in the pre-learnt subspace accurately because of the
disparity brought by ethnicity, age, gender, etc. In the paper, we
propose a parametric T-spline morphable model (T-splineMM) for 3D face
representation, which has great advantages of fitting data from an
unknown source accurately. In the model, we describe a face by C-2
T-spline surface, and divide the face surface into several shape units
(SUs), according to facial action coding system (FACS), on T-mesh
instead of on the surface directly. A fitting algorithm is proposed to
optimize coefficients of T-spline control point components along
pre-learnt identity and expression subspaces, as well as to optimize the
details in refinement progress. As any pre-learnt subspace is not
complete to handle the variety and details of faces and expressions, it
covers a limited span of morphing. SUs division and detail refinement
make the model fitting the facial muscle deformation in a larger span of
morphing subspace. We conduct experiments on face scan data, kinect data
as well as the space-time data to test the performance of detail
fitting, robustness to missing data and noise, and to demonstrate the
effectiveness of our model. Convincing results are illustrated to
demonstrate the effectiveness of our model compared with the popular
methods.},
annote = {30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Honolulu, HI, JUL 21-26, 2017},
author = {Peng, Weilong and Feng, Zhiyong and Xu, Chao and Su, Yong},
booktitle = {30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2017)},
doi = {10.1109/CVPR.2017.585},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IEEE Comp Soc; CVF},
pages = {5515--5523},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Parametric T-spline Face Morphable Model for Detailed Fitting in Shape Subspace}},
year = {2017}
}
@article{ISI:000441233300018,
abstract = {In this paper, we propose a new approach for 3D face verification based
on tensor representation. Face challenges, such as illumination,
expression and pose, are modeled as a multilinear algebra problem where
facial images are represented as high order tensors. Particularly, to
account for head pose variations, several pose scans are generated from
a single depth image using Euler transformation. Multi-bloc local phase
quantization (MBLPQ) histogram features are extracted from depth face
images and arranged as a third order tensor. The dimensionality of the
tensor is reduced based on the higher-order singular value decomposition
(HOSVD). HOSVD projects the input tensor in a new subspace in which the
dimension of each tensor mode is reduced. To discriminate faces of
different persons, we utilize the Enhanced Fisher Model (EFM).
Experimental evaluations on CASIA-3D database, which contains large head
pose variations, demonstrate the effectiveness of the proposed approach.
A verification rate of 98.60{\%} is obtained.},
author = {Chouchane, Ammar and Ouamane, Abdelmalik and Boutellaa, Elhocine and Belahcene, Mebarka and Bourennane, Salah},
doi = {10.1007/s11042-017-5478-z},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {16},
pages = {20697--20714},
title = {{3D face verification across pose based on euler rotation and tensors}},
volume = {77},
year = {2018}
}
@article{ISI:000412965200011,
abstract = {We propose a novel feature extraction approach for 3D facial expression
recognition by incorporating non-rigid registration in face-model-free
analysis, which in turn makes feasible data-driven, i.e., feature
model-free recognition of expressions. The resulting simplicity of
feature representation is due to the fact that facial information is
adapted to the input faces via shape model-free dense registration, and
this provides a dynamic feature extraction mechanism. This approach
eliminates the necessity of complex feature representations as required
in the case of static feature extraction methods, where the complexity
arises from the necessity to model the local context; higher degree of
complexity persists in deep feature hierarchies enabled by end-to-end
learning on large-scale datasets. Face-model-free recognition implies
independence from limitations and biases due to committed face models,
bypassing complications of model fitting, and avoiding the burden of
manual model construction. We show via information gain maps that
non-rigid registration enables extraction of highly informative
features, as it provides invariance to local shifts due to physiognomy
(subject invariance) and residual pose misalignments; in addition, it
allows estimation of local correspondences of expressions. To maximize
the recognition rate, we use the strategy of employing a rich but
computationally manageable set of local correspondence structures, and
to this effect we propose a framework to optimally select multiple
registration references. Our features are re-sampled surface curvature
values at individual coordinates which are chosen per expression-class
and per reference pair. We show the superior performance of our novel
dynamic feature extraction approach on three distinct recognition
problems, namely, action unit detection, basic expression recognition,
and emotion dimension recognition. (C) 2017 Elsevier Inc. All rights
reserved.},
author = {Savran, Arman and Sankur, Bulent},
doi = {10.1016/j.cviu.2017.07.005},
issn = {1077-3142},
journal = {COMPUTER VISION AND IMAGE UNDERSTANDING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {146--165},
title = {{Non-rigid registration based model-free 3D facial expression recognition}},
volume = {162},
year = {2017}
}
@article{ISI:000436283000050,
abstract = {Modern facial motion capture systems employ a two-pronged approach for
capturing and rendering facial motion. Visual data (2D) is used for
tracking the facial features and predicting facial expression, whereas
Depth (3D) data is used to build a series of expressions on 3D face
models. An issue with modern research approaches is the use of a single
data stream that provides little indication of the 3D facial structure.
We compare and analyse the performance of Convolutional Neural Networks
(CNN) using visual, Depth and merged data to identify facial features in
real-time using a Depth sensor. First, we review the facial landmarking
algorithms and its datasets for Depth data. We address the limitation of
the current datasets by introducing the Kinect One Expression Dataset
(KOED). Then, we propose the use of CNNs for the single data stream and
merged data streams for facial landmark detection. We contribute to
existing work by performing a full evaluation on which streams are the
most effective for the field of facial landmarking. Furthermore, we
improve upon the existing work by extending neural networks to predict
into 3D landmarks in real-time with additional observations on the
impact of using 2D landmarks as auxiliary information. We evaluate the
performance by using Mean Square Error (MSE) and Mean Average Error
(MAE). We observe that the single data stream predicts accurate facial
landmarks on Depth data when auxiliary information is used to train the
network. The codes and dataset used in this paper will be made
available.},
author = {Kendrick, Connah and Tan, Kevin and Walker, Kevin and Yap, Moi Hoon},
doi = {10.3390/sym10060230},
issn = {2073-8994},
journal = {SYMMETRY-BASEL},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jun},
number = {6},
title = {{Towards Real-Time Facial Landmark Detection in Depth Data Using Auxiliary Information}},
volume = {10},
year = {2018}
}
@article{ISI:000414883800006,
abstract = {In this paper, we address the problem of quantifying the facial
asymmetry from 3D face sequence (4D). We investigate the role of 4D data
to reveal the amount of both static and dynamic asymmetry in the
clinical case of facial paralysis. The goal is to provide tools to
clinicians to evaluate quantitatively facial paralysis treatment based
on Botulinum Toxin (BT), which can provide qualitative and quantitative
evaluations. To this end, Dense Scalar Fields (DSFs), based on
Riemannian analysis of 3D facial shape, is proposed to quantify facial
deformations. To assess this approach, a new 3D facial sequences of 16
patients data set is collected, before and after injecting the BT. For
each patient, we have collected 8 facial expressions before and after
injecting BT. Experimental results obtained on this data set show that
the proposed approach allows clinicians to evaluate more accurately the
facial asymmetry before and after the treatment. (C) 2017 Elsevier B.V.
All rights reserved.},
author = {Desrosiers, Paul Audain and Bennis, Yasmine and Daoudi, Mohamed and {Ben Amor}, Boulbaba and Guerreschi, Pierre},
doi = {10.1016/j.imavis.2017.08.006},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {nov},
pages = {67--88},
title = {{Analyzing of facial paralysis by shape analysis of 3D face sequences}},
volume = {67},
year = {2017}
}
@inproceedings{ISI:000374476000057,
abstract = {Computational approaches to investigating face attractiveness have
become an emerging topic in facial analysis research. Integrating
techniques from image analysis, pattern recognition and machine
learning, this subarea aims to explore the nature, components and
impacts of facial attractiveness and to develop computational algorithms
to analyze the attractiveness of a face. In this paper we develop an
attractiveness computation model for both frontal and profile images
(2.5D). We focus on the role of geometric ratios in the determination of
facial attractivenss. Stepwise regression is used as the feature
selection method to select the discriminatory variables from a huge set
of data-driven ratios. Decision tree is then used to generate an
automated classifier for both frontal and profile computation models.
The BJUT-3D Face Database is pre-processed and tested as our
experimental dataset. The low statistic errors and high correlation
indicate the accuracy of our computation models.},
annote = {5th International Conference on Intelligence Science and Big Data
Engineering (IScIDE), Suzhou, PEOPLES R CHINA, JUN 14-16, 2015},
author = {Liu, Shu and Fan, Yangyu and Guo, Zhe and Samal, Ashok},
booktitle = {INTELLIGENCE SCIENCE AND BIG DATA ENGINEERING: IMAGE AND VIDEO DATA ENGINEERING, ISCIDE 2015, PT I},
doi = {10.1007/978-3-319-23989-7_57},
editor = {{He, X and Gao, X and Zhang, Y and Zhou, ZH and Liu, ZY and Fu, B and Hu, F and Zhang, Z}},
isbn = {978-3-319-23989-7; 978-3-319-23987-3},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Chinese Golden Triangle Informat Sci {\&} Intelligence Sci Forum},
pages = {564--573},
series = {Lecture Notes in Computer Science},
title = {{2.5D Facial Attractiveness Computation Based on Data-Driven Geometric Ratios}},
volume = {9242},
year = {2015}
}
@article{ISI:000388777400069,
abstract = {Across-media face recognition refers to recognizing face images from
different sources (e.g., face sketch, 3D face model, and low resolution
image). In spite of promising processes achieved in face recognition
recent years, across-media face recognition is still a challenging
problem due to the difficulty of feature matching between different
modalities. In this paper, we propose a latent face model that creates
mappings from a hidden space to different media space. Images from
different media of the same person share the same latent vector in
hidden space. A coupled Joint Bayesian model is used to calculate the
joint probability of two faces from different media. To verify the
effectiveness of our proposed method, extensive experiments conducted on
various databases: self-collected low-resolution vs. high-resolution
database, sketches vs. photos databases, 3D face model vs. photos on LFW
database. Experimental results show that our method boosts the
performance of face recognition with images from different sources. (C)
2016 Elsevier B.V. All rights reserved.},
author = {Lv, Jiang-Jing and Huang, Jia-Shui and Zhou, Xiang-Dong and Zhou, Xi and Feng, Yong},
doi = {10.1016/j.neucom.2016.08.036},
issn = {0925-2312},
journal = {NEUROCOMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {735--745},
title = {{Latent face model for across-media face recognition}},
volume = {216},
year = {2016}
}
@article{ISI:000406751100005,
abstract = {Pose-invariant face alignment is a very challenging problem in computer
vision, which is used as a prerequisite for many facial analysis tasks,
e.g., face recognition, expression recognition, and 3D face
reconstruction. Recently, there have been a few attempts to tackle this
problem, but still more research is needed to achieve higher accuracy.
In this paper, we propose a face alignment method that aligns an image
with arbitrary poses, by combining the powerful cascaded CNN regressors,
3D Morphable Model (3DMM), and mirrorability constraint. The core of our
proposed method is a novel 3DMM fitting algorithm, where the camera
projection matrix parameters and 3D shape parameters are estimated by a
cascade of CNN-based regressors. Furthermore, we impose the
mirrorability constraint during the CNN learning by employing a novel
loss function inside the siamese network. The dense 3D shape enables us
to design pose-invariant appearance features for effective CNN learning.
Extensive experiments are conducted on the challenging large-pose face
databases (AFLW and AFW), with comparison to the state of the art.},
author = {Jourabloo, Amin and Liu, Xiaoming},
doi = {10.1007/s11263-017-1012-z},
issn = {0920-5691},
journal = {INTERNATIONAL JOURNAL OF COMPUTER VISION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {2},
pages = {187--203},
title = {{Pose-Invariant Face Alignment via CNN-Based Dense 3D Model Fitting}},
volume = {124},
year = {2017}
}
@article{ISI:000460829200061,
abstract = {Mobile Laser Scanning (MLS) is a versatile remote sensing technology
based on Light Detection and Ranging (lidar) technology that has been
utilized for a wide range of applications. Several previous reviews
focused on applications or characteristics of these systems exist in the
literature, however, reviews of the many innovative data processing
strategies described in the literature have not been conducted in
sufficient depth. To this end, we review and summarize the state of the
art for MLS data processing approaches, including feature extraction,
segmentation, object recognition, and classification. In this review, we
first discuss the impact of the scene type to the development of an MLS
data processing method. Then, where appropriate, we describe relevant
generalized algorithms for feature extraction and segmentation that are
applicable to and implemented in many processing approaches. The methods
for object recognition and point cloud classification are further
reviewed including both the general concepts as well as technical
details. In addition, available benchmark datasets for object
recognition and classification are summarized. Further, the current
limitations and challenges that a significant portion of point cloud
processing techniques face are discussed. This review concludes with our
future outlook of the trends and opportunities of MLS data processing
algorithms and applications.},
author = {Che, Erzhuo and Jung, Jaehoon and Olsen, Michael J},
doi = {10.3390/s19040810},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {4},
title = {{Object Recognition, Segmentation, and Classification of Mobile Laser Scanning Point Clouds: A State of the Art Review}},
volume = {19},
year = {2019}
}
@article{ISI:000425868600001,
abstract = {This study assessed whether presenting 3D face stimuli could facilitate
children's facial expression recognition. Seventy-one children aged
between 3 and 6 participated in the study. Their task was to judge
whether a face presented in each trial showed a happy or fearful
expression. Half of the face stimuli were shown with 3D representations,
whereas the other half of the images were shown as 2D pictures. We
compared expression recognition under these conditions. The results
showed that the use of 3D faces improved the speed of facial expression
recognition in both boys and girls. Moreover, 3D faces improved boys'
recognition accuracy for fearful expressions. Since fear is the most
difficult facial expression for children to recognize, the facilitation
effect of 3D faces has important practical implications for children
with difficulties in facial expression recognition. The potential
benefits of 3D representation for other expressions also have
implications for developing more realistic assessments of children's
expression recognition.},
author = {Wang, Lamei and Chen, Wenfeng and Li, Hong},
doi = {10.1038/srep45464},
issn = {2045-2322},
journal = {SCIENTIFIC REPORTS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
title = {{Use of 3D faces facilitates facial expression recognition in children}},
volume = {7},
year = {2017}
}
@article{ISI:000446151100037,
abstract = {Most human expression variations cause a non-rigid deformation of face
scans, which is a challenge today. In this article, we present a novel
framework for 3D face recognition that uses a geometry and local shape
descriptor in a matching process to overcome the distortions caused by
expressions in faces. This algorithm consists of four major components.
First, the 3D face model is presented at different scales. Second,
isometric-invariant features on each scale are extracted. Third, the
geometric information is obtained on the 3D surface in terms of radial
and level facial curves. Fourth, the feature vectors on each scale are
concatenated with their corresponding geometric information. We
conducted a number of experiments using two well-known and challenging
datasets, namely, the GavabDB and Bosphorus datasets, and superior
recognition performance has been achieved. The new system displays an
overall rank-1 identification rate of 98.9{\%} for all faces with neutral
and non-neutral expressions on the GavabDB database. (C) 2017 Elsevier
Ltd. All rights reserved.},
author = {Abbad, Abdelghafour and Abbad, Khalid and Tairi, Hamid},
doi = {10.1016/j.compeleceng.2017.08.017},
issn = {0045-7906},
journal = {COMPUTERS {\&} ELECTRICAL ENGINEERING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {525--537},
title = {{3D face recognition: Multi-scale strategy based on geometric and local descriptors}},
volume = {70},
year = {2018}
}
@inproceedings{ISI:000457843605028,
abstract = {The progress we are currently witnessing in many computer vision
applications, including automatic face analysis, would not be made
possible without tremendous efforts in collecting and annotating large
scale visual databases. To this end, we propose 4DFAB, a new large scale
database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes).
4DFAB contains recordings of 180 subjects captured in four different
sessions spanning over a five-year period. It contains 4D videos of
subjects displaying both spontaneous and posed facial behaviours. The
database can be used for both face and facial expression recognition, as
well as behavioural biometrics. It can also be used to learn very
powerful blendshapes for parametrising facial behaviour. In this paper,
we conduct several experiments and demonstrate the usefulness of the
database for various applications. The database will be made publicly
available for research purposes.},
annote = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Salt Lake City, UT, JUN 18-23, 2018},
author = {Cheng, Shiyang and Kotsia, Irene and Pantic, Maja and Zafeiriou, Stefanos},
booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2018.00537},
isbn = {978-1-5386-6420-9},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; CVF; IEEE Comp Soc},
pages = {5117--5126},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications}},
year = {2018}
}
@article{ISI:000431281900081,
abstract = {Facial reconstruction is a technique that aims to reproduce the
individual facial characteristics based on interpretation of the skull,
with the objective of recognition leading to identification. The aim of
this paper was to evaluate the accuracy and recognition level of
three-dimensional (3D) computerized forensic craniofacial reconstruction
(CCFR) performed in a blind test on open-source software using computed
tomography (CT) data from live subjects. Four CCFRs were produced by one
of the researchers, who was provided with information concerning the
age, sex, and ethnic group of each subject. The CCFRs were produced
using Blender (R) with 3D models obtained from the CT data and templates
from the MakeHuman (R) program. The evaluation of accuracy was carried
out in CloudCompare, by geometric comparison of the CCFR to the subject
3D face model (obtained from the CT data). A recognition level was
performed using the Picasa (R) recognition tool with a frontal
standardized photography, images of the subject CT face model and the
CCFR. Soft-tissue depth and nose, ears and mouth were based on published
data, observing Brazilian facial parameters. The results were presented
from all the points that form the CCFR model, with an average for each
comparison between 63{\%} and 74{\%} with a distance -2.5 {\textless}= x {\textless}= 2.5 mm
from the skin surface. The average distances were 1.66 to 0.33 mm and
greater distances were observed around the eyes, cheeks, mental and
zygomatic regions. Two of the four CCFRs were correctly matched by the
Picasa (R) tool. Free software programs are capable of producing 3D
CCFRs with plausible levels of accuracy and recognition and therefore
indicate their value for use in forensic applications.},
author = {Miranda, Geraldo Elias and Wilkinson, Caroline and Roughley, Mark and Beaini, Thiago Leite and {Haltenhoff Melani}, Rodolfo Francisco},
doi = {10.1371/journal.pone.0196770},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {5},
title = {{Assessment of accuracy and recognition of three-dimensional computerized forensic craniofacial reconstruction}},
volume = {13},
year = {2018}
}
@article{ISI:000386225000016,
abstract = {Performing face recognition across 3D scans with different resolution is
now attracting an increasing interest thanks to the introduction of a
new generation of depth cameras, capable of acquiring color/depth images
over time. In fact, these devices acquire and provide depth data with
much lower resolution compared with the 3D high-resolution scanners
typically used for face recognition applications. If data are acquired
without user cooperation, the problem is even more challenging, and the
gap of resolution between probe and gallery scans can yield to a severe
loss in terms of recognition accuracy. Based on these premises, we
propose a method to build a higher resolution 3D face model from 3D data
acquired by a low-resolution scanner. This face model is built using
data acquired when a person passes in front of the scanner, without
assuming any particular cooperation. The 3D data are registered and
filtered by combining a model of the expected distribution of the
acquisition error with a variant of the lowess method to remove outliers
and build the final face model. The proposed approach is evaluated in
terms of accuracy of face reconstruction and face recognition.},
author = {Bondi, Enrico and Pala, Pietro and Berretti, Stefano and {Del Bimbo}, Alberto},
doi = {10.1109/TIFS.2016.2601059},
issn = {1556-6013},
journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {12},
pages = {2843--2853},
title = {{Reconstructing High-Resolution Face Models From Kinect Depth Sequences}},
volume = {11},
year = {2016}
}
@article{ISI:000353891500010,
abstract = {There are more than 1,000 tornadoes in the United States each year, yet
engineers do not typically design for tornadoes because of insufficient
information about wind loads. Collecting building-level damage data in
the aftermath of tornadoes can improve the understanding of tornado
winds, but these data are difficult to collect because of safety, time,
and access constraints. This study presents and tests an automated
geographic information system (GIS) method using postevent point cloud
data collected by terrestrial scanners and preevent aerial images to
calculate the percentage of roof and wall damage and estimate wind
speeds at an individual building scale. Simulations determined that for
typical point cloud density ({\textgreater}25points/m2), a GIS raster cell size of
40-50cm resulted in less than 10{\%} error in damaged roof and wall
detection. Data collected after recent tornadoes were used to correlate
wind speed estimates and the percent of detected damage. The developed
method estimated wind speeds from damage data collected after the 2011
Tuscaloosa, AL tornado at finer scales than the typical large-scale
assessments done by reconnaissance engineers.},
author = {Kashani, Alireza G and Crawford, Patrick S and Biswas, Sufal K and Graettinger, Andrew J and Grau, David},
doi = {10.1061/(ASCE)CP.1943-5487.0000389},
issn = {0887-3801},
journal = {JOURNAL OF COMPUTING IN CIVIL ENGINEERING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {3},
title = {{Automated Tornado Damage Assessment and Wind Speed Estimation Based on Terrestrial Laser Scanning}},
volume = {29},
year = {2015}
}
@article{ISI:000447286200001,
abstract = {Due to object recognition accuracy limitations, unmanned ground vehicles
(UGVs) must perceive their environments for local path planning and
object avoidance. To gather high-precision information about the UGV's
surroundings, Light Detection and Ranging (LiDAR) is frequently used to
collect large-scale point clouds. However, the complex spatial features
of these clouds, such as being unstructured, diffuse, and disordered,
make it difficult to segment and recognize individual objects. This
paper therefore develops an object feature extraction and classification
system that uses LiDAR point clouds to classify 3D objects in urban
environments. After eliminating the ground points via a height threshold
method, this describes the 3D objects in terms of their geometrical
features, namely their volume, density, and eigenvalues. A
back-propagation neural network (BPNN) model is trained (over the course
of many iterations) to use these extracted features to classify objects
into five types. During the training period, the parameters in each
layer of the BPNN model are continually changed and modified via
back-propagation using a non-linear sigmoid function. In the system, the
object segmentation process supports obstacle detection for autonomous
driving, and the object recognition method provides an environment
perception function for terrain modeling. Our experimental results
indicate that the object recognition accuracy achieve 91.5{\%} in outdoor
environment.},
author = {Song, Wei and Zou, Shuanghui and Tian, Yifei and Fong, Simon and Cho, Kyungeun},
doi = {10.1186/s13673-018-0152-7},
issn = {2192-1962},
journal = {HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
title = {{Classifying 3D objects in LiDAR point clouds with a back-propagation neural network}},
volume = {8},
year = {2018}
}
@inproceedings{ISI:000406771300100,
abstract = {Efficient detection of three dimensional (3D) objects in point clouds is
a challenging problem. Performing 3D descriptor matching or 3D
scanning-window search with detector are both time-consuming due to the
3-dimensional complexity. One solution is to project 3D point cloud into
2D images and thus transform the 3D detection problem into 2D space, but
projection at multiple viewpoints and rotations produce a large amount
of 2D detection tasks, which limit the performance and complexity of the
2D detection algorithm choice. We propose to use convolutional neural
network (CNN) for the 2D detection task, because it can handle all
viewpoints and rotations for the same class of object together, as well
as predicting multiple classes of objects with the same network, without
the need for individual detector for each object class. We further
improve the detection efficiency by concatenating two extra levels of
early rejection networks with binary outputs before the multi-class
detection network. Experiments show that our method has competitive
overall performance with at least one-order of magnitude speedup
comparing with latest 3D point cloud detection methods.},
annote = {23rd International Conference on Pattern Recognition (ICPR), Mexican
Assoc Comp Vis Robot {\&} Neural Comp, Cancun, MEXICO, DEC 04-08, 2016},
author = {Pang, Guan and Neumann, Ulrich},
booktitle = {2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)},
isbn = {978-1-5090-4847-2},
issn = {1051-4651},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm; Elsevier; IBM Res; INTEL; CONACYT},
pages = {585--590},
series = {International Conference on Pattern Recognition},
title = {{3D Point Cloud Object Detection with Multi-View Convolutional Neural Network}},
year = {2016}
}
@article{ISI:000449193300001,
abstract = {3D face recognition is an important topic in the field of pattern
recognition and computer graphic. We propose a novel approach for 3D
face recognition using local conformal parameterization and iso-geodesic
stripes. In our framework, the 3D facial surface is considered as a
Riemannian 2-manifold. The surface is mapped into the 2D circle
parameter domain using local conformal parameterization. In the
parameter domain, the geometric features are extracted from the
iso-geodesic stripes. Combining the relative position measure, Chain 2D
Weighted Walkthroughs (C2DWW), the 3D face matching results can be
obtained. The geometric features from iso-geodesic stripes in parameter
domain are robust in terms of head poses, facial expressions, and some
occlusions. In the experiments, our method achieves a high recognition
accuracy of 3D facial data from the Texas3D and Bosphorus3D face
database.},
author = {Lv, Chenlei and Zhao, Junli},
doi = {10.1155/2018/4707954},
issn = {1024-123X},
journal = {MATHEMATICAL PROBLEMS IN ENGINEERING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
title = {{3D Face Recognition based on Local Conformal Parameterization and Iso-Geodesic Stripes Analysis}},
year = {2018}
}
@inproceedings{8075548,
abstract = {Developing multimedia embedded applications continues to flourish. In fact, a biometric facial recognition system can be used not only on PCs abut also in embedded systems, it is a potential enhancer to meet security and surveillance needs. The analysis of facial recognition consists offoursteps: face analysis, face expressions' recognition, missing data completion and full face recognition. This paper proposes a hardware architecture based on an adaptation approach foran algorithm which has proven good face detection and recognition in 3D space. The proposed application was tested using a co design technique based on a mixed Hardware Software architecture: the FPGA platform.},
annote = {24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Frikha, Tarek and Chaabane, Faten and Said, Boukhchim and Drira, Hassen and Abid, Mohamed and {Ben Amar}, Chokri and Lille, Lifl},
booktitle = {2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)},
doi = {10.1109/ATSIP.2017.8075548},
isbn = {978-1-5386-0551-6},
keywords = {biometrics (access control),embedded systems,etapa1,face,id341,ieeexplore,izaias,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,id341,ieeexplore,izaias,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {may},
pages = {1--5},
publisher = {IEEE},
title = {{Embedded approach for a Riemannian-based framework of analyzing 3D faces}},
url = {http://ieeexplore.ieee.org/document/8075548/},
year = {2017}
}
@inproceedings{ISI:000386931400187,
abstract = {3D partial face recognition under missing parts, occlusions and data
corruptions is a major challenge for the practical application of the
techniques of 3D face recognition. Moreover, one individual can only
provide one sample for training in most practical scenarios, and thus
the face recognition with single sample problem is another highly
challenging task. We propose an efficient framework for 3D partial face
recognition with single sample addressing both of the two problems.
First, we represent a facial scan with a set of keypoint based local
geometrical descriptors, which gains sufficient robustness to partial
facial data along with expression/pose variations. Then, a two-step
modified collaborative representation classification scheme is proposed
to address the single sample recognition problem. A class-based
probability estimation is given during the first classification step,
and the obtained result is then incorporated into the modified
collaborative representation classification as a locality constraint to
improve its classification performance. Extensive experiments on the
Bosphorus and FRGC v2.0 datasets demonstrate the efficiency of the
proposed approach when addressing the problem of 3D partial face
recognition with single sample.},
annote = {IEEE 11th Conference on Industrial Electronics and Applications (ICIEA),
Hefei, PEOPLES R CHINA, JUN 05-07, 2016},
author = {Lei, Yinjie and Feng, Siyu and Zhou, Xinzhi and Guo, Yulan},
booktitle = {PROCEEDINGS OF THE 2016 IEEE 11TH CONFERENCE ON INDUSTRIAL ELECTRONICS AND APPLICATIONS (ICIEA)},
isbn = {978-1-4673-8644-9},
issn = {2156-2318},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; IEEE Ind Elect Soc; IEEE Ind Elect Chapter; IEEE Singapore Sect; Anhui Univ},
pages = {994--999},
series = {IEEE Conference on Industrial Electronics and Applications},
title = {{An efficient 3D partial face recognition approach with single sample}},
year = {2016}
}
@inproceedings{ISI:000392265900097,
abstract = {The vision of the surrounding and people that are within eyeshot
influences the human well-being and safety. The rationale of system
development that allows recognizing faces from difficult perspectives
online and informing timely about approaching people is undisputed.
The manuscript describes the methods of automatic detection of
equilibrium face points in the bitmap image and methods of forming 3D
face model. The optimal search algorithm for equilibrium points has been
chosen. The method of forming 3D face model basing on a single bitmap
image and building up the face image rotated to the preset angle has
been proposed. The algorithm for estimating the angle and algorithm of
the face image rotation have been implemented. The manuscript also
reviews the existing methods of forming 3D face model. The algorithm for
the formation of 3D face model from a single bitmap image and a set of
individual 3D models have been proposed as well as the algorithm for
forming different face angles with the calculated 3D face model aimed to
create biometric vectors cluster. Operation results of the algorithm for
face images formation from different angles have been presented. (C)
2017 Published by Future Academy www.FutureAcademy.org.uk},
annote = {3rd International Scientific Symposium on Lifelong Wellbeing in the
World (WELLSO), Tomsk Polytechn Univ, Tomsk, RUSSIA, SEP 11-16, 2016},
author = {Nebaba, S G and Zakharova, A A and Sidorenko, T V and Viitman, V R},
booktitle = {III INTERNATIONAL SCIENTIFIC SYMPOSIUM ON LIFELONG WELLBEING IN THE WORLD (WELLSO 2016)},
doi = {10.15405/epsbs.2017.01.97},
editor = {{Casati, F and Barysheva, GA and Krieger, W}},
issn = {2357-1330},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {735--744},
series = {European Proceedings of Social and Behavioural Sciences},
title = {{Methods of Automatic Face Angle Recognition for Life Support and Safety Systems}},
volume = {19},
year = {2017}
}
@inproceedings{ISI:000414287400034,
abstract = {Face detection and landmark localization have been extensively
investigated and are the prerequisite for many face applications, such
as face recognition and 3D face reconstruction. Most existing methods
achieve success on only one of the two problems. In this paper, we
propose a coupled encoder-decoder network to jointly detect faces and
localize facial key points. The encoder and decoder generate response
maps for facial landmark localization. Moreover, we observe that the
intermediate feature maps from the encoder and decoder have strong power
in describing facial regions, which motivates us to build a unified
framework by coupling the feature maps for multi-scale cascaded face
detection. Experiments on face detection show strongly competitive
results against the existing methods on two public benchmarks. The
landmark localization further shows consistently better accuracy than
state-of-the-arts on three face-in-the-wild databases.},
annote = {12th IEEE International Conference on Automatic Face and Gesture
Recognition (FG), Washington, DC, MAY 30-JUN 03, 2017},
author = {Wang, Lezi and Yu, Xiang and Metaxas, Dimitris N},
booktitle = {2017 12TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2017)},
doi = {10.1109/FG.2017.40},
isbn = {978-1-5090-4023-0},
issn = {2326-5396},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; Baidu; Mitsubishi Elect Res Labs Inc; 3dMD; DI4D; Syst {\&} Technol Res; ObjectVideo Labs; MUKH Technologies; IEEE Comp Soc},
pages = {251--257},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{A Coupled Encoder-Decoder Network for Joint Face Detection and Landmark Localization}},
year = {2017}
}
@article{ISI:000435048200005,
abstract = {A two dimensional (2D) laser scanner was mounted at the front part of a
small 4-wheel autonomous robot with differential steering, at an angle
of 30 degrees pointing downwards. The machine was able to drive between
maize rows and collect concurrent time-stamped data. A robotic total
station tracked the position of a prism mounted on the vehicle. The
total station and laser scanner data were fused to generate a three
dimensional (3D) point cloud. This 3D representation was used to detect
individual plant positions, which are of particular interest for
applications such as phenotyping, individual plant treatment and
precision weeding. Two different methodologies were applied to the 3D
point cloud to estimate the position of the individual plants. The first
methodology used the Euclidian Clustering on the entire point cloud. The
second methodology utilised the position of an initial plant and the
fixed plant spacing to search iteratively for the best clusters. The two
algorithms were applied at three different plant growth stages. For the
first method, results indicated a detection rate up to 73.7{\%} with a
root mean square error of 3.6 cm. The second method was able to detect
all plants (100{\%} detection rate) with an accuracy of 2.7-3.0 cm, taking
the plant spacing of 13 cm into account.},
author = {Reiser, David and Vazquez-Arellano, Manuel and Paraforos, Dimitris S and Garrido-Izard, Miguel and Griepentrog, Hans W},
doi = {10.1016/j.compind.2018.03.023},
issn = {0166-3615},
journal = {COMPUTERS IN INDUSTRY},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {42--52},
title = {{Iterative individual plant clustering in maize with assembled 2D LiDAR data}},
volume = {99},
year = {2018}
}
@inproceedings{ISI:000353328200021,
abstract = {In the context of face modeling, probably the most well-known approach
to represent 3D faces is the 3D Morphable Model (3DMM). When 3DMM is
fitted to a 2D image, the shape as well as the texture and illumination
parameters are simultaneously estimated. However, if real facial texture
is needed, texture extraction from the 2D image is necessary. This paper
addresses the possible problems in texture extraction of a single image
caused by self-occlusion. Unlike common approaches that leverage the
symmetric property of the face by mirroring the visible facial part,
which is sensitive to inhomogeneous illumination, this work first
generates a virtual texture map for the skin area iteratively by
averaging the color of neighbored vertices. Although this step creates
unrealistic, overly smoothed texture, illumination stays constant
between the real and virtual texture. In the second pass, the mirrored
texture is gradually blended with the real or generated texture
according to the visibility. This scheme ensures a gentle handling of
illumination and yet yields realistic texture. Because the blending area
only relates to non-informative area, main facial features still have
unique appearance in different face halves. Evaluation results reveal
realistic rendering in novel poses robust to challenging illumination
conditions and small registration errors.},
annote = {Conference on Image Processing - Machine Vision Applications VIII, San
Francisco, CA, FEB 10-11, 2015},
author = {Qu, Chengchao and Monari, Eduardo and Schuchert, Tobias and Beyerer, Juergen},
booktitle = {IMAGE PROCESSING: MACHINE VISION APPLICATIONS VIII},
editor = {{Lam, EY and Niel, KS}},
isbn = {978-1-62841-495-0},
issn = {0277-786X},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {Soc Imaging Sci {\&} Technol; SPIE},
series = {Proceedings of SPIE},
title = {{Realistic Texture Extraction for 3D Face Models Robust to Self-Occlusion}},
volume = {9405},
year = {2015}
}
@inproceedings{ISI:000413068300005,
abstract = {In this paper, we propose and compare three methods for recognizing
emotions from facial expressions using 4D videos. In the first two
methods, the 3D faces are re-sampled by using curves to extract the
feature information. Two different methods are presented to resample the
faces in an intelligent way using parallel curves and radial curves. The
movement of the face is measured through these curves using two frames:
neutral and peak frame. The deformation matrix is formed by computing
the distance point to point on the corresponding curves of the neutral
frame and peak frame. This matrix is used to create the feature vector
that will be used for classification using Support Vector Machine (SVM).
The third method proposed is to extract the feature information from the
face by using surface normals. At every point on the frame, surface
normals are extracted. The deformation matrix is formed by computing the
Euclidean distances between the corresponding normals at a point on
neutral and peak frames. This matrix is used to create the feature
vector that will be used for classification of emotions using SVM. The
proposed methods are analyzed and they showed improvement over existing
literature.},
annote = {8th International Conference on Intelligent Human Computer Interaction
(IHCI), Pilani, INDIA, DEC 12-13, 2016},
author = {Prathusha, Sai S and Suja, P and Tripathi, Shikha and Louis, R},
booktitle = {INTELLIGENT HUMAN COMPUTER INTERACTION, IHCI 2016},
doi = {10.1007/978-3-319-52503-7_5},
editor = {{Basu, A and Das, S and Horain, P and Bhattacharya, S}},
isbn = {978-3-319-52503-7; 978-3-319-52502-0},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Council Sci {\&} Ind Res, Cent Elect Engn Res Inst Pilani; Birla Inst Technol {\&} Sci Pilani; Indian Inst Informat Technol},
pages = {51--64},
series = {Lecture Notes in Computer Science},
title = {{Emotion Recognition from Facial Expressions of 4D Videos Using Curves and Surface Normals}},
volume = {10127},
year = {2017}
}
@inproceedings{ISI:000450395900040,
abstract = {An improved algorithm based on the feature of facial contour curve is
proposed for 3D face recognition. The paper first analyzes the
physiological structure of human face, extracts the feature points in
the 3D face, and then extracts the face contour line, and combines them
to form the feature model to realize the 3D face recognition. The
experimental results show that this method can effectively improve the
three-dimensional face recognition rate with strong anti-interference
ability.},
annote = {5th International Conference on Electrical {\&} Electronics Engineering
and Computer Science (ICEEECS), Beijing, PEOPLES R CHINA, JUN 29-30,
2018},
author = {Weizheng, Zhao and Weiwei, Tang},
booktitle = {2018 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL {\&} ELECTRONICS ENGINEERING AND COMPUTER SCIENCE (ICEEECS 2018)},
editor = {{Wu, A}},
isbn = {978-1-912407-04-0},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {181--186},
title = {{Improvement Measures of 3D Face Recognition Algorithm Based on Features of Facial Contour Curve}},
year = {2018}
}
@article{ISI:000417481500002,
abstract = {As the interest in human face grows, facial landmarks become more and
more important for a large variety of fields and applications.
Multipurpose medical is evidently leading in this sense, but others such
as skull study for crime scenes, sex estimation, and attractiveness
quantification, morphological and cephalometric analyses are present. A
cluster analysis of the examined papers is performed depending on scope,
landmarking method, and facial database features. The purpose is to face
these topics by providing the reader with a comprehensive view of what
3D facial landmarks are and what ``they have been up to{\{}''{\}} in 2014 and
2015. The aim is to offer to users the very up-to-date scenario, the
best outcomes, i.e., the latest frontier of landmarks' talents and
skills. The third dimension allowed to select the most prominent
contributions, especially in terms of scientific advance innovativeness.},
author = {Marcolin, Federica},
doi = {10.1504/IJBM.2017.10009329},
issn = {1755-8301},
journal = {INTERNATIONAL JOURNAL OF BIOMETRICS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {4},
pages = {279--304},
title = {{Miscellaneous expertise of 3D facial landmarks in recent literature}},
volume = {9},
year = {2017}
}
@inproceedings{ISI:000446968900008,
abstract = {We propose a first investigation towards a methodology for exploiting 3D
descriptors in suspect retrieval in the context of crime investigation.
In this field, the standard method is to construct a facial composite,
based on witness description, by an artist of via software, then search
a match for it in legal databases. An alternative or complementary
scheme would be to define a system of 3D facial attributes that can fit
human verbal face description and use them to annotate face databases.
Such framework allows a more efficient search of legal face database and
more effective suspect shortlisting. In this paper, we describe some
first steps towards that goal, whereby we define some novel 3D face
attributes, we analyze their capacity for face categorization though a
hieratical clustering analysis. Then we present some experiments, using
a cohort of 107 subjects, assessing the extent to which some faces
partition based on some of these attributes meets its human-based
counterpart. Both the clustering analysis and the experiments results
reveal encouraging indicators for this novel proposed scheme.},
annote = {6th International Workshop on Representations, Analysis and Recognition
of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
OCT 27-29, 2016},
author = {Werghi, Naoufel and Drira, Hassen},
booktitle = {REPRESENTATIONS, ANALYSIS AND RECOGNITION OF SHAPE AND MOTION FROM IMAGING DATA},
doi = {10.1007/978-3-319-60654-5_8},
editor = {{BenAmor, B and Chaieb, F and Ghorbel, F}},
isbn = {978-3-319-60654-5; 978-3-319-60653-8},
issn = {1865-0929},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {84--94},
series = {Communications in Computer and Information Science},
title = {{Towards a Methodology for Retrieving Suspects Using 3D Facial Descriptors}},
volume = {684},
year = {2017}
}
@inproceedings{ISI:000455228100040,
abstract = {Other-race effect affects the performance of multi-race facial
expression recognition significantly. Though this phenomenon has been
noticed by psychologists and computer vision researchers for decades,
few work has been done to eliminate this influence caused by other-race
effect. This work proposes an ICA-based other-race effect elimination
method for 3D facial expression recognition. Firstly, the local depth
features are extracted from 3D face point clouds, and then independent
component analysis is used to project the features into a subspace in
which the feature components are mutually independent. Second, a mutual
information based feature selection method is adopted to determine
race-sensitive features. Finally, the features after race-sensitive
information elimination are utilized to conduct facial expression
recognition. The proposed method is evaluated on BU-3DFE database, and
the results reveal that the proposed method is effective to other-race
effect elimination and could improve the multi-race facial expression
recognition performance.},
annote = {13th Chinese Conference on Biometric Recognition (CCBR), Urumqi, PEOPLES
R CHINA, AUG 11-12, 2018},
author = {Xue, Mingliang and Duan, Xiaodong and Liu, Wanquan and Wang, Yuehai},
booktitle = {BIOMETRIC RECOGNITION, CCBR 2018},
doi = {10.1007/978-3-319-97909-0_40},
editor = {{Zhou, J and Wang, Y and Sun, Z and Jia, Z and Feng, J and Shan, S and Ubul, K and Guo, Z}},
isbn = {978-3-319-97909-0; 978-3-319-97908-3},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Chinese Assoc Artificial Intelligence; Chinese Acad Sci, Inst Automat; Springer; Xinjiang Univ},
pages = {367--376},
series = {Lecture Notes in Computer Science},
title = {{An ICA-Based Other-Race Effect Elimination for Facial Expression Recognition}},
volume = {10996},
year = {2018}
}
@article{ISI:000351134600001,
abstract = {The application of terrestrial laser scanning (TLS) in capturing forest
inventory parameters such as diameter at breast height, height and
diameters along stem profiles, and in monitoring forest growth, was
investigated and validated by comparison with conventionally measured
individual tree parameters and plot-level forest growth in a stand of
Sitka spruce (Picea sitchensis (Bong.) Carr.) in Ireland. The data
acquisition for all the plots with different tree sizes and different
slopes was carried out using a terrestrial laser scanner (FARO LS 800
HE80) in November 2007 and November 2009, using the same plot centres
and measurement procedures. The point cloud data were processed with
Autostem (TM) software. The results showed that TLS enables the
acquisition of forest stand parameters with an acceptable accuracy.
Pruning of the lower branches did not improve tree recognition and the
number of (partly) occluded trees stayed the same. Over the 2-year
period, the average difference between the volume increment of the trees
visible to the scanner derived using the conventional method and
Autostem (TM) was 4.77 m(3) ha(-1) and resulted in scanner-derived
estimates that were lower than the estimates obtained by conventional
method by 6.1 {\%}. Using a simple correction factor to account for
occlusion in the laser scanner data, the difference between these
estimates for all trees in the stand became an over-estimation by 6.96
m(3) ha(-1) (8.1 {\%}). At heights up along the stems {\textgreater} 15 m, the errors
in stem diameter estimates started to escalate.},
author = {Mengesha, Taye and Hawkins, Michael and Nieuwenhuis, Maarten},
doi = {10.1007/s10342-014-0844-0},
issn = {1612-4669},
journal = {EUROPEAN JOURNAL OF FOREST RESEARCH},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {mar},
number = {2},
pages = {211--222},
title = {{Validation of terrestrial laser scanning data using conventional forest inventory methods}},
volume = {134},
year = {2015}
}
@inproceedings{ISI:000369718100001,
abstract = {Biometric system using face recognition is the frontier of the security
across various applications in the fields of multimedia, medicine,
civilian surveillance, robotics, etc. Differences in illumination
levels, pose variations, eye-wear, facial hair, aging and disguise are
some of the current challenges in face recognition. The ear, which is
turning out to be a promising biometric identifier having some desirable
properties such as universality, uniqueness, permanence, can also be
used along with face for better performance of the system. A multi-modal
biometric system combining 2D face, 3D face (depth image) and ear
modalities using Microsoft Kinect and Webcam is proposed to address
these challenges to some extent. Also avoiding redundancy in the
extracted features for better processing speed is another challenge in
designing the system. After careful survey of the existing algorithms
applied to 2D face, 3D face and ear data, we focus on the well-known PCA
(Principal Component Analysis) based Eigen Faces algorithm for ear and
face recognition to obtain a better performance with minimal
computational requirements. The resulting proposed system turns out
insensitive to lighting conditions, pose variations, aging and can
completely replace the current recognition systems economically and
provide a better security. A total of 109 subjects participated in
diversified data acquisition sessions involving multiple poses,
illuminations, eyewear and persons from different age groups. The
dataset is also a first attempt on the stated combination of biometrics
and is a contribution to the field of Biometrics by itself for future
experiments. The results are obtained separately against each biometric
and final decision is obtained using all the individual results for
higher accuracy. The proposed system performed at 98.165 {\%} verification
rate which is greater than either of the dual combinations or each of
the stated modality in a statistical and significant manner.},
annote = {2nd International Conference on Computational Intelligence in Data
Mining (ICCIDM), Bhubaneswar, INDIA, DEC 05-06, 2015},
author = {Boggaram, Achyut Sarma and Mallampalli, Pujitha Raj and Muthyala, Chandrasekhar Reddy and Manjusha, R},
booktitle = {COMPUTATIONAL INTELLIGENCE IN DATA MINING, VOL 1, CIDM 2015},
doi = {10.1007/978-81-322-2734-2_1},
editor = {{Behera, HS and Mohapatra, DP}},
isbn = {978-81-322-2734-2; 978-81-322-2732-8},
issn = {2194-5357},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Roland Inst Technol},
pages = {1--11},
series = {Advances in Intelligent Systems and Computing},
title = {{A Novel Approach for Biometric Authentication System Using Ear, 2D Face and 3D Face Modalities}},
volume = {410},
year = {2016}
}
@article{ISI:000358417700010,
abstract = {In this study, a novel method is proposed for gender classification by
adding facial depth features to texture features. Accordingly, the
three-dimensional (3D) generic elastic model is used to reconstruct the
3D model from human face using only a single 2D frontal image. Then, the
texture and depth are extracted from the reconstructed face model.
Afterwards, the local Gabor binary pattern (LGBP) is applied to both
facial texture and reconstructed depth to extract the feature vectors
from both texture and reconstructed depth images. Finally, by combining
2D and 3D feature vectors, the final LGBP histogram bins are generated
and classified by the support vector machine. Favourable outcomes are
acquired for gender classification on the labelled faces in the wild and
FERET databases based on the proposed method compared to several
state-of-the-arts in gender classification.},
author = {Moeini, Ali and Faez, Karim and Moeini, Hossein},
doi = {10.1049/iet-ipr.2014.0733},
issn = {1751-9659},
journal = {IET IMAGE PROCESSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {8},
pages = {690--698},
title = {{Real-world gender classification via local Gabor binary pattern and three-dimensional face reconstruction by generic elastic model}},
volume = {9},
year = {2015}
}
@inproceedings{ISI:000380617000020,
abstract = {Video surveillance has been applied in more and more fields for security
in last decade years, video-based face recognition therefore became an
important task of an intelligent monitoring system. However, among these
captured video faces there are many non-frontal faces. As a result, the
state-of-art face algorithms would become worse when they were employed
to recognize video faces. On the other hand, it was a common phenomenon
especially at video monitoring field that only one training sample per
person is gained from their identification card. The single sample per
person (SSPP) results in effecting even not taking advantage of some
fine algorithms such LDA. In order to effectively improve the correct
recognition rate of multi-pose face recognition with a single frontal
training sample, this paper proposed a face recognition algorithm based
on 3D modeling. In the proposed algorithm, firstly a 2D frontal face
with high-resolution was taken to build a 3D face model, and then
several virtual faces with different poses were produced from the 3D
face model. At last, both the original frontal face image and virtual
face images were put into a gallery set. The algorithm was evaluated on
SCface database using traditional PCA and LDA methods. The result showed
that the proposed approach could effectively improve video face
recognition rate and the correct recognition rate went up about 13{\%} by
LDA compared with traditional PCA. Therefore, the method that was
proposed to create virtual looking down training samples was an
effective algorithm and could be considered to apply in intelligent
video monitoring system.},
annote = {11th International Conference on Natural Computation (ICNC) / 12th
International Conference on Fuzzy Systems and Knowledge Discovery
(FSKD), Zhangjiajie, PEOPLES R CHINA, AUG 15-17, 2015},
author = {Hu, Xiao and Liao, Qixin and Peng, Shaohu},
booktitle = {2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)},
editor = {{Xiao, Z and Tong, Z and Li, K and Wang, X and Li, K}},
isbn = {978-1-4673-7679-2},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; CAS IEEE Circuits {\&} Syst Soc; Hunan Univ; Jishou Univ},
pages = {113--117},
title = {{Video Surveillance Face Recognition by More Virtual Training Samples Based on 3D Modeling}},
year = {2015}
}
@article{ISI:000389801200007,
abstract = {In this paper, we present a new algorithm that utilizes low-quality red,
green, blue and depth (RGB-D) data from the Kinect sensor for face
recognition under challenging conditions. This algorithm extracts
multiple features and fuses them at the feature level. A Finer Feature
Fusion technique is developed that removes redundant information and
retains only the meaningful features for possible maximum class
separability. We also introduce a new 3D face database acquired with the
Kinect sensor which has released to the research community. This
database contains over 5,000 facial images (RGB-D) of 52 individuals
under varying pose, expression, illumination and occlusions. Under the
first three variations and using only the noisy depth data, the proposed
algorithm can achieve 72.5 {\%} recognition rate which is significantly
higher than the 41.9 {\%} achieved by the baseline LDA method. Combined
with the texture information, 91.3 {\%} recognition rate has achieved
under illumination, pose and expression variations. These results
suggest the feasibility of low-cost 3D sensors for real-time face
recognition.},
author = {Li, Billy Y L and Mian, Ajmal S and Liu, Wanquan and Krishna, Aneesh},
doi = {10.1007/s10044-015-0456-4},
issn = {1433-7541},
journal = {PATTERN ANALYSIS AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
number = {4},
pages = {977--987},
title = {{Face recognition based on Kinect}},
volume = {19},
year = {2016}
}
@inproceedings{ISI:000389381200037,
abstract = {In this paper, we propose a novel idea for automatic facial expression
analysis with the aim of resolving the existing challenges in 2D images.
The subtle combination of the geometry-based method with the
appearance-based features in depth and color images contributes to
increasing in distinguishable features among various facial expressions.
Particular functions are utilised to calculate the correlation between
expressions in order to determine the exact facial expression. Our
approach consists of a sequence of steps including estimating the normal
vector of facial surface, then extracting the geometric features such as
the orientation of normal vector in the point cloud. The useful color
information is known as LBP. According to the result of the experiment,
we demonstrate that the effective fusion scheme of texture and shape
feature on color and depth images. In comparison with the non fusion
scheme, our fusion scheme has resulted in the increase of recognition
under low and high illuminated light, about 19.84{\%} and 1.59{\%},
respectively.},
annote = {8th Asian Conference on Intelligent Information and Database Systems
(ACIIDS), Da Nang, VIETNAM, MAR 14-16, 2016},
author = {Truong, Trung and Ly, Ngoc},
booktitle = {Intelligent Information and Database Systems, ACIIDS 2016, Pt II},
doi = {10.1007/978-3-662-49390-8_37},
editor = {{Nguyen, NT and Trawinski, B and Fujita, H and Hong, TP}},
isbn = {978-3-662-49390-8; 978-3-662-49389-2},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Vietnam Korea Friendship Informat Technol Coll; Wroclaw Univ Technol; IEEE SMC Tech Comm Computat Collect Intelligence; Bina Nusantara Univ; Ton Duc Thang Univ; Quang Binh Univ},
pages = {377--387},
series = {Lecture Notes in Artificial Intelligence},
title = {{Building the Facial Expressions Recognition System Based on RGB-D Images in High Performance}},
volume = {9622},
year = {2016}
}
@article{ISI:000435193700027,
abstract = {Many biophysical forest properties such as wood volume and leaf area
index (LAI) require prior knowledge on either photosynthetic or
non-photosynthetic components. Laser scanning appears to be a helpful
technique in nondestructively quantifying forest structures, as it can
acquire an accurate three-dimensional point cloud of objects. In this
study, we propose an unsupervised geometry-based method named Dynamic
Segment Merging (DSM) to identify non-photosynthetic components of trees
by semantically segmenting tree point clouds, and examining the linear
shape prior of each resulting segment. We tested our method using one
single tree dataset and four plot-level datasets, and compared our
results to a supervised machine learning method. We further demonstrated
that by using an optimal neighborhood selection method that involves
multi-scale analysis, the results were improved. Our results showed that
the overall accuracy ranged from 81.8{\%} to 92.0{\%} with an average value
of 87.7{\%}. The supervised machine learning method had an average overall
accuracy of 86.4{\%} for all datasets, on account of a collection of
manually delineated representative training data. Our study indicates
that separating tree photosynthetic and non-photosynthetic components
from laser scanning data can be achieved in a fully unsupervised manner
without the need of training data and user intervention.},
author = {Wang, Di and Brunner, Jasmin and Ma, Zhenyu and Lu, Hao and Hollaus, Markus and Pang, Yong and Pfeifer, Norbert},
doi = {10.3390/f9050252},
issn = {1999-4907},
journal = {FORESTS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {5},
title = {{Separating Tree Photosynthetic and Non-Photosynthetic Components from Point Cloud Data Using Dynamic Segment Merging}},
volume = {9},
year = {2018}
}
@inproceedings{ISI:000368591300013,
abstract = {In last years, the emergence of 3D shape in face recognition is due to
its robustness to pose and illumination changes. These attractive
benefits are not all the challenges to achieve satisfactory recognition
rate. Other challenges such as facial expressions and computing time of
matching algorithms remain to be explored. In this context, we propose
our 3D face recognition approach using 3D wavelet networks. Our approach
contains two stages: learning stage and recognition stage. For the
training we propose a novel algorithm based on 3D fast wavelet
transform. From 3D coordinates of the face (x,y,z), we proceed to
voxelization to get a 3D volume which will be decomposed by 3D fast
wavelet transform and modeled after that with a wavelet network, then
their associated weights are considered as vector features to represent
each training face. For the recognition stage, an unknown identity face
is projected on all the training WN to obtain a new vector features
after every projection. A similarity score is computed between the old
and the obtained vector features. To show the efficiency of our
approach, experimental results were performed on all the FRGC v.2
benchmark.},
annote = {8th International Conference on Machine Vision (ICMV), Barcelona, SPAIN,
NOV 19-21, 2015},
author = {Said, Salwa and Jemai, Olfa and Zaied, Mourad and {Ben Amar}, Chokri},
booktitle = {EIGHTH INTERNATIONAL CONFERENCE ON MACHINE VISION (ICMV 2015)},
doi = {10.1117/12.2228368},
editor = {{Verikas, A and Radeva, P and Nikolaev, D}},
isbn = {978-1-5106-0116-1},
issn = {0277-786X},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
series = {Proceedings of SPIE},
title = {{3D Fast Wavelet Network Model-Assisted 3D Face Recognition}},
volume = {9875},
year = {2015}
}
@inproceedings{ISI:000371977803080,
abstract = {With the growing availability and wide distribution of low-cost,
high-performance 3D imaging sensors, the image analysis community has
witnessed an increased demand for solutions to the challenges of
activity recognition and person identification. We propose an integrated
framework, based on graph signal processing, that simultaneously
performs both tasks using a single set of features. The novelty of our
approach is based on the fact that the set of features used for activity
recognition accommodates person identification without additional
computation. The analysis is based on the extracted structure-invariant
graph (skeleton). The Laplacian of the skeleton is used both to identify
the person and recognize the performed activity. While person
identification is achieved directly from the analysis of the Laplacian,
activity recognition is obtained after transformation, into the graph
spectral domain, of the vectorized form of the skeletal joints 3D
coordinates. Feature vectors for activity recognition are then derived,
in this domain, from the covariance matrices evaluated over fixed-length
sequential video segments. Both classification tasks are implemented
using linear support vector machines (SVM). When applied to real
activity datasets, our approach shows an improved performance over the
existing state-of-the-art.},
annote = {IEEE International Conference on Image Processing (ICIP), Quebec City,
CANADA, SEP 27-30, 2015},
author = {Batabyal, Tamal and Vaccari, Andrea and Acton, Scott T},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4799-8339-1},
issn = {1522-4880},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; IEEE Signal Proc Soc},
pages = {3270--3274},
series = {IEEE International Conference on Image Processing ICIP},
title = {{UGraSP: A UNIFIED FRAMEWORK FOR ACTIVITY RECOGNITION AND PERSON IDENTIFICATION USING GRAPH SIGNAL PROCESSING}},
year = {2015}
}
@article{ISI:000373410000015,
abstract = {In this letter, the authors propose a new embedding scheme for
image-based continuous face pose estimation. The main contributions are
as follows. First, it is shown that the concept of label-sensitive
Locality Preserving Projections, proposed for age estimation, can be
used for model-less face pose estimation. Second, the authors propose a
linear embedding by exploiting the connections between facial features
and pose labels via a sparse coding scheme. The resulting technique is
called Sparse Label sensitive Locality Preserving Projections
(Sp-LsLPP). Third, for enhancing the discrimination between poses, the
projections obtained by Sp-LsLPP are fed to a Discriminant Embedding
that exploits the continuous labels. The resulting framework has less
parameters compared to related works. It has been applied to the problem
of model-less face yaw angle estimation (person independent 3D face pose
estimation). It was tested on three databases: FacePix, Taiwan, and
Columbia. It was conveniently compared with other linear and non-linear
techniques. The experimental results confirm that the proposed framework
can outperform, in general, the existing ones. (C) 2016 Elsevier Ltd.
All rights reserved.},
author = {Dornaika, F and Chahla, C and Khattar, F and Abdallah, F and Snoussi, H},
doi = {10.1016/j.engappai.2016.01.035},
issn = {0952-1976},
journal = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {168--176},
title = {{Discriminant sparse label-sensitive embedding: Application to image-based face pose estimation}},
volume = {50},
year = {2016}
}
@article{ISI:000375611700005,
abstract = {The authors propose a photometric method to recover facial shape that is
consistent with expected facial proportions. The method borrows ideas
from photometric sampling, a technique that estimates shape from
continuous variations of a light source around a single circular path.
This approach aims at enriching photometric information by including
variations of the light source along its zenith angle. To this end, a
luminance matrix describing lighting response along both azimuth and
zenith angles of the light source is built for each pixel. A method
based on fitting sine functions onto the singular vectors of the
collected luminance matrices is proposed for estimating a surface normal
map. The estimated surface normals are later refined to maximize a
facial proportion criterion and finally be integrated. Experiments
demonstrate that our approach successfully approximates 3D face shape
while preserving facial proportions within the limits of expected depth.},
author = {Hernandez-Rodriguez, Felipe and Castelan, Mario},
doi = {10.1007/s00138-016-0755-9},
issn = {0932-8092},
journal = {MACHINE VISION AND APPLICATIONS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {4},
pages = {483--497},
title = {{A photometric sampling method for facial shape recovery}},
volume = {27},
year = {2016}
}
@article{ISI:000399520700020,
abstract = {Facial expression verification has been extensively exploited due to its
wide application in affective computing, robotic vision, man-machine
interaction and medical diagnosis. With the recent development of
Internet-of-Things (loT), there is a need of mobile-targeted facial
expression verification, where face scrambling has been proposed for
privacy protection during image/video distribution over public network.
Consequently, facial expression verification needs to be carried out in
a scrambled domain, bringing out new challenges in facial expression
recognition. An immediate impact from face scrambling is that
conventional semantic facial components become not identifiable, and 3D
face models cannot be clearly fitted to a scrambled image. Hence, the
classical facial action coding system cannot be applied to facial
expression recognition in the scrambled domain. To cope with chaotic
signals from face scrambling, this paper proposes an new approach - Many
Graph Embedding (MGE) to discover discriminative patterns from the
subspaces of chaotic patterns, where the facial expression recognition
is carried out as a fuzzy combination from many graph embedding. In our
experiments, the proposed MGE was evaluated on three scrambled facial
expression datasets: JAFFE, MUG and CK++. The benchmark results
demonstrated that the proposed method is able to improve the recognition
accuracy, making our method a promising candidate for the scrambled
facial expression recognition in the emerging privacy-protected loT
applications. (C) 2017 Elsevier Ltd. All rights reserved.},
author = {Jiang, Richard and Ho, Anthony T S and Cheheb, Ismahane and Al-Maadeed, Noor and Al-Maadeed, Somaya and Bouridane, Ahmed},
doi = {10.1016/j.patcog.2017.02.003},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
pages = {245--251},
title = {{Emotion recognition from scrambled facial images via many graph embedding}},
volume = {67},
year = {2017}
}
@inproceedings{ISI:000374793800022,
abstract = {In this work we describe a novel one-shot face recognition setup.
Instead of using a 3D scanner to reconstruct the face, we acquire a
single photo of the face of a person while a rectangular pattern is been
projected over it. Using this unique image, it is possible to extract 3D
low-level geometrical features without the explicit 3D reconstruction.
To handle expression variations and occlusions that may occur (e.g.
wearing a scarf or a bonnet), we extract information just from the
eyes-forehead and nose regions which tend to be less influenced by
facial expressions. Once features are extracted, SVM hyper-planes are
obtained from each subject on the database (one vs all approach), then
new instances can be classified according to its distance to each of
those hyper-planes. The advantage of our method with respect to other
ones published in the literature, is that we do not need and explicit 3D
reconstruction. Experiments with the Texas 3D Database and with new
acquired data are presented, which shows the potential of the presented
framework to handle different illumination conditions, pose and facial
expressions.},
annote = {20th Iberoamerican Congress on Pattern Recognition (CIARP), Montevideo,
URUGUAY, NOV 09-12, 2015},
author = {{Matias Di Martino}, J and Fernandez, Alicia and Ferrari, Jose},
booktitle = {PROGRESS IN PATTERN RECOGNITION, IMAGE ANALYSIS, COMPUTER VISION, AND APPLICATIONS, CIARP 2015},
doi = {10.1007/978-3-319-25751-8_22},
editor = {{Pardo, A and Kittler, J}},
isbn = {978-3-319-25751-8; 978-3-319-25750-1},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {IAPR; Uruguayan IAPR Chapter; Argentine Soc Pattern Recognit; Special Interest Grp Brazilian Comp Soc; Chilean Assoc Pattern Recognit; Cuban Assoc Pattern Recognit; Mexican Assoc Comp Vis, Neural Comp {\&} Robot; Spanish Assoc Pattern Recognit {\&} Image Anal; },
pages = {176--183},
series = {Lecture Notes in Computer Science},
title = {{One-Shot 3D-Gradient Method Applied to Face Recognition}},
volume = {9423},
year = {2015}
}
@article{ISI:000457666900042,
abstract = {3D registration is a very active topic, spanning research areas such as
computational geometry, computer graphics and pattern recognition. It
aims to solve spatial transformation that aligns two point clouds. In
this work we propose the use of a single direction sensor, such as an
accelerometer or a magnetometer, commonly available on contemporary
mobile platforms, such as tablets and smartphones. Both sensors have
been heavily investigated earlier, but only for joint use with other
sensors, such as gyroscopes and GPS. We show a time-efficient and
accurate 3D registration method that takes advantage of only either an
accelerometer or a magnetometer. We demonstrate a 3D reconstruction of
individual point clouds and the proposed 3D registration method on a
tablet equipped with an accelerometer or a magnetometer. However, we
point out that the proposed method is not restricted to mobile
platforms. Indeed, it can easily be applied in any 3D measurement system
that is upgradable with some ubiquitous direction sensor, for example by
adding a smartphone equipped with either an accelerometer or a
magnetometer. We compare the proposed method against several
state-of-the-art methods implemented in the open source Point Cloud
Library (PCL). The proposed method outperforms the PCL methods tested,
both in terms of processing time and accuracy. (C) 2018 Elsevier Ltd.
All rights reserved.},
author = {Pribanic, Tomislav and Petkovic, Tomislav and Donlic, Matea},
doi = {10.1016/j.patcog.2018.12.008},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {532--546},
title = {{3D registration based on the direction sensor measurements}},
volume = {88},
year = {2019}
}
@inproceedings{ISI:000440475200002,
abstract = {3D face recognition has gain a paramount importance over 2D due to its
potential to address the limitations of 2D face recognition against the
variation in facial poses, angles, occlusions etc. Research in 3D face
recognition has accelerated in recent years due to the development of
low cost 3D Kinect camera sensor. This has leads to the development of
few RGB-D database across the world. Here in this paper we introduce the
base results of our 3D facial database (GU-RGBD database) comprising
variation in pose (0 degrees, 45 degrees, 90 degrees, -45 degrees, -90
degrees), expression (smile, eyes closed), occlusion (half face covered
with paper) and illumination variation using Kinect. We present a
proposed noise removal non-linear interpolation filter for the patches
present in the depth images. The results were obtained on three face
recognition algorithms and fusion at matching score level for
recognition and verification rate. The obtained results indicated that
the performance with our proposed filter shows improvement over pose
with score level fusion using sum rule.},
annote = {10th Indian Conference on Vision, Graphics and Image Processing
(ICVGIP), IIT Guwahati, Guwahati, INDIA, DEC 19, 2016},
author = {Gaonkar, A A and Gad, M D and Vetrekar, N T and Tilve, Vithal Shet and Gad, R S},
booktitle = {COMPUTER VISION, GRAPHICS, AND IMAGE PROCESSING, ICVGIP 2016},
doi = {10.1007/978-3-319-68124-5_2},
editor = {{Mukherjee, S and Mukherjee, S and Mukherjee, DP and Sivaswamy, J and Awate, S and Setlur, S and Namboodiri, AM and Chaudhury, S}},
isbn = {978-3-319-68124-5; 978-3-319-68123-8},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {15--26},
series = {Lecture Notes in Computer Science},
title = {{Experimental Evaluation of 3D Kinect Face Database}},
volume = {10481},
year = {2017}
}
@article{ISI:000438827800001,
abstract = {Face Recognition {\{}[{\}}Pentland et al 1991] confronts innumerable hurdles
in the form of variations in lighting conditions during image capture,
Occlusions, damage in facial portions due to accidents etc. Hence
recovery of the complete picture of a human face from partially occluded
images is quite a challenge in Image Processing. Facial imaging proposed
in this paper is the method of generating 3-D face mask of a subject.
This process involves receiving one frontal image of the subject from a
digital imaging device (Cameras in Smartphone, tablet or PC) and
applying algorithms for edge recovery of the face. Once the face edges
are detected, the light intensity incident on the face is mapped as a 3D
histogram. The intensity gradients formed due to the contour {\{}[{\}}Kass et
al 1998] of the face are then mapped on to the face as depth points in a
grid-based space. Finally, the representation determined from the
database is used to generate 3D facial data of the subject based on the
best mapping.},
author = {Nazim, Khalid S A and Harsha, S and Bhaskar, N and {Al Quhayz}, Hani},
issn = {1738-7906},
journal = {INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND NETWORK SECURITY},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jun},
number = {6},
pages = {1--7},
title = {{Facial Image Reconstruction From A Single Frontal Image Using Intensity Histograms and 3-D Mapping}},
volume = {18},
year = {2018}
}
@inproceedings{ISI:000382326300052,
abstract = {This paper presents an approach that automatically (but parametrically)
reconstructs 2-D/3-D building footprints using 3-D synthetic aperture
radar (SAR) tomography (TomoSAR) point clouds. These point clouds are
generated by processing SAR image stacks via SAR tomographic inversion.
The proposed approach reconstructs the building outline by exploiting
both the roof and facade points. Initial building footprints are derived
by applying the alpha shapes method on pre-segmented point clusters of
individual buildings. A recursive angular deviation based refinement is
then carried out to obtain refined/smoothed 2-D polygonal boundaries. A
robust fusion framework then fuses the information pertaining to
building facades to the smoothed polygons. Afterwards, a rectilinear
building identification procedure is adopted and constraints are added
to yield geometrically correct and visually aesthetic building shapes.
The proposed approach is illustrated and validated using TomoSAR point
clouds generated from a stack of TerraSAR-X high-resolution spotlight
images from ascending orbit covering approximately 1.5 km(2) area in the
city of Berlin, Germany.},
annote = {ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015},
author = {Shahzad, M and Zhu, X X},
booktitle = {ISPRS GEOSPATIAL WEEK 2015},
doi = {10.5194/isprsannals-II-3-W5-385-2015},
editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Rottensteiner, F and Yang, M and Christophe, S and Coltekin, A and Bredif, M}},
issn = {2194-9034},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {W5},
organization = {WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging {\&} Sensors; WG VII 7 Synergy Radar; ISPRS},
pages = {385--392},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{RECONSTRUCTION OF BUILDING FOOTPRINTS USING SPACEBORNE TOMOSAR POINT CLOUDS}},
volume = {II-3},
year = {2015}
}
@article{ISI:000391965900001,
abstract = {This paper presents a novel approach to recognize and estimate pose of
the 3D objects in cluttered range images. The key technical breakthrough
of the developed approach can enable robust object recognition and
localization under undesirable condition such as environmental
illumination variation as well as optical occlusion to viewing the
object partially. First, the acquired point clouds are segmented into
individual object point clouds based on the developed 3D object
segmentation for randomly stacked objects. Second, an efficient
shape-matching algorithm called Sub-OBB based object recognition by
using the proposed oriented bounding box (OBB) regional area-based
descriptor is performed to reliably recognize the object. Then, the 3D
position and orientation of the object can be roughly estimated by
aligning the OBB of segmented object point cloud with OBB of matched
point cloud in a database generated from CAD model and 3D virtual
camera. To detect accurate pose of the object, the iterative closest
point (ICP) algorithm is used to match the object model with the
segmented point clouds. From the feasibility test of several scenarios,
the developed approach is verified to be feasible for object pose
recognition and localization.},
author = {Hoang, Dinh-Cuong and Chen, Liang-Chia and Nguyen, Thanh-Hung},
doi = {10.1088/1361-6501/aa513a},
issn = {0957-0233},
journal = {MEASUREMENT SCIENCE AND TECHNOLOGY},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {2},
title = {{Sub-OBB based object recognition and localization algorithm using range images}},
volume = {28},
year = {2017}
}
@article{Li:2018:EFR:3198485.3198687,
abstract = {This study proposes a 3D face recognition method using multiple subject-specific curves insensitive to intra-subject distortions caused by expression variations. Considering that most sharp variances in facial convex regions are closely related to the bone structure, the convex crest curves are first extracted as the most vital subject-specific facial curves based on the principal curvature extrema in convex local surfaces. Then, the central profile curve and the horizontal contour curve passing through the nose tip are detected by using the precise localization of the nose tip and symmetry plane. Based on their discriminative power and robustness to expression changes, the three types of curves are fused with appropriate weights at the feature-level and used for matching 3D faces with the iterative closest point algorithm. The combination of multiple expression-insensitive curves is complementary and provides sufficient and stable facial surface features for face recognition. In addition, for each convex crest curve, an expression-irrelevant factor is assigned as the adaptive weight to improve the face matching performance. The results of experiments using two public 3D databases, GavabDB and BU-3DFE, demonstrate the effectiveness of the proposed method, and its recognition rates on both databases reflect an encouraging performance.},
address = {Amsterdam, The Netherlands, The Netherlands},
annote = {29/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
29/04 Exclu{\'{i}}do (etapa 1)

Janeiro de 2018},
author = {Li, Ye and Wang, YingHui and Liu, Jing and Hao, Wen},
doi = {10.1016/j.neucom.2017.09.070},
issn = {0925-2312},
journal = {Neurocomput.},
keywords = {3D face recognition,Expression-insensitive,Feature-level,Fusion,Subject-specific curve,acm,etapa1,id395,import{\_}poly,poly,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {acm,etapa1,id395,import{\_}poly,poly,revisao{\_}scopus,revisao{\_}webofscience},
number = {C},
pages = {1295--1307},
publisher = {Elsevier Science Publishers B. V.},
title = {{Expression-insensitive 3D Face Recognition by the Fusion of Multiple Subject-specific Curves}},
url = {https://doi.org/10.1016/j.neucom.2017.09.070},
volume = {275},
year = {2018}
}
@inproceedings{ISI:000380516600070,
abstract = {In the past decade, the interest in using 3D data for biometric person
authentication has increased significantly, propelled by the
availability of affordable 3D sensors. The adoption of 3D features has
been especially successful in face recognition applications, leading to
several commercial 3D face recognition products. In other biometric
modalities such as hand recognition, several studies have shown the
potential advantage of using 3D geometric information, however, no
commercial-grade systems are currently available. In this paper, we
present a contactless 3D hand recognition system based on the novel
Intel RealSense camera, the first mass-produced embeddable 3D sensor.
The small form factor and low cost make this sensor especially appealing
for commercial biometric applications, however, they come at the price
of lower resolution compared to more expensive 3D scanners used in
previous research. We analyze the robustness of several existing 2D and
3D features that can be extracted from the images captured by the
RealSense camera and study the use of metric learning for their fusion.},
annote = {International Conference on Biometrics (ICB), Phuket, THAILAND, MAY
19-22, 2015},
author = {Svoboda, Jan and Bronstein, Michael M and Drahansky, Martin},
booktitle = {2015 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)},
isbn = {978-1-4799-7824-3},
issn = {2376-4201},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {SIEW-SNGIEM AWARD; Kasetsart Univ; CHANWANICH; ST Elect; SAFRAN Morpho},
pages = {452--457},
series = {International Conference on Biometrics},
title = {{Contactless biometric hand geometry recognition using a low-cost 3D camera}},
year = {2015}
}
@inproceedings{ISI:000434349200016,
abstract = {We present HoloFace, an open-source framework for face alignment, head
pose estimation and facial attribute retrieval for Microsoft HoloLens.
HoloFace implements two state-of-the-art face alignment methods which
can be used interchangeably: one running locally and one running on a
remote backend. Head pose estimation is accomplished by fitting a
deformable 3D model to the landmarks localized using face alignment. The
head pose provides both the rotation of the head and a position in the
world space. The parameters of the fitted 3D face model provide
estimates of facial attributes such as mouth opening or smile. Together
the above information can be used to augment the faces of people seen by
the HoloLens user, and thus their interaction. Potential usage scenarios
include facial recognition, emotion recognition, eye gaze tracking and
many others. We demonstrate the capabilities of our framework by
augmenting the faces of people seen through the HoloLens with various
objects and animations.},
annote = {18th IEEE Winter Conference on Applications of Computer Vision (WACV),
NV, MAR 12-15, 2018},
author = {Kowalski, Marek and Nasarzewski, Zbigniew and Galinski, Grzegorz and Garbat, Piotr},
booktitle = {2018 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV 2018)},
doi = {10.1109/WACV.2018.00022},
isbn = {978-1-5386-4886-5},
issn = {2472-6737},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; IEEE Comp Soc; IEEE Biometr Council; Cognex; Google; Honeywell; Kitware; Netflix; SAP; Amazon; Percept Automata; Verisk Analyt},
pages = {141--149},
series = {IEEE Winter Conference on Applications of Computer Vision},
title = {{HoloFace: Augmenting Human-to-Human Interactions on HoloLens}},
year = {2018}
}
@inproceedings{ISI:000387959204074,
abstract = {We present an algorithm for automatic detection of a large number of
anthropometric landmarks on 3D faces. Our approach does not use texture
and is completely shape based in order to detect landmarks that are
morphologically significant. The proposed algorithm evolves level set
curves with adaptive geometric speed functions to automatically extract
effective seed points for dense correspondence. Correspondences are
established by minimizing the bending energy between patches around seed
points of given faces to those of a reference face. Given its
hierarchical structure, our algorithm is capable of establishing
thousands of correspondences between a large number of faces. Finally, a
morphable model based on the dense corresponding points is fitted to an
unseen query face for transfer of correspondences and hence automatic
detection of landmarks. The proposed algorithm can detect any number of
pre-defined landmarks including subtle landmarks that are even difficult
to detect manually. Extensive experimental comparison on two benchmark
databases containing 6, 507 scans shows that our algorithm outperforms
six state of the art algorithms.},
annote = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Boston, MA, JUN 07-12, 2015},
author = {Gilani, Syed Zulqarnain and Shafait, Faisal and Mian, Ajmal},
booktitle = {2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
isbn = {978-1-4673-6964-0},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE},
pages = {4639--4648},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Shape-based Automatic Detection of a Large Number of 3D Facial Landmarks}},
year = {2015}
}
@inproceedings{ISI:000390841700083,
abstract = {We propose a robust method for 3D face recognition using 3D to 2D
modeling and facial curvatures detection. The 3D2D algorithm permits to
transform 3D images into 3D triangular mesh, then the mesh model is
deformed and fitted to the 2D space in order to obtain a 2D smoother
mesh. Then, we apply Gabor wavelets to the deformed model in order to
exploit surface curves in the detection of salient face features. The
classification of the final Gabor facial model is performed using the
support vector machines (SVM). To demonstrate the quality of our
technique, we give some experiments using the 3D AJMAL faces database.
The experimental results prove that the proposed method is able to give
a good recognition quality and a high accuracy rate.},
annote = {2nd International Conference on Advanced Technologies for Signal and
Image Processing (ATSIP), Monastir, TUNISIA, MAR 21-23, 2016},
author = {Torkhani, Ghada and Ladgham, Anis and Mansouri, Mohamed Nejib and Sakly, Anis},
booktitle = {2016 2ND INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL AND IMAGE PROCESSING (ATSIP)},
isbn = {978-1-4673-8526-8},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; IEEE Tunisia Sect; ATMS Lab; ATSI; IEEE Explore; EMB; ENIS Sch; Telecom Paris; Supelec; CESBIO; Telecom SudParis; ENIT; Univ Paris Sud; IEEE Signal Proc Soc Tunisia Chapter; ISAAM Inst; Minist Higher Educ Res; IEEE EMP Tunisia Chapter; Novartis Comp},
pages = {447--452},
title = {{Gabor-SVM Applied to 3D-2D Deformed Mesh Model}},
year = {2016}
}
@inproceedings{ISI:000446968900004,
abstract = {With the growth of face recognition, the spoofing mask attacks attract
more attention in biometrics research area. In recent years, the
countermeasures based on the texture and depth image against spoofing
mask attacks have been reported, but the research based on 3D meshed
sample has not been studied yet. In this paper, we propose to apply 3D
shape analysis based on principal curvature measures to describe the
meshed facial surface. Meanwhile, a verification protocol based on this
feature descriptor is designed to verify person identity and to evaluate
the anti-spoofing performance on Morpho database. Furthermore, for
simulating a real-life testing scenario, FRGCv2 database is enrolled as
an extension of face scans to augment the ratio of genuine face samples
to fraud mask samples. The experimental results show that our system can
guarantee a high verification rate for genuine faces and the
satisfactory anti-spoofing performance against spoofing mask attacks in
parallel.},
annote = {6th International Workshop on Representations, Analysis and Recognition
of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
OCT 27-29, 2016},
author = {Tang, Yinhang and Chen, Liming},
booktitle = {REPRESENTATIONS, ANALYSIS AND RECOGNITION OF SHAPE AND MOTION FROM IMAGING DATA},
doi = {10.1007/978-3-319-60654-5_4},
editor = {{BenAmor, B and Chaieb, F and Ghorbel, F}},
isbn = {978-3-319-60654-5; 978-3-319-60653-8},
issn = {1865-0929},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {41--55},
series = {Communications in Computer and Information Science},
title = {{Shape Analysis Based Anti-spoofing 3D Face Recognition with Mask Attacks}},
volume = {684},
year = {2017}
}
@inproceedings{ISI:000361841100052,
abstract = {We present a systematic study on the relationship between the 3D shape
of a hand that is about to grasp an object and recognition of the object
to be grasped. In this paper, we investigate the direction from the
shape of the hand to object recognition for unimpaired users. Our work
shows that the 3D shape of a grasping hand from an egocentric point of
view can help improve recognition of the objects being grasped. Previous
work has attempted to exploit hand interactions or gaze information in
the egocentric setting to guide object segmentation. However, all such
analyses are conducted in 2D. We hypothesize that the 3D shape of a
grasping hand is highly correlated to the physical attributes of the
object being grasped. Hence, it can provide very beneficial visual
information for object recognition. We validate this hypothesis by first
building a 3D, egocentric vision pipeline to segment and reconstruct
dense 3D point clouds of the grasping hands. Then, visual descriptors
are extracted from the point cloud and subsequently fed into an object
recognition system to recognize the object being grasped. Our
experiments demonstrate that the 3D hand shape can indeed greatly help
improve the visual recognition accuracy, when compared with the baseline
where only 2D image features are utilized.},
annote = {13th European Conference on Computer Vision (ECCV), Zurich, SWITZERLAND,
SEP 06-12, 2014},
author = {Lin, Yizhou and Hua, Gang and Mordohai, Philippos},
booktitle = {COMPUTER VISION - ECCV 2014 WORKSHOPS, PT III},
doi = {10.1007/978-3-319-16199-0_52},
editor = {{Agapito, L and Bronstein, MM and Rother, C}},
isbn = {978-3-319-16199-0; 978-3-319-16198-3},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {746--762},
series = {Lecture Notes in Computer Science},
title = {{Egocentric Object Recognition Leveraging the 3D Shape of the Grasping Hand}},
volume = {8927},
year = {2015}
}
@inproceedings{ISI:000380489200045,
abstract = {In this paper, a new technique, i.e. decremental depth bunches have been
presented where two facial discriminating mechanisms have also been
implemented for recognizing the individuals. Notably, based on
variations of the depth values, different bunches of face regions (i.e.
the small components) are extracted by differentiating the depth
information that eventually describes detailed facial surface
information. Now, from each bunches, statistical attributes as well as
Hough peaks are encountered to initiate two feature vectors for
feature-based as well as a holistic mechanism for classification by K-NN
and Cosine distance respectively. The proposed mechanism is explicitly
dependent on facial depth information that have been accomplished in
range face images. Therefore, authors have considered two databases,
namely: Frav3D and Bosphorus, that contains laser as well as structured
light 3D scanner based procured 3D face images respectively.},
annote = {IEEE Region 10 Conference, Macao, PEOPLES R CHINA, NOV 01-04, 2015},
author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
booktitle = {TENCON 2015 - 2015 IEEE REGION 10 CONFERENCE},
isbn = {978-1-4799-8641-5},
issn = {2159-3442},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IPIM; CIUR; IEEE SINGAPORE SEC; CPSS; manetic; Melco Crown Entertainment; MACAO WATER; cadence ACADEMIC NETWORK; MuleSoft; cem; Wanlida; MANGO MESSAGING TECH; SEE; FDCT},
series = {TENCON IEEE Region 10 Conference Proceedings},
title = {{Decremental Depth Bunch Based 3D Face Recognition from Range Image}},
year = {2015}
}
@article{ISI:000385213200015,
abstract = {In the last decades, a lot of 3D face recognition techniques have been
proposed. They can be divided into three parts, holistic matching
techniques, feature-based techniques and hybrid techniques. In this
paper, a hybrid technique is used, where, a prototype of a new hybrid
face recognition technique depends on 3D face scan images are designed,
simulated and implemented. Some geometric rules are used for analyzing
and mapping the face. Image processing is used to get the
two-dimensional values of predetermined and specific facial points,
software programming is used to perform a three-dimensional coordinates
of the predetermined points and to calculate several geometric parameter
ratios and relations. Neural network technique is used for processing
the calculated geometric parameters and then performing facial
recognition. The new design is not affected by variant pose,
illumination and expression and has high accurate level compared with
the 2D analysis. Moreover, the proposed algorithm is of higher
performance than latest's published biometric recognition algorithms in
terms of cost, confidentiality of results, and availability of design
tools.},
author = {Issa, Haitham and Issa, Sali and Issa, Mohammad},
issn = {1683-3198},
journal = {INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {5},
pages = {590--594},
title = {{New Prototype of Hybrid 3D-Biometric Facial Recognition System}},
volume = {13},
year = {2016}
}
@article{ISI:000372791200003,
abstract = {The desire to reconstruct 3-D face models with expressions from 2-D face
images fosters increasing interest in addressing the problem of face
modeling. This task is important and challenging in the field of
computer animation. Facial contours and wrinkles are essential to
generate a face with a certain expression; however, these details are
generally ignored or are not seriously considered in previous studies on
face model reconstruction. Thus, we employ coupled radius basis function
networks to derive an intermediate 3-D face model from a single 2-D face
image. To optimize the 3-D face model further through landmarks, a
coupled dictionary that is related to 3-D face models and their
corresponding 3-D landmarks is learned from the given training set
through local coordinate coding. Another coupled dictionary is then
constructed to bridge the 2-D and 3-D landmarks for the transfer of
vertices on the face model. As a result, the final 3-D face can be
generated with the appropriate expression. In the testing phase, the 2-D
input faces are converted into 3-D models that display different
expressions. Experimental results indicate that the proposed approach to
facial expression synthesis can obtain model details more effectively
than previous methods can.},
author = {Liang, Haoran and Liang, Ronghua and Song, Mingli and He, Xiaofei},
doi = {10.1109/TCYB.2015.2417211},
issn = {2168-2267},
journal = {IEEE TRANSACTIONS ON CYBERNETICS},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {4},
pages = {890--901},
title = {{Coupled Dictionary Learning for the Detail-Enhanced Synthesis of 3-D Facial Expressions}},
volume = {46},
year = {2016}
}
@article{ISI:000442238900004,
abstract = {In this study, a framework for view-invariant gait recognition on the
basis of markerless motion tracking and dynamic time warping (DTW)
transform is presented. The system consists of a proposed markerless
motion capture system as well as introduced classification method of
mocap data. The markerless system estimates the three-dimensional
locations of skeleton driven joints. Such skeleton-driven point clouds
represent poses over time. The authors align point clouds in every pair
of frames by calculating the minimal sum of squared distances between
the corresponding joints. A point cloud distance measure with temporal
context has been utilised in k-nearest neighbours algorithm to compare
time instants of motion sequences. To enhance the generalisation of the
recognition and to shorten the processing time, for every individual a
single multidimensional time series among several multidimensional time
series describing the individual's gait is established. The correct
classification rate has been determined on the basis of a real dataset
of human gait. It contains 230 gait cycles of 22 subjects. The tracking
results on the basis of markerless motion capture are referenced to
Vicon system, whereas the achieved accuracies of recognition are
compared with the ones obtained by DTW that is based on rotational data.},
author = {Switonski, Adam and Krzeszowski, Tomasz and Josinski, Henryk and Kwolek, Bogdan and Wojciechowski, Konrad},
doi = {10.1049/iet-bmt.2017.0134},
issn = {2047-4938},
journal = {IET BIOMETRICS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {5},
pages = {415--422},
title = {{Gait recognition on the basis of markerless motion tracking and DTW transform}},
volume = {7},
year = {2018}
}
@article{ISI:000378042100004,
abstract = {The paper presents a face recognition system based on 3D features with
the aim of verifying the identity of subjects accessing a controlled
Ambient Intelligence Environment and customizing all the services
accordingly. The proposed approach relies on stereoscopic face
acquisition and 3D mesh reconstruction avoiding non-automated and
expensive 3D scanners, unsuitable for real time applications in general.
A bidimensional feature descriptor is extracted from each 3D mesh. It
consists in a color image transferring face's 3D features in a 2D space.
An automatic weighting mask of each authorized person improves the
robustness of recognition in presence of diverse facial expressions and
beard. The experiments conducted show high average recognition rate and
a measurable effectiveness of both flesh mask and expression weighting
mask.},
author = {Abate, Andrea F and Narducci, Fabio and Ricciardi, Stefano},
doi = {10.1478/AAPP.932A4},
issn = {0365-0359},
journal = {ATTI ACCADEMIA PELORITANA DEI PERICOLANTI-CLASSE DI SCIENZE FISICHE MATEMATICHE E NATURALI},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2},
title = {{BIOMETRICS EMPOWERED AMBIENT INTELLIGENCE ENVIRONMENT}},
volume = {93},
year = {2015}
}
@inproceedings{ISI:000428410700165,
abstract = {Real-world face recognition using a single sample per person (SSPP) is a
challenging task. The problem is exacerbated if the conditions under
which the gallery image and the probe set are captured are completely
different. To address these issues from the perspective of domain
adaptation, we introduce an SSPP domain adaptation network (SSPP-DAN).
In the proposed approach, domain adaptation, feature extraction, and
classification are performed jointly using a deep architecture with
domain-adversarial training. However, the SSPP characteristic of one
training sample per class is insufficient to train the deep
architecture. To overcome this shortage, we generate synthetic images
with varying poses using a 3D face model. Experimental evaluations using
a realistic SSPP dataset show that deep domain adaptation and image
synthesis complement each other and dramatically improve accuracy.
Experiments on a benchmark dataset using the proposed approach show
state-of-the-art performance.},
annote = {24th IEEE International Conference on Image Processing (ICIP), Beijing,
PEOPLES R CHINA, SEP 17-20, 2017},
author = {Hong, Sungeun and Im, Woobin and Ryu, Jongbin and Yang, Hyun S},
booktitle = {2017 24TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-5090-2175-8},
issn = {1522-4880},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers Signal Proc Soc},
pages = {825--829},
series = {IEEE International Conference on Image Processing ICIP},
title = {{SSPP-DAN: DEEP DOMAIN ADAPTATION NETWORK FOR FACE RECOGNITION WITH SINGLE SAMPLE PER PERSON}},
year = {2017}
}
@article{ISI:000413880800004,
abstract = {Previous studies have used principal component analysis (PCA) to
investigate the craniofacial relationship, as well as sex determination
using facial factors. However, few studies have investigated the extent
to which the choice of principal components (PCs) affects the analysis
of craniofacial relationship and sexual dimorphism. In this paper, we
propose a PCA-based method for visual and quantitative analysis, using
140 samples of 3D heads (70 male and 70 female), produced from computed
tomography (CT) images. There are two parts to the method. First, skull
and facial landmarks are manually marked to guide the model's
registration so that dense corresponding vertices occupy the same
relative position in every sample. Statistical shape spaces of the skull
and face in dense corresponding vertices are constructed using PCA.
Variations in these vertices, captured in every principal component
(PC), are visualized to observe shape variability. The correlations of
skull- and face-based PC scores are analysed, and linear regression is
used to fit the craniofacial relationship. We compute the PC
coefficients of a face based on this craniofacial relationship and the
PC scores of a skull, and apply the coefficients to estimate a 3D face
for the skull. To evaluate the accuracy of the computed craniofacial
relationship, the mean and standard deviation of every vertex between
the two models are computed, where these models are reconstructed using
real PC scores and coefficients. Second, each PC in facial space is
analysed for sex determination, for which support vector machines (SVMs)
are used. We examined the correlation between PCs and sex, and explored
the extent to which the choice of PCs affects the expression of sexual
dimorphism. Our results suggest that skull- and face-based PCs can be
used to describe the craniofacial relationship and that the accuracy of
the method can be improved by using an increased number of face-based
PCs. The results show that the accuracy of the sex classification is
related to the choice of PCs. The highest sex classification rate is
91.43{\%} using our method.},
author = {Shui, Wuyang and Zhou, Mingquan and Maddock, Steve and He, Taiping and Wang, Xingce and Deng, Qingqiong},
doi = {10.1016/j.compbiomed.2017.08.023},
issn = {0010-4825},
journal = {COMPUTERS IN BIOLOGY AND MEDICINE},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
pages = {33--49},
title = {{A PCA-Based method for determining craniofacial relationship and sexual dimorphism of facial shapes}},
volume = {90},
year = {2017}
}
@article{ISI:000397013700050,
abstract = {Identifying individual trees and delineating their canopy structures
from the forest point cloud data acquired by an airborne LiDAR (Light
Detection And Ranging) has significant implications in forestry
inventory. Once accurately identified, tree structural attributes such
as tree height, crown diameter, canopy based height and diameter at
breast height can be derived. This paper focuses on a novel
computationally efficient method to adaptively calibrate the kernel
bandwidth of a computational scheme based on mean shift-a non-parametric
probability density-based clustering technique-to segment the 3D
(three-dimensional) forest point clouds and identify individual tree
crowns. The basic concept of this method is to partition the 3D space
over each test plot into small vertical units (irregular columns
containing 3D spatial features from one or more trees) first, by using a
fixed bandwidth mean shift procedure and a small square grouping
technique, and then rough estimation of crown sizes for distinct trees
within a unit, based on an original 2D (two-dimensional) incremental
grid projection technique, is applied to provide a basis for dynamical
calibration of the kernel bandwidth for an adaptive mean shift procedure
performed in each partition. The adaptive mean shift-based scheme, which
incorporates our proposed bandwidth calibration method, is validated on
10 test plots of a dense, multi-layered evergreen broad-leaved forest
located in South China. Experimental results reveal that this approach
can work effectively and when compared to the conventional point-based
approaches (e.g., region growing, k-means clustering, fixed bandwidth or
multi-scale mean shift), its accuracies are relatively high: it detects
86 percent of the trees ({\{}''{\}}recall{\{}''{\}}) and 92 percent of the
identified trees are correct ({\{}''{\}}precision{\{}''{\}}), showing good potential
for use in the area of forest inventory.},
author = {Hu, Xingbo and Chen, Wei and Xu, Weiyang},
doi = {10.3390/rs9020148},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2},
title = {{Adaptive Mean Shift-Based Identification of Individual Trees Using Airborne LiDAR Data}},
volume = {9},
year = {2017}
}
@article{ISI:000436148300003,
abstract = {In this paper, we propose a bimodal 3D facial recognition method aimed
at increasing the recognition rate and reducing the effect of
illumination, pose, expression, ages, and occlusion on facial
recognition. There are two features extracted from the multiscale
sub-blocks in both the 3D mode depth map and 2D mode intensity map,
which are the local gradient pattern (LGP) feature and the weighted
histogram of gradient orientation (WHGO) feature. LGP and WHGO features
are cascaded to form the 3D facial feature vector LGP-WHGO, and are
further trained and identified by the support vector machine (SVM).
Experiments on the CASIA database, FRGC v2.0 database, and Bosphorus
database show that, the proposed method can efficiently extract the
structure information and texture information of the facial image, and
have a robustness to illumination, expression, occlusion and pose.},
author = {Guo, Yingchun and Wei, Ruoyu and Liu, Yi},
doi = {10.3390/info9030048},
issn = {2078-2489},
journal = {INFORMATION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
number = {3},
title = {{Weighted Gradient Feature Extraction Based on Multiscale Sub-Blocks for 3D Facial Recognition in Bimodal Images}},
volume = {9},
year = {2018}
}
@article{ISI:000362485300007,
abstract = {Efficient registration across pose is the most challenging research area
for accurate recognition of human face images. Authors have discussed a
tool that has been developed for registration of face images across
poses using the nose tip of the face images. The nose tip has been
considered here because of its stability in situations such as
variations in pose, expression, etc. The aim of this investigation is to
develop a face registration tool called ``Register-My-Face{\{}''{\}} with a
working methodology in all the three directions, namely yaw, pitch, and
roll. This tool has been developed for three-dimensional (3-D) face
registration, which is inspired by analyzing the ``depth values{\{}''{\}} of
face range images. The registration of the face is done using a
geometrical technique which is based on computing the corresponding
rotation in three orthogonal directions. The advantages of the designed
tool are that it does not need any training phase for accurate detection
of the nose tip, and this method can handle large pose variations,
including 90 deg pose variations about the Y-axis in both the positive
and negative directions. The method that has been followed to develop
this tool is also independent of facial expression, occlusion and
illumination variations. Moreover, it quickly detects the nose tip
because it does not need to process the entire face surface, but only
requires the isolated nose region. The tool has been integrated with
three different databases; GavabDB, Bosphorus, and Frav3D, and the
investigation highlights the robustness of the tool. Additionally, for
exploring the performance of the tool, a SIMULINK model for hardware
interface is also developed with a discrete solver and is tested on two
different configuration setups and executed in two different execution
modes with two simulation stop timings 10.0 and 1.0. This model can
proceed according to the algorithm with a minimum of 6.640 s to register
an unregistered raw 3-D face scan input image from the Frav3D database.
Accuracies of the nose region of 98.87{\%}, 94.44{\%}, and 98.08{\%} for the
Frav3D, GavabDB, and Bosphorus databases, respectively, are observed.
For nose tip detection, the success rates are 98.91{\%} for the Frav3D
database, 98.74{\%} for the GavabDB database, and 96.03{\%} for the
Bosphorus database. Based on the success rate of nose tip detection, the
registration process is implemented on three databases. Registration
accuracy, computed between a neutral and the registered range face image
for the Frav3D database is 87.5{\%} for GavabDB and 89.87{\%} for Bosphorus
database, and the rate of success is 70.23{\%}. (C) 2015 SPIE and IS{\&}T},
author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
doi = {10.1117/1.JEI.24.4.043007},
issn = {1017-9909},
journal = {JOURNAL OF ELECTRONIC IMAGING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
number = {4},
title = {{Register-My-Face: a tool to register three-dimensional face images}},
volume = {24},
year = {2015}
}
@inproceedings{ISI:000444905600019,
abstract = {In this paper, we present an efficient method for 3D face recognition
based on vector quantization of both geometrical and visual proprieties
of the face. The method starts by describing each 3D face using a set of
orderless features, and use then the Bag-of-Features paradigm to
construct the face signature. We analyze the performance of three
well-known classifiers: the Naive Bayes, the Multilayer perceptron and
the Random forests. The results reported on the FRGCv2 dataset show the
effectiveness of our approach and prove that the method is robust to
facial expression.},
annote = {12th International Joint Conference on Computer Vision, Imaging and
Computer Graphics Theory and Applications (VISIGRAPP), Porto, PORTUGAL,
FEB 27-MAR 01, 2017},
author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Declercq, David and Benouareth, Abdallah},
booktitle = {PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5},
doi = {10.5220/0006101701870193},
editor = {{Imai, F and Tremeau, A and Braz, J}},
isbn = {978-989-758-226-4},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {Inst Syst {\&} Technologies Informat, Control {\&} Commun; ACM SIGGRAPH; AFIG; Eurographics},
pages = {187--193},
title = {{Geometrical and Visual Feature Quantization for 3D Face Recognition}},
year = {2017}
}
@article{7194796,
abstract = {In this paper, we investigate a single-sample periocular-based alignment-robust face recognition technique that is pose-tolerant under unconstrained face matching scenarios. Our Spartans framework starts by utilizing one single sample per subject class, and generate new face images under a wide range of 3D rotations using the 3D generic elastic model which is both accurate and computationally economic. Then, we focus on the periocular region where the most stable and discriminant features on human faces are retained, and marginalize out the regions beyond the periocular region since they are more susceptible to expression variations and occlusions. A novel facial descriptor, high-dimensional Walsh local binary patterns, is uniformly sampled on facial images with robustness toward alignment. During the learning stage, subject-dependent advanced correlation filters are learned for pose-tolerant non-linear subspace modeling in kernel feature space followed by a coupled max-pooling mechanism which further improve the performance. Given any unconstrained unseen face image, the Spartans can produce a highly discriminative matching score, thus achieving high verification rate. We have evaluated our method on the challenging Labeled Faces in the Wild database and solidly outperformed the state-of-the-art algorithms under four evaluation protocols with a high accuracy of 89.69{\%}, a top score among image-restricted and unsupervised protocols. The advancement of Spartans is also proven in the Face Recognition Grand Challenge and Multi-PIE databases. In addition, our learning method based on advanced correlation filters is much more effective, in terms of learning subject-dependent pose-tolerant subspaces, compared with many well-established subspace methods in both linear and non-linear cases.},
annote = {17/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
17/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Juefei-Xu, Felix and Luu, Khoa and Savvides, Marios},
doi = {10.1109/TIP.2015.2468173},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Periocular-based recognition,advanced correlation filters,alignment robustness,etapa1,gil,id63,ieeexplore,pose tolerance,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,singlesample 3D face synthesis,unconstrained face recognition},
mendeley-tags = {etapa1,gil,id63,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {dec},
number = {12},
pages = {4780--4795},
title = {{Spartans: Single-sample periocular-based alignment-robust recognition technique applied to non-frontal scenarios}},
url = {http://ieeexplore.ieee.org/document/7194796/},
volume = {24},
year = {2015}
}
@article{ISI:000427009100019,
abstract = {This paper presents a 3-D object recognition method and its
implementation on a robotic navigation aid to allow real-time detection
of indoor structural objects for the navigation of a blind person. The
method segments a point cloud into numerous planar patches and extracts
their inter-plane relationships (IPRs). Based on the existing IPRs of
the object models, the method defines six high level features (HLFs) and
determines the HLFs for each patch. A Gaussian-mixture-model-based plane
classifier is then devised to classify each planar patch into one
belonging to a particular object model. Finally, a recursive plane
clustering procedure is used to cluster the classified planes into the
model objects. As the proposed method uses geometric context to detect
an object, it is robust to the object's visual appearance change. As a
result, it is ideal for detecting structural objects (e.g., stairways,
doorways, and so on). In addition, it has high scalability and
parallelism. The method is also capable of detecting some indoor
nonstructural objects. Experimental results demonstrate that the
proposed method has a high success rate in object recognition.},
author = {Ye, Cang and Qian, Xiangfei},
doi = {10.1109/TNSRE.2017.2748419},
issn = {1534-4320},
journal = {IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {2},
pages = {441--450},
title = {{3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired}},
volume = {26},
year = {2018}
}
@inproceedings{ISI:000425239601072,
abstract = {We show how a simple convolutional neural network (CNN) can be trained
to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head
pose, directly from image intensities. We further explain how this
FacePoseNet (FPN) can be used to align faces in 2D and 3D as an
alternative to explicit facial landmark detection for these tasks. We
claim that in many cases the standard means of measuring landmark
detector accuracy can be misleading when comparing different face
alignments. Instead, we compare our FPN with existing methods by
evaluating how they affect face recognition accuracy on the IJB-A and
IJB-B benchmarks: using the same recognition pipeline, but varying the
face alignment method. Our results show that (a) better landmark
detection accuracy measured on the 300W benchmark does not necessarily
imply better face recognition accuracy. (b) Our FPN provides superior 2D
and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces
at a small fraction of the computational cost of comparably accurate
landmark detectors. For many purposes, FPN is thus a far faster and far
more accurate face alignment method than using facial landmark
detectors.},
annote = {16th IEEE International Conference on Computer Vision (ICCV), Venice,
ITALY, OCT 22-29, 2017},
author = {Chang, Feng-Ju and Tran, Anh Tuan and Hassner, Tal and Masi, Iacopo and Nevatia, Ram and Medioni, Gerard},
booktitle = {2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW 2017)},
doi = {10.1109/ICCVW.2017.188},
isbn = {978-1-5386-1034-3},
issn = {2473-9936},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IEEE Comp Soc},
pages = {1599--1608},
series = {IEEE International Conference on Computer Vision Workshops},
title = {{FacePoseNet: Making a Case for Landmark-Free Face Alignment}},
year = {2017}
}
@inproceedings{ISI:000400688200019,
abstract = {Landmarks are unique points that can be located on every face. Facial
landmarks typically recognized by people are correlated with
anthropomorphic points. Our purpose is to employ in 3D face recognition
such landmarks that are easy to interpret. Face understanding is
construed as identification of face characteristic points with automatic
labeling of them. In this paper, we apply methods based on Self
Organizing Maps to understand 3D faces.},
annote = {15th International Conference on Artificial Intelligence and Soft
Computing (ICAISC), Zakopane, POLAND, JUN 12-16, 2016},
author = {Starczewski, Janusz T and Pabiasz, Sebastian and Vladymyrska, Natalia and Marvuglia, Antonino and Napoli, Christian and Wozniak, Marcin},
booktitle = {ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING, (ICAISC 2016), PT II},
doi = {10.1007/978-3-319-39384-1_19},
editor = {{Rutkowski, L and Korytkowski, M and Scherer, R and Tadeusiewicz, R and Zadeh, LA and Zurada, JM}},
isbn = {978-3-319-39384-1},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Polish Neural Network Soc; Univ Social Sci; Czestochowa Univ Technol, Inst Computat Intelligence},
pages = {210--217},
series = {Lecture Notes in Artificial Intelligence},
title = {{Self Organizing Maps for 3D Face Understanding}},
volume = {9693},
year = {2016}
}
@article{ISI:000413881700008,
abstract = {This paper proposes a novel 3D face recognition method using the local
covariance descriptor and Riemannian kernel sparse coding in order to
accurately evaluate the intrinsic correlation of the extracted features
and further improve the 3D face recognition accuracy. Firstly, the
keypoints are detected by the farthest point sampling method, and the
corresponding keypoint neighborhood is extracted by the specified radius
associated with geodesic distance. Then, different types of the
efficient features are selected to construct the local covariance
descriptor with inherent property. Finally, the appropriate Riemannian
kernel sparse coding is used to identify the faces in probe.
Experimental evaluation has been performed on two challenging 3D face
datasets, FRGC v2.0 and Bosphorus, which indicates that the proposed
approach can significantly improve the identification accuracy comparing
with other state-of-the-art methods. (C) 2017 Elsevier Ltd. All rights
reserved.},
author = {Deng, Xing and Da, Feipeng and Shao, Haijian},
doi = {10.1016/j.compeleceng.2017.01.028},
issn = {0045-7906},
journal = {COMPUTERS {\&} ELECTRICAL ENGINEERING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {81--91},
title = {{Efficient 3D face recognition using local covariance descriptor and Riemannian kernel sparse coding}},
volume = {62},
year = {2017}
}
@article{ISI:000372355200007,
abstract = {In this paper, we present a novel approach for fusing shape and texture
local binary patterns (LBPs) on a mesh for 3D face recognition. Using a
recently proposed framework, we compute LBP directly on the face mesh
surface, then we construct a grid of the regions on the facial surface
that can accommodate global and partial descriptions. Compared with its
depth-image counterpart, our approach is distinguished by the following
features: 1) inherits the intrinsic advantages of mesh surface (e.g.,
preservation of the full geometry); 2) does not require normalization;
and 3) can accommodate partial matching. In addition, it allows early
level fusion of texture and shape modalities. Through experiments
conducted on the BU-3DFE and Bosphorus databases, we assess different
variants of our approach with regard to facial expressions and missing
data, also in comparison to the state-of-the-art solutions.},
author = {Werghi, Naoufel and Tortorici, Claudio and Berretti, Stefano and {Del Bimbo}, Alberto},
doi = {10.1109/TIFS.2016.2515505},
issn = {1556-6013},
journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {5},
pages = {964--979},
title = {{Boosting 3D LBP-Based Face Recognition by Fusing Shape and Texture Descriptors on the Mesh}},
volume = {11},
year = {2016}
}
@inproceedings{ISI:000434996800035,
abstract = {We investigated how different appearances in the favorable impressions
of 3D avatar faces affect face-recognition performances by humans. We
conducted an encoding and testing experiment using synthesized facial
images and artificially manipulated the strength of the perceived
impressions in three different dimensions. We also subjectively assessed
the favorability of the synthesized faces that were used as visual
stimuli in face-recognition tests and found that facial transformation,
which decreased the favorability impressions, generally deteriorates
human face-recognition performance.},
annote = {International Workshop on Advanced Image Technology (IWAIT), Chiang Mai,
THAILAND, JAN 07-09, 2018},
author = {Hada, Momoko and Yamada, Ryoko and Akamatsu, Shigeru},
booktitle = {2018 INTERNATIONAL WORKSHOP ON ADVANCED IMAGE TECHNOLOGY (IWAIT)},
isbn = {978-1-5386-2615-3},
issn = {2306-2274},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
series = {Proceedings of International Workshop on Advanced Image Technology},
title = {{How does the transformation of an avatar face giving a favorable impression affect human recognition of the face?}},
year = {2018}
}
@article{ISI:000434085600046,
abstract = {The computer graphics and vision communities have dedicated long
standing efforts in building computerized tools for reconstructing,
tracking, and analyzing human faces based on visual input. Over the past
years rapid progress has been made, which led to novel and powerful
algorithms that obtain impressive results even in the very challenging
case of reconstruction from a single RGB or RGB-D camera. The range of
applications is vast and steadily growing as these technologies are
further improving in speed, accuracy, and ease of use. Motivated by this
rapid progress, this state-of-the-art report summarizes recent trends in
monocular facial performance capture and discusses its applications,
which range from performance-based animation to real-time facial
reenactment. We focus our discussion on methods where the central task
is to recover and track a three dimensional model of the human face
using optimization-based reconstruction algorithms. We provide an
in-depth overview of the underlying concepts of real-world image
formation, and we discuss common assumptions and simplifications that
make these algorithms practical. In addition, we extensively cover the
priors that are used to better constrain the under-constrained monocular
reconstruction problem, and discuss the optimization techniques that are
employed to recover dense, photo-geometric 3D face models from monocular
2D data. Finally, we discuss a variety of use cases for the reviewed
algorithms in the context of motion capture, facial animation, as well
as image and video editing.},
annote = {39th Annual Conference of the European-Association-for-Computer-Graphics
(EUROGRAPHICS), Delft, NETHERLANDS, APR 16-20, 2018},
author = {Zollhoefer, M and Thies, J and Garrido, P and Bradley, D and Beeler, T and Perez, P and Stamminger, M and Niessner, M and Theobalt, C},
doi = {10.1111/cgf.13382},
institution = {European Assoc Comp Graph; TU Delft; Tomtom; Adobe; Disney Res; StyleShoots; Activision; KAUST; Vrvis},
issn = {0167-7055},
journal = {COMPUTER GRAPHICS FORUM},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2},
pages = {523--550},
title = {{State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications}},
volume = {37},
year = {2018}
}
@inproceedings{ISI:000359471600007,
abstract = {Deep Neural Networks (DNNs) have established themselves as a dominant
technique in machine learning. DNNs have been top performers on a wide
variety of tasks including image classification, speech recognition, and
face recognition.(1-3) Convolutional neural networks (CNNs) have been
used in nearly all of the top performing methods on the Labeled Faces in
the Wild (LFW) dataset.(3-6) In this talk and accompanying paper, I
attempt to provide a review and summary of the deep learning techniques
used in the state-of-the-art. In addition, I highlight the need for both
larger and more challenging public datasets to benchmark these systems.
Despite the ability of DNNs and autoencoders to perform unsupervised
feature learning, modern facial recognition pipelines still require
domain specific engineering in the form of re-alignment. For example, in
Facebook's recent Deep Face paper, a 3D ``frontalization{\{}''{\}} step lies
at the beginning of the pipeline. This step creates a 3D face model for
the incoming image and then uses a series of affine transformations of
the fiducial points to ``frontalize{\{}''{\}} the image. This step enables the
Deep Face system to use a neural network architecture with locally
connected layers without weight sharing as opposed to standard
convolutional layers.(6) Deep learning techniques combined with large
datasets have allowed research groups to surpass human level performance
on the LFW dataset.(3,5)
The high accuracy (99.63{\%} for Face Net at the time of publishing) and
utilization of outside data (hundreds of millions of images in the case
of Google's Face Net) suggest that current face verification benchmarks
such as LFW may not be challenging enough, nor provide enough data, for
current techniques.(3,5) There exist a variety of organizations with
mobile photo sharing applications that would be capable of releasing a
very large scale and highly diverse dataset of facial images captured on
mobile devices. Such an ``Image Net for Face Recognition{\{}''{\}} would
likely receive a warm welcome from researchers and practitioners alike.},
annote = {Conference on Biometric and Surveillance Technology for Human and
Activity Identification XII, Baltimore, MD, APR 22, 2015},
author = {Balaban, Stephen},
booktitle = {BIOMETRIC AND SURVEILLANCE TECHNOLOGY FOR HUMAN AND ACTIVITY IDENTIFICATION XII},
doi = {10.1117/12.2181526},
editor = {{Kakadiaris, IA and Kumar, A and Scheirer, WJ}},
isbn = {978-1-62841-573-5},
issn = {0277-786X},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Deep learning and face recognition: the state of the art}},
volume = {9457},
year = {2015}
}
@article{Moeini20161,
abstract = {In this paper, a novel feature extraction method is proposed for facial expression recognition by extracting the feature from facial depth and 3D mesh alongside texture. Accordingly, the 3D Facial Expression Generic Elastic Model (3D FE-GEM) method is used to reconstruct an expression-invariant 3D model from the human face. Then, the texture, depth and mesh are extracted from the reconstructed face model. Afterwards, the Local Binary Pattern (LBP), proposed 3D High-Low Local Binary Pattern (3DH-LLBP) and Local Normal Binary Patterns (LNBPs) are applied to texture, depth and mesh of the face, respectively, to extract the feature from 2D images. Finally, the final feature vectors are generated through feature fusion and are classified by the Support Vector Machine (SVM). Convincing results are acquired for facial expression recognition on the CK+, CK, JAFFE and Bosphorus image databases compared to several state-of-the-art methods. {\textcopyright} 2015 Elsevier Inc. All rights reserved.},
annote = {cited By 4},
author = {Moeini, A and Faez, K and Sadeghi, H and Moeini, H},
doi = {10.1016/j.jvcir.2015.11.006},
journal = {Journal of Visual Communication and Image Representation},
keywords = {gil,isi,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {gil,isi,revisao{\_}scopus,revisao{\_}webofscience},
pages = {1--14},
title = {{2D facial expression recognition via 3D reconstruction and feature fusion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950162274{\&}doi=10.1016{\%}2Fj.jvcir.2015.11.006{\&}partnerID=40{\&}md5=cc175747fc6e3ed4c7eae6a795bad1cb},
volume = {35},
year = {2016}
}
@inproceedings{ISI:000457719100010,
abstract = {We present a system for accurate real-time 3D face verification using a
low-quality consumer depth camera. To verify the identity of a subject,
we built a high-quality reference model offline by fitting a 3D
morphable model to a sequence of low-quality depth images. At runtime,
we compare the similarity between the reference model and a single depth
image by aligning the model to the image and measuring differences
between every point on the two facial surfaces. The model and the image
will not match exactly due to sensor noise, occlusions, as well as
changes in expression, hairstyle, and eye-wear; therefore, we leverage a
data driven approach to determine whether or not the model and the image
match. We train a random decision forest to verify the identity of a
subject where the point-to-point distances between the reference model
and the depth image are used as input features to the classifier. Our
approach runs in real-time and is designed to continuously authenticate
a user as he/she uses his/her device. In addition, our proposed method
outperforms existing 2D and 3D face verification methods on a benchmark
data set.},
annote = {15th Conference on Computer and Robot Vision (CRV), Toronto, CANADA, MAY
08-11, 2018},
author = {Meyer, Gregory P and Do, Minh N},
booktitle = {2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV)},
doi = {10.1109/CRV.2018.00020},
isbn = {978-1-5386-6481-0},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Canadian Image Proc {\&} Pattern Recognit Soc; Assoc Canadienne Traitement Images Reconnaissance Formes; MDA; Modiface; NextAI; SPORTLOGiQ; StradigiAI; York Univ Vis Sci Applicat Program; York Univ Ctr Vis Res; ElementAI; EPSON; Miovision; Trans Plan},
pages = {71--79},
title = {{Real-time 3D Face Verification with a Consumer Depth Camera}},
year = {2018}
}
@inproceedings{ISI:000380388000083,
abstract = {The problem of fitting a 3D facial model to a 3D mesh has received a lot
of attention the past 15-20 years. The majority of the techniques fit a
general model consisting of a simple parameterisable surface or a mean
3D facial shape. The drawback of this approach is that is rather
difficult to describe the non-rigid aspect of the face using just a
single facial model. One way to capture the 3D facial deformations is by
means of a statistical 3D model of the face or its parts. This is
particularly evident when we want to capture the deformations of the
mouth region. Even though statistical models of face are generally
applied for modelling facial intensity, there are few approaches that
fit a statistical model of 3D faces. In this paper, in order to capture
and describe the non-rigid nature of facial surfaces we build a
part-based statistical model of the 3D facial surface and we combine it
with non-rigid iterative closest point algorithms. We show that the
proposed algorithm largely outperforms state-of-the-art algorithms for
3D face fitting and alignment especially when it comes to the
description of the mouth region.},
annote = {IEEE 11th International Conference and Workshops on Automatic Face and
Gesture Recognition (FG), Ljubljana, SLOVENIA, MAY 04-08, 2015},
author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic, Maja},
booktitle = {2015 11TH IEEE INTERNATIONAL CONFERENCE AND WORKSHOPS ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG), VOL. 4},
isbn = {978-1-4799-6026-2},
issn = {2326-5396},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; IEEE Comp Soc; IEEE Biometric Council},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{Active Nonrigid ICP Algorithm}},
year = {2015}
}
@inproceedings{ISI:000380792100051,
abstract = {Face and facial attributes represent meaningful definition about a
variety of information to discriminate an individual from others and for
developing a computational model for automatic face recognition purpose.
However, in this work, selection of relevant features from newly created
face space is the pivotal contribution of the authors. Here, authors
have demonstrated a new face space `Complement Component' that have been
used to extract the four basic components along X, and Y axes in four
directions. Later, authors have experimented the discriminative
attributes from these face spaces for recognition purpose. Here,
comparison of the proposed method has been reported by examining its
success on two well accepted 3D face databases, namely: Frav3D and
Texas3D. In case of 2D face images, it does not contain depth like
information i.e. Z-values in X-Y plane through intensity values.
Therefore, it has not been undertaken during this investigation.},
annote = {IEEE International Conference on Computer Graphics, Vision and
Information Security (CGVIS), KIIT Univ, Bhubaneswar, INDIA, NOV 02-03,
2015},
author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
booktitle = {2015 IEEE International Conference on Computer Graphics, Vision and Information Security (CGVIS)},
isbn = {978-1-4673-7437-8},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE Kolkata Sect; IEEE KIIT Student Branch; IEEE; Webprit Solut},
pages = {275--278},
title = {{3D Face Recognition from Complement Component Range Face Images}},
year = {2015}
}
@article{ISI:000464920200012,
abstract = {A novel weighted hybrid classifier and a high-order, local normal
derivative pattern descriptor are proposed for 3D face recognition. The
local derivative pattern (LDP) captures the detailed information based
on the local derivative variation in different directions. The LDP is
computed on three normal maps in x-, y-, and z-directions and on
different scales. The surface normal captures the orientation of a
surface at each point of 3D data. More informative local shape
information is extracted using the surface normal, as compared to depth.
The nth-order LDP on the surface normal is proposed to encode the more
detailed features from the (n-1)th-order's local derivative direction
variations. An extreme learning machine (ELM)-based autoencoder, using a
multilayer network structure, is employed to select more discriminant
features and to provide a faster training speed. A weighted hybrid
framework is proposed to handle facial challenges using a combination of
the ELM and the sparse representation classifier (SRC). The advantage of
speed for the ELM and the accuracy for the SRC in a weighted scheme is
used to enhance the performance of the recognition system. Experimental
results regarding four famous 3D face databases illustrate the
generalization and effectiveness of the proposed method in terms of both
computational cost and recognition accuracy.},
author = {Soltanpour, Sima and Wu, Qing Ming Jonathan},
doi = {10.1109/TIP.2019.2893524},
issn = {1057-7149},
journal = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {jun},
number = {6},
pages = {3020--3033},
title = {{Weighted Extreme Sparse Classifier and Local Derivative Pattern for 3D Face Recognition}},
volume = {28},
year = {2019}
}
@article{ISI:000357910700010,
abstract = {The research on 3D facial expression recognition has attracted numbers
of interests due to its superiority to 2D data and it has been greatly
promoted in recent years. However, its performance needs to be further
improved and its data structure needs to be further analyzed to keep its
automation well as the mesh structure of 3D face models cannot be
applied directly to algebraic operations. This paper addresses these
problems with multiple strategies, so that 3D facial expression
recognition can be automatically implemented and its performance is
subsequently enhanced. Firstly, an image-like-structure is proposed to
represent the 3D face models, so that algebraic operations can be
directly applied to analyze 3D data. Based on this image-like-structure,
the strategies of irregular division schemes and the entropy weighted
blocks are employed to improve the recognition accuracy. The former aims
to keep the integrity of local structure; the latter is employed to
emphasize the contribution of different facial regions. Both of them can
be separately or jointly, utilized to facial feature descriptors. With
the remarkable experimental results based on LBP and LIP, we can
conclude that these strategies are available to promote the performance
of automatic 3D facial expression recognition, which draws a promising
direction for automatic 3D facial expression recognition. (C) 2015
Elsevier B.V. All rights reserved.},
author = {Li, Xiaoli and Ruan, Qiuqi and An, Gaoyun and Jin, Yi and Zhao, Ruizhen},
doi = {10.1016/j.neucom.2015.02.063},
issn = {0925-2312},
journal = {NEUROCOMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {89--98},
title = {{Multiple strategies to enhance automatic 3D facial expression recognition}},
volume = {161},
year = {2015}
}
@inproceedings{ISI:000377348700061,
abstract = {As the most distinct feature point in facial landmarks, nose tip plays a
significant role in 3D facial studies. Successful detection of nose tip
can facilitate many 3D facial studies tasks. In this paper, we propose a
novel method to detect nose tip robustly. The method is robust to noise,
need not training, can handle large rotations and occlusions. We first
remove small isolated connected regions and noise from the input range
image, then establish scale-space by robust smoothing the preprocessed
range image. In each scale of the scale-space, we compute multi-angle
energy of each point, then we use hierarchical clustering method to
cluster the points whose multi-angle energies are larger than a
threshold value. In the largest cluster, we can find one point with the
largest multi-angle energy. For all scales of the scale-space, we get a
series of such points and apply hierarchical clustering again for these
points, nose tip will have the largest multi-angle energy in the largest
cluster. We evaluate our method in FRGC v2.0 3D face database and
BOSPHORUS 3D face database. The experimental results verify the
robustness of our method with a high nose tip detection rate.},
annote = {IEEE Advanced Information Technology, Electronic and Automation Control
Conference (IAEAC), Chongqing, PEOPLES R CHINA, DEC 19-20, 2015},
author = {Liu, Jian and Zhang, Quan and Tang, Chaojing},
booktitle = {2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC)},
isbn = {978-1-4799-1980-2},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IEEE Beijing Sect; Global Union Acad Sci {\&} Technol; Chongqing Global Union Acad Sci {\&} Technol},
pages = {309--315},
title = {{CoMES: A Novel Method for Robust Nose Tip Detection in Face Range Images}},
year = {2015}
}
@inproceedings{ISI:000377335600061,
abstract = {Autism Spectrum Disorder (ASD) impairs an individual's non-verbal skills
including natural and contextual facial expressions. Such impairments
may manifest as odd facial expressions (facial oddity) based on
subjective evaluations of facial images. A few studies conducted on
individuals with ASD have focused on the physiology of facial muscle
usage by employing eletrophysiological sensors in response to visual
stimuli. The sensors are placed directly on the face and may inhibit or
limit the spontaneous facial response which may be too subtle for
subjective human evaluations. This study uses a non-intrusive 3D facial
imaging sensor that captures detailed geometric information of the face
to facilitate quantification and detection of subtle changes in facial
expression based on the physiology of facial muscle. A novel computer
vision and data mining approach is developed from curve-based geometric
feature of 3D facial data to discern the changes in the facial muscle
actions. A pilot study is conducted with sixteen subjects (8 subjects
with ASD and 8 typically-developing controls) where 3D facial images
have been captured in response to visual stimuli involving 3D facial
expressions. Statistical analyses reveal a significantly asymmetric
facial muscle action in subjects with ASD compared to the
typically-developing controls. This study demonstrates feasibility of
using non-intrusive facial imaging sensor data in evaluating possible
physiology-based impairments.},
annote = {IEEE International Conference on Bioinformatics and Biomedicine,
Washington, DC, NOV 09-12, 2015},
author = {Samad, Manar D and Bobzien, Jonna L and Harrington, John W and Iftekharuddin, Khan M},
booktitle = {PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIOINFORMATICS AND BIOMEDICINE},
editor = {{Huan, J and Miyano, S and Shehu, A and Hu, X and Ma, B and Rajasekaran, S and Gombar, VK and Schapranow, IM and Yoo, IH and Zhou, JY and Chen, B and Pai, V and Pierce, B}},
isbn = {978-1-4673-6798-1},
issn = {2156-1125},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; IEEE Comp Soc; Natl Sci Fdn},
pages = {337--342},
series = {IEEE International Conference on Bioinformatics and Biomedicine-BIBM},
title = {{Analysis of Facial Muscle Activation in Children with Autism using 3D Imaging}},
year = {2015}
}
@inproceedings{ISI:000381427400036,
abstract = {Human face recognition based on geometrical structure has been an area
of interest among researchers for the past few decades especially in
pattern recognition. 3D Face recognition systems are of interest in this
context. The main advantage of 3D Face recognition is the availability
of geometrical information of the face structure which is more or less
unique for a subject. This paper focuses on the problems of person
identification using 3D Face data. Use of unregistered 3D Face data for
feature extraction significantly increases the operational speed of the
system with huge database enrollment. In this work, unregistered Face
data, i.e. both texture and depth is fed to a classifier in spectral
representations of the same data. 2-D Discrete Contourlet Transform and
2-D Discrete Fourier Transform is used here for the spectral
representation which forms the feature matrix. Fusion of texture and
depth statistical information of face is proposed in this paper since
the individual schemes are of lower performance. Application of
statistical method seems to degrade the performance of the system when
applied to texture data and was effective in the case of depth data.
Fusion of the matching scores proves that the recognition accuracy can
be improved significantly by fusion of scores of multiple
representations. FRAV3D database is used for testing the algorithm.},
annote = {International Symposium on Intelligent Systems Technologies and
Applications (ISTA), SCMS Grp Inst, SCMS Sch Engn {\&} Technol, Kochi,
INDIA, AUG 10-13, 2015},
author = {Naveen, S and Moni, R S},
booktitle = {INTELLIGENT SYSTEMS TECHNOLOGIES AND APPLICATIONS, VOL 1},
doi = {10.1007/978-3-319-23036-8_36},
editor = {{Berretti, S and Thampi, SM and Srivastava, PR}},
isbn = {978-3-319-23036-8; 978-3-319-23035-1},
issn = {2194-5357},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {411--425},
series = {Advances in Intelligent Systems and Computing},
title = {{Contourlet and Fourier Transform Features Based 3D Face Recognition System}},
volume = {384},
year = {2016}
}
@inproceedings{ISI:000390841200018,
abstract = {In this paper, we present a large-scale database consisting of low cost
Kinect 3D face videos, namely Lock3DFace, for 3D face analysis,
particularly for 3D Face Recognition (FR). To the best of our knowledge,
Lock3DFace is currently the largest low cost 3D face database for public
academic use. The 3D samples are highly noisy and contain a diversity of
variations in expression, pose, occlusion, time lapse, and their
corresponding texture and near infrared channels have changes in
lighting condition and radiation intensity, allowing for evaluating FR
methods in complex situations. Furthermore, based on Lock3DFace, we
design the standard experimental protocol for low-cost 3D FR, and give
the baseline performance of individual subsets belonging to different
scenarios for fair comparison in the future.},
annote = {9th International Conference on Biometrics (ICB), Halmstad Univ,
Halmstad, SWEDEN, JUN 13-16, 2016},
author = {Zhang, Jinjin and Huang, Di and Wang, Yunhong and Sun, Jia},
booktitle = {2016 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)},
editor = {{Fierrez, J and Li, SZ and Ross, A and Veldhuis, R and AlonsoFernandez, F and Bigun, J}},
isbn = {978-1-5090-1869-7},
issn = {2376-4201},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IAPR Tech Comm 4 Biometr; IEEE Biometr Council; Fingerprint Cards AB; Safran Ident {\&} Secur; EU Horizon 2020 Project IDENT; Speed Ident AB; Cognitec},
series = {International Conference on Biometrics},
title = {{Lock3DFace: A Large-Scale Database of Low-Cost Kinect 3D Faces}},
year = {2016}
}
@article{ISI:000389208600010,
abstract = {The paper proposes a novel framework for 3D face verification using
dimensionality reduction based on highly distinctive local features in
the presence of illumination and expression variations. The histograms
of efficient local descriptors are used to represent distinctively the
facial images. For this purpose, different local descriptors are
evaluated, Local Binary Patterns (LBP), Three-Patch Local Binary
Patterns (TPLBP), Four-Patch Local Binary Patterns (FPLBP), Binarized
Statistical Image Features (BSIF) and Local Phase Quantization (LPQ).
Furthermore, experiments on the combinations of the four local
descriptors at feature level using simply histograms concatenation are
provided. The performance of the proposed approach is evaluated with
different dimensionality reduction algorithms: Principal Component
Analysis (PCA), Orthogonal Locality Preserving Projection (OLPP) and the
combined PCA+EFM (Enhanced Fisher linear discriminate Model). Finally,
multi-class Support Vector Machine (SVM) is used as a classifier to
carry out the verification between imposters and customers. The proposed
method has been tested on CASIA-3D face database and the experimental
results show that our method achieves a high verification performance.},
author = {Ammar, Chouchane and Mebarka, Belahcene and Abdelmalik, Ouamane and Salah, Bourennane},
doi = {10.3745/JIPS.02.0037},
issn = {1976-913X},
journal = {JOURNAL OF INFORMATION PROCESSING SYSTEMS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {3},
pages = {468--488},
title = {{Evaluation of Histograms Local Features and Dimensionality Reduction for 3D Face Verification}},
volume = {12},
year = {2016}
}
@article{ISI:000402732800006,
abstract = {3D face was recently investigated for various applications, including
biometrics and diagnosis. Describing facial surface, i.e. how it bends
and which kinds of patches is composed by, is the aim of studies of Face
Analysis, whose ultimate goal is to identify which features could be
extracted from three-dimensional faces depending on the application. In
this study, we propose 105 novel geometrical descriptors for Face
Analysis. They are generated by composing primary geometrical
descriptors such as mean, Gaussian, principal curvatures, shape index,
curvedness, and the coefficients of the fundamental forms, and by
applying standard functions such as sine, cosine, and logarithm to them.
The new descriptors were mapped on 217 facial depth maps and analysed in
terms of descriptiveness of facial shape and exploitability for
localizing landmark points. Automatic landmark extraction stands as the
final aim of this analysis. Results showed that some newly generated
descriptors were sounder than the primary ones, meaning that their local
behaviours in correspondence to a landmark position is thoroughly
specific and can be registered with high similarity on every face of our
dataset.},
author = {Marcolin, Federica and Vezzetti, Enrico},
doi = {10.1007/s11042-016-3741-3},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jun},
number = {12},
pages = {13805--13834},
title = {{Novel descriptors for geometrical 3D face analysis}},
volume = {76},
year = {2017}
}
@article{Sghaier:2018:NTF:3193702.3193706,
abstract = {This manuscript presents an improved system research that can detect and recognize the person in 3D space automatically and without the interaction of the people's faces. This system is based not only on a quantum computation and measurements to extract the vector features in the phase of characterization but also on learning algorithm (using SVM) to classify and recognize the person. This research presents an improved technique for automatic 3D face recognition using anthropometric proportions and measurement to detect and extract the area of interest which is unaffected by facial expression. This approach is able to treat incomplete and noisy images and reject the non-facial areas automatically. Moreover, it can deal with the presence of holes in the meshed and textured 3D image. It is also stable against small translation and rotation of the face. All the experimental tests have been done with two 3D face datasets FRAV 3D and GAVAB. Therefore, the test's results of the proposed approach are promising because they showed that it is competitive comparable to similar approaches in terms of accuracy, robustness, and flexibility. It achieves a high recognition performance rate of 95.35{\%} for faces with neutral and non-neutral expressions for the identification and 98.36{\%} for the authentification with GAVAB and 100{\%} with some gallery of FRAV 3D datasets.},
address = {Hershey, PA, USA},
annote = {02/05/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
02/05/2018 Exclu{\'{i}}do (etapa 1)

janeiro de 2018},
author = {Sghaier, Souhir and Farhat, Wajdi and Souani, Chokri},
doi = {10.4018/IJACI.2018010104},
issn = {1941-6237},
journal = {International Journal of Ambient Computing and Intelligence},
keywords = {3D Face,Anthropometric,Euclidean Distance,Eye Corners,Feature Extraction,Learning,Measurements,Nose Tip,acm,etapa1,gil,id431,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {acm,etapa1,gil,id431,revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {60--77},
publisher = {IGI Global},
title = {{Novel Technique for 3D Face Recognition Using Anthropometric Methodology}},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJACI.2018010104},
volume = {9},
year = {2018}
}
@inproceedings{ISI:000389499000053,
abstract = {This paper investigates the other-race-effects in automatic 3D facial
expression recognition, giving the computational analysis of the
recognition performance obtained from two races, namely white and east
Asian. The 3D face information is represented by local depth feature,
and then a feature learning process is used to obtain race-sensitive
features to simulate the other-race-effect. The learned features from
own race and other race are then used to do facial expression
recognition. The proposed analysis is conducted on BU-3DFE database, and
the results show that the learned features from one race achieve better
recognition performance on the own-race faces. It reveals that the
other-race-effect are significant in facial expression recognition
problem, which confirms the results of psychological experiment results.},
annote = {11th Chinese Conference on Biometric Recognition (CCBR), Chengdu,
PEOPLES R CHINA, OCT 14-16, 2016},
author = {Xue, Mingliang and Duan, Xiaodong and Zhou, Juxiang and Wang, Cunrui and Wang, Yuangang and Li, Zedong and Liu, Wanquan},
booktitle = {Biometric Recognition},
doi = {10.1007/978-3-319-46654-5_53},
editor = {{You, Z and Wang, Y and Sun, Z and Shan, S and Zheng, W and Feng, J and Zhao, Q}},
isbn = {978-3-319-46654-5; 978-3-319-46653-8},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Chinese Assoc Artificial Intelligence; Chinese Acad Sci, Inst Automat; Springer; Sichuan Univ; Wisesoft Co Ltd},
pages = {483--493},
series = {Lecture Notes in Computer Science},
title = {{A Computational Other-Race-Effect Analysis for 3D Facial Expression Recognition}},
volume = {9967},
year = {2016}
}
@inproceedings{ISI:000427598702135,
abstract = {This paper explores the novel use of multiple depth sensors to overcome
occlusions and improve localization and tracking of body extremities.
The usage of data from only depth sensors not only overcomes visual
challenges associated with RGB sensors under low illumination, but also
protects the identity of surveyed person with high confidentiality. For
integrating depth information from multiple sources, the paper presents
first an overview of a novel calibration method for multiple depth
sensors. In case of occlusion of any fiducial point in the primary
sensor's depth image, co-ordinates of the point can be obtained from the
frame of other sensors using the calibration parameters. To localize
salient body parts such as hands, head and feet, a surface triangular
mesh is applied on generated 3D point cloud from the primary sensor. The
geodesic extrema from the mesh coincide with body extremities. The body
extremities can be identified based on those relative geodesic distances
between the extremities. Once the body parts are labelled, a portion of
body can be targeted and evaluated for specific gait analysis and
visualization. For the performance evaluation, our calibration method
has fared well in comparison to other available techniques. Also, our
proposed localization of salient body parts is able to successfully tag
the specific body part i.e. the head region.},
annote = {IEEE International Conference on Systems, Man, and Cybernetics (SMC),
Banff, CANADA, OCT 05-08, 2017},
author = {Mohsin, Nasreen and Payandeh, Shahram},
booktitle = {2017 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC)},
isbn = {978-1-5386-1645-1},
issn = {1062-922X},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE},
pages = {2736--2741},
series = {IEEE International Conference on Systems Man and Cybernetics Conference Proceedings},
title = {{Localization and Identification of Body Extremities Based on Data from Multiple Depth Sensors}},
year = {2017}
}
@inproceedings{ISI:000425236400020,
abstract = {Accurate facial landmarks localization (FLL) plays an important role in
face recognition, face tracking and 3D face reconstruction. It can be
formulated as a regression problem, which outputs facial landmarks
positions from the detected face image. Deep constitutional neural
network (CNN) has achieved great success in vision tasks, but it is
insignificant to use it directly. In this paper, instead of adopting CNN
model straightforwardly, we combine different convolutional features
with extreme machine learning (ELM) in a cascade framework to achieve
accurate FLL. Specifically, we extract globally and spatially
convolutional feature in the first stage for containing better
localization property by training deep CNN, which takes the whole face
region as input and concatenates lower layers with higher layers. Then,
we extract locally and correlatedly convolutional feature in the
following stages for preserving shape constraint by building
multi-objective CNN, which inputs local patches centered at the current
landmarks and concatenates independent subnetwork of each landmark
together. Moreover, the regressor embedded in CNN is replaced by the
robust ELM for accurate shape regression. Extensive experiments
demonstrate that our method performs better in challenging datasets.},
annote = {4th International Conference on Behavioral, Economic Advance in
Behavioral, Economic, Socio-Cultural Computing (BESC), Krakow, POLAND,
OCT 16-18, 2017},
author = {Li, Huifang and Li, Yidong and Liu, Wenhua and Dong, Hairong},
booktitle = {PROCEEDINGS OF 4TH INTERNATIONAL CONFERENCE ON BEHAVIORAL, ECONOMIC ADVANCE IN BEHAVIORAL, ECONOMIC, SOCIOCULTURAL COMPUTING (BESC)},
editor = {{Demazeau, Y and Gao, J and Xu, G and Kozlak, J and Muller, K and Razzak, I and Chen, H and Gu, Y}},
isbn = {978-1-5386-2365-7},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
title = {{Coarse-to-fine Facial Landmarks Localization based on Convolutional Feature}},
year = {2017}
}
@article{ISI:000386741300011,
abstract = {In this paper we propose a robust face recognition algorithm for low
resolution RGB-D Kinect data. Many techniques are proposed for image
preprocessing due to the noisy depth data. First, facial symmetry is
exploited based on the 3D point cloud to obtain a canonical frontal view
image irrespective of the initial pose and then depth data is converted
to XYZ normal maps. Secondly, multi-channel Discriminant Transforms are
then used to project RGB to DCS (Discriminant Color Space) and normal
maps to DNM (Discriminant Normal Maps). Finally, a Multi-channel Robust
Sparse Coding method is proposed that codes the multiple channels (DCS
or DNM) of a test image as a sparse combination of training samples with
different pixel weighting. Weights are calculated dynamically in an
iterative process to achieve robustness against variations in pose,
illumination, facial expressions and disguise. In contrast to existing
techniques, our multi-channel approach is more robust to variations.
Reconstruction errors of the test image (DCS and DNM) are normalized and
fused to decide its identity. The proposed algorithm is evaluated on
four public databases. It achieves 98.4{\%} identification rate on
CurtinFaces, a Kinect database with 4784 RGB-D images of 52 subjects.
Using a first versus all protocol on the Bosphorus, CASIA and FRGC v2
databases, the proposed algorithm achieves 97.6{\%}, 95.6{\%} and 95.2{\%}
identification rates respectively. To the best of our knowledge, these
are the highest identification rates reported so far for the first three
databases. (C) 2016 Elsevier B.V. All rights reserved.},
author = {Li, Billy Y L and Xue, Mingliang and Mian, Ajmal and Liu, Wanquan and Krishna, Aneesh},
doi = {10.1016/j.neucom.2016.06.012},
issn = {0925-2312},
journal = {NEUROCOMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
pages = {93--108},
title = {{Robust RGB-D face recognition using Kinect sensor}},
volume = {214},
year = {2016}
}
@inproceedings{ISI:000380407300092,
abstract = {This paper presents an implementation of face recognition, which is a
very important task of human face identification. Line scratch detection
in images is a highly challenging situation because of various
characteristics of this defect. Few characteristics are considered with
the different texture and geometry of images. We propose a useful
algorithm for frame-by-frame line scratch detection in face image which
deals with 3D approach and a filtering of detection. The temporary
filtering algorithm can be used to remove false detection due to thin
vertical structures by detecting the scratches on an image. Experimental
evaluation can be detecting the lines and scratches on a face image and
they used to solve this difficult approach. Our method is used with
missing parts in an image. Three-dimensional face recognition is an
extended method of facial recognition is considered according with the
geometry and texture of a face. It has been elaborated that 3D face
recognition methods can provide high accuracy as well as high detection
with a comparison of 2D recognition. 3D avoids such mismatch effect of
2D face recognition algorithms. Additionally, most 3D scanners achieve
both a 3D mesh and the texture of a face image. This allows combining
the output of pure 3D matches with the more traditional algorithms of 2D
face recognition, thus producing better performance.},
annote = {International Conference on Pervasive Computing (ICPC), Pune, INDIA, JAN
08-10, 2015},
author = {Pawar, Asmita A and Patil, Nitin N},
booktitle = {2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)},
isbn = {978-1-4799-6272-3},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE Pune Sect; IEEE Comp Soc; Savitribai Phule Pune Univ; IEEE Commun Soc Pune Chapter; Sinhgad Inst; Sakal Times},
title = {{Recognition of 3-D Faces with Missing Parts and Line Scratch Removal using New Technique}},
year = {2015}
}
@article{ISI:000371667200009,
abstract = {The problem of facial expression recognition in dynamic sequences of 3D
face scans has received a significant amount of attention in the recent
past whereas the problem of retrieval in this type of data has not. A
novel retrieval scheme for such data is introduced in this paper. It is
the first spatio-temporal retrieval scheme ever used for retrieval in
dynamic sequences of 3D face scans. The proposed scheme automatically
detects specific facial landmarks and uses them to create a
spatio-temporal descriptor. At first, geometric as well as topological
information of the 3D face scans is captured by using the detected
landmarks. In the sequel, the aforementioned spatial information is
filtered by using wavelet transformation, resulting to our final
spatio-temporal descriptor. Our descriptor is invariant to the number of
the 3D face scans of a facial expression sequence. The proposed
retrieval scheme exploits the Square of Euclidean distance in order to
compare descriptors corresponding to different 3D facial sequences. A
detailed evaluation of the introduced retrieval scheme is presented
showing that it outperforms previous state-of-the-art retrieval schemes.
Experiments have been conducted using the six prototypical expressions
of the standard data set BU - 4DFE. Finally, a majority voting
methodology based on the retrieval results is used to achieve
unsupervised dynamic 3D facial expression recognition. The achieved
classification accuracy outperforms the state-of-the-art supervised
dynamic 3D facial expression recognition techniques.},
annote = {3DOR Workshop, Strasbourg, FRANCE, APR 06, 2014},
author = {Danelakis, Antonios and Theoharis, Theoharis and Pratikakis, Ioannis},
doi = {10.1007/s00371-015-1142-7},
issn = {0178-2789},
journal = {VISUAL COMPUTER},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {2},
pages = {257--269},
title = {{A robust spatio-temporal scheme for dynamic 3D facial expression retrieval}},
volume = {32},
year = {2016}
}
@inproceedings{ISI:000361841100053,
abstract = {To help visually impaired people recognize people in their daily life, a
3D face feature registration approach is proposed with a RGB-D sensor.
Compared to 2D face recognition methods, 3D data based approaches are
more robust to the influence of face orientations and illumination
changes. Different from most 3D data based methods, we employ a one-step
ICP registration approach that is much less time consuming. The error
tolerance of the 3D registration approach is analyzed with various error
levels in 3D measurements. The method is tested with a Kinect sensor, by
analyzing both the angular and distance errors to recognition
performance. A number of other potential benefits in using 3D face data
are also discussed, such as RGB image rectification, multiple-view face
integration, and facial expression modeling, all useful for social
interactions of visually impaired people with others.},
annote = {13th European Conference on Computer Vision (ECCV), Zurich, SWITZERLAND,
SEP 06-12, 2014},
author = {Li, Wei and Li, Xudong and Goldberg, Martin and Zhu, Zhigang},
booktitle = {COMPUTER VISION - ECCV 2014 WORKSHOPS, PT III},
doi = {10.1007/978-3-319-16199-0_53},
editor = {{Agapito, L and Bronstein, MM and Rother, C}},
isbn = {978-3-319-16199-0; 978-3-319-16198-3},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {763--777},
series = {Lecture Notes in Computer Science},
title = {{Face Recognition by 3D Registration for the Visually Impaired Using a RGB-D Sensor}},
volume = {8927},
year = {2015}
}
@article{ISI:000371563600001,
abstract = {Researchers in different fields such as image processing, neural
sciences, computer programs and psychophysics have investigated number
of problems related to facial recognition by machines and humans since
1975. Automatic recognition of the human emotions using facial
expression is an important, but difficult problem. This study introduces
a novel and automatic approach to analyze and recognize human facial
expressions and emotions using a Metaheuristic Algorithm (MA), which
hybridizes iterated local search and Genetic Algorithms with
Back-Propagation algorithm (ILSGA-BP). Back Propagation algorithm (BP)
was used to train and test the extracted features from the extracted
right eye, left eye and mouth using radial curves and Cubic Bezier
curves, M4 was used to enhance and optimize the initial weights of the
traditional BP. FEEDTUM facial expression database was used in this
study for training and testing processes with seven different emotions
namely; surprise, happiness, disgust, neutral, fear, sadness and anger.
A comparison of the results obtained using the extracted features from
the radial curves, Cubic Bezier curves and the combination of them were
conducted. The comparison shows the superiority of the combination of
the radial curves and the Cubic Bezier curves with percentage ranges
between 87{\%} and 97{\%} over the radial curves alone with a percentage
ranges between 80{\%} and 97{\%} and over the Cubic Bezier curves with a
percentage ranges between 83{\%} and 97{\%}. Moreover, based on the
extracted features using the radial curves, Cubic Bezier curves and the
combination of them, the experimental results show that the proposed
ILSGA-BP algorithm outperformed the BP algorithm with overall accuracy
88{\%}, 89{\%} and 93.4{\%} respectively, compared to 83{\%}, 82{\%} and 85{\%}
respectively using BP algorithm.},
author = {Alsmadi, Mutasem},
issn = {1683-3198},
journal = {INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {1A, SI},
pages = {133--141},
title = {{Facial Recognition under Expression Variations}},
volume = {13},
year = {2016}
}
@article{ISI:000440385500001,
abstract = {Nowadays face recognition systems are facing a new problem after having
won the challenge of reliability. The problem is that these systems have
become vulnerable to attacks by identity theft. In order to deceive the
recognition systems hackers use several methods, such as the use of face
images or videos of people belonging to the system database. Luckily,
this type of attack is thwarted by the use of adapted systems. But
unfortunately another type of attack that uses 3D face masks appeared.
This type of attack is very efficient, since as will be shown, a high
percentage of hackers who use 3D masks can mislead a good facial
recognition system, like the one used in our investigation. In this
paper, a new method is proposed for the detection of hackers that use 3D
masks to deceive face recognition systems. This method uses the Angular
Radial Transformation (ART) to extract pertinent features that are fed
into a classifier to decide whether the captured image represents a face
image. The performance of the proposed method was evaluated using a
public 3D Mask Attack Database (3DMAD). The obtained results show the
efficiency of the proposed method, since it can reduce the error rate in
discriminating between a real face and a face mask down to 0.90{\%}. (C)
2017 Production and hosting by Elsevier B.V. on behalf of Faculty of
Computers and Information, Cairo University. This is an open access
article under the CC BY-NC-ND license
(http://creativecommons.org/licenses/by-nc-nd/4.0/).},
author = {Hamdan, Bensenane and Mokhtar, Keche},
doi = {10.1016/j.eij.2017.10.001},
issn = {1110-8665},
journal = {EGYPTIAN INFORMATICS JOURNAL},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jul},
number = {2},
pages = {75--82},
title = {{The detection of spoofing by 3D mask in a 2D identity recognition system}},
volume = {19},
year = {2018}
}
@article{ISI:000368744300013,
abstract = {The problem of facial expression recognition in dynamic sequences of 3D
face scans has received a significant amount of attention in the recent
past whereas the problem of retrieval in this type of data has not. A
novel retrieval methodology for such data is introduced in this paper.
The proposed methodology automatically detects specific facial landmarks
and uses them to create a descriptor. This descriptor is the
concatenation of three sub-descriptors which capture topological as well
as geometric information of the 3D face scans. The motivation behind the
proposed hybrid facial expression descriptor is the fact that some
facial expressions, like happiness and surprise, are characterized by
obvious changes in the mouth topology while others, like anger, fear and
sadness, produce geometric but no significant topological changes. The
proposed retrieval scheme exploits the Dynamic Time Warping technique in
order to compare descriptors corresponding to different 3D facial
sequences. A detailed evaluation of the introduced retrieval scheme is
presented showing that it outperforms previous state-of-the-art
retrieval schemes. Experiments have been conducted using the six
prototypical expressions of the standard dataset BU-4DFE and the eight
prototypical expressions of the recently available dataset BP4D-
Spontaneous. Finally, a majority voting scheme based on the retrieval
results is used to achieve unsupervised dynamic 3D facial expression
recognition. The achieved classification accuracy is comparable to the
state-of-the-art supervised dynamic 3D facial expression recognition
techniques. (C) 2015 Elsevier Ltd. All rights reserved.},
author = {Danelakis, Antonios and Theoharis, Theoharis and Pratikakis, Ioannis and Perakis, Panagiotis},
doi = {10.1016/j.patcog.2015.10.012},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {174--185},
title = {{An effective methodology for dynamic 3D facial expression retrieval}},
volume = {52},
year = {2016}
}
@inproceedings{ISI:000451039807048,
abstract = {The objective of this paper was to develop a new algorithm to segment
individual trees directly by using the three-dimensional space
characteristic of airborne light detection and ranging point cloud data.
The local maximum method was used in the initial segmentation and the
error identification tree exclusion. On the basis of the point cloud
spatial distribution of individual trees and the adjacent relationship
with the other trees, a point cloud clustering method was developed to
decide the points belonging to the individual trees. This algorithm was
tested by 6 forest plots in the Genhe forestry reserve. The results
showed that this algorithm could segment individual trees quickly and
accurately, and the overall accuracy of this algorithm was 96.3{\%}.},
annote = {38th IEEE International Geoscience and Remote Sensing Symposium
(IGARSS), Valencia, SPAIN, JUL 22-27, 2018},
author = {Li, Shihua and Su, Lian and Liu, Yuhan and He, Ze},
booktitle = {IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM},
isbn = {978-1-5386-7150-4},
issn = {2153-6996},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers Geoscience {\&} Remote Sensing Soc; European Space Agcy},
pages = {7520--7523},
series = {IEEE International Symposium on Geoscience and Remote Sensing IGARSS},
title = {{SEGMENTATION OF INDIVIDUAL TREES BASED ON A POINT CLOUD CLUSTERING METHOD USING AIRBORNE LIDAR DATA}},
year = {2018}
}
@inproceedings{ISI:000395499500098,
abstract = {The article presents the use of modern close range photogrammetry for
possessing highly accurate 3D models of the human face (including the
ears). Modern methods used to obtain precise data describing the
construction of a human face, and even the whole human body, should
allow to get finished measurement material in a very short time. Those
features belong to the optical scanning technology. Comparative analysis
of models of the human face has been made (created from the cloud of
points obtained from optical scanner) for the same person as well as for
two different persons. Among other things, the parameters describing the
similarity of a human face (in particular, the similarity of the human
ear) based primarily on the analysis of the differences between the
points of the model (comparison of several hundred thousand points on
the model) were determined. Ultimately, these parameters can be used to
identify persons. Considering the great opportunities presented methods,
other potential applications have been presented.},
annote = {16th International Multidisciplinary Scientific Geoconference (SGEM
2016), Albena, BULGARIA, JUN 30-JUL 06, 2016},
author = {Bobkowska, Katarzyna and Janowski, Artur and Przyborski, Marek and Szulwic, Jakub},
booktitle = {INFORMATICS, GEOINFORMATICS AND REMOTE SENSING CONFERENCE PROCEEDINGS, SGEM 2016, VOL II},
isbn = {978-619-7105-59-9},
issn = {1314-2704},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Bulgarian Acad Sci; Acad Sci Czech Republ; Latvian Acad Sci; Polish Acad Sci; Russian Acad Sci; Serbian Acad Sci {\&} Arts; Slovak Acad Sci; Natl Acad Sci Ukraine; Inst Water Problem {\&} Hydropower NAS KR; Natl Acad Sci Armenia; Sci Council Japan; World Acad S},
pages = {767+},
series = {International Multidisciplinary Scientific GeoConference-SGEM},
title = {{A NEW METHOD OF PERSONS IDENTIFICATION BASED ON COMPARATIVE ANALYSIS OF 3D FACE MODELS}},
year = {2016}
}
@article{ISI:000463151400027,
abstract = {Recognition of sleep posture and its changes are related to information
monitoring in a number of health-related applications such as apnea
prevention and elderly care. This paper uses a less privacy-invading
approach to classify sleep postures of a person in various
configurations including side and supine postures. In order to
accomplish this, a single depth sensor has been utilized to collect
selective depth signals and populated a dataset associated with the
depth data. The data is then analyzed by a novel frequency-based feature
selection approach. These extracted features were then correlated in
order to rank their information content in various 2D scans from the 3D
point cloud in order to train a support vector machine (SVM). The data
of subjects are collected under two conditions. First when they were
covered with a thin blanket and second without any blanket. In order to
reduce the dimensionality of the feature space, a T-test approach is
employed to determine the most dominant set of features in the frequency
domain. The proposed recognition approach based on the frequency domain
is also compared with an approach using feature vector defined based on
skeleton joints. The comparative studies are performed given various
scenarios and by a variety of datasets. Through our study, it is shown
that our proposed method offers better performance to that of the
joint-based method.},
author = {Rasouli, Maryam S D and Payandeh, Shahram},
doi = {10.1007/s12652-018-0796-1},
issn = {1868-5137},
journal = {JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {5, SI},
pages = {1999--2014},
title = {{A novel depth image analysis for sleep posture estimation}},
volume = {10},
year = {2019}
}
@inproceedings{ISI:000373009100042,
abstract = {Development of new databases contributing much among researchers for
solving many challenging tasks that might have an important role during
the implementation of efficient algorithms to handle all difficulties
for an automatic system. In this paper, authors have introduced the
issues and approaches that have been considered during image acquisition
procedure during designing of own face database. This database consists
of almost all the challenges in the domain of computer vision especially
face recognition. Acquisition of database's images are done in our own
institute's laboratory with variations of facial actions (i.e. movement
of facial units, expression), illumination, occlusion, as well as a
pose. Along with the 3D face images, corresponding 2D face images have
also been captured using Structured Light Scanner (SLS). Particularly,
this image acquisition technique is not harmful as laser scanner does.
Moreover, authors have made the visualization of practical
representation of laboratory setup within this article that would again
be helpful to the researchers for better understanding the image
acquisition procedure in detail. In this databases, authors have
accomplished the X, Y planes along with range face image and
corresponding 2D image of human face.},
annote = {3rd International Conference on Information System Design and
Intelligent Applications (INDIA), ANITS Campus, Visakhapatnam, INDIA,
JAN 08-09, 2016},
author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
booktitle = {INFORMATION SYSTEMS DESIGN AND INTELLIGENT APPLICATIONS, VOL 2, INDIA 2016},
doi = {10.1007/978-81-322-2752-6_42},
editor = {{Satapathy, SC and Mandal, JK and Udgata, SK and Bhateja, V}},
isbn = {978-81-322-2752-6; 978-81-322-2750-2},
issn = {2194-5357},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Anil Neerukonda Inst Technol {\&} Sci, Det CSE; ANITS CSI Student Branch},
pages = {425--436},
series = {Advances in Intelligent Systems and Computing},
title = {{Issues and Approaches to Design of a Range Image Face Database}},
volume = {434},
year = {2016}
}
@article{ISI:000351796000002,
abstract = {3D face recognition and emotion analysis play important roles in many
fields of communication and edutainment An effective facial descriptor,
with higher discriminating capability for face recognition and higher
descriptiveness for facial emotion analysis, is a challenging issue.
However, in the practical applications, the descriptiveness and
discrimination are independent and contradictory to each other. 3D
facial data provide a promising way to balance these two aspects. In
this paper, a robust regional bounding spherical descriptor (RBSR) is
proposed to facilitate 3D face recognition and emotion analysis. In our
framework, we first segment a group of regions on each 3D facial point
cloud by shape index and spherical bands on the human face. Then the
corresponding facial areas are projected to regional bounding spheres to
obtain our regional descriptor. Finally, a regional and global
regression mapping (RGRM) technique is employed to the weighted regional
descriptor for boosting the classification accuracy. Three largest
available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the
performance comparison and the experimental results show a consistently
better performance for 3D face recognition and emotion analysis. (C)
2015 Elsevier B.V. All rights reserved.},
author = {Ming, Yue},
doi = {10.1016/j.imavis.2014.12.003},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
pages = {14--22},
title = {{Robust regional bounding spherical descriptor for 3D face recognition and emotion analysis}},
volume = {35},
year = {2015}
}
@inproceedings{ISI:000391534900063,
abstract = {In this paper, we present a new radial string representation and
matching approach for 3D face recognition under expression variations
and partial occlusions. The radial strings are an indexed collection of
strings emanating from the nose tip of a face scan. The matching between
two radial strings is conducted through a dynamic programming process,
in which a partial matching mechanism is established to find those
unoccluded substrings effectively. Moreover, the most discriminative and
stable radial strings are selected optimally by the well-known AdaBoost
algorithm to achieve a composite classifier for 3D face recognition
under facial expression changes. Experimental results on the GavabDB and
the Bosphorus databases show that the proposed approach achieves
promising results for human face recognition with expressions and
occlusions.},
annote = {International Conference on Digital Image Computing - Techniques and
Applications (DICTA), Gold Coast, AUSTRALIA, NOV 30-DEC 02, 2016},
author = {Yu, Xun and Gao, Yongsheng and Zhou, Jun},
booktitle = {2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA)},
editor = {{Liew, AWC and Lovell, B and Fookes, C and Zhou, J and Gao, Y and Blumenstein, M and Wang, Z}},
isbn = {978-1-5090-2896-2},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Australian Govt, Dept Defence, Defence Sci {\&} Technol Grp; IAPR; Canon Informat Syst Res Australia; IEEE; Griffith Univ; APRS},
pages = {436--441},
title = {{Boosting Radial Strings for 3D Face Recognition with Expressions and Occlusions}},
year = {2016}
}
@inproceedings{ISI:000414298700135,
abstract = {Super-resolution face image acquisition system is indispensable in
people's life. Under the condition of low illumination, the illumination
environment difference is too big or the light is insufficient, which
leads to the traditional image acquisition system can not collect the
high quality face image, and the limitation is poor. Based on open
source computer vision library (OpenCV) in the C++ environment
configuration, the use of Three Dimension (3D) face recognition
technology algorithm, design a set of low illumination conditions of the
super resolution face image acquisition system. Experiments show that
the design scheme with real-time focusing speed), fast (single
acquisition 0.05 seconds), accurate (facial recognition rate of 99.3{\%})
etc. characteristics, be able to fully meet the needs of low
illumination conditions for super-resolution of face image acquisition.},
annote = {2nd International Conference on Image, Vision and Computing (ICIVC),
Chengdu, PEOPLES R CHINA, JUN 02-04, 2017},
author = {Luo, Min and Luo, Yadong and Li, Hui and Zhang, Xia and Yang, Yongkui},
booktitle = {2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017)},
isbn = {978-1-5090-6238-6},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; Sichuan Prov Comp Sci; Singapore Inst Elect; Chengdu Univ Informat Technol; Chinese Acad Sci Co Ltd, Chengdu Informat Technol},
pages = {680--683},
title = {{Design and Implementation of High Resolution Face Image Acquisition System under Low Illumination Based on the Open Source Computer Vision Library}},
year = {2017}
}
@article{ISI:000429630300007,
abstract = {This research proposes a method for 3D face recognition in various
conditions using 3D constrained local model (CLM-Z). In this method, a
combination of 2D images (RGBs) and depth images (Ds) captured by Kinect
has been used. After detecting the face and smoothing the depth image,
CLM-Z model has been used to model and detect the important points of
the face. These points are described using Histogram of Oriented
Gradients (HOG), Local Binary Patterns (LBP), and 3D Local Binary
Patterns (3DLBP). Finally, each face is recognized by a Support Vector
Machine (SVM). The challenging situations are changes of lighting,
facial expression and head pose. The results on CurtinFaces and IIIT-D
datasets demonstrate that the proposed method outperformed
state-of-the-art methods under illumination, expression and pitch pose
conditions and comparable results were obtained in other cases.
Additionally, our proposed method is robust even when the training data
has not been carefully collected.},
author = {Kaashki, Nastaran Nourbakhsh and Safabakhsh, Reza},
doi = {10.1016/j.jvcir.2018.02.003},
issn = {1047-3203},
journal = {JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {66--85},
title = {{RGB-D face recognition under various conditions via 3D constrained local model}},
volume = {52},
year = {2018}
}
@inproceedings{ISI:000389586800019,
abstract = {Generating a user-specific 3D face model is useful for a variety of
applications, such as facial animation, games or movie industries.
Recently, there have been spectacular developments in 3D sensors,
however, accurately recovering the 3D shape model from a single image is
a major challenge of computer vision and graphics. In this paper, we
present a method that can not only acquire a 3D shape from only a single
face image but also reconstruct facial expression. To accomplish this, a
3D face database with a variety of identities and facial expressions was
restructured as a data array which was decomposed for the acquisition of
bilinear models. With this model, we represent facial variances as two
kinds of elements: expressions and identities. Then, target face image
is fitted to 3D model while estimating its expression and shape
parameters. As application example, we transferred expressions to
reconstructed 3D models and naturally applied new facial expressions to
show the efficiency of the proposed method.},
annote = {4th International Conference on Distributed, Ambient and Pervasive
Interactions (DAPI) held as part of 18th International Conference on
Human-Computer Interaction (HCI International), Toronto, CANADA, JUL
17-22, 2016},
author = {Hong, Yu-Jin and Nam, Gi Pyo and Choi, Heeseung and Cho, Junghyun and Kim, Ig-Jae},
booktitle = {DISTRIBUTED, AMBIENT AND PERVASIVE INTERACTIONS, (DAPI 2016)},
doi = {10.1007/978-3-319-39862-4_19},
editor = {{Streitz, N and Markopoulos, P}},
isbn = {978-3-319-39862-4; 978-3-319-39861-7},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {202--209},
series = {Lecture Notes in Computer Science},
title = {{3-Dimensional Face from a Single Face Image with Various Expressions}},
volume = {9749},
year = {2016}
}
@inproceedings{ISI:000457843609059,
abstract = {This paper presents SO-Net, a permutation invariant architecture for
deep learning with orderless point clouds. The SO-Net models the spatial
distribution of point cloud by building a Self-Organizing Map (SOM).
Based on the SOM, SO-Net performs hierarchical feature extraction on
individual points and SOM nodes, and ultimately represents the input
point cloud by a single feature vector. The receptive field of the
network can be systematically adjusted by conducting point-to-node k
nearest neighbor search. In recognition tasks such as point cloud
reconstruction, classification, object part segmentation and shape
retrieval, our proposed network demonstrates performance that is similar
with or better than state-of-the-art approaches. In addition, the
training speed is significantly faster than existing point cloud
recognition networks because of the parallelizability and simplicity of
the proposed architecture. Our code is available at the project
website.(1)},
annote = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Salt Lake City, UT, JUN 18-23, 2018},
author = {Li, Jiaxin and Chen, Ben M and Lee, Gim Hee},
booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2018.00979},
isbn = {978-1-5386-6420-9},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; CVF; IEEE Comp Soc},
pages = {9397--9406},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{SO-Net: Self-Organizing Network for Point Cloud Analysis}},
year = {2018}
}
@inproceedings{ISI:000417429000016,
abstract = {Fast and robust 3D reconstruction of facial geometric structure from a
single image is a challenging task with numerous applications, but there
exist two problems when applied ``in the wild{\{}''{\}}: the 3D estimates are
unstable for different photos of the same subject; the 3D estimates are
over-regularized and generic. In response, a robust method for
regressing discriminative 3D morphable face models(3DMM) is described to
support face recognition and 3D mask printing. Combining the local data
sets with the public data sets, improving the exiting 3DMM fitting
method and then using a convolutional neural network(CNN) to improve
reconstruction effect. The ground truth 3D faces of the CNN are the
pooled 3DMM parameters extracted from the photos of the same subject.
Using CNN to regress 3DMM shape and texture parameters directly from an
input photo and offering a method for generating huge numbers of labeled
examples. There are two key points of the paper: one is the training
data generation for the model training; the other is the training of 3D
reconstruction model. Experimental results and analysis show that this
method costs much less time than traditional methods of 3D face
modeling, and it is improved for different races on photos with any
angles than the existing methods based on deep learning, and the system
has better robustness.},
annote = {10th International Conference on Intelligent Computation Technology and
Automation (ICICTA), Changsha, PEOPLES R CHINA, OCT 09-10, 2017},
author = {Fangmin, Li and Ke, Chen and Xinhua, Liu},
booktitle = {2017 10TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTATION TECHNOLOGY AND AUTOMATION (ICICTA 2017)},
doi = {10.1109/ICICTA.2017.23},
isbn = {978-1-5386-1230-9},
issn = {1949-1263},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Changsha Univ Sci {\&} Technol, Commun Res Inst; Cent S Univ, Shenzhen Res Inst; Hunan City Coll, Dept Urban Management},
pages = {71--74},
series = {International Conference on Intelligent Computation Technology and Automation},
title = {{3D Face Reconstruction Based on Convolutional Neural Network}},
year = {2017}
}
@article{ISI:000348880300019,
abstract = {In this paper, we present a fully automated multimodal Curvelet-based
approach for textured 3D face recognition. The proposed approach relies
on a novel multimodal keypoint detector capable of repeatably
identifying keypoints on textured 3D face surfaces. Unique local surface
descriptors are then constructed around each detected keypoint by
integrating Curvelet elements of different orientations, resulting in
highly descriptive rotation invariant features. Unlike previously
reported Curvelet-based face recognition algorithms which extract global
features from textured faces only, our algorithm extracts both texture
and 3D local features. In addition, this is achieved across a number of
frequency bands to achieve robust and accurate recognition under varying
illumination conditions and facial expressions. The proposed algorithm
was evaluated using three well-known and challenging datasets, namely
FRGC v2, BU-3DFE and Bosphorus datasets. Reported results show superior
performance compared to prior art, with 99.2{\%}, 95.1{\%} and 91{\%}
verification rates at 0.001 FAR for FRGC v2, BU-3DFE and Bosphorus
datasets, respectively. (C) 2014 Elsevier Ltd. All rights reserved.},
author = {Elaiwat, S and Bennamoun, M and Boussaid, F and El-Sallam, A},
doi = {10.1016/j.patcog.2014.10.013},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {4},
pages = {1235--1246},
title = {{A Curve let-based approach for textured 3D face recognition}},
volume = {48},
year = {2015}
}
@article{ISI:000463462600049,
abstract = {Extracting efficient features from the large volume of 3D facial data
directly is extremely difficult in 3D face recognition (3D-FR) with the
latest methods, which mostly require heavy computations and manual
processing steps. This paper presents a computationally efficient 3D-FR
system based on a novel Frenet frame-based feature that is derived from
the 3D facial iso-geodesic curves. In terms of the evaluation of the
proposed method, we conducted a number of experiments on the CASIA 3D
face database, and a superior recognition performance has been achieved.
The performance evaluation suggests that the pose invariance attribute
of the features relieves the need of an expensive 3D face registration
in the face preprocessing procedure, where we take less time to process
conversely. Our experiments further demonstrate that the proposed method
not only achieves competitive recognition performance when compared with
some existing techniques for 3D-FR, but also is computationally
efficient. (C) 2019 Elsevier Inc. All rights reserved.},
author = {Shi, Biao and Zang, Huaijuan and Zheng, Rongsheng and Zhan, Shu},
doi = {10.1016/j.jvcir.2019.02.002},
issn = {1047-3203},
journal = {JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {455--460},
title = {{An efficient 3D face recognition approach using Frenet feature of iso-geodesic curves}},
volume = {59},
year = {2019}
}
@inproceedings{ISI:000427083300058,
abstract = {Autism is a developmental disorder involving qualitative impairments in
social interaction. One source of those impairments are difficulties
with facial expressions of emotion. Autistic people often have
difficulty to recognize or to understand other people's emotions and
feelings, or expressing their own. This work proposes a method to
automatically recognize seven basic emotions among autistic children in
real time: Happiness, Anger, Sadness, Surprise, Fear, Disgust, and
Neutral. The method uses the Microsoft Kinect sensor to track and
identify points of interest from the 3D face model and it is based on
the {\$}P point-cloud recognizer to identify multi-stroke emotions as
point-clouds. The experimental results show that our system can achieve
above 94.28{\%} recognition rate. Our study provides a novel clinical tool
to help children with autism to assisting doctors in operating rooms.},
annote = {Intelligent Systems and Computer Vision (ISCV), Fez, MOROCCO, APR 17-19,
2017},
author = {Jazouli, Maha and Majda, Aicha and Zarghili, Arsalane},
booktitle = {2017 INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV)},
editor = {{Nfaoui, E and Boumhidi, J and Sabri, A and Yahyaouy, A and Bouchaffra, D}},
isbn = {978-1-5090-4062-9},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; Univ Sidi Mohamed ben Abdellah; IEEE Comp Soc; IEEE Morocco Sect},
title = {{A {\$}P Recognizer for Automatic Facial Emotion Recognition using Kinect Sensor}},
year = {2017}
}
@inproceedings{ISI:000371977803154,
abstract = {A 3D face recognition method using region-based extended local binary
pattern (eLBP) is proposed. First, the depth image converted from the
preprocessed 3D pointclouds is normalized. Then, different regions
according to their distortions under facial expressions are extracted by
binary masks and represented by the uniform pattern of extended LBP.
Finally, sparse representation classifier (SRC) is adopted for
classification on the single region. Feature-level and score-level
fusion with weight-sparse representation classifier (W-SRC) are also
tested and compared, and the latter has better performance. The
experiments on FRGC v2.0 database demonstrate that the proposed method
is robust and efficient.},
annote = {IEEE International Conference on Image Processing (ICIP), Quebec City,
CANADA, SEP 27-30, 2015},
author = {Lv, Shiwen and Da, Feipeng and Deng, Xing},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4799-8339-1},
issn = {1522-4880},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; IEEE Signal Proc Soc},
pages = {3635--3639},
series = {IEEE International Conference on Image Processing ICIP},
title = {{A 3D Face Recognition Method Using Region-Based Extended Local Binary Pattern}},
year = {2015}
}
@article{ISI:000349271500019,
abstract = {Facial expressions are a powerful tool that communicates a person's
emotional state and subsequently his/her intentions. Compared to 2D face
images, 3D face images offer more granular cues that are not available
in the 2D images. However, one major setback of 3D faces is that they
impose a higher dimensionality than 2D faces. In this paper, we attempt
to address this problem by proposing a fully automatic 3D facial
expression recognition model that tackles the high dimensionality
problem in a twofold solution. First, we transform the 3D faces into the
2D plane using conformal mapping. Second, we propose a Differential
Evolution (DE) based optimization algorithm to select the optimal facial
feature set and the classifier parameters simultaneously. The optimal
features are selected from a pool of Speed Up Robust Features (SURF)
descriptors of all the prospective facial points. The proposed model
yielded an average recognition accuracy of 79{\%} using the Bosphorus
database and 79.36{\%} using the BU-3DFE database. In addition, we exploit
the facial muscular movements to enhance the probability estimation (PE)
of Support Vector Machine (SVM). Joint application of feature selection
with the proposed enhanced PE (EPE) yielded an average recognition
accuracy of 84{\%} using the Bosphorus database and 85.81{\%} using the
BU-3DFE database, which is statistically significantly better (at p {\textless}
0.01 and p {\textless} 0.001, respectively) if compared to the individual exploit
of the optimal features only. (C) 2014 Elsevier Ltd. All rights
reserved.},
author = {Azazi, Amal and Lotfi, Syaheerah Lebai and Venkat, Ibrahim and Fernandez-Martinez, Fernando},
doi = {10.1016/j.eswa.2014.10.042},
issn = {0957-4174},
journal = {EXPERT SYSTEMS WITH APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {6},
pages = {3056--3066},
title = {{Towards a robust affect recognition: Automatic facial expression recognition in 3D faces}},
volume = {42},
year = {2015}
}
@inproceedings{ISI:000352725200038,
abstract = {Numerous applications related to urban scene analysis demand automatic
recognition of buildings and distinct sub-elements. For example, if
LiDAR data is available, only 3D information could be leveraged for the
segmentation. However, this poses several risks, for instance, the
in-plane objects cannot be distinguished from their surroundings. On the
other hand, if only image based segmentation is performed, the geometric
features (e.g., normal orientation, planarity) are not readily
available. This renders the task of detecting the distinct sub-elements
of the building with similar radiometric characteristic infeasible. In
this paper the individual sub-elements of buildings are recognized
through sub-segmentation of the building using geometric and radiometric
characteristics jointly. 3D points generated from Unmanned Aerial
Vehicle (UAV) images are used for inferring the geometric
characteristics of roofs and facades of the building. However, the
image-based 3D points are noisy, error prone and often contain gaps.
Hence the segmentation in 3D space is not appropriate. Therefore, we
propose to perform segmentation in image space using geometric features
from the 3D point cloud along with the radiometric features. The initial
detection of buildings in 3D point cloud is followed by the segmentation
in image space using the region growing approach by utilizing various
radiometric and 3D point cloud features. The developed method was tested
using two data sets obtained with UAV images with a ground resolution of
around 1-2 cm. The developed method accurately segmented most of the
building elements when compared to the plane-based segmentation using 3D
point cloud alone.},
annote = {Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
Univ Munchen, Munich, GERMANY, MAR 25-27, 2015},
author = {Vetrivel, A and Gerke, M and Kerle, N and Vosselman, G},
booktitle = {PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I},
doi = {10.5194/isprsarchives-XL-3-W2-261-2015},
editor = {{Stilla, U and Heipke, C}},
issn = {2194-9034},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {W2},
organization = {ISPRS},
pages = {261--268},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{Segmentation of UAV-based images incorporating 3D point cloud information}},
volume = {40-3},
year = {2015}
}
@inproceedings{ISI:000428410702188,
abstract = {This paper proposes a novel descriptor based on the local derivative
pattern (LDP) for 3D face recognition. Compared to the local binary
pattern (LBP), LDP can capture more detailed information by encoding
directional pattern features. It is based on the local derivative
variations that extract high-order local information. We propose a novel
discriminative facial shape descriptor, local normal derivative pattern
(LNDP) that extracts LDP from the surface normal. Using surface normal,
the orientation of a surface at each point is determined as a
first-order surface differential. Three normal component images are
extracted by estimating three components of normal vectors in x, y, and
z channels. Each normal component is divided into several patches and
encoded using LDP. The final descriptor is created by concatenating
histograms of the LNDP on each patch. Experimental results on two famous
3D face databases, FRGC v2.0 and Bosphorus illustrate the effectiveness
of the proposed descriptor.},
annote = {24th IEEE International Conference on Image Processing (ICIP), Beijing,
PEOPLES R CHINA, SEP 17-20, 2017},
author = {Soltanpour, Sima and Wu, Q M Jonathan},
booktitle = {2017 24TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-5090-2175-8},
issn = {1522-4880},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers Signal Proc Soc},
pages = {2811--2815},
series = {IEEE International Conference on Image Processing ICIP},
title = {{HIGH-ORDER LOCAL NORMAL DERIVATIVE PATTERN (LNDP) FOR 3D FACE RECOGNITION}},
year = {2017}
}
@article{ISI:000381123900010,
abstract = {In this paper, we propose a robust unsupervised algorithm for automatic
alignment of two manifolds in different datasets with possibly different
dimensionalities. The significant contribution is that the proposed
alignment algorithm is performed automatically without any assumptions
on the correspondences between the two manifolds. For such purpose, we
first automatically extract local feature histograms at each point of
the manifolds and establish an initial similarity between the two
datasets by matching their histogram-based features. Based on such
similarity, an embedding space is estimated where the distance between
the two manifolds is minimized while maximally retaining the original
structure of the manifolds. The elegance of this idea is that such
complicated problem is formulated as a generalized eigenvalue problem,
which can be easily solved. The alignment process is achieved by
iteratively increasing the sparsity of correspondence matrix until the
two manifolds are correctly aligned and consequently one can reveal
their joint structure. We demonstrate the effectiveness of our algorithm
on different datasets by aligning protein structures, 3D face models and
facial images of different subjects under pose and lighting variations.
Finally, we also compare with a state-of-the-art algorithm and the
results show the superiority of the proposed manifold alignment in terms
of vision effect and numerical accuracy.},
author = {Fan, Ke and Mian, Ajmal and Liu, Wanquan and Li, Ling},
doi = {10.1007/s00138-016-0772-8},
issn = {0932-8092},
journal = {MACHINE VISION AND APPLICATIONS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {6},
pages = {929--942},
title = {{Unsupervised manifold alignment using soft-assign technique}},
volume = {27},
year = {2016}
}
@inproceedings{ISI:000387649600042,
abstract = {From last few decades, generating a 3D face model from an human drawn
sketch has caught the interest of many researchers in the area of image
processing and face recognition. It has various applications in 3D
cartoon modelling, police investigation and verification, and in Image
Processing. Many techniques are there to generate 3D models from a
sketch. 3D landmark estimation, 2D landmark detection, and synthesis of
texture and surface with respect to 3-D morphable model are the steps,
respectively, to generate the 3D face model. 3D face modelling using
these steps has a higher rate of accuracy of identification of a person
from her sketch and no proper photograph. In this piece of literature,
we present a review on efficient technique that can be used to generate
3D face from sketch drawn by human.},
annote = {5th IEEE International Conference on Computation of Power, Energy,
Information and Communication (ICCPEIC), Adhiparasakthi Engn Coll,
Melmaruvathur, INDIA, APR 20-21, 2016},
author = {Amolik, Akshay and Ahamad, Syed Tahir and Dey, Sourav and Manjula, R},
booktitle = {2016 INTERNATIONAL CONFERENCE ON COMPUTATION OF POWER, ENERGY INFORMATION AND COMMUNICATION (ICCPEIC)},
isbn = {978-1-5090-0900-8},
issn = {2472-4033},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; Dept Elect {\&} Elect Engn},
pages = {237--243},
series = {International Conference on Computation of Power, Energy Information and Communication},
title = {{3D Face View Generation from Human Drawn Sketch: A review}},
year = {2016}
}
@article{ISI:000349308600007,
abstract = {Three-dimensional (3D) facial data offer the potential to overcome the
difficulties caused by the variation of head pose and illumination in 2D
face recognition. In 3D face recognition, localisation of nose tip is
essential to face normalisation, face registration and pose correction
etc. Most of the existing methods of nose tip detection on 3D face deal
mainly with frontal or near-frontal poses or are rotation sensitive.
Many of them are training-based or model-based. In this study, a novel
method of nose tip detection is proposed. Using pose-invariant
differential surface features - high-order and low-order curvatures, it
can detect nose tip on 3D faces under various poses automatically and
accurately. Moreover, it does not require training and does not depend
on any particular model. Experimental results on GavabDB verify the
robustness and accuracy of the proposed method.},
author = {Li, Ye and Wang, YingHui and Wang, BingBo and Sui, LianSheng},
doi = {10.1049/iet-cvi.2014.0070},
issn = {1751-9632},
journal = {IET COMPUTER VISION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {1},
pages = {75--84},
title = {{Nose tip detection on three-dimensional faces using pose-invariant differential surface features}},
volume = {9},
year = {2015}
}
@article{ISI:000440122900009,
abstract = {The surface quality of three-dimensional (3-D) curved surfaces is one of
the most important factors that can directly influence the performance
of the final product. This paper presents a systematic approach for
detection and monitoring of defects on 3-D curved surfaces based on
high-density point cloud data. Firstly, an algorithm to remove outliers
and a boundary recognition algorithm are proposed to divide the entire
3-D curved surface including millions of measured points into multiple
sub-regions. Secondly, two new evaluation indexes based on wavelet
packet entropy and normal vector are explored to represent the features
of the multiple sub-regions to determine whether the sub-regions are
out-of-limit (OOL) of specifications. Thirdly, three quality parameters
representing quality characteristics of a curved surface are presented
and their values are calculated based on the clusters of OOL
sub-regions. Finally, three individual control charts are presented to
monitor the three quality parameters. As long as any quality parameter
is out of the control range, the manufacturing process of the curved
surface is determined to be out-of-control (OOC). The results of a case
study show that the proposed approach can effectively identify the OOC
manufacturing process and detect defects on 3-D curved surfaces.},
author = {Huang, Delin and Du, Shichang and Li, Guilong and Zhao, Chen and Deng, Yafei},
doi = {10.1016/j.precisioneng.2018.03.001},
issn = {0141-6359},
journal = {PRECISION ENGINEERING-JOURNAL OF THE INTERNATIONAL SOCIETIES FOR PRECISION ENGINEERING AND NANOTECHNOLOGY},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
pages = {79--95},
title = {{Detection and monitoring of defects on three-dimensional curved surfaces based on high-density point cloud data}},
volume = {53},
year = {2018}
}
@article{ISI:000218884500001,
abstract = {In this paper the pivotal contribution of the authors is to recognize
the 3D face images from range images in the unconstrained environment
i.e. under varying illumination, pose as well as occlusion that are
considered to be the most challenging task in the domain of face
recognition. During this investigation, face images have been normalized
in terms of pose registration as well as occlusion restoration using
ERFI (Energy Range Face Image) model. 3D face images are inherently
illumination invariant due its point-based representation of data along
three axes. Here, other than quantitative analysis, a subjective
analysis is also carried out. However, synthesized datasets have been
accomplished to investigate the performance of recognition rate from
Frav3D and Bosphorus databases using SIFT and SURF like features.
Moreover, weighted fusion of these individual feature sets is also done.
Later these feature sets have been classified by K-NN and Sequence
Matching Technique and achieved maximum recognition rates of 99.17{\%} and
98.81{\%} for Frav3D and GavabDB databases respectively.},
author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
doi = {10.4018/ijsda.2015040101},
issn = {2160-9772},
journal = {INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2},
pages = {1--20},
title = {{Illumination, Pose and Occlusion Invariant Face Recognition from Range Images Using ERFI Model}},
volume = {4},
year = {2015}
}
@inproceedings{8314888,
abstract = {This manuscript introduces a novel 3D face authentication system inspired from the advantageous capacities of Gabor-Edge filters. The approach studies 3D face difficulties such as expression variety, different rotations and exposure to illuminations. The proposed systems starts by preprocessing the 3D face images to resolve acquisition problems. Then, a filtering process is performed by implanting our 3D Gabor-Edge technique extended based on the classic 3D Gabor masks. The next step is to achieve the classification of facial features from the edge saliency by the artificial Neural Network Classifier (NNC). The evaluation of the adopted system is achieved by exporting common datasets from GavabDB database. Experimental results are reported to prove the high accuracy rates of our method compared to the recent researches in the same biometric field.},
annote = {05/06/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
05/06/2018 Exclu{\'{i}}do (etapa 1)},
author = {Torkhani, G and Ladgham, A and Sakly, A},
booktitle = {2017 18th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering (STA)},
doi = {10.1109/STA.2017.8314888},
keywords = {Authentication,Face,Face recognition,Feature extra,estela,etapa1,id511,ieeexplore,revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {estela,etapa1,id511,ieeexplore,revisao{\_}ieeexplore,revisao{\_}webofscience},
pages = {578--582},
title = {{3D Gabor-Edge filters applied to face depth images}},
year = {2017}
}
@inproceedings{ISI:000380445000302,
abstract = {Approach to construct 3D face model from an artist drawn sketch is an
area of interest in image processing from last few decades. It has
various application like police investigation, 3D cartoon modeling. From
an individual's sketch it is possible to construct 3D face model using
various techniques. To construct 3D face views, from the individuals
sketch steps required are 2D landmark detection, 3D landmark estimation,
surface and texture synthesis with reference to 3D morphable model. This
system is beneficial for the purpose of increasing the identification
accuracy of the persons whose photographs are not available. In this
paper, we tend to surveyed different techniques to construct 3D face
views from artist drawn sketch.},
annote = {International Conference on Industrial Instrumentation and Control, Coll
Engn Pune, Pune, INDIA, MAY 28-30, 2015},
author = {Shirke, Vikas D and Gawande, Ujwalla},
booktitle = {2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INSTRUMENTATION AND CONTROL (ICIC)},
isbn = {978-1-4799-7165-7},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE Pune Sect},
pages = {1582--1586},
title = {{Generation of 3D Face Views from Artist Drawn Sketch: A Review}},
year = {2015}
}
@inproceedings{ISI:000457843602003,
abstract = {Deep networks trained on millions of facial images are believed to be
closely approaching human-level performance in face recognition.
However, open world face recognition still remains a challenge.
Although, 3D face recognition has an inherent edge over its 2D
counterpart, it has not benefited from the recent developments in deep
learning due to the unavailability of large training as well as large
test datasets. Recognition accuracies have already saturated on existing
3D face datasets due to their small gallery sizes. Unlike 2D
photographs, 3D facial scans cannot be sourced from the web causing a
bottleneck in the development of deep 3D face recognition networks and
datasets. In this backdrop, we propose a method for generating a large
corpus of labeled 3D face identities and their multiple instances for
training and a protocol for merging the most challenging existing 3D
datasets for testing. We also propose the first deep CNN model designed
specifically for 3D face recognition and trained on 3.1 Million 3D
facial scans of 100K identities. Our test dataset comprises 1,853
identities with a single 3D scan in the gallery and another 31K scans as
probes, which is several orders of magnitude larger than existing ones.
Without fine tuning on this dataset, our network already outperforms
state of the art face recognition by over 10{\%}. We fine tune our network
on the gallery set to perform end-to-end large scale 3D face recognition
which further improves accuracy. Finally, we show the efficacy of our
method for the open world face recognition problem.},
annote = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Salt Lake City, UT, JUN 18-23, 2018},
author = {Gilani, Syed Zulqarnain and Mian, Ajmal},
booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2018.00203},
isbn = {978-1-5386-6420-9},
issn = {1063-6919},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {IEEE; CVF; IEEE Comp Soc},
pages = {1896--1905},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Learning from Millions of 3D Scans for Large-scale 3D Face Recognition}},
year = {2018}
}
@article{ISI:000462540400122,
abstract = {Latest advances of deep learning paradigm and 3D imaging systems have
raised the necessity for more complete datasets that allow exploitation
of facial features such as pose, gender or age. In our work, we propose
a new facial dataset collected with an innovative RGB-D multi-camera
setup whose optimization is presented and validated. 3DWF includes 3D
raw and registered data collection for 92 persons from low-cost RGB-D
sensing devices to commercial scanners with great accuracy. 3DWF
provides a complete dataset with relevant and accurate visual
information for different tasks related to facial properties such as
face tracking or 3D face reconstruction by means of annotated density
normalized 2K clouds and RGB-D streams. In addition, we validate the
reliability of our proposal by an original data augmentation method from
a massive set of face meshes for facial landmark detection in 2D domain,
and by head pose classification through common Machine Learning
techniques directed towards proving alignment of collected data.},
author = {Quintana, Marcos and Karaoglu, Sezer and Alvarez, Federico and {Manuel Menendez}, Jose and Gevers, Theo},
doi = {10.3390/s19051103},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
number = {5},
title = {{Three-D Wide Faces (3DWF): Facial Landmark Detection and 3D Reconstruction over a New RGB-D Multi-Camera Dataset}},
volume = {19},
year = {2019}
}
@inproceedings{ISI:000446968900007,
abstract = {In the past 10 years, Soft-Biometrics recognition using 3D face has
become prevailing, with many successful research works developed. In
contrast, the usage of facial parts for Soft-Biometrics recognition
remains less investigated. In particular, the nasal shape contains rich
information for demographic perception. They are usually free from
hair/glasses occlusions, and stay robust to facial expressions, which
are challenging issues 3D face analysis. In this work, we propose the
idea of 3D nasal Soft-Biometrics recognition. To this end, the simple 3D
coordinates features are derived from the radial curves representation
of the 3D nasal shape. With the 466 earliest scans of FRGCv2 dataset
(mainly neutral), we achieved 91{\%} gender (Male/Female) and 94{\%}
ethnicity (Asian/Non-asian) classification rates in 10-fold
cross-validation. It demonstrates the richness of the nasal shape in
presenting the two Soft-Biometrics, and the effectiveness of the
proposed recognition scheme. The performances are further confirmed by
more rigorous cross-dataset experiments, which also demonstrates the
generalization ability of propose approach. When experimenting on the
whole FRGCv2 dataset (40{\%} are expressive), comparable recognition
performances are achieved, which confirms the general knowledge that the
nasal shape stays robust during facial expressions.},
annote = {6th International Workshop on Representations, Analysis and Recognition
of Shape and Motion from Imaging Data (RFMI), Sidi Bou Said, TUNISIA,
OCT 27-29, 2016},
author = {Xia, Baiqiang},
booktitle = {REPRESENTATIONS, ANALYSIS AND RECOGNITION OF SHAPE AND MOTION FROM IMAGING DATA},
doi = {10.1007/978-3-319-60654-5_7},
editor = {{BenAmor, B and Chaieb, F and Ghorbel, F}},
isbn = {978-3-319-60654-5; 978-3-319-60653-8},
issn = {1865-0929},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {75--83},
series = {Communications in Computer and Information Science},
title = {{3D Nasal Shape: A New Basis for Soft-Biometrics Recognition}},
volume = {684},
year = {2017}
}
@inproceedings{ISI:000418791700005,
abstract = {This paper proposes a unified facial motion tracking and expression
recognition framework for monocular video. For retrieving facial motion,
an online weight adaptive statistical appearance method is embedded into
the particle filtering strategy by using a deformable facial mesh model
served as an intermediate to bring input images into correspondence by
means of registration and deformation. For recognizing facial
expression, facial animation and facial expression are estimated
sequentially for fast and efficient applications, in which facial
expression is recognized by static anatomical facial expression
knowledge. In addition, facial animation and facial expression are
simultaneously estimated for robust and precise applications, in which
facial expression is recognized by fusing static and dynamic facial
expression knowledge. Experiments demonstrate the high tracking
robustness and accuracy as well as the high facial expression
recognition score of the proposed framework.},
annote = {23rd International Conference on MultiMedia Modeling (MMM), Reykjavik
Univ, Reykjavik, ICELAND, JAN 04-06, 2017},
author = {Yu, Jun},
booktitle = {MULTIMEDIA MODELING, MMM 2017, PT II},
doi = {10.1007/978-3-319-51814-5_5},
editor = {{Amsaleg, L and Gudmundsson, GP and Gurrin, C and Jonsson, BP and Satoh, S}},
isbn = {978-3-319-51814-5; 978-3-319-51813-8},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {50--62},
series = {Lecture Notes in Computer Science},
title = {{A Unified Framework for Monocular Video-Based Facial Motion Tracking and Expression Recognition}},
volume = {10133},
year = {2017}
}
@inproceedings{ISI:000380447200087,
abstract = {3D Head pose estimation using a hybrid pose estimation method is
discussed in this paper. The only equipment used is simple webcam that
is available on most computers and laptops today. The hybrid method
consists of tracking facial landmarks of face and using geometrical face
pose estimation to compute distances to estimate 3D position of head.
The pose estimation system works real time as it is ultimately will be
used for 2D-3D face recognition system. Actual data show reasonable
error for rotation along each axis (Yaw, Pitch and Roll) by using only
few facial landmarks.},
annote = {IEEE 2015 International Conference on Signal and Image Processing
Applications (ICSIPA), Kuala Lumpur, MALAYSIA, OCT 19-21, 2015},
author = {Goodarzi, Farhad and Saripan, M Iqbal},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING APPLICATIONS (ICSIPA)},
isbn = {978-1-4799-8996-6},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; ICSIPA; IEEE SIGNAL PROC SOC},
pages = {482--487},
title = {{Real time face pose estimation using geometrical features}},
year = {2015}
}
@article{ISI:000408370500007,
abstract = {The outcome for patients diagnosed with facial palsy has been shown to
be linked to rehabilitation. Dense 3D morphable models have been shown
within the computer vision to create accurate representations of human
faces even from single 2D images. This has the potential to provide
feedback to both the patient and medical expert dealing with the
rehabilitation plan. It is proposed that a framework for the creation
and measuring of patient facial movement consisting of a hybrid 2D
facial landmark fitting technique which shows better accuracy in testing
than current methods and 3D model fitting.},
author = {Storey, Gary and Jiang, Richard and Bouridane, Ahmed},
doi = {10.1049/htl.2017.0023},
issn = {2053-3713},
journal = {HEALTHCARE TECHNOLOGY LETTERS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {4},
pages = {145--148},
title = {{Role for 2D image generated 3D face models in the rehabilitation of facial palsy}},
volume = {4},
year = {2017}
}
@article{ISI:000414762100022,
abstract = {Facial landmark localization is important to many facial recognition and
analysis tasks, such as face attributes analysis, head pose estimation,
3D face modeling, and facial expression analysis. In this paper, we
propose a new approach to localizing landmarks in facial image by deep
convolutional neural network (DCNN). We make two enhancements on the CNN
to adapt it to the feature localization task as follows. First, we
replace the commonly used max pooling by depth-wise convolution to
obtain better localization performance. Second, we define a response map
for each facial points as a 2D probability map indicating the presence
likelihood, and train our model with a KL divergence loss. To obtain
robust localization results, our approach first takes the expectations
of the response maps of enhanced CNN and then applies auto-encoder model
to the global shape vector, which is effective to rectify the outlier
points by the prior global landmark configurations. The proposed ECNN
method achieves 5.32{\%} mean error on the experiments on the 300-W
dataset, which is comparable to the state-of-the-art performance on this
standard benchmark, showing the effectiveness of our methods. (C) 2017
Elsevier B.V. All rights reserved.},
author = {Deng, Weihong and Fang, Yuke and Xu, Zhenqi and Hu, Jiani},
doi = {10.1016/j.neucom.2017.07.052},
issn = {0925-2312},
journal = {NEUROCOMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
pages = {222--229},
title = {{Facial landmark localization by enhanced convolutional neural network}},
volume = {273},
year = {2018}
}
@article{ISI:000382679900012,
abstract = {A robust technique for recognition of 3D faces which performs well with
face images with various poses, expressions and occlusions. In this
method, the face images represented in 3D mesh format are smoothed using
trilinear interpolation and then converted to 2.5D image or range
images. Nose-tip which is the most prominent feature on human face is
detected first on the corner points selected by 3D Harris corner and
curvedness at those corner points. K-Means clustering is applied to
group those corner points in 2 groups. The cluster of points with larger
curvedness values represents the possible locations of nose-tip.
Nose-tip is finally localized using Mean-Gaussian curvature values of
the prospective corner points in that cluster. Using the nose-tip
location, other facial landmarks namely corners of the eyes and mouth
are located and a facial graph is generated. The dimensionality of 2.5D
feature space is that, depth values are stored at each (x, y) grid of
the 2.5D image, so a 3D face image uses some function to map the depth
value at any pixel position to the intensity with which that pixel will
be displayed. Here finally extracted features for each subject is of
dimensionality {\{}[{\}}1x21], taking into account the Euclidean distances in
three dimensional form between each feature points detected
automatically. Taking Euclidean distances between all pairs of landmark
points as features, face images are classified using Multilayer
Perceptron (MLP), as well as Support Vector Machines (SVM). Maximum
recognition rates of 75 and 87.5 {\%} have been obtained in case of
Bosphorus Databases, 62.5 and 87.5 {\%} in case of GavabDB databases, 75
and 87.5 {\%} in case of Frav3D Databases by Multilayer Perceptron and
Support Vector Machines respectively.},
author = {Bagchi, Parama and Bhattacharjee, Debotosh and Nasipuri, Mita},
doi = {10.1007/s11042-015-2835-7},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {18},
pages = {11059--11096},
title = {{A robust analysis, detection and recognition of facial features in 2.5D images}},
volume = {75},
year = {2016}
}
@inproceedings{ISI:000390782003008,
abstract = {3D face recognition with partial occlusions is a highly challenging
problem. In this paper, we propose a novel radial string representation
and matching approach to recognize 3D facial scans in the presence of
partial occlusions. Here we encode 3D facial surfaces into an indexed
collection of radial strings emanating from the nosetips and Dynamic
Programming (DP) is then used to measure the similarity between two
radial strings. In order to address the recognition problems with
partial occlusions, a partial matching mechanism is established in our
approach that effectively eliminates those occluded parts and finds the
most discriminative parts during the matching process. Experimental
results on the Bosphorus database demonstrate that the proposed approach
yields superior performance on partially occluded data.},
annote = {23rd IEEE International Conference on Image Processing (ICIP), Phoenix,
AZ, SEP 25-28, 2016},
author = {Yu, Xun and Gao, Yongsheng and Zhou, Jun},
booktitle = {2016 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4673-9961-6},
issn = {1522-4880},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers, Signal Proc Soc},
pages = {3016--3020},
series = {IEEE International Conference on Image Processing ICIP},
title = {{3D FACE RECOGNITION UNDER PARTIAL OCCLUSIONS USING RADIAL STRINGS}},
year = {2016}
}
@inproceedings{ISI:000380405100160,
abstract = {This work proposes a new algorithm for 3D face recognition. The
algorithm uses 3D shape data without color or texture information and
exploits local curvature information which is a measure with high
discriminant capability and robust to deformations such as rotation and
scaling. In order to reduce high dimensionality of typical face surfaces
our approach uses a conformal parameterization, preserving angles of
original faces and simplifies the correspondence problem. Experimental
results are presented and discussed using CASIA and Gavab databases.},
annote = {International Conference on Computational Science and Computational
Intelligence (CSCI), Las Vegas, NV, DEC 07-09, 2015},
author = {{Adriana Echeagaray-Patron}, Beatriz and Miramontes-Jaramillo, Daniel and Kober, Vitaly},
booktitle = {2015 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND COMPUTATIONAL INTELLIGENCE (CSCI)},
doi = {10.1109/CSCI.2015.133},
editor = {{Arabnia, HR and Deligiannidis, L and Tran, QN}},
isbn = {978-1-4673-9795-7},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Amer Council on Sci {\&} Educ},
pages = {843--844},
title = {{Conformal parameterization and curvature analysis for 3D facial recognition}},
year = {2015}
}
@article{7932891,
abstract = {Face analysis from 2D images and videos is a central task in many multimedia applications. Methods developed to this end perform either face recognition or facial expression recognition, and in both cases results are negatively influenced by variations in pose, illumination, and resolution of the face. Such variations have a lower impact on 3D face data, which has given the way to the idea of using a 3D morphable model as an intermediate tool to enhance face analysis on 2D data. In this paper, we propose a new approach for constructing a 3D morphable shape model (called DL-3DMM) and show our solution can reach the accuracy of deformation required in applications where fine details of the face are concerned. For constructing the model, we start from a set of 3D face scans with large variability in terms of ethnicity and expressions. Across these training scans, we compute a point-topoint dense alignment, which is accurate also in the presence of topological variations of the face. The DL-3DMM is constructed by learning a dictionary of basis components on the aligned scans. The model is then fitted to 2D target faces using an efficient regularized ridge-regression guided by 2D/3D facial landmark correspondences in order to generate pose-normalized face images. Comparison between the DL-3DMM and the standard PCA-based 3DMM demonstrates that in general a lower reconstruction error can be obtained with our solution. Application to action unit detection and emotion recognition from 2D images and videos shows competitive results with state of the art methods on two benchmark datasets.},
annote = {23/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
23/04/2018 Exclu{\'{i}}do (etapa 1)
04/05/2018 Revisado (etapa 1)},
author = {Ferrari, Claudio and Lisanti, Giuseppe and Berretti, Stefano and Bimbo, Alberto Del},
doi = {10.1109/TMM.2017.2707341},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {emotion recognition,estela,etapa1,face recognition,id174,ieeexplore,image enhance,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {estela,etapa1,id174,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {dec},
number = {12},
pages = {2666--2679},
title = {{A Dictionary Learning-Based 3D Morphable Shape Model}},
url = {http://ieeexplore.ieee.org/document/7932891/},
volume = {19},
year = {2017}
}
@inproceedings{8122665,
abstract = {3D face recognition is a popular research area due to its vast application in biometrics and security. Local feature-based methods gain importance in the recent years due to their robustness under degradation conditions. In this paper, a novel high-order local pattern descriptor in combination with sparse representation based classifier (SRC) is proposed for expression robust 3D face recognition. 3D point clouds are converted to depth maps after preprocessing. Multi-directional derivatives are applied in spatial space to encode the depth maps based on the local derivative pattern (LDP) scheme. Directional pattern features are calculated according to local derivative variations. Since LDP computes spatial relationship of neighbors in a local region, it extracts distinct information from the depth map. Multiscale depth-LDP is presented as a novel descriptor for 3D face recognition. The descriptor is employed along with the SRC to increase the range data distinctiveness. A histogram on the derivative pattern creates a spatial feature descriptor that represents the distinctive micro-patterns from 3D data. We evaluate the proposed algorithm on two famous 3D face databases, FRGC v2.0 and Bosphorus. The experimental results demonstrate that the proposed approach achieves acceptable performance under facial expression.},
annote = {15/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
15/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Soltanpour, Sima and Wu, Q. M. Jonathan},
booktitle = {2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
doi = {10.1109/SMC.2017.8122665},
isbn = {978-1-5386-1645-1},
keywords = {emotion recognition,etapa1,face recognition,feature extra,gil,id54,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,gil,id54,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {oct},
pages = {560--565},
publisher = {IEEE},
title = {{Multiscale depth local derivative pattern for sparse representation based 3D face recognition}},
url = {http://ieeexplore.ieee.org/document/8122665/},
year = {2017}
}
@article{Xia:2015:CFA:2774262.2774465,
abstract = {Although human face averageness and symmetry are valuable clues in social perception (such as attractiveness, masculinity/femininity, and healthy/ sick), in the literature of facial attribute recognition, little consideration has been given to them. In this work, we propose to study the morphological differences between male and female faces by analyzing the averageness and symmetry of their 3D shapes. In particular, we address the following questions: (i) is there any relationship between gender and face averageness/symmetry? and (ii) if this relationship exists, which specific areas on the face are involved? To this end, we propose first to capture densely both the face shape averageness (AVE) and symmetry (SYM) using our Dense Scalar Field (DSF), which denotes the shooting directions of geodesics between facial shapes. Then, we explore such representations by using classical machine learning techniques, the Feature Selection (FS) methods and Random Forest (RF) classification algorithm. Experiments conducted on the FRGCv2 dataset show that a significant relationship exists between gender and facial averageness/symmetry when achieving a classification rate of 93.7{\%} on the 466 earliest scans of subjects (mainly neutral) and 92.4{\%} on the whole FRGCv2 dataset (including facial expressions).},
address = {New York, NY, USA},
annote = {18/06/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
18/05/2018 Exclu{\'{i}}do (etapa 1)},
author = {Xia, Baiqiang and {Ben Amor}, Boulbaba and Drira, Hassen and Daoudi, Mohamed and Ballihi, Lahoucine},
doi = {10.1016/j.patcog.2014.09.021},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {3D Face,Dense scalar field,Face averageness,Face symmetry,Feature selection,Gender classification,Random Forest,acm,estela,etapa1,id483,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {acm,estela,etapa1,id483,revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
number = {3},
pages = {746--758},
publisher = {Elsevier Science Inc.},
title = {{Combining face averageness and symmetry for 3D-based gender classification}},
volume = {48},
year = {2015}
}
@inproceedings{ISI:000455228100045,
abstract = {Depth information has been proven useful for face recognition. However,
existing depth-image-based face recognition methods still suffer from
noisy depth values and varying poses and expressions. In this paper, we
propose a novel method for normalizing facial depth images to frontal
pose and neutral expression and extracting robust features from the
normalized depth images. The method is implemented via two deep
convolutional neural networks (DCNN), normalization network (Net(N)) and
feature extraction network (Net(F)). Given a facial depth image, Net(N)
first converts it to an HHA image, from which the 3D face is
reconstructed via a DCNN. Net(N) then generates a pose-and-expression
normalized (PEN) depth image from the reconstructed 3D face. The PEN
depth image is finally passed to Net(F), which extracts a robust feature
representation via another DCNN for face recognition. Our preliminary
evaluation results demonstrate the superiority of the proposed method in
recognizing faces of arbitrary poses and expressions with depth images.},
annote = {13th Chinese Conference on Biometric Recognition (CCBR), Urumqi, PEOPLES
R CHINA, AUG 11-12, 2018},
author = {Feng, Ziqing and Zhao, Qijun},
booktitle = {BIOMETRIC RECOGNITION, CCBR 2018},
doi = {10.1007/978-3-319-97909-0_45},
editor = {{Zhou, J and Wang, Y and Sun, Z and Jia, Z and Feng, J and Shan, S and Ubul, K and Guo, Z}},
isbn = {978-3-319-97909-0; 978-3-319-97908-3},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Chinese Assoc Artificial Intelligence; Chinese Acad Sci, Inst Automat; Springer; Xinjiang Univ},
pages = {418--427},
series = {Lecture Notes in Computer Science},
title = {{Robust Face Recognition with Deeply Normalized Depth Images}},
volume = {10996},
year = {2018}
}
@article{ISI:000433431400011,
abstract = {One of the major challenges encountered by current face recognition
techniques lies in the difficulties of handling varying poses, i.e.,
recognition of faces in arbitrary in-depth rotations. The face image
differences caused by rotations are often larger than the inter-person
differences used in distinguishing identities. Face recognition across
pose, on the other hand, has great potentials in many applications
dealing with uncooperative subjects, in which the full power of face
recognition being a passive biometric technique can be implemented and
utilized. Extensive efforts have been put into the research toward
pose-invariant face recognition in recent years and many prominent
approaches have been proposed. However, several issues in face
recognition across pose still remain open, such as lack of understanding
about subspaces of pose variant images, problem intractability in 3D
face modelling, complex face surface reflection mechanism, etc. This
paper provides a critical survey of researches on image-based face
recognition across pose. The existing techniques are comprehensively
reviewed and discussed. They are classified into different categories
according to their methodologies in handling pose variations. Their
strategies, advantages/disadvantages and performances are elaborated. By
generalizing different tactics in handling pose variations and
evaluating their performances, several promising.},
author = {Ullah, Faizan and Shah, Sabir and Shah, Dilawar and Abdusalam and Ali, Shujaat},
doi = {10.4108/eai.13-4-2018.154477},
issn = {2032-9407},
journal = {EAI ENDORSED TRANSACTIONS ON SCALABLE INFORMATION SYSTEMS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {16},
title = {{Protocol for Systematic Literature Review of Face Recognition in Uncontrolled Environment}},
volume = {4},
year = {2018}
}
@article{ISI:000369518500015,
abstract = {In this work, we present a robust face authentication approach merging
multiple descriptors and exploiting both 3D and 2D information. First,
we correct the heads rotation in 3D by iterative closest point
algorithm, followed by an efficient preprocessing phase. Then, we
extract different features namely: multi-scale local binary patterns
(MSLBP), novel statistical local features (SLF), Gabor wavelets, and
scale invariant feature transform (SIFT). The principal component
analysis followed by enhanced fisher linear discriminant model is used
for dimensionality reduction and classification. Finally, fusion at the
score level is carried out using two-class support vector machines.
Extensive experiments are conducted on the CASIA 3D faces database. The
evaluation of individual descriptors clearly showed the superiority of
the proposed SLF features. In addition, applying the (3D + 2D)
multimodal score level fusion, the best result is obtained by combining
the SLF with the MSLBP + SIFT descriptor yielding in an equal error rate
of 0.98{\%} and a recognition rate of RR = 97.22 {\%}.},
author = {Ouamane, A and Belahcene, M and Benakcha, A and Bourennane, S and Taleb-Ahmed, A},
doi = {10.1007/s11760-014-0712-x},
issn = {1863-1703},
journal = {SIGNAL IMAGE AND VIDEO PROCESSING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {129--137},
title = {{Robust multimodal 2D and 3D face authentication using local feature fusion}},
volume = {10},
year = {2016}
}
@article{Ratyal2015241,
abstract = {In this paper we present a novel pose and expression invariant approach for 3D face registration based on intrinsic coordinate system characterized by nose tip, horizontal nose plane and vertical symmetry plane of the face. It is observed that distance of nose tip from 3D scanner is reduced after pose correction which is presented as a quantifying heuristic for proposed registration scheme. In addition, motivated by the fact that a single classifier cannot be generally efficient against all face regions, a two tier ensemble classifier based 3D face recognition approach is presented which employs Principal Component Analysis (PCA) for feature extraction and Mahalanobis Cosine (MahCos) matching score for classification of facial regions with weighted Borda Count (WBC) based combination and a re-ranking stage. The performance of proposed approach is corroborated by extensive experiments performed on two databases: GavabDB and FRGC v2.0, confirming effectiveness of fusion strategies to improve performance. {\^{A}}{\textcopyright} 2015 Elsevier Ltd. All rights reserved.},
annote = {cited By 5
24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (Etapa 1)
24/04/2018 Exclu{\'{i}}do (Etapa 1)},
author = {Ratyal, N I and Taj, I A and Bajwa, U I and Sajid, M},
doi = {10.1016/j.compeleceng.2015.06.007},
journal = {Computers and Electrical Engineering},
keywords = {acm,artur,etapa1,id183,isi,revisao{\_}scopus,revisao{\_}webofscience,robson,scopus},
mendeley-tags = {acm,artur,etapa1,id183,isi,revisao{\_}scopus,revisao{\_}webofscience,robson,scopus},
pages = {241--255},
title = {{3D face recognition based on pose and expression invariant alignment}},
volume = {46},
year = {2015}
}
@article{ISI:000356105100042,
abstract = {To reconstruct 3D face from single monocular image, this paper proposes
an approach which comprises three steps. First, a set of 3D facial
features is recovered from 2D features extracted from the image. The
features are recovered by solving equations derived from a regularized
scaled orthogonal projection. The regularization is achieved by a global
shape constraint exploiting a prior reference 3D facial shape. Second,
we warp a high-resolution reference 3D face, using both recovered 3D
features and local shape constraint at each model points. Last,
realistic 3D face is obtained through texture synthesis. Compared with
existing approach, the proposed feature recovery method has higher
accuracy, and it is robust to facial pose variation appeared on the
given image. Moreover, the model warping method based on local shape
constraints can warp a high-resolution reference 3D face using few 3D
features more reasonably and accurately. The proposed approach generates
realistic 3D face with impressive visual effect. (C) 2014 Elsevier B.V.
All rights reserved.},
author = {Zhang, Jian and Tao, Dapeng and Bian, Xiangjuan and Zhan, Xiaosi},
doi = {10.1016/j.neucom.2014.08.039},
issn = {0925-2312},
journal = {NEUROCOMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {C},
pages = {1535--1543},
title = {{Monocular face reconstruction with global and local shape constraints}},
volume = {149},
year = {2015}
}
@article{ISI:000454941500027,
abstract = {Contemporary face recognition system is often based on either 2D
(texture) or 3D (texture + shape) face modality. An alternative modality
that utilizes range (depth) facial images, namely 2.5D face recognition
emerges. In this paper, we propose a 2.5D face descriptor that based on
the Regional Covariance Matrix (RCM), a powerful means of feature fusion
technique and a novel classifier dubbed Random Maxout Extreme Learning
Machine (RMELM). The RCM of interest is constructed based on the
Principal Component Analysis (PCA) filters responses of facial texture
and/or range image, wherein the PCA filters are learned from a two-layer
PCA network. The RMELM is an ELM variant where the activation function
is based on the locally linear maxout function, in place of typical
global non-linear functions in ELM. Since the RCM is a special case of
symmetric positive definite matrix that resides on the Tensor manifold;
a gap exists in between RCM and RMELM, which is a vector-based
classifier. To bridge the gap, we flatten the manifold by transforming
the RCM to a feature vector via a matrix logarithm operator.
Experimental results from two public 3D face databases, FRGC v2.O
database and Gavab database, validated our proposed method is promising
in 2.5D face recognition. (C) 2018 Elsevier B.V. All rights reserved.},
author = {Chong, Lee Ying and Ong, Thian Song and Teoh, Andrew Beng Jin},
doi = {10.1016/j.asoc.2018.11.024},
issn = {1568-4946},
journal = {APPLIED SOFT COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {358--372},
title = {{Feature fusions for 2.5D face recognition in Random Maxout Extreme Learning Machine}},
volume = {75},
year = {2019}
}
@article{ISI:000395521200012,
abstract = {Recognizing construction assets (e.g.,materials, equipment, labor) from
point cloud data of construction environments provides essential
information for engineering and management applications including
progress monitoring, safety management, supply-chain management, and
quality control. This study introduces a novel principal axes descriptor
(PAD) for construction-equipment classification from point cloud data.
Scattered as-is point clouds are first processed with downsampling,
segmentation, and clustering steps to obtain individual instances of
construction equipment. A geometric descriptor consisting of dimensional
variation, occupancy distribution, shape profile, and plane counting
features is then calculated to encode three-dimensional (3D)
characteristics of each equipment category. Using the derived features,
machine learning methods such as k-nearest neighbors and support vector
machine are employed to determine class membership among major
construction-equipment categories such as backhoe loader, bulldozer,
dump truck, excavator, and front loader. Construction-equipment
classification with the proposed PAD was validated using computer-aided
design (CAD)-generated point clouds as training data and laser-scanned
point clouds from an equipment yard as testing data. The recognition
performance was further evaluated using point clouds from a construction
site as well as a pose variation data set. PAD was shown to achieve a
higher recall rate and lower computation time compared to competing 3D
descriptors. The results indicate that the proposed descriptor is a
viable solution for construction-equipment classification from point
cloud data. (C) 2016 American Society of Civil Engineers.},
author = {Chen, Jingdao and Fang, Yihai and Cho, Yong K and Kim, Changwan},
doi = {10.1061/(ASCE)CP.1943-5487.0000628},
issn = {0887-3801},
journal = {JOURNAL OF COMPUTING IN CIVIL ENGINEERING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
number = {2},
title = {{Principal Axes Descriptor for Automated Construction-Equipment Classification from Point Clouds}},
volume = {31},
year = {2017}
}
@article{ISI:000379752600018,
abstract = {Facial expression is an important channel for human nonverbal
communication. This paper presents a novel and effective approach to
automatic 3D/4D facial expression recognition based on the muscular
movement model (MMM). In contrast to most of existing methods, the MMM
deals with such an issue in the viewpoint of anatomy. It first
automatically segments the input 3D face (frame) by localizing the
corresponding points within each muscular region of the reference using
iterative closest normal point. A set of features with multiple
differential quantities, including coordinate, normal, and shape index
values, are then extracted to describe the geometry deformation of each
segmented region. Meanwhile, we analyze the importance of these muscular
areas, and a score level fusion strategy is exploited to optimize their
weights by the genetic algorithm in the learning step. The support
vector machine and the hidden Markov model are finally used to predict
the expression label in 3D and 4D, respectively. The experiments are
conducted on the BU-3DFE and BU-4DFE databases, and the results achieved
clearly demonstrate the effectiveness of the proposed method.},
author = {Zhen, Qingkai and Huang, Di and Wang, Yunhong and Chen, Liming},
doi = {10.1109/TMM.2016.2557063},
issn = {1520-9210},
journal = {IEEE TRANSACTIONS ON MULTIMEDIA},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
number = {7},
pages = {1438--1450},
title = {{Muscular Movement Model-Based Automatic 3D/4D Facial Expression Recognition}},
volume = {18},
year = {2016}
}
@inproceedings{ISI:000454739700033,
abstract = {Majority of the face recognition algorithms use query faces captured
from uncontrolled, in the wild, environment. Because of cameras' limited
capabilities, it is common for these captured facial images to be
blurred or low resolution. Super resolution algorithms are therefore
crucial in improving the resolution of such images especially when the
image size is small and enlargement is required. This paper aims to
demonstrate the effect of one of the state-of-the-art algorithms in the
field of image super resolution. To demonstrate the functionality of the
algorithm, various before and after 3D face alignment cases are provided
using the images from the Labeled Faces in the Wild (lfw) dataset.
Resulting images are subject to test on a closed set recognition
protocol using unsupervised algorithms with high dimensional extracted
features. The inclusion of super resolution algorithm resulted in
significant improvement in recognition rate over recently reported
results obtained from unsupervised algorithms on the same dataset.},
annote = {IEEE Applied Imagery Pattern Recognition Workshop (AIPR), Washington,
DC, OCT 10-12, 2017},
author = {ElSayed, Ahmed and Mahmood, Ausif and Sobh, Tarek},
booktitle = {2017 IEEE APPLIED IMAGERY PATTERN RECOGNITION WORKSHOP (AIPR)},
isbn = {978-1-5386-1235-4},
issn = {1550-5219},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
series = {IEEE Applied Imagery Pattern Recognition Workshop},
title = {{Effect of Super Resolution on High Dimensional Features for Unsupervised Face Recognition in the Wild}},
year = {2017}
}
@article{ISI:000384279700004,
abstract = {The state-of-the-art methods in classifying 3-D representation of the
face involve challenges in extracting representative features directly
from the large volume of facial data. These methods mostly ignore the
effect of pose distortions on 3-D facial data and entail heavy
computations as well as manual processing steps. This work proposes a
novel Frenet frame-based generalized space curve representation method
for 3-D pose-invariant face and facial expression recognition and
classification. Three-dimensional facial curves are extracted from
either frontal or synthetically posed 3-D facial data to derive the
proposed Frenet frame-based features. A mathematical framework shows the
proof of pose invariance property for the features. The effectiveness of
the proposed method is evaluated in two recognition tasks: 3-D face
recognition (3D-FR) and 3-D facial expression recognition (3D-FER) using
benchmarked 3-D datasets. The proposed framework yields 96{\%} rank-I
recognition rate for 3D-FR and 91.4{\%} area under ROC curves for six
basic 3D-FER. The performance evaluation also shows that the proposed
mathematical framework yields pose-invariant 3D-FR and 3D-FER for a wide
range of pose angles. This pose invariance property of the Frenet
frame-based features alleviates the need for an expensive 3-D face
registration in the preprocessing step, which, in turn, enables a faster
processing time. The evaluation results further suggest that the
proposed method is not only computationally efficient and versatile, but
also offers competitive performance when compared with the existing
state-of-the-art methods reported for either 3D-FR or 3D-FER.},
author = {Samad, Manar D and Iftekharuddin, Khan M},
doi = {10.1109/THMS.2016.2515602},
issn = {2168-2291},
journal = {IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {4},
pages = {522--533},
title = {{Frenet Frame-Based Generalized Space Curve Representation for Pose-Invariant Classification and Recognition of 3-D Face}},
volume = {46},
year = {2016}
}
@article{ISI:000416263200014,
abstract = {This paper presents a novel and efficient deep fusion convolutional
neural network (DF-CNN) for multimodal 2D+3D facial expression
recognition (FER). DF-CNN comprises a feature extraction subnet, a
feature fusion subnet, and a softmax layer. In particular, each textured
three-dimensional (3D) face scan is represented as six types of 2D
facial attribute maps (i.e., geometry map, three normal maps, curvature
map, and texture map), all of which are jointly fed into DF-CNN for
feature learning and fusion learning, resulting in a highly concentrated
facial representation (32-dimensional). Expression prediction is
performed by two ways: 1) learning linear support vector machine
classifiers using the 32-dimensional fused deep features, or 2) directly
performing softmax prediction using the six-dimensional expression
probability vectors. Different from existing 3D FER methods, DF-CNN
combines feature learning and fusion learning into a single end-to-end
training framework. To demonstrate the effectiveness of DF-CNN, we
conducted comprehensive experiments to compare the performance of DF-CNN
with handcrafted features, pre-trained deep features, fine-tuned deep
features, and state-of-the-art methods on three 3D face datasets (i.e.,
BU-3DFE Subset I, BU-3DFE Subset II, and Bosphorus Subset). In all
cases, DF-CNN consistently achieved the best results. To the best of our
knowledge, this is the first work of introducing deep CNN to 3D FER and
deep learning-based feature level fusion for multimodal 2D+3D FER.},
author = {Li, Huibin and Sun, Jian and Xu, Zongben and Chen, Liming},
doi = {10.1109/TMM.2017.2713408},
issn = {1520-9210},
journal = {IEEE TRANSACTIONS ON MULTIMEDIA},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {12},
pages = {2816--2831},
title = {{Multimodal 2D+3D Facial Expression Recognition With Deep Fusion Convolutional Neural Network}},
volume = {19},
year = {2017}
}
@article{ISI:000433517100005,
abstract = {With the success of emerging RGB-D cameras such as the Kinect sensor,
combining the shape (depth) and texture information to improve the
quality of recognition became a trend among computer vision researchers.
In this work, we address the problem of face classification in the
context of RGB images and depth data. Inspired by the psychological
results for human face perception, this article focuses on (i) finding
out which facial parts are most effective at making the difference for
some social aspects of face perception (gender, ethnicity, and emotional
state), (ii) determining the optimal decision by combining the decision
rendered by the individual parts, and (iii) extracting the promising
features from RGB-D faces to exploit all the potential that this data
provide. Experimental results on EurecomKinect Face and CurtinFaces
databases show that the proposed approach improves the recognition
quality in many use cases.},
author = {Azzakhnini, Safaa and Ballihi, Lahoucine and Aboutajdine, Driss},
doi = {10.1145/3152125},
issn = {1551-6857},
journal = {ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {1, S},
title = {{Combining Facial Parts For Learning Gender, Ethnicity, and Emotional State Based on RGB-D Information}},
volume = {14},
year = {2018}
}
@inproceedings{ISI:000425239602072,
abstract = {Face datasets are a fundamental tool to analyze the performance of face
recognition algorithms. However, the accuracy achieved on current
benchmark datasets is saturated. Although multiple face datasets have
been published recently, they only focus on the number of samples and
lack diversity on facial appearance factors, such as pose and
illumination. In addition, while 3D data have been demonstrated improved
face recognition accuracy by a significant margin, only a few 3D face
datasets provide high quality 2D and 3D data. In this paper, we
introduce a new and challenging dataset, called UHDB31, which not only
allows direct measurement of the influence of pose, illumination, and
resolution on face recognition but also facilitates different
experimental configurations with both 2D and 3D data. We conduct a
series of experiments with various face recognition algorithms and point
out how far they are from solving the face recognition problem under
pose, illumination, and resolution variation. The dataset is publicly
available and free for research use(1).},
annote = {16th IEEE International Conference on Computer Vision (ICCV), Venice,
ITALY, OCT 22-29, 2017},
author = {Le, Ha A and Kakadiaris, Ioannis A},
booktitle = {2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW 2017)},
doi = {10.1109/ICCVW.2017.300},
isbn = {978-1-5386-1034-3},
issn = {2473-9936},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IEEE Comp Soc},
pages = {2555--2563},
series = {IEEE International Conference on Computer Vision Workshops},
title = {{UHDB31: A Dataset for Better Understanding Face Recognition across Pose and Illumination Variation}},
year = {2017}
}
@inproceedings{ISI:000400012304105,
abstract = {Multilinear models are widely used to represent the statistical
variations of 3D human faces as they decouple shape changes due to
identity and expression. Existing methods to learn a multilinear face
model degrade if not every person is captured in every expression, if
face scans are noisy or partially occluded, if expressions are
erroneously labeled, or if the vertex correspondence is inaccurate.
These limitations impose requirements on the training data that
disqualify large amounts of available 3D face data from being usable to
learn a multilinear model. To overcome this, we introduce the first
framework to robustly learn a multilinear model from 3D face databases
with missing data, corrupt data, wrong semantic correspondence, and
inaccurate vertex correspondence. To achieve this robustness to
erroneous training data, our framework jointly learns a multilinear
model and fixes the data. We evaluate our framework on two publicly
available 3D face databases, and show that our framework achieves a data
completion accuracy that is comparable to state-of-the-art tensor
completion methods. Our method reconstructs corrupt data more accurately
than state-of-the-art methods, and improves the quality of the learned
model significantly for erroneously labeled expressions.},
annote = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Seattle, WA, JUN 27-30, 2016},
author = {Bolkart, Timo and Wuhrer, Stefanie},
booktitle = {2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2016.531},
isbn = {978-1-4673-8851-1},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE Comp Soc; Comp Vis Fdn},
pages = {4911--4919},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{A Robust Multilinear Model Learning Framework for 3D Faces}},
year = {2016}
}
@inproceedings{ISI:000406538600016,
abstract = {In this paper, we present a flexible camera calibration for pose
normalization to accomplish a pose-invariant face recognition. The
accuracy of calibration can be easily influenced by errors of landmark
detection or various shapes of different faces and expressions. By
jointly using RANSAC and facial unique characters, we explore a flexible
calibration method to achieve a more accurate camera calibration and
pose normalization for face images. Our proposed method is able to
eliminate noisy facial landmarks and retain the ones which best match
the undeformable 3D face model. The experimental results show that our
method improves the accuracy of pose-invariant face recognition,
especially for the faces with unsatisfied landmark detection, variant
shapes, and exaggerated expressions.},
annote = {7th Chinese Conference on Pattern Recognition (CCPR), Chengdu, PEOPLES R
CHINA, NOV 05-07, 2016},
author = {Shao, Xiaohu and Cheng, Cheng and Liu, Yanfei and Zhou, Xiangdong},
booktitle = {PATTERN RECOGNITION (CCPR 2016), PT I},
doi = {10.1007/978-981-10-3002-4_16},
editor = {{Tan, T and Li, X and Chen, X and Zhou, J and Yang, J and Cheng, H}},
isbn = {978-981-10-3002-4; 978-981-10-3001-7},
issn = {1865-0929},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {191--200},
series = {Communications in Computer and Information Science},
title = {{Pose-Invariant Face Recognition Based on a Flexible Camera Calibration}},
volume = {662},
year = {2016}
}
@inproceedings{ISI:000413240500077,
abstract = {This paper proposes a 3D face recognition approach based on facial pose
estimation, which is robust to large pose variations in the
unconstrained scene. Deep learning method is used to facial pose
estimation, and the generation of partial MARS (Multimodal fAce and eaR
Spherical) map reduces the probability of feature points appearing in
the deformed region. Then we extract the features from the depth and
texture maps. Finally, the matching scores from two types of maps should
be calculated by Bayes decision to generate the final result. In the
large pose variations, the recognition rate of the method in this paper
is 94.6{\%}. The experimental results show that our approach has superior
performance than the existing methods used on the MARS map, and has
potential to deal with 3D face recognition in unconstrained scene.},
annote = {6th International Conference on Pattern Recognition Applications and
Methods (ICPRAM), Porto, PORTUGAL, FEB 24-26, 2017},
author = {Zhang, Tingting and Mu, Zhichun and Li, Yihang and Liu, Qing and Zhang, Yi},
booktitle = {ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS},
doi = {10.5220/0006244206330637},
editor = {{DeMarsico, M and DiBaja, GS and Fred, A}},
isbn = {978-989-758-222-6},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {633--637},
title = {{3D Face and Ear Recognition based on Partial MARS Map}},
year = {2017}
}
@article{ISI:000365181400013,
abstract = {In this paper, a novel method is proposed for unconstrained pose
invariant face recognition from only an image in a gallery. A 3D face is
initially reconstructed using only a 2D frontal image. Then, for each
person in the gallery, a Triplet Collaborative Dictionary Matrix (TCDM)
is created from all face poses by rotating the 3D reconstructed models
and extracting features in rotated face. Each TCDM is subsequently
rendered based On triplet angles of face poses. Finally, the
classification is performed by Collaborative Representation
Classification (CRC) with Regularized Least Square (RLS). Promising
results were acquired to handle pose changes on the FERET, LFW and video
face databases compared to state-of-the-art methods in pose-invariant
face recognition. (C) 2015 Elsevier B.V. All rights reserved.},
author = {Moeini, Ali and Faez, Karim and Moeini, Hossein},
doi = {10.1016/j.patrec.2015.08.012},
issn = {0167-8655},
journal = {PATTERN RECOGNITION LETTERS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {1},
pages = {83--89},
title = {{Unconstrained pose-invariant face recognition by a triplet collaborative dictionary matrix}},
volume = {68},
year = {2015}
}
@inproceedings{ISI:000380500900562,
abstract = {Facial expressions contain a lot of information about the feelings of a
human. It plays an important role in human-computer interaction. In this
paper, entropy based feature selection method applied to 3D facial
feature distances is presented for a facial expression recognition
system classifying the expressions into 6 basic classes based on
3-Dimensional (3D) face geometry. Our previous work on entropy based
feature selection has been improved by employing 3D feature distances
between the 83 points on the face as facial features. 3D distances are
more robust to rotations of the face and involve more accurate
information than 3D feature positions that are used in our previous
work. Entropy is applied in order to rank the feature distances for
feature selection. The system is tested on BU-3DFE database in person
independent manner and provides encouraging recognition rates.},
annote = {23nd Signal Processing and Communications Applications Conference (SIU),
Inonu Univ, Malatya, TURKEY, MAY 16-19, 2015},
author = {Yurtkan, Kamil and Soyel, Hamit and Demirel, Hasan},
booktitle = {2015 23RD SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE (SIU)},
isbn = {978-1-4673-7386-9},
issn = {2165-0608},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Dept Comp Engn {\&} Elect {\&} Elect Engn; Elect {\&} Elect Engn; Bilkent Univ},
pages = {2322--2325},
series = {Signal Processing and Communications Applications Conference},
title = {{ENTROPY DRIVEN FEATURE SELECTION FOR FACIAL EXPRESSION RECOGNITION BASED ON 3-D FACIAL FEATURE DISTANCES}},
year = {2015}
}
@article{ISI:000438224000004,
abstract = {Face recognition is a vastly researched topic in the field of computer
vision. A lot of work have been done for facial recognition in two
dimensions and three dimensions. The amount of work done with face
recognition invariant of image processing attacks is very limited. This
paper presents a total of three classes of image processing attacks on
face recognition system, namely image enhancement attacks, geometric
attacks and the image noise attacks. The well-known machine learning
techniques have been used to train and test the face recognition system
using two different databases namely Bosphorus Database and University
of Milano Bicocca three-dimensional (3D) Face Database (UMBDB). Three
classes of classification models, namely discriminant analysis, support
vector machine and k-nearest neighbor along with ensemble techniques
have been implemented. The significance of machine learning techniques
has been mentioned. The visual verification has been done with multiple
image processing attacks.},
author = {Sharma, Sahil and Kumar, Vijay},
doi = {10.1142/S0217984918502123},
issn = {0217-9849},
journal = {MODERN PHYSICS LETTERS B},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jul},
number = {19},
title = {{Performance evaluation of 2D face recognition techniques under image processing attacks}},
volume = {32},
year = {2018}
}
@article{ISI:000352441700012,
abstract = {Three-dimensional surface technologies particularly close range
photogrammetry and optical surface scanning have recently advanced into
affordable, flexible and accurate techniques. Forensic postmortem
investigation as performed on a daily basis, however, has not yet fully
benefited from their potentials. In the present paper, we tested two
approaches to 3D external body documentation - digital camera-based
photogrammetry combined with commercial Agisoft PhotoScan (R) software
and stereophotogrammetry-based Vectra H1 (R), a portable handheld
surface scanner. In order to conduct the study three human subjects were
selected, a living person, a 25-year-old female, and two forensic cases
admitted for postmortem examination at the Department of Forensic
Medicine, Hradec Kralove, Czech Republic (both 63-year-old males), one
dead to traumatic, self-inflicted, injuries (suicide by hanging), the
other diagnosed with the heart failure.
All three cases were photographed in 3608 manner with a Nikon 7000
digital camera and simultaneously documented with the handheld scanner.
In addition to having recorded the pre-autopsy phase of the forensic
cases, both techniques were employed in various stages of autopsy. The
sets of collected digital images (approximately 100 per case) were
further processed to generate point clouds and 3D meshes. Final 3D
models (a pair per individual) were counted for numbers of points and
polygons, then assessed visually and compared quantitatively using ICP
alignment algorithm and a cloud point comparison technique based on
closest point to point distances.
Both techniques were proven to be easy to handle and equally laborious.
While collecting the images at autopsy took around 20 min, the
post-processing was much more time-demanding and required up to 10 h of
computation time. Moreover, for the full-body scanning the
post-processing of the handheld scanner required rather time-consuming
manual image alignment. In all instances the applied approaches produced
high-resolution photorealistic, real sized or easy to calibrate 3D
surface models. Both methods equally failed when the scanned body
surface was covered with body hair or reflective moist areas. Still, it
can be concluded that single camera close range photogrammetry and
optical surface scanning using Vectra H1 scanner represent relatively
low-cost solutions which were shown to be beneficial for postmortem body
documentation in forensic pathology. (C) 2015 Elsevier Ireland Ltd. All
rights reserved.},
author = {Urbanova, Petra and Hejna, Petr and Jurda, Mikolas},
doi = {10.1016/j.forsciint.2015.03.005},
issn = {0379-0738},
journal = {FORENSIC SCIENCE INTERNATIONAL},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {77--86},
title = {{Testing photogrammetry-based techniques for three-dimensional surface documentation in forensic pathology}},
volume = {250},
year = {2015}
}
@inproceedings{8227850,
abstract = {Face recognition remains a challenge today as recognition performance is strongly affected by variability such as illumination, expressions and poses. In this work we apply Convolutional Neural Networks (CNNs) on the challenging task of both 2D and 3D face recognition. We constructed two CNN models, namely CNN-1 (two convolutional layers) and CNN-2 (one convolutional layer) for testing on 2D and 3D dataset. A comprehensive parametric study of two CNN models on face recognition is represented in which different combinations of activation function, learning rate and filter size are investigated. We find that CNN-2 has a better accuracy performance on both 2D and 3D face recognition. Our experimental results show that an accuracy of 85.15{\%} was accomplished using CNN-2 on depth images with FRGCv2.0 dataset (4950 images with 557 objectives). An accuracy of 95{\%} was achieved using CNN-2 on 2D raw image with the AT{\&}T dataset (400 images with 40 objectives). The results indicate that the proposed CNN model is capable to handle complex information from facial images in different dimensions. These results provide valuable insights into further application of CNN on 3D face recognition.},
annote = {26/04 em processo
26/04 exclus{\~{a}}o},
author = {Hu, H and Shah, S A A and Bennamoun, M and Molton, M},
booktitle = {TENCON 2017 - 2017 IEEE Region 10 Conference},
doi = {10.1109/TENCON.2017.8227850},
keywords = {etapa1,face recognition,feature extraction,feedforward ne,id314,ieeexplore,poly,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,id314,ieeexplore,poly,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
pages = {132--133},
title = {{2D and 3D face recognition using convolutional neural network}},
year = {2017}
}
@inproceedings{ISI:000354613300002,
abstract = {Face recognition based on machine vision has achieved great advances and
been widely used in the various fields. However, there are some
challenges on the face recognition, such as facial pose, variations in
illumination, and facial expression. So, this paper gives the recent
advances in 3D face recognition. 3D face recognition approaches are
categorized into four groups: minutiae approach, space transform
approach, geometric features approach, model approach. Several typical
approaches are compared in detail, including feature extraction,
recognition algorithm, and the performance of the algorithm. Finally,
this paper summarized the challenge existing in 3D face recognition and
the future trend. This paper aims to help the researches majoring on
face recognition.},
annote = {6th International Conference on Graphic and Image Processing (ICGIP),
Beijing, PEOPLES R CHINA, OCT 24-26, 2014},
author = {Luo, Jing and Geng, Shu Ze and Xiao, Zhao Xia and Xiu, Chun Bo},
booktitle = {SIXTH INTERNATIONAL CONFERENCE ON GRAPHIC AND IMAGE PROCESSING (ICGIP 2014)},
doi = {10.1117/12.2178750},
editor = {{Wang, Y and Jiang, X and Zhang, D}},
isbn = {978-1-62841-558-2},
issn = {0277-786X},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {Int Assoc Comp Sci {\&} Informat Technol; Wuhan Univ},
series = {Proceedings of SPIE},
title = {{A Review of recent advances in 3D Face Recognition}},
volume = {9443},
year = {2015}
}
@inproceedings{ISI:000457881301017,
abstract = {Recently, automated emergency brake systems for pedestrian have been
commercialized. However, they cannot detect crossing pedestrians when
turning at intersections because the field of view is not wide enough.
Thus, we propose to utilize a surround view camera system becoming
popular by making it into stereo vision which is robust for the
pedestrian recognition. However, conventional stereo camera technologies
cannot be applied due to fisheye cameras and uncalibrated camera poses.
Thus we have created the new method to absorb difference of the
pedestrian appearance between cameras by machine learning for the stereo
vision. The method of stereo matching between image patches in each
camera image was designed by combining D-Brief and NCC with SVM. Good
generalization performance was achieved by it compared with individual
conventional algorithms. Furthermore, feature amounts of the point cloud
reconstructed by the stereo pairs are utilized with Random Forest to
discriminate pedestrians. The algorithm was evaluated for the actual
camera images of crossing pedestrians at various intersections, and
96.0{\%} of pedestrian tracking rate with high position detection accuracy
was achieved. They were compared with Faster R-CNN as the best pattern
recognition technique, and our proposed method indicated better
detection performance.},
annote = {21st IEEE International Conference on Intelligent Transportation Systems
(ITSC), Maui, HI, NOV 04-07, 2018},
author = {Akita, Tokihiko and Yamauchi, Yuji and Fujiyoshi, Hironobu},
booktitle = {2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)},
isbn = {978-1-7281-0323-5},
issn = {2153-0009},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; IEEE Intelligent Transportat Syst Soc},
pages = {1103--1108},
series = {IEEE International Conference on Intelligent Transportation Systems-ITSC},
title = {{Machine Learning-based Stereo Vision Algorithm for Surround View Fisheye Cameras}},
year = {2018}
}
@article{ISI:000376708000002,
abstract = {In this paper, we investigate the contribution of dynamic evolution of
3D faces to identity recognition. To this end, we adopt a subspace
representation of the flow of curvature-maps computed on 3D facial
frames of a sequence, after normalizing their pose. Such representation
allows us to embody the shape as well as its temporal evolution within
the same subspace representation. Dictionary learning and sparse coding
over the space of fixed-dimensional subspaces, called Grassmann
manifold, have been used to perform face recognition. We have conducted
extensive experiments on the BU-4DFE dataset. The obtained results of
the proposed approach provide promising results. (C) 2016 Elsevier Ltd.
All rights reserved.},
author = {Alashkar, Taleb and {Ben Amor}, Boulbaba and Daoudi, Mohamed and Berretti, Stefano},
doi = {10.1016/j.patcog.2016.03.013},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {21--30},
title = {{A Grassmann framework for 4D facial shape analysis}},
volume = {57},
year = {2016}
}
@article{ISI:000409284700029,
abstract = {This paper contributes to 3D facial synthesis by presenting a novel
method for parameterization using Landmark Point detection. The approach
presented aims at improving facial recognition even in varying facial
expressions, and missing data in 3D facial models. As such, the prime
objective was to develop an automatically embedded process that can
detect any frontal face in 3D face recognition systems, with face
segmentation and surface curvature information. Using the hybrid
interpolation method, experiments on facial landmarks were performed on
4950 images from Face Recognition Grand Challenge database (FRGC).
Distinctive facial landmarks from the nose-tips, Limits mouth and two
eye corners formed the statistical inputs for Iterative Closest Point
(ICP) in the Point Distribution Model (PDM). Performance or landmark
localization is reported by using percentage deviation from the mean 3D
profile. Localization results and estimated data on landmark locations
demonstrate that the method confirms its effectiveness for proposed
application. (C) 2016 Elsevier B.V. All rights reserved.},
author = {Boukamcha, Hamdi and Hallek, Mohamed and Smach, Fethi and Atri, Mohamed},
doi = {10.1016/j.jocs.2016.11.015},
issn = {1877-7503},
journal = {JOURNAL OF COMPUTATIONAL SCIENCE},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
pages = {340--348},
title = {{Automatic landmark detection and 3D Face data extraction}},
volume = {21},
year = {2017}
}
@article{ISI:000409180500015,
abstract = {In order to solve the problem of low recognition accuracy in later
period which is caused by the too few extracted parameters in the 3D
face recognition, and the incapable formation of completed point cloud
structure. An automatic iterative interpolation algorithm is proposed.
The new and more accurate 3D face data points are obtained by automatic
iteration. This algorithm can be used to restore the data point cloud
information of 3D facial feature in 2D images by means of facial
three-legged structure formed by 3D face and automatic interpolation.
Thus, it can realize to shape the 3D facial dynamic model which can be
recognized and has high saturability. Experimental results show that the
interpolation algorithm can achieve the complete the construction of
facial feature based on the facial feature after 3D dynamic
reconstruction, and the validity is higher.},
author = {Sui, Dan and Hou, Deheng and Duan, Xinyu},
doi = {10.1007/s11042-015-3233-x},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {19},
pages = {19575--19589},
title = {{An interpolation algorithm fitted for dynamic 3D face reconstruction}},
volume = {76},
year = {2017}
}
@inproceedings{ISI:000401510000148,
abstract = {Feature selection from facial regions is a well-known approach to
increase the performance of 2D image-based face recognition systems. In
case of 3D modality, the approach of region-based feature selection for
face recognition is relatively new. In this context, this paper presents
an approach to evaluate the discrimination power of different regions of
a 3D facial surface for its potential use in face recognition systems.
We propose the use of weighted average of unit normal vector on the
facial surface as the feature for region-based face recognition from 3D
point cloud data (PCD). The iterative closest point algorithm is
employed for the registration of segmented regions of facial point
clouds. A metric based on angular distance between normals is introduced
to indicate the similarity between two surfaces of same facial region.
Finally, the intra class correlation based discrimination score is
formulated to find out the key facial regions such as the eyes, nose,
and mouth that are significant while recognizing a person with facial
surface PCD.},
annote = {9th International Conference on Electrical and Computer Engineering
(ICECE), Dhaka, BANGLADESH, DEC 20-22, 2016},
author = {Amin, Rafiul and Shams, A Farhan and Rahman, S M Mahbubur and Hatzinakos, Dimitrios},
booktitle = {2016 9TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING (ICECE)},
isbn = {978-1-5090-2963-1},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Bangladesh Univ Engn {\&} Technol, Dept Elect {\&} Elect Engn; Inst Elect {\&} Elect Engineers; Energypac; PARADISE Cables ltd; BTCL; Summit Communicat Ltd; Dhaka Power Distribut Co Ltd},
pages = {602--605},
series = {International Conference on Computer and Electrical Engineering ICCEE},
title = {{Evaluation of Discrimination Power of Facial Parts from 3D Point Cloud Data}},
year = {2016}
}
@article{ISI:000419961800018,
abstract = {We describe an approach for synthesizing a three-dimensional (3-D) face
structure from an image or images of a human face taken at a priori
unknown poses using gender and ethnicity specific 3-D generic models.
The synthesis process starts with a generic model, which is personalized
as images of the person become available using preselected landmark
points that are tessellated to form a high-resolution triangular mesh.
From a single image, two of the three coordinates of the model are
reconstructed in accordance with the given image of the person, while
the third coordinate is sampled from the generic model, and the
appearance is made in accordance with the image. With multiple images,
all coordinates and appearance are reconstructed in accordance with the
observed images. This method allows for accurate pose estimation as well
as face identification in 3-D rendering of a difficult two-dimensional
(2-D) face recognition problem into a much simpler 3-D surface matching
problem. The estimation of the unknown pose is achieved using the
Levenberg-Marquardt optimization process. Encouraging experimental
results are obtained in a controlled environment with high-resolution
images under a good illumination condition, as well as for images taken
in an uncontrolled environment under arbitrary illumination with
low-resolution cameras. (C) 2017 SPIE and IS{\&}T},
author = {Liu, Zexi and Cohen, Fernand},
doi = {10.1117/1.JEI.26.6.063005},
issn = {1017-9909},
journal = {JOURNAL OF ELECTRONIC IMAGING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
number = {6},
title = {{Synthesis and identification of three-dimensional faces from image(s) and three-dimensional generic models}},
volume = {26},
year = {2017}
}
@inproceedings{ISI:000413813100460,
abstract = {In this study, an SVM-based system is proposed for the classification of
facial expressions that are represented in 3D. Distance based features
are used as a feature vector, which are determined by the distances
between the different key points on the image. Study was conducted on a
subset (Happy, sadness, surprise) of Bosphorus 3D Face Database. 9
different fiducial points arc used to calculate a total of 5 distance
features. SVM classification was performed with K-fold cross validation
thus mean classification performance of different training and test
clusters were determined. {\%}85 success rate has achieved as a result of
the expression analysis performed on the 3D facial scans.},
annote = {25th Signal Processing and Communications Applications Conference (SIU),
Antalya, TURKEY, MAY 15-18, 2017},
author = {Soylemez, Omer Famk and Ergen, Burhan and Soylemez, Nesrin Hark},
booktitle = {2017 25TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE (SIU)},
isbn = {978-1-5090-6494-6},
issn = {2165-0608},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Turk Telekom; Arcelik A S; Aselsan; ARGENIT; HAVELSAN; NETAS; Adresgezgini; IEEE Turkey Sect; AVCR Informat Technologies; Cisco; i2i Syst; Integrated Syst {\&} Syst Design; ENOVAS; FiGES Engn; MS Spektral; Istanbul Teknik Univ},
series = {Signal Processing and Communications Applications Conference},
title = {{A 3D Facial Expression Recognition System Based On SVM Classifier Using Distance Based Features}},
year = {2017}
}
@inproceedings{ISI:000352725200030,
abstract = {The analysis of individual trees is an important field of research in
the forest remote sensing community. While the current state-of-the-art
mostly focuses on the exploitation of optical imagery and airborne LiDAR
data, modern SAR sensors have not yet met the interest of the research
community in that regard. This paper describes how several critical
parameters of individual deciduous trees can be extraced from airborne
multi-aspect TomoSAR point clouds: First, the point cloud is segmented
by unsupervised mean shift clustering. Then ellipsoid models are fitted
to the points of each cluster. Finally, from these 3D ellipsoids the
geometrical tree parameters location, height and crown radius are
extracted. Evaluation with respect to a manually derived reference
dataset prove that almost 86{\%} of all trees are localized, thus
providing a promising perspective for further research towards
individual tree recognition from SAR data.},
annote = {Joint ISPRS Conference on Photogrammetric Image Analysis (PIA) and High
Resolution Earth Imaging for Geospatial Information (HRIGI), Technische
Univ Munchen, Munich, GERMANY, MAR 25-27, 2015},
author = {Shahzad, M and Schmitt, M and Zhu, X X},
booktitle = {PIA15+HRIGI15 - JOINT ISPRS CONFERENCE, VOL. I},
doi = {10.5194/isprsarchives-XL-3-W2-205-2015},
editor = {{Stilla, U and Heipke, C}},
issn = {2194-9034},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {W2},
organization = {ISPRS},
pages = {205--209},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{SEGMENTATION AND CROWN PARAMETER EXTRACTION OF INDIVIDUAL TREES IN AN AIRBORNE TOMOSAR POINT CLOUD}},
volume = {40-3},
year = {2015}
}
@article{ISI:000356107200001,
abstract = {Building information models (BIMs) are increasingly being applied
throughout a building's lifecycle for various applications, such as
progressive construction monitoring and defect detection, building
renovation, energy simulation, and building system analysis in the
Architectural, Engineering, Construction, and Facility Management
(AEC/FM) domains. In conventional approaches, as-is BIM is primarily
manually created from point clouds, which is labor-intensive, costly,
and time consuming. This paper proposes a method for automatically
extracting building geometries from unorganized point clouds. The
collected raw data undergo data downsizing, boundary detection, and
building component categorization, resulting in the building components
being recognized as individual objects and their visualization as
polygons. The results of tests conducted on three collected as-is
building data to validate the technical feasibility and evaluate the
performance of the proposed method indicate that it can simplify and
accelerate the as-is building model from the point cloud creation
process. (C) 2015 Elsevier B.V. All rights reserved.},
author = {Wang, Chao and Cho, Yong K and Kim, Changwan},
doi = {10.1016/j.autcon.2015.04.001},
issn = {0926-5805},
journal = {AUTOMATION IN CONSTRUCTION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {1--13},
title = {{Automatic BIM component extraction from point clouds of existing buildings for sustainability applications}},
volume = {56},
year = {2015}
}
@inproceedings{ISI:000374793400004,
abstract = {This paper proposes a 3D face recognition approach using sphere depth
image, which is robust to pose variations in unconstrained environments.
The input 3D face point clouds is first transformed into sphere depth
images, and then represented as a 3DLBP image to enhance the
distinctiveness of smooth and similar facial depth images. An improved
SIFT algorithm is applied in the following matching process. The
improved SIFT algorithm employs the learning to rank approach to select
the keypoints with higher stability and repeatability instead of
manually rule-based method used by the original SIFT algorithm. The
proposed face recognition method is evaluated on CASIA 3D face database.
And the experimental results show our approach has superior performance
than many existing methods for 3D face recognition and handles pose
variations quite well.},
annote = {10th Chinese Conference on Biometric Recognition (CCBR), Tianjin,
PEOPLES R CHINA, NOV 13-15, 2015},
author = {Wang, Hanchao and Mu, Zhichun and Zeng, Hui and Huang, Mingming},
booktitle = {BIOMETRIC RECOGNITION, CCBR 2015},
doi = {10.1007/978-3-319-25417-3_4},
editor = {{Yang, J and Yang, J and Sun, Z and Shan, S and Zheng, W and Feng, J}},
isbn = {978-3-319-25417-3; 978-3-319-25416-6},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Chinese Assoc Artificial Intelligence; Springer; Civil Aviat Univ China; Tianjin Univ Sci {\&} Technol; CASIA, Inst Intelligent Recognit},
pages = {27--34},
series = {Lecture Notes in Computer Science},
title = {{3D Face Recognition Using Local Features Matching on Sphere Depth Representation}},
volume = {9428},
year = {2015}
}
@inproceedings{ISI:000365181700036,
abstract = {In this paper, we present a novel and robust approach for 3D faces
registration based on Energy Range Face Image (ERFI). ERFI is the
frontal face model for the individual people from the database. It can
be considered as a mean frontal range face image for each person. Thus,
the total energy of the frontal range face images has been preserved by
ERFI. For registration purpose, an interesting point or a land mark,
which is the nose tip (or `pronasal') from face surface is extracted.
Then, this landmark is exploited to correct the oriented faces by
applying the 3D geometrical rotation technique with respect to the ERFI
model for registration purpose. During the error calculation phase,
Manhattan distance metric between the localized `pronasal' landmark on
face image and that of ERFI model is determined on Euclidian space. The
accuracy is quantified with selection of cut-points `T' on measured
Manhattan distances along yaw, pitch and roll. The proposed method has
been tested on Frav3D database and achieved 82.5{\%} accurate pose
registration.},
annote = {3rd International Conference on Frontiers in Intelligent Computing -
Theory and Applications (FICTA), Bhubaneswar, INDIA, NOV 14-15, 2014},
author = {Ganguly, Suranjan and Bhattacharjee, Debotosh and Nasipuri, Mita},
booktitle = {PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON FRONTIERS OF INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2014, VOL 2},
doi = {10.1007/978-3-319-12012-6_36},
editor = {{Satapathy, SC and Biswal, BN and Udgata, SK and Mandal, JK}},
isbn = {978-3-319-12012-6; 978-3-319-12011-9},
issn = {2194-5357},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Bhubaneswar Engn Coll; Anil Neerukonda Inst Technol {\&} Sci, CSI Student Branch},
pages = {323--333},
series = {Advances in Intelligent Systems and Computing},
title = {{Range Face Image Registration Using ERFI from 3D Images}},
volume = {328},
year = {2015}
}
@inproceedings{ISI:000380496400016,
abstract = {Even if, most of 2D face recognition approaches reached recognition rate
more than 90{\%} in controlled environment, current days face recognition
systems degrade their performance in case of uncontrolled environment
which includes pose variations, illumination variations, expression
variations and ageing effect etc. Inclusion of 3D face analysis gives an
age over 2D face recognition as they give vital informations such as 3D
shape, texture and depth which improve discrimination power of an
algorithm. In this paper, we have investigated different 3D face
recognition approaches that are robust to changes in facial expressions
and illumination variations. 2D-PCA and 2D-LDA approaches have been
extended to 3D face recognition because they can directly work on 2D
depth image matrices rather than 1D vectors without need for
transformations before feature extraction. In turn, this reduces storage
space and time required for computations. 2D depth image is extracted
from 3D face model and nose region from depth mapped image has been
detected as a reference point for cropping stage to convert model into a
standard size. Two Dimensional Principal Component Analysis (2D-PCA) and
Two Dimensional Linear Discriminant analysis (2D-LDA) are employed to
obtain feature vectors globally compared to feature vectors obtained
locally using PCA or LDA. Finally, euclidean distance classifier is
applied for comparison of extracted features. A set of experiments on
GavabDB 3D face database, which has 61 individuals in total,
demonstrated that 3D face recognition using 2D-LDA method has achieved
recognition accuracy of 93.3{\%} and EER of 8.96{\%} over database, which is
higher compared to 2D-PCA. So, more optimized performance has been
achieved using 2D-LDA for 3D face recognition analysis.},
annote = {5th Nirma University International Conference on Engineering (NUiCONE),
Ahmedabad, INDIA, NOV 26-28, 2015},
author = {Marvadi, Dhara and Joshi, Maulin and Paunwala, Chirag and Vora, Aarohi},
booktitle = {2015 5TH NIRMA UNIVERSITY INTERNATIONAL CONFERENCE ON ENGINEERING (NUICONE)},
isbn = {978-1-4799-9991-0},
issn = {2375-1282},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Nirma Univ, Inst Technol},
series = {Nirma University International Conference on Engineering},
title = {{Comparative Analysis of 3D Face Recognition Using 2D-PCA and 2D-LDA Approaches}},
year = {2015}
}
@article{ISI:000353152700010,
abstract = {A multiple inputs-driven realistic facial animation system based on 3-D
virtual head for human-machine interface is proposed. The system can be
driven independently by video, text, and speech, thus can interact with
humans through diverse interfaces. The combination of parameterized
model and muscular model is used to obtain a tradeoff between
computational efficiency and high realism of 3-D facial animation. The
online appearance model is used to track 3-D facial motion from video in
the framework of particle filtering, and multiple measurements, i.e.,
pixel color value of input image and Gabor wavelet coefficient of
illumination ratio image, are infused to reduce the influence of
lighting and person dependence for the construction of online appearance
model. The tri-phone model is used to reduce the computational
consumption of visual co-articulation in speech synchronized viseme
synthesis without sacrificing any performance. The objective and
subjective experiments show that the system is suitable for
human-machine interaction.},
author = {Yu, Jun and Wang, Zeng-Fu},
doi = {10.1109/TCYB.2014.2341737},
issn = {2168-2267},
journal = {IEEE TRANSACTIONS ON CYBERNETICS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {5},
pages = {977--988},
title = {{A Video, Text, and Speech-Driven Realistic 3-D Virtual Head for Human-Machine Interface}},
volume = {45},
year = {2015}
}
@inproceedings{ISI:000457843605038,
abstract = {This paper proposes an encoder-decoder network to disentangle shape
features during 3D face reconstruction from single 2D images, such that
the tasks of reconstructing accurate 3D face shapes and learning
discriminative shape features for face recognition can be accomplished
simultaneously. Unlike existing 3D face reconstruction methods, our
proposed method directly regresses dense 3D face shapes from single 2D
images, and tackles identity and residual (i.e., non-identity)
components in 3D face shapes explicitly and separately based on a
composite 3D face shape model with latent representations. We devise a
training process for the proposed network with a joint loss measuring
both face identification error and 3D face shape reconstruction error.
To construct training data we develop a method for fitting 3D morphable
model (3DMM) to multiple 2D images of a subject. Comprehensive
experiments have been done on MICC, BU3DFE, LFW and YTF databases. The
results show that our method expands the capacity of 3DMM for capturing
discriminative shape features and facial detail, and thus outperforms
existing methods both in 3D face reconstruction accuracy and in face
recognition accuracy.},
annote = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Salt Lake City, UT, JUN 18-23, 2018},
author = {Liu, Feng and Zhu, Ronghang and Zeng, Dan and Zhao, Qijun and Liu, Xiaoming},
booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2018.00547},
isbn = {978-1-5386-6420-9},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; CVF; IEEE Comp Soc},
pages = {5216--5225},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition}},
year = {2018}
}
@article{ISI:000410870200001,
abstract = {Three-dimensional (3D) facial modeling and stereo matching-based methods
are widely used for 3D facial reconstruction from 2D single-view and
multiple-view images. However, these methods cannot realistically
reconstruct 3D faces because they use insufficient numbers of
macro-level Facial Feature Points (FFPs). This paper proposes an
accurate and person-specific 3D facial reconstruction method that uses
ample numbers of macro and micro-level FFPs to enable coverage of all
facial regions of high resolution facial images. Comparisons of 3D
facial images reconstructed using the proposed method for ground-truth
3D facial images from the Bosphorus 3D database show that the method is
superior to a conventional Active Appearance Model-Structure from Motion
(AAM + SfM)-based method in terms of average 3D root mean square error
between the reconstructed and ground-truth 3D faces. Further, the
proposed method achieved outstanding accuracy in local facial regions
such as the cheek areas where extraction of FFPs is difficult for
existing methods. (C) 2017 Elsevier B.V. All rights reserved.},
author = {Jo, Jaeik and Kim, Hyunjun and Kim, Jaihie},
doi = {10.1016/jimavis.2017.05.001},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {1--9},
title = {{3D facial shape reconstruction using macro- and micro-level features from high resolution facial images}},
volume = {64},
year = {2017}
}
@article{ISI:000385945000006,
abstract = {This paper addresses the problem of facial landmark localization and
tracking from a single camera. We present a two-stage cascaded
deformable shape model to effectively and efficiently localize facial
landmarks with large head pose variations. In initialization stage, we
propose a group sparse optimized mixture model to automatically select
the most salient facial landmarks. By introducing 3D face shape model,
we apply procrustes analysis to provide pose-aware landmark
initialization. In landmark localization stage, the first step uses
mean-shift local search with constrained local model to rapidly approach
the global optimum. The second step uses component-wise active contours
to discriminatively refine the subtle shape variation. Our framework
simultaneously handles face detection, pose-robust landmark localization
and tracking in real time. Extensive experiments are conducted on both
laboratory environmental databases and face-in-the-wild databases. The
results reveal that our approach consistently outperforms
state-of-the-art methods for face alignment and tracking.},
author = {Yu, Xiang and Huang, Junzhou and Zhang, Shaoting and Metaxas, Dimitris N},
doi = {10.1109/TPAMI.2015.2509999},
issn = {0162-8828},
journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
number = {11},
pages = {2212--2226},
title = {{Face Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Model}},
volume = {38},
year = {2016}
}
@article{ISI:000397995100002,
abstract = {The use of multispectral cameras deployed on unmanned aerial vehicles
(UAVs) in land cover and vegetation mapping applications continues to
improve and receive increasing recognition and adoption by resource
management and forest survey practitioners. Comparisons of different
camera data and platform performance characteristics are an important
contribution in understanding the role and operational capability of
this technology. In this article, object-based classification accuracies
for different cover types and vegetation species of interest in central
Ontario were examined using data from three UAV-based multispectral
cameras. Five land-cover classes (forest, shrub, herbaceous, bare soil,
and built-up) were determined to be up to 95{\%} correct overall with
calibrated multispectral Parrot Sequoia digital camera data compared to
independent field observations. The levels of classification accuracy
decreased approximately 10-15{\%} when spectrally less capable
consumer-grade RGB sensors were used. Multispectral Parrot Sequoia
classification accuracy was approximately 89{\%} when more detailed
vegetation classes, including individual deciduous tree species, shrub
communities and agricultural crops, were analysed. Additional work is
suggested in the use of such UAV multispectral and point cloud data in
ash tree discrimination to support emerald ash borer infestation
detection and management, and in analysis of functional and structural
vegetation characteristics (e.g. leaf area index).},
annote = {Conference on Small Unmanned Aerial Systems (sUAS) for Environmental
Research, Univ Worcester, Worcester, ENGLAND, JUN, 2016},
author = {Ahmed, Oumer S and Shemrock, Adam and Chabot, Dominique and Dillon, Chris and Williams, Griffin and Wasson, Rachel and Franklin, Steven E},
doi = {10.1080/01431161.2017.1294781},
issn = {0143-1161},
journal = {INTERNATIONAL JOURNAL OF REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {8-10},
pages = {2037--2052},
title = {{Hierarchical land cover and vegetation classification using multispectral data acquired from an unmanned aerial vehicle}},
volume = {38},
year = {2017}
}
@article{ISI:000448786500006,
abstract = {Recently, 3D facial datasets are more easily available, and at the same
time, the research of 3D face becomes more and more important. One of
the most important research fields is 3D face registration, which plays
an important role in face recognition, face shape analysis, and face
animation. However, one of the most challenging issues in 3D face
registration is to obtain a unique mapping for faces with different
expression and landmark constraints. In this paper, we propose a novel
conformal mapping algorithm to deal with the 3D face registration.
Besides, the calculation is about harmonic energy, which makes our
method applicable to low-quality meshes. We begin with a harmonic
mapping, then minimize the harmonic energy by a specific boundary
condition on surfaces, and to obtain the conformal mapping, finally, we
use a landmark-constrained surface registration algorithm to register
faces. Numerical experiments on various surfaces demonstrate the
efficiency and robustness of our method.},
annote = {International Conference of Intelligence Computation and Evolutionary
Computation (ICEC), Wuhan, PEOPLES R CHINA, DEC 07-10, 2017},
author = {Qian, Kun and Su, Kehua and Zhang, Jialing and Li, Yinghua},
doi = {10.1002/cpe.4654},
institution = {Informat Technol {\&} Ind Engn Res Ctr},
issn = {1532-0626},
journal = {CONCURRENCY AND COMPUTATION-PRACTICE {\&} EXPERIENCE},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
number = {22, SI},
title = {{A 3D face registration algorithm based on conformal mapping}},
volume = {30},
year = {2018}
}
@inproceedings{ISI:000406056300054,
abstract = {Facial expression which carries rich information of body behavior is the
leading carrier of human affective and the symbol of intelligence. The
main purpose of this paper is to recognize 3D human facial expression.
The research in this paper includes the expression feature extraction
algorithm and fusion with different kinds of feature. To contain more
local texture feature information, we proposed a new feature of 3D
facial expression named Local Threshold Binary Pattern (LTBP) which
based on Local Binary Pattern (LBP). We calculate the difference of gray
value standard between neighboring pixels and the center pixel as a
threshold to binary instead of the traditional LBP operation which only
comparison of size between neighboring pixels and the center pixel.
After we get the LTBP feature, we fuse the LTBP and HOG (Histogram of
Oriented Gradient) features to get multi-feature fusion for 3D facial
expression recognition. Our algorithm of 3D facial expression
recognition comprises three steps: (1) extracting two sets of feature
vectors and establishing the correlation criterion function between the
two sets of feature vectors; (2) solving the two sets canonical
projective vectors and extracting their canonical correlation features
by the framework of canonical correlation analysis algorithm; (3) doing
feature fusion for classification by using proposed strategy. We have
performed comprehensive experiments on the BU-3DFE database which is
presently the largest available 3D face database. We have achieved
verification rates of more than 90{\%} for the 3D facial expression
recognition.},
annote = {13th IEEE International Conference on Signal Processing (ICSP), Chengdu,
PEOPLES R CHINA, NOV 06-10, 2016},
author = {An, Shu and Ruan, Qiuqi},
booktitle = {PROCEEDINGS OF 2016 IEEE 13TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING (ICSP 2016)},
editor = {{Baozong, Y and Qiuqi, R and Yao, Z and Gaoyun AN}},
isbn = {978-1-5090-1345-6},
issn = {2164-5221},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; Inst Engn {\&} Technol; Union Radio Sci Int; Chinese Inst Elect; Beijing Jiaotong Univ; Int Conf Signal Proc; IEEE Beijing Sect; IET Beijing Local Network; Natl Nat Sci Fdn China; CIE Signal Proc Soc; IEEE Signal Proc Soc Beijing Chapter; IEEE Comp Soc},
pages = {265--270},
series = {International Conference on Signal Processing},
title = {{3D Facial Expression Recognition Algorithm using Local Threshold Binary Pattern and Histogram of Oriented Gradient}},
year = {2016}
}
@inproceedings{ISI:000426951900017,
abstract = {In this paper, we address the problem of gender classification based on
facial images. The Speeded Up Robust Feature (SURF) algorithm
descriptors are used as features to built dictionaries and a multi-task
Sparse Representation Classification (SRC) is used as classifier to
determine the gender of an individual face. Our approach uses smaller
and compact dictionaries by removing the redundant atoms from the
constructed ones.
The feasibility of using the SURF on the shape index map for gender
classification is demonstrated through experimental investigation
conducted on FRGCv2 dataset. The proposed approach achieves 91.04 +/-
1.19{\%} of correct gender classification rate using only 5{\%} of the size
of the dictionary and 97.83 +/- 0.76 {\%} is obtained using 23{\%} of the
size of the dictionary.},
annote = {2nd International Conference on Bio-engineering for Smart Technologies
(BioSMART), Paris, FRANCE, AUG 30-SEP 01, 2017},
author = {Bentaieb, Samia and Ouamri, Abdelaziz and Keche, Mokhtar and Nait-Ali, Amine},
booktitle = {2017 2ND INTERNATIONAL CONFERENCE ON BIO-ENGINEERING FOR SMART TECHNOLOGIES (BIOSMART)},
editor = {{Naitali, A}},
isbn = {978-1-5386-0706-0},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IEEE France Sect; IEEE France Sect EMBS Chapterie; Univ Paris Creteil Val Marne; Lab Images Signaux Systeme Intelligents; ESME; Inst Mines Telecom, Telecom SudParis},
title = {{Gender Classification from 3D Face Images using Multi-Task Sparse Representation over Reduced Dictionary}},
year = {2017}
}
@inproceedings{ISI:000367310300024,
abstract = {Expression change is the major cause of local plastic deformation of the
facial surface. The intra-class differences with large expression change
somehow are larger than the inter-class differences as it's difficult to
distinguish the same individual with facial expression change. In this
paper, an expression-robust 3D face recognition method is proposed by
learning expression deformation model. The expression of the individuals
on the training set is modeled by principal component analysis, the main
components are retained to construct the facial deformation model. For
the test 3D face, the shape difference between the test and the neutral
face in training set is used for reconstructing the expression change by
the constructed deformation model. The reconstruction residual error is
used for face recognition. The average recognition rate on GavabDB and
self-built database reaches 85.1{\%} and 83{\%}, respectively, which shows
strong robustness for expression changes.},
annote = {7th International Conference on Graphic and Image Processing (ICGIP),
Singapore, SINGAPORE, OCT 23-25, 2015},
author = {Guo, Zhe and Liu, Shu and Wang, Yi and Lei, Tao},
booktitle = {SEVENTH INTERNATIONAL CONFERENCE ON GRAPHIC AND IMAGE PROCESSING (ICGIP 2015)},
doi = {10.1117/12.2228002},
editor = {{Wang, Y and Jiang, X}},
isbn = {978-1-5106-0058-4},
issn = {0277-786X},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {Wuhan Univ; Int Assoc Comp Sci {\&} Informat Technol},
series = {Proceedings of SPIE},
title = {{Learning Deformation Model for Expression-Robust 3D Face Recognition}},
volume = {9817},
year = {2015}
}
@inproceedings{ISI:000406996500085,
abstract = {In this work, we address the problem of human skeleton estimation when
multiple depth cameras are available. We propose a system that takes
advantage of the knowledge of the camera poses to create a collaborative
virtual depth image of the person in the scene which consists of points
from all the cameras and that represents the person in a frontal pose.
This depth image is fed as input to the open-source body part detector
in the Point Cloud Library. A further contribution of this work is the
improvement of this detector obtained by introducing two new components:
as a pre-processing, a people detector is applied to remove the
background from the depth map before estimating the skeleton, while an
alpha-beta tracking is added as a post-processing step for filtering the
obtained joint positions over time. The overall system has been proven
to effectively improve the skeleton estimation on two sequences of
people in different poses acquired from two first-generation Microsoft
Kinect.},
annote = {14th International Conference on Intelligent Autonomous Systems (IAS),
Shanghai Jiao Tong Univ, Shanghai, PEOPLES R CHINA, JUL 03-07, 2016},
author = {Carraro, Marco and Munaro, Matteo and Roitberg, Alina and Menegatti, Emanuele},
booktitle = {INTELLIGENT AUTONOMOUS SYSTEMS 14},
doi = {10.1007/978-3-319-48036-7_85},
editor = {{Chen, W and Hosoda, K and Menegatti, E and Shimizu, M and Wang, H}},
isbn = {978-3-319-48036-7; 978-3-319-48035-0},
issn = {2194-5357},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {1155--1167},
series = {Advances in Intelligent Systems and Computing},
title = {{Improved Skeleton Estimation by Means of Depth Data Fusion from Multiple Depth Cameras}},
volume = {531},
year = {2017}
}
@inproceedings{ISI:000380483800005,
abstract = {Being the most distinct feature point in 3D facial landmarks, nose tip
plays a significant role in 3D facial studies such as face detection,
face recognition, facial features extraction, face alignment, etc.
Successful detection of nose tip can facilitate many tasks of 3D facial
studies. In this paper, we propose a novel method to detect nose tip
robustly. The method is robust to noise, needs not training, can handle
large rotations and occlusions. To reduce computational cost, we first
remove small isolated regions from the input range image, then establish
scale-space by robust smoothing the preprocessed range image. In each
scale of the scale-space, the Multi-angle Energy (ME) of each point is
computed and sorted in descending order. Then the first. points in the
descending order list are obtained and hierarchical clustering method is
used to cluster these points. In the first h largest clusters, we can
find one point with the largest ME. For all scales of the scale-space,
we get a series of such points which are treated as nose tip candidates.
For these candidates, we apply hierarchical clustering again. In the
obtained largest cluster, we compute the mean value of ME. The ME of
nose tip will be closest to the mean value. We evaluate our method in
two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The
experimental results verify the robustness of our method with a high
nose tip detection rate.},
annote = {International Conference on 3D Imaging (IC3D), Liege, BELGIUM, DEC
14-15, 2015},
author = {Liu, Jian and Zhang, Quan and Zhang, Chen and Tang, Chaojing},
booktitle = {2015 INTERNATIONAL CONFERENCE ON 3D IMAGING (IC3D)},
isbn = {978-1-5090-1265-7},
issn = {2379-1772},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {The Inst of Elect and Elect Engn; Signal Proc Soc; 3D Stereo Media},
series = {International Conference on 3D Imaging},
title = {{ROBUST NOSE TIP DETECTION FOR FACE RANGE IMAGES BASED ON LOCAL FEATURES IN SCALE- SPACE}},
year = {2015}
}
@article{ISI:000428490900009,
abstract = {3D face recognition is an increasing popular modality for biometric
authentication, for example in the iPhoneX. Landmarking plays a
significant role in region based face recognition algorithms. The
accuracy and consistency of the landmarking will directly determine the
effectiveness of feature extraction and hence the overall recognition
performance. While surface normals have been shown to provide high
performing features for face recognition, their use in landmarking has
not been widely explored. To this end, a new 3D facial landmarking
algorithm based on thresholded surface normal maps is proposed, which is
applicable to widely used 3D face databases. The benefits of employing
surface normals are demonstrated for both facial roll and yaw rotation
calibration and nasal landmarks localization. Results on the Bosphorus,
FRGC and BU-3DFE databases show that the detected landmarks possess high
within class consistency and accuracy under different expressions. For
several key landmarks the performance achieved surpasses that of
state-of-the-art techniques and is also training free and
computationally efficient. The use of surface normals therefore provides
a useful representation of the 3D surface and the proposed landmarking
algorithm provides an effective approach to localising the key nasal
landmarks. (C) 2018 Elsevier Ltd. All rights reserved.},
author = {Gao, Jiangning and Evans, Adrian N},
doi = {10.1016/j.patcog.2018.01.011},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jun},
pages = {120--132},
title = {{Expression robust 3D face landmarking using thresholded surface normals}},
volume = {78},
year = {2018}
}
@article{ISI:000444795500006,
abstract = {In video surveillance, face recognition (FR) systems are employed to
detect individuals of interest appearing over a distributed network of
cameras. The performance of still-tovideo FR systems can decline
significantly because faces captured in unconstrained operational domain
(OD) over multiple video cameras have a different underlying data
distribution compared to faces captured under controlled conditions in
the enrollment domain with a still camera. This is particularly true
when individuals are enrolled to the system using a single reference
still. To improve the robustness of these systems, it is possible to
augment the reference set by generating synthetic faces based on the
original still. However, without the knowledge of the OD, many synthetic
images must be generated to account for all possible capture conditions.
FR systems may, therefore, require complex implementations and yield
lower accuracy when training on many less relevant images. This paper
introduces an algorithm for domain-specific face synthesis (DSFS) that
exploits the representative intra-class variation information available
from the OD. Prior to operation (during camera calibration), a compact
set of faces from unknown persons appearing in the OD is selected
through affinity propagation clustering in the captured condition space
(defined by pose and illumination estimation). The domain-specific
variations of these face images are then projected onto the reference
still of each individual by integrating an image-based face relighting
technique inside the 3-D reconstruction framework. A compact set of
synthetic faces is generated that resemble individuals of interest under
the capture conditions relevant to the OD. In a particular
implementation based on sparse representation classification, the
synthetic faces generated with the DSFS are employed to form a
cross-domain dictionary that accounts for structured sparsity, where the
dictionary blocks combine the original and synthetic faces of each
individual. Experimental results obtained with videos from the
Chokepoint and COX-S2V data sets reveal that augmenting the reference
gallery set of still-to-video FR systems using the proposed DSFS
approach can provide a significantly higher level of accuracy compared
with the state-of-the-art approaches, with only a moderate increase in
its computational complexity.},
author = {Mokhayeri, Fania and Granger, Eric and Bilodeau, Guillaume-Alexandre},
doi = {10.1109/TIFS.2018.2866295},
issn = {1556-6013},
journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
number = {3},
pages = {757--772},
title = {{Domain-Specific Face Synthesis for Video Face Recognition From a Single Sample Per Person}},
volume = {14},
year = {2019}
}
@article{ISI:000395116200010,
abstract = {Human face images are the basis not only for person recognition, but for
also identifying other attributes like gender, age, ethnicity, and
emotional states of a person. Therefore, face is an important biometric
identifier in the law enforcement and human-computer interaction (HCI)
systems. The 3D human face recognition is emerging as a significant
biometric technology. Research interest into 3D face recognition has
increased during recent years due to availability of improved 3D
acquisition devices and processing algorithms. A 3D face image is
represented by 3D meshes or range images which contain depth
information. In this paper, the objective is to propose a new 3D face
recognition method based on radon transform and symbolic factorial
discriminant analysis using KNN and SVM classifier with similarity and
dissimilarity measures, which are applied on 3D facial range images. The
experimentation is done using three publicly available databases,
namely, Bhosphorus, Texas and CASIA 3D face database. The experimental
results demonstrate the effectiveness of the proposed method.},
author = {Hiremath, Manjunatha and Hiremath, P S},
doi = {10.1142/S0218001417560067},
issn = {0218-0014},
journal = {INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {4},
title = {{3D Face Recognition Based on Symbolic FDA Using SVM Classifier with Similarity and Dissimilarity Distance Measure}},
volume = {31},
year = {2017}
}
@inproceedings{ISI:000392417700075,
abstract = {3D face was recently investigated for various applications, including
biometrics and diagnosis. Describing facial surface, i.e. how it bends
and which kinds of patches is composed by, is the aim of studies in Face
Analysis, whose ultimate goal is to identify which features could be
extracted from three-dimensional faces depending on the application. In
this study, we propose 54 novel geometrical descriptors for Face
Analysis. They are generated by composing primary-geometrical
descriptors such as mean. Gaussian. principal curvatures, shape index,
curvedness, and the coefficients of the fundamental forms. The new
descriptors were mapped on 217 facial depth maps and analysed in terms
of descriptiveness of facial shape and exploitability for localizing
landmark points. Automatic landmark extraction stands as the final aim
of this analysis. Results showed that the newly generated descriptors
are suitable to 3D face description and to support landmark localization
procedures.},
annote = {International Joint Conference on Mechanics, Design Engineering and
Advanced Manufacturing (JCM), Catania, ITALY, SEP 14-16, 2016},
author = {Marcolin, Federica and Violante, Maria Grazia and Moos, Sandro and Vezzetti, Enrico and Tornincasa, Stefano and Dagnes, Nicole and Speranza, Domenico},
booktitle = {ADVANCES ON MECHANICS, DESIGN ENGINEERING AND MANUFACTURING},
doi = {10.1007/978-3-319-45781-9_75},
editor = {{Eynard, B and Nigrelli, V and Oliveri, SM and PerisFajarnes, G and Rizzuti, S}},
isbn = {978-3-319-45781-9; 978-3-319-45780-2},
issn = {2195-4356},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Ateliers Interetablissements Productique Poles Resources Informatiques MECAnique; Design {\&} Methods Ind Engn Soc; Asociac Espanola Ingn Grafica; Univ Catania, Rapid Prototyping {\&} Geometr Modelling Lab; Univ Catania, Dept Elect, Elect {\&} Informat Engn},
pages = {747--756},
series = {Lecture Notes in Mechanical Engineering},
title = {{Three-dimensional face analysis via new geometrical descriptors}},
year = {2017}
}
@inproceedings{ISI:000454996700065,
abstract = {Meaningful facial parts can convey key cues for both facial action unit
detection and expression prediction. Textured 3D face scan can provide
both detailed 3D geometric shape and 2D texture appearance cues of the
face which are beneficial for Facial Expression Recognition (FER).
However, accurate facial parts extraction as well as their fusion are
challenging tasks. In this paper, a novel system for 3D FER is designed
based on accurate facial parts extraction and deep feature fusion of
facial parts. Experiments are conducted on the BU-3DFE database,
demonstrating the effectiveness of combing different facial parts,
texture and depth cues and reporting the state-of-the-art results in
comparison with all existing methods under the same setting.},
annote = {13th IEEE International Conference on Automatic Face {\&} Gesture
Recognition (FG), Xi an, PEOPLES R CHINA, MAY 15-19, 2018},
author = {Jan, Asim and Ding, Huaxiong and Meng, Hongying and Chen, Liming and Li, Huibin},
booktitle = {PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE {\&} GESTURE RECOGNITION (FG 2018)},
doi = {10.1109/FG.2018.00075},
isbn = {978-1-5386-2335-0},
issn = {2326-5396},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE Comp Soc; IEEE Biometr Council},
pages = {466--472},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{Accurate Facial Parts Localization and Deep Learning for 3D Facial Expression Recognition}},
year = {2018}
}
@inproceedings{ISI:000454996700029,
abstract = {Face alignment plays an important role for robust face recognition and
analysis applications in the wild. While a number of face alignment
methods are available, large-pose face alignment remains a very
challenging problem due to the ambiguity of facial keypoints in 2D face
images. Recent attempts to solve this problem via 3D model fitting show
more robustness against large poses and 2D ambiguity, but their accuracy
and speed are still limited. We propose a 3D reconstruction based method
to quickly and accurately detect 2D facial landmarks and estimate their
visibility. By designing a cascaded multi-task CNN model, we can
efficiently reconstruct the 3D face shape, together with pose estimation
as an auxiliary task. Finally, the landmarks on 3D shape are projected
to the 2D face image to get the 2D landmarks and their visibility.
Experimental results on the challenging 300W-LP, AFLW2000-3D, and AFLW
databases show that the proposed approach can be comparable with the
state-of-the-art methods and is able to run in real time (32ms per
image) on 3.4 GHz CPU.},
annote = {13th IEEE International Conference on Automatic Face {\&} Gesture
Recognition (FG), Xi an, PEOPLES R CHINA, MAY 15-19, 2018},
author = {Zhang, Gang and Han, Hu and Shan, Shiguang and Song, Xingguang and Chen, Xilin},
booktitle = {PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE {\&} GESTURE RECOGNITION (FG 2018)},
doi = {10.1109/FG.2018.00039},
isbn = {978-1-5386-2335-0},
issn = {2326-5396},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE Comp Soc; IEEE Biometr Council},
pages = {210--217},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{Face Alignment across Large Pose via MT-CNN based 3D Shape Reconstruction}},
year = {2018}
}
@article{ISI:000445204800002,
abstract = {Manually monitoring and documenting trees is labour intensive. Lidar
provides a possible solution for automatic tree-inventory generation.
Existing approaches for segmenting trees from original point cloud data
lack scalable and efficient methods that separate individual trees
sampled by different laser-scanning systems with sufficient quality
under all circumstances. In this study a new algorithm for efficient
individual tree delineation from lidar point clouds is presented and
validated. The proposed algorithm first resamples the points using
cuboid (modified voxel) cells. Consecutively connected cells are
accumulated by vertically traversing cell layers. Trees in close
proximity are identified, based on a novel cell-adjacency analysis. The
scalable performance of this algorithm is validated on airborne, mobile
and terrestrial laser-scanning point clouds. Validation against ground
truth demonstrates an improvement from 89{\%} to 94{\%} relative to a
state-of-the-art method while computation time is similar.
Resume La detection et la documentation manuelle des arbres est une
tache fastidieuse. Le lidar offre une solution possible pour
l'inventaire automatique des arbres. Les approches existantes pour la
segmentation des arbres dans des nuages bruts de points ne proposent pas
de methodes efficaces et adaptees a toutes les echelles pour separer des
arbres individuels echantillonnes par differents systemes lidar avec une
qualite acceptable en toute circonstance. Cette etude propose et valide
un nouvel algorithme pour la delimitation efficace d'arbres individuels
a partir de nuages de points lidar. L'algorithme propose commence par
reechantillonner les points dans des cellules cubiques (voxels), puis
regroupe les cellules connexes en traversant verticalement les couches
de cellules. Les arbres proches sont identifies grace a une nouvelle
analyse d'adjacence de cellules. La performance de cetalgorithme en
termes d'adaptabilite au changement d'echelle est validee a partir de
nuages de points issus de systemes laser a balayage aerien, mobile et
terrestre. Une validation basee sur des donnees de terrain de reference
fait etat d'une amelioration de 89{\%} a 94{\%} par rapport a des methodes
connues pour un temps de calcul comparable.
Zusammenfassung Eine uberwachung und Dokumentation von Baumen ist sehr
arbeitsaufwandig. Lidar bietet das Potential fur automatische
Bauminventur. Es gibt Ansatze zur Segmentierung von Baumen aus
Punktwolken, die allerdings noch nicht in der Lage sind, einzelne Baume
in Punktwolken verschiedener Lidar-Systeme zuverlassig und mit
ausreichender Qualitat unter vielfaltigen realen Bedingungen zu
separieren. Diese Studie stellt einen neuen Algorithmus zur effizienten
Erfassung von Baumen in Lidar-Punktwolken dar. Der Algorithmus bildet
Punkte mit Hilfe von quaderformigen (Voxel) Zellen um. Nacheinander
verbundene Zellen werden durch vertikale Traverse der Zellschichten
akkumuliert. Baume in nachster Nachbarschaft werden durch eine neuartige
Zellanalyse identifiziert. Der Vorteil der Skalierbarkeit des
Algorithmus wird an flugzeuggestutzten, mobilen und terrestrischen
Laserscanpunktwolken validiert. Anhand von Solldaten ist festzustellen,
dass bei gleicher Rechenzeit, eine Verbesserung von 89{\%} bis 94{\%} im
Vergleich zu aktuellen Verfahren erzielt werden kann.
Resumen Monitorizar y documentar manualmente arboles es un trabajo
intensivo. El lidar proporciona una posible solucion para la generacion
automatica del inventario de arboles. Los enfoques existentes para
segmentar arboles a partir originalmente de nubes de puntos lidar
carecen de metodos escalables y eficientes que separen arboles
individuales muestreados por diferentes sistemas lidar con calidad
suficiente bajo todas las circunstancias. En este estudio, se presenta y
valida un algoritmo nuevo para la delimitacion eficiente de arboles
individuales a partir de nubes de puntos lidar. El algoritmo propuesto
primero remuestrea los puntos usando celulas cuboides (voxels). Los
voxels adyacentes se acumulan atravesando verticalmente las capas de
voxels. Basados en un nuevo analisis de adyacencia de voxels se
identifican arboles que estan proximos. El rendimiento escalable de este
algoritmo se valida con nubes de puntos lidar aerotransportados, moviles
y terrestres. La validacion con verdad terreno demuestra una mejora del
89{\%} al 94{\%} en comparacion con un metodo de vanguardia, mientras que el
tiempo de calculo es similar.},
author = {Wang, Jinhu and Lindenbergh, Roderik and Menenti, Massimo},
doi = {10.1111/phor.12247},
issn = {0031-868X},
journal = {PHOTOGRAMMETRIC RECORD},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {163},
pages = {315--340},
title = {{Scalable individual tree delineation in 3D point clouds}},
volume = {33},
year = {2018}
}
@article{ISI:000400896700002,
abstract = {We present an approach for face recognition using synthesized
three-dimensional (3-D) shape information together with two-dimensional
(2-D) color in a deep convolutional neural network (DCNN). As 3-D facial
shape is hardly affected by the extrinsic 2-D texture changes caused by
illumination, make-up, and occlusions, it could provide more reliable
complementary features in harmony with the 2-D color feature in face
recognition. Unlike other approaches that use 3-D shape information with
the help of an additional depth sensor, our approach generates a
personalized 3-D face model by using only face landmarks in the 2-D
input image. Using the personalized 3-D face model, we generate a
frontalized 2-D color facial image as well as 3-D facial images (e.g., a
depth image and a normal image). In our DCNN, we first feed 2-D and 3-D
facial images into independent convolutional layers, where the low-level
kernels are successfully learned according to their own characteristics.
Then, we merge them and feed into higher-level layers under a single
deep neural network. Our proposed approach is evaluated with labeled
faces in the wild dataset and the results show that the error rate of
the verification rate at false acceptance rate 1{\%} is improved by up to
32.1{\%} compared with the baseline where only a 2-D color image is used.
(C) 2017 SPIE and IS{\&}T},
author = {Rhee, Seon-Min and Yoo, ByungIn and Han, Jae-Joon and Hwang, Wonjun},
doi = {10.1117/1.JEI.26.2.020502},
issn = {1017-9909},
journal = {JOURNAL OF ELECTRONIC IMAGING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
number = {2},
title = {{Deep neural network using color and synthesized three-dimensional shape for face recognition}},
volume = {26},
year = {2017}
}
@inproceedings{ISI:000389640300019,
abstract = {3D Morphable Face Models (3DMM) have been used in face recognition for
some time now. They can be applied in their own right as a basis for 3D
face recognition and analysis involving 3D face data. However their
prevalent use over the last decade has been as a versatile tool in 2D
face recognition to normalise pose, illumination and expression of 2D
face images. A 3DMM has the generative capacity to augment the training
and test databases for various 2D face processing related tasks. It can
be used to expand the gallery set for pose-invariant face matching. For
any 2D face image it can furnish complementary information, in terms of
its 3D face shape and texture. It can also aid multiple frame fusion by
providing the means of registering a set of 2D images. A key enabling
technology for this versatility is 3D face model to 2D face image
fitting. In this paper recent developments in 3D face modelling and
model fitting will be overviewed, and their merits in the context of
diverse applications illustrated on several examples, including pose and
illumination invariant face recognition, and 3D face reconstruction from
video.},
annote = {9th International Conference on Articulated Motion and Deformable
Objects (AMDO), Palma de Mallorca, SPAIN, JUL 13-15, 2016},
author = {Kittler, Josef and Huber, Patrik and Feng, Zhen-Hua and Hu, Guosheng and Christmas, William},
booktitle = {ARTICULATED MOTION AND DEFORMABLE OBJECTS},
doi = {10.1007/978-3-319-41778-3_19},
editor = {{Perales, FJ and Kittler, J}},
isbn = {978-3-319-41778-3; 978-3-319-41777-6},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {185--206},
series = {Lecture Notes in Computer Science},
title = {{3D Morphable Face Models and Their Applications}},
volume = {9756},
year = {2016}
}
@article{ISI:000401888600006,
abstract = {Tropical forests are a key component of the global carbon cycle, and
mapping their carbon density is essential for understanding human
influences on climate and for ecosystem-service-based payments for
forest protection. Discrete-return airborne laser scanning (ALS) is
increasingly recognised as a high-quality technology for mapping
tropical forest carbon, because it generates 3D point clouds of forest
structure from which aboveground carbon density (ACD) can be estimated.
Area-based models are state of the art when it comes to estimating ACD
from ALS data, but discard tree-level information contained within the
ALS point cloud. This paper compares area based and tree-centric models
for estimating ACD in lowland old-growth forests in Sabah, Malaysia.
These forests are challenging to map because of their immense height. We
compare the performance of (a) an area-based model developed by Asner
and Mascaro (2014), and used primarily in the neotropics hitherto, with
(b) a tree-centric approach that uses a new algorithm (itcSegment) to
locate trees within the ALS canopy height model, measures their heights
and crown widths, and calculates biomass from these dimensions. We find
that Asner and Mascaro's model needed regional calibration, reflecting
the distinctive structure of Southeast Asian forests. We also discover
that forest basal area is closely related to canopy gap fraction
measured by ALS, and use this finding to refine Asner and Mascaro's
model. Finally, we show that our tree-centric approach is less accurate
at estimating ACD than the best-performing area-based model (RMSE 18{\%}
vs 13{\%}). Tree-centric modelling is appealing because it is based on
summing the biomass of individual trees, but until algorithms can detect
understory trees reliably and estimate biomass from crown dimensions
precisely, areas-based modelling will remain the method of choice. (C)
2017 The Authors. Published by Elsevier Inc.},
author = {Coomes, David A and Dalponte, Michele and Jucker, Tommaso and Asner, Gregory P and Banin, Lindsay F and Burslem, David F R P and Lewis, Simon L and Nilus, Reuben and Phillips, Oliver L and Phua, Mui-How and Qie, Lan},
doi = {10.1016/j.rse.2017.03.017},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jun},
pages = {77--88},
title = {{Area-based vs tree-centric approaches to mapping forest carbon in Southeast Asian forests from airborne laser scanning data}},
volume = {194},
year = {2017}
}
@article{Bellil:2016:GWN:2877705.2877756,
abstract = {{\textcopyright} 2014, Springer Science+Business Media New York. The first handicap in 3D faces recognizing under unconstrained problem is the largest variability of the visual aspect when we use various sources. This great variability complicates the task of identifying persons from their 3D facial scans and it is the most reason that bring to face detection and recognition of the major problems in pattern recognition fields, biometrics and computer vision. We propose a new 3D face identification and recognition method based on Gappy Wavelet Neural Network (GWNN) that is able to provide better accuracy in the presence of facial occlusions. The proposed approach consists of three steps: the first step is face detection. The second step is to identify and remove occlusions. Occluded regions detection is done by considering that occlusions can be defined as local face deformations. These deformations are detected by a comparison between the input facial test wavelet coefficients and wavelet coefficients of generic face model formed by the mean data base faces. They are beneficial for neighborhood relationships between pixels rotation, dilation and translation invariant. Then, occluded regions are refined by removing wavelet coefficient above a certain threshold. Finally, the last stage of processing and retrieving is made based on wavelet neural network to recognize and to restore 3D occluded regions that gathers the most. The experimental results on this challenging database demonstrate that the proposed approach improves recognition rate performance from 93.57 to 99.45 {\%} which represents a competitive result compared to the state of the art.},
address = {Hingham, MA, USA},
annote = {22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Bellil, Wajdi and Brahim, Hajer and {Ben Amar}, Chokri},
doi = {10.1007/s11042-014-2294-6},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {3D face recognition; Wavelets,Gappy data,Occlusion detection,Wavelet neural network,acm,etapa1,gil,id125,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {acm,etapa1,gil,id125,revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {365--380},
publisher = {Kluwer Academic Publishers},
title = {{Gappy wavelet neural network for 3D occluded faces: detection and recognition}},
url = {http://dx.doi.org/10.1007/s11042-014-2294-6 http://link.springer.com/10.1007/s11042-014-2294-6},
volume = {75},
year = {2016}
}
@article{ISI:000459798800005,
abstract = {Facial landmarking is a fundamental task in automatic machine-based face
analysis. The majority of existing techniques for such a problem are
based on 2D images; however, they suffer from illumination and pose
variations that may largely degrade landmarking performance. The
emergence of 3D data theoretically provides an alternative to overcome
these weaknesses in the 2D domain. This article proposes a novel
approach to 3D facial landmarking, which combines both the advantages of
feature-based methods as well as model-based ones in a progressive
three-stage coarse-to-fine manner (initial, intermediate, and fine
stages). For the initial stage, a few fiducial landmarks (i.e., the nose
tip and two inner eye corners) are robustly detected through curvature
analysis, and these points are further exploited to initialize the
subsequent stage. For the intermediate stage, a statistical model is
learned in the feature space of three normal components of the facial
point-cloud rather than the smooth original coordinates, namely Active
Normal Model (ANM). For the fine stage, cascaded regression is employed
to locally refine the landmarks according to their geometry attributes.
The proposed approach can accurately localize dozens of fiducial points
on each 3D face scan, greatly surpassing the feature-based ones, and it
also improves the state of the art of the model-based ones in two
aspects: sensitivity to initialization and deficiency in discrimination.
The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE
databases, and competitive results are achieved in comparison with
counterparts in the literature, clearly demonstrating its effectiveness.},
author = {Sun, Jia and Huang, Di and Wang, Yunhong and Chen, Liming},
doi = {10.1145/3282833},
issn = {1551-6857},
journal = {ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {1},
title = {{Expression Robust 3D Facial Landmarking via Progressive Coarse-to-Fine Tuning}},
volume = {15},
year = {2019}
}
@inproceedings{ISI:000382327100024,
abstract = {Synthetic aperture radar interferometry (InSAR) has been an established
method for long term large area monitoring. Since the launch of
meter-resolution spaceborne SAR sensors, the InSAR community has shown
that even individual buildings can be monitored in high level of detail.
However, the current deformation analysis still remains at a primitive
stage of pixel-wise motion parameter inversion and manual identification
of the regions of interest. We are aiming at developing an automatic
urban infrastructure monitoring approach by combining InSAR and the
semantics derived from optical images, so that the deformation analysis
can be done systematically in the semantic/object level. This paper
explains how we transfer the semantic meaning derived from optical image
to the InSAR point clouds, and hence different semantic classes in the
InSAR point cloud can be automatically extracted and monitored. Examples
on bridges and railway monitoring are demonstrated.},
annote = {ISPRS Geospatial Week, La Grande Motte, FRANCE, SEP 28-OCT 03, 2015},
author = {Wang, Yuanyuan and Zhu, Xiao Xiang},
booktitle = {ISPRS GEOSPATIAL WEEK 2015},
doi = {10.5194/isprsarchives-XL-3-W3-153-2015},
editor = {{Mallet, C and Paparoditis, N and Dowman, I and Elberink, SO and Raimond, AM and Sithole, G and Rabatel, G and Rottensteiner, F and Briottet, X and Christophe, S and Coltekin, A and Patane, G}},
issn = {2194-9034},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {W3},
organization = {WG III 2 Point Cloud Proc; WG V III Terrestrial 3D Imaging {\&} Sensors; WG VII 7 Synergy Radar; ISPRS},
pages = {153--160},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{SEMANTIC INTERPRETATION OF INSAR ESTIMATES USING OPTICAL IMAGES WITH APPLICATION TO URBAN INFRASTRUCTURE MONITORING}},
volume = {40-3},
year = {2015}
}
@inproceedings{ISI:000457843607026,
abstract = {Recently proposed robust 3D face alignment methods establish either
dense or sparse correspondence between a 3D face model and a 2D facial
image. The use of these methods presents new challenges as well as
opportunities for facial texture analysis. In particular by sampling the
image using the fitted model, a facial UV can be created. Unfortunately,
due to self-occlusion, such a UV map is always incomplete. In this
paper, we propose a framework for training Deep Convolutional Neural
Network (DCNN) to complete the facial UV map extracted from in-the-wild
images. To this end, we first gather complete UV maps by fitting a 3D
Morphable Model (3DMM) to various multiview image and video datasets, as
well as leveraging on a new 3D dataset with over 3,000 identities.
Second, we devise a meticulously designed architecture that combines
local and global adversarial DCNNs to learn an identity-preserving
facial UV completion model. We demonstrate that by attaching the
completed UV to the fitted mesh and generating instances of arbitrary
poses, we can increase pose variations for training deep face
recognition/verification models, and minimise pose discrepancy during
testing, which lead to better performance. Experiments on both
controlled and in-the-wild UV datasets prove the effectiveness of our
adversarial UV completion model. We achieve state-of-the-art
verification accuracy, 94.05{\%}, under the CFP frontal-profile protocol
only by combining pose augmentation during training and pose discrepancy
reduction during testing. We will release the first in-the-wild UV
dataset (we refer as WildUV) that comprises of complete facial UV maps
from 1,892 identities for research purposes.},
annote = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Salt Lake City, UT, JUN 18-23, 2018},
author = {Deng, Jiankang and Cheng, Shiyang and Xue, Niannan and Zhou, Yuxiang and Zafeiriou, Stefanos},
booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2018.00741},
isbn = {978-1-5386-6420-9},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; CVF; IEEE Comp Soc},
pages = {7093--7102},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition}},
year = {2018}
}
@article{ISI:000464484600009,
abstract = {Person re-identification is typically performed using 2D still images or
videos, where photometric appearance is the main visual cue used to
discover the presence of a target subject when switching from different
camera views across time. This invalidates any application where a
person may change dress across subsequent acquisitions as can be the
case of patients monitoring at home. Differently from RGB data, 3D
information as acquired by depth cameras can open the way to person
re-identification based on biometric cues such as distinguishing traits
of the body or face. However, the accuracy of skeleton and face geometry
extracted from depth data is not always adequate to enable person
recognition, since both these features are affected by the pose of the
subject and the distance from the camera. In this paper, we propose a
method to derive a robust skeleton representation from a depth sequence
and to complement it with a highly discriminative face feature. This is
obtained by selecting skeleton and face samples based on their quality
and using the temporal redundancy across the sequence to derive and
refine cumulated models for both of them. Extracting skeleton and face
features from such cumulated models and combining them for the
recognition allow us to improve rank-1 re-identification accuracy
compared to individual cues. A comparative evaluation on three benchmark
datasets also shows results at the state-of-the-art. (C) 2019 Elsevier
Ltd. All rights reserved.},
author = {Pala, Pietro and Seidenari, Lorenzo and Berretti, Stefano and {Del Bimbo}, Alberto},
doi = {10.1016/j.cag.2019.01.003},
issn = {0097-8493},
journal = {COMPUTERS {\&} GRAPHICS-UK},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {69--80},
title = {{Enhanced skeleton and face 3D data for person re-identification from depth cameras}},
volume = {79},
year = {2019}
}
@inproceedings{ISI:000427154700002,
abstract = {With the global demand for extra security systems, and the growing of
human-machine interaction, facial analysis in unconstrained environments
(in the wild) became a hot-topic in recent computer vision research.
Unconstrained environments include surveillance footage, social media
photos and live broadcasts. This type of images and videos include no
control over illumination, position, size, occlusion, and facial
expressions. Successful facial processing methods for controlled
scenarios are unable to pledge with challenging circumstances.
Consequently, methods tailored for handling those situations are
indispensable for the face analysis research progress. This work
presents a comprehensive review of state-of-the-art methods, drawing
attention to the complications derived from in the wild scenarios and
the behavior differences when applied to the controlled images. The main
topics to be covered are: (1) face detection; (2) facial image quality;
(3) head pose estimation; (4) face alignment; (5) 3D face
reconstruction; (6) gender and age estimation; (7) facial expressions
and emotions; and (8) face recognition. Finally, available code and
applications for in the wild face analysis are presented, followed by a
discussion on future directions.},
annote = {30th SIBGRAPI Conference on Graphics, Patterns and Images Tutorials
(SIBGRAPI-T), Niteroi, BRAZIL, OCT 17-20, 2017},
author = {Zavan, Flavio H de B and Gasparin, Nathaly and Batista, Julio C and Silva, Luan P E and Albiero, Vitor and Bellon, Olga R P and Silva, Luciano},
booktitle = {2017 30TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES TUTORIALS (SIBGRAPI-T)},
doi = {10.1109/SIBGRAPI-T.2017.11},
isbn = {978-1-5386-0619-3},
issn = {1530-1834},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Brazilian Comp Soc; UFRJ; PUC Rio; NVIDIA; IBM; Univ Fed Fluminense, Inst Computacao; Univ Fed Rio Janeiro, Programa Engn Sistemas Computacao; Pontificia Univ Catolica Rio aneiro, Dept Informatica; Univ Fed Rio Janeiro; ACM SIGGRAPH; CAPES; CNPq; SIBGRAPI},
pages = {9--16},
series = {SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing},
title = {{Face Analysis in the Wild}},
year = {2017}
}
@inproceedings{ISI:000380427900055,
abstract = {Head pose estimation helps to align a 3D face model to a 2D image, which
is critical to research requiring dense 2D-to-2D or 3D-to-2D
correspondence. Traditional pose estimation relies strongly on the
accuracy of landmarks, so it is sensitive to missing or incorrect
landmarks. In this paper, we propose a landmark-free approach to
estimate the pose projection matrix. The method can be used to estimate
this matrix in unconstrained scenarios and we demonstrate its
effectiveness through multiple head pose estimation experiments.},
annote = {IEEE 7th International Conference on Biometrics Theory, Applications and
Systems (BTAS), Arlington, VA, SEP 08-11, 2015},
author = {Wu, Yuhang and Xu, Xiang and Shah, Shishir K and Kakadiaris, Ioannis A},
booktitle = {2015 IEEE 7TH INTERNATIONAL CONFERENCE ON BIOMETRICS THEORY, APPLICATIONS AND SYSTEMS (BTAS 2015)},
isbn = {978-1-4799-8777-1},
issn = {2474-9680},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE},
series = {International Conference on Biometrics Theory Applications and Systems},
title = {{Towards fitting a 3D dense facial model to a 2D image: A landmark-free approach}},
year = {2015}
}
@inproceedings{ISI:000426973200059,
abstract = {Inage-based 3D face reconstruction has great potential in different
areas, such as facial recognition, facial analysis, and facial
animation. Due to the variations in image quality, single-image-based 3D
face reconstruction might not be sufficient to accurately reconstruct a
3D face. To overcome this limitation, multi-view 3D face reconstruction
uses multiple images of the same subject and aggregates complementary
information for better accuracy. Though theoretically appealing, there
are multiple challenges in practice. Amnong these challenges, the most
significant is that it is difficult to establish coherent and accurate
correspondence among a set of images, especially when these images are
captured in different conditions. In this paper, we propose a method,
Deep Recurrent 3D FAce Reconstruction (DRFAR), to solve the task of
multi-view 3D face reconstruction using a subspace representation of the
3D facial shape and a deep recurrent neural network that consists of
both a deep convolutional neural network (DCNN) and a recurrent neural
network (RNN). The DCNN disentangles the facial identity and the facial
expression components for each single image independently, while the RNN
fuses identity-related features from the DCNN and aggregates the
identity specific contextual information, or the identity signal, from
the whole set of images to predict the facial identity parameter, which
is robust to variations in image quality and is consistent over the
whole set of images. Through extensive experiments, we evaluate our
proposed method and demonstrate its superiority over existing methods.},
annote = {IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
OCT 01-04, 2017},
author = {Dou, Pengfei and Kakadiaris, Ioannis A},
booktitle = {2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)},
isbn = {978-1-5386-1124-1},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE},
pages = {483--492},
title = {{Multi-View 3D Face Reconstruction with Deep Recurrent Neural Networks}},
year = {2017}
}
@article{ISI:000412378800003,
abstract = {Automatic human Facial Expressions Recognition (FER) is becoming of
increased interest. FER finds its applications in many emerging areas
such as affective computing and intelligent human computer interaction.
Most of the existing work on FER has been done using 2D data which
suffers from inherent problems of illumination changes and pose
variations. With the development of 3D image capturing technologies, the
acquisition of 3D data is becoming a more feasible task. The 3D data
brings a more effective solution in addressing the issues raised by its
2D counterpart. State-of-the-art 3D FER methods are often based on a
single descriptor which may fail to handle the large inter-class and
intra-class variability of the human facial expressions. In this work,
we explore, for the first time, the usage of covariance matrices of
descriptors, instead of the descriptors themselves, in 3D FER. Since
covariance matrices are elements of the non-linear manifold of Symmetric
Positive Definite (SPD) matrices, we particularly look at the
application of manifold-based classification to the problem of 3D FER.
We evaluate the performance of the proposed framework on the BU-3DFE and
the Bosphorus datasets, and demonstrate its superiority compared to the
state-of-the-art methods. (C) 2017 Elsevier Ltd. All rights reserved.},
author = {Hariri, Walid and Tabia, Hedi and Farah, Nadir and Benouareth, Abdallah and Declercq, David},
doi = {10.1016/j.engappai.2017.05.009},
issn = {0952-1976},
journal = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {25--32},
title = {{3D facial expression recognition using kernel methods on Riemannian manifold}},
volume = {64},
year = {2017}
}
@inproceedings{ISI:000388117502023,
abstract = {In the recent days, the facial biometric system is widely used for the
mobile payments and other surveillance systems. Its popularity is going
to be increased because of its easiness to use and also it is user
friendly. But the main problem in this system is its vulnerability to
the spoof attacks made by 2D or 3D face masks or printed photographs. In
order to guard against face spoofing, the anti-spoofing methods have
been developed to do liveliness detection. In this paper, the different
type of face spoofing attacks and the different techniques used for
anti-spoofing arc analyzed.},
annote = {3rd International Conference on Computing for Sustainable Global
Development (INDIACom), New Delhi, INDIA, MAR 16-18, 2016},
author = {Bagga, Manpreet and Singh, Baijit},
booktitle = {PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT},
editor = {{Hoda, MN}},
isbn = {978-9-3805-4419-9},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {GGSIP Univ; Govt India, Minist Sci {\&} Technol, Dept Sci {\&} Technol; Council Sci {\&} Ind Res; All India Council Tech Educ; Inst Elect {\&} Telecommunicat Engineers, Delhi Ctr; Inst Engn {\&} Technol, Delhi Local Networks; Jagdishprasad Jhabarmal Tibrewala Univ; Bhar},
pages = {2037--2042},
title = {{Spoofing Detection In Face Recognition: A Review}},
year = {2016}
}
@article{Lei:2016:TWC:2875518.2875660,
abstract = {3D face recognition with the availability of only partial data (missing parts, occlusions and data corruptions) and single training sample is a highly challenging task. This paper presents an efficient 3D face recognition approach to address this challenge. We represent a facial scan with a set of local Keypoint-based Multiple Triangle Statistics (KMTS), which is robust to partial facial data, large facial expressions and pose variations. To address the single sample problem, we then propose a Two-Phase Weighted Collaborative Representation Classification (TPWCRC) framework. A class-based probability estimation is first calculated based on the extracted local descriptors as a prior knowledge. The resulting class-based probability estimation is then incorporated into the proposed classification framework as a locality constraint to further enhance its discriminating power. Experimental results on six challenging 3D facial datasets show that the proposed KMTS-TPWCRC framework achieves promising results for human face recognition with missing parts, occlusions, data corruptions, expressions and pose variations.},
address = {New York, NY, USA},
annote = {21/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
21/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Lei, Yinjie and Guo, Yulan and Hayat, Munawar and Bennamoun, Mohammed and Zhou, Xinzhi},
doi = {10.1016/j.patcog.2015.09.035},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {3D face recognition,3D representation,Partial facial data,Single sample problem,Sparse representation,acm,etapa1,gil,id111,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {acm,etapa1,gil,id111,revisao{\_}scopus,revisao{\_}webofscience},
month = {apr},
number = {C},
pages = {218--237},
publisher = {Elsevier Science Inc.},
title = {{A Two-Phase Weighted Collaborative Representation for 3D partial face recognition with single sample}},
url = {http://dx.doi.org/10.1016/j.patcog.2015.09.035 http://linkinghub.elsevier.com/retrieve/pii/S0031320315003660},
volume = {52},
year = {2016}
}
@inproceedings{ISI:000454677900019,
abstract = {Facial Recognition is a commonly used technology in security-related
applications. It has been thoroughly studied and scrutinized for its
number of practical real-world applications. On the road ahead of
understanding this technology, there remain several obstacles. In this
paper, methods of 3D face recognition are examined by measuring
quantifiable applications and results. In facial recognition, three
Dimensional Morphable Model (3DMM) techniques have attracted more and
more attention as effectiveness in use increases over time. 3DMM
provides automation and more accurate image rendering when compared to
other traditional techniques. The accuracy in image rendering comes at a
cost; as 3DMM requires more focus on texture estimation,
shape-controlling limits, and extrinsic variations, accurately matching
fitting models, feature tracking and precision identification. We have
underlined different issues in comparison based on these methods.},
annote = {1st International Conference on Emerging Technologies in Computing
(ICETIC), London Metropolitan Univ, London, ENGLAND, AUG 23-24, 2018},
author = {Khan, Muhammad Sajid and Jehanzeb, Muhammad and Babar, Muhammad Imran and Faisal, Shah and Ullah, Zabeeh and Amin, Siti Zulaikha Binti Mohamad},
booktitle = {EMERGING TECHNOLOGIES IN COMPUTING, ICETIC 2018},
doi = {10.1007/978-3-319-95450-9_19},
editor = {{Miraz, MH and Excell, P and Ware, A and Soomro, S and Ali, M}},
isbn = {978-3-319-95450-9; 978-3-319-95449-3},
issn = {1867-8211},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Int Assoc Educators {\&} Researchers; IEEE ComSoc Bahrain Chapter; British Comp Soc, N Wales Branch; EAI},
pages = {220--236},
series = {Lecture Notes of the Institute for Computer Sciences Social Informatics and Telecommunications Engineering},
title = {{Face Recognition Analysis Using 3D Model}},
volume = {200},
year = {2018}
}
@inproceedings{ISI:000423869700004,
abstract = {The concept of remote sensing is to provide information about a
wide-range area without making physical contact with this area. If,
additionally to satellite imagery, images and videos taken by drones
provide a more up-to-date data at a higher resolution, or accurate
vector data is downloadable from the Internet, one speaks of sensor data
fusion. The concept of sensor data fusion is relevant for many
applications, such as virtual tourism, automatic navigation, hazard
assessment, etc. In this work, we describe sensor data fusion aiming to
create a semantic 3D model of an extremely interesting yet challenging
dataset: An alpine region in Southern Germany. A particular challenge of
this work is that rock faces including overhangs are present in the
input airborne laser point cloud. The proposed procedure for
identification and reconstruction of overhangs from point clouds
comprises four steps: Point cloud preparation, filtering out vegetation,
mesh generation and texturing. Further object types are extracted in
several interesting subsections of the dataset: Building models with
textures from UAV (Unmanned Aerial Vehicle) videos, hills reconstructed
as generic surfaces and textured by the orthophoto, individual trees
detected by the watershed algorithm, as well as the vector data for
roads retrieved from openly available shape files and GPS-device tracks.
We pursue geo-specific reconstruction by assigning texture and width to
roads of several pre-determined types and modeling isolated trees and
rocks using commercial software. For visualization and simulation of the
area, we have chosen the simulation system Virtual Battlespace 3 (VBS3).
It becomes clear that the proposed concept of sensor data fusion allows
a coarse reconstruction of a large scene and, at the same time, an
accurate and up-to-date representation of its relevant subsections, in
which simulation can take place.},
annote = {17th SPIE Conference on Earth Resources and Environmental Remote
Sensing/GIS Applications, Warsaw, POLAND, SEP 12-14, 2017},
author = {Haufel, Gisela and Bulatov, Dimitri and Solbrig, Peter},
booktitle = {EARTH RESOURCES AND ENVIRONMENTAL REMOTE SENSING/GIS APPLICATIONS VIII},
doi = {10.1117/12.2278237},
editor = {{Michel, U and Schulz, K and Nikolakopoulos, KG and Civco, D}},
isbn = {978-1-5106-1321-8; 978-1-5106-1320-1},
issn = {0277-786X},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Sensor Data Fusion for Textured Reconstruction and Virtual Representation of Alpine Scenes}},
volume = {10428},
year = {2017}
}
@article{ISI:000422943700008,
abstract = {This paper presents new approaches for gait and activity analysis based
on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed
algorithms are embedded into an integrated 4D vision and visualization
system, which is able to analyze and interactively display real
scenarios in natural outdoor environments with walking pedestrians. The
main focus of the investigations is gait-based person reidentification
during tracking and recognition of specific activity patterns, such as
bending, waving, making phone calls, and checking the time looking at
wristwatches. The descriptors for training and recognition are observed
and extracted from realistic outdoor surveillance scenarios, where
multiple pedestrians are walking in the field of interest following
possibly intersecting trajectories; thus, the observations might often
be affected by occlusions or background noise. Since there is no public
database available for such scenarios, we created and published a new
Lidar-based outdoor gait and activity data set on our website that
contains point cloud sequences of 28 different persons extracted and
aggregated from 35-min-long measurements. The presented results confirm
that both efficient gait-based identification and activity recognition
are achievable in the sparse point clouds of a single RMB Lidar sensor.
After extracting the people trajectories, we synthesized a
free-viewpoint video, in which moving avatar models follow the
trajectories of the observed pedestrians in real time, ensuring that the
leg movements of the animated avatars are synchronized with the real
gait cycles observed in the Lidar stream.},
author = {Benedek, Csaba and Galai, Bence and Nagy, Balazs and Janko, Zsolt},
doi = {10.1109/TCSVT.2016.2595331},
issn = {1051-8215},
journal = {IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {101--113},
title = {{Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance System}},
volume = {28},
year = {2018}
}
@inproceedings{ISI:000380803300154,
abstract = {The biometric identification by face is among one of the most widely
used methods of biometric identification. Due to it provides a faster
and more accurate identification; it was implemented into area of
security 3D face reader by Broadway manufacturer was used to measure. It
is equipped with the 3D camera system, which uses the method of
structured light scanning and saves the template into the 3D model of
face. The obtained data were evaluated by software Turnstile Enrolment
Application (TEA). The measurements were used 3D face reader the
Broadway 3D. First, the person was scanned and stored in the database.
Thereafter person has already been compared with the stored template in
the database for each method. Finally, a measure of reliability was
evaluated for the Broadway 3D face reader.},
annote = {International Conference on Numerical Analysis and Applied Mathematics
(ICNAAM), Rhodes, GREECE, SEP 23-29, 2015},
author = {Jasek, Roman and Talandova, Hana and Adamek, Milan},
booktitle = {PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON NUMERICAL ANALYSIS AND APPLIED MATHEMATICS 2015 (ICNAAM-2015)},
doi = {10.1063/1.4951908},
editor = {{Simos, T and Tsitouras, C}},
isbn = {978-0-7354-1392-4},
issn = {0094-243X},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
series = {AIP Conference Proceedings},
title = {{Methodology of the Determination of the Uncertainties by Using the Biometric Device the Broadway 3D}},
volume = {1738},
year = {2016}
}
@inproceedings{ISI:000400617600065,
abstract = {Face as a biometric identification in computer vision is an important
medium, in areas such as video surveillance, animation games, security
anti-terrorist has a very wide range of applications, creating vivid,
strong visibility of 3d face model, now has become a challenging in the
field of computer vision is one of the important topics. At first, this
paper used the zhongxing-micro ZC301P cameras to build a binocular
stereo vision system for recording images. After the camera calibration
and binocular calibration, the three-dimensional data of facial images
were extracted using the functions of OpenCV computer vision library,
and then 3d face model were reconstructed preliminary by DirectX.
According the reconstruction process, the human face three-dimensional
reconstruction software was designed and developed. The paper laid the
foundation for the next step work that is to obtain more clear and
strong visibility of 3d face.},
annote = {5th International Conference on Audio, Language and Image Processing
(ICALIP), Shanghai, PEOPLES R CHINA, JUL 11-12, 2016},
author = {Yin, Jing and Yang, XiaoFang},
booktitle = {PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP)},
isbn = {978-1-5090-0654-0},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE; IET; IEEE CIS Shanghai Chapter; IET Shanghai Local Network; Shanghai Univ; Natl Sci {\&} Technol Near Surface Detect Lab; Shanghai Univ Engn Sci; Tongji Univ; Fudan Univ; Shanghai Jiao Tong Univ},
pages = {341--344},
title = {{3D FACIAL RECONSTRUCTION OF BASED ON OPENCV AND DIRECTX}},
year = {2016}
}
@article{ISI:000459941200022,
abstract = {Face reconstruction is a popular topic in 3D vision system. However,
traditional methods often depend on monocular cues, which contain few
feature pixels and only use their location information while ignoring a
lot of textural information. Furthermore, they are affected by the
accuracy of the feature extraction method and occlusion. Here, we
propose a novel facial reconstruction framework that accurately extracts
the 3D shapes and poses of faces from images captured at multi-views. It
extends the traditional method using the monocular bilinear model to the
multi-view-based bilinear model by incorporating the feature prior
constraint and the texture constraint, which are learned from multi-view
images. The feature prior constraint is used as a shape prior to
allowing us to estimate accurate 3D facial contours. Furthermore, the
texture constraint extracts a high-precision 3D facial shape where
traditional methods fail because of their limited number of feature
points or the mostly texture-less and texture-repetitive nature of the
input images. Meanwhile, it fully explores the implied 3D information of
the multi-view images, which also enhances the robustness of the
results. Additionally, the proposed method uses only two or more
uncalibrated images with an arbitrary baseline, estimating calibration
and shape simultaneously. A comparison with the state-of-the-art
monocular bilinear model-based method shows that the proposed method has
a significantly higher level of accuracy.},
author = {Tian, Liang and Liu, Jing and Guo, Wei},
doi = {10.3390/s19030459},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {3},
title = {{Three-Dimensional Face Reconstruction Using Multi-View-Based Bilinear Model}},
volume = {19},
year = {2019}
}
@article{ISI:000451316500001,
abstract = {3D face recognition has become a trending research direction in both
industry and academia. It inherits advantages from traditional 2D face
recognition, such as the natural recognition process and a wide range of
applications. Moreover, 3D face recognition systems could accurately
recognize human faces even under dim lights and with variant facial
positions and expressions, in such conditions 2D face recognition
systems would have immense difficulty to operate. This paper summarizes
the history and the most recent progresses in 3D face recognition
research domain. The frontier research results are introduced in three
categories: pose-invariant recognition, expression-invariant
recognition, and occlusion-invariant recognition. To promote future
research, this paper collects information about publicly available 3D
face databases. This paper also lists important open problems.},
author = {Zhou, Song and Xiao, Sheng},
doi = {10.1186/s13673-018-0157-2},
issn = {2192-1962},
journal = {HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {nov},
title = {{3D face recognition: a survey}},
volume = {8},
year = {2018}
}
@inproceedings{ISI:000360175900188,
abstract = {3D Face recognition has been an area of interest for the past few
decades in pattern recognition. This paper focuses on problems of person
identification using 3D Face data. Here unregistered Face data, i.e.
both texture and depth is fed to classifier in spectral representations
of data. 2D Discrete Fourier Transform (DFT) is used for spectral
representation. Fusion of scores improves the recognition accuracy
significantly since use of depth information alone in spectral
representation was not sufficient to increase accuracy. Statistical
method seems to degrade performance of system when applied to texture
data and was effective for depth data. (C) 2015 The Authors. Published
by Elsevier B.V.},
annote = {International Conference on Information and Communication Technologies
(ICICT), Kochi, INDIA, DEC 03-05, 2014},
author = {Naveen, S and Moni, R S},
booktitle = {PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES, ICICT 2014},
doi = {10.1016/j.procs.2015.02.078},
editor = {{Samuel, P}},
issn = {1877-0509},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {Cochin Uni Sci {\&} Technol, Sch Engn; TEQIP Phase II},
pages = {1537--1545},
series = {Procedia Computer Science},
title = {{Multimodal Face Recognition System using Spectral Transformation of 2D Texture feature and Statistical processing of Face Range Images}},
volume = {46},
year = {2015}
}
@inproceedings{ISI:000371977802159,
abstract = {In this paper, we present a novel approach for fusing shape and texture
local binary patterns (LBP) for 3D face recognition. Using the framework
proposed in {\{}[{\}}1], we compute LBP directly on the face mesh surface,
then we construct a grid of the regions on the facial surface that can
accommodate global and partial descriptions. Compared to its depth-image
counterpart, our approach is distinguished by the following features: a)
inherits the intrinsic advantages of mesh surface; b) does not require
normalization; c) can accommodate partial matching. In addition, it
allows early-level fusion of texture and shape modalities. Through
experiments conducted on the BU-3DFE and Bosphorus databases, we assess
different variants of our approach with regard to facial expressions and
missing data.},
annote = {IEEE International Conference on Image Processing (ICIP), Quebec City,
CANADA, SEP 27-30, 2015},
author = {Tortorici, Claudio and Werghi, Naoufel and Berretti, Stefano},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4799-8339-1},
issn = {1522-4880},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; IEEE Signal Proc Soc},
pages = {2670--2674},
series = {IEEE International Conference on Image Processing ICIP},
title = {{BOOSTING 3D LBP-BASED FACE RECOGNITION BY FUSING SHAPE AND TEXTURE DESCRIPTORS ON THE MESH}},
year = {2015}
}
@inproceedings{ISI:000374793400019,
abstract = {Face recognition in unconstrained environments is often influenced by
pose variations. And the problem is basically the identification that
uses partial data. In this paper, a method fusing structure and texture
information is proposed to solve the problem. In the register phase, the
approximate 180 degree information of face is acquired, and the data
used to identify individual is obtained from a random single view. Pure
face is extracted from 3D data first, then convert the original data to
the form of spherical depth map (SDM) and spherical texture map (STM),
which are invariant to out-plane rotation, subsequently facilitating the
successive alignment-free identification that is robust to pose
variations. We make identification through sparse representation for its
well performance with the two maps. Experiments show that our proposed
method gets a high recognition rate with pose and expression variations.},
annote = {10th Chinese Conference on Biometric Recognition (CCBR), Tianjin,
PEOPLES R CHINA, NOV 13-15, 2015},
author = {Liu, Shuai and Mu, Zhichun and Huang, Hongbo},
booktitle = {BIOMETRIC RECOGNITION, CCBR 2015},
doi = {10.1007/978-3-319-25417-3_19},
editor = {{Yang, J and Yang, J and Sun, Z and Shan, S and Zheng, W and Feng, J}},
isbn = {978-3-319-25417-3; 978-3-319-25416-6},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Chinese Assoc Artificial Intelligence; Springer; Civil Aviat Univ China; Tianjin Univ Sci {\&} Technol; CASIA, Inst Intelligent Recognit},
pages = {151--159},
series = {Lecture Notes in Computer Science},
title = {{3D Face Recognition Fusing Spherical Depth Map and Spherical Texture Map}},
volume = {9428},
year = {2015}
}
@inproceedings{ISI:000405560700088,
abstract = {We propose a new biometric approach where the tissue thickness of a
person's forehead is used as a biometric feature. Given that the spatial
registration of two 3D laser scans of the same human face usually
produces a low error value, the principle of point cloud registration
and its error metric can be applied to human classification techniques.
However, by only considering the spatial error, it is not possible to
reliably verify a person's identity. We propose to use a novel
near-infrared laser-based head tracking system to determine an
additional feature, the tissue thickness, and include this in the error
metric. Using MRI as a ground truth, data from the foreheads of 30
subjects was collected from which a 4D reference point cloud was created
for each subject. The measurements from the near-infrared system were
registered with all reference point clouds using the ICP algorithm.
Afterwards, the spatial and tissue thickness errors were extracted,
forming a 2D feature space. For all subjects, the lowest feature
distance resulted from the registration of a measurement and the
reference point cloud of the same person.
The combined registration error features yielded two clusters in the
feature space, one from the same subject and another from the other
subjects. When only the tissue thickness error was considered, these
clusters were less distinct but still present. These findings could help
to raise safety standards for head and neck cancer patients and lays the
foundation for a future human identification technique.},
annote = {Conference on Medical Imaging - Image-Guided Procedures, Robotic
Interventions, and Modeling, Orlando, FL, FEB 14-16, 2017},
author = {Manit, Jirapong and Bremer, Christina and Schweikard, Achim and Ernst, Floris},
booktitle = {MEDICAL IMAGING 2017: IMAGE-GUIDED PROCEDURES, ROBOTIC INTERVENTIONS, AND MODELING},
doi = {10.1117/12.2254963},
editor = {{Webster, RJ and Fei, B}},
isbn = {978-1-5106-0715-6; 978-1-5106-0716-3},
issn = {0277-786X},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {SPIE; Alpin Med Syst; Siemens Healthineers; No Digital Inc},
series = {Proceedings of SPIE},
title = {{Patient identification using a near-infrared lasers canner}},
volume = {10135},
year = {2017}
}
@article{ISI:000408398200010,
abstract = {Recognition of faces typically occurs via holistic processing where
individual features are combined to provide an overall facial
representation. However, when faces are inverted, there is greater
reliance on featural processing where faces are recognized based on
their individual features. These findings are based on a substantial
number of studies using 2-dimensional (2D) faces and it is unknown
whether these results can be extended to 3-dimensional (3D) faces, which
have more depth information that is absent in the typical 2D stimuli
used in face recognition literature. The current study used the face
inversion paradigm as a means to investigate how holistic and featural
processing are differentially influenced by 2D and 3D faces. Twenty-five
participants completed a delayed face-matching task consisting of
upright and inverted faces that were presented as both 2D and 3D
stereoscopic images. Recognition accuracy was significantly higher for
3D upright faces compared to 2D upright faces, providing support that
the enriched visual information in 3D stereoscopic images facilitates
holistic processing that is essential for the recognition of upright
faces. Typical face inversion effects were also obtained, regardless of
whether the faces were presented in 2D or 3D. Moreover, recognition
performances for 2D inverted and 3D inverted faces did not differ. Taken
together, these results demonstrated that 3D stereoscopic effects
influence face recognition during holistic processing but not during
featural processing. Our findings therefore provide a novel perspective
that furthers our understanding of face recognition mechanisms, shedding
light on how the integration of stereoscopic information in 3D faces
influences face recognition processes. (c) 2017 Elsevier Ltd. All rights
reserved.},
author = {Eng, Z H D and Yick, Y Y and Guo, Y and Xu, H and Reiner, M and Cham, T J and Chen, S H A},
doi = {10.1016/j.visres.2017.06.004},
issn = {0042-6989},
journal = {VISION RESEARCH},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {78--85},
title = {{3D faces are recognized more accurately and faster than 2D faces, but with similar inversion effects}},
volume = {138},
year = {2017}
}
@article{7012060,
abstract = {In this paper, a novel method for face recognition under pose and expression variations is proposed from only a single image in the gallery. A 3D probabilistic facial expression recognition generic elastic model is proposed to reconstruct a 3D model from real-world human face using only a single 2D frontal image with/without facial expressions. Then, a feature library matrix (FLM) is generated for each subject in the gallery from all face poses by rotating the 3D reconstructed models and extracting features in the rotated face pose. Therefore, each FLM is subsequently rendered for each subject in the gallery based on triplet angles of face poses. In addition, before matching the FLM, an initial estimate of triplet angles is obtained from the face pose in probe images using an automatic head pose estimation approach. Then, an array of the FLM is selected for each subject based on the estimated triplet angles. Finally, the selected arrays from FLMs are compared with extracted features from the probe image by iterative scoring classification using the support vector machine. Convincing results are acquired to handle pose and expression changes on the Bosphorus, Face Recognition Technology (FERET), Carnegie Mellon University-Pose, Illumination, and Expression (CMU-PIE), and Labeled Faces in the Wild (LFW) face databases compared with several state-of-the-art methods in pose-invariant face recognition. The proposed method not only demonstrates an excellent performance by obtaining high accuracy on all four databases but also outperforms other approaches realistically.},
annote = {09/05/2018 Em processo de sele{\c{c}}{\~{a}}o (Etapa 1)
09/05/2018 Exclu{\'{i}}do (Etapa 1)},
author = {Moeini, A and Moeini, H},
doi = {10.1109/TIFS.2015.2393553},
issn = {1556-6013},
journal = {IEEE Transactions on Information Forensics and Security},
keywords = {artur,etapa1,face recognition,feature extraction,id462,ieeexplore,image classifi,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {artur,etapa1,id462,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {5},
pages = {969--984},
title = {{Real-World and Rapid Face Recognition Toward Pose and Expression Variations via Feature Library Matrix}},
volume = {10},
year = {2015}
}
@article{Guo2016403,
abstract = {This paper presents a local feature based shape matching algorithm for expression-invariant 3D face recognition. Each 3D face is first automatically detected from a raw 3D data and normalized to achieve pose invariance. The 3D face is then represented by a set of keypoints and their associated local feature descriptors to achieve robustness to expression variations. During face recognition, a probe face is compared against each gallery face using both local feature matching and 3D point cloud registration. The number of feature matches, the average distance of matched features, and the number of closest point pairs after registration are used to measure the similarity between two 3D faces. These similarity metrics are then fused to obtain the final results. The proposed algorithm has been tested on the FRGC v2 benchmark and a high recognition performance has been achieved. It obtained the state-of-the-art results by achieving an overall rank-1 identification rate of 97.0{\%} and an average verification rate of 99.01{\%} at 0.001 false acceptance rate for all faces with neutral and non-neutral expressions. Further, the robustness of our algorithm under different occlusions has been demonstrated on the Bosphorus dataset. {\textcopyright} 2016 Elsevier B.V.},
annote = {cited By 7
28/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
28/04 Exclu{\'{i}}do (etapa 1)},
author = {Guo, Y and Lei, Y and Liu, L and Wang, Y and Bennamoun, M and Sohel, F},
doi = {10.1016/j.patrec.2016.04.003},
journal = {Pattern Recognition Letters},
keywords = {etapa1,id384,isi,poly,revisao{\_}scopus,revisao{\_}webofscience,scopus},
mendeley-tags = {etapa1,id384,isi,poly,revisao{\_}scopus,revisao{\_}webofscience,scopus},
pages = {403--412},
title = {{EI3D: Expression-invariant 3D face recognition based on feature and shape matching}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966659227{\&}doi=10.1016{\%}2Fj.patrec.2016.04.003{\&}partnerID=40{\&}md5=b09b19b7a5436b53278c02d001e93910},
volume = {83},
year = {2016}
}
@article{ISI:000363075300013,
abstract = {Tree detection and tree species recognition are bottlenecks of the
airborne remote sensing-based single tree inventories. The effect of
these factors in forest attribute estimation can be reduced if airborne
measurements are aided with tree mapping information that is collected
from the ground. The main objective here was to demonstrate the use of
terrestrial laser scanning-derived (TLS) tree maps in aiding airborne
laser scanning-based (ALS) single tree inventory (multisource single
tree inventory, MS-STI) and its capability in predicting diameter
distribution in various forest conditions. Automatic measurement of TLS
point clouds provided the tree maps and the required reference
information from the tree attributes. The study area was located in Evo,
Finland, and the reference data was acquired from 27 different sample
plots with varying forest conditions. The workflow of MS-STI included:
(1) creation of automatic tree map from TLS point clouds, (2) automatic
diameter at breast height (DBH) measurement from TLS point clouds, (3)
individual tree detection (ITD) based on ALS, (4) matching the ITD
segments to the field-measured reference, (5) ALS point cloud metric
extraction from the single tree segments and (6) DBH estimation based on
the derived metrics. MS-STI proved to be accurate and efficient method
for DBH estimation and predicting diameter distribution. The overall
accuracy (root mean squared error, RMSE) of the DBH was 36.9 mm. Results
showed that the DBH accuracy decreased if the tree density (trees/ha)
increased. The highest accuracies were found in old-growth forests (tree
densities less than 500 stems/ha). MS-STI resulted in the best
accuracies regarding Norway spruce (Picea abies (L.) H.
Karst.)-dominated forests (RMSE of 29.9 mm). Diameter distributions were
predicted with low error indices, thereby resulting in a good fit
compared to the reference. Based on the results, diameter distribution
estimation with MS-STI is highly dependent on the forest structure and
the accuracy of the tree maps that are used. The most important
development step in the future for the MS-STI and automatic measurements
of the TLS point cloud is to develop tree species recognition methods
and further develop tree detection techniques. The possibility of using
MLS or harvester data as a basis for the required tree maps should also
be assessed in the future. (C) 2015 International Society for
Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
B.V. All rights reserved.},
author = {Kankare, Ville and Liang, Xinlian and Vastaranta, Mikko and Yu, Xiaowei and Holopainen, Markus and Hyyppa, Juha},
doi = {10.1016/j.isprsjprs.2015.07.007},
issn = {0924-2716},
journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {161--171},
title = {{Diameter distribution estimation with laser scanning based multisource single tree inventory}},
volume = {108},
year = {2015}
}
@article{ISI:000434382900048,
abstract = {This study proposes a novel automatic method for facial landmark
localization relying on geometrical properties of 3D facial surface
working both on complete faces displaying different emotions and in
presence of occlusions. In particular, 12 descriptors coming from
Differential Geometry including the coefficients of the fundamental
forms, Gaussian, mean, principal curvatures, shape index and curvedness
are extracted as facial features and their local geometric properties
are exploited to localize 13 soft-tissue landmarks from eye and nose
areas. The method is deterministic and is backboned by a thresholding
technique designed by studying the behaviour of each geometrical
descriptor in correspondence to the locus of each landmark. Occlusions
are managed by a detection algorithm based on geometrical properties
which allows to proceed with the landmark localization avoiding the
covered areas. Experimentations were carried out on 3132 faces of the
Bosphorus database and of a 230-sized internal database, including
expressive and occluded ones (mouth, eye, and eyeglasses occlusions),
obtaining 4.75 mm mean localization error.},
author = {Vezzetti, Enrico and Marcolin, Federica and Tornincasa, Stefano and Ulrich, Luca and Dagnes, Nicole},
doi = {10.1007/s11042-017-5025-y},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jun},
number = {11},
pages = {14177--14205},
title = {{3D geometry-based automatic landmark localization in presence of facial occlusions}},
volume = {77},
year = {2018}
}
@article{ISI:000449893800008,
abstract = {We investigate the problem of Facial Expression Recognition (FER) using
3D data. Building from one of the most successful frameworks for facial
analysis using exclusively 3D geometry, we extend the analysis from a
curve-based representation into a spectral representation, which allows
a complete description of the underlying surface that can be further
tuned to the desired level of detail. Spectral representations are based
on the decomposition of the geometry in its spatial frequency
components, much like a Fourier transform, which are related to
intrinsic characteristics of the surface. In this work, we propose the
use of Graph Laplacian Features (GLFs), which result from the projection
of local surface patches into a common basis obtained from the Graph
Laplacian eigenspace. We extract patches around facial landmarks and
include a state-of-the-art localization algorithm to allow for
fully-automatic operation. The proposed approach is tested on the three
most popular databases for 3D FER (BU-3DFE, Bosphorus and BU-4DFE) in
terms of expression and AU recognition. Our results show that the
proposed GLFs consistently outperform the curves-based approach as well
as the most popular alternative for spectral representation, Shape-DNA,
which is based on the Laplace Beltrami Operator and cannot provide a
stable basis that guarantee that the extracted signatures for the
different patches are directly comparable. Interestingly, the accuracy
improvement brought by GLFs is obtained also at a lower computational
cost. Considering the extraction of patches as a common step between the
three compared approaches, the curves-based framework requires a costly
elastic deformation between corresponding curves (e.g. based on splines)
and Shape-DNA requires computing an eigen-decomposition of every new
patch to be analyzed. In contrast, GLFs only require the projection of
the patch geometry into the Graph Laplacian eigenspace, which is common
to all patches and can therefore be pre-computed off-line. We also show
that 14 automatically detected landmarks are enough to achieve high FER
and AU detection rates, only slightly below those obtained when using
sets of manually annotated landmarks. (C) 2018 Elsevier B.V. All rights
reserved.},
annote = {12th IEEE International Conference on Automatic Face and Gesture
Recognition (FG), Washington, DC, MAY 30-JUN 03, 2017},
author = {Derkach, Dmytro and Sukno, Federico M},
doi = {10.1016/j.imavis.2018.09.007},
institution = {IEEE; Baidu; Mitsubishi Elect Res Labs Inc; 3dMD; DI4D; Syst {\&} Technol Res; ObjectVideo Labs; MUKH Technologies; IEEE Comp Soc},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
pages = {86--98},
title = {{Automatic local shape spectrum analysis for 3D facial expression recognition}},
volume = {79},
year = {2018}
}
@article{ISI:000418370200006,
abstract = {This paper presents a comprehensive survey of facial feature point
detection with the assistance of abundant manually labeled images.
Facial feature point detection favors many applications such as face
recognition, animation, tracking, hallucination, expression analysis and
3D face modeling. Existing methods are categorized into two primary
categories according to whether there is the need of a parametric shape
model: parametric shape model-based methods and nonparametric shape
model-based methods. Parametric shape model-based methods are further
divided into two secondary classes according to their appearance models:
local part model-based methods (e.g. constrained local model) and
holistic model-based methods (e.g. active appearance model).
Nonparametric shape model-based methods are divided into several groups
according to their model construction process: exemplar-based methods,
graphical model-based methods, cascaded regression-based methods, and
deep learning based methods. Though significant progress has been made,
facial feature point detection is still limited in its success by wild
and real-world conditions: large variations across poses, expressions,
illuminations, and occlusions. A comparative illustration and analysis
of representative methods provides us a holistic understanding and deep
insight into facial feature point detection, which also motivates us to
further explore more promising future schemes. (c) 2017 Elsevier B.V.
All rights reserved.},
author = {Wang, Nannan and Gao, Xinbo and Tao, Dacheng and Yang, Heng and Li, Xuelong},
doi = {10.1016/j.neucom.2017.05.013},
issn = {0925-2312},
journal = {NEUROCOMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
pages = {50--65},
title = {{Facial feature point detection: A comprehensive survey}},
volume = {275},
year = {2018}
}
@article{ISI:000428416300007,
abstract = {In this paper, we propose a novel approach for 3D face reconstruction
from multi-facial images. Given original pose-variant images, coarse 3D
face templates are initialized to reconstruct a refined 3D face mesh in
an iteration manner. Then, we warp original facial images to the 2D
meshes projected from 3D using Sparse Mesh Affine Warp (SMAW). Finally,
we weight the face patches in each view respectively and map the patch
with higher weight to a canonical UV space. For facial images with
arbitrary pose, their invisible regions are filled with the
corresponding UV patches. Poisson editing is applied to blend different
patches seamlessly. We evaluate the proposed method on LFW dataset in
terms of texture refinement and face recognition. The results
demonstrate competitive performance compared to state-of-the-art
methods.},
author = {Gao, Wanshun and Zhao, Xi and An, Jun and Zou, Jianhua},
doi = {10.1142/S0219691318400064},
issn = {0219-6913},
journal = {INTERNATIONAL JOURNAL OF WAVELETS MULTIRESOLUTION AND INFORMATION PROCESSING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
number = {2, SI},
title = {{Multi-pose 3D facial texture refinement for face recognition}},
volume = {16},
year = {2018}
}
@article{ISI:000371781700044,
abstract = {This paper seeks to compare encoded features from both two-dimensional
(2D) and three-dimensional (3D) face images in order to achieve
automatic gender recognition with high accuracy and robustness. The
Fisher vector encoding method is employed to produce 2D, 3D, and fused
features with escalated discriminative power. For 3D face analysis, a
two-source photometric stereo (PS) method is introduced that enables 3D
surface reconstructions with accurate details as well as desirable
efficiency. Moreover, a 2D + 3D imaging device, taking the two-source PS
method as its core, has been developed that can simultaneously gather
color images for 2D evaluations and PS images for 3D analysis. This
system inherits the superior reconstruction accuracy from the standard
(three or more light) PS method but simplifies the reconstruction
algorithm as well as the hardware design by only requiring two light
sources. It also offers great potential for facilitating human computer
interaction by being accurate, cheap, efficient, and nonintrusive. Ten
types of low-level 2D and 3D features have been experimented with and
encoded for Fisher vector gender recognition. Evaluations of the Fisher
vector encoding method have been performed on the FERET database, Color
FERET database, LFW database, and FRGCv2 database, yielding 97.7{\%},
98.0{\%}, 92.5{\%}, and 96.7{\%} accuracy, respectively. In addition, the
comparison of 2D and 3D features has been drawn from a self-collected
dataset, which is constructed with the aid of the 2D + 3D imaging device
in a series of data capture experiments. With a variety of experiments
and evaluations, it can be proved that the Fisher vector encoding method
outperforms most state-of-the-art gender recognition methods. It has
also been observed that 3D features reconstructed by the two-source PS
method are able to further boost the Fisher vector gender recognition
performance, i.e., up to a 6{\%} increase on the self-collected database.
(C) 2016 Optical Society of America},
author = {Zhang, Wenhao and Smith, Melvyn L and Smith, Lyndon N and Farooq, Abdul},
doi = {10.1364/JOSAA.33.000333},
issn = {1084-7529},
journal = {JOURNAL OF THE OPTICAL SOCIETY OF AMERICA A-OPTICS IMAGE SCIENCE AND VISION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {mar},
number = {3},
pages = {333--344},
title = {{Gender recognition from facial images: two or three dimensions?}},
volume = {33},
year = {2016}
}
@article{Liang:2015:BMD:2805325.2805648,
abstract = {Due to the difficulties associated with the collection of 3D samples, 3D face recognition technologies often have to work with smaller than desirable sample sizes. With the aim of enlarging the training number for each subject, we divide each training image into several patches. However, this immediately introduces two further problems for 3D models: high computational cost and dispersive features caused by the divided 3D image patches. We therefore first map 3D face images into 2D depth images, which greatly reduces the dimension of the samples. Though the depth images retain most of the robust features of 3D images, such as pose and illumination invariance, they lose many discriminative features of the original 3D samples. In this study, we propose a Bayesian learning framework to extract the discriminative features from the depth images. Specifically, we concentrate the features of the intra-class patches to a mean feature by maximizing the multivariate Gaussian likelihood function, and, simultaneously, enlarge the distances between the inter-class mean features by maximizing the exponential priori distribution of the mean features. For classification, we use the nearest neighbor classifier combined with the Mahalanobis distance to calculate the distance between the features of the test image and items in the training set. Experiments on two widely-used 3D face databases demonstrate the efficiency and accuracy of our proposed method compared to relevant state-of-the-art methods.},
address = {New York, NY, USA},
annote = {26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Liang, Ronghua and Shen, Wenjia and Li, Xiao-Xin and Wang, Haixia},
doi = {10.1016/j.ins.2015.03.063},
issn = {00200255},
journal = {Information Sciences},
keywords = {3D face recognition,Bayesian learning,Depth image,Single training sample per person,acm,estela,etapa1,id283,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {acm,estela,etapa1,id283,revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
number = {C},
pages = {406--417},
publisher = {Elsevier Science Inc.},
title = {{Bayesian multi-distribution-based discriminative feature extraction for 3D face recognition}},
url = {http://dx.doi.org/10.1016/j.ins.2015.03.063 http://linkinghub.elsevier.com/retrieve/pii/S0020025515002364},
volume = {320},
year = {2015}
}
@article{ISI:000401381100021,
abstract = {We present an image-based 3D face shape reconstruction method which
transfers shape cues inferred from source face images to guide the
reconstruction of the target face. Specifically, a sparse face shape
adaption mechanism is used to generate a target-specific reference shape
by adaptively and selectively combining source face shapes. This
reference shape can also facilitate the reconstruction optimization for
the target shape. As an off-line process, each source shape has been
derived from a set of given sufficient source images (more than 9) based
on a non-Lambertian reflectance model. Such a process allows for the
existence of cast shadow and specularity, and more accurately infers the
source shape. Guided by the target-specific reference shape, the shape
of a target face can be estimated using a small number of images (even
only one). The proposed reconstruction method refers to a lighting
estimation and an albedo estimation for the target face. No standard 3D
shape (such as the high-precision scanned 3D face) is required in the
reconstruction process. Compared to the state-of-the-arts including the
Photometric Stereo, Tensor Spline, the single reference based method,
and the GEM algorithm, the proposed sparse transfer model can produce
visually better facial details and obtain smaller reconstruction errors.
(C) 2017 Elsevier Ltd. All rights reserved.},
author = {Hu, Jian-Fang and Zheng, Wei-Shi and Xie, Xiaohua and Lai, Jianhuang},
doi = {10.1016/j.patcog.2017.03.029},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {272--285},
title = {{Sparse transfer for facial shape-from-shading}},
volume = {68},
year = {2017}
}
@article{ISI:000433517100002,
abstract = {Computer Vision and Multimedia solutions are now offering an increasing
number of applications ready for use by end users in everyday life. Many
of these applications are centered for detection, representation, and
analysis of face and body. Methods based on 2D images and videos are the
most widespread, but there is a recent trend that successfully extends
the study to 3D human data as acquired by a new generation of 3D
acquisition devices. Based on these premises, in this survey, we provide
an overview on the newly designed techniques that exploit 3D human data
and also prospect the most promising current and future research
directions. In particular, we first propose a taxonomy of the
representation methods, distinguishing between spatial and temporal
modeling of the data. Then, we focus on the analysis and recognition of
3D humans from 3D static and dynamic data, considering many applications
for body and face.},
author = {Berretti, Stefano and Daoudi, Mohamed and Turaga, Pavan and Basu, Anup},
doi = {10.1145/3182179},
issn = {1551-6857},
journal = {ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {1, S},
title = {{Representation, Analysis, and Recognition of 3D Humans: A Survey}},
volume = {14},
year = {2018}
}
@article{ISI:000392292000002,
abstract = {3D face shape is essentially a non-rigid free-form surface, which will
produce non-rigid deformation under expression variations. In terms of
that problem, a promising solution named Coherent Point Drift (CPD)
non-rigid registration for the non-rigid region is applied to eliminate
the influence from the facial expression while guarantees 3D surface
topology. In order to take full advantage of the extracted
discriminative feature of the whole face under facial expression
variations, the novel expression-robust 3D face recognition method using
feature-level fusion and feature-region fusion is proposed. Furthermore,
the Principal Component Analysis and Linear Discriminant Analysis in
combination with Rotated Sparse Regression (PL-RSR) dimensionality
reduction method is presented to promote the computational efficiency
and provide a solution to the curse of dimensionality problem, which
benefit the performance optimization. The experimental evaluation
indicates that the proposed strategy has achieved the rank-1 recognition
rate of 97.91 {\%} and 96.71 {\%} based on Face Recognition Grand Challenge
(FRGC) v2.0 and Bosphorus respectively, which means the proposed
approach outperforms state-of-the-art approach.},
author = {Deng, Xing and Da, Feipeng and Shao, Haijian},
doi = {10.1007/s11042-015-3012-8},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {13--31},
title = {{Expression-robust 3D face recognition based on feature-level fusion and feature-region fusion}},
volume = {76},
year = {2017}
}
@inproceedings{7797090,
abstract = {3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases.},
annote = {23/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
23/04/2018 Exclu{\'{i}}do (etapa 1)
04/05/2018 Revisado (etapa 1)},
author = {Gilani, S Z and Mian, A},
booktitle = {2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
doi = {10.1109/DICTA.2016.7797090},
keywords = {convolution,estela,etapa1,face recognition,feature extraction,id181,ieeexplore,im,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {estela,etapa1,id181,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
pages = {1--8},
title = {{Towards Large-Scale 3D Face Recognition}},
year = {2016}
}
@article{ISI:000215156100010,
abstract = {Automatic human facial recognition is an important and complicated task;
it is necessary to design algorithms capable of recognizing the constant
patterns in the face and to use computing resources efficiently. In this
paper we present a novel algorithm to recognize the human face in real
time; the system's input is the depth and color data from the Microsoft
KinectTM device. The algorithm recognizes patterns/shapes on the point
cloud topography. The template of the face is based in facial geometry;
the forensic theory classifies the human face with respect to constant
patterns: cephalometric points, lines, and areas of the face. The
topography, relative position, and symmetry are directly related to the
craniometric points. The similarity between a point cloud cluster and a
pattern description is measured by a fuzzy pattern theory algorithm. The
face identification is composed by two phases: the first phase
calculates the face pattern hypothesis of the facial points, configures
each point shape, the related location in the areas, and lines of the
face. Then, in the second phase, the algorithm performs a search on
these face point configurations.},
author = {Fernandez-Cervantes, Victor and Garcia, Arturo and {Antonio Ramos}, Marco and Mendez, Andres},
doi = {10.13053/CyS-19-3-2015},
issn = {1405-5546},
journal = {COMPUTACION Y SISTEMAS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {3},
pages = {529--546},
title = {{Facial Geometry Identification through Fuzzy Patterns with RGBD Sensor}},
volume = {19},
year = {2015}
}
@article{ISI:000374364300013,
abstract = {In this paper, we design a unified 3D face authentication system for
practical use. First, we propose a facial depth recovery method to
construct a facial depth map from stereoscopic videos. It.effectively
utilize prior facial information and incorporate the visibility term to
classify static and dynamic pixels for robust depth estimation.
Secondly, in order to make 3D face authentication more accurate and
consistent, we present an intrinsic scale feature detection for
interesting points on 3D facial mesh regions. Then, a novel feature
descriptor is proposed, called Local Mesh Scale -Invariant Feature
Transform (LMSIFT) to reflect the different face recognition abilities
in different facial regions. Finally, the sparse optimization problem of
visual codebook is used to 3D face learning. We evaluate our approach on
publicly available 3D face databases and self-collected realistic scene
databases. We also develop an interactive education system to
investigate its performance in practice, which demonstrates the high
performance of the proposed approach for accurate 3D face
authentication. Compared with previous popular approaches, our system
has consistently better performance in terms of effectiveness,
robustness and universality. (C) 2015 Elsevier B.V. All rights reserved.},
author = {Ming, Yue and Hong, Xiaopeng},
doi = {10.1016/j.neucom.2015.07.127},
issn = {0925-2312},
journal = {NEUROCOMPUTING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {SI},
pages = {117--130},
title = {{A unified 3D face authentication framework based on robust local mesh SIFT feature}},
volume = {184},
year = {2016}
}
@article{ISI:000457666900036,
abstract = {We propose a novel method for measuring the nasal similarity among 3D
faces. Firstly, we construct a representation for the nose shape, which
is composed of a set of geodesic curves, each crosses the bridge of the
nose. Next, using these geodesic curves, we formulate a similarity
measure to compare among noses in the curve shape space. Under the
Riemannian framework, the shape space is a quotient space for which the
scaling, translation and rotation are removed. Since the nose similarity
measure is based on the shape comparison, the proposed method has the
following advantages: (1) the similarity measure is robust to facial
expressions since the nose is not affected by facial expressions; (2)
the geometric features of the nose shape match well with the human
perception; (3) the similarity measure is independent of the mesh grid
because the chosen nose curves are not sensitive to the triangular mesh
model. We construct a nasal hierarchical structure for noses
organization which is based on nose similarity measure results. In our
experiments, we evaluate the performance of the proposed method and
compare it with competing methods on three public face databases namely,
FRGC2.0, Texas3D and BosphorusDB. The results show superiority of the
proposed method in terms of both the speed and the accuracy when the
nasal measurements are processed in the nasal hierarchical structure and
the nasal samples with low sampling rate (5{\%}-25{\%} of original point
cloud). (C) 2018 Elsevier Ltd. All rights reserved.},
author = {Lv, Chenlei and Wu, Zhongke and Wang, Xingce and Zhou, Mingquan and Toh, Kar-Ann},
doi = {10.1016/j.patcog.2018.12.006},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {458--469},
title = {{Nasal similarity measure of 3D faces based on curve shape space}},
volume = {88},
year = {2019}
}
@article{ISI:000379266300013,
abstract = {Paper introduces a 3-D shape representation scheme for automatic face
analysis and identification, and demonstrates its invariance to facial
expression. The core of this scheme lies on the combination of
statistical shape modelling and non-rigid deformation matching. While
the former matches 3-D faces with facial expression, the latter provides
a low-dimensional feature vector that controls the deformation of model
for matching the shape of new input, thereby enabling robust
identification of 3-D faces. The proposed scheme is also able to handle
the pose variation without large part of missing data. To assist the
establishment of dense point correspondences, a modified
free-form-deformation based on B-spline warping is applied with the help
of extracted landmarks. The hybrid iterative closest point method is
introduced for matching the models and new data. The feasibility and
effectiveness of the proposed method was investigated using standard
publicly available Gavab and BU-3DFE datasets, which contain faces with
expression and pose changes. The performance of the system was compared
with that of nine benchmark approaches. The experimental results
demonstrate that the proposed scheme provides a competitive solution for
face recognition.},
author = {Quan, Wei and Matuszewski, Bogdan J and Shark, Lik-Kwan},
doi = {10.1007/s10044-014-0439-x},
issn = {1433-7541},
journal = {PATTERN ANALYSIS AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {3},
pages = {765--781},
title = {{Statistical shape modelling for expression-invariant face analysis and recognition}},
volume = {19},
year = {2016}
}
@article{ISI:000423587100013,
abstract = {3D face similarity is a critical issue in computer vision, computer
graphics and face recognition and so on. Since Fr,chet distance is an
effective metric for measuring curve similarity, a novel 3D face
similarity measure method based on Fr,chet distances of geodesics is
proposed in this paper. In our method, the surface similarity between
two 3D faces is measured by the similarity between two sets of 3D curves
on them. Due to the intrinsic property of geodesics, we select geodesics
as the comparison curves. Firstly, the geodesics on each 3D facial model
emanating from the nose tip point are extracted in the same initial
direction with equal angular increment. Secondly, the Fr,chet distances
between the two sets of geodesics on the two compared facial models are
computed. At last, the similarity between the two facial models is
computed based on the Fr,chet distances of the geodesics obtained in the
second step. We verify our method both theoretically and practically. In
theory, we prove that the similarity of our method satisfies three
properties: reflexivity, symmetry, and triangle inequality. And in
practice, experiments are conducted on the open 3D face database GavaDB,
Texas 3D Face Recognition database, and our 3D face database. After the
comparison with iso-geodesic and Hausdorff distance method, the results
illustrate that our method has good discrimination ability and can not
only identify the facial models of the same person, but also distinguish
the facial models of any two different persons.},
author = {Zhao, Jun-Li and Wu, Zhong-Ke and Pan, Zhen-Kuan and Duan, Fu-Qing and Li, Jin-Hua and Lv, Zhi-Han and Wang, Kang and Chen, Yu-Cong},
doi = {10.1007/s11390-018-1814-7},
issn = {1000-9000},
journal = {JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {207--222},
title = {{3D Face Similarity Measure by Fr,chet Distances of Geodesics}},
volume = {33},
year = {2018}
}
@article{ISI:000440782400006,
abstract = {Caricatures refer to a representation of a person, in which the
distinctive features are deliberately exaggerated, with several studies
showing that humans perform better at recognizing people from
caricatures than using original images. Inspired by this observation,
this paper introduces the first fully automated caricature-based face
recognition approach capable of working with data acquired in the wild.
Our approach leverages the 3D face structure from a single 2D image and
compares it with a reference model for obtaining a compact
representation of face features deviations. This descriptor is
subsequently deformed using a ``measure locally, weight globally{\{}''{\}}
strategy to resemble the caricature drawing process. The deformed
deviations are incorporated in the 3D model using the Laplacian mesh
deformation algorithm, and the 2D face caricature image is obtained by
projecting the deformed model in the original camera view. To
demonstrate the advantages of caricature-based face recognition, we
train the VGG-face network from scratch using either original face
images (baseline) or caricatured images and use these models for
extracting face descriptors from the LFW, IJB-A, and MegaFace data sets.
The experiments show an increase in the recognition accuracy when using
caricatures rather than original images. Moreover, our approach achieves
competitive results with the state-of-the-art face recognition methods,
even without explicitly tuning the network for any of the evaluation
sets.},
author = {Neves, Joao and Proenca, Hugo},
doi = {10.1109/TIFS.2018.2846617},
issn = {1556-6013},
journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {151--161},
title = {{``A Leopard Cannot Change Its Spots{\{}''{\}}: Improving Face Recognition Using 3D-Based Caricatures}},
volume = {14},
year = {2019}
}
@article{ISI:000433909100002,
abstract = {The paper presents a dictionary integration algorithm using 3D morphable
face models (3DMM) for pose-invariant collaborative-representation-based
face classification. To this end, we first fit a 3DMM to the 2D face
images of a dictionary to reconstruct the 3D shape and texture of each
image. The 3D faces are used to render a number of virtual 2D face
images with arbitrary pose variations to augment the training data, by
merging the original and rendered virtual samples to create an extended
dictionary. Second, to reduce the information redundancy of the extended
dictionary and improve the sparsity of reconstruction coefficient
vectors using collaborative-representation-based classification (CRC),
we exploit an on-line class elimination scheme to optimise the extended
dictionary by identifying the training samples of the most
representative classes for a given query. The final goal is to perform
pose-invariant face classification using the proposed dictionary
integration method and the on-line pruning strategy under the CRC
framework. Experimental results obtained for a set of well-known face
data sets demonstrate the merits of the proposed method, especially its
robustness to pose variations.},
author = {Song, Xiaoning and Feng, Zhen-Hua and Hu, Guosheng and Kittler, Josef and Wu, Xiao-Jun},
doi = {10.1109/TIFS.2018.2833052},
issn = {1556-6013},
journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {nov},
number = {11},
pages = {2734--2745},
title = {{Dictionary Integration Using 3D Morphable Face Models for Pose-Invariant Collaborative-Representation-Based Classification}},
volume = {13},
year = {2018}
}
@inproceedings{ISI:000380429100020,
abstract = {With the increasing availability of low-cost 3D data acquisition
devices, the use of 3D face data for the recognition of individuals is
becoming more appealing and computationally feasible. This paper
proposes a completely automatic algorithm for face registration and
matching. The algorithm is based on the extraction of stable 3D facial
features characterizing the face and the subsequent construction of a
signature manifold. The facial features are extracted by performing a
continuous-to-discrete scale-space analysis. Registration is driven from
the matching of triplets of feature points and the registration error is
computed as shape matching score. Conversely to most techniques in the
literature, a major advantage of the proposed method is that no data
pre-processing is required. Therefore all presented results have been
obtained exclusively from the raw data available from the 3D acquisition
device.
The method has been tested on the Bosphorus 3D face database and the
performances compared to the ICP baseline algorithm. Even in presence of
noise in the data, the algorithm proved to be very robust and reported
identification performances which are aligned to the current state of
the art, but without requiring any pre-processing of the raw data.},
annote = {2015 3rd International Workshop on Biometrics and Forensics (IWBF),
Gjovik, NORWAY, MAR 03-04, 2015},
author = {Lagorio, A and Cadoni, M and Grosso, E and Tistarelli, M},
booktitle = {2015 INTERNATIONAL WORKSHOP ON BIOMETRICS AND FORENSICS (IWBF)},
isbn = {978-1-4799-8105-2},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {European Cooperat Sci {\&} Technol; IEEE; COST Act IC1106; Inst Engn {\&} Technol; European Assoc Signal Proc; European Assoc Biometr; HOGSKOLEN},
title = {{A 3D ALGORITHM FOR UNSUPERVISED FACE IDENTIFICATION}},
year = {2015}
}
@article{ISI:000398720100002,
abstract = {Small unmanned aerial vehicle (UAV) based remote sensing is a rapidly
evolving technology. Novel sensors and methods are entering the market,
offering completely new possibilities to carry out remote sensing tasks.
Three-dimensional (3D) hyperspectral remote sensing is a novel and
powerful technology that has recently become available to small UAVs.
This study investigated the performance of UAV-based photogrammetry and
hyperspectral imaging in individual tree detection and tree species
classification in boreal forests. Eleven test sites with 4151 reference
trees representing various tree species and developmental stages were
collected in June 2014 using a UAV remote sensing system equipped with a
frame format hyperspectral camera and an RGB camera in highly variable
weather conditions. Dense point clouds were measured photogrammetrically
by automatic image matching using high resolution RGB images with a 5 cm
point interval. Spectral features were obtained from the hyperspectral
image blocks, the large radiometric variation of which was compensated
for by using a novel approach based on radiometric block adjustment with
the support of in-flight irradiance observations. Spectral and 3D point
cloud features were used in the classification experiment with various
classifiers. The best results were obtained with Random Forest and
Multilayer Perceptron (MLP) which both gave 95{\%} overall accuracies and
an F-score of 0.93. Accuracy of individual tree identification from the
photogrammetric point clouds varied between 40{\%} and 95{\%}, depending on
the characteristics of the area. Challenges in reference measurements
might also have reduced these numbers. Results were promising,
indicating that hyperspectral 3D remote sensing was operational from a
UAV platform even in very difficult conditions. These novel methods are
expected to provide a powerful tool for automating various environmental
close-range remote sensing tasks in the very near future.},
author = {Nevalainen, Olli and Honkavaara, Eija and Tuominen, Sakari and Viljanen, Niko and Hakala, Teemu and Yu, Xiaowei and Hyyppa, Juha and Saari, Heikki and Polonen, Ilkka and Imai, Nilton N and Tommaselli, Antonio M G},
doi = {10.3390/rs9030185},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {mar},
number = {3},
title = {{Individual Tree Detection and Classification with UAV-Based Photogrammetric Point Clouds and Hyperspectral Imaging}},
volume = {9},
year = {2017}
}
@article{ISI:000438934400005,
abstract = {Methodologies for 3D face recognition which work in the presence of
occlusions are core for the current needs in the field of identification
of suspects, as criminals try to take advantage of the weaknesses among
the implemented security systems by camouflaging themselves and
occluding their face with eyeglasses, hair, hands, or covering their
face with scarves and hats. Recent occlusion detection and restoration
strategies for recognition purposes of 3D partially occluded faces with
unforeseen objects are here presented in a literature review. The
research community has worked on face recognition systems under
controlled environments, but uncontrolled conditions have been
investigated in a lesser extent. The paper details the experiments and
databases used to handle the problem of occlusion and the results
obtained by different authors. Lastly, a comparison of various
techniques is presented and some conclusions are drawn referring to the
best outcomes.},
author = {Dagnes, Nicole and Vezzetti, Enrico and Marcolin, Federica and Tornincasa, Stefano},
doi = {10.1007/s00138-018-0933-z},
issn = {0932-8092},
journal = {MACHINE VISION AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
number = {5},
pages = {789--813},
title = {{Occlusion detection and restoration techniques for 3D face recognition: a literature review}},
volume = {29},
year = {2018}
}
@inproceedings{ISI:000450073500030,
abstract = {This paper proposes a technique to perform segmentation for the
meaningful regions that part of the face captured by 3D scanners or 3D
sensors, automatically. Each part recognition of the scanned face is
vital for the 3D applications such as modeling, animation and 3D
printing. We transfer the template model labeled with the meaningful
part to the scanned face model to find the corresponding part of each
meaningful part of the template model. This technique can be used to the
several applications such as 3D face modeling, facial animation, virtual
facial surgery and 3D printing.},
annote = {IEEE International Conference on Consumer Electronics (ICCE), Las Vegas,
NV, JAN 12-14, 2018},
author = {Lim, Seong-Jae and Hwang, Bon-Woo and Yoon, Seung-Uk and Choi, Jin Sung and Park, Chang-Joon},
booktitle = {2018 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE)},
editor = {{Mohanty, SP and Corcoran, P and Li, H and Sengupta, A and Lee, JH}},
isbn = {978-1-5386-3025-9},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE},
series = {International Conference on Consumer Electronics},
title = {{Automatic 3D Face Component Analysis Technique}},
year = {2018}
}
@inproceedings{ISI:000457843608057,
abstract = {We present a method for training a regression network from image pixels
to 3D morphable model coordinates using only unlabeled photographs. The
training loss is based on features from a facial recognition network,
computed on the -fly by rendering the predicted faces with a
differentiable renderer To make training from features feasible and
avoid network fooling effects, we introduce three objectives: a batch
distribution loss that encourages the output distribution to match the
distribution of the morphable model, a loop back loss that ensures the
network can correctly reinterpret its own output, and a multi-view
identity loss that compares the features of the predicted 3D face and
the input photograph from multiple viewing angles. We train a regression
network using these objectives, a set of unlabeled photographs, and the
morphable model itself and demonstrate state-of-the-art results.},
annote = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Salt Lake City, UT, JUN 18-23, 2018},
author = {Genova, Kyle and Cole, Forrester and Maschinot, Aaron and Sarna, Aaron and Vlasic, Daniel and Freeman, William T},
booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
doi = {10.1109/CVPR.2018.00874},
isbn = {978-1-5386-6420-9},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; CVF; IEEE Comp Soc},
pages = {8377--8386},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Unsupervised Training for 3D Morphable Model Regression}},
year = {2018}
}
@article{ISI:000428320200008,
abstract = {In 2016, we described that missense variants in parts of exons 30 and 31
of CREBBP can cause a phenotype that differs from Rubinstein-Taybi
syndrome (RSTS). Here we report on another 11 patients with variants in
this region of CREBBP (between bp 5,128 and 5,614) and two with variants
in the homologous region of EP300. None of the patients show
characteristics typical for RSTS. The variants were detected by exome
sequencing using a panel for intellectual disability in all but one
individual, in whom Sanger sequencing was performed upon clinical
recognition of the entity. The main characteristics of the patients are
developmental delay (90{\%}), autistic behavior (65{\%}), short stature
(42{\%}), and microcephaly (43{\%}). Medical problems include feeding
problems (75{\%}), vision (50{\%}), and hearing (54{\%}) impairments,
recurrent upper airway infections (42{\%}), and epilepsy (21{\%}). Major
malformations are less common except for cryptorchidism (46{\%} of males),
and cerebral anomalies (70{\%}). Individuals with variants between bp
5,595 and 5,614 of CREBBP show a specific phenotype (ptosis, telecanthi,
short and upslanted palpebral fissures, depressed nasal ridge, short
nose, anteverted nares, short columella, and long philtrum). 3D face
shape demonstrated resemblance to individuals with a duplication of
16p13.3 (the region that includes CREBBP), possibly indicating a gain of
function. The other affected individuals show a less specific phenotype.
We conclude that there is now more firm evidence that variants in these
specific regions of CREBBP and EP300 result in a phenotype that differs
from RSTS, and that this phenotype may be heterogeneous.},
author = {Menke, Leonie A and Gardeitchik, Thatjana and Hammond, Peter and Heimdal, Ketil R and Houge, Gunnar and Hufnagel, Sophia B and Ji, Jianling and Johansson, Stefan and Kant, Sarina G and Kinning, Esther and Leon, Eyby L and Newbury-Ecob, Ruth and Paolacci, Stefano and Pfundt, Rolph and Ragge, Nicola K and Rinne, Tuula and Ruivenkamp, Claudia and Saitta, Sulagna C and Sun, Yu and Tartaglia, Marco and Terhal, Paulien A and van Essen, Anthony J and Vigeland, Magnus D and Xiao, Bing and Hennekam, Raoul C and Study, D D D},
doi = {10.1002/ajmg.a.38626},
issn = {1552-4825},
journal = {AMERICAN JOURNAL OF MEDICAL GENETICS PART A},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {4},
pages = {862--876},
title = {{Further delineation of an entity caused by CREBBP and EP300 mutations but not resembling Rubinstein-Taybi syndrome}},
volume = {176},
year = {2018}
}
@article{ISI:000401423700003,
abstract = {Large trees are important to a wide variety of wildlife, including many
species of conservation concern, such as the California spotted owl
(Strix occidentalis occidentalis). Light detection and ranging (LiDAR)
has been successfully utilized to identify the density of large-diameter
trees, either by segmenting the LiDAR point cloud into individual trees,
or by building regression models between variables extracted from the
LiDAR point cloud and field data. Neither of these methods is easily
accessible for most land managers due to the reliance on specialized
software, and much available LiDAR data are being underutilized due to
the steep learning curve required for advanced processing using these
programs. This study derived a simple, yet effective method for
estimating the density of large-stemmed trees from the LiDAR canopy
height model, a standard raster product derived from the LiDAR point
cloud that is often delivered with the LiDAR and is easy to process by
personnel trained in geographic information systems (GIS). Ground plots
needed to be large (1 ha) to build a robust model, but the spatial
accuracy of plot center was less crucial to model accuracy. We also
showed that predicted large tree density is positively linked to
California spotted owl nest sites.},
author = {Kramer, Heather A and Collins, Brandon M and Gallagher, Claire V and Keane, John J and Stephens, Scott L and Kelly, Maggi},
doi = {10.1002/ecs2.1593},
issn = {2150-8925},
journal = {ECOSPHERE},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {12},
title = {{Accessible light detection and ranging: estimating large tree density for habitat identification}},
volume = {7},
year = {2016}
}
@article{ISI:000347747000011,
abstract = {Establishing correct correspondences between two faces with different
viewpoints has played an important role in 3D face reconstruction and
other computer-vision applications. Usually, face images are considered
to lack sufficient distinctive features to establish a large number of
correspondences on uncalibrated images. In this paper, we investigate
pore-scale facial features, which are formed from pores, fine wrinkles,
and hair. These features have many characteristics that make them
suitable for matching facial images under different variations. Using
both biological observation and computervision consideration, a new
framework is devised for pore-scale facial-feature extraction and
matching. The matching difficulty under various skin appearances of
different subjects and imaging distortion is also analyzed. For further
improving the matching performance and tackling distortions such as
varying illuminations and unfocused blurring, a pore-to-pore
correspondences dataset is established for training a more distinctive
and compact descriptor. Experiments are conducted on a face database
containing 105 subjects, and the results prove that the pore-scale
features are highly distinctive; face images with a minimum resolution
of 600 x 700 (0.4 mega) pixels contain sufficient details to perform a
reliable matching in different poses. Generally, our algorithm can
establish between 500 and 2000 correct correspondences on a pair of
uncalibrated face images of the same person. Furthermore, the proposed
methods can be applied to face recognition, 3D reconstruction, etc. (C)
2014 Elsevier Ltd. All rights reserved.},
author = {Li, Dong and Lam, Kin-Man},
doi = {10.1016/j.patcog.2014.09.026},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
number = {3},
pages = {732--745},
title = {{Design and learn distinctive features from pore-scale facial keypoints}},
volume = {48},
year = {2015}
}
@article{ISI:000357545400014,
abstract = {The localization and reconstruction of individual trees as well as the
extraction of their geometrical parameters is an important field of
research in both forestry and remote sensing. While the current
state-of-the-art mostly focuses on the exploitation of optical imagery
and airborne LiDAR data, modern SAR sensors have not yet met the
interest of the research community in that regard. This paper presents a
prototypical processing chain for the reconstruction of individual
deciduous trees: First, single-pass multi-baseline InSAR data acquired
from multiple aspect angles are used for the generation of a layover-
and shadow-free 3D point cloud by tomographic SAR processing. The
resulting point cloud is then segmented by unsupervised mean shift
clustering, before ellipsoid models are fitted to the points of each
cluster. From these 3D ellipsoids the relevant geometrical tree
parameters are extracted. Evaluation with respect to a manually derived
reference dataset prove that almost 74{\%} of all trees are successfully
segmented and reconstructed, thus providing a promising perspective for
further research toward individual tree recognition from SAR data. (C)
2015 The Authors. Published by Elsevier Inc This is an open access
article under the CC BY-NC-ND license.},
author = {Schmitt, Michael and Shahzad, Muhammad and Zhu, Xiao Xiang},
doi = {10.1016/j.rse.2015.05.012},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {175--185},
title = {{Reconstruction of individual trees from multi-aspect TomoSAR data}},
volume = {165},
year = {2015}
}
@inproceedings{ISI:000389494900012,
abstract = {Recent Face Analysis advances have focused the attention on studying and
formalizing 3D facial shape. Landmarks, i.e. typical points of the face,
are perfectly suited to the purpose, as their position on visage shape
allows to build up a map of each human being's appearance. This turns to
be extremely useful for a large variety of fields and related
applications. In particular, the forensic context is taken into
consideration in this study. This work is intended as a survey of
current research advances in forensic science involving 3D facial
landmarks. In particular, by selecting recent scientific contributions
in this field, a literature review is proposed for in-depth analyzing
which landmarks are adopted, and how, in this discipline. The main
outcome concerns the identification of a leading research branch, which
is landmark-based facial reconstruction from skull. The choice of
selecting 3D contributions is driven by the idea that the most
innovative Face Analysis research trends work on three-dimensional data,
such as depth maps and meshes, with three-dimensional software and
tools. The third dimension improves the accurateness and is robust to
colour and lightning variations.},
annote = {3rd International Conference on Augmented Reality, Virtual Reality and
Computer Graphics (SALENTO AVR), Otranto, ITALY, JUN 15-18, 2016},
author = {Vezzetti, Enrico and Marcolin, Federica and Tornincasa, Stefano and Moos, Sandro and Violante, Maria Grazia and Dagnes, Nicole and Monno, Giuseppe and Uva, Antonio Emmanuele and Fiorentino, Michele},
booktitle = {Augmented Reality, Virtual Reality, and Computer Graphics, Pt I},
doi = {10.1007/978-3-319-40621-3_12},
editor = {{DePaolis, LT and Mongelli, A}},
isbn = {978-3-319-40621-3; 978-3-319-40620-6},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Univ Salento, Dept Engn Innovat},
pages = {172--180},
series = {Lecture Notes in Computer Science},
title = {{Facial Landmarks for Forensic Skull-Based 3D Face Reconstruction: A Literature Review}},
volume = {9768},
year = {2016}
}
@article{ISI:000410197200003,
abstract = {This paper presents an effective 3D face keypoint detection, description
and matching framework based on three principle curvature measures.
These measures give a unified definition of principle curvatures for
both smooth and discrete surfaces. They can be reasonably computed based
on the normal cycle theory and the geometric measure theory. The strong
theoretical basis of these measures provides us a solid discrete
estimation method on real 3D face scans represented as triangle meshes.
Based on these estimated measures, the proposed method can automatically
detect a set of sparse and discriminating 3D facial feature points. The
local facial shape around each 3D feature point is comprehensively
described by histograms of these principal curvature measures. To
guarantee the pose invariance of these descriptors, three principle
curvature vectors of these principle curvature measures are employed to
assign the canonical directions. Similarity comparison between faces is
accomplished by matching all these curvature-based local shape
descriptors using the sparse representation-based reconstruction method.
The proposed method was evaluated on three public databases, i.e. FRGC
v2.0, Bosphorus, and Gavab. Experimental results demonstrated that the
three principle curvature measures contain strong complementarity for 3D
facial shape description, and their fusion can largely improve the
recognition performance. Our approach achieves rank-one recognition
rates of 99.6, 95.7, and 97.9{\%} on the neutral subset, expression
subset, and the whole FRGC v2.0 databases, respectively. This indicates
that our method is robust to moderate facial expression variations.
Moreover, it also achieves very competitive performance on the pose
subset (over 98.6{\%} except Yaw 90A degrees) and the occlusion subset
(98.4{\%}) of the Bosphorus database. Even in the case of extreme pose
variations like profiles, it also significantly outperforms the
state-of-the-art approaches with a recognition rate of 57.1{\%}. The
experiments carried out on the Gavab databases further demonstrate the
robustness of our method to varies head pose variations.},
author = {Tang, Yinhang and Li, Huibin and Sun, Xiang and Morvan, Jean-Marie and Chen, Liming},
doi = {10.1007/s10851-017-0728-2},
issn = {0924-9907},
journal = {JOURNAL OF MATHEMATICAL IMAGING AND VISION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {2},
pages = {211--233},
title = {{Principal Curvature Measures Estimation and Application to 3D Face Recognition}},
volume = {59},
year = {2017}
}
@inproceedings{ISI:000380586200014,
abstract = {To address the problem of 3D face modeling based on a set of landmarks
on images, the traditional feature-based morphable model, using face
class-specific information, makes direct use of these 2D points to infer
a dense 3D face surface. However, the unknown depth of landmarks
degrades accuracy considerably. A promising solution is to predict the
depth of landmarks at first. Bases on this idea, a two-stage estimation
method is proposed to compute the depth value of landmarks from two
images. And then, the estimated 3D landmarks are applied to a
deformation algorithm to make a precise 3D dense facial shape. Test
results on synthesized images with known ground-truth show that the
proposed two-stage estimation method can obtain landmarks' depth both
effectively and efficiently, and further that the reconstructed accuracy
is greatly enhanced with the estimated 3D landmarks. Reconstruction
results of real-world photos are rather realistic.},
annote = {IEEE International Conference on Identity, Security and Behavior
Analysis (ISBA), Hong Kong, PEOPLES R CHINA, MAR 23-25, 2015},
author = {Gong, Xun and Fu, Zehua and Li, Xinxin and Feng, Lin},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IDENTITY, SECURITY AND BEHAVIOR ANALYSIS (ISBA)},
isbn = {978-1-4799-1974-1},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE},
title = {{A Two-Stage Estimation Method for Depth Estimation of Facial Landmarks}},
year = {2015}
}
@inproceedings{ISI:000446394502083,
abstract = {This work proposes a process for efficiently searching over combinations
of individual object 6D pose hypotheses in cluttered scenes, especially
in cases involving occlusions and objects resting on each other. The
initial set of candidate object poses is generated from state-of-the-art
object detection and global point cloud registration techniques. The
best scored pose per object by using these techniques may not be
accurate due to overlaps and occlusions. Nevertheless, experimental
indications provided in this work show that object poses with lower
ranks may be closer to the real poses than ones with high ranks
according to registration techniques. This motivates a global
optimization process for improving these poses by taking into account
scene-level physical interactions between objects. It also implies that
the Cartesian product of candidate poses for interacting objects must be
searched so as to identify the best scene-level hypothesis. To perform
the search efficiently, the candidate poses for each object are
clustered so as to reduce their number but still keep a sufficient
diversity. Then, searching over the combinations of candidate object
poses is performed through a Monte Carlo Tree Search (MCTS) process that
uses the similarity between the observed depth image of the scene and a
rendering of the scene given the hypothesized pose as a score that
guides the search procedure. MCTS handles in a principled way the
tradeoff between fine-tuning the most promising poses and exploring new
ones, by using the Upper Confidence Bound (UCB) technique. Experimental
results indicate that this process is able to quickly identify in
cluttered scenes physically-consistent object poses that are
significantly closer to ground truth compared to poses found by point
cloud registration methods.},
annote = {IEEE International Conference on Robotics and Automation (ICRA),
Brisbane, AUSTRALIA, MAY 21-25, 2018},
author = {Mitash, Chaitanya and Boularias, Abdeslam and Bekris, Kostas E},
booktitle = {2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
isbn = {978-1-5386-3081-5},
issn = {1050-4729},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {IEEE; CSIRO; Australian Govt, Dept Def Sci {\&} Technol; DJI; Queensland Univ Technol; Woodside; Baidu; Bosch; Houston Mechatron; Kinova Robot; KUKA; Hit Robot Grp; Honda Res Inst; iRobot; Mathworks; NuTonomy; Ouster; Uber},
pages = {3331--3338},
series = {IEEE International Conference on Robotics and Automation ICRA},
title = {{Improving 6D Pose Estimation of Objects in Clutter via Physics-aware Monte Carlo Tree Search}},
year = {2018}
}
@article{ISI:000397220000009,
abstract = {Gaze behavior during scene and object recognition can highlight the
relevant information for a task. For example, salience maps-highlighting
regions that have heightened luminance, contrast, color, etc. in a
scenecan be used to predict gaze targets. Certain tasks, such as face
recognition, result in a typical pattern of fixations on high salience
features. While local salience of a 2-D feature may contribute to gaze
behavior and object recognition, we are perfectly capable of recognizing
objects from 3-D depth cues devoid of meaningful 2-D features. Faces can
be recognized from pure texture, binocular disparity, or
structure-from-motion displays (Dehmoobadsharifabadi {\&} Farivar, 2016;
Farivar, Blanke, {\&} Chaudhuri, 2009; Liu, Collin, Farivar, {\&} Chaudhuri,
2005), and yet these displays are devoid of local salient 2-D features.
We therefore sought to determine whether gaze behavior is driven by an
underlying 3-D representation that is depth-cue invariant or depth-cue
specific. By using a face identification task comprising morphs of 3-D
facial surfaces, we were able to measure identification thresholds and
thereby equate for task difficulty across different depth cues. We found
that gaze behavior for faces defined by shading and texture cues was
highly comparable, but we observed some deviations for faces defined by
binocular disparity. Interestingly, we found no effect of task
difficulty on gaze behavior. The results are discussed in the context of
depth-cue invariant representations for facial surfaces, with gaze
behavior being constrained by low-level limits of depth extraction from
specific cues such as binocular disparity.},
author = {Akhavein, Hassan and Farivar, Reza},
doi = {10.1167/17.2.9},
issn = {1534-7362},
journal = {JOURNAL OF VISION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2},
title = {{Gaze behavior during 3-D face identification is depth cue invariant}},
volume = {17},
year = {2017}
}
@inproceedings{ISI:000371977804130,
abstract = {The need to compare separate objects arises in a wide range of
applications. In one approach for comparing objects, `ShapeDNA' is
constructed to give a numerical fingerprint representing an individual
object. Shape-DNA is a cropped set of eigenvalues of the
Laplace-Beltrami operator for the surface of the object. In this paper,
we compute the Shape-DNA of surfaces using the closest point method. Our
approach may be applied to a variety of surface representations
including triangulations, point clouds and certain analytical shapes. A
2D multidimensional scaling plot illustrates that similar objects form
groups based on the Shape-DNAs. Our method has the benefit that it may
be applied to surfaces defined by dense point clouds without requiring
the construction of point connectivity.},
annote = {IEEE International Conference on Image Processing (ICIP), Quebec City,
CANADA, SEP 27-30, 2015},
author = {Arteaga, Reynaldo J and Ruuth, Steven J},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4799-8339-1},
issn = {1522-4880},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; IEEE Signal Proc Soc},
pages = {4511--4515},
series = {IEEE International Conference on Image Processing ICIP},
title = {{LAPLACE-BELTRAMI SPECTRA FOR SHAPE COMPARISON OF SURFACES IN 3D USING THE CLOSEST POINT METHOD}},
year = {2015}
}
@inproceedings{ISI:000387959204037,
abstract = {``Frontalization{\{}''{\}} is the process of synthesizing frontal facing views
of faces appearing in single unconstrained photos. Recent reports have
suggested that this process may substantially boost the performance of
face recognition systems. This, by transforming the challenging problem
of recognizing faces viewed from unconstrained viewpoints to the easier
problem of recognizing faces in constrained, forward facing poses.
Previous frontalization methods did this by attempting to approximate 3D
facial shapes for each query image. We observe that 3D face shape
estimation from unconstrained photos may be a harder problem than
frontalization and can potentially introduce facial misalignments.
Instead, we explore the simpler approach of using a single, unmodified,
3D surface as an approximation to the shape of all input faces. We show
that this leads to a straightforward, efficient and easy to implement
method for frontalization. More importantly, it produces aesthetic new
frontal views and is surprisingly effective when used for face
recognition and gender estimation.},
annote = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Boston, MA, JUN 07-12, 2015},
author = {Hassner, Tal and Harel, Shai and Paz, Eran and Enbar, Roee},
booktitle = {2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
isbn = {978-1-4673-6964-0},
issn = {1063-6919},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE},
pages = {4295--4304},
series = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Effective Face Frontalization in Unconstrained Images}},
year = {2015}
}
@article{ISI:000449993800083,
abstract = {To meet a growing demand for accurate high-fidelity vegetation cover
mapping in urban areas toward biodiversity conservation and assessing
the impact of climate change, this paper proposes a complete approach to
species and vitality classification at single tree level by synergistic
use of multimodality 3D remote sensing data. So far, airborne laser
scanning system (ALS or airborne LiDAR) has shown promising results in
tree cover mapping for urban areas. This paper analyzes the potential of
mobile laser scanning system/mobile mapping system (MLS/MMS)-based
methods for recognition of urban plant species and characterization of
growth conditions using ultra-dense LiDAR point clouds and provides an
objective comparison with the ALS-based methods. Firstly, to solve the
extremely intensive computational burden caused by the classification of
ultra-dense MLS data, a new method for the semantic labeling of LiDAR
data in the urban road environment is developed based on combining a
conditional random field (CRF) for the context-based classification of
3D point clouds with shape priors. These priors encode geometric
primitives found in the scene through sample consensus segmentation.
Then, single trees are segmented from the labelled tree points using the
3D graph cuts algorithm. Multinomial logistic regression classifiers are
used to determine the fine deciduous urban tree species of conversation
concern and their growth vitality. Finally, the weight-of-evidence
(WofE) based decision fusion method is applied to combine the
probability outputs of classification results from the MLS and ALS data.
The experiment results obtained in city road corridors demonstrated that
point cloud data acquired from the airborne platform achieved even
slightly better results in terms of tree detection rate, tree species
and vitality classification accuracy, although the tree vitality
distribution in the test site is less balanced compared to the species
distribution. When combined with MLS data, overall accuracies of 78{\%}
and 74{\%} for tree species and vitality classification can be achieved,
which has improved by 5.7{\%} and 4.64{\%} respectively compared to the
usage of airborne data only.},
author = {Wu, Jianwei and Yao, Wei and Polewski, Przemyslaw},
doi = {10.3390/rs10091403},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {9},
title = {{Mapping Individual Tree Species and Vitality along Urban Road Corridors with LiDAR and Imaging Sensors: Point Density versus View Perspective}},
volume = {10},
year = {2018}
}
@inproceedings{ISI:000406771301004,
abstract = {Spoofing detection is essential for practical face recognition system.
Based on the fact that genuine face has special geometric curvatures
across surface, this paper brings forward an ultra-fast yet accurate
spoofing detection approach using a low-cost stereo camera. To obtain
curvatures, the three dimensional shapes of selected facial landmarks
are analyzed, by fitting point cloud around each landmark to a specific
partial face surface. Spoofing detection is then performed by evaluating
curvatures of each landmark and integrating them together. Experiments
verify that the approach is able to detect spoofed faces in printed
photographs without or with various bending at FAR equal to 0.00{\%}.
Meanwhile, genuine faces have a trivial opportunity to be falsely
rejected: FRR is 0.59{\%} for near frontal faces and less than 5{\%} for
faces with large varying poses. Detection time is 51 milliseconds when
executed on a single processor {\{}[{\}}1] running at a clock frequency of
266M Hz, this makes the detection very suitable for embedded face
recognition system.},
annote = {23rd International Conference on Pattern Recognition (ICPR), Mexican
Assoc Comp Vis Robot {\&} Neural Comp, Cancun, MEXICO, DEC 04-08, 2016},
author = {Tian, Guifen and Mori, Tatusya and Okuda, Yuji},
booktitle = {2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)},
isbn = {978-1-5090-4847-2},
issn = {1051-4651},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm; Elsevier; IBM Res; INTEL; CONACYT},
pages = {1017--1022},
series = {International Conference on Pattern Recognition},
title = {{Spoofing Detection for Embedded Face Recognition System using A Low Cost Stereo Camera}},
year = {2016}
}
@article{ISI:000450379200003,
abstract = {The assessment of the health conditions of trees in forests is extremely
important for biodiversity, forest management, global environment
monitoring, and carbon dynamics. There is a vast amount of research
using remote sensing (RS) techniques for the assessment of the current
condition of a forest, but only a small number of these are concerned
with detection and classification of dead trees. Among the available RS
techniques, only the airborne laser scanner (ALS) enables dead tree
detection at the single tree level with high accuracy.
The main objective of the study was to identify spruce, pine and
deciduous trees by alive or dead classifications. Three RS data sets
including ALS (leaf-on and leaf-off) and color-infrared (CIR) imagery
(leaf-on) were used for the study. We used intensity and structural
variables from the ALS data and spectral information derived from aerial
imagery for the classification procedure. Additionally, we tested the
differences in the classification accuracy of all variants contained in
the data integration. In the study, the random forest (RF) classifier
was used. The study was carried out in the Polish part of the Bialowieia
Forest (BF).
In general, we can state that all classifications, with different
combinations of ALS features and CIR, resulted in high overall accuracy
(OA {\textgreater}= 90{\%}) and Kappa (kappa {\textgreater} 0.86). For the best variant
(CIR{\_}ALS(WSn-FH)), the mean values of overall accuracy and Kappa were
equal to 94.3{\%} and 0.93, respectively. The leaf -on point cloud
features alone produced the lowest accuracies (OA = 75-81{\%} and x =
0.68-0.76). Improvements of 0-0.04 in the Kappa coefficient and 0-3.1{\%}
in the overall classification accuracy were found after the point cloud
normalization for all variants. Full -height point cloud features (F)
produced lower accuracies than the results based on features calculated
for half of the tree height point clouds (H) and combined FH.
The importance of each of the predictors for different data sets for
tree species classification provided by the RF algorithm was
investigated. The lists of top features were the same, independent of
intensity normalization. For the classification based on both of the
point clouds (leaf on and leaf-off), three structural features (a
proportion of first returns for both half -height and full -height
variants and the canopy relief ratio of points) and two intensity
features from first returns and half -height variant (the coefficient of
variation and skewness) were rated as the most important. In the
classification based on the point cloud with CIR features, two image
features were among the most important (the NDVI and mean value of
reflectance in the green band).},
author = {Kaminska, Agnieszka and Lisiewicz, Maciej and Sterenczak, Krzysztof and Kraszewski, Bartlomiej and Sadkowski, Rafal},
doi = {10.1016/j.rse.2018.10.005},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {31--43},
title = {{Species-related single dead tree detection using multi-temporal ALS data and CIR imagery}},
volume = {219},
year = {2018}
}
@inproceedings{ISI:000433001500012,
abstract = {Face recognition in presence of illumination changes, variant pose and
different facial expressions is a challenging problem. In this paper, a
method for 3D face reconstruction using photometric stereo and without
knowing the illumination directions and facial expression is proposed in
order to achieve improvement in face recognition. A dimensionality
reduction method was introduced to represent the face deformations due
to illumination variations and self shadows in a lower space. The
obtained mapping function was used to determine the illumination
direction of each input image and that direction was used to apply
photometric stereo. Experiments with faces were performed in order to
evaluate the performance of the proposed scheme. From the experiments it
was shown that the proposed approach results very accurate 3D surfaces
without knowing the light directions and with a very small differences
compared to the case of known directions. As a result the proposed
approach is more general and requires less restrictions enabling 3D face
recognition methods to operate with less data.},
annote = {4th IAPR TC 9 Workshop on Pattern Recognition of Social Signals in
Human-Computer-Interaction (MPRSS), Cancun, MEXICO, DEC 04, 2016},
author = {Villarini, Barbara and Gkelias, Athanasios and Argyriou, Vasilios},
booktitle = {MULTIMODAL PATTERN RECOGNITION OF SOCIAL SIGNALS IN HUMAN-COMPUTER-INTERACTION, MPRSS 2016},
doi = {10.1007/978-3-319-59259-6_12},
editor = {{Schwenker, F and Scherer, S}},
isbn = {978-3-319-59259-6; 978-3-319-59258-9},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Int Assoc Pattern Recognit, TC 9 Pattern Recognit Human Comp Interact; Ulm Univ; Univ So Calif; Transreg Collaborat Res Ctr SFB TRR 62 Compan Technol Cognit Tech Syst},
pages = {140--152},
series = {Lecture Notes in Artificial Intelligence},
title = {{Photometric Stereo for 3D Face Reconstruction Using Non Linear Illumination Models}},
volume = {10183},
year = {2017}
}
@article{ISI:000412265100009,
abstract = {Pose variations are still challenging problems in 3D face recognition
because large pose variations will cause self-occlusion and result in
missing data. In this paper, a new method for pose-invariant 3D face
recognition is proposed to handle significant pose variations. For pose
estimation and registration, a coarse-to-fine strategy is proposed to
detect landmarks under large yaw variations. At the coarse search step,
candidate landmarks are detected using HK curvature analysis and
subdivided according to a facial geometrical structure-based
classification strategy. At the fine search step, candidate landmarks
are identified and labeled by comparing with a Facial Landmark Model. By
using the half face matching, we perform the matching step with respect
to frontal scans and side scans. Experiments carried out on the
Bosphorus and UND/FRGC v2.0 databases show that our method has high
accuracy and robustness to pose variations. (C) 2017 Elsevier B.V. All
rights reserved.},
author = {Liang, Yan and Zhang, Yun and Zeng, Xian-Xian},
doi = {10.1016/j.image.2017.05.004},
issn = {0923-5965},
journal = {SIGNAL PROCESSING-IMAGE COMMUNICATION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {84--90},
title = {{Pose-invariant 3D face recognition using half face}},
volume = {57},
year = {2017}
}
@inproceedings{8099647,
abstract = {Monocular 3D facial shape reconstruction from a single 2D facial image has been an active research area due to its wide applications. Inspired by the success of deep neural networks (DNN), we propose a DNN-based approach for End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different from recent works that reconstruct and refine the 3D face in an iterative manner using both an RGB image and an initial 3D facial shape rendering, our DNN model is end-to-end, and thus the complicated 3D rendering process can be avoided. Moreover, we integrate in the DNN architecture two components, namely a multi-task loss function and a fusion convolutional neural network (CNN) to improve facial expression reconstruction. With the multi-task loss function, 3D face reconstruction is divided into neutral 3D facial shape reconstruction and expressive 3D facial shape reconstruction. The neutral 3D facial shape is class-specific. Therefore, higher layer features are useful. In comparison, the expressive 3D facial shape favors lower or intermediate layer features. With the fusion-CNN, features from different intermediate layers are fused and transformed for predicting the 3D expressive facial shape. Through extensive experiments, we demonstrate the superiority of our end-to-end framework in improving the accuracy of 3D face reconstruction.},
annote = {18/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
18/04/2018 Exclu{\'{i}}do (etapa 1)},
archivePrefix = {arXiv},
arxivId = {1704.05020},
author = {Dou, Pengfei and Shah, Shishir K. and Kakadiaris, Ioannis A.},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.164},
eprint = {1704.05020},
isbn = {9781538611241},
issn = {1063-6919},
keywords = {convolution,etapa1,face recognition,gil,id84,ieeexplore,image colour analysis,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,gil,id84,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
pages = {1503--1512},
publisher = {IEEE},
title = {{End-to-end 3D face reconstruction with deep neural networks}},
url = {http://arxiv.org/abs/1704.05020},
year = {2017}
}
@article{ISI:000397371800015,
abstract = {Pose-invariant face recognition (PIFR) refers to the ability that
recognizes face images with arbitrary pose variations. Among existing
PIFR algorithms, pose normalization has been proved to be an effective
approach which preserves texture fidelity, but usually depends on
precise 3D face models or at high computational cost. In this paper, we
propose an highly efficient PIFR algorithm that effectively handles the
main challenges caused by pose variation. First, a dense grid of 3D
facial landmarks are projected to each 2D face image, which enables
feature extraction in an pose adaptive manner. Second, for the local
patch around each landmark, an optimal warp is estimated based on
homography to correct texture deformation caused by pose variations. The
reconstructed frontal-view patches are then utilized for face
recognition with traditional face descriptors. The homography-based
normalization is highly efficient and the synthesized frontal face
images are of high quality. Finally, we propose an effective approach
for occlusion detection, which enables face recognition with visible
patches only. Therefore, the proposed algorithm effectively handles the
main challenges in PIFR. Experimental results on four popular face
databases demonstrate that the proposed approach performs well on both
constrained and unconstrained environments.},
author = {Ding, Changxing and Tao, Dacheng},
doi = {10.1016/j.patcog.2016.11.024},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jun},
number = {SI},
pages = {144--152},
title = {{Pose-invariant face recognition with homography-based normalization}},
volume = {66},
year = {2017}
}
@article{ISI:000411545400029,
abstract = {One of the main modules in a face recognition system is feature
extraction, which has a significant effect on the whole system
performance. In the past decades, various types of feature extractors
and descriptors have been proposed for 3D face recognition. Although
several literature reviews have been carried out on 3D face recognition
algorithms, only a few studies have been performed on feature extraction
methods. The latter have a vital role to overcome degradation
conditions, such as face expression variations and occlusions. Depending
on the types of features used in 3D face recognition, these methods can
be divided into two categories: global and local feature-based methods.
Local feature-based methods have been effectively applied in the
literature, as they are more robust to occlusions and missing data. This
survey presents a state-of-the-art for 3D face recognition using local
features, with the main focus being the extraction of these features.
(C) 2017 Elsevier Ltd. All rights reserved.},
author = {Soltanpour, Sima and Boufama, Boubakeur and Wu, Q M Jonathan},
doi = {10.1016/j.patcog.2017.08.003},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {391--406},
title = {{A survey of local feature methods for 3D face recognition}},
volume = {72},
year = {2017}
}
@article{ISI:000428416300008,
abstract = {Facial landmarking locates the key facial feature points on facial data, which provides not only information on semantic facial structures, but also prior knowledge for other types of facial analysis. However, most of the existing works still focus on the 2D facial image which is quite sensitive to the lighting condition changes. In order to address this limitation, this paper proposed a coarse-to-fine method only based on the 3D facial scan data extracted from professional equipment to automatically and accurately estimate the landmark localization. Specifically, we firstly trained a convolutional neural network (CNN) to initialize the face landmarks instead of the mean shape. Then the proposed cascade regression networks learn the mapping function between 3D facial geometry feature and landmarks location. Tested on Bosphorus database, the experimental results demonstrated effectiveness and accuracy of the proposed method for 22 landmarks. Compared with other methods, the results in several points demonstrate state-of-the-art performance.},
author = {Kai, Wang and An, Jun and Zhao, Xi and Zou, Jianhua},
doi = {10.1142/S0219691318400076},
issn = {0219-6913},
journal = {International Journal of Wavelets, Multiresolution and Information Processing},
keywords = {correction,revisao{\_}webofscience},
mendeley-tags = {correction,revisao{\_}webofscience},
month = {mar},
number = {02},
pages = {1840007},
title = {{Accurate landmarking from 3D facial scans by CNN and cascade regression}},
url = {https://www.worldscientific.com/doi/abs/10.1142/S0219691318400076},
volume = {16},
year = {2018}
}
@article{ISI:000390977800011,
abstract = {In this paper, we propose a 3D-2D framework for face recognition that is
more practical than 3D-3D, yet more accurate than 2D-2D. For 3D-2D face
recognition, the gallery data comprises of 3D shape and 2D texture data
and the probes are arbitrary 2D images. A 3D-2D system (UR2D) is
presented that is based on a 3D deformable face model that allows
registration of 3D and 2D data, face alignment, and normalization of
pose and illumination. During enrollment, subject-specific 3D models are
constructed using 3D+2D data. For recognition, 2D images are represented
in a normalized image space using the gallery 3D models and
landmark-based 3D-2D projection estimation. A method for bidirectional
relighting is applied for non-linear, local illumination normalization
between probe and gallery textures, and a global orientation-based
correlation metric is used for pairwise similarity scoring. The
generated, personalized, pose- and light- normalized signatures can be
used for one-to-one verification or one-to-many identification. Results
for 3D-2D face recognition on the UHDB11 3D-2D database with 2D images
under large illumination and pose variations support our hypothesis
that, in challenging datasets, 3D-2D outperforms 2D-2D and decreases the
performance gap against 3D-3D face recognition. Evaluations on FRGC v2.0
3D-2D data with frontal facial images, demonstrate that the method can
generalize to databases with different and diverse illumination
conditions. (C) 2016 Published by Elsevier Inc.},
author = {Kakadiaris, Ioannis A and Toderici, George and Evangelopoulos, Georgios and Passalis, Georgios and Chu, Dat and Zhao, Xi and Shah, Shishir K and Theoharis, Theoharis},
doi = {10.1016/j.cviu.2016.04.012},
issn = {1077-3142},
journal = {COMPUTER VISION AND IMAGE UNDERSTANDING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
pages = {137--151},
title = {{3D-2D face recognition with pose and illumination normalization}},
volume = {154},
year = {2017}
}
@inproceedings{ISI:000364714000046,
abstract = {In this work, we take advantage of the superiority of Spectral Graph
Theory in classification application and propose a novel deep learning
framework for face analysis which is called Spectral Regression
Discriminant Analysis Network (SRDANet). Our SRDANet model shares the
same basic architecture of Convolutional Neural Network (CNN), which
comprises three basic components: convolutional filter layer, nonlinear
processing layer and feature pooling layer. While it is different from
traditional deep learning network that in our convolutional layer, we
extract the leading eigenvectors from patches in facial image which are
used as filter kernels instead of randomly initializing kernels and
update them by stochastic gradient descent (SGD). And the output of all
cascaded convolutional filter layers is used as the input of nonlinear
processing layer. In the following nonlinear processing layer, we use
hashing method for nonlinear processing. In feature pooling layer, the
block-based histograms are employed to pooling output features instead
of max-pooling technique. At last, the output of feature pooling layer
is considered as one final feature output of our model. Different from
the previous single-task research for face analysis, our proposed
approach demonstrates an excellent performance in face recognition and
expression recognition with 2D/3D facial images simultaneously.
Extensive experiments conducted on many different face analysis
databases demonstrate the efficiency of our proposed SRDANet model.
Databases such as Extended Yale B, PIE, ORL are used for 2D face
recognition, FRGC v2 is used for 3D face recognition and BU-3DFE is used
for 3D expression recognition.},
annote = {8th International Conference on Intelligent Robotics and Applications
(ICIRA), Portsmouth, ENGLAND, AUG 24-27, 2015},
author = {Tian, Lei and Fan, Chunxiao and Ming, Yue and Shi, Jiakun},
booktitle = {INTELLIGENT ROBOTICS AND APPLICATIONS, ICIRA 2015, PT I},
doi = {10.1007/978-3-319-22879-2_46},
editor = {{Liu, H and Kubota, N and Zhu, X and Dillmann, R and Zhou, D}},
isbn = {978-3-319-22879-2; 978-3-319-22878-5},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {499--510},
series = {Lecture Notes in Artificial Intelligence},
title = {{SRDANet: An Efficient Deep Learning Algorithm for Face Analysis}},
volume = {9244},
year = {2015}
}
@article{ISI:000428322500017,
abstract = {Face Recognition is widely used applications such as of mobile phone
unlocking, credit card authentication and person authentication in
airports. The face biometric authentication system can be easily spoofed
by printed photograph, replay video of the legitimate user and 3D face
mask. This paper proposes hybrid feature descriptors to detect the face
spoofing attack (printed photograph and replay video attacks). The
proposed method extracts three different feature descriptors such as
Color moment, Haralick texture and Color Local Binary Pattern (CLBP)
feature descriptors. The extracted features are concatenated and
classified by Logistic Regression. The performance of the proposed
method is evaluated on the Michigan State University Mobile Face
Spoofing Database (MSU-MFSD) dataset and found to achieve better results
than state-of-the-art methods.},
annote = {3rd International Symposium on Intelligent Systems Technologies and
Applications (ISTA), Manipal Univ, Manipal, INDIA, SEP 13-16, 2017},
author = {Mohanraj, V and Chakkaravarthy, S Sibi and Gogul, I and Kumar, V Sathiesh and Kumar, Ranajit and Vaidehi, V},
doi = {10.3233/JIFS-169436},
issn = {1064-1246},
journal = {JOURNAL OF INTELLIGENT {\&} FUZZY SYSTEMS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {3},
pages = {1411--1419},
title = {{Hybrid feature descriptors to detect face spoof attacks}},
volume = {34},
year = {2018}
}
@inproceedings{7581407,
abstract = {The tracking of facial activities from video is an important and challenging problem. Now a day, many computer vision techniques have been proposed to characterize the facial activities in the three levels (from local to global). First level is the bottom level, in which the facial feature tracking focuses on detecting and tracking of the prominent local landmarks surrounding facial components (e.g. mouth, eyebrow, etc), in second level the facial action units (AUs) characterize the specific behaviors of these local facial components (e.g. mouth open, eyebrow raiser, etc) and the third level is facial expression level, which represents subjects emotions (e.g. Surprise, Happy, Anger, etc.) and controls the global muscular movement of the whole face. Most of the existing methods focus on one or two levels of facial activities, and track (or recognize) them separately. In this paper, various facial action tracking models and techniques are compared in different conditions such as the performance of Active Facial Tracking for Fatigue Detection, Real Time 3D Face Pose Tracking from an Uncalibrated Camera, Simultaneous facial action tracking and expression recognition using a particle filter and Simultaneous Tracking and Facial Expression Recognition using Multiperson and Multiclass Autoregressive Models.},
annote = {17/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
17/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Yadav, Prem Chand and Singh, H. V. and Patel, Ankit Kumar and Singh, Anurag},
booktitle = {International Conference on Emerging Trends in Electrical, Electronics and Sustainable Energy Systems, ICETEESES 2016},
doi = {10.1109/ICETEESES.2016.7581407},
isbn = {9781509021185},
keywords = {Face and Facial Features,Facial Expression,Simultaneous Tracking and Recognition,Stochastic Tracking and Fatigue Detection,etapa1,gil,id69,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,gil,id69,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
pages = {347--349},
publisher = {IEEE},
title = {{A comparative analysis of different facial action tracking models and techniques}},
url = {http://ieeexplore.ieee.org/document/7581407/},
year = {2016}
}
@inproceedings{ISI:000392743800092,
abstract = {Three dimensional models obtained from imagery have an arbitrary scale
and therefore have to be scaled. Automatically scaling these models
requires the detection of objects in these models which can be
computationally intensive. Real-time object detection may pose problems
for applications such as indoor navigation. This investigation poses the
idea that relational cues, specifically height ratios, within indoor
environments may offer an easier means to obtain scales for models
created using imagery. The investigation aimed to show two things, (a)
that the size of objects, especially the height off ground is consistent
within an environment, and (b) that based on this consistency, objects
can be identified and their general size used to scale a model. To test
the idea a hypothesis is first tested on a terrestrial lidar scan of an
indoor environment. Later as a proof of concept the same test is applied
to a model created using imagery. The most notable finding was that the
detection of objects can be more readily done by studying the ratio
between the dimensions of objects that have their dimensions defined by
human physiology. For example the dimensions of desks and chairs are
related to the height of an average person. In the test, the difference
between generalised and actual dimensions of objects were assessed. A
maximum difference of 3.96{\%} (2.93cm) was observed from automated
scaling. By analysing the ratio between the heights (distance from the
floor) of the tops of objects in a room, identification was also
achieved.},
annote = {23rd Congress of the
International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
Prague, CZECH REPUBLIC, JUL 12-19, 2016},
author = {Kadamen, Jayren and Sithole, George},
booktitle = {XXIII ISPRS CONGRESS, COMMISSION III},
doi = {10.5194/isprsarchives-XLI-B3-617-2016},
editor = {{Halounova, L and Schindler, K and Limpouch, A and Pajdla, T and Safar, V and Mayer, H and Elberink, SO and Mallet, C and Rottensteiner, F and Bredif, M and Skaloud, J and Stilla, U}},
issn = {2194-9034},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {B3},
organization = {Int Soc Photogrammetry {\&} Remote Sensing},
pages = {617--624},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{AUTOMATICALLY DETERMINING SCALE WITHIN UNSTRUCTU RED POINT CLOUDS}},
volume = {41},
year = {2016}
}
@inproceedings{ISI:000370974903006,
abstract = {Understanding social context is an important skill for robots that share
a space with humans. In this paper, we address the problem of
recognizing gender, a key piece of information when interacting with
people and understanding human social relations and rules. Unlike
previous work which typically considered faces or frontal body views in
image data, we address the problem of recognizing gender in RGB-D data
from side and back views as well. We present a large, gender-balanced,
annotated, multi-perspective RGB-D dataset with full-body views of over
a hundred different persons captured with both the Kinect v1 and Kinect
v2 sensor. We then learn and compare several classifiers on the Kinect
v2 data using a HOG baseline, two state-of-the-art deep-learning
methods, and a recent tessellation-based learning approach. Originally
developed for person detection in 3D data, the latter is able to learn
the best selection, location and scale of a set of simple point cloud
features. We show that for gender recognition, it outperforms the other
approaches for both standing and walking people while being very
efficient to compute with classification rates up to 150 Hz.},
annote = {IEEE International Conference on Robotics and Automation (ICRA),
Seattle, WA, MAY 26-30, 2015},
author = {Linder, Timm and Wehner, Sven and Arras, Kai O},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
isbn = {978-1-4799-6923-4},
issn = {1050-4729},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE},
pages = {3039--3045},
series = {IEEE International Conference on Robotics and Automation ICRA},
title = {{Real-Time Full-Body Human Gender Recognition in (RGB)-D Data}},
year = {2015}
}
@inproceedings{ISI:000385263000030,
abstract = {In this paper, we introduce a novel approach to bypass modern face
authentication systems. More specifically, by leveraging a handful of
pictures of the target user taken from social media, we show how to
create realistic, textured, 3D facial models that undermine the security
of widely used face authentication solutions. Our framework makes use of
virtual reality (VR) systems, incorporating along the way the ability to
perform animations (e.g., raising an eyebrow or smiling) of the facial
model, in order to trick liveness detectors into believing that the 3D
model is a real human face. The synthetic face of the user is displayed
on the screen of the VR device, and as the device rotates and translates
in the real world, the 3D face moves accordingly. To an observing face
authentication system, the depth and motion cues of the display match
what would be expected for a human face.
We argue that such VR-based spoofing attacks constitute a fundamentally
new class of attacks that point to a serious weaknesses in camera-based
authentication systems: Unless they incorporate other sources of
verifiable data, systems relying on color image data and camera motion
are prone to attacks via virtual realism. To demonstrate the practical
nature of this threat, we conduct thorough experiments using an
end-to-end implementation of our approach and show how it undermines the
security of several face authentication solutions that include both
motion-based and liveness detectors.},
annote = {25th USENIX Security Symposium, Austin, TX, AUG 10-12, 2016},
author = {Xu, Yi and Price, True and Frahm, Jan-Michael and Monrose, Fabian},
booktitle = {PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM},
isbn = {978-1-931971-32-4},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {USENIX; Facebook; NSF; Cisco; Google; Microsoft; Neustar; IBM Res; Symantec; ACM Queue; ADMIN; CRC Press; Linux Pro Magazine; NetApp; VMWare; LXer; UserFriendly Org; OReilly Media; No Starch Press; Virus Bulletin},
pages = {497--512},
title = {{Virtual U: Defeating Face Liveness Detection by Building Virtual Models From Your Public Photos}},
year = {2016}
}
@article{ISI:000426222800032,
abstract = {Pose and illumination variations are very challenging for face
recognition with a single sample per person (SSPP). In this paper, we
address this issue by a Pose-Aware Metric Learning (PAML) approach. Our
primary idea is ``from one to many{\{}''{\}}: Synthesizing many images of
sufficient pose and illumination variability from the single training
image, based on which metric learning approach is applied to reduce
these ``synthesized{\{}''{\}} variations at each quantified pose. For this
purpose, given a single frontal training image, a multi-depth generic
elastic model and an extended generic elastic model are developed to
synthesize facial images of the target pose with varying 3D shape
(depth) and illumination variations respectively. To reduce these
``synthesized{\{}''{\}} variability, Pose-Aware Metric spaces are separately
learnt by linear regression analysis at each quantized pose, and
pose-invariant recognition is performed in the corresponding metric
space. By preserving the detailed texture and reducing the shape
variability, the PAML method achieves an 100{\%} accuracy on the Multi-PIE
database under the test setting across poses, which is significantly
better than the traditional methods that use a large generic image
ensemble to learn the cross-pose transformations. On the more
challenging setting across both poses and illuminations, PAML
outperforms the recent deep learning approaches by over 10{\%} accuracy.
(C) 2017 Elsevier Ltd. All rights reserved.},
author = {Deng, Weihong and Hu, Jiani and Wu, Zhongjun and Guo, Jun},
doi = {10.1016/j.patcog.2017.10.020},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {426--437},
title = {{From one to many: Pose-Aware Metric Learning for single-sample face recognition}},
volume = {77},
year = {2018}
}
@article{ISI:000395844700018,
abstract = {Reconstructing 3D face models from 2D face images is usually done by
using a single reference 3D face model or some gender/ethnicity specific
3D face models. However, different persons, even those of the same
gender or ethnicity, usually have significantly different faces in terms
of their overall appearance, which forms the base of person recognition
via faces. Consequently, existing 3D reference model based methods have
limited capability of reconstructing precise 3D face models for a large
variety of persons. In this paper, we propose to explore a reservoir of
diverse reference models for 3D face reconstruction from forensic
mugshot face images, where facial examplars coherent with the input
determine the final shape estimation. Specifically, our 3D face
reconstruction is formulated as an energy minimization problem with: 1)
shading constraint from multiple input face images, 2) distortion and
self-occlusion based color consistency between different views, and 3)
depth uncertainty based smoothness constraint on adjacent pixels. The
proposed energy is minimized in a coarse to fine way, where the shape
refinement step is done by using a multi label segmentation algorithm.
Experimental results on challenging datasets demonstrate that the
proposed algorithm is capable of recovering high quality 3D face models.
We also show that our reconstructed models successfully boost face
recognition accuracy. (C) 2016 Elsevier B.V. All rights reserved.},
author = {Zeng, Dan and Zhao, Qijun and Long, Shuqin and Li, Jing},
doi = {10.1016/j.imavis.2016.03.001},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {193--203},
title = {{Examplar coherent 3D face reconstruction from forensic mugshot database}},
volume = {58},
year = {2017}
}
@inproceedings{ISI:000380434700004,
abstract = {We present an open source cross platform technology for 3D face tracking
and analysis. It contains a full stack of components for complete face
understanding: detection, head pose tracking, facial expression and
action units recognition. Given a depth sensor, one can combine
FaceCept3D modules to fulfill a specific application scenario. Key
advantages of the technology include real time processing speed and
ability to handle extreme head pose variations. Possible application
areas of the technology range from human computer interaction to active
aging, where precise and real-time analysis is required. The technology
is available to community.},
annote = {IEEE International Conference on Computer Vision Workshops, santigo,
CHILE, DEC 11-18, 2015},
author = {Tulyakov, Sergey and Vieriu, Radu-Laurentiu and Sangineto, Enver and Sebe, Nicu},
booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW)},
doi = {10.1109/ICCVW.2015.13},
isbn = {978-0-7695-5720-5},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {CPS; IEEE Comp Soc; amazon; Microsoft; SENSETIME; Baidu; intel; facebook; Adobe; Panasonic; Google; OMRON; blippar; iRobot; HISCENE; nVIDIA; Viscqvery; AiCUre; M/Tec},
pages = {28--33},
title = {{FaceCept3D: Real Time 3D Face Tracking and Analysis}},
year = {2015}
}
@article{Krotewicz201665,
abstract = {This paper concerns the novel region-based ear-assisted 3D face recognition based on iterative closest point (ICP) algorithm in face expression changing scenario. The proposed algorithm for 3D face biometric recognition was prepared and tested. As a first contribution, current state-of-the-art in the field of 3D face recognition is presented and the main approaches to the problem are briefly described. Furthermore, all the data processing steps: preprocessing, segmentation, feature extraction and feature comparison are described in detail. As a second contribution, the algorithm behaviour is scrutinised on the DMCSv1 database and the results in the form of DET curves are highlighted in this paper. Also, the comparison with results obtained by means of the algorithm neglecting ear regions is provided. It occurs that ear geometry information added to face as an auxiliary input to iterative closest point can greatly improve recognition results especially in the case of very strong facial expressions. Equal error rate does not exceed 6.25{\%} on arbitrary data subset. In the last section, conclusions are formulated and plans for future work are presented. {\textcopyright} 2016 Inderscience Enterprises Ltd.},
annote = {cited By 0
18/05/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
18/05/2018 Exclu{\'{i}}do (etapa 1)},
author = {Krotewicz, P},
doi = {10.1504/IJBM.2016.077148},
journal = {International Journal of Biometrics},
keywords = {acm,estela,etapa1,id487,isi,revisao{\_}scopus,revisao{\_}webofscience,scopus},
mendeley-tags = {acm,estela,etapa1,id487,isi,revisao{\_}scopus,revisao{\_}webofscience,scopus},
number = {1},
pages = {65--81},
title = {{Novel ear-assisted 3D face recognition under expression variations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976902847{\&}doi=10.1504{\%}2FIJBM.2016.077148{\&}partnerID=40{\&}md5=887b19cd5da35df8a4cf29dba780a556},
volume = {8},
year = {2016}
}
@article{ISI:000458711300008,
abstract = {This paper tackles the problem of people re-identification by using soft
biometrics features. The method works on RGB-D data (color point clouds)
to determine the best matching among a database of possible users. For
each subject under testing, skeletal information in three-dimensions is
used to regularize the pose and to create a skeleton standard posture
(SSP). A partition grid, whose sizes depend on the SSP, groups the
samples of the point cloud accordingly to their position. Every group is
then studied to build the person signature. The same grid is then used
for the other subjects of the database to preserve information about
possible shape differences among users. The effectiveness of this novel
method has been tested on three public datasets. Numerical experiments
demonstrate an improvement of results with reference to the current
state-of-the-art, with recognition rates of 97.84{\%} (on a partition of
BIWI RGBD-ID), 61.97{\%} (KinectREID) and 89.71{\%} (RGBD-ID), respectively.
(C) 2019 Elsevier Ltd. All rights reserved.},
author = {Patruno, Cosimo and Marani, Roberto and Cicirelli, Grazia and Stella, Ettore and D'Orazio, Tiziana},
doi = {10.1016/j.patcog.2019.01.003},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {77--90},
title = {{People re-identification using skeleton standard posture and color descriptors from RGB-D data}},
volume = {89},
year = {2019}
}
@article{ISI:000386396800002,
abstract = {Three-dimensional morphable model (3DMM) is a powerful tool for
recovering 3D shape and texture from a single facial image. The success
of 3DMM relies on two things: an effective optimization strategy and a
realistic approach to synthesizing face images. However, most previous
methods have focused on developing an optimization strategy under
Phong's synthesis approach. In this paper, we adopt a more realistic
synthesis technique that fully considers illumination and reflectance in
the 3DMM fitting process. Using the sphere harmonic illumination model
(SHIM), our new synthesis approach can account for more lighting factors
than Phong's model. Spatially varying specular reflectance is also
introduced into the synthesis process. Under SHIM, the cost function is
nearly linear for all parameters, which simplifies the optimization. We
apply our new optimization algorithm to determine the shape and texture
parameters simultaneously. The accuracy of the recovered shape and
texture can be improved significantly by considering the spatially
varying specular reflectance. Hence, our algorithm produces an enhanced
shape and texture compared with previous SHIM-based methods that recover
shape from feature points. Although we use just a single input image in
a profile pose, our approach gives plausible results. Experiments on a
well-known image database show that, compared to state-of-the-art
methods based on Phong's model, the proposed approach enhances the
robustness of the 3DMM fitting results under extreme lighting and
profile pose.},
author = {Ma, Mingyang and Peng, Silong and Hu, Xiyuan},
doi = {10.1007/s00371-015-1158-z},
issn = {0178-2789},
journal = {VISUAL COMPUTER},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {10},
pages = {1223--1238},
title = {{A lighting robust fitting approach of 3D morphable model for face reconstruction}},
volume = {32},
year = {2016}
}
@inproceedings{ISI:000414287400080,
abstract = {3D scanning and 3D printing techniques, as the technical impetus of 3D
face recognition, also boost unconsciously the security threat against
it from the spoofing attacks via manufactured mask. In order to improve
the robustness of 3D face recognition system, several countermeasures
against mask attacks based on photometric features have been reported in
recent years. However, the anti-spoofing approach involving 3D meshed
face scan and the related 3D facial features have not been studied yet.
For filling this gap, in this paper, we propose to exploit the
anti-spoofing performance of geometric attributes based 3D facial
description. It synthesises the advantages of the selected geometric
attributes, named principal curvature measures, and the meshSIFT-based
feature descriptor. Specifically, the estimation of geometric attributes
is coherent to the property of discrete surface, and the feature related
to them can accurately describe the shape of facial surface. These
characteristics are beneficial to discovering the geometry-based
dissimilarity between genuine face and fraud mask. In the experiment
part, the baselines of verification and anti-spoofing performance are
evaluated on the Morpho database. Furthermore, for simulating a
real-world scenario and testing the corresponding anti-spoofing
performance, the size of genuine face set is massively extended by
uniting the Morpho database and the FRGC v2.0 database to increase the
ratio of genuine faces to fraud masks. The evaluation results prove that
the proposed 3D face verification system can guarantee competitive
verification accuracy for genuine faces and promising anti-spoofing
performance against mask attacks.},
annote = {12th IEEE International Conference on Automatic Face and Gesture
Recognition (FG), Washington, DC, MAY 30-JUN 03, 2017},
author = {Tang, Yinhang and Chen, Liming},
booktitle = {2017 12TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2017)},
doi = {10.1109/FG.2017.74},
isbn = {978-1-5090-4023-0},
issn = {2326-5396},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE; Baidu; Mitsubishi Elect Res Labs Inc; 3dMD; DI4D; Syst {\&} Technol Res; ObjectVideo Labs; MUKH Technologies; IEEE Comp Soc},
pages = {589--595},
series = {IEEE International Conference on Automatic Face and Gesture Recognition and Workshops},
title = {{3D Facial Geometric Attributes based Anti-spoofing Approach against Mask Attacks}},
year = {2017}
}
@article{Emambakhsh2017,
abstract = {The potential of the nasal region for expression robust 3D face recognition is thoroughly investigated by a novel five-step algorithm. First, the nose tip location is coarsely detected and the face is segmented, aligned and the nasal region cropped. Then, a very accurate and consistent nasal landmarking algorithm detects seven keypoints on the nasal region. In the third step, a feature extraction algorithm based on the surface normals of Gabor-wavelet filtered depth maps is utilised and, then, a set of spherical patches and curves are localised over the nasal region to provide the feature descriptors. The last step applies a genetic algorithm-based feature selector to detect the most stable patches and curves over different facial expressions. The algorithm provides the highest reported nasal region-based recognition ranks on the FRGC, Bosphorus and BU-3DFE datasets. The results are comparable with, and in many cases better than, many state-of-the-art 3D face recognition algorithms, which use the whole facial domain. The proposed method does not rely on sophisticated alignment or denoising steps, is very robust when only one sample per subject is used in the gallery, and does not require a training step for the landmarking algorithm. https://github.com/mehryaragha/NoseBiometrics},
author = {Emambakhsh, Mehryar and Evans, Adrian},
doi = {10.1109/TPAMI.2016.2565473},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Face recognition,Gabor wavelets,facial landmarking,feature selection,nose region,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience,surface normals},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {may},
number = {5},
pages = {995--1007},
title = {{Nasal Patches and Curves for Expression-Robust 3D Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7467565/},
volume = {39},
year = {2017}
}
@inproceedings{ISI:000380558900020,
abstract = {3D modeling of point clouds is an important but time-consuming process,
inspiring extensive research in automatic methods. Prior efforts focus
on primitive geometry, street structures or indoor objects, but
industrial data has rarely been pursued. Our work presents a method for
automatic modeling and recognition of 3D industrial site point clouds,
dividing the task into 3 separate sub-problems: pipe modeling, plane
classification, and object recognition. The results are integrated to
obtain the complete model, revealing some issues during the integration,
solved by utilizing information gained from each individual process.
Experiments show that the presented method automatically models large
and complex industrial scenes with a quality that outperforms leading
commercial modeling software and is comparable to professional hand-made
models.},
annote = {14th IAPR International Conference on Machine Vision Applications (MVA),
Tokyo, JAPAN, MAY 18-22, 2015},
author = {Pang, Guan and Qiu, Rongqi and Huang, Jing and You, Suya and Neumann, Ulrich},
booktitle = {2015 14th IAPR International Conference on Machine Vision Applications (MVA)},
isbn = {978-4-9011-2214-6},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {MVA Organisation; IAPR TC-8; National Institute of Advanced Industrial Science and Technology (AIST); The Telecommunications Advancement Foundation; KDDI Foundation},
pages = {22--25},
title = {{Automatic 3D Industrial Point Cloud Modeling and Recognition}},
year = {2015}
}
@inproceedings{ISI:000404959000120,
abstract = {In the article there are presented the results of recognition of seven
emotional states (neutral, joy, sadness, surprise, anger, fear, disgust)
based on facial expressions. Coefficients describing elements of facial
expressions, registered for six subjects, were used as features. The
features have been calculated for three-dimensional face model. The
classification of features were performed using k-NN classifier and MLP
neural network. (C) 2017 The Authors. Published by Elsevier B.V.},
annote = {International Conference on Computational Science (ICCS), Zurich,
SWITZERLAND, JUN 12-14, 2017},
author = {Tarnowski, Pawel and Kolodziej, Marcin and Majkowski, Andrzej and Rak, Remigiusz J},
booktitle = {INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE (ICCS 2017)},
doi = {10.1016/j.procs.2017.05.025},
editor = {{Koumoutsakos, P and Lees, M and Krzhizhanovskaya, V and Dongarra, J and Sloot, P}},
issn = {1877-0509},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {1175--1184},
series = {Procedia Computer Science},
title = {{Emotion recognition using facial expressions}},
volume = {108},
year = {2017}
}
@inproceedings{ISI:000399004300032,
abstract = {Person identification using face as a cue is one of the most prominent
and robust technique. This paper presents 3D face recognition system
using Radial curves and Back Propagation Neural Networks (BPNN). The
face images used for experimentation are under various challenges like
illumination, pose variation, expression and occlusions. The features of
images are extracted using Eigen vectors. These features are compared
using radial curves on the face starting from center of the face to the
end of the face. Each corresponding curve is matched using Euclidean
Distance classifier. The BPNN is used to train the features for face
matching. The proposed algorithms are tested on ORL and DMCE database.
The performance analysis is based on recognition rate accuracy of the
system. The proposed radial curve system yields recognition rate
accuracy of 100 {\%} for images from the ORL database and 98 {\%} for the
images from DMCE database.},
annote = {1st International Conference on Data Engineering and Communication
Technology (ICDECT), Christ Inst Management, Lavasa, INDIA, MAR 10-11,
2016},
author = {Keshwani, Latasha and Pete, Dnyandeo},
booktitle = {PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON DATA ENGINEERING AND COMMUNICATION TECHNOLOGY, ICDECT 2016, VOL 2},
doi = {10.1007/978-981-10-1678-3_32},
editor = {{Satapathy, SC and Bhateja, V and Joshi, A}},
isbn = {978-981-10-1678-3; 978-981-10-1677-6},
issn = {2194-5357},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Aspire Res Fdn},
pages = {333--344},
series = {Advances in Intelligent Systems and Computing},
title = {{Comparative Analysis of Frontal Face Recognition Using Radial Curves and Back Propagation Neural Network}},
volume = {469},
year = {2017}
}
@article{ISI:000353332000012,
abstract = {Illumination normalization of face image for face recognition and facial
expression recognition is one of the most frequent and difficult
problems in image processing. In order to obtain a face image with
normal illumination, our method firstly divides the input face image
into sixteen local regions and calculates the edge level percentage in
each of them. Secondly, three local regions, which meet the requirements
of lower complexity and larger average gray value, are selected to
calculate the final illuminant direction according to the error function
between the measured intensity and the calculated intensity, and the
constraint function for an infinite light source model. After knowing
the final illuminant direction of the input face image, the Retinex
algorithm is improved from two aspects: (1) we optimize the surround
function; (2) we intercept the values in both ends of histogram of face
image, determine the range of gray levels, and stretch the range of gray
levels into the dynamic range of display device. Finally, we achieve
illumination normalization and get the final face image. Unlike previous
illumination normalization approaches, the method proposed in this paper
does not require any training step or any knowledge of 3D face and
reflective surface model. The experimental results using extended Yale
face database B and CMU-PIE show that our method achieves better
normalization effect comparing with the existing techniques.},
author = {Yi, Jizheng and Mao, Xia and Chen, Lijiang and Xue, Yuli and Rovetta, Alberto and Caleanu, Catalin-Daniel},
doi = {10.1371/journal.pone.0122200},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {4},
title = {{Illumination Normalization of Face Image Based on Illuminant Direction Estimation and Improved Retinex}},
volume = {10},
year = {2015}
}
@inproceedings{ISI:000390287300035,
abstract = {Real time user independent facial expression recognition is important
for virtual agents but challenging. However, since in real time
recognition users are not necessarily presenting all the emotions, some
proposed methods are not applicable. In this paper, we present a new
approach that instead of using the traditional base face normalization
on whole face shapes, performs normalization on the point cloud of each
landmark. The result shows that our method outperforms the other two
when the user input does not contain all six universal emotions.},
annote = {16th International Conference on Intelligent Virtual Agents (IVA), Los
Angeles, CA, SEP 20-23, 2016},
author = {Pan, Zhenglin and Polceanu, Mihai and Lisetti, Christine},
booktitle = {INTELLIGENT VIRTUAL AGENTS, IVA 2016},
doi = {10.1007/978-3-319-47665-0_35},
editor = {{Traum, D and Swartout, W and Khooshabeh, P and Kopp, S and Scherer, S and Leuski, A}},
isbn = {978-3-319-47665-0; 978-3-319-47664-3},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Alelo; Springer; Univ So Calif, Inst Creat Technologies},
pages = {369--372},
series = {Lecture Notes in Artificial Intelligence},
title = {{On Constrained Local Model Feature Normalization for Facial Expression Recognition}},
volume = {10011},
year = {2016}
}
@inproceedings{ISI:000427752100101,
abstract = {In this paper, a contour map human facial recognition algorithm is
proposed to implement the three-dimensional (3D) face recognition with
the Kinect Xbox One. Since the scale of 3D depth data collected from
Kinect is tremendous, the face recognition process cannot be handled in
real time. To improve the speed and accuracy of the recognition process,
the proposed algorithm turns the 3D depth data to the two-dimensional
(2D) contour map. Furthermore, due to the 3D depth data obtained by
Kinect, there is no need of expensive, ponderous and slow 3D scanners.
Ten male and female subjects were involved in the validation experiment
and the results verify that the proposed algorithm was feasible for face
recognition. In addition, compared with other methods, Eigenface, Local
Binary Patterns (LBP) and Linear Discriminant Analysis (LDA), the
proposed algorithm has the better security and reliability.},
annote = {4th International Conference on Systems and Informatics (ICSAI),
Hangzhou, PEOPLES R CHINA, NOV 11-13, 2017},
author = {Cheng, Zijun and Shi, Tianwei and Cui, Wenhua and Dong, Yunqi and Fang, Xuehan},
booktitle = {2017 4TH INTERNATIONAL CONFERENCE ON SYSTEMS AND INFORMATICS (ICSAI)},
isbn = {978-1-5386-1107-4},
issn = {2474-0217},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE Syst Man {\&} Cybernet Soc; Shanghai Dianji Univ; Shanghai Dianji Univ, Sch Elect {\&} Informat},
pages = {555--559},
series = {International Conference on Systems and Informatics},
title = {{3D Face Recognition Based on Kinect Depth Data}},
year = {2017}
}
@inproceedings{ISI:000380433000180,
abstract = {Nose tip localization is an important step for registration,
preprocessing and recognition of 3D face data. In this paper, we propose
a new approach for the nose tip detection that is robust to pose and
expression variations and in presence of occlusions. From a rotated 3D
face, we extract facial curves that are matched to a profile curve
model. An optimal matching using the Riemannian geometry, based on the
Elastic Shape Analysis is performed to obtain the accurate nose tip. The
proposed method requires no training and can locate the nose tip in less
than 6 seconds. Experiments are performed on the Bosphorus database.
Quantitative analysis and comparison with the ground truth locations are
provided. The results confirm that our approach achieves 97.68{\%} with
error no larger than 12 mm and 98.19{\%} within 20 mm.},
annote = {INTERNATIONAL CONFERENCE ON CONTROL ENGINEERING {\&} INFORMATION
TECHNOLOGY (CEIT), Tlemcen, ALGERIA, MAY 25-27, 2015},
author = {Bentaieb, Samia and Ouamri, Abdelaziz and Keche, Mokhtar},
booktitle = {3RD INTERNATIONAL CONFERENCE ON CONTROL, ENGINEERING {\&} INFORMATION TECHNOLOGY (CEIT 2015)},
isbn = {978-1-4799-8213-4},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
title = {{Nose Tip Localization on a Three Dimensional Face across Pose, Expressions and Occlusions Variations in a Riemannian Context}},
year = {2015}
}
@article{ISI:000398720100091,
abstract = {In this paper, we present a novel framework for detecting individual
trees in densely sampled 3D point cloud data acquired in urban areas.
Given a 3D point cloud, the objective is to assign point-wise labels
that are both class-aware and instance-aware, a task that is known as
instance-level segmentation. To achieve this, our framework addresses
two successive steps. The first step of our framework is given by the
use of geometric features for a binary point-wise semantic
classification with the objective of assigning semantic class labels to
irregularly distributed 3D points, whereby the labels are defined as
``tree points{\{}''{\}} and ``other points{\{}''{\}}. The second step of our
framework is given by a semantic segmentation with the objective of
separating individual trees within the ``tree points{\{}''{\}}. This is
achieved by applying an efficient adaptation of the mean shift algorithm
and a subsequent segment-based shape analysis relying on semantic rules
to only retain plausible tree segments. We demonstrate the performance
of our framework on a publicly available benchmark dataset, which has
been acquired with a mobile mapping system in the city of Delft in the
Netherlands. This dataset contains 10.13 M labeled 3D points among which
17.6{\%} are labeled as ``tree points{\{}''{\}}. The derived results clearly
reveal a semantic classification of high accuracy (up to 90.77{\%}) and an
instance-level segmentation of high plausibility, while the simplicity,
applicability and efficiency of the involved methods even allow applying
the complete framework on a standard laptop computer with a reasonable
processing time (less than 2.5 h).},
author = {Weinmann, Martin and Weinmann, Michael and Mallet, Clement and Bredif, Mathieu},
doi = {10.3390/rs9030277},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {mar},
number = {3},
title = {{A Classification-Segmentation Framework for the Detection of Individual Trees in Dense MMS Point Cloud Data Acquired in Urban Areas}},
volume = {9},
year = {2017}
}
@inproceedings{ISI:000390782003007,
abstract = {We propose a 3D face modeling and recognition system using an RGB-D
stream in the presence of large pose changes. In the previous work, all
facial data points are registered with a reference to improve the
accuracy of 3D face model from a low-resolution depth sequence. This
registration often fails when applied to non-frontal faces. It causes
inaccurate 3D face models and poor performance of matching. We address
this problem by pre-aligning each input face ('frontalization') before
the registration, which avoids registration failures. For each frame,
our method estimates the 3D face pose, assesses the quality of data,
segments the facial region, frontalizes it. and performs an accurate
registration with the previous 3D model. The 3D 3D recognition system
using accurate 3D models from our method outperforms other face
recognition systems and shows 100{\%} rank 1 recognition accuracy on a
dataset with 30 subjects.},
annote = {23rd IEEE International Conference on Image Processing (ICIP), Phoenix,
AZ, SEP 25-28, 2016},
author = {Kim, Donghyun and Choi, Jongmoo and Leksut, Jatuporn Toy and Medioni, Gerard},
booktitle = {2016 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
isbn = {978-1-4673-9961-6},
issn = {1522-4880},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Inst Elect {\&} Elect Engineers; Inst Elect {\&} Elect Engineers, Signal Proc Soc},
pages = {3011--3015},
series = {IEEE International Conference on Image Processing ICIP},
title = {{ACCUTE 3D FACE MODELING AND RECOGNITION FROM RGB-D STREAM IN THE PRESENCE OF LARGE POSE CNGES}},
year = {2016}
}
@article{ISI:000404061800006,
abstract = {A person's well-being status is reflected by their face through a
combination of facial expressions and physical signs. The SEMEOTICONS
project translates the semeiotic code of the human face into
measurements and computational descriptors that are automatically
extracted from images, videos, and three-dimensional scans of the face.
SEMEOTICONS developed a multisensory platform in the form of a smart
mirror to identify signs related to cardio-metabolic risk. The aim was
to enable users to self-monitor their well-being status over time and
guide them to improve their lifestyle. Significant scientific and
technological challenges have been addressed to build the multisensory
mirror, from touchless data acquisition, to real-time processing and
integration of multimodal data.},
author = {Henriquez, Pedro and Matuszewski, Bogdan J and Andreu-Cabedo, Yasmina and Bastiani, Luca and Colantonio, Sara and Coppini, Giuseppe and D'Acunto, Mario and Favilla, Riccardo and Germanese, Danila and Giorgi, Daniela and Marraccini, Paolo and Martinelli, Massimo and Morales, Maria-Aurora and Pascali, Maria Antonietta and Righi, Marco and Salvetti, Ovidio and Larsson, Marcus and Stromberg, Tomas and Randeberg, Lise and Bjorgan, Asgeir and Giannakakis, Giorgos and Pediaditis, Matthew and Chiarugi, Franco and Christinaki, Eirini and Marias, Kostas and Tsiknakis, Manolis},
doi = {10.1109/TMM.2017.2666545},
issn = {1520-9210},
journal = {IEEE TRANSACTIONS ON MULTIMEDIA},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
number = {7},
pages = {1467--1481},
title = {{Mirror Mirror on the Wall ... An Unobtrusive Intelligent Multisensory Mirror for Well-Being Status Self-Assessment and Visualization}},
volume = {19},
year = {2017}
}
@inproceedings{ISI:000453087200079,
abstract = {Based on the MPEG-4 standard, this paper uses emotion recognition
technology to achieve a realistic facial animation with
three-dimensional human faces driven by text. In the aspect of realizing
facial animation with rich emotional changes, a Chinese syllable mouth
type parameter database and 6 basic expression parameter libraries are
established. Finally, weighted fusion of various action elements is
performed on face animation parameters to output the animation sequence
of human face.},
annote = {International Conference on Electrical, Control, Automation and Robotics
(ECAR), Xiamen, PEOPLES R CHINA, SEP 16-17, 2018},
author = {Wang, Zhong-min and Yin, Gui-lan and Wang, Rui-lai},
booktitle = {2018 INTERNATIONAL CONFERENCE ON ELECTRICAL, CONTROL, AUTOMATION AND ROBOTICS (ECAR 2018)},
isbn = {978-1-6059-5579-7},
issn = {2475-885X},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {448--453},
series = {DEStech Transactions on Engineering and Technology Research},
title = {{Research on Mouth Adaptation and Facial Expression Synthesis of Three-dimensional Human Face}},
volume = {307},
year = {2018}
}
@article{Gilani:2017:DDA:3103259.3103504,
abstract = {We present a multilinear algorithm to automatically establish dense point-to-point correspondence over an arbitrarily large number of population specific 3D faces across identities, facial expressions and poses. The algorithm is initialized with a subset of anthropometric landmarks detected by our proposed Deep Landmark Identification Network which is trained on synthetic images. The landmarks are used to segment the 3D face into Voronoi regions by evolving geodesic level set curves. Exploiting the intrinsic features of these regions, we extract discriminative keypoints on the facial manifold to elastically match the regions across faces for establishing dense correspondence. Finally, we generate a Region based 3D Deformable Model which is fitted to unseen faces to transfer the correspondences. We evaluate our algorithm on the tasks of facial landmark detection and recognition using two benchmark datasets. Comparison with thirteen state-of-the-art techniques shows the efficacy of our algorithm.},
address = {New York, NY, USA},
annote = {18/05/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
18/05/2018 Exclu{\'{i}}do (etapa 1)},
author = {Gilani, Syed Zulqarnain and Mian, Ajmal and Eastwood, Peter},
doi = {10.1016/j.patcog.2017.04.013},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {3D face morphing,Deep learning,Dense 3D face correspondence,Face recognition,Keypoint detection,Landmark identification,Shape descriptor,acm,estela,etapa1,id488,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {acm,estela,etapa1,id488,revisao{\_}scopus,revisao{\_}webofscience},
month = {sep},
number = {C},
pages = {238--250},
publisher = {Elsevier Science Inc.},
title = {{Deep, dense and accurate 3D face correspondence for generating population specific deformable models}},
url = {https://doi.org/10.1016/j.patcog.2017.04.013 http://linkinghub.elsevier.com/retrieve/pii/S0031320317301644},
volume = {69},
year = {2017}
}
@inproceedings{ISI:000392739800107,
abstract = {(Semi)-automatic facade reconstruction from terrestrial LiDAR point
clouds is often affected by both quality of point cloud itself and
imperfectness of object recognition algorithms. In this paper, we employ
regularities, which exist on facades, to mitigate these problems. For
example, doors, windows and balconies often have orthogonal and parallel
boundaries. Many windows are constructed with the same shape. They may
be arranged at the same lines and distance intervals, so do different
windows. By identifying regularities among objects with relatively poor
quality, these can be applied to calibrate the objects and improve their
quality. The paper focuses on the regularities among the windows, which
is the majority of objects on the wall. Regularities are classified into
three categories: within an individual window, among similar windows and
among different windows. Nine cases are specified as a reference for
exploration. A hierarchical clustering method is employed to identify
and apply regularities in a feature space, where regularities can be
identified from clusters. To find the corresponding features in the nine
cases of regularities, two phases are distinguished for similar and
different windows. In the first phase, ICP (iterative closest points) is
used to identify groups of similar windows. The registered points and a
number of transformation matrices are used to identify and apply
regularities among similar windows. In the second phase, features are
extracted from the boundaries of the different windows. When applying
regularities by relocating windows, the connections, called chains,
established among the similar windows in the first phase are preserved.
To test the performance of the algorithms, two datasets from terrestrial
LiDAR point clouds are used. Both show good effects on the reconstructed
model, while still matching with original point cloud, preventing over
or under-regularization.},
annote = {23rd Congress of the
International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS),
Prague, CZECH REPUBLIC, JUL 12-19, 2016},
author = {Zhou, K and Gorte, B and Zlatanova, S},
booktitle = {XXIII ISPRS Congress, Commission V},
doi = {10.5194/isprsarchives-XLI-B5-749-2016},
editor = {{Halounova, L and Safar, V and Remondino, F and Hodac, J and Pavelka, K and Shortis, M and Rinaudo, F and Scaioni, M and Boehm, J and RiekeZapp, D}},
issn = {2194-9034},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {B5},
organization = {Int Soc Photogrammetry {\&} Remote Sensing},
pages = {749--755},
series = {International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences},
title = {{EXPLORING REGULARITIES FOR IMPROVING FACADE RECONSTRUCTION FROM POINT CLOUDS}},
volume = {41},
year = {2016}
}
@article{ISI:000346542300028,
abstract = {3D facial expression recognition has been greatly promoted for
overcoming the inherent drawbacks of 2D facial expression recognition
and has achieved superior recognition accuracy to the 2D. In this paper,
a novel holistic, full-automatic approach for 3D facial expression
recognition is proposed. First, 3D face models are represented in
2D-image-like structure which makes it possible to take advantage of the
wealth of 2D methods to analyze 3D models. Then an enhanced facial
representation, namely polytypic multi-block local binary patterns
(P-MLBP), is proposed. The P-MLBP involves both the feature-based
irregular divisions to depict the facial expressions accurately and the
fusion of depth and texture information of 3D models to enhance the
facial feature. Based on the BU-3DFE database, three kinds of
classifiers are employed to conduct 3D facial expression recognition for
evaluation. Their experimental results outperform the state of the art
and show the effectiveness of P-MLBP for 3D facial expression
recognition. Therefore, the proposed strategy is validated for 3D facial
expression recognition; and its simplicity opens a promising direction
for fully automatic 3D facial expression recognition. (C) 2014 Elsevier
B.V. All rights reserved.},
author = {Li, Xiaoli and Ruan, Qiuqi and Jin, Yi and An, Gaoyun and Zhao, Ruizhen},
doi = {10.1016/j.sigpro.2014.09.033},
issn = {0165-1684},
journal = {SIGNAL PROCESSING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {mar},
pages = {297--308},
title = {{Fully automatic 3D facial expression recognition using polytypic multi-block local binary patterns}},
volume = {108},
year = {2015}
}
@inproceedings{ISI:000374793800033,
abstract = {In the present work we propose a method for detecting the nose and eyes
position when we observe a scene that contains a face. The main goal of
the proposed technique is that it capable of bypassing the 3D explicit
mapping of the face and instead take advantage of the information
available in the Depth gradient map of the face. To this end we will
introduce a simple false positive rejection approach restricting the
distance between the eyes, and between the eyes and the nose. The main
idea is to use nose candidates to estimate those regions where is
expected to find the eyes, and vice versa. Experiments with Texas
database are presented and the proposed approach is testes when data
presents different power of noise and when faces are in different
positions with respect to the camera.},
annote = {20th Iberoamerican Congress on Pattern Recognition (CIARP), Montevideo,
URUGUAY, NOV 09-12, 2015},
author = {{Matias Di Martino}, J and Fernandez, Alicia and Ferrari, Jose},
booktitle = {PROGRESS IN PATTERN RECOGNITION, IMAGE ANALYSIS, COMPUTER VISION, AND APPLICATIONS, CIARP 2015},
doi = {10.1007/978-3-319-25751-8_33},
editor = {{Pardo, A and Kittler, J}},
isbn = {978-3-319-25751-8; 978-3-319-25750-1},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {IAPR; Uruguayan IAPR Chapter; Argentine Soc Pattern Recognit; Special Interest Grp Brazilian Comp Soc; Chilean Assoc Pattern Recognit; Cuban Assoc Pattern Recognit; Mexican Assoc Comp Vis, Neural Comp {\&} Robot; Spanish Assoc Pattern Recognit {\&} Image Anal; },
pages = {271--278},
series = {Lecture Notes in Computer Science},
title = {{Automatic Eyes and Nose Detection Using Curvature Analysis}},
volume = {9423},
year = {2015}
}
@inproceedings{ISI:000383222400036,
abstract = {An efficient and above all cheap solutions, biometrics provide extensive
information in access control applications. 3D mode provides great new
opportunities in this sector in recent years. Firstly the paper
discusses the formal work done in this domain discussing approach based
on curvature calculation, alignment of surfaces, feature selection and
facial curve including different techniques for 3D facial recognition
data. The second part is about the steps required for the preprocessing
phase of the 3D face data. Various experiments conducted and results
obtained.},
annote = {IEEE/ACIS 14th International Conference on Software Engineering
Research, Management and Application (SERA), Towson Univ, Baltimore, MD,
JUN 08-10, 2016},
author = {Zribi, Soumaya and Khadhraoui, Taher and Benzarti, Faouzi and Amiri, Hamid},
booktitle = {2016 IEEE/ACIS 14TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING RESEARCH, MANAGEMENT AND APPLICATIONS (SERA)},
editor = {{Song, YT}},
isbn = {978-1-5090-0809-4},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE Comp Soc; IEEE; Int Assoc Comp {\&} Informat Sci; Shanghai Univ; Shanghai Key Lab Comp Software Testing {\&} Evaluating},
pages = {281--285},
title = {{Automatic 3D Face Preprocessing}},
year = {2016}
}
@article{ISI:000463672800006,
abstract = {Analysis and visualization of human facial expressions and its
applications are useful but challenging. This paper presents a novel
approach to analyze the facial expressions from images through learning
of a 3D morphable face model and a quantitative information
visualization scheme for exploring this type of visual data. More
specifically, a 3D face database with various facial expressions is
employed to build a nonnegative matrix factorization (NMF) part-based
morphable 3D face model. From an input image, a 3D face with expression
can be reconstructed iteratively by using the NMF morphable 3D face
model as a priori knowledge, from which basis parameters and a
displacement map are extracted as features for facial emotion analysis
and visualization. Based upon the features, two support vector
regressions are trained to determine the fuzzy valence-arousal (VA)
values to quantify the emotions. The continuously changing emotion
status can be intuitively analyzed by visualizing the VA values in VA
space. Our emotion analysis and visualization system, based on 3D NMF
morphable face model, detect expressions robustly from various head
poses, face sizes and lighting conditions and is fully automatic to
compute the VA values from images or a sequence of video with various
facial expressions. To evaluate our novel method, we test our system on
publicly available databases and evaluate the emotion analysis and
visualization results. We also apply our method to quantifying emotion
changes during motivational interviews. These experiments and
applications demonstrate the effectiveness and accuracy of our method.},
author = {Jin, Hai and Wang, Xun and Lian, Yuanfeng and Hua, Jing},
doi = {10.1007/s00371-018-1482-1},
issn = {0178-2789},
journal = {VISUAL COMPUTER},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {4},
pages = {535--548},
title = {{Emotion information visualization through learning of 3D morphable face model}},
volume = {35},
year = {2019}
}
@article{ISI:000436350700038,
abstract = {Monocular 3D face reconstruction from a single image has been an active
research topic due to its wide applications. It has been demonstrated
that the 3D face can be reconstructed efficiently using a PCA-based
subspace model for facial shape representation and facial landmarks for
model parameter estimation. However, due to the limited expressiveness
of the subspace model and the inaccuracy of landmark detection, most
existing methods are not robust to pose and illumination variation. To
overcome this limitation, this work proposes a coupled-dictionary model
for parametric facial shape representation and a two-stage framework for
3D face reconstruction from a single 2D image by using facial landmarks.
Motivated by image super-resolution, the proposed coupled-model consists
of two dictionaries for the sparse and the dense 3D facial shapes,
respectively. In the first stage, the sparse 3D face is estimated from
facial landmarks by using partial least-squares regression. In the
second stage, the dense 3D face is reconstructed by 3D super-resolution
on the estimated sparse 3D face. Comprehensive experimental evaluations
demonstrate that the proposed coupled-dictionary model outperforms the
PCA-based subspace model in 3D face modeling accuracy and that the
proposed framework achieves much lower reconstruction error on facial
images with pose and illumination variations compared to
state-of-the-art algorithms. Moreover, qualitative analysis demonstrates
that the proposed method is generalizable to different types of data,
including facial images, portraits, and facial sketches. (C) 2018
Published by Elsevier Ltd.},
author = {Dou, Pengfei and Wu, Yuhang and Shah, Shishir K and Kakadiaris, Ioannis A},
doi = {10.1016/j.patcog.2018.03.002},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {515--527},
title = {{Monocular 3D facial shape reconstruction from a single 2D image with coupled-dictionary learning and sparse coding}},
volume = {81},
year = {2018}
}
@inproceedings{7603151,
abstract = {In modern days the demand for biometrics increases rapidly. The world still needs to solve many problems and answer to lot of questions regarding to biometrics for creating better solutions for recognition and verification of objects. Biometrics has become really important topic of our security. Number of input samples per person affects recognition in modern algorithms used for face recognition. We can say that single-sample problem is one of the most challenging problems in face recognition. Most of face recognition algorithms requires at least two samples per person but it is very important to create a system where only one sample per person would achieve a good performance. This topic is especially related to passport and id photos because they include only one picture of person. In this paper, we explain process of reconstruction and generating 3D face model created from id or passport photo. We also present our results that shows influence of generated new samples on face recognition.},
annote = {26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Sopiak, Dominik and Oravec, Milos and Pavlovicova, Jarmila and Bukovcikova, Zuzana and Dittingerova, Monika and Bilanska, Alexandra and Novotna, Maria and Gontkovic, Jozef},
booktitle = {2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
doi = {10.1109/FSKD.2016.7603151},
isbn = {978-1-5090-4093-3},
keywords = {3D face model,3D morphable model,estela,etapa1,face recognition,id318,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {estela,etapa1,id318,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {aug},
pages = {58--62},
publisher = {IEEE},
title = {{Generating face images based on 3D morphable model}},
url = {http://ieeexplore.ieee.org/document/7603151/},
year = {2016}
}
@article{ISI:000433351100010,
abstract = {In this paper, we propose a framework for online spontaneous emotion
detection, such as happiness or physical pain, from depth videos. Our
approach consists on mapping the video streams onto a Grassmann manifold
(i.e., space of k-dimensional linear subspaces) to form
time-parameterized trajectories. To this end, depth videos are
decomposed into short-time subsequences, each approximated by a
k-dimensional linear subspace, which is in turn a point on the Grassmann
manifold. Then, the temporal evolution of subspaces gives rise to a
precise mathematical representation of trajectories on the underlying
manifold. In the final step, extracted spatio-temporal features based on
computing the velocity vectors along the trajectories, termed Geometric
Motion History (GMH), are fed to an early event detector based on
Structured Output SVM, which enables online emotion detection from
partially-observed data. Experimental results obtained on the publicly
available Cam3D Kinect and BP4D-spontaneous databases validate the
proposed solution. The first database has served to exemplify the
proposed framework using depth sequences of the upper part of the body
collected using depth-consumer cameras, while the second database
allowed the application of the same framework to physical pain detection
from high-resolution and long 3D-face sequences.},
author = {Alashkar, Taleb and {Ben Amor}, Boulbaba and Daoudi, Mohamed and Berretti, Stefano},
doi = {10.1109/TAFFC.2016.2623718},
issn = {1949-3045},
journal = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2},
pages = {271--284},
title = {{Spontaneous Expression Detection from 3D Dynamic Sequences by Analyzing Trajectories on Grassmann Manifolds}},
volume = {9},
year = {2018}
}
@article{ISI:000448833400016,
abstract = {In this study, a fully automatic pose and expression invariant 3D face
alignment algorithm is proposed to handle frontal and profile face
images which is based on a two pass course to fine alignment strategy.
The first pass of the algorithm coarsely aligns the face images to an
intrinsic coordinate system (ICS) through a single 3D rotation and the
second pass aligns them at fine level using a minimum nose tip-scanner
distance (MNSD) approach. For facial recognition, multi-view faces are
synthesized to exploit real 3D information and test the efficacy of the
proposed system. Due to optimal separating hyper plane (OSH), Support
Vector Machine (SVM) is employed in multi-view face verification (FV)
task. In addition, a multi stage unified classifier based face
identification (FI) algorithm is employed which combines results from
seven base classifiers, two parallel face recognition algorithms and an
exponential rank combiner, all in a hierarchical manner. The performance
figures of the proposed methodology are corroborated by extensive
experiments performed on four benchmark datasets: GavabDB, Bosphorus,
UMB-DB and FRGC v2.0. Results show mark improvement in alignment
accuracy and recognition rates. Moreover, a computational complexity
analysis has been carried out for the proposed algorithm which reveals
its superiority in terms of computational efficiency as well.},
author = {Ratyal, Naeem and Taj, Imtiaz and Bajwa, Usama and Sajid, Muhammad},
doi = {10.3837/tiis.2018.10.016},
issn = {1976-7277},
journal = {KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {10},
pages = {4903--4929},
title = {{Pose and Expression Invariant Alignment based Multi-View 3D Face Recognition}},
volume = {12},
year = {2018}
}
@article{ISI:000454376800005,
abstract = {Image-based 3D face reconstruction has great potential in different
areas, such as facial recognition, facial analysis, and facial
animation. Due to the variations in image quality, single-image-based 3D
face reconstruction might not be sufficient to accurately reconstruct a
3D face. To overcome this limitation, multi-view 3D face reconstruction
uses multiple images of the same subject and aggregates complementary
information for better accuracy. Though appealing, there are multiple
challenges in practice. Among these challenges, the most significant is
the difficulty to establish coherent and accurate correspondence among a
set of images, especially when these images are captured under
unconstrained in-the-wild condition. This work proposes a method, Deep
Recurrent 3D FAce Reconstruction (DRFAR), to solve the task of
multi-view 3D face reconstruction using a subspace representation of the
3D facial shape and a deep recurrent neural network that consists of
both a deep convolutional neural network (DCNN) and a recurrent neural
network (RNN). The DCNN disentangles the facial identity and the facial
expression components for each single image independently, while the RNN
fuses identity-related features from the DCNN and aggregates the
identity specific contextual information, or the identity signal, from
the whole set of images to estimate the facial identity parameter, which
is robust to variations in image quality and is consistent over the
whole set of images. Experimental results indicate significant
improvement over state-of-the-art in both the accuracy and the
consistency of 3D face reconstruction. Moreover, face recognition
results on 11B-A with the UR2D face recognition pipeline indicate that,
compared to single-view 3D face reconstruction, the proposed multi-view
3D face reconstruction algorithm can improve the face identification
accuracy of UR2D by two percentage points in Rank-1 identification rate.
(C) 2018 Published by Elsevier B.V.},
author = {Dou, Pengfei and Kakadiaris, Ioannis A},
doi = {10.1016/j.imavis.2018.09.004},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {80--91},
title = {{Multi-view 3D face reconstruction with deep recurrent neural networks}},
volume = {80},
year = {2018}
}
@incollection{Ming:2015:RLS:2940229.2940265,
abstract = {In this paper, a robust 3D local SIFT feature is proposed for 3D face recognition. For preprocessing the original 3D face data, facial regional segmentation is first employed by fusing curvature characteristics and shape band mechanism. Then, we design a new local descriptor for the extracted regions, called 3D local Scale-Invariant Feature Transform (3D LSIFT). The key point detection based on 3D LSIFT can effectively reflect the geometric characteristic of 3D facial surface by encoding the gray and depth information captured by 3D face data. Then, 3D LSIFT descriptor extends to describe the discrimination on 3D faces. Experimental results based on the common international 3D face databases demonstrate the higher-qualified performance of our proposed algorithm with effectiveness, robustness, and universality.},
address = {New York, NY, USA},
annote = {26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Ming, Yue and Jin, Yi},
booktitle = {Proceedings of the 8th International Conference on Intelligent Robotics and Applications - Volume 9246},
doi = {10.1007/978-3-319-22873-0_31},
isbn = {978-3-319-22872-3},
keywords = {3D Local Scale-Invariant Feature Transform,3D face recognition,Depth information,Facial region segmentation,acm,estela,etapa1,id278,revisao{\_}webofscience},
mendeley-tags = {acm,estela,etapa1,id278,revisao{\_}webofscience},
pages = {352--359},
publisher = {Springer-Verlag New York, Inc.},
series = {ICIRA 2015},
title = {{Robust 3D Local SIFT Features for 3D Face Recognition}},
url = {http://dx.doi.org/10.1007/978-3-319-22873-0{\_}31 http://link.springer.com/10.1007/978-3-319-22873-0{\_}31},
year = {2015}
}
@article{ISI:000360999400005,
abstract = {Face recognition is being widely accepted as a biometric technique
because of its non-intrusive nature. Despite extensive research on 2-D
face recognition, it suffers from poor recognition rate due to pose,
illumination, expression, ageing, makeup variations and occlusions. In
recent years, the research focus has shifted toward face recognition
using 3-D facial surface and shape which represent more discriminating
features by the virtue of increased dimensionality. This paper presents
an extensive survey of recent 3-D face recognition techniques in terms
of feature detection, classifiers as well as published algorithms that
address expression and occlusion variation challenges followed by our
critical comments on the published work. It also summarizes remarkable
3-D face databases and their features used for performance evaluation.
Finally we suggest vital steps of a robust 3-D face recognition system
based on the surveyed work and identify a few possible directions for
research in this area.},
author = {Patil, Hemprasad and Kothari, Ashwin and Bhurchandi, Kishor},
doi = {10.1007/s10462-015-9431-0},
issn = {0269-2821},
journal = {ARTIFICIAL INTELLIGENCE REVIEW},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {3},
pages = {393--441},
title = {{3-D face recognition: features, databases, algorithms and challenges}},
volume = {44},
year = {2015}
}
@article{ISI:000393265800001,
abstract = {The open-set problem is among the problems that have significantly
changed the performance of face recognition algorithms in real-world
scenarios. Open-set operates under the supposition that not all the
probes have a pair in the gallery. Most face recognition systems in
real-world scenarios focus on handling pose, expression and illumination
problems on face recognition. In addition to these challenges, when the
number of subjects is increased for face recognition, these problems are
intensified by look-alike faces for which there are two subjects with
lower intra-class variations. In such challenges, the inter-cldss
similarity is higher than the intra-class variation for these two
subjects. In fact, these look-alike faces can be created as intrinsic,
situation-based and also by facial plastic surgery. This work introduces
three real-world open-set face recognition methods across facial plastic
surgery changes and a look-alike face by 3D face reconstruction and
sparse representation. Since some real world databases for face
recognition do not have multiple images per person in the gallery, With
just one image per subject in the gallery, this paper proposes a novel
idea to overcome this challenge by 3D modeling from gallery images and
synthesizing them for generating several images. Accordingly, a 3D model
is initially reconstructed from frontal face images in a real-world
gallery. Then, each 3D reconstructed face in the gallery is synthesized
to several possible views and a sparse dictionary is generated based on
the synthesized face image for each person. Also, a likeness dictionary
is defined and its optimization problem is solved by the proposed
method. Finally, the face recognition is performed for open-set face
recognition using three proposed representation classifications.
Promising results are achieved for face recognition across plastic
surgery and look-alike faces on three databases including the plastic
surgery face, look-alike face and LFW databases compared to several
state-of-the-art methods. Also, several real-world and open-set
scenarios are performed to evaluate the proposed method on these
databases in real-world scenarios. (C) 2016 Elsevier B.V. All rights
reserved.},
author = {Moeini, Ali and Faez, Karim and Moeini, Hossein and Safai, Armon Matthew},
doi = {10.1016/j.imavis.2016.11.002},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
pages = {1--14},
title = {{Open-set face recognition across look-alike faces in real-world scenarios}},
volume = {57},
year = {2017}
}
@article{ISI:000456899900003,
abstract = {Building damage assessment is a critical task following major hurricane
events. Use of remotely sensed data to support building damage
assessment is a logical choice considering the difficulty of gaining
ground access to the impacted areas immediately after hurricane events.
However, a remote sensing based damage assessment approach is often only
capable of detecting severely damaged buildings. In this study, an
airborne LiDAR based approach is proposed to assess multi-level
hurricane damage at the community scale. In the proposed approach,
building clusters are first extracted using a density-based algorithm. A
novel cluster matching algorithm is proposed to robustly match
post-event and pre-event building clusters. Multiple features including
roof area and volume, roof orientation, and roof shape are computed as
building damage indicators. A hierarchical determination process is then
employed to identify the extent of damage to each building object. The
results of this study suggest that our proposed approach is capable of
1) recognizing building objects, 2) extracting damage features, and 3)
characterizing the extent of damage to individual building properties.},
author = {Zhou, Zixiang and Gong, Jie and Hu, Xuan},
doi = {10.1016/j.autcon.2018.10.018},
issn = {0926-5805},
journal = {AUTOMATION IN CONSTRUCTION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {30--45},
title = {{Community-scale multi-level post-hurricane damage assessment of residential buildings using multi-temporal airborne LiDAR data}},
volume = {98},
year = {2019}
}
@article{ISI:000396382500043,
abstract = {Remote sensing data allow large scale observation of forested
ecosystems. Forest assessment benefits from information about individual
trees. Multibaseline SAR interferometry (InSAR) is able to generate
dense point clouds of forest canopies, similar to airborne laser
scanning (ALS). This type of point cloud was generated using data from
the Ka-band MEMPHIS system, acquired over a mainly coniferous forest
near Vordemwald in the Swiss Midlands. This point cloud was segmented
using an advanced clustering technique to detect individual trees and
derive their positions, heights, and crown diameters. To evaluate the
InSAR point cloud properties and limitations, it was compared to
products derived from ALS and stereo-photogrammetry. All point clouds
showed similar geolocation accuracies with 02-0.3 m relative shifts.
Both InSAR and photogrammetry techniques yielded points predominantly
located in the upper levels of the forest vegetation, while ALS provided
points from the top of the canopy down to the understory and forest
floor. The canopy height models agreed very well with each other, with
R-2 values between 0.84 and 0.89. The detected trees and their estimated
physical and structural parameters were validated by comparing them to
reference forestry data. A detection rate of similar to 90{\%} was
achieved for larger trees, corresponding to half of the reference trees.
The smaller trees were detected with a success rate of similar to 50{\%}.
The tree height was slightly underestimated, with a R-2 value of 0.63.
The estimated crown diameter agreed on an average sense, however with a
relatively low R-2 value of 0.19. Very high success rates ({\textgreater}90{\%}) were
obtained when matching the trees detected from the InSAR-data with those
detected from the ALS- and photogrammetry-data. There, InSAR tree
heights were in the mean 1-1.5 m lower, with high R-2 values ranging
between 0.8 and 0.9. Our results demonstrate the use of millimeter wave
SAR interferometry data as an alternative to ALS- and
photogrammetry-based data for forest monitoring. (C) 2016 Elsevier Inc.
All rights reserved.},
author = {Magnard, Christophe and Morsdorf, Felix and Small, David and Stilla, Uwe and Schaepman, Michael E and Meier, Erich},
doi = {10.1016/j.rse.2016.09.018},
issn = {0034-4257},
journal = {REMOTE SENSING OF ENVIRONMENT},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
pages = {567--580},
title = {{Single tree identification using airborne multibaseline SAR interferometry data}},
volume = {186},
year = {2016}
}
@article{ISI:000395844700002,
abstract = {The problem of fitting a 3D facial model to a 3D mesh has received a lot
of attention the past 15-20years. The majority of the techniques fit a
general model consisting of a simple parameterisable surface or a mean
3D facial shape. The drawback of this approach is that is rather
difficult to describe the non-rigid aspect of the face using just a
single facial model. One way to capture the 3D facial deformations is by
means Of a statistical 3D model of the face or its parts. This is
particularly evident when we want to capture the deformations of the
mouth region. Even though statistical models of face are generally
applied for modelling facial intensity, there are few approaches that
fit a statistical model of 3D faces. In this paper, in order to capture
and describe the non-rigid nature of facial surfaces we build a
part-based statistical model of the 3D facial surface and we combine it
with non-rigid iterative closest point algorithms. We show that the
proposed algorithm largely outperforms state-of-the-art algorithms for
3D face fitting and alignment especially when it comes to the
description of the mouth region. (C) 2016 Elsevier B.V. All rights
reserved.},
author = {Cheng, Shiyang and Marras, Ioannis and Zafeiriou, Stefanos and Pantic, Maja},
doi = {10.1016/j.imavis.2016.10.007},
issn = {0262-8856},
journal = {IMAGE AND VISION COMPUTING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {3--12},
title = {{Statistical non-rigid ICP algorithm and its application to 3D face alignment}},
volume = {58},
year = {2017}
}
@article{ISI:000349588900008,
abstract = {This paper presents a representation of 3D facial motion sequences that
allows performing statistical analysis of 3D face shapes in motion. The
resulting statistical analysis is applied to automatically generate
realistic facial animations and to recognize dynamic facial expressions.
To perform statistical analysis of 3D facial shapes in motion over
different subjects and different motion sequences, a large database of
motion sequences needs to be brought in full correspondence. Existing
algorithms that compute correspondences between 3D facial motion
sequences either require manual input or suffer from instabilities
caused by drift. For large databases, algorithms that require manual
interaction are not practical. We propose an approach to robustly
compute correspondences between a large set of facial motion sequences
in a fully automatic way using a multilinear model as statistical prior.
In order to register the motion sequences, a good initialization is
needed. We obtain this initialization by introducing a landmark
prediction method for 3D motion sequences based on Markov Random Fields.
Using this motion sequence registration, we find a compact
representation of each motion sequence consisting of one vector of
coefficients for identity and a high dimensional curve for expression.
Based on this representation, we synthesize new motion sequences and
perform expression recognition. We show experimentally that the obtained
registration is of high quality, where 56{\%} of all vertices are at
distance at most 1 mm from the input data, and that our synthesized
motion sequences look realistic. (C) 2014 Elsevier Inc. All rights
reserved.},
author = {Bolkart, Timo and Wuhrer, Stefanie},
doi = {10.1016/j.cviu.2014.06.013},
issn = {1077-3142},
journal = {COMPUTER VISION AND IMAGE UNDERSTANDING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
pages = {100--115},
title = {{3D faces in motion: Fully automatic registration and statistical analysis}},
volume = {131},
year = {2015}
}
@article{ISI:000375932300027,
abstract = {It is well known that higher level features can represent the abstract
semantics of original data. We propose a multiple scales combined deep
learning network to learn a set of high-level feature representations
through each stage of convolutional neural network for face recognition,
which is named as multiscaled principle component analysis (PCA) Network
(MS-PCANet). There are two main differences between our model and the
traditional deep learning network. On the one hand, we get the prefixed
filter kernels by learning the principal component of images' patches
using PCA, nonlinearly process the convolutional results by using simple
binary hashing, and pool them using spatial pyramid pooling method. On
the other hand, in our model, the output features of several stages are
fed to the classifier. The purpose of combining feature representations
from multiple stages is to provide multiscaled features to the
classifier, since the features in the latter stage are more global and
invariant than those in the early stage. Therefore, our MS-PCANet
feature compactly encodes both holistic abstract information and local
specific information. Extensive experimental results show our MS-PCANet
model can efficiently extract high-level feature presentations and
outperform state-of-the-art face/expression recognition methods on
multiple modalities benchmark face-related datasets. (C) 2016 SPIE and
IS{\&}T},
author = {Tian, Lei and Fan, Chunxiao and Ming, Yue},
doi = {10.1117/1.JEI.25.2.023025},
issn = {1017-9909},
journal = {JOURNAL OF ELECTRONIC IMAGING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {mar},
number = {2},
title = {{Multiple scales combined principle component analysis deep learning network for face recognition}},
volume = {25},
year = {2016}
}
@inproceedings{ISI:000364991200038,
abstract = {We propose a method for extracting fiducial points from human faces that
uses 3D information only and is based on two key steps: multi-scale
curvature analysis, and the reliable tracking of features in a
scale-space based on curvature. Our scale-space analysis, coupled to
careful use of prior information based on variability boundaries of
anthropometric facial proportions, does not require a training step,
because it makes direct use of morphological characteristics of the
analyzed surface. The proposed method precisely identifies important
fiducial points and is able to extract new fiducial points that were
previously unrecognized, thus paving the way to more effective
recognition algorithms.},
annote = {18th International Conference on Image Analysis and Processing (ICIAP),
Genoa, ITALY, SEP 07-11, 2015},
author = {{De Giorgis}, Nikolas and Rocca, Luigi and Puppo, Enrico},
booktitle = {IMAGE ANALYSIS AND PROCESSING - ICIAP 2015, PT I},
doi = {10.1007/978-3-319-23231-7_38},
editor = {{Murino, V and Puppo, E}},
isbn = {978-3-319-23231-7; 978-3-319-23230-0},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Datalogic; Google Inc; Centro Studi Gruppo Orizzonti Holding; Ansaldo Energia; EBIT Esaote; Softeco; eVS embedded Vis Syst S r l; 3DFlow S r l; Camelot Biomed Syst S r l; Ist Italiano Tecnologia, Pattern Anal {\&} Comp Vis Dept; Univ Genova; Univ Verona; Cam},
pages = {421--431},
series = {Lecture Notes in Computer Science},
title = {{Scale-Space Techniques for Fiducial Points Extraction from 3D Faces}},
volume = {9279},
year = {2015}
}
@article{Moeini201546,
abstract = {In this paper, a novel feature extraction method was proposed for facial expression recognition. A 3D Facial Expression Generic Elastic Model (3D FE-GEM) was proposed to reconstruct an expression-invariant 3D model of each human face in the present database using only a single 2D frontal image with/without facial expressions. Then, the texture and depth of the face were extracted from the reconstructed model. Afterwards, the Gabor filter bank was applied to both texture and reconstructed depth of the face to extract the feature vectors from both texture and reconstructed depth images. Finally, by combining 2D and 3D feature vectors, the final feature vectors are generated and classified by the Support Vector Machine (SVM). Favorable outcomes were acquired for facial expression recognition on the available image database based on the proposed method compared to several state-of-the-arts in facial expression recognition. {\textcopyright} Springer International Publishing Switzerland 2015.},
annote = {cited By 3
24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (Etapa 1)
24/04/2018 Exclu{\'{i}}do (Etapa 1)},
author = {Moeini, A and Moeini, H},
doi = {10.1007/978-3-319-13737-7_5},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {artur,etapa1,id214,isi,revisao{\_}webofscience,scopus},
mendeley-tags = {artur,etapa1,id214,isi,revisao{\_}webofscience,scopus},
pages = {46--57},
title = {{Multimodal facial expression recognition based on 3D face reconstruction from 2D images}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925307571{\&}doi=10.1007{\%}2F978-3-319-13737-7{\_}5{\&}partnerID=40{\&}md5=2491691ba3dd009bb212d65659191449},
volume = {8912},
year = {2015}
}
@article{ISI:000361492600008,
abstract = {Detection of facial feature is fundamental for applications such as
security, biometrics, 3D face modeling and personal authentication.
Active Shape Model (ASM) is one of the most popular local texture models
for face detection. This paper presents an issue related to face
detection based on ASM, and proposes an efficient extraction algorithm
for facial landmarks suitable for use on mobile devices. We modifies the
original ASM to improve its performance with three changes; (1)
Improving the initialization model using the center of the eyes by using
a feature map of color information, (2) Constructing modified model
definition and fitting more landmarks than the classical ASM, and (3)
Extending and building a 2-D profile model for detecting faces in input
image. The proposed method is evaluated on dataset containing over 700
images of faces, and experimental results reveal that the proposed
algorithm exhibited a significant improvement of over 10.2 {\%} in average
success ratio, compared to the classic ASM, clearly outperforming on
success rate and computing time.},
author = {Lee, Yong-Hwan and Kim, Cheong Ghil and Kim, Youngseop and Whangbo, Taeg Keun},
doi = {10.1007/s11042-013-1565-y},
issn = {1380-7501},
journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {20},
pages = {8821--8830},
title = {{Facial landmarks detection using improved active shape model on android platform}},
volume = {74},
year = {2015}
}
@article{ISI:000361934300008,
abstract = {We present a fully automatic multimodal 2D + 3D feature-based facial
expression recognition approach and demonstrate its performance on the
BU-3DFE database. Our approach combines multi-order gradient-based local
texture and shape descriptors in order to achieve efficiency and
robustness. First, a large set of fiducial facial landmarks of 20 face
images along with their 3D face scans are localized using a novel
algorithm namely incremental Parallel Cascade of Linear Regression
(iPar-CLR). Then, a novel Histogram of Second Order Gradients (HSOG)
based local image descriptor in conjunction with the widely used
first-order gradient based SIFT descriptor are used to describe the
local texture around each 2D landmark. Similarly, the local geometry
around each 3D landmark is described by two novel local shape
descriptors constructed using the first-order and the second-order
surface differential geometry quantities, i.e., Histogram of mesh
Gradients (meshHOG) and Histogram of mesh Shape index (curvature
quantization, meshHOS). Finally, the Support Vector Machine (SVM) based
recognition results of all 2D and 3D descriptors are fused at both
feature-level and score-level to further improve the accuracy.
Comprehensive experimental results demonstrate that there exist
impressive complementary characteristics between the 2D and 3D
descriptors. We use the BU-3DFE benchmark to compare our approach to the
state-of-the-art ones. Our multimodal feature-based approach outperforms
the others by achieving an average recognition accuracy of 86.32{\%}.
Moreover, a good generalization ability is shown on the Bosphorus
database. (C) 2015 Elsevier Inc. All rights reserved.},
author = {Li, Huibin and Ding, Huaxiong and Huang, Di and Wang, Yunhong and Zhao, Xi and Morvan, Jean-Marie and Chen, Liming},
doi = {10.1016/j.cviu.2015.07.005},
issn = {1077-3142},
journal = {COMPUTER VISION AND IMAGE UNDERSTANDING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {nov},
pages = {83--92},
title = {{An efficient multimodal 2D+3D feature-based approach to automatic facial expression recognition}},
volume = {140},
year = {2015}
}
@article{ISI:000460647500026,
abstract = {Face recognition could be a technology capable of distinguishing or
confirming an individual from a digital image or a video frame from a
video supply. Face recognition technology is employed in wide selection
of applications like authentication, access management, and police
investigation. It is finding applications in all industries ranging from
retail, advertising to banking etc. It is to this extent that Large
retailers are using facial recognition to recognize customers and
present offers, they also use it to catch shoplifters. Deep learning
Network is influencing every aspect of computer vision technology and
research. In this paper, we are depicting the role and achievements of
different deep models for face recognition in images and videos, we have
also compared recent algorithms for face recognition.},
author = {Dubey, Arun Kumar and Jain, Vanita},
doi = {10.1080/02522667.2019.1582875},
issn = {0252-2667},
journal = {JOURNAL OF INFORMATION {\&} OPTIMIZATION SCIENCES},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2, SI},
pages = {547--558},
title = {{A review of face recognition methods using deep learning network}},
volume = {40},
year = {2019}
}
@article{ISI:000397013700010,
abstract = {This paper investigated the potential of multispectral airborne laser
scanning (ALS) data for individual tree detection and tree species
classification. The aim was to develop a single-sensor solution for
forest mapping that is capable of providing species-specific
information, required for forest management and planning purposes.
Experiments were conducted using 1903 ground measured trees from 22
sample plots and multispectral ALS data, acquired with an Optech Titan
scanner over a boreal forest, mainly consisting of Scots pine (Pinus
Sylvestris), Norway spruce (Picea Abies), and birch (Betula sp.), in
southern Finland. ALS-features used as predictors for tree species were
extracted from segmented tree objects and used in random forest
classification. Different combinations of features, including point
cloud features, and intensity features of single and multiple channels,
were tested. Among the field-measured trees, 61.3{\%} were correctly
detected. The best overall accuracy (OA) of tree species classification
achieved for correctly-detected trees was 85.9{\%} (Kappa = 0.75), using a
point cloud and single-channel intensity features combination, which was
not significantly different from the ones that were obtained either
using all features (OA = 85.6{\%}, Kappa = 0.75), or single-channel
intensity features alone (OA = 85.4{\%}, Kappa = 0.75). Point cloud
features alone achieved the lowest accuracy, with an OA of 76.0{\%}.
Field-measured trees were also divided into four categories. An
examination of the classification accuracy for four categories of trees
showed that isolated and dominant trees can be detected with a detection
rate of 91.9{\%}, and classified with a high overall accuracy of 90.5{\%}.
The corresponding detection rate and accuracy were 81.5{\%} and 89.8{\%} for
a group of trees, 26.4{\%} and 79.1{\%} for trees next to a larger tree, and
7.2{\%} and 53.9{\%} for trees situated under a larger tree, respectively.
The results suggest that Channel 2 (1064 nm) contains more information
for separating pine, spruce, and birch, followed by channel 1 (1550 nm)
and channel 3 (532 nm) with an overall accuracy of 81.9{\%}, 78.3{\%}, and
69.1{\%}, respectively. Our results indicate that the use of multispectral
ALS data has great potential to lead to a single-sensor solution for
forest mapping.},
author = {Yu, Xiaowei and Hyyppa, Juha and Litkey, Paula and Kaartinen, Harri and Vastaranta, Mikko and Holopainen, Markus},
doi = {10.3390/rs9020108},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2},
title = {{Single-Sensor Solution to Tree Species Classification Using Multispectral Airborne Laser Scanning}},
volume = {9},
year = {2017}
}
@article{ISI:000439601100001,
abstract = {To use low-cost depth sensors such as Kinect for three-dimensional face
recognition with an acceptable rate of recognition, the challenges of
filling up nonmeasured pixels and smoothing of noisy data need to be
addressed. The main goal of this article is presenting solutions for
aforementioned challenges as well as offering feature extraction methods
to reach the highest level of accuracy in the presence of different
facial expressions and occlusions. To use this method, a domestic
database was created. First, the noisy pixels-called holes-of depth
image is removed by solving multiple linear equations resulted from the
values of the surrounding pixels of the holes. Then, bilateral and block
matching 3-D filtering approaches, as representatives of local and
nonlocal filtering approaches, are used for depth image smoothing.
Curvelet transform as a well-known nonlocal feature extraction technique
applied on both RGB and depth images. Two unsupervised dimension
reduction techniques, namely, principal component analysis and
independent component analysis, are used to reduce the dimension of
extracted features. Finally, support vector machine is used for
classification. Experimental results show a recognition rate of 90{\%} for
just depth images and 100{\%} when combining RGB and depth data of a
Kinect sensor which is much higher than other recently proposed
algorithms.},
author = {Mohammadi, Shahram and Gervei, Omid},
doi = {10.1177/1729881418787743;1729881418787743},
issn = {1729-8814},
journal = {INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jul},
number = {4},
title = {{Using nonlocal filtering and feature extraction approaches in three-dimensional face recognition by Kinect}},
volume = {15},
year = {2018}
}
@inproceedings{ISI:000380606700172,
abstract = {The biometric identification by the face is one of the oldest biometric
identification. With increasing progress in the area of identification
by the face this technique was implemented into area of security, where
it provides a faster and more accurate identification. The 3D face
reader uses for the identification of the person: eyes, mouth, nose, and
in contrast to 2D readers also chin and cheeks. 3D face reader by
Broadway manufacturer was used to measure the physiological similarities
of family members. It is equipped with the 3D camera system, which uses
the method of structured light scanning and saves the template into the
3D model of face. The obtained data were evaluated by software Turnstile
Enrolment Application (TEA). The participants of the measurement were
members of three different families. Each person was compared with the
previously saved templates of other family members. Using this method
the similarity of family members was evalua00ted.},
annote = {International Conference on Logistics, Informatics and Service Sciences
(LISS), Beijing Jiaotong Univ, Int Ctr Informat Res, Barcelona, SPAIN,
JUL 27-29, 2015},
author = {Talandova, Hana and Kralik, Lukas and Adamek, Milan},
booktitle = {2015 INTERNATIONAL CONFERENCE ON LOGISTICS, INFORMATICS AND SERVICE SCIENCES (LISS)},
editor = {{Zhang, Z and Zhang, R and Fernandez, V and Liu, S}},
isbn = {978-1-4799-1891-1},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {Beijing Jiaotong Univ, China Ctr Ind Secur Res; Beijing Jiaotong Univ, Sch Econ {\&} Management; IEEE SMC; Natl Nat Sci Fdn China; K C Wong Educ Fdn; Sino EU Doctoral Sch Sustainabil Engn; EU FP7; Beijing Logist Informat Res Base; Key Lab Logist Management {\&}},
title = {{Determination of the Physiological Similarities of Family Members by Using a Broadway 3D Biometric Device}},
year = {2015}
}
@article{ISI:000451733800040,
abstract = {Bricks are the vital component of most masonry structures. Their
maintenance is critical to the protection of masonry buildings.
Terrestrial Light Detection and Ranging (TLidar) systems provide massive
point cloud data in an accurate and fast way. TLidar enables us to
sample and store the state of a brick surface in a practical way. This
article aims to extract individual bricks from an unorganized pile of
bricks sampled by a dense point cloud. The method automatically segments
and models the individual bricks. The methodology is divided into five
main steps: Filter needless points, brick boundary points removal,
coarse segmentation using 3D component analysis, planar segmentation and
grouping, and brick reconstruction. A novel voting scheme is used to
segment the planar patches in an effective way. Brick reconstruction is
based on the geometry of single brick and its corresponding nominal size
(length, width and height). The number of bricks reconstructed is around
75{\%}. An accuracy assessment is performed by comparing 3D coordinates of
the reconstructed vertices to the manually picked vertices. The standard
deviations of differences along x, y and z axes are 4.55 mm, 4.53 mm and
4.60 mm, respectively. The comparison results indicate that the accuracy
of reconstruction based on the introduced methodology is high and
reliable. The work presented in this paper provides a theoretical basis
and reference for large scene applications in brick-like structures.
Meanwhile, the high-accuracy brick reconstruction lays the foundation
for further brick displacement estimation.},
author = {Shen, Yueqian and Lindenbergh, Roderik and Wang, Jinguo and Ferreira, Vagner G},
doi = {10.3390/rs10111709},
issn = {2072-4292},
journal = {REMOTE SENSING},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {nov},
number = {11},
title = {{Extracting Individual Bricks from a Laser Scan Point Cloud of an Unorganized Pile of Bricks}},
volume = {10},
year = {2018}
}
@article{ISI:000370350100005,
abstract = {Accurate separation of photosynthetic and nonphotosynthetic components
in a forest canopy from 3-D terrestrial laser scanning (TLS) data is a
challenging but of key importance to understand the spatial distribution
of the radiation regime, photosynthetic processes, and carbon and water
exchanges of the forest canopy. The objective of this paper was to
improve current methods for separating photosynthetic and
nonphotosynthetic components in TLS data of forest canopies by adding
two additional filters only based on its geometric information. By
comparing the proposed approach with the eigenvalues plus color
information-based method, we found that the proposed approach could
effectively improve the overall producer's accuracy from 62.12{\%} to
95.45{\%}, and the overall classification producer's accuracy would
increase from 84.28{\%} to 97.80{\%} as the forest leaf area index (LAI)
decreases from 4.15 to 3.13. In addition, variations in tree species had
negligible effects on the final classification accuracy, as shown by the
overall producer's accuracy for coniferous (93.09{\%}) and broadleaf
(94.96{\%}) trees. To remove quantitatively the effects of the woody
materials in a forest canopy for improving TLS-based LAI estimates, we
also computed the ``woody-to-total area ratio{\{}''{\}} based on the
classified linear class points from an individual tree. Automatic
classification of the forest point cloud data set will facilitate the
application of TLS on retrieving 3-D forest canopy structural
parameters, including LAI and leaf and woody area ratios.},
author = {Ma, Lixia and Zheng, Guang and Eitel, Jan U H and Moskal, L Monika and He, Wei and Huang, Huabing},
doi = {10.1109/TGRS.2015.2459716},
issn = {0196-2892},
journal = {IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {2},
pages = {679--696},
title = {{Improved Salient Feature-Based Approach for Automatically Separating Photosynthetic and Nonphotosynthetic Components Within Terrestrial Lidar Point Cloud Data of Forest Canopies}},
volume = {54},
year = {2016}
}
@inproceedings{7358753,
abstract = {{\textcopyright} 2015 IEEE. Periocular recognition has recently become an active topic in biometrics. Typically it uses 2D image data of the periocular region. This paper is the first description of combining 3D shape structure with 2D texture. A simple and effective technique using iterative closest point (ICP) was applied for 3D periocular region matching. It proved its strength for relatively unconstrained eye region capture, and does not require any training. Local binary patterns (LBP) were applied for 2D image based periocular matching. The two modalities were combined at the score-level. This approach was evaluated using the Bosphorus 3D face database, which contains large variations in facial expressions, head poses and occlusions. The rank-1 accuracy achieved from the 3D data (80{\%}) was better than that for 2D (58{\%}), and the best accuracy (83{\%}) was achieved by fusing the two types of data. This suggests that significant improvements to periocular recognition systems could be achieved using the 3D structure information that is now available from small and inexpensive sensors.},
annote = {25/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
25/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Chen, Lulu and Ferryman, James},
booktitle = {2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems, BTAS 2015},
doi = {10.1109/BTAS.2015.7358753},
isbn = {9781479987764},
keywords = {etapa1,face recognition,gil,id253,ieeexplore,image matching,image texture,iter,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {etapa1,gil,id253,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
month = {sep},
pages = {1--6},
publisher = {IEEE},
title = {{Combining 3D and 2D for less constrained periocular recognition}},
url = {http://ieeexplore.ieee.org/document/7358753/},
year = {2015}
}
@article{ISI:000443025400016,
abstract = {Face recognition algorithms customarily utilize query faces captured
from uncontrolled, in the wild, environments. The quality of these
facial images is affected by various internal factors, including the
quality of sensors used in outdoor cameras as well as external ones,
such as the quality and direction of light. These factors adversely
affect the overall quality of the captured images often causing blurring
and/or low resolution, a phenomena commonly referred to as image
degradation. Super-resolution algorithms are highly effective in
improving the resolution of degraded images, more so if the captured
face is small requiring scaling up. With this motivation, this research
aims at demonstrating the effect of one of the state-of-the-art image
super-resolution algorithms on the labeled faces in the wild (lfw)
dataset. In this regard, several cases are analyzed to demonstrate the
effectiveness of the super-resolution algorithm. Each case is then
investigated independently comparing the order of execution before or
after the 3D face alignment step. Following this, resulting images are
tested on a closed set face recognition protocol using unsupervised
algorithms with high-dimensional extracted features. The inclusion of
super-resolution resulted in improvement in the recognition rate
compared to unsupervised algorithm results reported in the literature.},
author = {ElSayed, Ahmed and Kongar, Elif and Mahmood, Ausif and Sobh, Tarek},
doi = {10.1007/s11760-018-1289-6},
issn = {1863-1703},
journal = {SIGNAL IMAGE AND VIDEO PROCESSING},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
number = {7},
pages = {1353--1360},
title = {{Unsupervised face recognition in the wild using high-dimensional features under super-resolution and 3D alignment effect}},
volume = {12},
year = {2018}
}
@article{ISI:000371787800087,
abstract = {Augmented user experiences in the cultural heritage domain are in
increasing demand by the new digital native tourists of 21st century. In
this paper, we propose a novel solution that aims at assisting the
visitor during an outdoor tour of a cultural site using the unique first
person perspective of wearable cameras. In particular, the approach
exploits computer vision techniques to retrieve the details by proposing
a robust descriptor based on the covariance of local features. Using a
lightweight wearable board, the solution can localize the user with
respect to the 3D point cloud of the historical landmark and provide him
with information about the details at which he is currently looking.
Experimental results validate the method both in terms of accuracy and
computational effort. Furthermore, user evaluation based on real-world
experiments shows that the proposal is deemed effective in enriching a
cultural experience.},
author = {Alletto, Stefano and Abati, Davide and Serra, Giuseppe and Cucchiara, Rita},
doi = {10.3390/s16020237},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {2},
title = {{Exploring Architectural Details Through a Wearable Egocentric Vision Device}},
volume = {16},
year = {2016}
}
@article{ISI:000390884300006,
abstract = {The ability to recognize an unfamiliar individual on the basis of prior
exposure to a photograph is notoriously poor and prone to errors, but
recognition accuracy is improved when multiple photographs are
available. In applied situations, when only limited real images are
available (e.g., from a mugshot or CCTV image), the generation of new
images might provide a technological prosthesis for otherwise fallible
human recognition. We report two experiments examining the effects of
providing computer-generated additional views of a target face. In
Experiment 1, provision of computer-generated views supported better
target face recognition than exposure to the target image alone and
equivalent performance to that for exposure of multiple photograph
views. Experiment 2 replicated the advantage of providing generated
views, but also indicated an advantage for multiple viewings of the
single target photograph. These results strengthen the claim that
identifying a target face can be improved by providing multiple
synthesized views based on a single target image. In addition, our
results suggest that the degree of advantage provided by synthesized
views may be affected by the quality of synthesized material.},
author = {Jones, Scott P and Dwyer, Dominic M and Lewis, Michael B},
doi = {10.1080/17470218.2016.1158302},
issn = {1747-0218},
journal = {QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
number = {5},
pages = {906--918},
title = {{The utility of multiple synthesized views in the recognition of unfamiliar faces}},
volume = {70},
year = {2017}
}
@inproceedings{ISI:000426973200029,
abstract = {This paper presents a straight-forward yet efficient, and
expression-robust 3D face recognition approach by exploring location
sensitive sparse representation of deep normal patterns (DNP). In
particular given raw 3D facial surfaces, we first run 3D face
pre-processing pipeline, including nose tip detection, face region
cropping, and pose normalization. The 3D coordinates of each normalized
3D facial surface are then projected into 2D plane to generate geometry
images, from which three images of facial surface normal components are
estimated. Each normal image is then fed into a pre-trained deep face
net to generate deep representations of facial surface normals, i.e.,
deep normal patterns. Considering the importance of different facial
locations, we propose a location sensitive sparse representation
classifier (LS-SRC) for similarity measure among deep normal patterns
associated with different 3D faces. Finally, simple score-level fusion
of different normal components are used for the final decision. The
proposed approach achieves significantly high performance, and reporting
rank-one scores of 98.01{\%}, 97.60{\%}, and 96.13{\%} on the FRGC v2.0,
Bosphorus, and BU-3DFE databases when only one sample per subject is
used in the gallery. These experimental results reveals that the
performance of 3D face recognition would be constantly improved with the
aid of training deep models from massive 2D face images, which opens the
door for future directions of 3D face recognition.},
annote = {IEEE International Joint Conference on Biometrics (IJCB), Denver, CO,
OCT 01-04, 2017},
author = {Li, Huibin and Sun, Jian and Chen, Liming},
booktitle = {2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)},
isbn = {978-1-5386-1124-1},
keywords = {revisao{\_}ieeexplore,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}webofscience},
organization = {IEEE},
pages = {234--242},
title = {{Location-Sensitive Sparse Representation of Deep Normal Patterns for Expression-robust 3D Face Recognition}},
year = {2017}
}
@inproceedings{ISI:000463335100004,
abstract = {With recent advances in artificial intelligence and pattern recognition,
automatic facial expression recognition draws a great deal of interest.
In this area, most of works involved 2D imagery. However, they present
some challenges related to pose, illumination variation and
self-occlusion. To deal with these problems, we propose to reconstruct
the face in 3D space, from only one 2D image, using the 3D Morphable
Model (3DMM). Thus, thanks to its robustness against pose and
illumination variations, 3DMM offers high-resolution model and fast
fitting functionality. Then, given the reconstructed 3D face, we extract
a set of features, which are effective to describe shape changes and
expression-related facial appearance, using Mesh-Local Binary Pattern
(mesh-LBP). Obtained results proved the effectiveness of combining 3DMM
and mesh-LBP for automatic facial expression recognition from 2D single
image. In fact, to evaluate the proposed method against state-of-the-art
methods, a comparative study shows that the method outperforms existing
ones.},
annote = {18th International Conference on Advanced Concepts for Intelligent
Vision Systems (ACIVS), Antwerp, BELGIUM, SEP 18-21, 2017},
author = {Bejaoui, Hela and Ghazouani, Haythem and Barhoumi, Walid},
booktitle = {ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS (ACIVS 2017)},
doi = {10.1007/978-3-319-70353-4_4},
editor = {{BlancTalon, J and Penne, R and Philips, W and Popescu, D and Scheunders, P}},
isbn = {978-3-319-70353-4; 978-3-319-70352-7},
issn = {0302-9743},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {Antwerp Univ; Commonwealth Sci {\&} Ind Res Org; Ghent Univ},
pages = {39--50},
series = {Lecture Notes in Computer Science},
title = {{Fully Automated Facial Expression Recognition Using 3D Morphable Model and Mesh-Local Binary Pattern}},
volume = {10617},
year = {2017}
}
@inproceedings{ISI:000385794300011,
abstract = {Light detection and ranging (LIDAR) technology offers the capability to
rapidly capture high-resolution, 3-dimensional surface data with
centimeter-level accuracy for a large variety of applications. Due to
the foliage-penetrating properties of LIDAR systems, these geospatial
data sets can detect ground surfaces beneath trees, enabling the
production of high-fidelity bare earth elevation models. Precise
characterization of the ground surface allows for identification of
terrain and non-terrain points within the point cloud, and facilitates
further discernment between natural and man-made objects based solely on
structural aspects and relative neighboring parameterizations. A
framework is presented here for automated extraction of natural and
man-made features that does not rely on coincident ortho-imagery or
point RGB attributes. The TEXAS (Terrain EXtraction And Segmentation)
algorithm is used first to generate a bare earth surface from a lidar
survey, which is then used to classify points as terrain or non-terrain.
Further classifications are assigned at the point level by leveraging
local spatial information. Similarly classed points are then clustered
together into regions to identify individual features. Descriptions of
the spatial attributes of each region are generated, resulting in the
identification of individual tree locations, forest extents, building
footprints, and 3-dimensional building shapes, among others. Results of
the fully-automated feature extraction algorithm are then compared to
ground truth to assess completeness and accuracy of the methodology.},
annote = {Conference on Laser Radar Technology and Applications XXI, Baltimore,
MD, APR 19-20, 2016},
author = {Magruder, Lori A and Leigh, Holly W and Soderlund, Alexander and Clymer, Bradley and Baer, Jessica and Neuenschwander, Amy L},
booktitle = {LASER RADAR TECHNOLOGY AND APPLICATIONS XXI},
doi = {10.1117/12.2223845},
editor = {{Turner, MD and Kamerman, GW}},
isbn = {978-1-5106-0073-7},
issn = {0277-786X},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
organization = {SPIE},
series = {Proceedings of SPIE},
title = {{Automated feature extraction for 3-dimensional point clouds}},
volume = {9832},
year = {2016}
}
@inproceedings{ISI:000425498402048,
abstract = {Given an image of a street scene in a city, this paper develops a new
method that can quickly and precisely pinpoint at which location (as
well as viewing direction) the image was taken, against a pre-stored
large-scale 3D point-cloud map of the city. We adopt the recently
developed 2D-3D direct feature matching framework for this task
{\{}[{\}}23,31,32,42-44]. This is a challenging task especially for
large-scale problems. As the map size grows bigger, many 3D points in
the wider geographical area can be visually very similar-or even
identical-causing severe ambiguities in 2D-3D feature matching. The key
is to quickly and unambiguously find the correct matches between a query
image and the large 3D map. Existing methods solve this problem mainly
via comparing individual features' visual similarities in a local and
per feature manner, thus only local solutions can be found, inadequate
for large-scale applications.
In this paper, we introduce a global method which harnesses global
contextual information exhibited both within the query image and among
all the 3D points in the map. This is achieved by a novel global ranking
algorithm, applied to a Markov network built upon the 3D map, which
takes account of not only visual similarities between individual 2D-3D
matches, but also their global compatibilities (as measured by
co-visibility) among all matching pairs found in the scene. Tests on
standard benchmark datasets show that our method achieved both higher
precision and comparable recall, compared with the state-of-the-art.},
annote = {16th IEEE International Conference on Computer Vision (ICCV), Venice,
ITALY, OCT 22-29, 2017},
author = {Liu, Liu and Li, Hongdong and Dai, Yuchao},
booktitle = {2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)},
doi = {10.1109/ICCV.2017.260},
isbn = {978-1-5386-1032-9},
issn = {1550-5499},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
organization = {IEEE; IEEE Comp Soc},
pages = {2391--2400},
series = {IEEE International Conference on Computer Vision},
title = {{Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map}},
year = {2017}
}
@article{Deng20171305,
abstract = {A novel adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition is proposed in this paper. Firstly, the novel facial coarse-to-fine landmarks localization method based on Active Shape Model and Gabor wavelets transformation is proposed to exactly and automatically locate facial landmarks in range image. Secondly, the multi-scale fusion of the pyramid local binary patterns (F-PLBP) based on the irregular segmentation associated with the located landmarks is proposed to extract the discriminative feature. Thirdly, a sparse representation-based classifier based on the adaptive feature selection (A-SRC) using the distribution of the reconstruction residual is presented to select the expression-robust feature and identify the faces. Finally, the experimental evaluation based on FRGC v2.0 indicates that the adaptive feature selection method using F-PLBP combined with the A-SRC can obtain the high recognition accuracy by performing the higher discriminative power to overcome the influence from the facial expression variations. {\textcopyright} 2017, Springer-Verlag London.},
annote = {cited By 0
01/05/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
01/05/2018 Exclu{\'{i}}do (etapa 1)},
author = {Deng, X and Da, F and Shao, H},
doi = {10.1007/s11760-017-1087-6},
journal = {Signal, Image and Video Processing},
keywords = {estela,etapa1,id422,isi,revisao{\_}scopus,revisao{\_}webofscience,scopus},
mendeley-tags = {estela,etapa1,id422,isi,revisao{\_}scopus,revisao{\_}webofscience,scopus},
number = {7},
pages = {1305--1312},
title = {{Adaptive feature selection based on reconstruction residual and accurately located landmarks for expression-robust 3D face recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017095710{\&}doi=10.1007{\%}2Fs11760-017-1087-6{\&}partnerID=40{\&}md5=fc06247cc3ca7870221563a085266269},
volume = {11},
year = {2017}
}
@article{ISI:000344204000007,
abstract = {The 3D Morphable Model (3DMM) and the Structure from Motion (SfM)
methods are widely used for 3D facial reconstruction from 2D single-view
or multiple-view images. However, model-based methods suffer from
disadvantages such as high computational costs and vulnerability to
local minima and head pose variations. The SfM-based methods require
multiple facial images in various poses. To overcome these
disadvantages, we propose a single-view-based 3D facial reconstruction
method that is person-specific and robust to pose variations. Our
proposed method combines the simplified 3DMM and the SfM methods. First,
2D initial frontal Facial Feature Points (FFPs) are estimated from a
preliminary 3D facial image that is reconstructed by the simplified
3DMM. Second, a bilateral symmetric facial image and its corresponding
FFPs are obtained from the original side-view image and corresponding
FFPs by using the mirroring technique. Finally, a more accurate the 3D
facial shape is reconstructed by the SfM using the frontal, original,
and bilateral symmetric FFPs. We evaluated the proposed method using
facial images in 35 different poses. The reconstructed facial images and
the ground-truth 3D facial shapes obtained from the scanner were
compared. The proposed method proved more robust to pose variations than
3DMM. The average 3D Root Mean Square Error (RMSE) between the
reconstructed and ground-truth 3D faces was less than 2.6 mm when 2D
FFPs were manually annotated, and less than 3.5 mm when automatically
annotated. (C) 2014 Elsevier Ltd. All rights reserved.},
author = {Jo, Jaeik and Choi, Heeseung and Kim, Ig-Jae and Kim, Jaihie},
doi = {10.1016/j.patcog.2014.07.013},
issn = {0031-3203},
journal = {PATTERN RECOGNITION},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jan},
number = {1},
pages = {73--85},
title = {{Single-view-based 3D facial reconstruction method robust against pose variations}},
volume = {48},
year = {2015}
}
@article{ISI:000397373000001,
abstract = {Robust and effective capture and reconstruction of 3D face models
directly by smartphone users enables many applications. This paper
presents a novel 3D face modeling and reconstruction solution that
robustly and accurately acquire 3D face models from a couple of images
captured by a single smartphone camera. Two selfie photos of a subject
taken from the front and side are first used to guide our Non-Negative
Matrix Factorization (NMF) induced part-based face model to iteratively
reconstruct an initial 3D face of the subject. Then, an iterative detail
updating method is applied to the initial generated 3D face to
reconstruct facial details through optimizing lighting parameters and
local depths. Our iterative 3D face reconstruction method permits fully
automatic registration of a part based face representation to the
acquired face data and the detailed 2D/3D features to build a
high-quality 3D face model. The NMF part-based face representation
learned from a 3D face database facilitates effective global and
adaptive local detail data fitting alternatively. Our system is flexible
and it allows users to conduct the capture in any uncontrolled
environment. We demonstrate the capability of our method by allowing
users to capture and reconstruct their 3D faces by themselves. (C) 2016
Elsevier B.V. All rights reserved.},
author = {Jin, Hai and Wang, Xun and Zhong, Zichun and Hua, Jing},
doi = {10.1016/j.cagd.2016.11.001},
issn = {0167-8396},
journal = {COMPUTER AIDED GEOMETRIC DESIGN},
keywords = {revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}scopus,revisao{\_}webofscience},
month = {jan},
pages = {1--13},
title = {{Robust 3D face modeling and reconstruction from frontal and side images}},
volume = {50},
year = {2017}
}
@inproceedings{ISI:000380427900044,
abstract = {Despite the great progress achieved in unconstrained face recognition,
pose variations still remain a challenging and unsolved practical issue.
We propose a novel framework for multi-view face recognition based on
extracting and matching pose-robust face signatures from 2D images.
Specifically, we propose an efficient method for monocular 3D face
reconstruction, which is used to lift the 2D facial appearance to a
canonical texture space and estimate the self-occlusion. On the lifted
facial texture we then extract various local features, which are further
enhanced by the occlusion encodings computed on the self-occlusion mask,
resulting in a pose-robust face signature, a novel feature
representation of the original 2D facial image. Extensive experiments on
two public datasets demonstrate that our method not only simplifies the
matching of multi-view 2D facial images by circumventing the requirement
for pose-adaptive classifiers, but also achieves superior performance.},
annote = {IEEE 7th International Conference on Biometrics Theory, Applications and
Systems (BTAS), Arlington, VA, SEP 08-11, 2015},
author = {Dou, Pengfei and Zhang, Lingfeng and Wu, Yuhang and Shah, Shishir K and Kakadiaris, Ioannis A},
booktitle = {2015 IEEE 7TH INTERNATIONAL CONFERENCE ON BIOMETRICS THEORY, APPLICATIONS AND SYSTEMS (BTAS 2015)},
isbn = {978-1-4799-8777-1},
issn = {2474-9680},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
organization = {IEEE},
series = {International Conference on Biometrics Theory Applications and Systems},
title = {{Pose-Robust Face Signature for Multi-View Face Recognition}},
year = {2015}
}
@article{ISI:000441334300102,
abstract = {Given that facial features contain a wide range of identification
information and cannot be completely represented by a single feature,
the fusion of multiple features is particularly significant for
achieving a robust face recognition performance, especially when there
is a big difference between the test sets and the training sets. This
has been proven in both traditional and deep learning approaches. In
this work, we proposed a novel method named C2D-CNN (color 2-dimensional
principal component analysis (2DPCA)-convolutional neural network).
C2D-CNN combines the features learnt from the original pixels with the
image representation learnt by CNN, and then makes decision-level
fusion, which can significantly improve the performance of face
recognition. Furthermore, a new CNN model is proposed: firstly, we
introduce a normalization layer in CNN to speed up the network
convergence and shorten the training time. Secondly, the layered
activation function is introduced to make the activation function
adaptive to the normalized data. Finally, probabilistic max-pooling is
applied so that the feature information is preserved to the maximum
extent while maintaining feature invariance. Experimental results show
that compared with the state-of-the-art method, our method shows better
performance and solves low recognition accuracy caused by the difference
between test and training datasets.},
author = {Li, Jing and Qiu, Tao and Wen, Chang and Xie, Kai and Wen, Fang-Qing},
doi = {10.3390/s18072080},
issn = {1424-8220},
journal = {SENSORS},
keywords = {revisao{\_}webofscience},
mendeley-tags = {revisao{\_}webofscience},
month = {jul},
number = {7},
title = {{Robust Face Recognition Using the Deep C2D-CNN Model Based on Decision-Level Fusion}},
volume = {18},
year = {2018}
}
@inproceedings{8250221,
abstract = {With the global demand for extra security systems, and the growing of human-machine interaction, facial analysis in unconstrained environments (in the wild) became a hot-topic in recent computer vision research.Unconstrained environments include surveillance footage, social media photos and live broadcasts.This type of images and videos include no control over illumination, position, size, occlusion, and facial expressions. Successful facial processing methods for controlled scenarios are unable to pledge with challenging circumstances. Consequently, methods tailored for handling those situations are indispensable for the face analysis research progress. This work presents a comprehensive review of state-of-the-art methods, drawing attention to the complications derived from in the wild scenarios and the behavior differences when applied to the controlled images.The main topics to be covered are: (1) face detection; (2) facial image quality; (3) head pose estimation; (4) face alignment; (5) 3D face reconstruction; (6) gender and age estimation; (7) facial expressions and emotions; and (8) face recognition. Finally, available code and applications for in the wild face analysis are presented,followed by a discussion on future directions.},
annote = {From Duplicate 2 (Face Analysis in the Wild - Zavan, F.H.D.B.; Gasparin, N; Batista, J C; E Silva, L P; Albiero, V; Bellon, O R P; Silva, L)

cited By 1},
author = {Zavan, F.H.D.B. and Gasparin, N and Batista, J C and {E Silva}, L P and Albiero, V and Bellon, O R P and Silva, L and {d. B. Zavan}, F H and Gasparin, N and Batista, J C and e. Silva, L P and Albiero, V and Bellon, O R P and Silva, L},
booktitle = {Proceedings - 2017 30th SIBGRAPI Conference on Graphics, Patterns and Images Tutorials SIBGRAPI-T 2017},
doi = {10.1109/SIBGRAPI-T.2017.11},
issn = {2474-0705},
keywords = {computer vision,face recognition,image reconstruct,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {9--16},
title = {{Face Analysis in the Wild}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050220731{\&}doi=10.1109{\%}2FSIBGRAPI-T.2017.11{\&}partnerID=40{\&}md5=3744f98d04fd75ae2f03f957b471838f},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Pham20171851,
abstract = {We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker capable of handling a wide range of target scene depths, whose performances are demonstrated in our extensive experiments better than those of the state-of-the-arts. {\textcopyright} 2016 IEEE.},
annote = {From Duplicate 2 (Robust real-time performance-driven 3D face tracking - Pham, H X; Pavlovic, V; Cai, J; Cham, T.-J.)

cited By 2},
author = {Pham, H X and Pavlovic, V and Cai, J and Cham, T.-J. and And},
booktitle = {2016 23rd International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2016.7899906},
keywords = {cameras,computational geometry,face recognition,im,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1851--1856},
title = {{Robust real-time performance-driven 3D face tracking}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019106844{\&}doi=10.1109{\%}2FICPR.2016.7899906{\&}partnerID=40{\&}md5=3e60c1814d147400cdda963f8f555dd2},
year = {2016}
}
@inproceedings{7391814,
abstract = {Being the most distinct feature point in 3D facial landmarks, nose tip plays a significant role in 3D facial studies such as face detection, face recognition, facial features extraction, face alignment, etc. Successful detection of nose tip can facilitate many tasks of 3D facial studies. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, needs not training, can handle large rotations and occlusions. To reduce computational cost, we first remove small isolated regions from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, the Multi-angle Energy (ME) of each point is computed and sorted in descending order. Then the first m points in the descending order list are obtained and hierarchical clustering method is used to cluster these points. In the first h largest clusters, we can find one point with the largest ME. For all scales of the scale-space, we get a series of such points which are treated as nose tip candidates. For these candidates, we apply hierarchical clustering again. In the obtained largest cluster, we compute the mean value of ME. The ME of nose tip will be closest to the mean value. We evaluate our method in two well-known 3D face databases, namely FRGC v2.0 and BOSPHORUS. The experimental results verify the robustness of our method with a high nose tip detection rate. {\textcopyright} 2015 IEEE.},
annote = {From Duplicate 2 (Robust nose tip detection for face range images based on local features in scale-space - Liu, J; Zhang, Q; Zhang, C; Tang, C)

cited By 1},
author = {Liu, J and Zhang, Q and Zhang, C and Tang, C and And},
booktitle = {2015 International Conference on 3D Imaging (IC3D)},
doi = {10.1109/IC3D.2015.7391814},
keywords = {face recognition,feature extraction,object detecti,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1--8},
title = {{Robust nose tip detection for face range images based on local features in scale-space}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963541984{\&}doi=10.1109{\%}2FIC3D.2015.7391814{\&}partnerID=40{\&}md5=df7d12e321f564181753e5d93c034bad},
year = {2016}
}
@inproceedings{7378673,
abstract = {Three-dimensional body measurement is determined by measuring the size of each part of the body and body shape, in order to study the morphological characteristics of the human body, and provide basic information for human industrial design, ergonomics, engineering design, anthropological research, and medicine. This paper presents a three-dimensional point cloud data extraction method of facial feature points by doing nose points with examples, using this method can accurately locate the feature points, and there is no specific stations toward requirements in the process of scanning point cloud of the human body, it is convenient and quick. The results show that this method can meet the ergonomics and other features to quickly locate and measure the body size requirements. {\textcopyright} 2015 IEEE.},
annote = {From Duplicate 1 (A method of extracting human facial feature points based on 3D laser scanning point cloud data - Xia, H; Huang, T; Chen, G)

cited By 0},
author = {And and Xia, H and Huang, T and Chen, G},
booktitle = {2015 23rd International Conference on Geoinformatics},
doi = {10.1109/GEOINFORMATICS.2015.7378673},
issn = {2161-024X},
keywords = {computer graphics,face recognition,feature extract,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1--3},
title = {{A method of extracting human facial feature points based on 3D laser scanning point cloud data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962437313{\&}doi=10.1109{\%}2FGEOINFORMATICS.2015.7378673{\&}partnerID=40{\&}md5=5604340c250b337bda78d65c17e2cb5d},
volume = {2016-Janua},
year = {2016}
}
@inproceedings{Fenglan2017704,
abstract = {At present, a person-specific 3D model generation technology has been an active research topic for scientists in the field of computer vision and computer graphics. The paper proposes a 3D modeling method based on single photograph, by analysis of individual difference like geometric features which present the shape and locations of facial components. An extension of the basic Active Appearance Model is proposed to localize some facial feature points in the frontal image and all these feature points are aligned to fit the generic 3D face model to a specialized one to reflect the given person's face, being both more robust and faster. Then an optimized texture mapping of cylindrical projection and model adjustment technologies are presented, to synthesize personal virtual face realistically. This method rectifies these previous defects, such as the difficulty in gaining access to acquiring the accurate data of a 3D face model, slowly manual location methods, incomplete facial information mining. The approach gives a significant improvement in both the reliability and the overall accuracy of 3D modeling. {\textcopyright} 2016 IEEE.},
annote = {From Duplicate 1 (Generation of person-specific 3D model based on single photograph - Fenglan, H; Sun, T; Bu, F)

cited By 1},
author = {And and Fenglan, H and Sun, T and Bu, F},
booktitle = {2016 2nd IEEE International Conference on Computer and Communications (ICCC)},
doi = {10.1109/CompComm.2016.7924793},
keywords = {computer graphics,computer vision,data mining,face,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {704--707},
title = {{Generation of person-specific 3D model based on single photograph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020206654{\&}doi=10.1109{\%}2FCompComm.2016.7924793{\&}partnerID=40{\&}md5=f005f39ae06e9a02f8426026f3c1e891},
year = {2016}
}
@inproceedings{7156442,
abstract = {We propose an automatic facial landmarks detection in 3D mesh manifold. The method is based on 3D Constrained Local Model (CLM) which learns both global variations in 3D face scan and local ones around every vertex landmark. Differently from the other approaches of CLM, our contribution is a full 3D mesh. The framework exploits the intrinsic 3D features around the mesh vertices by utilizing histogram-based mesh Local Binary Patterns (mesh-LBP). The experiments are conducted on publicly available 3D face scans Bosphorus database. {\textcopyright} 2015 IEEE.},
annote = {From Duplicate 2 (Mesh LBP features for 3D constrained local model - El Rai, M C; Werghi, N; Tortorici, C; Al-Muhairi, H; Al Safar, H)

cited By 0},
author = {{El Rai}, M C and Werghi, N and Tortorici, C and Al-Muhairi, H and {Al Safar}, H and Rai, M C E and Werghi, N and Tortorici, C and Al-Muhairi, H and Safar, H A},
booktitle = {2015 International Conference on Information and Communication Technology Research (ICTRC)},
doi = {10.1109/ICTRC.2015.7156442},
keywords = {face recognition,feature extraction,mesh generatio,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {144--147},
title = {{Mesh LBP features for 3D constrained local model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944128208{\&}doi=10.1109{\%}2FICTRC.2015.7156442{\&}partnerID=40{\&}md5=f31b9796bfb499a038cffd35fa3bf121},
year = {2015}
}
@inproceedings{7986860,
abstract = {Nonrigid Structure-From-Motion is a well-known approach to estimate time-varying 3D structures from 2D input image sequences. For challenging problems such as the reconstruction of human faces, state-of-the-art approaches estimate statistical shape spaces from training data. It is common practice to use orthographic or weak-perspective camera models to map 3D to 2D points. We propose to use a projective camera model combined with a multilinear tensor-based face model, enabling approximation of a dense 3D face surface by sparse 2D landmarks. Using a projective camera is beneficial, as it is able to handle perspective projections and particular camera motions which are critical for affine models. We show how the nonlinearity of the projective model can be linearized so that its parameters can be estimated by an alternating-least-squares approach. This enables simple and fast estimation of the model parameters. The effectiveness of the proposed algorithm is demonstrated using challenging real image data. {\textcopyright} 2017 MVA Organization All Rights Reserved.},
annote = {From Duplicate 2 (Projective structure from facial motion - Grashof, S; Ackermann, H; Kuhnke, F; Ostermann, J; Brandt, S S)

cited By 1},
author = {Grashof, S and Ackermann, H and Kuhnke, F and Ostermann, J and Brandt, S S and Gra{\ss}hof, S and Ackermann, H and Kuhnke, F and Ostermann, J and Brandt, S S},
booktitle = {Proceedings of the 15th IAPR International Conference on Machine Vision Applications, MVA 2017},
doi = {10.23919/MVA.2017.7986860},
keywords = {face recognition,image reconstruction,image sequen,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {298--301},
title = {{Projective structure from facial motion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027883345{\&}doi=10.23919{\%}2FMVA.2017.7986860{\&}partnerID=40{\&}md5=33cd033ecb39dcbe075d920e3ce3715e},
year = {2017}
}
@inproceedings{7428562,
abstract = {3D facial landmarks play a significant role in many 3D facial studies. Due to the complex geometry of high resolution 3D human face, landmark detection remains to be a challenge problem. This paper proposes a novel method to detect landmarks automatically for high resolution 3D faces without learning or training, solely using geometric information. For high resolution 3D faces, geodesic remeshing is used to reduce the vertices number, then the remeshed 3D faces are parameterized and interpolated on a regular grid, which enable us to compute the differential geometric features more efficiently and accurately. To detect landmarks, we extract global and local constraints for each landmark by considering relative positions of landmarks and differential geometric features, then landmarks are detected by combine these constraints. We evaluate our method and compare it with other methods which are mostly related to ours in a large scale publicly available 3D face data set, BJUT-3D face database. The experimental results show that our method is robust and effective, and achieves better performance than existing methods.},
annote = {From Duplicate 2 (Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information - Liu, J; Zhang, Q; Tang, C)

cited By 0},
author = {Liu, J and Zhang, Q and Tang, C},
booktitle = {2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
doi = {10.1109/IAEAC.2015.7428562},
keywords = {face recognition,feature extraction,image resoluti,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {276--284},
title = {{Automatic landmark detection for high resolution non-rigid 3D faces based on geometric information}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966359479{\&}doi=10.1109{\%}2FIAEAC.2015.7428562{\&}partnerID=40{\&}md5=d07bdc62c3beb1c60304198eb193ba5c},
year = {2016}
}
@inproceedings{8296814,
abstract = {Current approaches for facial landmark detection and head pose estimation first perform landmark detection, followed by fitting 3D face model or regression model to estimate head pose. Different from the existing methods, in this paper, we propose a unified method, called Coupled Cascade Regression (CCR), for simultaneous facial landmark detection and head pose estimation. At each cascade level, two separate regressors are learned to update the landmark locations and 3D face model parameters based on the local appearance features, respectively. Since 2D facial landmark locations and head pose parameters are related, we further apply the projection model to refine the prediction results in each cascade iteration and make them consistent. As a result, CCR can leverage both the learning methods and the projection model to simultaneously perform facial landmark detection and pose estimation to enhance the performances of both tasks. Experimental results on 300-W and BU datasets indicate that our proposed CCR method outperforms many conventional methods both for landmark detection and head pose estimation.},
annote = {From Duplicate 1 (Coupled cascade regression for simultaneous facial landmark detection and head pose estimation - Gou, C; Wu, Y; Wang, F.-Y.; Ji, Q)

cited By 2},
author = {Gou, C and Wu, Y and Wang, F.-Y. and Ji, Q},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2017.8296814},
issn = {2381-8549},
keywords = {face recognition,image enhancement,iterative metho,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {2906--2910},
title = {{Coupled cascade regression for simultaneous facial landmark detection and head pose estimation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045296105{\&}doi=10.1109{\%}2FICIP.2017.8296814{\&}partnerID=40{\&}md5=bbf355c9bb4fa4416e5d8af78c33014d},
volume = {2017-Septe},
year = {2018}
}
@inproceedings{Geetha20181882,
abstract = {Face Recognition is one of the biometric technique to vestige the given faces. We present a 3D face recognition method using Hadoop to recognize 3D faces under varying expressions, lighting and different poses to overcome the challenges of the 2D face recognition. In this paper, a threshold facial region of an image is detected and preprocessing is done based through the image excellence. If the selected face is frontal face with good lighting, extract the prerequisite features and do the necessary comparison steps to recognize the faces. In case, if the selected face is in bad lighting, then perform histogram equalization and normalization to increase the contrast. Different Poses and expressions are the very challenging zones which require surplus pre-processing to improve the performance of the face recognition system. Hence an enhanced normalization method called 3D Morphable Model are used as a pre- processing technique to create a frontal view from a non-frontal view and also merge images with different views in to a single frontal view. Next to diminish the number of features used for recognition process; we emulate the linear discriminant analysis method for further classification. Eventually, we used an open-source Hadoop Image Processing Interface (HIPI) to act as an interface for MapReduce technology for recognition. {\textcopyright} 2017 IEEE.},
annote = {From Duplicate 1 (3D face recognition using Hadoop - Geetha, G; Safa, M; Fancy, C; Chittal, K)

cited By 0},
author = {Geetha, G and Safa, M and Fancy, C and Chittal, K},
booktitle = {2017 International Conference on Energy, Communication, Data Analytics and Soft Computing, ICECDS 2017},
doi = {10.1109/ICECDS.2017.8389776},
keywords = {biometrics (access control),face recognition,featu,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1882--1885},
title = {{3D face recognition using Hadoop}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050073609{\&}doi=10.1109{\%}2FICECDS.2017.8389776{\&}partnerID=40{\&}md5=426f7e7f8b23f4019df744e279422d63},
year = {2018}
}
@inproceedings{Liu20181619,
abstract = {Face alignment is a classic problem in the computer vision field. Previous works mostly focus on sparse alignment with a limited number of facial landmark points, i.e., facial landmark detection. In this paper, for the first time, we aim at providing a very dense 3D alignment for large-pose face images. To achieve this, we train a CNN to estimate the 3D face shape, which not only aligns limited facial landmarks but also fits face contours and SIFT feature points. Moreover, we also address the bottleneck of training CNN with multiple datasets, due to different landmark markups on different datasets, such as 5, 34, 68. Experimental results show our method not only provides high-quality, dense 3D face fitting but also outperforms the state-of-the-art facial landmark detection methods on challenging datasets. Our model can run at real time during testing and it's available at http:///cvlab.cse.msu.edu/project-pifa.html. {\textcopyright} 2017 IEEE.},
annote = {From Duplicate 1 (Dense Face Alignment - Liu, Y; Jourabloo, A; Ren, W; Liu, X)

cited By 10},
author = {Liu, Y and Jourabloo, A and Ren, W and Liu, X},
booktitle = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
doi = {10.1109/ICCVW.2017.190},
issn = {2473-9944},
keywords = {computer vision,convolution,face recognition,featu,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1619--1628},
title = {{Dense Face Alignment}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046269553{\&}doi=10.1109{\%}2FICCVW.2017.190{\&}partnerID=40{\&}md5=180ea024a11b87baf1e8ef371cefd050},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{7426891,
abstract = {This paper is about a new method of generation texture for 3D face model. The 3D model is used as a signing avatar to help deaf people to interact easily. This approach is based on the MPEG-4 standard for facial detection points from two orthogonal photos representing the front and side view of a human head. The two photos are cut, combined and finally deformed corresponding to an UV-map to finally generate a texture which can be mapped on the 3D head.},
annote = {From Duplicate 2 (Human face texture generation based on MPEG-4 standard - Lagha, I; Othman, A)

cited By 0},
author = {Lagha, I and Othman, A},
booktitle = {2015 5th International Conference on Information and Communication Technology and Accessibility, ICTA 2015},
doi = {10.1109/ICTA.2015.7426891},
keywords = {avatars,face recognition,human fac,revisao{\_}ieeexplore,revisao{\_}scopus,solid modelling},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1--5},
title = {{Human face texture generation based on MPEG-4 standard}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988310674{\&}doi=10.1109{\%}2FICTA.2015.7426891{\&}partnerID=40{\&}md5=1ed60d9429b2b8f8342f9bc015895aff},
year = {2016}
}
@inproceedings{7424213,
abstract = {This work proposes a new algorithm for 3D face recognition. The algorithm uses 3D shape data without color or texture information and exploits local curvature information which is a measure with high discriminant capability and robust to deformations such as rotation and scaling. In order to reduce high dimensionality of typical face surfaces our approach uses a conformal parameterization, preserving angles of original faces and simplifies the correspondence problem. Experimental results are presented and discussed using CASIA and Gavab databases.},
annote = {From Duplicate 1 (Conformal parameterization and curvature analysis for 3D facial recognition - Echeagaray-Patron, B A; Miramontes-Jaramillo, D; Kober, V)

cited By 20},
author = {Echeagaray-Patron, B A and Miramontes-Jaramillo, D and Kober, V},
booktitle = {2015 International Conference on Computational Science and Computational Intelligence (CSCI)},
doi = {10.1109/CSCI.2015.133},
keywords = {3D facial recogn,face recognition,revisao{\_}ieeexplore,revisao{\_}scopus,visual databases},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {843--844},
title = {{Conformal parameterization and curvature analysis for 3D facial recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964412211{\&}doi=10.1109{\%}2FCSCI.2015.133{\&}partnerID=40{\&}md5=6a3fc9e7dac95a6f25aacc425aa40af5},
year = {2016}
}
@inproceedings{Zafeiriou20182503,
abstract = {Recently, deformable face alignment is synonymous to the task of locating a set of 2D sparse landmarks in intensity images. Currently, discriminatively trained Deep Convolutional Neural Networks (DCNNs) are the state-of-the-art in the task of face alignment. DCNNs exploit large amount of high quality annotations that emerged the last few years. Nevertheless, the provided 2D annotations rarely capture the 3D structure of the face (this is especially evident in the facial boundary). That is, the annotations neither provide an estimate of the depth nor correspond to the 2D projections of the 3D facial structure. This paper summarises our efforts to develop (a) a very large database suitable to be used to train 3D face alignment algorithms in images captured "in-the-wild" and (b) to train and evaluate new methods for 3D face landmark tracking. Finally, we report the results of the first challenge in 3D face tracking "in-the-wild".},
annote = {From Duplicate 2 (The 3D Menpo Facial Landmark Tracking Challenge - Zafeiriou, S; Chrysos, G G; Roussos, A; Ververas, E; Deng, J; Trigeorgis, G)

cited By 5},
author = {Zafeiriou, S and Chrysos, G G and Roussos, A and Ververas, E and Deng, J and Trigeorgis, G},
booktitle = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
doi = {10.1109/ICCVW.2017.16},
issn = {2473-9944},
keywords = {DC,face recognition,feature extraction,neural nets,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {2503--2511},
title = {{The 3D Menpo Facial Landmark Tracking Challenge}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046289153{\&}doi=10.1109{\%}2FICCVW.2017.16{\&}partnerID=40{\&}md5=5d07f5e693e1ebbbb60a3281183dea15},
volume = {2018-Janua},
year = {2018}
}
@conference{Goodarzi2016482,
abstract = {3D Head pose estimation using a hybrid pose estimation method is discussed in this paper. The only equipment used is simple webcam that is available on most computers and laptops today. The hybrid method consists of tracking facial landmarks of face and using geometrical face pose estimation to compute distances to estimate 3D position of head. The pose estimation system works real time as it is ultimately will be used for 2D-3D face recognition system. Actual data show reasonable error for rotation along each axis (Yaw, Pitch and Roll) by using only few facial landmarks. {\textcopyright} 2015 IEEE.},
annote = {cited By 2},
author = {Goodarzi, F and Saripan, M I},
booktitle = {IEEE 2015 International Conference on Signal and Image Processing Applications, ICSIPA 2015 - Proceedings},
doi = {10.1109/ICSIPA.2015.7412239},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {482--487},
title = {{Real time face pose estimation using geometrical features}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971673740{\&}doi=10.1109{\%}2FICSIPA.2015.7412239{\&}partnerID=40{\&}md5=73dad8ff5c977adc334d5cf02bcb4097},
year = {2016}
}
@inproceedings{8634657,
abstract = {How will I look afterwards? is a common question asked by the patients undergoing a cosmetic procedure. Cosmetic practitioners at present can only offer subjective and descriptive replies. This subjective prediction is a serious concern for patients undergoing cosmetic treatment and therefore necessitates the development of automatic techniques for facial quantification. This paper proposes a novel machine learning approach to quantify and predict the outcome of 3D facial rejuvenation prior to actual cosmetic procedure. The facial rejuvenation prediction results are achieved by estimating the dermal filler volume in 3D faces. This involves estimation of structural changes in 3D face images and to learn underlying structural mapping. Our preliminary experimental results show that the proposed model achieves superior prediction accuracy on real world dataset compared to baseline methods. The computational time analysis shows that the proposed technique is very efficient (at test time) which makes it suitable for real time applications.},
author = {{Ali Shah}, S A and Bennamoun, M and Molton, M},
booktitle = {2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)},
doi = {10.1109/IVCNZ.2018.8634657},
issn = {2151-2205},
keywords = {cosmetics;face recognition;learning (artificial in,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
pages = {1--6},
title = {{A Fully Automatic Framework for Prediction of 3D Facial Rejuvenation}},
year = {2018}
}
@inproceedings{7517246,
abstract = {This paper presents a virtual try-on system to correctly visualize 3D objects (e.g., glasses) in the face of a given user. By capturing the image and depth information of a user through a low-cost RGB-D camera, we apply a face tracking technique to detect specific landmarks in the facial image. These landmarks and the point cloud reconstructed from the depth information are combined to optimize a 3D facial morphable model that fits as good as possible to the user's head and face. At the end, we deform the chosen 3D objects from its rest shape to a deformed shape matching the specific facial shape of the user. The last step projects and renders the 3D object into the original image, with enhanced precision and in proper scale, showing the selected object in the user's face. We validate the performance of our system on eight different subjects (four male and four female) and show results numerically and visually. Our results demonstrate that, by fitting a facial model to the user's face, the rendered virtual 3D objects look more realistic.},
author = {Azevedo, P and {Dos Santos}, T O and {De Aguiar}, E},
booktitle = {2016 XVIII Symposium on Virtual and Augmented Reality (SVR)},
doi = {10.1109/SVR.2016.12},
keywords = {augmented reality;face recognition;image capture;i,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1--9},
title = {{An Augmented Reality Virtual Glasses Try-On System}},
year = {2016}
}
@inproceedings{Yin:2003:FRB:982507.982509,
address = {New York, NY, USA},
author = {Yin, Lijun and Yourst, Matt T},
booktitle = {Proceedings of the 2003 ACM SIGMM Workshop on Biometrics Methods and Applications},
doi = {10.1145/982507.982509},
isbn = {1-58113-779-6},
keywords = {face identification,face modeling,revisao{\_}acm,super-resolution},
mendeley-tags = {revisao{\_}acm},
pages = {1--8},
publisher = {ACM},
series = {WBMA '03},
title = {{3D Face Recognition Based on High-resolution 3D Face Modeling from Frontal and Profile Views}},
url = {http://doi.acm.org/10.1145/982507.982509},
year = {2003}
}
@inproceedings{8688162,
abstract = {In this paper, we present a novel panoramic image model for scattered point clouds, and apply it to the problem of place recognition. We project a point cloud onto a sphere, and then the sphere is divided into a set of individual grids by longitudes and latitudes. Each grid is regard as a pixel and its value is computed using the geometrical relationship among the points in the grid and its neighbors. For convenience, the sphere is transferred into a flat. Since point clouds are converted to 2D images, we use ORB features and bag of words technique to solve place recognition problem. Our experimental results show that our image model is a more universal one and achieve a good performance in place recognition in both accuracy and efficiency.},
author = {Cao, F and Yan, F and Gu, Y and Ding, C and Zhuang, Y and Wang, W},
booktitle = {2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)},
doi = {10.1109/CYBER.2018.8688162},
issn = {2379-7711},
keywords = {revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {79--83},
title = {{A Novel Image Model of Point Clouds and its Application in Place Recognition}},
year = {2018}
}
@inproceedings{8658722,
abstract = {State-of-the-art methods for 3D reconstruction of faces from a single image require 2D-3D pairs of ground-truth data for supervision. Such data is costly to acquire, and most datasets available in the literature are restricted to pairs for which the input 2D images depict faces in a near fronto-parallel pose. Therefore, many data-driven methods for single-image 3D facial reconstruction perform poorly on profile and near-profile faces. We propose a method to improve the performance of single-image 3D facial reconstruction networks by utilizing the network to synthesize its own training data for fine-tuning, comprising: (i) single-image 3D reconstruction of faces in near-frontal images without ground-truth 3D shape; (ii) application of a rigid-body transformation to the reconstructed face model; (iii) rendering of the face model from new viewpoints; and (iv) use of the rendered image and corresponding 3D reconstruction as additional data for supervised fine-tuning. The new 2D-3D pairs thus produced have the same high-quality observed for near fronto-parallel reconstructions, thereby nudging the network towards more uniform performance as a function of the viewing angle of input faces. Application of the proposed technique to the fine-tuning of a state-of-the-art single-image 3D-reconstruction network for faces demonstrates the usefulness of the method, with particularly significant gains for profile or near-profile views.},
author = {Xing, Y and Tewari, R and Mendonca, P},
booktitle = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
doi = {10.1109/WACV.2019.00113},
issn = {1550-5790},
keywords = {face recognition;image reconstruction;rendering (c,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {jan},
pages = {1014--1023},
title = {{A Self-Supervised Bootstrap Method for Single-Image 3D Face Reconstruction}},
year = {2019}
}
@conference{Koch2016689,
abstract = {This paper presents an approach for automatically aligning the non-overlapping interior and exterior parts of a 3D building model computed from image based 3D reconstructions. We propose a method to align the 3D reconstructions by identifying corresponding 3D structures that are part of the interior and exterior model (e.g. openings like windows). In this context, we point out the potential of using 3D line segments to enrich the information of point clouds generated by SfMs and show how this can be used for interpreting the scene and matching individual reconstructions. {\textcopyright} 2016 IEEE.},
annote = {cited By 9},
author = {Koch, T and Korner, M and Fraundorfer, F},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2016.91},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {689--697},
title = {{Automatic Alignment of Indoor and Outdoor Building Models Using 3D Line Segments}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010218095{\&}doi=10.1109{\%}2FCVPRW.2016.91{\&}partnerID=40{\&}md5=937da73c58750f8829c3a2d760233242},
year = {2016}
}
@article{Zhang20153357,
abstract = {Expression and pose variations are two major challenges for 3D face recognition. This paper presents a method to cope with these two challenges by fusing the matching results of adaptive multiple regions on the 3D face. First, one approach is proposed for pose correction of 3D face based on three landmark points: nose tip, nasion, and subnasale. Then multiple regions are adaptively chosen from the facial surface, which include nose, left and right eye-forehead regions, left and right cheeks, and mouth-chin region. Next, a least trimmed square Hausdorff distance method is applied for region matching. Moreover, to obtain a better overall performance, several score-level and rank-level fusion schemes are used to fuse the contribution of each region. The proposed approach is evaluated on the Bosphorus and the BU-3DFE databases, and yields good results. The study shows that the proposed algorithm is robust to expression and pose changes. {\textcopyright}, 2015, Binary Information Press. All right reserved.},
annote = {cited By 0},
author = {Zhang, C and Gu, Y and Wang, Y and Li, F and Zhan, Y and Pi, J and Qu, L},
doi = {10.12733/jcis14297},
journal = {Journal of Computational Information Systems},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {9},
pages = {3357--3369},
title = {{Adaptive multiple regions matching for 3D face recognition under expression and pose variations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938320850{\&}doi=10.12733{\%}2Fjcis14297{\&}partnerID=40{\&}md5=af76d7b96bb6eca2db8bc3e1babc780a},
volume = {11},
year = {2015}
}
@conference{Soltanpour20182811,
abstract = {This paper proposes a novel descriptor based on the local derivative pattern (LDP) for 3D face recognition. Compared to the local binary pattern (LBP), LDP can capture more detailed information by encoding directional pattern features. It is based on the local derivative variations that extract high-order local information. We propose a novel discriminative facial shape descriptor, local normal derivative pattern (LNDP) that extracts LDP from the surface normal. Using surface normal, the orientation of a surface at each point is determined as a first-order surface differential. Three normal component images are extracted by estimating three components of normal vectors in x, y, and z channels. Each normal component is divided into several patches and encoded using LDP. The final descriptor is created by concatenating histograms of the LNDP on each patch. Experimental results on two famous 3D face databases, FRGC v2.0 and Bosphorus illustrate the effectiveness of the proposed descriptor. {\textcopyright} 2017 IEEE.},
annote = {cited By 1},
author = {Soltanpour, S and Wu, Q M J},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2017.8296795},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {2811--2815},
title = {{High-order local normal derivative pattern (LNDP) for 3D face recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045292850{\&}doi=10.1109{\%}2FICIP.2017.8296795{\&}partnerID=40{\&}md5=12c33512c432220af4c4c49f5116433e},
volume = {2017-Septe},
year = {2018}
}
@inproceedings{Rizvi:2011:HMF:2007052.2007065,
address = {New York, NY, USA},
author = {Rizvi, Qaim Mehdi and Abbas, Qamar and Ahmad, Hasan},
booktitle = {Proceedings of the International Conference on Advances in Computing and Artificial Intelligence},
doi = {10.1145/2007052.2007065},
isbn = {978-1-4503-0635-5},
keywords = {average threshold value,gray scale pixel,localization,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {61--64},
publisher = {ACM},
series = {ACAI '11},
title = {{A Hybrid Method for 3D Face Localization}},
url = {http://doi.acm.org/10.1145/2007052.2007065},
year = {2011}
}
@inproceedings{8014991,
abstract = {We propose a novel 3D-assisted coarse-to-fine extreme-pose facial landmark detection system in this work. For a given face image, our system first refines the face bounding box with landmark locations inferred from a 3D face model generated by a Recurrent 3D Regressor at coarse level. Another R3R is then employed to fit a 3D face model onto the 2D face image cropped with the refined bounding box at fine-scale. 2D landmark locations inferred from the fitted 3D face are further adjusted with the popular 2D regression method, i.e. LBF. The 3D-assisted coarse-to-fine strategy and the 2D adjustment process explicitly ensure both the robustness to extreme face poses and bounding box disturbance and the accuracy towards pixel-level landmark displacement. Extensive experiments on the Menpo Challenge test sets demonstrate the superior performance of our system.},
author = {Xiao, S and Li, J and Chen, Y and Wang, Z and Feng, J and Yan, S and Kassim, A},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2017.257},
issn = {2160-7516},
keywords = {face recognition;object detection;solid modelling;,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {2060--2068},
title = {{3D-Assisted Coarse-to-Fine Extreme-Pose Facial Landmark Detection}},
year = {2017}
}
@article{Wang201682,
abstract = {We propose an automatic method for fast reconstruction of indoor scenes from raw point scans, which is a fairly challenging problem due to the restricted accessibility and the cluttered space for indoor environment. We first detect and remove points representing the ground, walls and ceiling from the input data and cluster the remaining points into different groups, referred to as sub-scenes. Our approach abstracts the sub-scenes with geometric primitives, and accordingly constructs the topology graphs with structural attributes based on the functional parts of objects (namely, anchors). To decompose sub-scenes into individual indoor objects, we devise an anchor-guided subgraph matching algorithm which leverages template graphs to partition the graphs into subgraphs (i.e., individual objects), which is capable of handling arbitrarily oriented objects within scenes. Subsequently, we present a data-driven approach to model individual objects, which is particularly formulated as a model instance recognition problem. A Randomized Decision Forest (RDF) is introduced to achieve robust recognition on decomposed indoor objects with raw point data. We further exploit template fitting to generate the geometrically faithful model to the input indoor scene. We visually and quantitatively evaluate the performance of our framework on a variety of synthetic and raw scans, which comprehensively demonstrates the efficiency and robustness of our reconstruction method on raw scanned point clouds, even in the presence of noise and heavy occlusions. {\textcopyright} 2016 Elsevier B.V. All rights reserved.},
annote = {cited By 2},
author = {Wang, J and Xie, Q and Xu, Y and Zhou, L and Ye, N},
doi = {10.1016/j.cagd.2016.02.012},
journal = {Computer Aided Geometric Design},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {82--94},
title = {{Cluttered indoor scene modeling via functional part-guided graph matching}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960173025{\&}doi=10.1016{\%}2Fj.cagd.2016.02.012{\&}partnerID=40{\&}md5=6a810baf82b6089cce5792c5905809c7},
volume = {43},
year = {2016}
}
@inproceedings{Nozawa:2015:FRS:2787626.2792634,
address = {New York, NY, USA},
author = {Nozawa, Naoki and Kuwahara, Daiki and Morishima, Shigeo},
booktitle = {ACM SIGGRAPH 2015 Posters},
doi = {10.1145/2787626.2792634},
isbn = {978-1-4503-3632-1},
keywords = {revisao{\_}acm,revisao{\_}scopus},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus},
pages = {57:1----57:1},
publisher = {ACM},
series = {SIGGRAPH '15},
title = {{3D Face Reconstruction from a Single Non-frontal Face Image}},
url = {http://doi.acm.org/10.1145/2787626.2792634},
year = {2015}
}
@inproceedings{7890167,
abstract = {We propose a three-dimensional (3D) face-modelling method from a single two-dimensional (2D) face image using a gallery of 2D face images and their corresponding 3D face models. Unlike existing methods, which require human effort, we provide a simple way to reconstruct 3D face models without user interaction. Our main approach is based on the idea that a particular coefficient that linearly combines vectors of 2D face images and outputs a vector that approximates the input image vector in terms of the vector norm can be reused in 3D models. Therefore, the pair of a 2D image and its 3D model plays an important role in our algorithm. Using the FaceGen software allows us to avoid the employed in previous works procedure whereby the 3D model is generated in a labor-intensive, expensive way. As a result, we are able to easily establish our 2D and 3D database. In this paper, we present a method for adopting the coefficients in 3D models and demonstrate the results of our algorithm.},
author = {Yun, J and Lee, J and Han, D and Ju, J and Kim, J},
booktitle = {2017 19th International Conference on Advanced Communication Technology (ICACT)},
doi = {10.23919/ICACT.2017.7890167},
keywords = {face recognition;image reconstruction;vectors;cost,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {629--632},
title = {{Cost-efficient 3D face reconstruction from a single 2D image}},
year = {2017}
}
@article{Dong2016,
abstract = {The paper proposes a robust framework for 3D facial movement tracking in real time using a monocular camera. It is designed to estimate the 3D face pose and local facial animation such as eyelid movement and mouth movement. The framework firstly utilizes the Discriminative Shape Regression method to locate the facial feature points on the 2D image and fuses the 2D data with a 3D face model using Extended Kalman Filter to yield 3D facial movement information. An alternating optimizing strategy is adopted to fit to different persons automatically. Experiments show that the proposed framework could track the 3D facial movement across various poses and illumination conditions. Given the real face scale the framework could track the eyelid with an error of 1 mm and mouth with an error of 2 mm. The tracking result is reliable for expression analysis or mental state inference. {\textcopyright} 2016 by the authors; licensee MDPI, Basel, Switzerland.},
annote = {cited By 2},
author = {Dong, Y and Wang, Y and Yue, J and Hu, Z},
doi = {10.3390/s16081157},
journal = {Sensors (Switzerland)},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {8},
title = {{Real time 3D facial movement tracking using a monocular camera}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979556449{\&}doi=10.3390{\%}2Fs16081157{\&}partnerID=40{\&}md5=bf4e57a3924fb246feab613ef0a62047},
volume = {16},
year = {2016}
}
@inproceedings{Kopinski:2016:DLA:2994374.2994392,
address = {New York, NY, USA},
author = {Kopinski, Thomas and Sachara, Fabian and Handmann, Uwe},
booktitle = {Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
doi = {10.1145/2994374.2994392},
isbn = {978-1-4503-4750-1},
keywords = {Deep Learning,Object Recognition,mid-air gestures,revisao{\_}acm,revisao{\_}scopus},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus},
pages = {1--9},
publisher = {ACM},
series = {MOBIQUITOUS 2016},
title = {{A Deep Learning Approach to Mid-air Gesture Interaction for Mobile Devices from Time-of-Flight Data}},
url = {http://doi.acm.org/10.1145/2994374.2994392},
year = {2016}
}
@inproceedings{7457820,
author = {Manceau, J and Soladi{\'{e}}, C and S{\'{e}}guier, R},
booktitle = {2015 Visual Communications and Image Processing (VCIP)},
doi = {10.1109/VCIP.2015.7457820},
keywords = {etapa1,face recognition,human computer interaction,id261,ieeexplore,image,poly,revisao{\_}ieeexplore},
mendeley-tags = {etapa1,id261,ieeexplore,poly,revisao{\_}ieeexplore},
pages = {1--4},
title = {{3D facial clone based on depth patches}},
year = {2015}
}
@conference{Zhang20161439,
abstract = {Human face analysis is the basis for many other computer vision tasks, such as camera surveillance, entrance authorization and age estimation. With 3D face models, the vision task based on facial analysis can usually achieve a higher accuracy than the 2D cases since it provides more information with the additional dimension. However, most existing 3D face reconstruction methods suffer from complicated processing and high computation. This paper presents a novel method that simplifies the 3D face reconstruction process with only one shot of Kinect data. The output of the system is a high density of 3D face point cloud with smoother surface. This provides rich details of the human face for other computer vision tasks. Experiments with real world data show promising results using the proposed method. {\textcopyright} 2015 IEEE.},
annote = {cited By 3},
author = {Zhang, S and Yu, H and Dong, J and Wang, T and Ju, Z and Liu, H},
booktitle = {Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015},
doi = {10.1109/SMC.2015.255},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {1439--1444},
title = {{Automatic Reconstruction of Dense 3D Face Point Cloud with a Single Depth Image}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964490778{\&}doi=10.1109{\%}2FSMC.2015.255{\&}partnerID=40{\&}md5=b3e1cafbcbcde3674270b25529af093d},
year = {2016}
}
@article{Cole:1998:IAA:288076.288077,
address = {New York, NY, USA},
author = {Cole, Ron and Carmell, Tim and Connors, Pam and Macon, Mike and Wouters, Johan and de Villiers, Jacques and Tarachow, Alice and Massaro, Dominic and Cohen, Michael and Beskow, Jonas and Yang, Jie and Meier, Uwe and Waibel, Alex and Stone, Pat and Davis, Alice and Soland, Chris and Fortier, George},
doi = {10.1145/288076.288077},
issn = {0163-5727},
journal = {SIGCAPH Comput. Phys. Handicap.},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
number = {61},
pages = {5--10},
publisher = {ACM},
title = {{Intelligent Animated Agents for Interactive Language Training}},
url = {http://doi.acm.org/10.1145/288076.288077},
year = {1998}
}
@inproceedings{8590014,
abstract = {Facial expression editing in face sketch is an important and challenging problem in computer vision community as facial animation and modeling. For criminal investigation and portrait drawing, automatic expression editing tools for face sketch improve work efficiency obviously and reduce professional requirements for users. In this paper, we propose a novel method for facial expression editing in face sketch using shape space theory. The new facial expressions in the sketch images can be regenerated automatically. The method includes two components: 1) face sketch modeling; 2) expression editing. The face sketch modeling constructs 3D face sketch data from 3D facial database to match the 2D face sketch. Using facial landmarks, the "shape" of the face sketch is represented in shape space. The shape space is a manifold space which removes the rigid transform group. In shape space, the accurate 3D face sketch model is obtained which is consistent to the original 2D face sketch. For expression editing, we change the parameters of 3D face sketch model in the shape space to obtain new expressions. The expression transfer in 3D face sketch model can be mapped into the 2D face sketch. The advantages of our method are: full-automatic in modeling process; no requirements of drawing skills to user and friendly interaction; robustness to head poses and different scales. In experiments, we use the 3D facial database, FaceWareHouse, to construct the 3D face sketch model and use face sketch images from database: CUHK Face sketch Database (CUFS) to show the performance of expression editing. Experimental results demonstrate that our method can effectively edit facial expressions in face sketch with high consistency and fidelity.},
author = {Lv, C and Wu, Z and Wang, X and Zhang, D and Liu, X and Zhou, M},
booktitle = {2018 International Conference on Cyberworlds (CW)},
doi = {10.1109/CW.2018.00019},
keywords = {computer vision;emotion recognition;face recogniti,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {33--40},
title = {{Facial Expression Editing in Face Sketch Using Shape Space Theory}},
year = {2018}
}
@inproceedings{Abate:2004:FRS:2384368.2384392,
address = {Aire-la-Ville, Switzerland, Switzerland},
author = {Abate, A F and Nappi, M and Ricciardi, S and Sabatino, G},
booktitle = {Proceedings of the 5th International Conference on Virtual Reality, Archaeology and Intelligent Cultural Heritage},
doi = {10.2312/VAST/VAST04/185-191},
isbn = {3-905673-18-5},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {185--191},
publisher = {Eurographics Association},
series = {VAST'04},
title = {{3D Face Reconstruction from Skull Aimed to Archaeological Applications. The Site of Murecine: A Case Study}},
url = {http://dx.doi.org/10.2312/VAST/VAST04/185-191},
year = {2004}
}
@inproceedings{8099972,
abstract = {We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face models visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.},
author = {Sheng, L and Cai, J and Cham, T and Pavlovic, V and Ngan, K N},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.489},
issn = {1063-6919},
keywords = {face recognition;object tracking;pose estimation;r,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {4598--4607},
title = {{A Generative Model for Depth-Based Robust 3D Facial Pose Tracking}},
year = {2017}
}
@article{7879309,
abstract = {This paper presents a 6-degree of freedom (DOF) pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired. The PE method involves two-graph simultaneous localization and mapping (SLAM) processes to reduce the accumulative pose error of the device. In the first step, the floor plane is extracted from the 3-D camera's point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch, and Zerrors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y, and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a visually impaired person in an indoor environment. The system uses the estimated pose and floor plan to locate the device user in a building and guides the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.},
author = {Zhang, H and Ye, C},
doi = {10.1109/TNSRE.2017.2682265},
issn = {1534-4320},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
keywords = {Three-Dimensional;Patient Identification Systems;,biomedical engineering;geometry;indoor navigation;,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
number = {9},
pages = {1592--1604},
title = {{An Indoor Wayfinding System Based on Geometric Features Aided Graph SLAM for the Visually Impaired}},
volume = {25},
year = {2017}
}
@inproceedings{Dutta201699,
abstract = {In recent year, 3D face images play a major role in face recognition over corresponding 2D images. In this work, the 3D range images are used for face recognition based on region classifiers. Three different regions: eye, nose, and mouth separately classify a face image locally. At first, local binary patterns (LBP) are calculated for all pixels on face images. A new image formed with these LBP values are cropped and then divided into three horizontal regions, namely eye, nose, and mouth. For each such region histogram of oriented gradient (HOG) is used for feature vector creation. Two databases: Frav3D of 106 different subjects and our database consisting of 102 various individuals are used for recognition. Only frontal images with occlusion, expression and neutral images from those databases are utilized for this proposed system. Two fold cross validation technique with a nearest centroid-based classifier is used for classification. In the case of decision level fusion, recognition accuracy is 88.86{\%} on Frav3D database and 77.5{\%} for our newly created database. On the other hand, score level fusion has shown 78.5{\%} recognition accuracy for FRAV3D database and 65.55{\%} for our database. {\textcopyright} 2016 IEEE.},
annote = {cited By 0

17/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
17/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Dutta, Koushik and Bhattacharjee, Debotosh and Nasipuri, Mita},
booktitle = {2016 1st International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)},
doi = {10.1109/ICITISEE.2016.7803055},
isbn = {978-1-5090-1567-2},
keywords = {3D range image,Decision level fusion,Histogram of the oriented gradient,Local binary pattern,Region classifier,Score level fusion,etapa1,gil,id65,ieeexplore,isi,revisao{\_}scopus,scopus},
mendeley-tags = {etapa1,gil,id65,ieeexplore,isi,revisao{\_}scopus,scopus},
month = {aug},
pages = {99--104},
publisher = {IEEE},
title = {{Expression and occlusion invariant 3D face recognition based on region classifier}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011277941{\&}doi=10.1109{\%}2FICITISEE.2016.7803055{\&}partnerID=40{\&}md5=1f7c5e310e926e850e0c41e4fc391244 http://ieeexplore.ieee.org/document/7803055/},
year = {2016}
}
@inproceedings{Sucontphunt:2010:HFI:1836845.1836955,
address = {New York, NY, USA},
author = {Sucontphunt, Tanasai and Deng, Zhigang and Neumann, Ulrich},
booktitle = {ACM SIGGRAPH 2010 Posters},
doi = {10.1145/1836845.1836955},
isbn = {978-1-4503-0393-4},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {102:1----102:1},
publisher = {ACM},
series = {SIGGRAPH '10},
title = {{3D Human Face Identity Transfer Using Deformation Gradient}},
url = {http://doi.acm.org/10.1145/1836845.1836955},
year = {2010}
}
@conference{Marvadi2016,
abstract = {Even if, most of 2D face recognition approaches reached recognition rate more than 90{\%} in controlled environment, current days face recognition systems degrade their performance in case of uncontrolled environment which includes pose variations, illumination variations, expression variations and ageing effect etc. Inclusion of 3D face analysis gives an age over 2D face recognition as they give vital informations such as 3D shape, texture and depth which improve discrimination power of an algorithm. In this paper, we have investigated different 3D face recognition approaches that are robust to changes in facial expressions and illumination variations. 2D-PCA and 2D-LDA approaches have been extended to 3D face recognition because they can directly work on 2D depth image matrices rather than 1D vectors without need for transformations before feature extraction. In turn, this reduces storage space and time required for computations. 2D depth image is extracted from 3D face model and nose region from depth mapped image has been detected as a reference point for cropping stage to convert model into a standard size. Two Dimensional Principal Component Analysis (2D-PCA) and Two Dimensional Linear Discriminant analysis (2D-LDA) are employed to obtain feature vectors globally compared to feature vectors obtained locally using PCA or LDA. Finally, euclidean distance classifier is applied for comparison of extracted features. A set of experiments on GavabDB 3D face database, which has 61 individuals in total, demonstrated that 3D face recognition using 2D-LDA method has achieved recognition accuracy of 93.3{\%} and EER of 8.96{\%} over database, which is higher compared to 2D-PCA. So, more optimized performance has been achieved using 2D-LDA for 3D face recognition analysis. {\textcopyright} 2015 IEEE.},
annote = {cited By 1},
author = {Marvadi, D and Paunwala, C and Joshi, M and Vora, A},
booktitle = {NUiCONE 2015 - 5th Nirma University International Conference on Engineering},
doi = {10.1109/NUICONE.2015.7449603},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Comparative analysis of 3D face recognition using 2D-PCA and 2D-LDA approaches}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966327409{\&}doi=10.1109{\%}2FNUICONE.2015.7449603{\&}partnerID=40{\&}md5=b5b3315314aa606ed6ef4b3af1a2625f},
year = {2016}
}
@inproceedings{7177468,
abstract = {The face reveals the healthy status of an individual, through a combination of physical signs and facial expressions. The project SEMEOTICONS is translating the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, images, and 3D scans of the face. SEMEOTICONS is developing a multisensory platform, in the form of a smart mirror, looking for signs related to cardio-metabolic risk. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. Building the multisensory mirror requires addressing significant scientific and technological challenges, from touch-less data acquisition, to real-time processing and integration of multimodal data.},
author = {Andreu-Cabedo, Y and Castellano, P and Colantonio, S and Coppini, G and Favilla, R and Germanese, D and Giannakakis, G and Giorgi, D and Larsson, M and Marraccini, P and Martinelli, M and Matuszewski, B and Milanic, M and Pascali, M and Pediaditis, M and Raccichini, G and Randeberg, L and Salvetti, O and Stromberg, T},
booktitle = {2015 IEEE International Conference on Multimedia and Expo (ICME)},
doi = {10.1109/ICME.2015.7177468},
issn = {1945-7871},
keywords = {data integration;face recognition;feature extracti,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1--6},
title = {{Mirror mirror on the wall {\#}x2026; An intelligent multisensory mirror for well-being self-assessment}},
year = {2015}
}
@inproceedings{7406406,
abstract = {Supervised Descent Method (SDM) has proven successful in many computer vision applications such as face alignment, tracking and camera calibration. Recent studies which used SDM, achieved state of the-art performance on facial landmark localization in depth images [4]. In this study, we propose to use ridge regression instead of least squares regression for learning the SDM, and to change feature sizes in each iteration, effectively turning the landmark search into a coarse to fine process. We apply the proposed method to facial landmark localization on the Bosphorus 3D Face Database, using frontal depth images with no occlusion. Experimental results confirm that both ridge regression and using adaptive feature sizes improve the localization accuracy considerably.},
author = {Camg{\"{o}}z, N C and Truc, V and Gokberk, B and Akarun, L and Kindiroglu, A A},
booktitle = {2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
doi = {10.1109/ICCVW.2015.57},
keywords = {computer vision;face recognition;image sensors;ite,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {378--383},
title = {{Facial Landmark Localization in Depth Images Using Supervised Ridge Descent}},
year = {2015}
}
@conference{Hammer2016,
abstract = {The detection of objects, or persons, is a common task in the fields of environment surveillance, object observation or danger defense. There are several approaches for automated detection with conventional imaging sensors as well as with LiDAR sensors, but for the latter the real-time detection is hampered by the scanning character and therefore by the data distortion of most LiDAR systems. The paper presents a solution for real-time data acquisition of a flash LiDAR sensor with synchronous raw data analysis, point cloud calculation, object detection, calculation of the next best view and steering of the pan-tilt head of the sensor. As a result the attention is always focused on the object, independent of the behavior of the object. Even for highly volatile and rapid changes in the direction of motion the object is kept in the field of view. The experimental setup used in this paper is realized with an elementary person detection algorithm in medium distances (20 m to 60 m) to show the efficiency of the system for objects with a high angular speed. It is easy to replace the detection part by any other object detection algorithm and thus it is easy to track nearly any object, for example a car or a boat or an UAV in various distances. {\textcopyright} 2016 SPIE.},
annote = {cited By 2},
author = {Hammer, M and Hebel, M and Arens, M},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2240640},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Automated object detection and tracking with a flash LiDAR system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011029544{\&}doi=10.1117{\%}2F12.2240640{\&}partnerID=40{\&}md5=b07dd0e6e54d97fdabd5857cd0810c72},
volume = {9988},
year = {2016}
}
@conference{Quan201545,
abstract = {The aims of this paper are to introduce a 3-D shape matching scheme for automatic face recognition and to demonstrate its invariance to pose and facial expressions. The core of this scheme lies on the combination of non-rigid deformation registration and statistical shape modelling. While the former matches 3-D faces regardless of facial expression variations, the latter provides a low-dimensional feature vector that describes the deformation after the shape matching process, thereby enabling robust identification of 3-D faces. In order to assist establishment of accurate dense point correspondences, an isometric embedding shape representation is introduced, which is able to transform 3-D faces to a canonical form that retains the intrinsic geometric structure and achieve shape alignment of 3-D faces independent from individual's facial expression. The feasibility and effectiveness of the proposed method was investigated using standard publicly available Gavab and BU-3DFE databases, which contain faces expressions and pose variations. The performance of the system was compared with the existing benchmark approaches and it demonstrates that the proposed scheme provides a competitive solution for the face recognition task with real-world practicality.},
annote = {cited By 1},
author = {Quan, W and Matuszewski, B J and Shark, L.-K.},
booktitle = {ICPRAM 2015 - 4th International Conference on Pattern Recognition Applications and Methods, Proceedings},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {45--52},
title = {{3-D shape matching for face analysis and recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938888045{\&}partnerID=40{\&}md5=232ea318938db81af84e5520771102aa},
volume = {2},
year = {2015}
}
@article{7776921,
abstract = {Given a photo collection of “unconstrained” face images of one individual captured under a variety of unknown pose, expression, and illumination conditions, this paper presents a method for reconstructing a 3D face surface model of the individual along with albedo information. Unlike prior work on face reconstruction that requires large photo collections, we formulate an approach to adapt to photo collections with a high diversity in both the number of images and the image quality. To achieve this, we incorporate prior knowledge about face shape by fitting a 3D morphable model to form a personalized template, following by using a novel photometric stereo formulation to complete the fine details, under a coarse-to-fine scheme. Our scheme incorporates a structural similarity-based local selection step to help identify a common expression for reconstruction while discarding occluded portions of faces. The evaluation of reconstruction performance is through a novel quality measure, in the absence of ground truth 3D scans. Superior large-scale experimental results are reported on synthetic, Internet, and personal photo collections.},
author = {Roth, J and Tong, Y and Liu, X},
doi = {10.1109/TPAMI.2016.2636829},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {face recognition;image reconstruction;shape recogn,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
number = {11},
pages = {2127--2141},
title = {{Adaptive 3D Face Reconstruction from Unconstrained Photo Collections}},
volume = {39},
year = {2017}
}
@conference{Beumier20152291,
abstract = {In the context of 3D face recognition, facial surfaces are advantageously captured by a structured light acquisition system, which is typically quick, low cost and uses off-the-shelve components. The light pattern projected, a key aspect of the structured light approach, makes the major difference between developed systems. In most of them, elements of the light pattern must be identified by a property such as element thickness or colour. We present in this paper the design of projected patterns that led to the realisation of three 3D acquisition prototypes. {\textcopyright} 2004 EUSIPCO.},
annote = {cited By 0},
author = {Beumier, C},
booktitle = {European Signal Processing Conference},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {2291--2294},
title = {{Design of coded structured light pattern for 3D facial surface capture}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979948896{\&}partnerID=40{\&}md5=b09f05fe749fab292462602ba77b999e},
volume = {06-10-Sept},
year = {2015}
}
@article{Li20153739,
abstract = {This paper proposed a system for a real 3D facial reconstruction method based on multi-view stereo vision generated using an orthographic 3D model. Multi-view stereopsis is an effective technology for expanding perspective and reducing noise. The presented multi-view stereo vision is implemented as a multi-view stereo vision using calibration, stereo matching, combining, reconstruction and texture mapping procedures. This paper makes a systemic contribution and two technical contributions. Its systemic contribution is that it demonstrates a state-of-the-art passive stereo vision system for meaningful orthographic 3D facial reconstruction. We have tested our method on various subjects, including two actors and an artificial plastic human head model. The primary technical contribution is an algorithm that combines stereo vision data based on clustering, which includes the following procedures: outlier detection, filling and data combining. The second contribution is a multi-view calibration method, which results in an orthographic position model. An orthographic position is an important requirement and is useful in various medical fields, e. g., maxillofacial surgery research. In addition, we describe a cluster sampling algorithm for undersampling point clouds. The presented sampling algorithm keeps data on an average distribution. A quantitative evaluation proves that the system is an effective, low-cost and high-resolution solution to reconstructing a 3D face model without markings or structured light. 1548-7741/Copyright {\textcopyright} 2015 Binary Information Press},
annote = {cited By 1},
author = {Li, K and Zeng, D and Zhang, J and Lin, R and Gao, L and Liao, X},
doi = {10.12733/jics20106046},
journal = {Journal of Information and Computational Science},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {10},
pages = {3739--3753},
title = {{A real sense 3D face reconstruction system based on multi-view stereo vision}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937578970{\&}doi=10.12733{\%}2Fjics20106046{\&}partnerID=40{\&}md5=7130d5d45220a0ddafde996ead128596},
volume = {12},
year = {2015}
}
@conference{Ma20161983,
abstract = {Finger vein recognition has attracted increasing attention in biometric identification field. To solve the problems of lack of depth information in recognition based on 2D image and no enough obvious features used for recognition based on 3D point cloud, a novel approach of identifying individuals using 3D point clouds matching of finger vein and contour is proposed in this paper. A pair of vein images of a finger are captured under Near Infrared(NIR) light using our binocular vision device. The edge and vein contours of finger are extracted as features used for describe finger vein and then 3D point cloud of finger vein and contour is reconstructed though binocular vision technique. At last, personal identification based on matching results between template and reconstructed 3D point cloud using Iterative Closest Point(ICP) algorithm can be achieved. The experimental results show that proposed method can generate more discriminative features to represent 3D model of finger vein and contour so that the accuracy of matching is enhanced. {\textcopyright} 2016 IEEE.},
annote = {cited By 1},
author = {Ma, Z and Fang, L and Duan, J and Xie, S and Wang, Z},
booktitle = {2016 IEEE International Conference on Mechatronics and Automation, IEEE ICMA 2016},
doi = {10.1109/ICMA.2016.7558870},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {1983--1988},
title = {{Personal identification based on finger vein and contour point clouds matching}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991209152{\&}doi=10.1109{\%}2FICMA.2016.7558870{\&}partnerID=40{\&}md5=dca93b520578a8a2df7e476855495845},
year = {2016}
}
@inproceedings{8237443,
abstract = {Despite remarkable progress of face analysis techniques, detecting landmarks on large-pose faces is still difficult due to self-occlusion, subtle landmark difference and incomplete information. To address these challenging issues, we introduce a novel recurrent 3D-2D dual learning model that alternatively performs 2D-based 3D face model refinement and 3D-to-2D projection based 2D landmark refinement to reliably reason about self-occluded landmarks, precisely capture the subtle landmark displacement and accurately detect landmarks even in presence of extremely large poses. The proposed model presents the first loop-closed learning framework that effectively exploits the informative feedback from the 3D-2D learning and its dual 2D-3D refinement tasks in a recurrent manner. Benefiting from these two mutual-boosting steps, our proposed model demonstrates appealing robustness to large poses (up to profile pose) and outstanding ability to capture fine-scale landmark displacement compared with existing 3D models. It achieves new state-of-the-art on the challenging AFLW benchmark. Moreover, our proposed model introduces a new architectural design that economically utilizes intermediate features and achieves 4× faster speed than its deep learning based counterparts.},
author = {Xiao, S and Feng, J and Liu, L and Nie, X and Wang, W and Yan, S and Kassim, A},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.181},
issn = {2380-7504},
keywords = {face recognition;feature extraction;learning (arti,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1642--1651},
title = {{Recurrent 3D-2D Dual Learning for Large-Pose Facial Landmark Detection}},
year = {2017}
}
@article{Yue2015508,
abstract = {A fully automatic facial-expression recognition (FER) system on 3D expression mesh models was proposed. The system didn't need human interaction from the feature extraction stage till the facial expression classification stage. The features extracted from a 3D expression mesh model were a bunch of radial facial curves to represent the spatial deformation of the geometry features on human face. Each facial curve was a surface line on the 3D face mesh model, begun from the nose tip and ended at the boundary of the previously trimmed 3D face points cloud. Then Euclid distance was employed to calculate the difference between facial curves extracted from the neutral face and each face with different expressions of one person as feature. By employing support vector machine (SVM) as classifier, the experimental results on the well-known 3D-BUFE dataset indicate that the proposed system could better classify the six prototypical facial expressions than state-of-art algorithms. {\textcopyright} 2015 Beijing Institute of Technology.},
annote = {cited By 0},
author = {Yue, L and Shen, T.-Z. and Zhang, C and Zhao, S.-Y. and Du, B.-Z.},
doi = {10.15918/j.jbit1004-0579.201524.0412},
journal = {Journal of Beijing Institute of Technology (English Edition)},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {4},
pages = {508--512},
title = {{Radial-curve-based facial expression recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957809484{\&}doi=10.15918{\%}2Fj.jbit1004-0579.201524.0412{\&}partnerID=40{\&}md5=4c2775af67311346b70321b4ed6b5401},
volume = {24},
year = {2015}
}
@conference{Mejia201526,
abstract = {In the advance of science, different disciplines are interested to study the human body, among of them we have the facial anthropometry, which studies the human face's dimension and proportions by detecting what is called anthropometric landmarks over the facial surface. Such facial landmarks can be detected by using either direct or indirect techniques. Within a 3D face image it is generally know that the most distinctive facial landmarks are the inner-corners and the tip of the nose, although their appearance varies among human races. Hence, most face processing application generally relay on a face model for comparison and analysis based on such anthropometric landmarks. This paper presents a survey on 3D face modeling, which in the best of our knowledge is missed in relate literature and it is necessary to propose a new 3D facial model, and such model is a primary step in any face processing application. In particular, we are aim to produce a useful 3D face model for research in key areas based on Mexican anthropometric analysis. {\textcopyright} 2014 IEEE.},
annote = {cited By 1},
author = {Mejia, S and Romero, M},
booktitle = {Proceedings - 2014 IEEE International Conference on Mechatronics, Electronics, and Automotive Engineering, ICMEAE 2014},
doi = {10.1109/ICMEAE.2014.31},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {26--31},
title = {{A Survey on 3D Face Modeling}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946121780{\&}doi=10.1109{\%}2FICMEAE.2014.31{\&}partnerID=40{\&}md5=d6e8fc3834ed7e3da28edf181caf6ff8},
year = {2015}
}
@article{Banita20182325,
abstract = {Face recognition are of great interest to researchers in terms of Image processing and Computer Graphics. In recent years, various factors become popular which clearly affect the face model. Which are ageing, universal facial expressions, and muscle movement. Similarly in terms of medical terminology the facial paralysis can be peripheral or central depending on the level of motor neuron lesion which can be below the nucleus of the nerve or supra nuclear. The various medical therapy used for facial paralysis are electroaccupunture, electro-therapy, laser acupuncture, manual acupuncture which is a traditional form of acupuncture. Imaging plays a great role in evaluation of degree of paralysis and also for faces recognition. There is a wide research in terms of facial expressions and facial recognition but lim-ited research work is available in facial paralysis. House- Brackmann Grading system is one of the simplest and easiest method to evalu-ate the degree of facial paralysis. During evaluation common facial expressions are recorded and are further evaluated by considering the focal points of the left or the right side of the face. This paper presents the classification of face recognition and its respective fuzzy rules to remove uncertainty in the result after evaluation of facial paralysis. {\textcopyright} 2018 Banita, Dr. Poonam Tanwar.},
annote = {cited By 0},
author = {Banita and Tanwar, P},
doi = {10.14419/ijet.v7i4.13619},
journal = {International Journal of Engineering and Technology(UAE)},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {4},
pages = {2325--2331},
title = {{Evaluation of 3d facial paralysis using fuzzy logic}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053901681{\&}doi=10.14419{\%}2Fijet.v7i4.13619{\&}partnerID=40{\&}md5=f68480ef298793b35ad707b17c64ab59},
volume = {7},
year = {2018}
}
@conference{Liu2016309,
abstract = {As the most distinct feature point in facial landmarks, nose tip plays a significant role in 3D facial studies. Successful detection of nose tip can facilitate many 3D facial studies tasks. In this paper, we propose a novel method to detect nose tip robustly. The method is robust to noise, need not training, can handle large rotations and occlusions. We first remove small isolated connected regions and noise from the input range image, then establish scale-space by robust smoothing the preprocessed range image. In each scale of the scale-space, we compute multi-angle energy of each point, then we use hierarchical clustering method to cluster the points whose multi-angle energies are larger than a threshold value. In the largest cluster, we can find one point with the largest multi-angle energy. For all scales of the scale-space, we get a series of such points and apply hierarchical clustering again for these points, nose tip will have the largest multi-angle energy in the largest cluster. We evaluate our method in FRGC v2.0 3D face database and BOSPHORUS 3D face database. The experimental results verify the robustness of our method with a high nose tip detection rate. {\textcopyright} 2015 IEEE.},
annote = {cited By 0},
author = {Liu, J and Zhang, Q and Tang, C},
booktitle = {Proceedings of 2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2015},
doi = {10.1109/IAEAC.2015.7428566},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {309--315},
title = {{CoMES: A novel method for robust nose tip detection in face range images}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966378274{\&}doi=10.1109{\%}2FIAEAC.2015.7428566{\&}partnerID=40{\&}md5=fa04fdce67df4cdba620b187f71d84a7},
year = {2016}
}
@conference{Ghiass201525,
abstract = {In this paper we describe a novel algorithm for head pose estimation from low-quality RGB-D data acquired using a consumer-level device such as Microsoft Kinect. We focus our attention on the wellknown challenges in the processing of depth point-clouds which include spurious data, noise, and missing data caused by occlusion. Our algorithm performs pose estimation by fitting a 3D morphable model which explicitly includes pose parameters. Several important novelties are described. (i) We propose a method for automatic removal of the majority of spurious depth data which uses facial feature detection in the associated RGB image. By back-projecting the corresponding image loci and intersecting them with the 3D point-cloud we construct the facial features plane used to crop the point-cloud. (ii) Both high convergence speed and high fitting accuracy are achieved by formulating the fitting objective function to include both point-to-point and point-to-plane point-cloud matching terms. (iii) The effect of misleading point-cloud matches caused by noisy or missing data is reduced by using the Tukey biweight function as a robust statistic and by employing a re-weighting scheme for different terms in the fitting objective function. (iv) Lastly, the proposed algorithm is evaluated on the standard benchmark Biwi Kinect Head Pose Database on which it is shown to outperform substantially the current state-of-the-art, achieving more than a 20-fold reduction in error estimates of all three Euler angles i.e. yaw, pitch, and roll. A thorough analysis of the results is used both to gain full insight into the behaviour of the described algorithm as well as to highlight important methodological issues which future authors should consider in the evaluation of pose estimation algorithms. {\textcopyright} 2015 ACM.},
annote = {cited By 14},
author = {Ghiass, R S and Arandjelovic, O and Laurendeau, D},
booktitle = {HCMC 2015 - Proceedings of the 2nd Workshop on Computational Models of Social Interactions: Human-Computer-Media Communication, co-located with ACM MM 2015},
doi = {10.1145/2810397.2810401},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {25--34},
title = {{Highly accurate and fully automatic head pose estimation from a low quality consumer-level RGB-D sensor}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964388743{\&}doi=10.1145{\%}2F2810397.2810401{\&}partnerID=40{\&}md5=4a1ae22bfe2e6f774ff7b6768f8410fc},
year = {2015}
}
@conference{Sachara2018959,
abstract = {In this contribution we present a novel approach to transform data from time-of-flight (ToF) sensors to be interpretable by Convolutional Neural Networks (CNNs). As ToF data tends to be overly noisy depending on various factors such as illumination, reflection coefficient and distance, the need for a robust algorithmic approach becomes evident. By spanning a three-dimensional grid of fixed size around each point cloud we are able to transform three-dimensional input to become processable by CNNs. This simple and effective neighborhood-preserving methodology demonstrates that CNNs are indeed able to extract the relevant information and learn a set of filters, enabling them to differentiate a complex set of ten different gestures obtained from 20 different individuals and containing 600.000 samples overall. Our 20-fold cross-validation shows the generalization performance of the network, achieving an accuracy of up to 98.5{\%} on validation sets comprising 20.000 data samples. The real-time applicability of our system is demonstrated via an interactive validation on an infotainment system running with up to 40fps on an iPad in the vehicle interior. {\textcopyright} 2017 IEEE.},
annote = {cited By 2},
author = {Sachara, F and Kopinski, T and Gepperth, A and Handmann, U},
booktitle = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
doi = {10.1109/ITSC.2017.8317684},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {959--964},
title = {{Free-hand gesture recognition with 3D-CNNs for in-car infotainment control in real-time}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046260257{\&}doi=10.1109{\%}2FITSC.2017.8317684{\&}partnerID=40{\&}md5=0733742868868fdbfb5b88aa0a45c1e7},
volume = {2018-March},
year = {2018}
}
@conference{Park2015239,
abstract = {We introduce a facial expression training system using the bilinear shape model which helps people to practice making a facial expression. The user face on the camera preview screen is reconstructed into a 3D face model and the model is transformed to a blend shape model which represents the facial expressions. This way, the system can precisely analyze the facial expression of users. With a target 3D face model appearing on the screen, the 3D face model changes its facial expression, which leads the user to change his facial expression to become look like same. The system recognizes whether the facial expression of the user is the same as the one of 3D face models. As the system gives the various missions to users to change his facial expression, they can practice facial expressions. It can be used for Bell's palsy patients who need face rehabilitation exercise or those who need to practice unique facial expressions such as stewardess smile or facial mimicry. {\textcopyright} 2015 ACM.},
annote = {cited By 0},
author = {Park, B.-H. and Oh, S.-Y.},
booktitle = {HAI 2015 - Proceedings of the 3rd International Conference on Human-Agent Interaction},
doi = {10.1145/2814940.2814985},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {239--241},
title = {{Facial Expression Training System using Bilinear Shape Model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962812240{\&}doi=10.1145{\%}2F2814940.2814985{\&}partnerID=40{\&}md5=b0a381250690a7c70eec5b9c77492523},
year = {2015}
}
@inproceedings{Berretti:2006:DRF:1178677.1178683,
address = {New York, NY, USA},
author = {Berretti, Stefano and {Del Bimbo}, Alberto and Pala, Pietro},
booktitle = {Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval},
doi = {10.1145/1178677.1178683},
isbn = {1-59593-495-2},
keywords = {3D face recognition,expression invariance,geodesic distance,revisao{\_}acm,spatial relationships,weighted walkthroughs},
mendeley-tags = {revisao{\_}acm},
pages = {13--22},
publisher = {ACM},
series = {MIR '06},
title = {{Description and Retrieval of 3D Face Models Using Iso-geodesic Stripes}},
url = {http://doi.acm.org/10.1145/1178677.1178683},
year = {2006}
}
@conference{Cheng2018555,
abstract = {In this paper, a contour map human facial recognition algorithm is proposed to implement the three-dimensional (3D) face recognition with the Kinect Xbox One. Since the scale of 3D depth data collected from Kinect is tremendous, the face recognition process cannot be handled in real time. To improve the speed and accuracy of the recognition process, the proposed algorithm turns the 3D depth data to the two-dimensional (2D) contour map. Furthermore, due to the 3D depth data obtained by Kinect, there is no need of expensive, ponderous and slow 3D scanners. Ten male and female subjects were involved in the validation experiment and the results verify that the proposed algorithm was feasible for face recognition. In addition, compared with other methods, Eigenface, Local Binary Patterns (LBP) and Linear Discriminant Analysis (LDA), the proposed algorithm has the better security and reliability. {\textcopyright} 2017 IEEE.},
annote = {cited By 0},
author = {Cheng, Z and Shi, T and Cui, W and Dong, Y and Fang, X},
booktitle = {2017 4th International Conference on Systems and Informatics, ICSAI 2017},
doi = {10.1109/ICSAI.2017.8248353},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {555--559},
title = {{3D face recognition based on kinect depth data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046681940{\&}doi=10.1109{\%}2FICSAI.2017.8248353{\&}partnerID=40{\&}md5=196c95096dd8ecb21cab2899f3232315},
volume = {2018-Janua},
year = {2018}
}
@article{Berssenbrügge201539,
abstract = {Objective: Facial symmetry is an important factor affecting esthetics. Thus, its restoration is an essential task in maxillofacial surgery. The aim of this study is to develop an objective measure of facial asymmetry by a novel approach where both the shape and the color are taken into account and to validate its correlation with perception. Methods: Optical three-dimensional (3D) face scans of 30 healthy adults are performed. Face-specific asymmetry indices are calculated by quantifying color differences as well as spatial distances between 3D data of a face and its mirrored copy. Subjective ratings of symmetry and attractiveness of the faces by 100 subjects are used to validate these indices. Results: The symmetry ratings show significant correlations with color and geometric asymmetry indices. The attractiveness ratings correlate only weakly with both indices. However, the product of the indices exhibits significant correlations with both attractiveness and symmetry ratings. Conclusion: The presented combined asymmetry index comprising shape and coloring turned out to reflect subjective perception of both facial symmetry and attractiveness. It thus promises to be a valid objective measure for facial esthetics, which could contribute, e.g., to the evaluation of surgical methods as well as to the design of craniofacial prostheses. {\textcopyright} 2015, WDG. All rights reserved.},
annote = {cited By 4},
author = {Berssenbr{\"{u}}gge, P and Lingemann-Koch, M and Abeler, A and Runte, C and Jung, S and Kleinheinz, J and Denz, C and Dirksen, D},
doi = {10.1515/bmt-2014-0024},
journal = {Biomedizinische Technik},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {1},
pages = {39--47},
title = {{Measuring facial symmetry: A perception-based approach using 3D shape and color}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925359029{\&}doi=10.1515{\%}2Fbmt-2014-0024{\&}partnerID=40{\&}md5=ce7851ea3409282a880c96db81f6d4da},
volume = {60},
year = {2015}
}
@article{Abdelmalik20151,
abstract = {Recognition of traditional optical faces of color images or intensity presents many challenges, such as variations in lighting, pose and expression. In fact, the human face not only generates 2D texture information, but also 3D shape information. In this paper, we examine what information the contributions of depth and color to make facial recognition when the variation in lighting and expression are taken into account. We present three methods of feature extraction based on reduction of one-dimensional space: the Linear Discriminant Analysis (LDA), Enhanced Fisher Linear Discriminant Model (EFM) and the Direct LDA (DLDA). A theoretical presentation of these approaches and their applications on the depth images and color is made. It is also a comparative study on information fusion of depth and color for both levels: characteristics and scores to select the most effective features and robust and thus build a strong classifier. The concatenation of feature vectors and fusion of the pixels of the image depth and color: the average, the product, the minimum and maximum are used in the case of fusion characteristics. For the merger to level scores, we used the fuzzy Sugeno integral and Choquet and support vector machines (SVM). The experiments are performed on the database CASIA 3D Face, complex data sets with variations, including variations in lighting, expression and longstanding failures between two scans. The experimental results show the promising performance of the proposed system. Note that in our system, all processes are performed automatically.},
annote = {cited By 0},
author = {Abdelmalik, O and M{\'{e}}barka, B and Abdelhamid, B and Mohamed, B and Abdelmalik, T A},
journal = {Journal of Electrical Engineering},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {3},
pages = {1--8},
title = {{Identification of faces by multimodal information fusion of depth and color}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952907399{\&}partnerID=40{\&}md5=3b48061d17a217a61cdbe2faa9d8fc66},
volume = {15},
year = {2015}
}
@article{Zhao2018207,
abstract = {3D face similarity is a critical issue in computer vision, computer graphics and face recognition and so on. Since Fr{\'{e}}chet distance is an effective metric for measuring curve similarity, a novel 3D face similarity measure method based on Fr{\'{e}}chet distances of geodesics is proposed in this paper. In our method, the surface similarity between two 3D faces is measured by the similarity between two sets of 3D curves on them. Due to the intrinsic property of geodesics, we select geodesics as the comparison curves. Firstly, the geodesics on each 3D facial model emanating from the nose tip point are extracted in the same initial direction with equal angular increment. Secondly, the Fr{\'{e}}chet distances between the two sets of geodesics on the two compared facial models are computed. At last, the similarity between the two facial models is computed based on the Fr{\'{e}}chet distances of the geodesics obtained in the second step. We verify our method both theoretically and practically. In theory, we prove that the similarity of our method satisfies three properties: reflexivity, symmetry, and triangle inequality. And in practice, experiments are conducted on the open 3D face database GavaDB, Texas 3D Face Recognition database, and our 3D face database. After the comparison with iso-geodesic and Hausdorff distance method, the results illustrate that our method has good discrimination ability and can not only identify the facial models of the same person, but also distinguish the facial models of any two different persons. {\textcopyright} 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
annote = {cited By 0},
author = {Zhao, J.-L. and Wu, Z.-K. and Pan, Z.-K. and Duan, F.-Q. and Li, J.-H. and Lv, Z.-H. and Wang, K and Chen, Y.-C.},
doi = {10.1007/s11390-018-1814-7},
journal = {Journal of Computer Science and Technology},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {1},
pages = {207--222},
title = {{3D Face Similarity Measure by Fr{\'{e}}chet Distances of Geodesics}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041342736{\&}doi=10.1007{\%}2Fs11390-018-1814-7{\&}partnerID=40{\&}md5=8478bdecdc0eeeb6cb4dda3bb1ecda52},
volume = {33},
year = {2018}
}
@article{8500093,
abstract = {The collection of large-scale three-dimensional (3-D) face models has led to significant progress in the field of 3-D face alignment “in-the-wild,” with several methods being proposed toward establishing sparse or dense 3-D correspondences between a given 2-D facial image and a 3-D face model. Utilizing 3-D face alignment improves 2-D face alignment in many ways, such as alleviating issues with artifacts and warping effects in texture images. However, the utilization of 3-D face models introduces a new set of challenges for researchers. Since facial images are commonly captured in arbitrary recording conditions, a considerable amount of missing information and gross outliers is observed (e.g., due to self-occlusion, subjects wearing eye-glasses, and so on). To this end, in this paper we propose the Multi-Attribute Robust Component Analysis (MA-RCA), a novel technique that is suitable for facial UV maps containing a considerable amount of missing information and outliers, while additionally, elegantly incorporates knowledge from various available attributes, such as age and identity. We evaluate the proposed method on problems such as UV denoising, UV completion, facial expression synthesis, and age progression, where MA-RCA outperforms compared techniques.},
author = {Moschoglou, S and Ververas, E and Panagakis, Y and Nicolaou, M A and Zafeiriou, S},
doi = {10.1109/JSTSP.2018.2877108},
issn = {1932-4553},
journal = {IEEE Journal of Selected Topics in Signal Processing},
keywords = {face recognition;image texture;pose estimation;mul,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
number = {6},
pages = {1324--1337},
title = {{Multi-Attribute Robust Component Analysis for Facial UV Maps}},
volume = {12},
year = {2018}
}
@inproceedings{8122755,
abstract = {Facial wrinkle is one of the most prominent biological changes that accompanying the natural aging process. However, there are some external factors contributing to premature wrinkles development, such as sun exposure and smoking. Clinical studies have shown that heavy smoking causes premature wrinkles development. However, there is no computerised system that can automatically assess the facial wrinkles on the whole face. This study investigates the effect of smoking on facial wrinkling using a social habit face dataset and an automated computerised computer vision algorithm. The wrinkles pattern represented in the intensity of 0-255 was first extracted using a modified Hybrid Hessian Filter. The face was divided into ten predefined regions, where the wrinkles in each region was extracted. Then the statistical analysis was performed to analyse which region is effected mainly by smoking. The result showed that the density of wrinkles for smokers in two regions around the mouth was significantly higher than the non-smokers, at p-value of 0.05. Other regions are inconclusive due to lack of large-scale dataset. Finally, the wrinkle was visually compared between smoker and non-smoker faces by generating a generic 3D face model.},
author = {Osman, O F and Elbashir, R M I and Abbass, I E and Kendrick, C and Goyal, M and Yap, M H},
booktitle = {2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
doi = {10.1109/SMC.2017.8122755},
keywords = {ageing;computer vision;face recognition;image filt,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1081--1086},
title = {{Automated assessment of facial wrinkling: A case study on the effect of smoking}},
year = {2017}
}
@conference{An2017265,
abstract = {Facial expression which carries rich information of body behavior is the leading carrier of human affective and the symbol of intelligence. The main purpose of this paper is to recognize 3D human facial expression. The research in this paper includes the expression feature extraction algorithm and fusion with different kinds of feature. To contain more local texture feature information, we proposed a new feature of 3D facial expression named Local Threshold Binary Pattern (LTBP) which based on Local Binary Pattern (LBP). We calculate the difference of gray value standard between neighboring pixels and the center pixel as a threshold to binary instead of the traditional LBP operation which only comparison of size between neighboring pixels and the center pixel. After we get the LTBP feature, we fuse the LTBP and HOG (Histogram of Oriented Gradient) features to get multi-feature fusion for 3D facial expression recognition. Our algorithm of 3D facial expression recognition comprises three steps: (1) extracting two sets of feature vectors and establishing the correlation criterion function between the two sets of feature vectors; (2) solving the two sets canonical projective vectors and extracting their canonical correlation features by the framework of canonical correlation analysis algorithm; (3) doing feature fusion for classification by using proposed strategy. We have performed comprehensive experiments on the BU-3DFE database which is presently the largest available 3D face database. We have achieved verification rates of more than 90{\%} for the 3D facial expression recognition. {\textcopyright} 2016 IEEE.},
annote = {cited By 3},
author = {An, S and Ruan, Q},
booktitle = {International Conference on Signal Processing Proceedings, ICSP},
doi = {10.1109/ICSP.2016.7877838},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {265--270},
title = {{3D facial expression recognition algorithm using local threshold binary pattern and histogram of oriented gradient}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016330947{\&}doi=10.1109{\%}2FICSP.2016.7877838{\&}partnerID=40{\&}md5=3bbf297b8f793d91031b7c648e9ddc88},
year = {2017}
}
@article{Moeini2015,
abstract = {A study for feature extraction is proposed to handle the problem of facial appearance changes including facial makeup and plastic surgery in face recognition. To extend a face recognition method robust to facial appearance changes, features are individually extracted from facial depth on which facial makeup and plastic surgery have no effect. Then facial depth features are added to facial texture features to perform feature extraction. Accordingly, a three-dimensional (3-D) face is reconstructed from only a single two-dimensional (2-D) frontal image in real-world scenarios. Then the facial depth is extracted from the reconstructed model. Afterward, the dual-tree complex wavelet transform (DT-CWT) is applied to both texture and reconstructed depth images to extract the feature vectors. Finally, the final feature vectors are generated by combining 2-D and 3-D feature vectors, and are then classified by adopting the support vector machine. Promising results have been achieved for makeup-invariant face recognition on two available image databases including YouTube makeup and virtual makeup, and plastic surgery-invariant face recognition on a plastic surgery face database is compared to several state-of-the-art feature extraction methods. Several real-world scenarios are also planned to evaluate the performance of the proposed method on a combination of these three databases with 1102 subjects. {\textcopyright} 2015 SPIE and IS{\&}T.},
annote = {cited By 4},
author = {Moeini, A and Faez, K and Moeini, H},
doi = {10.1117/1.JEI.24.5.053028},
journal = {Journal of Electronic Imaging},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {5},
title = {{Face recognition across makeup and plastic surgery from real-world images}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946137315{\&}doi=10.1117{\%}2F1.JEI.24.5.053028{\&}partnerID=40{\&}md5=eb4607189d3e9ec16ca8381fe3208d44},
volume = {24},
year = {2015}
}
@inproceedings{7785121,
abstract = {Fast and robust three-dimensional reconstruction of facial geometric structure from a single image is a challenging task with numerous applications. Here, we introduce a learning-based approach for reconstructing a three-dimensional face from a single image. Recent face recovery methods rely on accurate localization of key characteristic points. In contrast, the proposed approach is based on a Convolutional-Neural-Network (CNN) which extracts the face geometry directly from its image. Although such deep architectures outperform other models in complex computer vision problems, training them properly requires a large dataset of annotated examples. In the case of three-dimensional faces, currently, there are no large volume data sets, while acquiring such big-data is a tedious task. As an alternative, we propose to generate random, yet nearly photo-realistic, facial images for which the geometric form is known. The suggested model successfully recovers facial shapes from real images, even for faces with extreme expressions and under various lighting conditions.},
annote = {23/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
23/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Richardson, Elad and Sela, Matan and Kimmel, Ron},
booktitle = {2016 Fourth International Conference on 3D Vision (3DV)},
doi = {10.1109/3DV.2016.56},
isbn = {978-1-5090-5407-7},
keywords = {estela,etapa1,face recognition,id177,ieeexplore,image reconstruction,learning (ar,revisao{\_}ieeexplore},
mendeley-tags = {estela,etapa1,id177,ieeexplore,revisao{\_}ieeexplore},
month = {oct},
pages = {460--469},
publisher = {IEEE},
title = {{3D Face Reconstruction by Learning from Synthetic Data}},
url = {http://ieeexplore.ieee.org/document/7785121/},
year = {2016}
}
@article{Anbarjafari2019125,
abstract = {The use of virtual reality (VR) has been exponentially increasing and due to that many researchers have started to work on developing new VR based social media. For this purpose it is important to have an avatar of the user which look like them to be easily generated by the devices which are accessible, such as mobile phones. In this paper, we propose a novel method of recreating a 3D human face model captured with a phone camera image or video data. The method focuses more on model shape than texture in order to make the face recognizable. We detect 68 facial feature points and use them to separate a face into four regions. For each area the best fitting models are found and are further morphed combined to find the best fitting models for each area. These are then combined and further morphed in order to restore the original facial proportions. We also present a method of texturing the resulting model, where the aforementioned feature points are used to generate a texture for the resulting model. {\textcopyright} 2019 Polish Academy of Sciences. All rights reserved.},
annote = {cited By 0},
author = {Anbarjafari, G and Haamer, R E and L{\"{U}}Si, I and Tikk, T and Valgma, L},
doi = {10.24425/bpas.2019.127341},
journal = {Bulletin of the Polish Academy of Sciences: Technical Sciences},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {1},
pages = {125--132},
title = {{3D face reconstruction with region based best fit blending using mobile phone for virtual reality based social media}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063808749{\&}doi=10.24425{\%}2Fbpas.2019.127341{\&}partnerID=40{\&}md5=bb3ab46a3eef3258f21d34c06e624c89},
volume = {67},
year = {2019}
}
@article{Shu:2016:EEE:2996392.2926713,
address = {New York, NY, USA},
author = {Shu, Zhixin and Shechtman, Eli and Samaras, Dimitris and Hadap, Sunil},
doi = {10.1145/2926713},
issn = {0730-0301},
journal = {ACM Trans. Graph.},
keywords = {Face editing,computational aesthetics,eye editing,gaze editing,image compositing,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
number = {1},
pages = {1:1----1:13},
publisher = {ACM},
title = {{EyeOpener: Editing Eyes in the Wild}},
url = {http://doi.acm.org/10.1145/2926713},
volume = {36},
year = {2016}
}
@article{8376028,
abstract = {A low-power scalable 3-D face frontalization processor is proposed for accurate face recognition in mobile devices. In spite of recent improvement in face recognition accuracy mainly from convolutional neural networks (CNNs), their performance is limited to face images with frontal view. For face recognition with human-level accuracy in real-life environment, in which most of the face images are captured from arbitrary angles, 3-D face frontalization is essential as a preprocessing stage for CNN-based face recognition algorithms. The proposed face frontalization processor shows scalability in two aspects: image resolution and accuracy. For low-power consumption and scalability, the processor proposes three features: 1) scalable processing element (PE) architecture with workload adaptation; 2) accuracy scalable regression weight quantization to reduce the external memory access (EMA) down to 81.3{\%}; and 3) pipelined memory-level zero-skipping to further reduce the EMA by 98.4{\%} without any latency overhead. From the proposed EMA reduction features, the EMA is reduced by 99.7{\%} with little accuracy degradation in face frontalization results. The proposed face frontalization processor is implemented in 65-nm CMOS process, and it shows 4.73 frames/s) throughput. Moreover, power consumption of the implemented face frontalization processor is 0.53 mW, which is suitable for applications on mobile devices.},
author = {Kang, S and Lee, J and Bong, K and Kim, C and Kim, Y and Yoo, H},
doi = {10.1109/JETCAS.2018.2845663},
issn = {2156-3357},
journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
keywords = {CMOS integrated circuits;face recognition;image re,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
number = {4},
pages = {873--883},
title = {{Low-Power Scalable 3-D Face Frontalization Processor for CNN-Based Face Recognition in Mobile Devices}},
volume = {8},
year = {2018}
}
@inproceedings{7471958,
abstract = {In this paper, we propose a fast and robust method which uses only a single frontal face image as input to reconstruct a plausible 3D face. Our method mainly consists of three stages: feature point detection, model adaptation in X-Y plane and model adjustment on Z-axis direction. At first stage, we detect some face regions such as face contour and facial components automatically. In these regions, we extract several feature points which can generally describe the structure of face. Subsequently, we apply several deformation processes and optimization procedures on an adjustable 3D face model in the X-Y plane based on these feature points. Finally, we present a method of insertion to obtain a dense and smooth model. Experimental results demonstrate the effectiveness and efficiency of our method as well as the robust adaptation to the complex imaging condition.},
author = {Wu, T and Zhou, F and Liao, Q},
booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2016.7471958},
issn = {2379-190X},
keywords = {face recognition;image reconstruction;optimisation,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1656--1660},
title = {{A fast 3D face reconstruction method from a single image using adjustable model}},
year = {2016}
}
@inproceedings{8453724,
abstract = {This paper proposes a new approach for the detection of tree locations around forest machines producing a situational model based on on-site terrestrial LiDAR data collected during harvesting operation. A triangularized ground model is used to planarize the point cloud in order to simplify the tree detection. The planarized ground makes the vertical cutting of the point cloud systematical. Tree stem lines detected from individual trees at individual scan views are used to guide the final alignment into global coordinates. The setup is numerically efficient and does not rely on any positioning and orientation system (POS) based e.g. on an inertial measurement unit (IMU) or global navigation satellite system (GNSS) or wheel rotation counter on the autonomous vehicle.},
author = {Sihvo, S and Virjonen, P and Nevalainen, P and Heikkonen, J},
booktitle = {2018 Baltic Geodetic Congress (BGC Geomatics)},
doi = {10.1109/BGC-Geomatics.2018.00075},
keywords = {optical radar;satellite navigation;tree detection;,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {364--367},
title = {{Tree Detection around Forest Harvester Based on Onboard LiDAR Measurements}},
year = {2018}
}
@conference{Hong2018825,
abstract = {Real-world face recognition using a single sample per person (SSPP) is a challenging task. The problem is exacerbated if the conditions under which the gallery image and the probe set are captured are completely different. To address these issues from the perspective of domain adaptation, we introduce an SSPP domain adaptation network (SSPP-DAN). In the proposed approach, domain adaptation, feature extraction, and classification are performed jointly using a deep architecture with domain-adversarial training. However, the SSPP characteristic of one training sample per class is insufficient to train the deep architecture. To overcome this shortage, we generate synthetic images with varying poses using a 3D face model. Experimental evaluations using a realistic SSPP dataset show that deep domain adaptation and image synthesis complement each other and dramatically improve accuracy. Experiments on a benchmark dataset using the proposed approach show state-of-the-art performance. {\textcopyright} 2017 IEEE.},
annote = {cited By 0},
author = {Hong, S and Im, W and Ryu, J and Yang, H S},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2017.8296396},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {825--829},
title = {{SSPP-DAN: Deep domain adaptation network for face recognition with single sample per person}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045330406{\&}doi=10.1109{\%}2FICIP.2017.8296396{\&}partnerID=40{\&}md5=5f40daaac3230a175fe0c0171dc4b0ce},
volume = {2017-Septe},
year = {2018}
}
@conference{Varney2015,
abstract = {One of the most difficult challenges of working with LiDAR data is the large amount of data points that are produced. Analysing these large data sets is an extremely time consuming process. For this reason, automatic perception of LiDAR scenes is a growing area of research. Currently, most LiDAR feature extraction relies on geometrical features specific to the point cloud of interest. These geometrical features are scene-specific, and often rely on the scale and orientation of the object for classification. This paper proposes a robust method for reduced dimensionality feature extraction of 3D objects using a volume component analysis (VCA) approach.1 This VCA approach is based on principal component analysis (PCA). PCA is a method of reduced feature extraction that computes a covariance matrix from the original input vector. The eigenvectors corresponding to the largest eigenvalues of the covariance matrix are used to describe an image. Block-based PCA is an adapted method for feature extraction in facial images because PCA, when performed in local areas of the image, can extract more significant features than can be extracted when the entire image is considered. The image space is split into several of these blocks, and PCA is computed individually for each block. This VCA proposes that a LiDAR point cloud can be represented as a series of voxels whose values correspond to the point density within that relative location. From this voxelized space, block-based PCA is used to analyze sections of the space where the sections, when combined, will represent features of the entire 3-D object. These features are then used as the input to a support vector machine which is trained to identify four classes of objects, vegetation, vehicles, buildings and barriers with an overall accuracy of 93.8{\%}. {\textcopyright} 2015 SPIE.},
annote = {cited By 0},
author = {Varney, N M and Asari, V K},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2179268},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {January},
title = {{Volume component analysis for classification of LiDAR data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937231577{\&}doi=10.1117{\%}2F12.2179268{\&}partnerID=40{\&}md5=f90b0a608ec94d6fb687c0271123252b},
volume = {9477},
year = {2015}
}
@article{8458443,
abstract = {We propose a new approach for 3D reconstruction of dynamic indoor and outdoor scenes in everyday environments, leveraging only cameras worn by a user. This approach allows 3D reconstruction of experiences at any location and virtual tours from anywhere. The key innovation of the proposed ego-centric reconstruction system is to capture the wearer's body pose and facial expression from near-body views, e.g. cameras on the user's glasses, and to capture the surrounding environment using outward-facing views. The main challenge of the ego-centric reconstruction, however, is the poor coverage of the near-body views - that is, the user's body and face are observed from vantage points that are convenient for wear but inconvenient for capture. To overcome these challenges, we propose a parametric-model-based approach to user motion estimation. This approach utilizes convolutional neural networks (CNNs) for near-view body pose estimation, and we introduce a CNN-based approach for facial expression estimation that combines audio and video. For each time-point during capture, the intermediate model-based reconstructions from these systems are used to re-target a high-fidelity pre-scanned model of the user. We demonstrate that the proposed self-sufficient, head-worn capture system is capable of reconstructing the wearer's movements and their surrounding environment in both indoor and outdoor situations without any additional views. As a proof of concept, we show how the resulting 3D-plus-time reconstruction can be immersively experienced within a virtual reality system (e.g., the HTC Vive). We expect that the size of the proposed egocentric capture-and-reconstruction system will eventually be reduced to fit within future AR glasses, and will be widely useful for immersive 3D telepresence, virtual tours, and general use-anywhere 3D content creation.},
author = {Cha, Y and Price, T and Wei, Z and Lu, X and Rewkowski, N and Chabra, R and Qin, Z and Kim, H and Su, Z and Liu, Y and Ilie, A and State, A and Xu, Z and Frahm, J and Fuchs, H},
doi = {10.1109/TVCG.2018.2868527},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {cameras;computer vision;convolution;face recogniti,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
number = {11},
pages = {2993--3004},
title = {{Towards Fully Mobile 3D Face, Body, and Environment Capture Using Only Head-worn Cameras}},
volume = {24},
year = {2018}
}
@article{8103313,
abstract = {In this article, we present a novel approach to synthesize a new 3D face model using weighted blending of multiscale details across different faces including human (see Figure 1a) and nonhuman characters (see Figures 1b and 1c). Our approach decomposes face models into component scales, with a correspondence of salient facial features across faces. Specifically, we create a MFM that hierarchically represents the face's spatial details. A 3D face mesh is parameterized into 2D parameter space and decomposed into a base surface and multiscale continuous displacement maps (CDMs). Each MFM represents face details from coarse to fine scales while providing full correspondences across CDMs.},
author = {Yoon, S and Lewis, J and Rhee, T},
doi = {10.1109/MCG.2017.4031069},
issn = {0272-1716},
journal = {IEEE Computer Graphics and Applications},
keywords = {computer graphics;computer graphics;MFM;CDMs;multi,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
number = {6},
pages = {65--75},
title = {{Blending Face Details: Synthesizing a Face Using Multiscale Face Models}},
volume = {37},
year = {2017}
}
@article{Majeed2015373,
abstract = {In order to increase the ability to track face movements with large head rotations, a 3D shape model is used in the system. In this paper, we present a robust nose tip detection method in 3D facial image that handling facial expression and hair occlusion. The 3D face smoothed by weighted median filter, the holes are filled by linear interpolation during the re-sampling phase and the 3D Gaussian filter is used to remove noise. Since the database, mesh contains unimportant parts like neck, shoulder, clothes and hair that can also change the overall appearance of a face. We propose a 3D mask to cut the face and crop useful part, which helps us to achieve sufficient accuracy for noise detection. Nose localization is one of the most significant tasks of any facial classification system, compared to other facial landmarks. We proceed to detect the point N from the front image following the assumption that the relevant point has the highest value of the Z axis. In this framework, the face model is determined from a frontal 3D face image. In this experiment, the performance rate is improved from 90.1{\%} to 98.3{\%}. As indicated by the experimental result, the proposed binary mask with maximum intensity method provides a significant improvement in performance of nose tip detection, also it success in different facial expression. {\textcopyright} 2015 SERSC.},
annote = {cited By 3},
author = {Majeed, R and Beiji, Z and Hatem, H},
doi = {10.14257/ijmue.2015.10.5.35},
journal = {International Journal of Multimedia and Ubiquitous Engineering},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {5},
pages = {373--382},
title = {{Nose tip detection in 3D face image based on maximum intensity algorithm}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932107713{\&}doi=10.14257{\%}2Fijmue.2015.10.5.35{\&}partnerID=40{\&}md5=d426923498038e74777ea5086e124eec},
volume = {10},
year = {2015}
}
@article{Sibbing2017285,
abstract = {We introduce a new markerless 3D face tracking approach for 2D videos captured by a single consumer grade camera. Our approach takes detected 2D facial features as input and matches them with projections of 3D features of a deformable model to determine its pose and shape. To make the tracking and reconstruction more robust we add a smoothness prior for pose and deformation changes of the faces. Our major contribution lies in the formulation of the deformation prior which we derive from a large database of facial animations showing different (dynamic) facial expressions of a fairly large number of subjects. We split these animation sequences into snippets of fixed length which we use to predict the facial motion based on previous frames. In order to keep the deformation model compact and independent from the individual physiognomy, we represent it by deformation gradients (instead of vertex positions) and apply a principal component analysis in deformation gradient space to extract the major modes of facial deformation. Since the facial deformation is optimized during tracking, it is particularly easy to apply them to other physiognomies and thereby re-target the facial expressions. We demonstrate the effectiveness of our technique on a number of examples. {\textcopyright} 2017 The Authors Computer Graphics Forum {\textcopyright} 2017 The Eurographics Association and John Wiley {\&} Sons Ltd.},
annote = {cited By 1},
author = {Sibbing, D and Kobbelt, L},
doi = {10.1111/cgf.13080},
journal = {Computer Graphics Forum},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {8},
pages = {285--301},
title = {{Building a Large Database of Facial Movements for Deformation Model-Based 3D Face Tracking}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013662978{\&}doi=10.1111{\%}2Fcgf.13080{\&}partnerID=40{\&}md5=3bb9743928540ba3c26d8b5a8d7c828d},
volume = {36},
year = {2017}
}
@conference{Casas2015,
abstract = {Creating and animating a realistic 3D human face is an important task in computer graphics. The capability of capturing the 3D face of a human subject and reanimate it quickly will find many applications in games, training simulations, and interactive 3D graphics. We demonstrate a system to capture photorealistic 3D faces and generate the blendshape models automatically using only a single commodity RGB-D sensor. Our method can rapidly generate a set of expressive facial poses from a single depth sensor, such as a Microsoft Kinect version 1, and requires no artistic expertise in order to process those scans. The system takes only a matter of seconds to capture and produce a 3D facial pose and only requires a few minutes of processing time to transform it into a blendshape-compatible model. Our main contributions include an end-to-end pipeline for capturing and generating face blendshape models automatically, and a registration method that solves dense correspondences between two face scans by utilizing facial landmarks detection and optical flows. We demonstrate the effectiveness of the proposed method by capturing different human subjects and puppeteering their 3D faces in an animation system with real-time facial performance retargeting.},
annote = {cited By 0},
author = {Casas, D and Alexander, O and Feng, A W and Fyffe, G and Ichikari, R and Debevec, P and Wang, R and Suma, E and Shapiro, A},
booktitle = {ACM SIGGRAPH 2015 Talks, SIGGRAPH 2015},
doi = {10.1145/2775280.2792540},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Blendshapes from commodity RGB-D sensors}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957944664{\&}doi=10.1145{\%}2F2775280.2792540{\&}partnerID=40{\&}md5=a6145b3a3a02581e6511e8db8eed5d93},
year = {2015}
}
@inproceedings{7284882,
abstract = {Performing face recognition across 3D scans of different resolution is now attracting an increasing interest thanks to the introduction of a new generation of depth cameras, capable of acquiring color/depth images over time. However, these devices have still a much lower resolution than the 3D high-resolution scanners typically used for face recognition applications. If data are acquired without user cooperation, the problem is even more challenging and the gap of resolution between probe and gallery scans can yield to a severe loss in terms of recognition accuracy. Based on these premises, we propose a method to build a higher-resolution 3D face model from 3D data acquired by a low-resolution scanner. This face model is built using data acquired when a person passes in front of the scanner, following an uncooperative protocol. To perform non-rigid registration of point sets and account for deformation of the face during the acquisition process, the Coherent Point Drift (CPD) method is used. Registered 3D data are filtered through a variant of the lowess method to remove outliers and build the final face model. The proposed approach is evaluated in terms of accuracy of face reconstruction and face recognition.},
author = {Bondi, E and Pala, P and Berretti, S and {Del Bimbo}, A},
booktitle = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
doi = {10.1109/FG.2015.7284882},
keywords = {face recognition;image filtering;image reconstruct,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--6},
title = {{Reconstructing high-resolution face models from Kinect depth sequences acquired in uncooperative contexts}},
volume = {07},
year = {2015}
}
@inproceedings{7158332,
abstract = {Face Hallucination (FH) differs from generic single-image super-resolution (SR) algorithms in its specific domain of application. By exploiting the common structures of human faces, magnification of lower resolution images can be achieved. Despite the growing interest in recent years, considerably less attention is paid to a crucial step in FH -- registration of facial images. In this work, registration techniques employed in the literature are first summarized and the importance of using well-aligned training and test images is demonstrated. A novel method to inversely map the high-resolution (HR) 3D training texture to the low-resolution (LR) 2D test image in arbitrary poses is then presented, which prevents information loss in LR images and is thus beneficial to SR. The effectiveness of our 3D approach is evaluated on the Multi-PIE and the PUT face databases. Superior qualitative and quantitative FH results to the state-of-the-art methods in all tested poses prove the necessity of accurate registration in FH. The merit of 3D FH in generating super-resolved frontal faces is also verified, revealing 30{\%} improvement in face recognition over the 2D approach under 30° of yaw rotation on the Multi-PIE dataset.},
author = {Qu, C and Herrmann, C and Monari, E and Schuchert, T and Beyerer, J},
booktitle = {2015 12th Conference on Computer and Robot Vision},
doi = {10.1109/CRV.2015.26},
keywords = {face recognition;image registration;image resoluti,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {139--146},
title = {{3D vs. 2D: On the Importance of Registration for Hallucinating Faces Under Unconstrained Poses}},
year = {2015}
}
@conference{Xiong2015,
abstract = {Optical devices are always used to digitize complex objects to get their shapes in form of point clouds. The results have no semantic meaning about the objects, and tedious process is indispensable to segment the scanned data to get meanings. The reason for a person to perceive an object correctly is the usage of knowledge, so Bayesian inference is used to the goal. A probabilistic And-Or-Graph is used as a unified framework of representation, learning, and recognition for a large number of object categories, and a probabilistic model defined on this And-Or-Graph is learned from a relatively small training set per category. Given a set of 3D scanned data, the Bayesian inference constructs a most probable interpretation of the object, and a semantic segment is obtained from the part decomposition. Some examples are given to explain the method. {\textcopyright} Copyright 2015 SPIE.},
annote = {cited By 0},
author = {Xiong, H and Xu, J and Xu, C and Pan, M},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2202969},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Parsing optical scanned 3D data by Bayesian inference}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963628533{\&}doi=10.1117{\%}2F12.2202969{\&}partnerID=40{\&}md5=51c0402133ae343fb47f22d8c7db33e6},
volume = {9675},
year = {2015}
}
@inproceedings{8237768,
abstract = {We present a minimalists but effective neural network that computes dense facial correspondences in highly unconstrained RGB images. Our network learns a per-pixel flow and a matchability mask between 2D input photographs of a person and the projection of a textured 3D face model. To train such a network, we generate a massive dataset of synthetic faces with dense labels using renderings of a morphable face model with variations in pose, expressions, lighting, and occlusions. We found that a training refinement using real photographs is required to drastically improve the ability to handle real images. When combined with a facial detection and 3D face fitting step, we show that our approach outperforms the state-of-the-art face alignment methods in terms of accuracy and speed. By directly estimating dense correspondences, we do not rely on the full visibility of sparse facial landmarks and are not limited to the model space of regression-based approaches. We also assess our method on video frames and demonstrate successful per-frame processing under extreme pose variations, occlusions, and lighting conditions. Compared to existing 3D facial tracking techniques, our fitting does not rely on previous frames or frontal facial initialization and is robust to imperfect face detections.},
author = {Yu, R and Saito, S and Li, H and Ceylan, D and Li, H},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.506},
issn = {2380-7504},
keywords = {face recognition;image texture;learning (artificia,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {4733--4742},
title = {{Learning Dense Facial Correspondences in Unconstrained Images}},
year = {2017}
}
@inproceedings{8470411,
abstract = {In this paper, a novel technique has been introduced for 3D face recognition based on the modified local binary pattern extracted from a 3D range image. The new LBP technique is applied to shape index of 3D facial surface data. The novelty of this technique illustrates that a modified local binary pattern termed as Triangular Local Binary Pattern (TR-LBP), which gives new texture representation of 3D facial surface for improvement of facial texture classification performance compared to other variants of LBP. In this paper, authors have also described the TR-LBP technique by extending it on 2D intensity image of same subjects. Entropy-based feature extraction is used for feature vector creation. Further, KNN is used for calculating classification accuracy on two popular 3D face databases: Frav3D and Bosphorous. Here the classifications results are compared with other two existing LBP techniques applied to range images and shape index (SI) form of range image respectively.},
author = {Dutta, K and Bhattacharjee, D and Nasipuri, M},
booktitle = {2018 Fifth International Conference on Emerging Applications of Information Technology (EAIT)},
doi = {10.1109/EAIT.2018.8470411},
keywords = {entropy;face recognition;feature extraction;image,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
month = {jan},
pages = {1--4},
title = {{TR-LBP: A modified Local Binary Pattem-based technique for 3D face recognition}},
year = {2018}
}
@inproceedings{8035340,
abstract = {In order to better learn the distributions of 2D and 3D faces and the mapping between them with limited training samples, a new 3D face reconstruction method based on progressive cascade regression is proposed. Firstly, it learns the mapping between 2D and 3D facial landmarks to estimate the initial 3D facial landmarks with a coupled space learning method. Secondly, a deformed space is constructed with the difference between the estimated initial landmarks and the ground truth of training samples; and more accurate 3D facial landmarks are reconstructed by modifying the initial 3D ones with shape compensations which are calculated by minimizing an objective function. Finally, the realistic 3D faces are reconstructed by a method that is based on a simple sparse regulation and shape deformation. The results on BJUT 3D face database demonstrate the effectiveness of the proposed method. In addition, compared with some typical methods, the method can get better subjective and objective results, especially in details.},
author = {Han, L and Xiao, Q and Liang, X},
booktitle = {2017 International Conference on Computer, Information and Telecommunication Systems (CITS)},
doi = {10.1109/CITS.2017.8035340},
keywords = {face recognition;image reconstruction;learning (ar,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {297--301},
title = {{3D face reconstruction based on progressive cascade regression}},
year = {2017}
}
@inproceedings{7163165,
abstract = {Face alignment is the problem of automatically locating detailed facial landmarks across different subjects, illuminations, and viewpoints. Previous methods can be divided into two broad categories. 2D-based methods locate a relatively small number of 2D fiducial points in real time while 3D-based methods fit a high-resolution 3D model offline at a much higher computational cost.},
author = {Jeni, L A and Girard, J M and Cohn, J F and Kanade, T},
booktitle = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
doi = {10.1109/FG.2015.7163165},
keywords = {face recognition;image coding;image resolution;lig,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1},
title = {{Real-time dense 3D face alignment from 2D video with automatic facial action unit coding}},
volume = {1},
year = {2015}
}
@inproceedings{8397011,
abstract = {Recent research shows that the aging patterns deeply learned from large-scale data lead to significant performance improvement on age estimation. However, the insight about why and how deep learning models achieved superior performance is inadequate. In this paper, we propose to analyze, visualize and understand the deep aging patterns. We first train a series of convolutional neural networks for age estimation, and then illustrate the learning outcomes using feature maps, activation histograms, and deconvolution. We also develop a visualization method that can compare the facial appearance and track its changes at different ages through the mapping between 2D images and a 3D face template. Our framework provides an innovative way to understand human facial aging process from a machine perspective.},
author = {Chen, S and Dong, M and Le, J and Barbat, S},
booktitle = {2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
doi = {10.1109/MIPR.2018.00055},
keywords = {data visualisation;face recognition;feedforward ne,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {226--231},
title = {{Understanding Human Aging Patterns from a Machine Perspective}},
year = {2018}
}
@conference{Wagner2015,
abstract = {Persons with walking frames are often limited with respect to visual acuity and they are depending on assistance. To give them the possibility to move free and autonomous in their environment an intelligent assistance system is proposed which can be mounted on a walking frame is used to observe the scene in walking direction, and obstacles have to be detected. Especially, stairways represent a high risk of injury if a collapse occurs. For stairway recognition, an algorithm is proposed which estimates normal vectors by using a covariance matrix and this makes it possible to segment the point cloud data provided by the Kinect sensor. The calculation of surface normal vectors of these regions helps to detect ascending and descending stairways. {\textcopyright} 2015 IEEE.},
annote = {cited By 1},
author = {Wagner, D and Kalischewski, K and Velten, J and Kummert, A},
booktitle = {2015 IEEE 9th International Workshop on Multidimensional (nD) Systems, nDS 2015},
doi = {10.1109/NDS.2015.7332646},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Detection of ascending and descending stairways by surface normal vectors}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971393508{\&}doi=10.1109{\%}2FNDS.2015.7332646{\&}partnerID=40{\&}md5=09fdb02ff56f88bbedc27be033b1ac76},
year = {2015}
}
@inproceedings{7577570,
abstract = {Defining motion area on the face of 3D virtual character starts with the mapping of skeleton movement. Every animated character requires special handling based on the characteristics of the size and location of the bone to support producing facial expressions correctly. This process is often done specifically for each face model to be used. This research tried to use a marker-based motion capture data as a reference for the automation of generating clusters adaptively in the face of 3D characters. Each vertex which forming expression on the faces of the 3D models selected as centroids of cluster and representation a motion area whose numbers will correspond with the number of feature-point markers of motion capture data. Clustering process is done with the synthesis of modified nearest neighbor approach with the feature-point value. The results obtained were able to demonstrate a clustering process for generating motion area in a variety of 3D face model.},
author = {Gunanto, S G and Hariadi, M and Yuniarno, E M},
booktitle = {2016 4th International Conference on Cyber and IT Service Management},
doi = {10.1109/CITSM.2016.7577570},
keywords = {face recognition;image capture;image motion analys,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--5},
title = {{Feature-points nearest neighbor clustering on 3D face models}},
year = {2016}
}
@conference{Ganguly2016,
abstract = {In this paper, a new technique, i.e. decremental depth bunches have been presented where two facial discriminating mechanisms have also been implemented for recognizing the individuals. Notably, based on variations of the depth values, different bunches of face regions (i.e. The small components) are extracted by differentiating the depth information that eventually describes detailed facial surface information. Now, from each bunches, statistical attributes as well as Hough peaks are encountered to initiate two feature vectors for feature-based as well as a holistic mechanism for classification by K-NN and Cosine distance respectively. The proposed mechanism is explicitly dependent on facial depth information that have been accomplished in range face images. Therefore, authors have considered two databases, namely: Frav3D and Bosphorus, that contains laser as well as structured light 3D scanner based procured 3D face images respectively. {\textcopyright} 2015 IEEE.},
annote = {cited By 1},
author = {Ganguly, S and Bhattacharjee, D and Nasipuri, M},
booktitle = {IEEE Region 10 Annual International Conference, Proceedings/TENCON},
doi = {10.1109/TENCON.2015.7372755},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Decremental depth bunch based 3D face recognition from range image}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962159083{\&}doi=10.1109{\%}2FTENCON.2015.7372755{\&}partnerID=40{\&}md5=22e866da77e8176ab874f3433272a714},
volume = {2016-Janua},
year = {2016}
}
@article{Pribanić2019532,
abstract = {3D registration is a very active topic, spanning research areas such as computational geometry, computer graphics and pattern recognition. It aims to solve spatial transformation that aligns two point clouds. In this work we propose the use of a single direction sensor, such as an accelerometer or a magnetometer, commonly available on contemporary mobile platforms, such as tablets and smartphones. Both sensors have been heavily investigated earlier, but only for joint use with other sensors, such as gyroscopes and GPS. We show a time-efficient and accurate 3D registration method that takes advantage of only either an accelerometer or a magnetometer. We demonstrate a 3D reconstruction of individual point clouds and the proposed 3D registration method on a tablet equipped with an accelerometer or a magnetometer. However, we point out that the proposed method is not restricted to mobile platforms. Indeed, it can easily be applied in any 3D measurement system that is upgradable with some ubiquitous direction sensor, for example by adding a smartphone equipped with either an accelerometer or a magnetometer. We compare the proposed method against several state-of-the-art methods implemented in the open source Point Cloud Library (PCL). The proposed method outperforms the PCL methods tested, both in terms of processing time and accuracy. {\textcopyright} 2018 Elsevier Ltd},
annote = {cited By 0},
author = {Pribani{\'{c}}, T and Petkovi{\'{c}}, T and Đonli{\'{c}}, M},
doi = {10.1016/j.patcog.2018.12.008},
journal = {Pattern Recognition},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {532--546},
title = {{3D registration based on the direction sensor measurements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058502105{\&}doi=10.1016{\%}2Fj.patcog.2018.12.008{\&}partnerID=40{\&}md5=fb0b59a93a24cf968774e117763621f5},
volume = {88},
year = {2019}
}
@inproceedings{Braathen:2001:FST:971478.971515,
address = {New York, NY, USA},
author = {Braathen, B and Bartlett, M S and Littlewort, G and Movellan, J R},
booktitle = {Proceedings of the 2001 Workshop on Perceptive User Interfaces},
doi = {10.1145/971478.971515},
keywords = {FACS,expression recognition,particle filters,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {1--5},
publisher = {ACM},
series = {PUI '01},
title = {{First Steps Towards Automatic Recognition of Spontaneous Facial Action Units}},
url = {http://doi.acm.org/10.1145/971478.971515},
year = {2001}
}
@conference{Kim20172362,
abstract = {We aim to reconstruct an accurate neutral 3D face model from an RGB-D video in the presence of extreme expression changes. Since each depth frame, taken by a low-cost sensor, is noisy, point clouds from multiple frames can be registered and aggregated to build an accurate 3D model. However, direct aggregation of multiple data produces erroneous results in natural interaction (e.g., talking and showing expressions). We propose to analyze facial expression from an RGB frame and neutralize the corresponding 3D point cloud if needed. We first estimate the person's expression by fitting blendshape coefficients using 2D facial landmarks for each frame and calculate an expression deformity (expression score). With the estimated expression score, we determine whether an input face is neutral or non-neutral. If the face is non-neutral, we proceed to neutralize the expression of the 3D point cloud in that frame. To neutralize the 3D point cloud of a face, we deform our generic 3D face model by applying the estimated blendshape coefficients, find displacement vectors from the deformed generic face to a neutral generic face, and apply the displacement vectors to the input 3D point cloud. After preprocessing frames in a video, we rank frames based on the expression scores and register the ranked frames into a single 3D model. Our system produces a neutral 3D face model in the presence of extreme expression changes even when neutral faces do not exist in the video. {\textcopyright} 2016 IEEE.},
annote = {cited By 0},
author = {Kim, D and Choi, J and Leksut, J T and Medioni, G},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7899989},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {2362--2367},
title = {{Expression invariant 3D face modeling from an RGB-D video}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019102884{\&}doi=10.1109{\%}2FICPR.2016.7899989{\&}partnerID=40{\&}md5=cf697dab534c995d033201bd62a61657},
year = {2017}
}
@inproceedings{7852696,
abstract = {This paper presents a method for correcting the posture of 3D face with unknown position and posture based on the standard face pose. We define a standard posture of face which is not affected by the head gesture. Then the algorithms of principal component analysis and iterative closest point are used to achieve the preliminary adjustment of the target model. For the precise calibration, we use the method of depth values iterative searching to find the point of nose precisely, and propose an algorithm of searching along a straight line to find the ridge line of nose. Ultimately we determine the difference between the posture of the target model and standard posture to obtain accurate calculation of the target model posture.},
author = {Li, F and Lai, C and Jin, S and Peng, Y},
booktitle = {2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
doi = {10.1109/CISP-BMEI.2016.7852696},
keywords = {calibration;computer graphics;face recognition;pri,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {135--139},
title = {{Automatic calibration of 3D human faces}},
year = {2016}
}
@conference{Alqahtani2018,
abstract = {The content of this article explores the use of 3D face tracking systems by implementing of active stereo vision cameras to ascertain the position of a person's face. The various experiments conducted in-depth have produced both promising and satisfying results for images that have enabled examiners to determine the disparity between images. The paper also explores some of the various challenges researchers are facing with the implementation of algorithms to construct cloud-points in from stereo-based images. The reviewed recommendations suggest on better software components that would avail final 3D computational images or reconstructions that can easily be matched to the original. The tracking system modules address the challenges of the practical application of face tracking including pose illumination and occlusion. The content of the paper evaluates the putting into practice of multi-view or multiple stereo cameras enhance the field of view to improve the performance of a 3D tracking system. Face tracking using functional multi-view stereo camera systems can significantly solve the correspondence problem and the issue of comparing scenes or image points given that extra views reduced ambiguity in matching. {\textcopyright} 2018 IEEE.},
annote = {cited By 0},
author = {Alqahtani, F and Chandran, V and Banks, J},
booktitle = {1st International Conference on Computer Applications and Information Security, ICCAIS 2018},
doi = {10.1109/CAIS.2018.8441988},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{3D Face Tracking Using Stereo Camera}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053554349{\&}doi=10.1109{\%}2FCAIS.2018.8441988{\&}partnerID=40{\&}md5=bff2dd175e861cd18ea97ac45749ed96},
year = {2018}
}
@conference{Liu201779,
abstract = {In this paper, we describe a novel approach for creating a 3D face model from an image or images of a human face taken at a priori unknown poses using gender and ethnicity specific 3D generic face models. This process starts with a generic face model, which is morphed as images of the person become available using pre-selected point landmarks that are tessellated to form a high resolution triangular mesh. The 3D face synthesis allows for accurate pose estimation as well as face identification in 3D. The estimation of the unknown pose is achieved through the use of a Levenberg-Marquardt optimization process. Encouraging experimental results are obtained in controlled environment with high resolution images under ideal illumination condition, as well as for images taken in uncontrolled environment under arbitrary illumination with low resolution cameras. {\textcopyright} 2017.},
annote = {cited By 1},
author = {Liu, Z and Cohen, F},
booktitle = {Proceedings of the International Conferences on Computer Graphics, Visualization, Computer Vision and Image Processing 2017 and Big Data Analytics, Data Mining and Computational Intelligence 2017 - Part of the Multi Conference on Computer Science and Info},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {79--86},
title = {{3D face reconstruction from image(s) based on gender and ethnicity generic models}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040181568{\&}partnerID=40{\&}md5=beb7013a2442a44ce5162bcc0f5793fa},
year = {2017}
}
@conference{Xu2018,
abstract = {In this paper, an instance segmentation method for tree extraction from MLS data sets in urban scenes is developed. The proposed method utilizes a supervoxel structure to organize the point clouds, and then extracts the detrended geometric features from the local context of supervoxels. Combined with the detrended features of the local context, the Random Forest (RF) classifier will be adopted to obtain the initial semantic labeling results of trees from point clouds. Afterwards, a local context-based regularization is iteratively performed to achieve global optimum on a global graphical model, in order to spatially smoothing the semantic labeling results. Finally, a graph-based segmentation is conducted to separate individual trees according to the semantic labeling results. The use of supervoxel structure can preserve the geometric boundaries of objects in the scene, and compared with point-based solutions, the supervoxel-based method can largely decrease the number of basic elements during the processing. Besides, the introduction of supervoxel contexts can extract the local information of an object making the feature extraction more robust and representative. Detrended geometric features can get over the redundant and in-salient information in the local context, so that discriminative features are obtained. Benefiting from the regularization process, the spatial smoothing is obtained based on initial labeling results from classic classifications such as RF classification. As a result, misclassification errors are removed to a large degree and semantic labeling results are thus smoothed. Based on the constructed global graphical model during the spatially smoothing process, a graph-based segmentation is applied to partition the graphical model for the clustering the instances of trees. The experiments on two test datasets have shown promising results, with an accuracy of the semantic labeling of trees reaching around 0.9. The segmentation of trees using graph-based algorithm also show acceptable results, with trees having simple structures and sparse distributions correctly separated, but for those cramped trees with complex structures, the points are over- or under-segmented. {\textcopyright} 2018 IEEE.},
annote = {cited By 0},
author = {Xu, Y and Sun, Z and Hoegner, L and Stilla, U and Yao, W},
booktitle = {2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing, PRRS 2018},
doi = {10.1109/PRRS.2018.8486220},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Instance segmentation of trees in urban areas from MLS point clouds using supervoxel contexts and graph-based optimization}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056548636{\&}doi=10.1109{\%}2FPRRS.2018.8486220{\&}partnerID=40{\&}md5=a370a2800eee69885fbaa013f60c824a},
year = {2018}
}
@conference{Starke201595,
abstract = {In this paper, we present an efficient and adaptive method for detecting and tracking multiple persons while providing real-time capability and high robustness to outlier noise. Given an RGB-D image data sequence, our algorithm combines two independent approaches for person detection. First, a cluster-based segmentation and classification on RGB-D point clouds and second a face detection on RGB images, where each method itself is post-processed by spatio-temporal filtering for tracking and sensitivity purposes. Our analysis and experimental results prove that the combined approach performs significantly better than the individual solutions and greatly reduces the number of false positives in situations where one detector fails. {\textcopyright} 2015 IEEE.},
annote = {cited By 1},
author = {Starke, S and Hendrich, N and Bistry, H and Zhang, J},
booktitle = {IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},
doi = {10.1109/MFI.2015.7295752},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {95--101},
title = {{Fast and robust detection and tracking of multiple persons on RGB-D data fusing spatio-temporal information}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951861796{\&}doi=10.1109{\%}2FMFI.2015.7295752{\&}partnerID=40{\&}md5=28e5e4a059d76aa228a15fa375d4fcf9},
volume = {2015-Octob},
year = {2015}
}
@inproceedings{7976644,
abstract = {In this paper we propose a new technique to detect and extract any part that we need from a 3D face. The regions of interest of our approach are the eyes and the nose. Moreover, we are interested in the extraction of the salient landmarks in 3D face. Therefore, in this paper a new method is proposed to detect the nose tip and eye corners from a three-dimensional face range image. Our original technique allows fully automated processing, treating incomplete and noisy input data, and automatically rejecting non-facial areas. Besides, it is robust against holes in 3D image and insensitive to facial expressions. Besides, it is stable against translation and rotation of the face, and it is suitable for different resolutions of images. All the experiments have been performed on the GAVAB and FRAV 3D databases. After applying the proposed method to the 3D face, experimental results show that it is comparable to state-of-the-art methods in terms of its accuracy and flexibility.},
author = {Sghaier, S and Souani, C and Faeidh, H and Besbes, K},
booktitle = {2016 Global Summit on Computer Information Technology (GSCIT)},
doi = {10.1109/GSCIT.2016.17},
keywords = {face recognition;image segmentation;visual databas,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {27--31},
title = {{Novel Technique for 3D Face Segmentation and Landmarking}},
year = {2016}
}
@article{Deng20155509,
abstract = {In order to eliminate the impact of facial expressions and improve the efficiency of calculation, this paper proposes a novel expression-robust 3D face recognition algorithm using region-based feature fusion technique based on multiscale wavelet transformations. The discrete wavelet transformation is applied to extract frequency component features of geometric image based on the semi-rigid face region as well as the non-rigid face region in order to reduce the influence from the facial expression using the Coherent Point Drift non-rigid point set registration. The dimensionality reduction methods are utilized to promote the computational efficiency, and the experimental results show that our algorithm outperforms state-of-the-art methods based on FRGC v2.0. Copyright {\textcopyright} 2015 Binary Information Press.},
annote = {cited By 0},
author = {Deng, X and Da, F and Shao, H},
doi = {10.12733/jcis14953},
journal = {Journal of Computational Information Systems},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {15},
pages = {5509--5517},
title = {{Expression-robust 3D face recognition using region-based multiscale wavelet feature fusion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950271521{\&}doi=10.12733{\%}2Fjcis14953{\&}partnerID=40{\&}md5=da8d93edfc030808fb309b097e409e94},
volume = {11},
year = {2015}
}
@proceedings{Cucchiara:2011:2072572,
address = {New York, NY, USA},
annote = {433117},
isbn = {978-1-4503-0998-1},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
publisher = {ACM},
title = {{J-HGBU '11: Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding}},
year = {2011}
}
@inproceedings{8673465,
abstract = {In this research work, an automated 3D face expression generation technique is presented, which is extracted from real life video of face motion. The face expression is extracted from real human face using Covariance Matrix for detecting face marker points and Mahalanobis Distance to calculate the distance of each marker points within frames. The technique of tracking points on face uses markers placed on key positions of face muscles, then by getting its position from all frames of pre-recoded face video using the distance algorithm the movement of each face muscle is detected and measured. The face muscles are marked with particular tracking markers that are detected and tracked by the system. This tracking occurs by using color segmentation using Huffman Transform, where we detect color of points and track the location and distance of each tracker points. The original and translated position values of each marker points are obtained and recorded in text file in vector values. The tracked values will be transferred in a 3D Animation software like MAYA and applied on a pre-Rigged 3D model of Human face. The 3D face will be rigged using joints to emulate the face muscle behavior.},
author = {Sayed, M and Bhatti, Z and Ismaili, I A},
booktitle = {2019 2nd International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)},
doi = {10.1109/ICOMET.2019.8673465},
keywords = {computer animation;face recognition;image colour a,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
month = {jan},
pages = {1--5},
title = {{Proposed Model for Facial Animation using Covariance Matrix and Mahalanobis Distance Algorithms}},
year = {2019}
}
@inproceedings{8575249,
abstract = {We propose the Hierarchical Detection Network (HDN) for the detection of facial palsy syndrome. This can be the first deep-learning based approach for the facial palsy detection. The proposed HDN consists of three component networks, the first detects faces, the second detects the landmarks on the detected faces, and the third detects the local palsy regions. The first and the third component networks are built on the Darknet framework, but with fewer layers of convolutions for shorter processing speed. The second component network employs the latest 3D face alignment network for locating the landmarks. The first component network employs a Na × Na grid over the overall input image, while the third component network employs a Nb × Nb grid over each detected face, making the HDN capable of efficiently locating the affected palsy regions. As previous approaches were evaluated on proprietary databases, we have collected 32 videos from YouTube and made the first public database for facial palsy study. To enhance the robustness against expression variations, we include the CK+ facial expression database in the training and testing phases. We show that the HDN does not just detect the local palsy regions, but also captures the frequency of the intensity, enabling the video-to-description diagnosis of the syndrome. Experiments show that the proposed approach offers an accurate and efficient solution for facial palsy detection/diagnosis.},
author = {Hsu, G J and Huang, W and Kang, J},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2018.00100},
issn = {2160-7516},
keywords = {face recognition;learning (artificial intelligence,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {693--6936},
title = {{Hierarchical Network for Facial Palsy Detection}},
year = {2018}
}
@inproceedings{He:2009:AFT:1734605.1734636,
address = {New York, NY, USA},
author = {He, X C and Yuk, S C and Chow, K P and Wong, K.-Y. K and Chung, R H Y},
booktitle = {Proceedings of the First International Conference on Internet Multimedia Computing and Service},
doi = {10.1145/1734605.1734636},
isbn = {978-1-60558-840-7},
keywords = {face model,face reconstruction,pose estimation,revisao{\_}acm,texture mapping},
mendeley-tags = {revisao{\_}acm},
pages = {123--128},
publisher = {ACM},
series = {ICIMCS '09},
title = {{Automatic 3D Face Texture Mapping Framework from Single Image}},
url = {http://doi.acm.org/10.1145/1734605.1734636},
year = {2009}
}
@inproceedings{7803052,
abstract = {Inside facial animation works there is an animator that need to be skilled enough to produce detailed animation, so the facial animation can be smooth when doing facial expressions. Every animated character requires special handling based on the characteristics of the size and location of the bone. This process, where every face model need special handling were time consuming and tedious work. For that issue this research propose method for using motion capture marker data in 3D face model for automatically segment weight motion area based on the feature point. Marker data that came from motion capture of human model will be used to represent a centroid of vertex cluster that forming expressions in animated character. The data grouping process will be spherical coordinate result calculation between feature point and vertices using modified nearest neighbor algorithm. The result obtained in this research will show the weight motion area that generated automatically from the feature point based on nearest neighbor algorithm in a 3D face model.},
annote = {26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Caesar, Rio and Suyoto and Gunanto, Samuel Gandang},
booktitle = {2016 1st International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)},
doi = {10.1109/ICITISEE.2016.7803052},
isbn = {978-1-5090-1567-2},
keywords = {computer animation,estela,etapa1,face recognition,id280,ieeexplore,image capture,revisao{\_}ieeexplore},
mendeley-tags = {estela,etapa1,id280,ieeexplore,revisao{\_}ieeexplore},
month = {aug},
pages = {81--86},
publisher = {IEEE},
title = {{An automatic 3D face model segmentation for acquiring weight motion area}},
url = {http://ieeexplore.ieee.org/document/7803052/},
year = {2016}
}
@conference{Li20181,
abstract = {Accurate facial landmarks localization (FLL) plays an important role in face recognition, face tracking and 3D face reconstruction. It can be formulated as a regression problem, which outputs facial landmarks positions from the detected face image. Deep constitutional neural network (CNN) has achieved great success in vision tasks, but it is insignificant to use it directly. In this paper, instead of adopting CNN model straightforwardly, we combine different convolutional features with extreme machine learning (ELM) in a cascade framework to achieve accurate FLL. Specifically, we extract globally and spatially convolutional feature in the first stage for containing better localization property by training deep CNN, which takes the whole face region as input and concatenates lower layers with higher layers. Then, we extract locally and correlatedly convolutional feature in the following stages for preserving shape constraint by building multi-objective CNN, which inputs local patches centered at the current landmarks and concatenates independent subnetwork of each landmark together. Moreover, the regressor embedded in CNN is replaced by the robust ELM for accurate shape regression. Extensive experiments demonstrate that our method performs better in challenging datasets. {\textcopyright} 2017 IEEE.},
annote = {cited By 1},
author = {Li, H and Li, Y and Liu, W and Dong, H},
booktitle = {Proceedings of 4th International Conference on Behavioral, Economic, and Socio-Cultural Computing, BESC 2017},
doi = {10.1109/BESC.2017.8256378},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {1--6},
title = {{Coarse-to-fine facial landmarks localization based on convolutional feature}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050556634{\&}doi=10.1109{\%}2FBESC.2017.8256378{\&}partnerID=40{\&}md5=0c3f54df7232e018dff1fe3ee83b9068},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{7835606,
abstract = {Facial Aging (FA) process as a natural phenomenon has been of great interest to many researchers as well as for studies in the field of gerontology and private sector firms like airports and police departments. As opposed to previous studies which focused on predicting the facial appearance changes of the individuals, this paper details the backward aging trajectory of face images in 3 dimensional environment. This model is learned from a database of three dimensional facial images which is built by five individual age groups between 3 to 35 years old. 3D facial aging modeling is a complex process since it affects both the shape and texture of the face. We propose the Statistical Rejuvenation Model (SRM), a morphable face model to transform the given adult face to its youthful appearance by changing its shape and texture simultaneously.},
author = {{Majid Zadeh Heravi}, F and Eslahi, M and Farazdaghi, E and Nait-Ali, A},
booktitle = {2016 International Conference on Bio-engineering for Smart Technologies (BioSMART)},
doi = {10.1109/BIOSMART.2016.7835606},
keywords = {face recognition;image texture;solid modelling;sta,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--4},
title = {{A Morphable Model to simulate rejuvenation trajectory of 3D face images: Preliminary results}},
year = {2016}
}
@inproceedings{Fang:2018:RCB:3197768.3201576,
address = {New York, NY, USA},
author = {Fang, Qinyuan and Kyrarini, Maria and Ristic-Durrant, Danijela and Gr{\"{a}}ser, Axel},
booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
doi = {10.1145/3197768.3201576},
isbn = {978-1-4503-6390-7},
keywords = {3D Point Cloud,Assistive Robotics,Mouth Detection,RGB-D Camera,Robot Control,revisao{\_}acm,revisao{\_}scopus},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus},
pages = {391--396},
publisher = {ACM},
series = {PETRA '18},
title = {{RGB-D Camera Based 3D Human Mouth Detection and Tracking Towards Robotic Feeding Assistance}},
url = {http://doi.acm.org/10.1145/3197768.3201576},
year = {2018}
}
@inproceedings{Zhao:2014:OSB:2557977.2558030,
address = {New York, NY, USA},
author = {Zhao, Xinshuang and Naguib, Ahmed M and Lee, Sukhan},
booktitle = {Proceedings of the 8th International Conference on Ubiquitous Information Management and Communication},
doi = {10.1145/2557977.2558030},
isbn = {978-1-4503-2644-5},
keywords = {elderly care robot,gesture recognition,human robot interaction,octree segmentation,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {54:1----54:8},
publisher = {ACM},
series = {ICUIMC '14},
title = {{Octree Segmentation Based Calling Gesture Recognition for Elderly Care Robot}},
url = {http://doi.acm.org/10.1145/2557977.2558030},
year = {2014}
}
@article{Paul201563,
abstract = {This paper provides solution to the problem in identifying humans from their three dimensional facial characteristics. For this reason a standard 3D facial recognition system was built and used in this research work. The system which also consist of several other systems were divided into simpler form for proper analysis and better performances. The sub-system consist of: registration, representation of faces, extraction of discriminative features, and fusion of matchers. For each of the sub-system, this paper evaluates the state of the art methods, and also propose new and better ones. This research uses generic face model which speeds up the correspondence establishment process. In facial representation schemes, implementation of diverse range of approaches such as point clouds, curvature-based descriptors, and range images were implored. While various feature extraction methods were used to determine the discriminative facial features. An in-depth analysis of decision-level fusion algorithms was perform. In addition to the evaluation of baseline fusion methods, we propose to use two novel fusion schemes where the first one employs a confidence-aided combination approach, and the second one implements a two-level serial integration method. Recognition simulations performed on the 3DRMA and the FRGC databases show that: generic face template-based rigid registration of faces is better than the non-rigid variant, principal curvature directions and surface normals have better discriminative power, representing faces using local patch descriptors can both reduce the feature dimensionality and improve the identification rate, and confidence-assisted fusion rules and serial two-stage fusion schemes have a potential to improve the accuracy when compared to other decision-level fusion rules.},
annote = {cited By 0},
author = {Paul, O I and Lu, Y},
doi = {10.6329/CIEE.2015.2.04},
journal = {International Journal of Electrical Engineering},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {2},
pages = {63--72},
title = {{Three-dimensional (3D) facial recognition and prediction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948449021{\&}doi=10.6329{\%}2FCIEE.2015.2.04{\&}partnerID=40{\&}md5=bf2146992f03c3829486315d16f3e9a6},
volume = {22},
year = {2015}
}
@conference{Ganapathi-Subramanian2018672,
abstract = {Real-life man-made objects often exhibit strong and easily-identifiable structure, as a direct result of their design or their intended functionality. Structure typically appears in the form of individual parts and their arrangement. Knowing about object structure can be an important cue for object recognition and scene understanding - a key goal for various AR and robotics applications. However, commodity RGB-D sensors used in these scenarios only produce raw, unorganized point clouds, without structural information about the captured scene. Moreover, the generated data is commonly partial and susceptible to artifacts and noise, which makes inferring the structure of scanned objects challenging. In this paper, we organize large shape collections into parameterized shape templates to capture the underlying structure of the objects. The templates allow us to transfer the structural information onto new objects and incomplete scans. We employ a deep neural network that matches the partial scan with one of the shape templates, then match and fit it to complete and detailed models from the collection. This allows us to faithfully label its parts and to guide the reconstruction of the scanned object. We showcase the effectiveness of our method by comparing it to other state-of-the-art approaches. {\textcopyright} 2018 IEEE.},
annote = {cited By 0},
author = {Ganapathi-Subramanian, V and Diamanti, O and Pirk, S and Tang, C and Niessner, M and Guibas, L},
booktitle = {Proceedings - 2018 International Conference on 3D Vision, 3DV 2018},
doi = {10.1109/3DV.2018.00082},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {672--681},
title = {{Parsing geometry using structure-aware shape templates}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056790125{\&}doi=10.1109{\%}2F3DV.2018.00082{\&}partnerID=40{\&}md5=2cab7fd610063052d24a883601fd3067},
year = {2018}
}
@conference{Nguyen201856,
abstract = {The study deals with the problem of estimating the accuracy and stability of 3D face models obtained by a stereo pair. The problem of the conditionality of the fundamental matrix, which is a mathematical stereo pair model, is considered. We prove that small changes of stereo camera parameters result in small changes in the solution of the problem of reconstructing three-dimensional coordinates. Several types of three-dimensional reconstruction optimization problems that are based on quality criteria are formulated. The paper also considers the issues of determining an object orientation in a three-dimensional space by position lines. A 3D image coding system utilizing invariant moments is proposed and the theoretical sensitivity of 3D invariants to geometric distortions is investigated. These results are used to obtain scaling invariants. Designing and studying such models is the important step to solve the analysis problem and determine the proximity of images, which is therefore necessary for their clustering and recognition. Copyright {\textcopyright} 2018 for the individual papers by the papers' authors.},
annote = {cited By 0},
author = {Nguyen, D T and Khachumov, V M and Khachumov, M V and Salpagarov, S I and Yakovlev, K S},
booktitle = {CEUR Workshop Proceedings},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {56--64},
title = {{On the estimation of accuracy and stability of 3D face modeling using a pair of stereo cameras}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062424913{\&}partnerID=40{\&}md5=0817bd89dd1130fd1d2eb176bc68be47},
volume = {2236},
year = {2018}
}
@conference{Teizer2017157,
abstract = {Fatalities resulting from cave-in hazards during excavation work in the United States account for 48{\%} of the trench fatalities in construction every year per Occupational Safety and Health Administration (OSHA) data. Recent trends indicate that fatalities from trench and excavation hazards in the US are increasing. Often the experience of safety inspectors and/or the designated competent person (CP) for trenching and excavation is vital when assessing sloping measurements with approved engineering survey tools. The degree of accidents, however, allows the conclusion that proper assessment and/or protection of excavation sites is not performed sufficiently in the field, or safety coordinators and/or adequately trained CPs are not on hand when needed. While existing assessment processes and protection methods are reviewed for potential improvement, this paper proposes a new compliance assistance prototype that incorporates state-of-the-art technology. The proposed prototype creates: (1) mobile field data acquisition with lowcost photo cameras to capture point cloud surveys of the as-built conditions of trenches, and (2) computational data processing to automatically extract trench height, width, and slope values. Early results to field-realistic experiments promise useful applications of the developed prototype for safety coordinators or adequately trained CPs.},
annote = {cited By 2},
author = {Teizer, J and Gerson, A M and Hilfert, T and Perschewski, M and K{\"{o}}nig, M},
booktitle = {ISARC 2017 - Proceedings of the 34th International Symposium on Automation and Robotics in Construction},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {157--164},
title = {{Mobile point cloud assessment for trench Safety audits}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032334202{\&}partnerID=40{\&}md5=3512b30e5f5270b207d745c60731289f},
year = {2017}
}
@inproceedings{8373908,
abstract = {In the past years, many studies have highlighted the relation between deviations from normal facial morphology (dysmorphology) and some genetic and mental disorders. Recent advances in methods for reconstructing the 3D geometry of the face from 2D images opens new possibilities for dysmorphology research without the need for specialized 3D imaging equipment. However, it is unclear whether these methods could reconstruct the facial geometry with the required accuracy. In this paper we present a comparative study of some of the most relevant approaches for 3D face reconstruction from 2D images, including photometric-stereo, deep learning and 3D Morphable Model fitting. We address the comparison in qualitatively and quantitatively terms using a public database consisting of 2D images and 3D scans from 100 people. Interestingly, we find that some methods produce quite noisy reconstructions that do not seem realistic, whereas others look more natural. However, the latter do not seem to adequately capture the geometric variability that exists between different subjects and produce reconstructions that look always very similar across individuals, thus questioning their fidelity.},
author = {Morales, A and Piella, G and Mart{\'{i}}nez, O and Sukno, F M},
booktitle = {2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
doi = {10.1109/FG.2018.00115},
keywords = {face recognition;image reconstruction;stereo image,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {731--738},
title = {{A Quantitative Comparison of Methods for 3D Face Reconstruction from 2D Images}},
year = {2018}
}
@article{ElSayed2017393,
abstract = {Face detection has an essential role in many applications. In this paper, we propose an efficient and robust method for face detection on a 3D point cloud represented by a weighted graph. This method classifies graph vertices as skin and non-skin regions based on a data mining predictive model. Then, the saliency degree of vertices is computed to identify the possible candidate face features. Finally, the matching between non-skin regions representing eyes, mouth and eyebrows and salient regions is done by detecting collisions between polytopes, representing these two regions. This method extracts faces from situations where pose variation and change of expressions can be found. The robustness is showed through different experimental results. Moreover, we study the stability of our method according to noise. Furthermore, we show that our method deals with 2D images. {\textcopyright} 2017 The Royal Photographic Society.},
annote = {cited By 0},
author = {{El Sayed}, A R and {El Chakik}, A and Alabboud, H and Yassine, A},
doi = {10.1080/13682199.2017.1358528},
journal = {Imaging Science Journal},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {7},
pages = {393--408},
title = {{3D face detection based on salient features extraction and skin colour detection using data mining}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028655269{\&}doi=10.1080{\%}2F13682199.2017.1358528{\&}partnerID=40{\&}md5=e5e603adbb5733b9bb5cace2dfe18ffc},
volume = {65},
year = {2017}
}
@conference{Yin2017341,
abstract = {Face as a biometric identification in computer vision is an important medium, in areas such as video surveillance, animation games, security anti-terrorist has a very wide range of applications, creating vivid, strong visibility of 3d face model, now has become a challenging in the field of computer vision is one of the important topics. At first, this paper used the zhongxing-micro ZC301P cameras to build a binocular stereo vision system for recording images. After the camera calibration and binocular calibration, the three-dimensional data of facial images were extracted using the functions of OpenCV computer vision library, and then 3d face model were reconstructed preliminary by DirectX. According the reconstruction process, the human face three-dimensional reconstruction software was designed and developed. The paper laid the foundation for the next step work that is to obtain more clear and strong visibility of 3d face. {\textcopyright} 2016 IEEE.},
annote = {cited By 1},
author = {Yin, J and Yang, X},
booktitle = {ICALIP 2016 - 2016 International Conference on Audio, Language and Image Processing - Proceedings},
doi = {10.1109/ICALIP.2016.7846562},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {341--344},
title = {{3D facial reconstruction of based on OpenCV and DirectX}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016115763{\&}doi=10.1109{\%}2FICALIP.2016.7846562{\&}partnerID=40{\&}md5=54854eae7b0e30a6849a20e0cc2a1e0b},
year = {2017}
}
@conference{Li2018234,
abstract = {This paper presents a straight-forward yet efficient, and expression-robust 3D face recognition approach by exploring location sensitive sparse representation of deep normal patterns (DNP). In particular, given raw 3D facial surfaces, we first run 3D face pre-processing pipeline, including nose tip detection, face region cropping, and pose normalization. The 3D coordinates of each normalized 3D facial surface are then projected into 2D plane to generate geometry images, from which three images of facial surface normal components are estimated. Each normal image is then fed into a pre-trained deep face net to generate deep representations of facial surface normals, i.e., deep normal patterns. Considering the importance of different facial locations, we propose a location sensitive sparse representation classifier (LS-SRC) for similarity measure among deep normal patterns associated with different 3D faces. Finally, simple score-level fusion of different normal components are used for the final decision. The proposed approach achieves significantly high performance, and reporting rank-one scores of 98.01{\%}, 97.60{\%}, and 96.13{\%} on the FRGC v2.0, Bosphorus, and BU-3DFE databases when only one sample per subject is used in the gallery. These experimental results reveals that the performance of 3D face recognition would be constantly improved with the aid of training deep models from massive 2D face images, which opens the door for future directions of 3D face recognition. {\textcopyright} 2017 IEEE.},
annote = {cited By 1},
author = {Li, H and Sun, J and Chen, L},
booktitle = {IEEE International Joint Conference on Biometrics, IJCB 2017},
doi = {10.1109/BTAS.2017.8272703},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {234--242},
title = {{Location-sensitive sparse representation of deep normal patterns for expression-robust 3D face recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046291057{\&}doi=10.1109{\%}2FBTAS.2017.8272703{\&}partnerID=40{\&}md5=e5c4065442f8bc899597efdc443ed01c},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{8687254,
abstract = {Various applications have been developed in facial research to aid the recognition of personal attributes, analysis of race, and personal authentication for the security industry and other research fields. Thus, it is possible to identify the differences in facial shape based on place of birth or country. The present study analyzes facial shape using scanned 3D facial images and investigates ways to extract facial landmarks from the 3D facial images. The detection of the facial landmark requires the normalization of the facial scale and position among in the 3D image data to analyze the facial shape. Therefore, it is difficult to obtain accurate facial landmarks from 3D facial images. Our method decomposes the task into the following three parts: (a) conversion of data from the 3D facial image to a 2D image, (b) extraction of facial landmarks from the 3D image using Convolutional Neural Network (CNN), (c) inversion of the identified facial landmarks from 2D to 3D images. In experiments, we compared the accuracy of this model for facial landmark detection.},
author = {Terada, T and Chen, Y and Kimura, R},
booktitle = {2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
doi = {10.1109/FSKD.2018.8687254},
keywords = {component;landmarks detection;3d facial image;poin,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {390--393},
title = {{3D Facial Landmark Detection Using Deep Convolutional Neural Networks}},
year = {2018}
}
@article{Chun201615693,
abstract = {In this paper, we present a new 3D head pose estimating approach based on real-time facial feature tracking and facial feature recovering method which copes with the surrounding light variation and various occlusion. The major facial features are obtained by Haar-like feature detection along with AdaBoost learning from an input video image. The detected facial features are robustly tracked by optical flow with a template matching scheme which continuously compensates for losing track of the initially detected features in a sequence of input images. The head pose of an input face image can be obtained by evaluating 3D information of facial features from the detected 2D eye-points, nose and lip. From the experiments, the proposed method shows effectiveness in tracking and recovering facial features and produces reliable result in head pose estimation. {\textcopyright} 2014, Springer Science+Business Media New York.},
annote = {cited By 1},
author = {Chun, J and Kim, W},
doi = {10.1007/s11042-014-2356-9},
journal = {Multimedia Tools and Applications},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {23},
pages = {15693--15708},
title = {{3D face pose estimation by a robust real time tracking of facial features}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911898412{\&}doi=10.1007{\%}2Fs11042-014-2356-9{\&}partnerID=40{\&}md5=1ecee098802b2511153fd3c281923bbb},
volume = {75},
year = {2016}
}
@inproceedings{7780547,
abstract = {We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds {\textgreater} 107 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.},
author = {Hackel, T and Wegner, J D and Schindler, K},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.178},
issn = {1063-6919},
keywords = {edge detection;feature extraction;graph theory;ima,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1610--1618},
title = {{Contour Detection in Unstructured 3D Point Clouds}},
year = {2016}
}
@inproceedings{Zeng:2019:PDG:3324320.3324370,
address = {USA},
author = {Zeng, Yingjie and Nie, Lanshun},
booktitle = {Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks},
isbn = {978-0-9949886-3-8},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {254--255},
publisher = {Junction Publishing},
series = {EWSN ?19},
title = {{Poster: Deep Gait Recognition via Millimeter Wave}},
url = {http://dl.acm.org/citation.cfm?id=3324320.3324370},
year = {2019}
}
@article{Jeni201713,
abstract = {To enable real-time, person-independent 3D registration from 2D video, we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60°. From a single 2D image of a person's face, a dense 3D shape is registered in real time for each frame. The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression. The algorithm first estimates the location of a dense set of landmarks and their visibility, then reconstructs face shapes by fitting a part-based 3D model. Because no assumptions are required about illumination or surface properties, the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video. The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction, extension to multi-view reconstruction, temporal integration for videos and 3D head-pose estimation. Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video. The software is available online at http://zface.org. {\textcopyright} 2016 Elsevier B.V.},
annote = {cited By 14},
author = {Jeni, L A and Cohn, J F and Kanade, T},
doi = {10.1016/j.imavis.2016.05.009},
journal = {Image and Vision Computing},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {13--24},
title = {{Dense 3D face alignment from 2D video for real-time use}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006747280{\&}doi=10.1016{\%}2Fj.imavis.2016.05.009{\&}partnerID=40{\&}md5=e04eee2e6c2caac1063ee1fce1853635},
volume = {58},
year = {2017}
}
@article{Chen201929,
abstract = {Two-stage Cumulative Attribute (CA) regression has been found effective in regression problems of computer vision such as facial age and crowd density estimation. The first stage regression maps input features to cumulative attributes that encode correlations between target values. The previous works have dealt with single output regression. In this work, we propose cumulative attribute spaces for 2- and 3-output (multivariate) regression. We show how the original CA space can be generalized to multiple output by the Cartesian product (CartCA). However, for target spaces with more than two outputs the CartCA becomes computationally infeasible and therefore we propose an approximate solution - multi-view CA (MvCA) - where CartCA is applied to output pairs. We experimentally verify improved performance of the CartCA and MvCA spaces in 2D and 3D face pose estimation and three-output (RGB) illuminant estimation for color constancy. {\textcopyright} 2018},
annote = {cited By 0},
author = {Chen, K and Jia, K and Huttunen, H and Matas, J and K{\"{a}}m{\"{a}}r{\"{a}}inen, J.-K.},
doi = {10.1016/j.patcog.2018.10.015},
journal = {Pattern Recognition},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {29--37},
title = {{Cumulative attribute space regression for head pose estimation and color constancy}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054715565{\&}doi=10.1016{\%}2Fj.patcog.2018.10.015{\&}partnerID=40{\&}md5=41f23130fd14bf52f929166f78c162b4},
volume = {87},
year = {2019}
}
@article{NourbakhshKaashki201866,
abstract = {This research proposes a method for 3D face recognition in various conditions using 3D constrained local model (CLM-Z). In this method, a combination of 2D images (RGBs) and depth images (Ds) captured by Kinect has been used. After detecting the face and smoothing the depth image, CLM-Z model has been used to model and detect the important points of the face. These points are described using Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), and 3D Local Binary Patterns (3DLBP). Finally, each face is recognized by a Support Vector Machine (SVM). The challenging situations are changes of lighting, facial expression and head pose. The results on CurtinFaces and IIIT-D datasets demonstrate that the proposed method outperformed state-of-the-art methods under illumination, expression and pitch pose conditions and comparable results were obtained in other cases. Additionally, our proposed method is robust even when the training data has not been carefully collected. {\textcopyright} 2018 Elsevier Inc.},
annote = {cited By 0},
author = {{Nourbakhsh Kaashki}, N and Safabakhsh, R},
doi = {10.1016/j.jvcir.2018.02.003},
journal = {Journal of Visual Communication and Image Representation},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {66--85},
title = {{RGB-D face recognition under various conditions via 3D constrained local model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042322207{\&}doi=10.1016{\%}2Fj.jvcir.2018.02.003{\&}partnerID=40{\&}md5=2b91afeff8a19c987036cf8a30b610dc},
volume = {52},
year = {2018}
}
@inproceedings{8373857,
abstract = {Estimating the 3D facial landmarks from a 2D image remains a challenging problem. Even though state-of-the-art 2D alignment methods are able to predict accurate landmarks for semi-frontal faces, the majority of them fail to provide semantically consistent landmarks for profile faces. A de facto solution to this problem is through 3D face alignment that preserves correspondence across different poses. In this paper, we proposed a Cascade Multi-view Hourglass Model for 3D face alignment, where the first Hourglass model is explored to jointly predict semi-frontal and profile 2D facial landmarks, after removing spatial transformations, another Hourglass model is employed to estimate the 3D facial shapes. To improve the capacity without sacrificing the computational complexity, the original residual bottleneck block in the Hourglass model is replaced by a parallel, multi-scale inception-resnet block. Extensive experiments on two challenging 3D face alignment datasets, AFLW2000-3D and Menpo-3D, show the robustness of the proposed method under continuous pose changes.},
author = {Deng, J and Zhou, Y and Cheng, S and Zaferiou, S},
booktitle = {2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
doi = {10.1109/FG.2018.00064},
keywords = {face recognition;pose estimation;3D facial landmar,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {399--403},
title = {{Cascade Multi-View Hourglass Model for Robust 3D Face Alignment}},
year = {2018}
}
@inproceedings{7533097,
abstract = {Existing 3D lighting consistency based forensic methods have some practical problems. They usually require additional images and human labor to reconstruct the 3D face model for lighting estimation, and furthermore, they cannot deal with expressional faces effectively. These drawbacks make them unusable in many practical cases. In this paper, we propose a more practical 3D lighting based forensic method by incorporating a facial landmark based 3D morphable model to efficiently fit the face shape. We also introduce a residual error based algorithm to automatically exclude outliers in lighting estimation. Our proposed method is fully automatic and very efficient compared to previous ones. Also, it does not depend on additional images and has better performance for expressional faces. Experiments on a realistic face dataset with variational lighting conditions indicate the efficacy and superiority of our method.},
author = {Peng, B and Wang, W and Dong, J and Tan, T},
booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7533097},
issn = {2381-8549},
keywords = {face recognition;image forensics;image reconstruct,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {3932--3936},
title = {{Automatic detection of 3D lighting inconsistencies via a facial landmark based morphable model}},
year = {2016}
}
@inproceedings{7821896,
abstract = {Computer facial animation aims to create an animated character expression as natural as possible as well as human facial expressions. Using the data marker catches facial motion capture, will be determined the location of the feature points of 3D face models to follow the motion of the marker points of human faces. To overcome the morphological differences between the face of the source with the character's face, then applied with radial basis retargeting process mapping so that the character's face can still display the natural expression. Using the data marker 2D, Radial Basis Function (RBF) transformation was applied to determine the position of the feature points on the 3D face models. RBF space transformation has good ability in determining the appropriate facial motion marker points on a human face to the character's face. Motion that occurs in 3D face models is scaled according to the relative scale between the source and the target.},
author = {{and S. G. Gunanto}},
booktitle = {2016 6th International Annual Engineering Seminar (InAES)},
doi = {10.1109/INAES.2016.7821896},
keywords = {computer animation;face recognition;feature extrac,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--5},
title = {{2D to 3D space transformation for facial animation based on marker data}},
year = {2016}
}
@article{Andreu20163,
abstract = {In the recent years personal health monitoring systems have been gaining popularity, both as a result of the pull from the general population, keen to improve well-being and early detection of possibly serious health conditions and the push from the industry eager to translate the current significant progress in computer vision and machine learning into commercial products. One of such systems is the Wize Mirror, built as a result of the FP7 funded SEMEOTICONS (SEMEiotic Oriented Technology for Individuals CardiOmetabolic risk self-assessmeNt and Self-monitoring) project. The project aims to translate the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, multispectral images, and 3D scans of the face. The multisensory platform, being developed as the result of that project, in the form of a smart mirror, looks for signs related to cardio-metabolic risks. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. This paper is focused on the description of the part of that system, utilising computer vision and machine learning techniques to perform 3D morphological analysis of the face and recognition of psycho-somatic status both linked with cardio-metabolic risks. The paper describes the concepts, methods and the developed implementations as well as reports on the results obtained on both real and synthetic datasets. {\textcopyright} 2016 The Authors. Published by Elsevier Inc.},
annote = {cited By 10},
author = {Andreu, Y and Chiarugi, F and Colantonio, S and Giannakakis, G and Giorgi, D and Henriquez, P and Kazantzaki, E and Manousos, D and Marias, K and Matuszewski, B J and Pascali, M A and Pediaditis, M and Raccichini, G and Tsiknakis, M},
doi = {10.1016/j.cviu.2016.03.018},
journal = {Computer Vision and Image Understanding},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {3--22},
title = {{Wize Mirror-a smart, multisensory cardio-metabolic risk monitoring system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963813154{\&}doi=10.1016{\%}2Fj.cviu.2016.03.018{\&}partnerID=40{\&}md5=f5ad3b569781bf18f9c8b19b9fc87e1e},
volume = {148},
year = {2016}
}
@conference{Böer2015299,
abstract = {This paper presents the Discriminative Generalized Hough Transform (DGHT) as a technique to localize landmarks in 3D face scans. While the DGHT has been successfully used for the detection of landmarks in 2D and 3D images this work extends the framework to be used with triangle meshes for the first time. Instead of edge features and their respective gradient direction, the relative positions and orientations of the mesh faces are utilized to describe the geometric structures which are relevant for the detection of a specific landmark. Implementing a coarse-to-fine strategy at first a decimated version of the mesh is used to locate the global region of the point of interest, followed by more detailed localizations on higher resolution meshes. The utilized shape models are created in an automated, discriminative training process which assigns individual weights to the single model points, aiming at an increased localization rate. The technique has been applied to detect 38 anthropometric facial landmarks on 99 3D face scans. With an average error of 1.9mm, the most accurate detection was performed for the right alare, the average error when considering all landmarks amounts to 5.1 mm. {\textcopyright} Springer-Verlag Berlin Heidelberg 2015.},
annote = {cited By 2},
author = {B{\"{o}}er, G and Hahmann, F and Buhr, I and Essig, H and Schramm, H},
booktitle = {Informatik aktuell},
doi = {10.1007/978-3-662-46224-9_52},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {299--304},
title = {{Detection of facial landmarks in 3D face scans using the discriminative generalized hough transform (DGHT)}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012215360{\&}doi=10.1007{\%}2F978-3-662-46224-9{\_}52{\&}partnerID=40{\&}md5=3b2ad1354317200c24de231f3f2bfcf5},
year = {2015}
}
@inproceedings{8695341,
abstract = {3D face reconstruction is a long-term and challenging problem, which has wide application scenarios, such as animation, recognition. Inspired by recent works in face landmark marching, we develop a method for 3D face reconstruction using a novel landmark updating optimization strategy. Our method is also based on minimization of the landmark projection error for calculation speed. To achieve more accuracy, contour landmarks and self-occluded landmarks need to be updated. For contour landmarks, the detected landmarks on 2D image are to updated by ?nding a nearest position on contour curve. For self-occluded landmarks, we render the model onto image plane, extract the edge of projected area, and generate new correspondence landmarks according to the edge pixels. We test our method quantitatively with ground truth data, and provide qualitative example reconstructions, show that our method performs better with experiments on MICC dataset.},
author = {Liu, P and Yu, Y and Zhou, Y and Du, S},
booktitle = {2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
doi = {10.1109/MIPR.2019.00082},
keywords = {Face;Three-dimensional displays;Solid modeling;Sha,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {403--408},
title = {{Single View 3D Face Reconstruction with Landmark Updating}},
year = {2019}
}
@inproceedings{8546220,
abstract = {3D face shape is more expressive and viewpoint-consistent than its 2D counterpart. However, 3D facial landmark localization in a single image is challenging due to the ambiguous nature of landmarks under 3D perspective. Existing approaches typically adopt a suboptimal two-step strategy, performing 2D landmark localization followed by depth estimation. In this paper, we propose the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial landmark localization, addressing it more effectively in an end-to-end fashion. First, a compact volumetric representation is proposed to encode the per-voxel likelihood of positions being the 3D landmarks. The dimensionality of such a representation is fixed regardless of the number of target landmarks, so that the curse of dimensionality could be avoided. Then, a stacked hourglass network is adopted to estimate the volumetric representation from coarse to fine, followed by a 3D convolution network that takes the estimated volume as input and regresses 3D coordinates of the face shape. In this way, the 3D structural constraints between landmarks could be learned by the neural network in a more efficient manner. Moreover, the proposed pipeline enables end-to-end training and improves the robustness and accuracy of 3D facial landmark localization. The effectiveness of our approach is validated on the 3DFAW and AFLW2000-3D datasets. Experimental results show that the proposed method achieves state-of-the-art performance in comparison with existing methods.},
author = {Zhang, H and Li, Q and Sun, Z},
booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2018.8546220},
issn = {1051-4651},
keywords = {face recognition;image representation;learning (ar,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {2202--2208},
title = {{Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark Localization}},
year = {2018}
}
@conference{Hu2016113,
abstract = {Video surveillance has been applied in more and more fields for security in last decade years, video-based face recognition therefore became an important task of an intelligent monitoring system. However, among these captured video faces there are many non-frontal faces. As a result, the state-of-art face algorithms would become worse when they were employed to recognize video faces. On the other hand, it was a common phenomenon especially at video monitoring field that only one training sample per person is gained from their identification card. The single sample per person (SSPP) results in effecting even not taking advantage of some fine algorithms such LDA. In order to effectively improve the correct recognition rate of multi-pose face recognition with a single frontal training sample, this paper proposed a face recognition algorithm based on 3D modeling. In the proposed algorithm, firstly a 2D frontal face with high-resolution was taken to build a 3D face model, and then several virtual faces with different poses were produced from the 3D face model. At last, both the original frontal face image and virtual face images were put into a gallery set. The algorithm was evaluated on SCface database using traditional PCA and LDA methods. The result showed that the proposed approach could effectively improve video face recognition rate and the correct recognition rate went up about 13{\%} by LDA compared with traditional PCA. Therefore, the method that was proposed to create virtual looking down training samples was an effective algorithm and could be considered to apply in intelligent video monitoring system. {\textcopyright} 2015 IEEE.},
annote = {cited By 3},
author = {Hu, X and Liao, Q and Peng, S},
booktitle = {Proceedings - International Conference on Natural Computation},
doi = {10.1109/ICNC.2015.7377975},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {113--117},
title = {{Video surveillance face recognition by more virtual training samples based on 3D modeling}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960408687{\&}doi=10.1109{\%}2FICNC.2015.7377975{\&}partnerID=40{\&}md5=85ab8b4d6d0540e55a76c0b2e6c9ba7e},
volume = {2016-Janua},
year = {2016}
}
@inproceedings{8578865,
abstract = {As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.},
author = {Tran, L and Liu, X},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00767},
issn = {2575-7075},
keywords = {decoding;image coding;image reconstruction;image r,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {7346--7355},
title = {{Nonlinear 3D Face Morphable Model}},
year = {2018}
}
@conference{Leo2018619,
abstract = {The main objective of expression-invariant 3D face recognition system is to recognize 3D human faces even under various expressions. This paper focuses on such face recognition system using an efficient combination of 3D Principal Component Analysis (PCA) and Support Vector Machine (SVM). In the proposed method, each face is registered initially using the novel Mean Landmark Points (MLPs) based registration which facilitates the accurate extraction of distinct features from facial region using 3D PCA. SVM based classification is then done on the extracted features and it is found that the recognition rate is improved considerably by carefully selecting the training dataset. Experimental results reported on Bosphorus 3D face database prove that the proposed approach achieves the rank-1 recognition rate of 96.29{\%} on near frontal 3D faces comprising of rich facial expressions. {\textcopyright} 2018 The Authors. Published by Elsevier B.V.},
annote = {cited By 0},
author = {Leo, M J and Suchitra, S},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2018.10.441},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {619--625},
title = {{SVM based expression-invariant 3D face recognition system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058271138{\&}doi=10.1016{\%}2Fj.procs.2018.10.441{\&}partnerID=40{\&}md5=4a50565188c5152b1c5f2e68a4a95066},
volume = {143},
year = {2018}
}
@inproceedings{Donate:2010:FEU:1877791.1877799,
address = {New York, NY, USA},
author = {Donate, Arturo and Liu, Xiuwen},
booktitle = {Proceedings of the 1st International Workshop on 3D Video Processing},
doi = {10.1145/1877791.1877799},
isbn = {978-1-4503-0159-6},
keywords = {3D reconstruction,computer vision,mapping,revisao{\_}acm,structure from motion,tracking,video processing},
mendeley-tags = {revisao{\_}acm},
pages = {31--36},
publisher = {ACM},
series = {3DVP '10},
title = {{3D Feature Extraction from Uncalibrated Video Clips}},
url = {http://doi.acm.org/10.1145/1877791.1877799},
year = {2010}
}
@inproceedings{8296673,
abstract = {In this paper, we present a fast and robust method to reconstruct a plausible three-dimension (3D) face from one single frontal face image. In training phase, we classify the faces into several groups based on the facial structures and propose to learn a mapping, known as the displacement mapping (DM) in this paper, for each group. DM relates two displacements: One displacements, denoted as 2D displacements, represent the differences between the positions of feature points on the 2D training faces and those on the reference 2D face that has been pre-defined for the corresponding group; another displacements, denoted as 3D displacements, are the differences between the positions of vertices on the reconstructed 3D face and those on the reference 3D face that is also pre-defined. During the reconstruction phase, we first classify the input face as one of the groups and calculate the 2D displacements. Then we take advantage of the 2D displacements and the learned DM to estimate the 3D displacements. Subsequently, 3D displacements can be used to obtain the precise 3D face by shifting the 3D reference face. Experiments on Basel face model (BFM) database as well as some real-world 2D face images demonstrate the effectiveness and efficiency of the proposed method, in comparison with some state-of-arts methods.},
author = {Wu, T and Zhou, F and Liao, Q},
booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2017.8296673},
issn = {2381-8549},
keywords = {face recognition;image reconstruction;facial struc,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {2204--2208},
title = {{Real-time 3D face reconstruction from one single image by displacement mapping}},
year = {2017}
}
@article{Wenhao2016333,
abstract = {This paper seeks to compare encoded features from both two-dimensional (2D) and three-dimensional (3D) face images in order to achieve automatic gender recognition with high accuracy and robustness. The Fisher vector encoding method is employed to produce 2D, 3D, and fused features with escalated discriminative power. For 3D face analysis, a two-source photometric stereo (PS) method is introduced that enables 3D surface reconstructions with accurate details as well as desirable efficiency. Moreover, a 2D + 3D imaging device, taking the two-source PS method as its core, has been developed that can simultaneously gather color images for 2D evaluations and PS images for 3D analysis. This system inherits the superior reconstruction accuracy from the standard (three or more light) PS method but simplifies the reconstruction algorithm as well as the hardware design by only requiring two light sources. It also offers great potential for facilitating human computer interaction by being accurate, cheap, efficient, and nonintrusive. Ten types of low-level 2D and 3D features have been experimented with and encoded for Fisher vector gender recognition. Evaluations of the Fisher vector encoding method have been performed on the FERET database, Color FERET database, LFW database, and FRGCv2 database, yielding 97.7{\%}, 98.0{\%}, 92.5{\%}, and 96.7{\%} accuracy, respectively. In addition, the comparison of 2D and 3D features has been drawn from a self-collected dataset, which is constructed with the aid of the 2D + 3D imaging device in a series of data capture experiments. With a variety of experiments and evaluations, it can be proved that the Fisher vector encoding method outperforms most state-of-the-art gender recognition methods. It has also been observed that 3D features reconstructed by the two-source PS method are able to further boost the Fisher vector gender recognition performance, i.e., up to a 6{\%} increase on the self-collected database. {\textcopyright} 2016 Optical Society of America.},
annote = {cited By 3},
author = {Wenhao, Z and Smith, M L and Smith, L N and Farooq, A},
doi = {10.1364/JOSAA.33.000333},
journal = {Journal of the Optical Society of America A: Optics and Image Science, and Vision},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {3},
pages = {333--344},
title = {{Gender recognition from facial images: Two or three dimensions?}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962138813{\&}doi=10.1364{\%}2FJOSAA.33.000333{\&}partnerID=40{\&}md5=3a8ae19c72c975c3cb011d23ca12d2f4},
volume = {33},
year = {2016}
}
@inproceedings{Ballihi:2011:SCN:2381170.2381189,
address = {Aire-la-Ville, Switzerland, Switzerland},
author = {Ballihi, L and Amor, B Ben and Daoudi, M and Srivastava, A and Aboutajdine, D},
booktitle = {Proceedings of the 4th Eurographics Conference on 3D Object Retrieval},
doi = {10.2312/3DOR/3DOR11/101-104},
isbn = {978-3-905674-31-6},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {101--104},
publisher = {Eurographics Association},
series = {3DOR '11},
title = {{Selecting 3D Curves on the Nasal Surface Using AdaBoost for Person Authentication}},
url = {http://dx.doi.org/10.2312/3DOR/3DOR11/101-104},
year = {2011}
}
@conference{Nowak2016389,
abstract = {The aim of this paper is to present the data acquisition system built for the Multimodal Biometric System for Contactless Persons Identification. The system is capable of capturing 3D scans of face and hand. The hardware and software architecture of the acquisition system designed by the authors is described. The authors describe in detail the system calibration algorithm and present sample results. {\textcopyright} 2016 Department of Microelectronics and Computer Science, Lodz University of Technology.},
annote = {cited By 0},
author = {Nowak, P S and Sankowski, W and Krotewicz, P},
booktitle = {Proceedings of the 23rd International Conference Mixed Design of Integrated Circuits and Systems, MIXDES 2016},
doi = {10.1109/MIXDES.2016.7529772},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {389--393},
title = {{3D face and hand scans acquisition system dedicated for Multimodal Biometric identification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992109556{\&}doi=10.1109{\%}2FMIXDES.2016.7529772{\&}partnerID=40{\&}md5=1e34ae62f025425d3f5bcf30c8b9a4ff},
year = {2016}
}
@inproceedings{7780824,
abstract = {Given a collection of "in-the-wild" face images captured under a variety of unknown pose, expression, and illumination conditions, this paper presents a method for reconstructing a 3D face surface model of an individual along with albedo information. Motivated by the success of recent face reconstruction techniques on large photo collections, we extend prior work to adapt to low quality photo collections with fewer images. We achieve this by fitting a 3D Morphable Model to form a personalized template and developing a novel photometric stereo formulation, under a coarse-to-fine scheme. Superior experimental results are reported on synthetic and real-world photo collections.},
author = {Roth, J and Tong, Y and Liu, X},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.455},
issn = {1063-6919},
keywords = {face recognition;image capture;image reconstructio,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {4197--4206},
title = {{Adaptive 3D Face Reconstruction from Unconstrained Photo Collections}},
year = {2016}
}
@conference{Ganguly2016275,
abstract = {Face and facial attributes represent meaningful definition about a variety of information to discriminate an individual from others and for developing a computational model for automatic face recognition purpose. However, in this work, selection of relevant features from newly created face space is the pivotal contribution of the authors. Here, authors have demonstrated a new face space 'Complement Component' that have been used to extract the four basic components along X, and Y axes in four directions. Later, authors have experimented the discriminative attributes from these face spaces for recognition purpose. Here, comparison of the proposed method has been reported by examining its success on two well accepted 3D face databases, namely: Frav3D and Texas3D. In case of 2D face images, it does not contain depth like information i.e. Z-values in X-Y plane through intensity values. Therefore, it has not been undertaken during this investigation. {\textcopyright} 2015 IEEE.},
annote = {cited By 1},
author = {Ganguly, S and Bhattachaijee, D and Nasipuri, M},
booktitle = {2015 IEEE International Conference on Computer Graphics, Vision and Information Security, CGVIS 2015},
doi = {10.1109/CGVIS.2015.7449936},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {275--278},
title = {{3D face recognition from complement component range face images}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966632732{\&}doi=10.1109{\%}2FCGVIS.2015.7449936{\&}partnerID=40{\&}md5=ac5698f1d98fa8b29941ffc837f84a7b},
year = {2016}
}
@article{Rajakumari2018665,
abstract = {The Face Recognition has been an active research area since the 1960 and is one of the most successful applications of image analysis and understanding. Face Recognition is an important part of many biometric, security and surveillance system as well as image and video indexing systems. Face Recognition is used to identify or verify a person from still image and video. 2D pictures are for the most part less demanding and more affordable to secure. The apparent advantages from utilizing 3D in respect to 2D information incorporate less variety saw because of elements, for example, cosmetics and less affectability to enlightenment changes. Essentially there are two principle purposes behind such a pattern (a) the first is extensive variety of business and law requirement applications (b) the accessibility of doable innovations. Many research conducted by 2D based face recognition techniques and their problems like increasing gallery size, illuminate variation, expression variation and pose variation have well known. My research with the help of PCA and HAAR (Wavelet Transform) and it will be effective for deciding an image or video. Here we displays a few tests keeping in mind the end goal to indicated light, posture and structure of the image. {\textcopyright} TJPRC Pvt. Ltd.},
annote = {cited By 0},
author = {Rajakumari, K and Nalini, C},
journal = {International Journal of Mechanical and Production Engineering Research and Development},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {665--671},
title = {{An enhancement of various techniques using 2D and 3D face recognition with different datasets}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063461054{\&}partnerID=40{\&}md5=5d205a407838f83ab702e65899a26b48},
volume = {8},
year = {2018}
}
@inproceedings{Xiao:2001:FFL:858375.858393,
address = {Darlinghurst, Australia, Australia},
author = {Xiao, Yi and Yan, Hong},
booktitle = {Proceedings of the Pan-Sydney Area Workshop on Visual Information Processing - Volume 11},
isbn = {0-909-92589-5},
keywords = {Voronoi diagram/Delaunay triangulation,eyes/mouth corners,facial feature extraction,point set cluster,revisao{\_}acm,skeleton},
mendeley-tags = {revisao{\_}acm},
pages = {103--108},
publisher = {Australian Computer Society, Inc.},
series = {VIP '01},
title = {{Facial Feature Location with Delaunay Triangulation/Voronoi Diagram Calculation}},
url = {http://dl.acm.org/citation.cfm?id=858375.858393},
year = {2001}
}
@inproceedings{Orlans:2003:PCS:982507.982519,
address = {New York, NY, USA},
author = {Orlans, Nicholas M and Piszcz, Alan T and Chavez, Richard J},
booktitle = {Proceedings of the 2003 ACM SIGMM Workshop on Biometrics Methods and Applications},
doi = {10.1145/982507.982519},
isbn = {1-58113-779-6},
keywords = {3D face modeling,biometric systems,biometrics testing,face recognition,performance evaluation,revisao{\_}acm,synthetic imagery},
mendeley-tags = {revisao{\_}acm},
pages = {58--64},
publisher = {ACM},
series = {WBMA '03},
title = {{Parametrically Controlled Synthetic Imagery Experiment for Face Recognition Testing}},
url = {http://doi.acm.org/10.1145/982507.982519},
year = {2003}
}
@inproceedings{7410807,
abstract = {We reconstruct a controllable model of a person from a large photo collection that captures his or her persona, i.e., physical appearance and behavior. The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned. Moreover, we show the ability to drive or puppeteer the captured person B using any other video of a different person A. In this scenario, B acts out the role of person A, but retains his/her own personality and character. Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem. We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video.},
author = {Suwajanakorn, S and Seitz, S M and Kemelmacher-Shlizerman, I},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.450},
issn = {2380-7504},
keywords = {face recognition;image reconstruction;image textur,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {3952--3960},
title = {{What Makes Tom Hanks Look Like Tom Hanks}},
year = {2015}
}
@inproceedings{7830326,
abstract = {As a challenging topic in computer graphics and computer vision, 3D face modeling has been applied in various fields including film and animation production, game development as well as medical analysis. In this paper, we mainly aims at face personalized modeling optimization based on KINECT2. The optimization of the scheme proposed is the mesh deformation based on the differential coordinates constrained by face++ feature points and facial muscle function, namely firstly by extracting the location of feature points on the color image and depth map to obtain the feature information of the personalized face, then aligning the intermediate results of the KINECT2 modeling to the feature information. It is good to keep the local grid differential property, at the same time. In this paper, the automatic UV mapping is realized by means of planar differential mesh deformation, which improves the accuracy and efficiency of manual adjustment of UV coordinates in Maya. The experimental results show that the optimization results are more fit in with the individual face.},
author = {Li, J and Zhang, Y and Xu, P and Lan, S and Li, S},
booktitle = {2016 9th International Symposium on Computational Intelligence and Design (ISCID)},
doi = {10.1109/ISCID.2016.1052},
issn = {2473-3547},
keywords = {computer vision;face recognition;feature extractio,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {197--201},
title = {{3D Personalized Face Modeling Based on KINECT2}},
volume = {1},
year = {2016}
}
@inproceedings{7453907,
annote = {28/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
28/04 Exclu{\'{i}}do (etapa 1)},
author = {Wang, X and Ruan, Q and Jin, Y and An, G},
booktitle = {6th International Conference on Wireless, Mobile and Multi-Media (ICWMMN 2015)},
doi = {10.1049/cp.2015.0943},
keywords = {etapa1,face recognition,feature extraction,id356,ieeexplore,image classifi,poly,revisao{\_}scopus},
mendeley-tags = {etapa1,id356,ieeexplore,poly,revisao{\_}scopus},
month = {nov},
pages = {192--196},
title = {{3D face recognition using closest point coordinates and spherical vector norms}},
year = {2015}
}
@inproceedings{7785124,
abstract = {We present a system which is able to reconstruct human faces on mobile devices with only on-device processing using the sensors which are typically built into a current commodity smart phone. Such technology can for example be used for facial authentication purposes or as a fast preview for further post-processing. Our method uses recently proposed techniques which compute depth maps by passive multi-view stereo directly on the device. We propose an efficient method which recovers the geometry of the face from the typically noisy point cloud. First, we show that we can safely restrict the reconstruction to a 2.5D height map representation. Therefore we then propose a novel low dimensional height map shape model for faces which can be fitted to the input data efficiently even on a mobile phone. In order to be able to represent instance specific shape details, such as moles, we augment the reconstruction from the shape model with a distance map which can be regularized efficiently. We thoroughly evaluate our approach on synthetic and real data, thereby we use both high resolution depth data acquired using high quality multi-view stereo and depth data directly computed on mobile phones.},
author = {Maninchedda, F and H{\"{a}}ne, C and Oswald, M R and Pollefeys, M},
booktitle = {2016 Fourth International Conference on 3D Vision (3DV)},
doi = {10.1109/3DV.2016.59},
keywords = {face recognition;image reconstruction;image resolu,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {489--498},
title = {{Face Reconstruction on Mobile Devices Using a Height Map Shape Model and Fast Regularization}},
year = {2016}
}
@conference{Yu2015,
abstract = {Facial pose estimation is an important part for facial analysis such as face and facial expression recognition. In most existing methods, facial features are essential for facial pose estimation. However, occluded key features and uncontrolled illumination of face images make the facial feature detection vulnerable. In this paper, we propose methods for facial pose estimation via dense reconstruction and sparse representation but avoid localizing facial features. The Sparse Representation Classifier (SRC) method has achieved successful results in face recognition. In this paper, we explore SRC in pose estimation. Sparse representation learns a dictionary of base functions, so each input pose can be approximated by a linear combination of just a sparse subset of the bases. The experiment conducted on the CMU Multiple face database has shown the effectiveness of the proposed method. {\textcopyright} 2014 IEEE.},
annote = {cited By 0},
author = {Yu, H and Liu, H},
booktitle = {IEEE SSCI 2014: 2014 IEEE Symposium Series on Computational Intelligence - RiiSS 2014: 2014 IEEE Symposium on Robotic Intelligence in Informationally Structured Space, Proceedings},
doi = {10.1109/RIISS.2014.7009177},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Facial pose estimation via dense and sparse representation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922591541{\&}doi=10.1109{\%}2FRIISS.2014.7009177{\&}partnerID=40{\&}md5=9a8e923143007069c93c512b48956a03},
year = {2015}
}
@article{8309989,
abstract = {We present an easy-to-use parametric image retouching method for thinning or fattening a face in a single portrait image while maintaining a close similarity to the source image. First, our method reconstructs a 3D face from the input face image using a morphable model. Second, according to the linear regression equation derived from the depth statistics of the soft tissue in the face and the user-set parameters of weight-change degree, we calculate the new positions of the feature points. The Laplacian deformation method is then used for non-feature points in the 3D face model. Our model-based reshaping process can achieve globally consistent editing effects without noticeable artifacts. We seamlessly blend the reshaped face region with the background using image retargeting method based on mesh parametrization. The effectiveness of our algorithm is demonstrated by experiments and user study.},
author = {Zhao, H and Jin, X and Huang, X and Chai, M and Zhou, K},
doi = {10.1109/MCG.2018.011461529},
issn = {0272-1716},
journal = {IEEE Computer Graphics and Applications},
keywords = {computer graphics;face recognition;image reconstru,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {jan},
number = {1},
pages = {77--90},
title = {{Parametric Reshaping of Portrait Images for Weight-change}},
volume = {38},
year = {2018}
}
@article{Fu201974,
abstract = {In this paper, a 4D tensor model is firstly constructed to explore efficient structural information and correlations from multi-modal data (both 2D and 3D face data). As the dimensionality of the generated 4D tensor is high, a tensor dimensionality reduction technique is in need. Since many real-world high-order data often reside in a low dimensional subspace, Tucker decomposition as a powerful technique is utilized to capture multilinear low-rank structure and to extract useful information from the generated 4D tensor data. Our goal is to use Tucker decomposition to obtain a set of core tensors with smaller sizes and factor matrices which are projected into the 4D tensor data for classification prediction. To characterize the involved similarities of the 4D tensor, the low-rank and sparse representation is built in terms of the low-rank structure of factor matrices and the sparsity of the core tensor in the Tucker decomposition of the generated 4D tensor. A tensor completion (TC) framework is embedded to recover the missing information in the 4D tensor modeling process. Thus, a novel tensor dimensionality reduction approach for 2D+3D facial expression recognition via low-rank tensor completion (FERLrTC) is proposed to solve the factor matrices in a majorization–minimization manner by using a rank reduction strategy. Numerical experiments are conducted with a full implementation on the BU-3DFE and Bosphorus databases and synthetic data to illustrate the effectiveness of the proposed approach. {\textcopyright} 2019 Elsevier B.V.},
annote = {cited By 0},
author = {Fu, Y and Ruan, Q and Luo, Z and Jin, Y and An, G and Wan, J},
doi = {10.1016/j.sigpro.2019.03.015},
journal = {Signal Processing},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {74--88},
title = {{FERLrTc: 2D+3D facial expression recognition via low-rank tensor completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063351781{\&}doi=10.1016{\%}2Fj.sigpro.2019.03.015{\&}partnerID=40{\&}md5=3336ddf68e4404bf074c46d33c559e5d},
volume = {161},
year = {2019}
}
@inproceedings{Lei:2012:STB:2425836.2425850,
address = {New York, NY, USA},
author = {Lei, Yinjie and Bennamoun, Mohammed and El-Sallam, Amar A},
booktitle = {Proceedings of the 27th Conference on Image and Vision Computing New Zealand},
doi = {10.1145/2425836.2425850},
isbn = {978-1-4503-1473-2},
keywords = {3D biometrics,3D face recognition,3D open curve,geodesic distance,revisao{\_}acm,structured light},
mendeley-tags = {revisao{\_}acm},
pages = {61--66},
publisher = {ACM},
series = {IVCNZ '12},
title = {{A Structured Template Based 3D Face Recognition Approach}},
url = {http://doi.acm.org/10.1145/2425836.2425850},
year = {2012}
}
@inproceedings{7301378,
abstract = {Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89{\%} with less than 100 training examples, which corresponds to 10{\%} of the pre-selected data pool.},
author = {Polewski, P and Yao, W and Heurich, M and Krzystek, P and Stilla, U},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2015.7301378},
issn = {2160-7516},
keywords = {environmental factors;feature extraction;forestry;,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {10--18},
title = {{Active learning approach to detecting standing dead trees from ALS point clouds combined with aerial infrared imagery}},
year = {2015}
}
@article{7792560,
abstract = {We present a fully automatic approach to real-time 3D face reconstruction from monocular in-the-wild videos. With the use of a cascaded-regressor-based face tracking and a 3D morphable face model shape fitting, we obtain a semidense 3D face shape.We further use the texture information from multiple frames to build a holistic 3D face representation from the video footage. Our system is able to capture facial expressions and does not require any person-specific training. We demonstrate the robustness of our approach on the challenging 300 Videos in the Wild (300- VW) dataset. Our real-time fitting framework is available as an open-source library at http://4dface.org.},
annote = {23/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
23/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Huber, Patrik and Kopp, Philipp and Christmas, William and Ratsch, Matthias and Kittler, Josef},
doi = {10.1109/LSP.2016.2643284},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
keywords = {estela,etapa1,face recognition,id169,ieeexplore,image fusion,image reconstruction,revisao{\_}ieeexplore},
mendeley-tags = {estela,etapa1,id169,ieeexplore,revisao{\_}ieeexplore},
month = {apr},
number = {4},
pages = {437--441},
title = {{Real-Time 3D Face Fitting and Texture Fusion on In-the-Wild Videos}},
url = {http://ieeexplore.ieee.org/document/7792560/},
volume = {24},
year = {2017}
}
@conference{Hou2015,
abstract = {Light field imaging is capable of capturing dense multi-view 2D images in one snapshot, which record both intensity values and directions of rays simultaneously. As an emerging 3D device, the light field camera has been widely used in digital refocusing, depth estimation, stereoscopic display, etc. Traditional multi-view stereo (MVS) methods only perform well on strongly texture surfaces, but the depth map contains numerous holes and large ambiguities on textureless or low-textured regions. In this paper, we exploit the light field imaging technology on 3D face modeling in computer vision. Based on a 3D morphable model, we estimate the pose parameters from facial feature points. Then the depth map is estimated through the epipolar plane images (EPIs) method. At last, the high quality 3D face model is exactly recovered via the fusing strategy. We evaluate the effectiveness and robustness on face images captured by a light field camera with different poses. {\textcopyright} 2015 SPIE.},
annote = {cited By 0},
author = {Hou, G and Liu, F and Sun, Z},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2205630},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Computer vision research with new imaging technology}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958212210{\&}doi=10.1117{\%}2F12.2205630{\&}partnerID=40{\&}md5=13e3557d5d6972c5eaad8f140118a8ad},
volume = {9813},
year = {2015}
}
@inproceedings{7430529,
abstract = {In this paper, we propose a live system for 3D reconstruction of human face using only single 2D camera without any 3D sensor. Lack of feature points and homogeneous skin color leads to low quality and success rate in conventional 3D reconstruction algorithms when applied to human face. Moreover, it requires difficult user interaction. To solve this problem, we adopt the facial shape and appearance model to our 3D reconstruction pipeline. This is the first approach which creates a complete 3D face model that can be directly used for 3D printing or virtual reality applications. Model can be acquired using modern smartphone in less than 40 seconds.},
author = {Limonov, A and Jeong, J and Kim, M and Kim, S and Kim, Y},
booktitle = {2016 IEEE International Conference on Consumer Electronics (ICCE)},
doi = {10.1109/ICCE.2016.7430529},
issn = {2158-4001},
keywords = {cameras;face recognition;image colour analysis;ima,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
month = {jan},
pages = {81--82},
title = {{Human face 3D reconstruction with handheld single 2D camera on mobile devices}},
year = {2016}
}
@conference{Wang2018,
abstract = {This paper proposes a method of mesh construction based on 2D portrait, which realizes the final 3d face model for arbitrary 2D face images by adjusting mesh. First of all need to 2D portrait photo for face detection, based on the photo collection of facial feature points, set up related parameters of the feature points, then according to the name of the feature points and corresponding coordinate parameters, the model of overall adjustment and local mesh vertex, get the final face image more realistic 3D face models. {\textcopyright} 2018 Association for Computing Machinery.},
annote = {cited By 0},
author = {Wang, Z and Wang, R and Yin, G and Wang, Z and Zhou, H and Zhang, H and Zhao, H and Zhang, K},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3148453.3306301},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Mesh establishment method based on 2D portrait}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062788447{\&}doi=10.1145{\%}2F3148453.3306301{\&}partnerID=40{\&}md5=7b50588a35d91fd06707024ecbbdecd9},
year = {2018}
}
@article{Schumacher201549,
abstract = {The goals of this paper are: (1) to enhance the quality of images of faces, (2) to enable 3D Morphable Models (3DMMs) to cope with severely degraded images, and (3) to reconstruct textured 3D faces with details that are not in the input images. Details that are lost in the input images due to blur, low resolution or occlusions, are filled in by the 3DMM and an additional texture enhancement algorithm that adds high-resolution details from example faces. By leveraging class-specific knowledge, this restoration process goes beyond what general image operations such as deblurring or inpainting can achieve. The benefit of the 3DMM for image restoration is that it can be applied to any pose and illumination, unlike image-based methods. However, it is only with the new fitting algorithm that 3DMMs can produce realistic faces from severely degraded images. The new method includes the blurring or downsampling operator explicitly into the analysis-by-synthesis algorithm. {\textcopyright} 2015 Elsevier B.V. All rights reserved.},
annote = {cited By 3},
author = {Schumacher, M and Piotraschke, M and Blanz, V},
doi = {10.1016/j.imavis.2015.06.004},
journal = {Image and Vision Computing},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {49--64},
title = {{Hallucination of facial details from degraded images using 3D face models}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935466420{\&}doi=10.1016{\%}2Fj.imavis.2015.06.004{\&}partnerID=40{\&}md5=27b647c8dcd456f6e54155f0ee0a2657},
volume = {40},
year = {2015}
}
@conference{Kamanditya2019,
abstract = {From limited two-dimensional recognition, facial recognition has now been developed to be able to recognize three-dimensional face images, which usually involves process of face pose estimation. As the conventional artificial neural networks has shown low recognition rate to this problem, Convolution Neural Network have been the most potential classifier to determine the pose estimation of a three-dimensional face images. Convolution operation is expected to minimize the effect of distortion and disorientation of the object, and able to efficiently reduce the required parameters. Results show that the CNN system could estimate the pose position of the 3D face images with high recognition rate, however, this recognition rate decline significantly for the noisy buried face images, showing the CNN still need improvement to deal with noisy environments. {\textcopyright} 2018 IEEE.},
annote = {cited By 0},
author = {Kamanditya, B and Kuswara, R P and Nugroho, M A and Kusumoputro, B},
booktitle = {2018 IEEE 5th International Conference on Engineering Technologies and Applied Sciences, ICETAS 2018},
doi = {10.1109/ICETAS.2018.8629150},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Convolution Neural Network for Pose Estimation of Noisy Three-Dimensional Face Images}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062888878{\&}doi=10.1109{\%}2FICETAS.2018.8629150{\&}partnerID=40{\&}md5=232ba33a7341381ec65ef455b6e0245c},
year = {2019}
}
@conference{Wu2015,
abstract = {In order to handle pose variation problem in face recognition, Generic Elastic Models (GEMs) was proposed as a low computational and efficient 3D face modeling method, which generates 3D face model from single frontal face image by elastically deforming a generic 3D depth map based on 2D observations of the input face image. In this paper, we extend GEMs to Multi-Depth GEMs (MD-GEMs) by utilizing multiple generic depth maps which merely vary in depth linearly in process of 3D face modeling, taking the assumption that face depth variation across individuals can be modeled by a linear transformation of generic depth map. Multiple 3D models are generated for each input frontal face. In recognition, the galleries are the 3D models constructed from the frontal face of each ID while the probe is a non-frontal face. The pose of input non-frontal face is estimated by a linear regression method and 3D models in the constructed database are rotated and rendered at the estimated pose. Corresponding 2D images are synthesized after 2D projection. After face alignment, the distances between the input image and synthesized images are calculated by a normalized correlation measure and thus the corresponding identity in the database is matched. Experiments on Multi-PIE verify the effectiveness of MD-GEMs on handling pose variation problem in face recognition. {\textcopyright} 2015 IEEE.},
annote = {cited By 3},
author = {Wu, Z and Li, J and Hu, J and Deng, W},
booktitle = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
doi = {10.1109/FG.2015.7163148},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Pose-invariant face recognition using 3D multi-depth generic elastic models}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944936819{\&}doi=10.1109{\%}2FFG.2015.7163148{\&}partnerID=40{\&}md5=278ecaac5b5e8f327f22655de729b82a},
year = {2015}
}
@article{Peng2016228,
abstract = {How to reconstruct 3D face model from wild photos is such a difficult issue that camera calibration is necessary and the images must be from video sequences. In this paper, a face reconstruction model with structure optimization is proposed to build 3D face surface with individual geometry and physical features reservation through wild face images directly and without camera calibration. Low rank and B-Spline are employed to estimate the aligned 2D structure, to calculate the depth information with SSIM, and to reconstruct the 3D face surface from control points and their space transformation. Furthermore, LFW and Bosphorus datasets, as well as Young-to-Aged samples, are introduced to verify the proposed approach and the experimental results demonstrate the feasibility and effectiveness even with different poses, expressions and age-variety. {\textcopyright} 2015 Elsevier B.V.},
annote = {cited By 9},
author = {Peng, W and Xu, C and Feng, Z},
doi = {10.1016/j.neucom.2015.11.090},
journal = {Neurocomputing},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {228--237},
title = {{3D face modeling based on structure optimization and surface reconstruction with B-Spline}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955737027{\&}doi=10.1016{\%}2Fj.neucom.2015.11.090{\&}partnerID=40{\&}md5=38e317e2fb1a0b0b79f8d01db6ccddec},
volume = {179},
year = {2016}
}
@inproceedings{Nozawa:2016:FGR:2945078.2945102,
address = {New York, NY, USA},
author = {Nozawa, Tsukasa and Kato, Takuya and Savkin, Pavel A and Nozawa, Naoki and Morishima, Shigeo},
booktitle = {ACM SIGGRAPH 2016 Posters},
doi = {10.1145/2945078.2945102},
isbn = {978-1-4503-4371-8},
keywords = {3D reconstruction,revisao{\_}acm,revisao{\_}scopus,shape from X,texture synthesis},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus},
pages = {24:1----24:2},
publisher = {ACM},
series = {SIGGRAPH '16},
title = {{3D Facial Geometry Reconstruction Using Patch Database}},
url = {http://doi.acm.org/10.1145/2945078.2945102},
year = {2016}
}
@article{Wu2016,
abstract = {We present a new anatomically-constrained local face model and fitting approach for tracking 3D faces from 2D motion data in very high quality. In contrast to traditional global face models, often built from a large set of blendshapes, we propose a local deformation model composed of many small subspaces spatially distributed over the face. Our local model offers far more flexibility and expressiveness than global blendshape models, even with a much smaller model size. This flexibility would typically come at the cost of reduced robustness, in particular during the under-constrained task of monocular reconstruction. However, a key contribution of this work is that we consider the face anatomy and introduce subspace skin thickness constraints into our model, which constrain the face to only valid expressions and helps counteract depth ambiguities in monocular tracking. Given our new model, we present a novel fitting optimization that allows 3D facial performance reconstruction from a single view at extremely high quality, far beyond previous fitting approaches. Our model is flexible, and can be applied also when only sparse motion data is available, for example with marker-based motion capture or even face posing from artistic sketches. Furthermore, by incorporating anatomical constraints we can automatically estimate the rigid motion of the skull, obtaining a rigid stabilization of the performance for free. We demonstrate our model and single-view fitting method on a number of examples, including, for the first time, extreme local skin deformation caused by external forces such as wind, captured from a single high-speed camera. {\textcopyright} 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
annote = {cited By 23},
author = {Wu, C and Bradley, D and Gross, M and Beeler, T},
doi = {10.1145/2897824.2925882},
journal = {ACM Transactions on Graphics},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {4},
title = {{An anatomically-constrained local deformation model for monocular face capture}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979998440{\&}doi=10.1145{\%}2F2897824.2925882{\&}partnerID=40{\&}md5=6464d6134dfa09e64e44e29ed6b85152},
volume = {35},
year = {2016}
}
@article{Giorgi2017549,
abstract = {In this paper, we analyse patterns in face shape variation due to weight gain. We propose the use of persistent homology descriptors to get geometric and topological information about the configuration of anthropometric 3D face landmarks. In this way, evaluating face changes boils down to comparing the descriptors computed on 3D face scans taken at different times. By applying dimensionality reduction techniques to the dissimilarity matrix of descriptors, we get a space in which each face is a point and face shape variations are encoded as trajectories in that space. Our results show that persistent homology is able to identify features which are well related to overweight and may help assessing individual weight trends. The research was carried out in the context of the European project SEMEOTICONS, which developed a multisensory platform which detects and monitors over time facial signs of cardio-metabolic risk. {\textcopyright} 2016, Springer-Verlag Berlin Heidelberg.},
annote = {cited By 1},
author = {Giorgi, D and Pascali, M A and Henriquez, P and Matuszewski, B J and Colantonio, S and Salvetti, O},
doi = {10.1007/s00371-016-1344-7},
journal = {Visual Computer},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {5},
pages = {549--563},
title = {{Persistent homology to analyse 3D faces and assess body weight gain}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007248182{\&}doi=10.1007{\%}2Fs00371-016-1344-7{\&}partnerID=40{\&}md5=59ad397deb50f8a2aac90bb60cd3a77b},
volume = {33},
year = {2017}
}
@inproceedings{7785119,
abstract = {We introduce a novel end-to-end real-time pose-robust 3D face tracking framework from RGBD videos, which is capable of tracking head pose and facial actions simultaneously in unconstrained environment without intervention or pre-calibration from a user. In particular, we emphasize tracking the head pose from profile to profile and improving tracking performance in challenging instances, where the tracked subject is at a considerably large distance from the camera and the quality of data deteriorates severely. To achieve these goals, the tracker is guided by an efficient multi-view 3D shape regressor, trained upon generic RGB datasets, which is able to predict model parameters despite large head rotations or tracking range. Specifically, the shape regressor is made aware of the head pose by inferring the possibility of particular facial landmarks being visible through a joint regression-classification local random forest framework, and piecewise linear regression models effectively map visibility features into shape parameters. In addition, the regressor is combined with a joint 2D+3D optimization that sparsely exploits depth information to further refine shape parameters to maintain tracking accuracy over time. The result is a robust on-line RGBD 3D face tracker that can model extreme head poses and facial expressions accurately in challenging scenes, which are demonstrated in our extensive experiments.},
annote = {26/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
26/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Pham, Hai X and Pavlovic, Vladimir},
booktitle = {2016 Fourth International Conference on 3D Vision (3DV)},
doi = {10.1109/3DV.2016.54},
isbn = {978-1-5090-5407-7},
keywords = {cameras,emotion recognition,estela,etapa1,face recognition,id306,ieeexplore,image,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {estela,etapa1,id306,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus},
month = {oct},
pages = {441--449},
publisher = {IEEE},
title = {{Robust Real-Time 3D Face Tracking from RGBD Videos under Extreme Pose, Depth, and Expression Variation}},
url = {http://ieeexplore.ieee.org/document/7785119/},
year = {2016}
}
@article{Talandova20166373,
abstract = {The biometric identification by the face is one of the oldest biometric identification. With increasing progress has been using of identification by the face was implemented into area of security, where it provides a faster and more accurate identification. The 3D face reader uses for the identification of the person: eyes, mouth, nose, and in contrast to 2D readers also chin and cheeks. 3D face reader by Broadway manufacturer was used to measure the physiological similarities of family members. It is equipped with the 3D camera system, which uses the method of structured light scanning and saves the template into the 3D model of face. The obtained data were evaluated by software Turnstile Enrolment Application (TEA). The participants of the measurement were members of three different families. Each person was compared with the previously saved templates of other family members. On basis of this fact was evaluated the similarity of family members. {\textcopyright} Research India Publications.},
annote = {cited By 0},
author = {Talandova, H and Kralik, L and Adamek, M},
journal = {International Journal of Applied Engineering Research},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {9},
pages = {6373--6375},
title = {{Determination of the uncertainties and the physiological similarities of family members by using the biometric device the broadway 3D}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026785113{\&}partnerID=40{\&}md5=712cdc729bf70e487251043cd050117c},
volume = {11},
year = {2016}
}
@article{Berretti:2012:DFF:2168752.2168759,
address = {New York, NY, USA},
author = {Berretti, Stefano and {Del Bimbo}, Alberto and Pala, Pietro},
doi = {10.1145/2168752.2168759},
issn = {2157-6904},
journal = {ACM Trans. Intell. Syst. Technol.},
keywords = {3D face recognition,Feature selection,ethnicity-based learning,iso-geodesic stripes,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
number = {3},
pages = {45:1----45:20},
publisher = {ACM},
title = {{Distinguishing Facial Features for Ethnicity-Based 3D Face Recognition}},
url = {http://doi.acm.org/10.1145/2168752.2168759},
volume = {3},
year = {2012}
}
@inproceedings{7881713,
abstract = {Driver's gaze direction is an indicator of driver state and plays a significantly role in driving safety. Traditional gaze zone estimation methods based on eye model have disadvantages due to the vulnerability under large head movement. Different from these methods, an appearance-based head pose-free eye gaze prediction method is proposed in this paper, for driver gaze zone estimation under free head movement. To achieve this goal, a gaze zone classifier is trained with head vectors and eye image features by random forest. The head vector is calculated by Pose from Orthography and Scaling with ITerations (POSIT) where a 3D face model is combined with facial landmark detection. And the eye image features are derived from eye images which extracted through eye region localization. These features are presented as the combination of sparse coefficients by sparse encoding with eye image dictionary, having good potential to carry information of the eye images. Experimental results show that the proposed method is applicable in real driving environment.},
author = {And and {and X. Fu}},
booktitle = {2017 IEEE International Conference on Big Data and Smart Computing (BigComp)},
doi = {10.1109/BIGCOMP.2017.7881713},
issn = {2375-9356},
keywords = {driver information systems;face recognition;featur,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {42--46},
title = {{Head pose-free eye gaze prediction for driver attention study}},
year = {2017}
}
@inproceedings{Berretti:2010:RFM:1877808.1877825,
address = {New York, NY, USA},
author = {Berretti, Stefano and {Del Bimbo}, Alberto and Pala, Pietro},
booktitle = {Proceedings of the ACM Workshop on 3D Object Retrieval},
doi = {10.1145/1877808.1877825},
isbn = {978-1-4503-0160-2},
keywords = {3D face recognition,facial profiles,revisao{\_}acm,sift keypoints},
mendeley-tags = {revisao{\_}acm},
pages = {81--86},
publisher = {ACM},
series = {3DOR '10},
title = {{Recognition of 3D Faces with Missing Parts Based on Profile Networks}},
url = {http://doi.acm.org/10.1145/1877808.1877825},
year = {2010}
}
@inproceedings{7298920,
abstract = {Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.},
author = {Ji, R},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298920},
issn = {1063-6919},
keywords = {Boltzmann machines;feature extraction;image colour,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {3013--3021},
title = {{Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery}},
year = {2015}
}
@article{Lin2019,
abstract = {Fruit detection in real outdoor conditions is necessary for automatic guava harvesting, and the branch-dependent pose of fruits is also crucial to guide a robot to approach and detach the target fruit without colliding with its mother branch. To conduct automatic, collision-free picking, this study investigates a fruit detection and pose estimation method by using a low-cost red–green–blue–depth (RGB-D) sensor. A state-of-the-art fully convolutional network is first deployed to segment the RGB image to output a fruit and branch binary map. Based on the fruit binary map and RGB-D depth image, Euclidean clustering is then applied to group the point cloud into a set of individual fruits. Next, a multiple three-dimensional (3D) line-segments detection method is developed to reconstruct the segmented branches. Finally, the 3D pose of the fruit is estimated using its center position and nearest branch information. A dataset was acquired in an outdoor orchard to evaluate the performance of the proposed method. Quantitative experiments showed that the precision and recall of guava fruit detection were 0.983 and 0.948, respectively; the 3D pose error was 23.43 ◦ ± 14.18 ◦ ; and the execution time per fruit was 0.565 s. The results demonstrate that the developed method can be applied to a guava-harvesting robot. {\textcopyright} 2019 by the authors. Licensee MDPI, Basel, Switzerland.},
annote = {cited By 2},
author = {Lin, G and Tang, Y and Zou, X and Xiong, J and Li, J},
doi = {10.3390/s19020428},
journal = {Sensors (Switzerland)},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {2},
title = {{Guava detection and pose estimation using a low-cost RGB-D sensor in the field}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060395875{\&}doi=10.3390{\%}2Fs19020428{\&}partnerID=40{\&}md5=9762acf3fef74ebb7d469ecb6dbc9e32},
volume = {19},
year = {2019}
}
@article{Mosslah201810482,
abstract = {Most facial recognition techniques are self-contained on a three-dimensional model to ensure the challenge posed by facial expressions that are variable depending on the situation. In this research we aim to provide the latest SDK package which is used to identify faces with attention to flipping because the face changes its expressions and using the technique of 3D-expandable Model 3DMM we can introduce facial changes, 3DMM also enables us to isolate identity variations from those resulting from changes in facial expressions. We face two problems that need to be addressed: accurate measurement of the parameters of the situation and computational efficiency. When the verification is performed with the adjustment, a new face view is created where the situation is corrected and the expression is disabled to define the expression we provide two methods for it. The first depends on the prior knowledge to illustrate the neutral expression image of the input image. While the second method is based on the idea of verification on the transfer of expression of the exposed face to the probe. Experiments using neutral and equivalent view with the FR SDK commercial standard are demonstrated on two-face databases, PIE and AR, thus, demonstrating a significant improvement in the experimental SDK's performance in terms of expression. {\textcopyright} Medwell Journals, 2018.},
annote = {cited By 0},
author = {Mosslah, A A and Mahdi, R H},
doi = {10.3923/jeasci.2018.10482.10489},
journal = {Journal of Engineering and Applied Sciences},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {24},
pages = {10482--10489},
title = {{3DMM fitting for 3D face reconstruction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059119186{\&}doi=10.3923{\%}2Fjeasci.2018.10482.10489{\&}partnerID=40{\&}md5=70419672a342ff6bb1c80c911d7ebc4a},
volume = {13},
year = {2018}
}
@inproceedings{7387550,
abstract = {Face recognition research mainly focuses on traditional 2D color images, which is extremely susceptible to be affected by external factors such as various viewpoints and has limited recognition accuracy. In order to achieve improved recognition performance, as well as the 3D face holds more abundant information than 2D, we present a 3D human face recognition algorithm using the Microsoft's Kinect. The proposed approach integrates the depth data with the RGB data to generate 3D face raw data and then extracts feature points, identifies the target via a two-level cascade classifier. Also, we build a 3D-face database including 16 individuals captured exclusively using Kinect. The experimental results indicate that the introduced algorithm can not only achieve better recognition accuracy in comparison to existing 2D and 3D face recognition algorithms when the probe face is exactly in front of Kinect sensor, but also can increase 9.3{\%} of recognition accuracy compared to the PCA-3D algorithm when it is not confronting the camera.},
annote = {09/05/2018 Em processo de sele{\c{c}}{\~{a}}o (Etapa 1)
09/05/2018 Exclu{\'{i}}do (Etapa 1)},
author = {Zhou, Wei and Chen, Jian Xin and Wang, Lei},
booktitle = {Proceedings of 2015 IEEE International Conference on Computer and Communications, ICCC 2015},
doi = {10.1109/CompComm.2015.7387550},
isbn = {9781467381253},
keywords = {3D face recognition,Kinect,RGB-D images,XML file,artur,classifier,etapa1,id459,ieeexplore,revisao{\_}scopus},
mendeley-tags = {artur,etapa1,id459,ieeexplore,revisao{\_}scopus},
month = {oct},
pages = {109--114},
publisher = {IEEE},
title = {{A RGB-D face recognition approach without confronting the camera}},
url = {http://ieeexplore.ieee.org/document/7387550/},
year = {2016}
}
@article{Chouchane:2015:FRU:2836876.2836879,
abstract = {This paper presents an automatic face recognition system in the presence of illumination, expressions and pose variations based on depth and intensity information. At first, the registration of 3D faces is achieved using iterative closest point (ICP). Nose tip point must be located using Maximum Intensity Method. This point usually has the largest depth value; however there is a problem with some unnecessary data such as: shoulders, hair, neck and parts of clothes; to cope with this issue, we propose the integral projection curves (IPC)-based facial area segmentation to extract the facial area. After that, the combined method principal component analysis (PCA) with enhanced Fisher model (EFM) is used to obtain the feature matrix vectors. Finally, the classification is performed using distance measurement and support vector machine (SVM). The experiments are implemented on two face databases CASIA3D and GavabDB; our results show that the proposed method achieves a high recognition performance. Online publication date: Mon, 05-Oct-2015},
address = {Inderscience Publishers, Geneva, SWITZERLAND},
annote = {24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
24/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Chouchane, Ammar and Belahcene, Mebarka and Bourennane, Salah},
doi = {10.1504/IJISTA.2015.072219},
issn = {1740-8865},
journal = {International Journal of Intelligent Systems Technologies and Applications},
keywords = {acm,etapa1,gil,id234,revisao{\_}scopus},
mendeley-tags = {acm,etapa1,gil,id234,revisao{\_}scopus},
number = {1},
pages = {50},
publisher = {Inderscience Publishers},
title = {{3D and 2D face recognition using integral projection curves based depth and intensity images}},
url = {http://www.inderscience.com/link.php?id=72219},
volume = {14},
year = {2015}
}
@conference{Kopinski2015336,
abstract = {We present a novel approach for improved hand-gesture recognition by a single time-of-flight(ToF) sensor in an automotive environment. As the sensor's lateral resolution is comparatively low, we employ a learning approach comprising multiple processing steps, including PCA-based cropping, the computation of robust point cloud descriptors and training of a Multilayer perceptron (MLP) on a large database of samples. A sophisticated temporal fusion technique boosts the overall robustness of recognition by taking into account data coming from previous classification steps. Overall results are very satisfactory when evaluated on a large benchmark set of ten different hand poses, especially when it comes to generalization on previously unknown persons. {\textcopyright} 2015 IEEE.},
annote = {cited By 7},
author = {Kopinski, T and Magand, S and Gepperth, A and Handmann, U},
booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
doi = {10.1109/IVS.2015.7225708},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {336--342},
title = {{A light-weight real-time applicable hand gesture recognition system for automotive applications}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949810838{\&}doi=10.1109{\%}2FIVS.2015.7225708{\&}partnerID=40{\&}md5=4fd7e4d7ee4b106cf99188a969570b3e},
volume = {2015-Augus},
year = {2015}
}
@conference{Bessaoudi20181,
abstract = {This paper presents an efficient framework for verification using 3D information based on high order tensor representation in uncontrolled conditions. The 3D depth images are subdivided into sub-blocks and the Multi-Scale Local Binarised Statistical Image Features (MSBSIF) + Multi-Scale local phase quantization (MSLPQ) histograms are extracted and concatenated from each block and organized as a 3rd order tensor. Moreover, two steps of dimensionally reduction to the face tensor are used. Firstly, Multilinear Principal Component Analysis (MPCA) is used to project the face tensor in a new subspace features in which the dimension of each mode tensor is reduced. After that, Enhanced Fisher Model (EFM) is applied to discriminate the faces of diverse persons in the database. Finally, the corresponding is achieved based distance measurement. The proposed approach (MPCA+EFM) has been evaluated on the challenging face database Bosporus 3D. The experimental results demonstrate that our method attains a high authentication performance. {\textcopyright} 2018 IEEE.},
annote = {cited By 0},
author = {Bessaoudi, M and Belahcene, M and Ouamane, A and Bourennane, S},
booktitle = {2018 4th International Conference on Advanced Technologies for Signal and Image Processing, ATSIP 2018},
doi = {10.1109/ATSIP.2018.8364461},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {1--5},
title = {{A novel approach based on high order tensor and multi-scale locals features for 3D face recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048474613{\&}doi=10.1109{\%}2FATSIP.2018.8364461{\&}partnerID=40{\&}md5=a3d3238410015813c361264d49d4dbc9},
year = {2018}
}
@conference{Tseng2015,
abstract = {Personalized design enhances the values added by a product or service by satisfying individual customer requirements. It has become a trend in consumer product development nowadays. This paper proposes a method for design personalization of the eyeglasses frame using anthropometric data. Three-dimensional face models were constructed using non-contact scanning devices. Principal Component Analysis (PCA) was applied to reduce the data complexity while preserving sufficient data variance. Kriging based parametric models correlate the mesh point coordinates of a face model to a set of feature parameters. The correlation allows synthesizing and controlling 3D facial geometry approximating to individual users with given parameter values. Rendering the synthesized geometry with human face images generates realistic face models. These models not only allow adjusting the frame design in real-time, but also evaluating whether or how the design style fits individual face characteristics. This study enhances the practical values of 3D anthropometric data by realizing the concept of human-centric design. {\textcopyright} Copyright 2015 by ASME.},
annote = {cited By 1},
author = {Tseng, C.-Y. and Wang, I.-J. and Chu, C.-H.},
booktitle = {Proceedings of the ASME Design Engineering Technical Conference},
doi = {10.1115/DETC2015-47065},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Product personalization using 3D parametric face models: An example of the eyeglass frame design}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979052826{\&}doi=10.1115{\%}2FDETC2015-47065{\&}partnerID=40{\&}md5=10bfc8ef027be99960e2e0ad957b5b78},
volume = {1B-2015},
year = {2015}
}
@conference{Elsayed2018,
abstract = {Majority of the face recognition algorithms use query faces captured from uncontrolled, in the wild, environment. Because of cameras' limited capabilities, it is common for these captured facial images to be blurred or low resolution. Super resolution algorithms are therefore crucial in improving the resolution of such images especially when the image size is small and enlargement is required. This paper aims to demonstrate the effect of one of the state-of-the-art algorithms in the field of image super resolution. To demonstrate the functionality of the algorithm, various before and after 3D face alignment cases are provided using the images from the Labeled Faces in the Wild (lfw) dataset. Resulting images are subject to test on a closed set recognition protocol using unsupervised algorithms with high dimensional extracted features. The inclusion of super resolution algorithm resulted in significant improvement in recognition rate over recently reported results obtained from unsupervised algorithms on the same dataset. {\textcopyright} 2017 IEEE.},
annote = {cited By 0},
author = {Elsayed, A and Mahmood, A and Sobh, T},
booktitle = {Proceedings - Applied Imagery Pattern Recognition Workshop},
doi = {10.1109/AIPR.2017.8457967},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Effect of super resolution on high dimensional features for unsupervised face recognition in the wild}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057564222{\&}doi=10.1109{\%}2FAIPR.2017.8457967{\&}partnerID=40{\&}md5=5bfeb78c1dcfc5925ab065e0fb215e88},
volume = {2017-Octob},
year = {2018}
}
@inproceedings{7780392,
abstract = {Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45), lacking the ability to align faces in large poses up to 90. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods.},
author = {Zhu, X and Lei, Z and Liu, X and Shi, H and Li, S Z},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.23},
issn = {1063-6919},
keywords = {face recognition;feature extraction;neural nets;po,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {146--155},
title = {{Face Alignment Across Large Poses: A 3D Solution}},
year = {2016}
}
@article{Moustakas:2009:CSU:1466664.1466675,
address = {London, UK, UK},
author = {Moustakas, Konstantinos and Nikolakis, Georgios and Tzovaras, Dimitrios and Carbini, Sebastien and Bernier, Olivier and Viallet, Jean Emmanuel},
doi = {10.1007/s00779-007-0166-z},
issn = {1617-4909},
journal = {Personal Ubiquitous Comput.},
keywords = {3D content-based search,Multimodal interfaces,Sketch,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
number = {1},
pages = {59--67},
publisher = {Springer-Verlag},
title = {{3D Content-based Search Using Sketches}},
url = {http://dx.doi.org/10.1007/s00779-007-0166-z},
volume = {13},
year = {2009}
}
@article{Islam:2012:RRA:2187671.2187676,
address = {New York, NY, USA},
author = {Islam, Syed M S and Bennamoun, Mohammed and Owens, Robyn A and Davies, Rowan},
doi = {10.1145/2187671.2187676},
issn = {0360-0300},
journal = {ACM Comput. Surv.},
keywords = {3D data representation,3D ear,3D face,Biometrics,detection,facial expressions,multimodal recognitionl,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
number = {3},
pages = {14:1----14:34},
publisher = {ACM},
title = {{A Review of Recent Advances in 3D Ear- and Expression-invariant Face Biometrics}},
url = {http://doi.acm.org/10.1145/2187671.2187676},
volume = {44},
year = {2012}
}
@inproceedings{7960597,
abstract = {In this study, an SVM-based system is proposed for the classification of facial expressions that are represented in 3D. Distance based features are used as a feature vector, which are determined by the distances between the different key points on the image. Study was conducted on a subset (Happy, sadness, surprise) of Bosphorus 3D Face Database. 9 different fiducial points are used to calculate a total of 5 distance features. SVM classification was performed with K-fold cross validation thus mean classification performance of different training and test clusters were determined. {\%}85 success rate has achieved as a result of the expression analysis performed on the 3D facial scans.},
author = {S{\"{o}}ylemez, {\"{O}} F and Ergen, B and S{\"{o}}ylemez, N H},
booktitle = {2017 25th Signal Processing and Communications Applications Conference (SIU)},
doi = {10.1109/SIU.2017.7960597},
keywords = {face recognition;feature extraction;image classifi,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--3},
title = {{A 3D facial expression recognition system based on SVM classifier using distance based features}},
year = {2017}
}
@conference{Casas2016121,
abstract = {Creating and animating realistic 3D human faces is an important element of virtual reality, video games, and other areas that involve interactive 3D graphics. In this paper, we propose a system to generate photorealistic 3D blendshape-based face models automatically using only a single consumer RGB-D sensor. The capture and processing requires no artistic expertise to operate, takes 15 seconds to capture and generate a single facial expression, and approximately 1 minute of processing time per expression to transform it into a blendshape model. Our main contributions include a complete end-To-end pipeline for capturing and generating photorealistic blendshape models automatically and a registration method that solves dense correspondences between two face scans by utilizing facial landmarks detection and optical flows. We demonstrate the effectiveness of the proposed method by capturing different human subjects with a variety of sensors and puppeteering their 3D faces with real-Time facial performance retargeting. The rapid nature of our method allows for just-in-Time construction of a digital face. To that end, we also integrated our pipeline with a virtual reality facial performance capture system that allows dynamic embodiment of the generated faces despite partial occlusion of the user's real face by the head-mounted display. {\textcopyright} 2016 ACM.},
annote = {cited By 4},
author = {Casas, D and Feng, A and Alexander, O and Fyffe, G and Debevec, P and Ichikari, R and Li, H and Olszewski, K and Suma, E and Shapiro, A},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2915926.2915936},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {121--129},
title = {{Rapid photorealistic blendshape modeling from RGB-D sensors}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986267324{\&}doi=10.1145{\%}2F2915926.2915936{\&}partnerID=40{\&}md5=4e00f5f15196523125f31937c990585a},
volume = {23-25-May-},
year = {2016}
}
@conference{Amin2017602,
abstract = {Feature selection from facial regions is a well-known approach to increase the performance of 2D image-based face recognition systems. In case of 3D modality, the approach of region-based feature selection for face recognition is relatively new. In this context, this paper presents an approach to evaluate the discrimination power of different regions of a 3D facial surface for its potential use in face recognition systems. We propose the use of weighted average of unit normal vector on the facial surface as the feature for region-based face recognition from 3D point cloud data (PCD). The iterative closest point algorithm is employed for the registration of segmented regions of facial point clouds. A metric based on angular distance between normals is introduced to indicate the similarity between two surfaces of same facial region. Finally, the intra class correlation based discrimination score is formulated to find out the key facial regions such as the eyes, nose, and mouth that are significant while recognizing a person with facial surface PCD. {\textcopyright} 2016 IEEE.},
annote = {cited By 0},
author = {Amin, R and Shams, A F and Rahman, S M M and Hatzinakos, D},
booktitle = {Proceedings of 9th International Conference on Electrical and Computer Engineering, ICECE 2016},
doi = {10.1109/ICECE.2016.7853992},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {602--605},
title = {{Evaluation of Discrimination power of facial parts from 3D point cloud data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016176950{\&}doi=10.1109{\%}2FICECE.2016.7853992{\&}partnerID=40{\&}md5=ed19a9638f37d7a968aaf2d3f22af2a1},
year = {2017}
}
@conference{Reji2018,
abstract = {This paper focuses on a region based methodology for expression in sensitive 3D face recognition process. Considering facial regions that are comparatively unchanging during expressions, results shows that using fifteen sub regions on the face can attain high 3D face recognition. We use a modified face recognition algorithm along with hierarchical contour based image registration for finding the similarity score. Our method operates in two modes: verification mode and confirmation mode. Crop 100 mm of frontal face region, apply preprocessing and automatically detect nose tip, translate the face image to origin and crop fifteen sub regions. The cropped sub regions are defined by cuboids which occupy more volumetric data, Nose Tip is the most projecting point of the face with the highest value along Z-axis so consider it as origin. The modified face recognition algorithm reduces the effects caused by facial expressions and artifacts. Finally a Hierarchical contour based image registration technique is applied which yields better results. The approach is applied on Bosphorus 3D datasets and achieved a verification rate of 95.3{\%} at 0.1{\%} false acceptance rate. In the identification scenario 99.3{\%} rank one recognition is achieved. {\textcopyright} 2017 IEEE.},
annote = {cited By 0},
author = {Reji, R and Sojanlal, P},
booktitle = {2017 IEEE International Conference on Computational Intelligence and Computing Research, ICCIC 2017},
doi = {10.1109/ICCIC.2017.8524581},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Region Based 3D Face Recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057946157{\&}doi=10.1109{\%}2FICCIC.2017.8524581{\&}partnerID=40{\&}md5=92c092fa4e2838ab444fcc0356fcdb75},
year = {2018}
}
@inproceedings{Yin:2004:GVF:1027527.1027611,
address = {New York, NY, USA},
author = {Yin, Lijun and Weiss, Kenny},
booktitle = {Proceedings of the 12th Annual ACM International Conference on Multimedia},
doi = {10.1145/1027527.1027611},
isbn = {1-58113-893-8},
keywords = {face modeling,facial expression,feature analysis,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {360--363},
publisher = {ACM},
series = {MULTIMEDIA '04},
title = {{Generating 3D Views of Facial Expressions from Frontal Face Video Based on Topographic Analysis}},
url = {http://doi.acm.org/10.1145/1027527.1027611},
year = {2004}
}
@conference{Tian20171017,
abstract = {Spoofing detection is essential for practical face recognition system. Based on the fact that genuine face has special geometric curvatures across surface, this paper brings forward an ultra-fast yet accurate spoofing detection approach using a low-cost stereo camera. To obtain curvatures, the three dimensional shapes of selected facial landmarks are analyzed, by fitting point cloud around each landmark to a specific partial face surface. Spoofing detection is then performed by evaluating curvatures of each landmark and integrating them together. Experiments verify that the approach is able to detect spoofed faces in printed photographs without or with various bending at FAR equal to 0.00{\%}. Meanwhile, genuine faces have a trivial opportunity to be falsely rejected: FRR is 0.59{\%} for near frontal faces and less than 5{\%} for faces with large varying poses. Detection time is 51 milliseconds when executed on a single processor [1] running at a clock frequency of 266M Hz, this makes the detection very suitable for embedded face recognition system. {\textcopyright} 2016 IEEE.},
annote = {cited By 1},
author = {Tian, G and Mori, T and Okuda, Y},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7899769},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {1017--1022},
title = {{Spoofing detection for embedded face recognition system using a low cost stereo camera}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019057068{\&}doi=10.1109{\%}2FICPR.2016.7899769{\&}partnerID=40{\&}md5=bf20d5a07ea9e072ba6cbe319eabd42f},
year = {2017}
}
@conference{Dube20175266,
abstract = {Place recognition in 3D data is a challenging task that has been commonly approached by adapting image-based solutions. Methods based on local features suffer from ambiguity and from robustness to environment changes while methods based on global features are viewpoint dependent. We propose SegMatch, a reliable place recognition algorithm based on the matching of 3D segments. Segments provide a good compromise between local and global descriptions, incorporating their strengths while reducing their individual drawbacks. SegMatch does not rely on assumptions of 'perfect segmentation', or on the existence of 'objects' in the environment, which allows for reliable execution on large scale, unstructured environments. We quantitatively demonstrate that SegMatch can achieve accurate localization at a frequency of 1Hz on the largest sequence of the KITTI odometry dataset. We furthermore show how this algorithm can reliably detect and close loops in real-time, during online operation. In addition, the source code for the SegMatch algorithm is made publicly available1. {\textcopyright} 2017 IEEE.},
annote = {cited By 22},
author = {Dube, R and Dugas, D and Stumm, E and Nieto, J and Siegwart, R and Cadena, C},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989618},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {5266--5272},
title = {{SegMatch: Segment based place recognition in 3D point clouds}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027968710{\&}doi=10.1109{\%}2FICRA.2017.7989618{\&}partnerID=40{\&}md5=c70ca5cddbbac1d6d8faa518ba50d188},
year = {2017}
}
@inproceedings{8346387,
abstract = {In many scenarios related to image processing, 3D face reconstruction is considered an essential part. A robust and fast method of 3D face reconstruction from a single 2D image is presented. This method consists of three main steps including extraction of features from single image, calculating the depth of image and adjustment of a 3D model on the direction of Z-axis. In the first step, the features of image are extracted by using supervised descent method (SDM). Using SDM, face regions like facial components (eyes, nose lips) and face contours are detected. Second step consists of depth prediction by implementation of multivariate Gaussian distribution. Finally, 3D face is constructed with the help of features and the depth information and 3D database. The proposed method has been verified by conducting several experiments depicted in evaluation section. Our method is robust in nature and gives good results even using a single image, comparing to other methods that use multiple images for reconstruction of 3D images.},
author = {Afzal, H M R and Luo, S and Afzal, M K},
booktitle = {2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)},
doi = {10.1109/ICOMET.2018.8346387},
keywords = {face recognition;feature extraction;Gaussian distr,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--5},
title = {{Reconstruction of 3D facial image using a single 2D image}},
year = {2018}
}
@inproceedings{7533187,
abstract = {This paper reports on a novel application of computer vision and image processing technologies to an interdisciplinary project in architectural history that seeks to help identify and visualize differences between homologous buildings constructed to a common template design. By identifying the mutations in homologous buildings, we assist humanists in giving voice to the contributions of the myriad additional “authors” for these buildings beyond their primary designers. We develop a framework for comparing 3D point cloud representations of homologous buildings captured using lidar: focusing on identifying similarities and differences, both among 3D scans of different buildings and between the 3D scans and the design specifications of architectural drawings. The framework addresses global and local alignment for highlighting gross differences as well as differences in individual structural elements and provides methods for readily highlighting the differences via suitable visualizations. The framework is demonstrated on pairs of homologous buildings selected from the Canadian and Ottoman rail networks. Results demonstrate the utility of the framework confirming differences already apparent to the humanist researchers and also revealing new differences that were not previously observed.},
author = {Ding, L and Elliethy, A and Freedenberg, E and Wolf-Johnson, S A and Romphf, J and Christensen, P and Sharma, G},
booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7533187},
issn = {2381-8549},
keywords = {buildings (structures);design engineering;optical,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {4378--4382},
title = {{Comparative analysis of homologous buildings using range imaging}},
year = {2016}
}
@inproceedings{7780741,
abstract = {Automated 3D reconstruction of faces from images is challenging if the image material is difficult in terms of pose, lighting, occlusions and facial expressions, and if the initial 2D feature positions are inaccurate or unreliable. We propose a method that reconstructs individual 3D shapes from multiple single images of one person, judges their quality and then combines the best of all results. This is done separately for different regions of the face. The core element of this algorithm and the focus of our paper is a quality measure that judges a reconstruction without information about the true shape. We evaluate different quality measures, develop a method for combining results, and present a complete processing pipeline for automated reconstruction.},
author = {Piotraschke, M and Blanz, V},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.372},
issn = {1063-6919},
keywords = {face recognition;feature extraction;image reconstr,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {3418--3427},
title = {{Automated 3D Face Reconstruction from Multiple Images Using Quality Measures}},
year = {2016}
}
@inproceedings{8237378,
abstract = {This paper investigates how far a very deep neural network is from attaining close to saturating performance on existing 2D and 3D face alignment datasets. To this end, we make the following 5 contributions: (a) we construct, for the first time, a very strong baseline by combining a state-of-the-art architecture for landmark localization with a state-of-the-art residual block, train it on a very large yet synthetically expanded 2D facial landmark dataset and finally evaluate it on all other 2D facial landmark datasets. (b)We create a guided by 2D landmarks network which converts 2D landmark annotations to 3D and unifies all existing datasets, leading to the creation of LS3D-W, the largest and most challenging 3D facial landmark dataset to date ({\~{}}230,000 images). (c) Following that, we train a neural network for 3D face alignment and evaluate it on the newly introduced LS3D-W. (d) We further look into the effect of all “traditional” factors affecting face alignment performance like large pose, initialization and resolution, and introduce a “new” one, namely the size of the network. (e) We show that both 2D and 3D face alignment networks achieve performance of remarkable accuracy which is probably close to saturating the datasets used. Training and testing code as well as the dataset can be downloaded from https://www.adrianbulat.com/face-alignment/.},
author = {Bulat, A and Tzimiropoulos, G},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.116},
issn = {2380-7504},
keywords = {face recognition;neural nets;stereo image processi,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1021--1030},
title = {{How Far are We from Solving the 2D 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)}},
year = {2017}
}
@inproceedings{8578301,
abstract = {Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10{\%}. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem.},
author = {{Zulqarnain Gilani}, S and Mian, A},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00203},
issn = {2575-7075},
keywords = {convolutional neural nets;face recognition;learnin,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1896--1905},
title = {{Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition}},
year = {2018}
}
@inproceedings{Guo:2018:SVF:3240876.3240913,
address = {New York, NY, USA},
author = {Guo, Xingyan and Jin, Yi and Li, Yidong and Xing, Junliang and Lang, Congyan},
booktitle = {Proceedings of the 10th International Conference on Internet Multimedia Computing and Service},
doi = {10.1145/3240876.3240913},
isbn = {978-1-4503-6520-8},
keywords = {3D face model,landmark detection,landmark smoothing,revisao{\_}acm,revisao{\_}scopus},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus},
pages = {20:1----20:7},
publisher = {ACM},
series = {ICIMCS '18},
title = {{Stabilizing Video Facial Landmark Detection and Tracking via Global and Local Filtering}},
url = {http://doi.acm.org/10.1145/3240876.3240913},
year = {2018}
}
@inproceedings{7780913,
abstract = {In this paper we present a method for computing dense correspondence between a set of 3D face meshes using functional maps. The functional maps paradigm brings with it a number of advantages for face correspondence. First, it allows us to combine various notions of correspondence. We do so by proposing a number of face-specific functions, suited to either within-or between-subject correspondence. Second, we propose a groupwise variant of the method allowing us to compute cycle-consistent functional maps between all faces in a training set. Since functional maps are of much lower dimension than point-to-point correspondences, this is feasible even when the input meshes are very high resolution. Finally, we show how a functional map provides a geometric constraint that can be used to filter feature matches between non-rigidly deforming surfaces.},
author = {Zhang, C and Smith, W A P and Dessein, A and Pears, N and Dai, H},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.544},
issn = {1063-6919},
keywords = {image filtering;image matching;image resolution;gr,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {5033--5041},
title = {{Functional Faces: Groupwise Dense Correspondence Using Functional Maps}},
year = {2016}
}
@conference{Chouchane2015,
abstract = {Face recognition in an uncontrolled condition such as illumination and expression variations is a challenging task. Local descriptor is one of the most efficient methods used to deal with these problems. In this paper, we present an automatic 3D face recognition approach based on three local descriptors, local phase quantization (LPQ), Three-Patch Local Binary Patterns (TPLBP) and Four-Patch Local Binary Patterns (TPLBP). Facial images are passing through one of the three descriptors and divided into sub-regions or rectangular blocks. The histogram of each sub-region is extracted and concatenated into a single feature vector. PCA (Principal Component Analysis) and EFM (Enhanced Fisher linear discriminant Model) are used to reduce the dimensionality of the resulting feature vectors. Finally, these vectors are sent to the classification step, when we use two methods; SVM (Support Victor Machine) and similarity measures. CASIA 3D face database is introduced to experimental evaluation. The experimental results illustrate a high recognition performance of the proposed approach. {\textcopyright} 2014 IEEE.},
annote = {cited By 4},
author = {Chouchane, A and Belahcene, M and Ouamane, A and Bourennane, S},
booktitle = {2014 4th International Conference on Image Processing Theory, Tools and Applications, IPTA 2014},
doi = {10.1109/IPTA.2014.7001925},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{3D face recognition based on histograms of local descriptors}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921721830{\&}doi=10.1109{\%}2FIPTA.2014.7001925{\&}partnerID=40{\&}md5=4bbf52130cd2484d05cce4611c376687},
year = {2015}
}
@inproceedings{Cohen:2018:BSB:3177148.3180081,
address = {New York, NY, USA},
author = {Cohen, Fernand S and Li, Chenxi},
booktitle = {Proceedings of the 2Nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
doi = {10.1145/3177148.3180081},
isbn = {978-1-4503-5290-1},
keywords = {3D building reconstruction,GPS,invariants,localization,revisao{\_}acm,salient features},
mendeley-tags = {revisao{\_}acm},
pages = {44--51},
publisher = {ACM},
series = {MedPRAI '18},
title = {{3D Building Synthesis Based on Images and Affine Invariant Salient Features}},
url = {http://doi.acm.org/10.1145/3177148.3180081},
year = {2018}
}
@inproceedings{7827274,
abstract = {In this paper, we proposed a new 3D face reconstruction approach to reconstruct the 3D face from a single 2D image with arbitrary pose. The proposed approach enhanced the 3D morphable model by introducing a new local face shape constraint in another space. Then, the face can be constrained in double spaces: the global face shape space and the local face structure space. The advantages of the proposed approach are: 1) it explores the global and local constraints in the double spaces; 2) it reconstructs the 3D face from a single face image with arbitrary pose. The approach is validated on BJUT{\_}3D face database, and the 3D facial expressions are generated on FacewareHouse and FaceScurb database.},
author = {Han, L and Xiao, Q and Wang, S},
booktitle = {2016 23rd International Conference on Mechatronics and Machine Vision in Practice (M2VIP)},
doi = {10.1109/M2VIP.2016.7827274},
keywords = {face recognition;image reconstruction;shape recogn,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
pages = {1--5},
title = {{3D face reconstruction with global and local constraints in double spaces}},
year = {2016}
}
@article{7053946,
abstract = {Distracted driving is one of the main causes of vehicle collisions in the United States. Passively monitoring a driver's activities constitutes the basis of an automobile safety system that can potentially reduce the number of accidents by estimating the driver's focus of attention. This paper proposes an inexpensive vision-based system to accurately detect Eyes Off the Road (EOR). The system has three main components: 1) robust facial feature tracking; 2) head pose and gaze estimation; and 3) 3-D geometric reasoning to detect EOR. From the video stream of a camera installed on the steering wheel column, our system tracks facial features from the driver's face. Using the tracked landmarks and a 3-D face model, the system computes head pose and gaze direction. The head pose estimation algorithm is robust to nonrigid face deformations due to changes in expressions. Finally, using a 3-D geometric analysis, the system reliably detects EOR.},
annote = {22/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
22/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Vicente, Francisco and Huang, Zehua and Xiong, Xuehan and {De La Torre}, Fernando and Zhang, Wende and Levi, Dan},
doi = {10.1109/TITS.2015.2396031},
isbn = {1524-9050 VO - 16},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Driver monitoring system,Head pose estimation,etapa1,eyes off the road detection,gaze estimation,gil,id143,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {etapa1,gil,id143,ieeexplore,revisao{\_}ieeexplore,revisao{\_}scopus},
month = {aug},
number = {4},
pages = {2014--2027},
pmid = {1582677},
title = {{Driver Gaze Tracking and Eyes off the Road Detection System}},
url = {http://ieeexplore.ieee.org/document/7053946/},
volume = {16},
year = {2015}
}
@article{Zhou2016717,
abstract = {We present a data-driven method for automatically generating a 3D cartoon of a real 3D face. Given a sparse set of 3D real faces and their corresponding cartoon faces modeled by an artist, our method models the face in each subspace as the deformation of its nearby exemplars and learn a mapping between the deformations defined by the real faces and their cartoon counterparts. To reduce the exemplars needed for learning, we regress a collection of linear mappings defined locally in both face geometry and identity spaces and develop a progressive scheme for users to gradually add new exemplars for training. At runtime, our method first finds the nearby exemplars of an input real face and then constructs the result cartoon face from the corresponding cartoon faces of the nearby real face exemplars and the local deformations mapped from the real face subspace. Our method greatly simplifies the cartoon generation process by learning artistic styles from a sparse set of exemplars. We validate the efficiency and effectiveness of our method by applying it to faces of different facial features. Results demonstrate that our method not only preserves the artistic style of the exemplars, but also keeps the unique facial geometric features of different identities. {\textcopyright} 2016, Springer-Verlag Berlin Heidelberg.},
annote = {cited By 3},
author = {Zhou, J and Tong, X and Liu, Z and Guo, B},
doi = {10.1007/s00371-016-1265-5},
journal = {Visual Computer},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {6-8},
pages = {717--727},
title = {{3D cartoon face generation by local deformation mapping}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973649600{\&}doi=10.1007{\%}2Fs00371-016-1265-5{\&}partnerID=40{\&}md5=a058f1631d67e4b37788167e5935116c},
volume = {32},
year = {2016}
}
@article{Duan2015674,
abstract = {Craniofacial reconstruction is to estimate a person[U+05F3]s face model from the skull. It can be applied in many fields such as forensic medicine, face animation. In this article, a regression modeling based method for craniofacial reconstruction is proposed, in which a statistical shape model is built for skulls and faces, respectively, and the relationship between them is extracted in the shape parameter spaces through partial least squares regression (PLSR). Craniofacial reconstruction is realized by using the relationship and the face statistical shape model. To better represent craniofacial shape variations and boost the reconstruction, both the skull and face are divided into five corresponding feature regions, and a mapping from each skull region to the corresponding face region is established. For an unknown skull, the five face regions are obtained through the five mappings, and the face is recovered by stitching the five face regions. The attributes such as age and body mass index (BMI) can be added into the mappings to achieve the face reconstruction with different attributes. Compared with other statistical learning based methods in literature, the proposed method more directly and reasonably reflects the relationship that the face shape is determined by the skull and influenced by some attributes. In addition, the proposed method does not need to locate landmarks, whose quantity and accuracy can highly affect the reconstruction. Experimental results validate the proposed method. {\textcopyright} 2014 Elsevier B.V.},
annote = {cited By 15},
author = {Duan, F and Huang, D and Tian, Y and Lu, K and Wu, Z and Zhou, M},
doi = {10.1016/j.neucom.2014.04.089},
journal = {Neurocomputing},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {P2},
pages = {674--682},
title = {{3D face reconstruction from skull by regression modeling in shape parameter spaces}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919479161{\&}doi=10.1016{\%}2Fj.neucom.2014.04.089{\&}partnerID=40{\&}md5=5ca5558ce2e3a8cb365ced144f3b67d3},
volume = {151},
year = {2015}
}
@conference{Maalej20181,
abstract = {In this paper, we design a multimodal framework for object detection, recognition and mapping based on the fusion of stereo camera frames, point cloud Velodyne LIDAR scans, and Vehicle-to-Vehicle (V2V) Basic Safety Messages (BSMs) that are exchanged using Dedicated Short Range Communication (DSRC). We merge the key features of rich texture descriptions of objects from 2D images using Convolutional Neural Networks (CNN). In addition, depth and distance between objects are provided by the 3D LIDAR point cloud and the awareness of hidden vehicles is achieved from BSMs' beacons. We present a joint pixel to point cloud and pixel to V2V correspondence of objects in frames of driving sequences in the KITTI Vision Benchmark Suite. We achieve this by using a semi-supervised manifold alignment approach to achieve camera-LIDAR and camera-V2V mapping of their recognized persons and cars that have the same underlying manifold. {\textcopyright} 2017 IEEE.},
annote = {cited By 3},
author = {Maalej, Y and Sorour, S and Abdel-Rahim, A and Guizani, M},
booktitle = {2017 IEEE Global Communications Conference, GLOBECOM 2017 - Proceedings},
doi = {10.1109/GLOCOM.2017.8254480},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {1--6},
title = {{VANETs Meet Autonomous Vehicles: A Multimodal 3D Environment Learning Approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046348567{\&}doi=10.1109{\%}2FGLOCOM.2017.8254480{\&}partnerID=40{\&}md5=181825ce0c3c81cae852013f14856e5e},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{DiPaola:2006:ERM:1183316.1183337,
address = {New York, NY, USA},
author = {DiPaola, Steve and Arya, Ali},
booktitle = {Proceedings of the 2006 ACM SIGGRAPH Symposium on Videogames},
doi = {10.1145/1183316.1183337},
isbn = {1-59593-386-7},
keywords = {affective communication,data driven animation,facial animation,procedural art,revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {143--149},
publisher = {ACM},
series = {Sandbox '06},
title = {{Emotional Remapping of Music to Facial Animation}},
url = {http://doi.acm.org/10.1145/1183316.1183337},
year = {2006}
}
@conference{Ahdid201873,
abstract = {In this paper, we present an automatic 3D face recognition system. This system is based on the representation of human faces surfaces as collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two facial surfaces, we compute a geodesic distance between a pair of facial curves using a Riemannian geometry. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008). {\textcopyright} 2017 IEEE.},
annote = {cited By 0},
author = {Ahdid, R and Taifi, K and Said, S and Fakir, M and Manaut, B},
booktitle = {Proceedings - 2017 14th International Conference on Computer Graphics, Imaging and Visualization, CGiV 2017},
doi = {10.1109/CGiV.2017.25},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {73--78},
title = {{Automatic face recognition system using iso-geodesic curves in riemanian manifold}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048323921{\&}doi=10.1109{\%}2FCGiV.2017.25{\&}partnerID=40{\&}md5=73b9e65d367cee00cdcb28e7b5ef55cb},
year = {2018}
}
@article{8219732,
abstract = {A set of images depicting faces with different expressions or in various ages consists of components that are shared across all images (i.e.,jointcomponents) imparting to the depicted object the properties of human faces as well asindividualcomponents that are related to different expressions or age groups. Discovering the common (joint) and individual components in facial images is crucial for applications such as facial expression transfer and age progression. The problem is rather challenging when dealing with images captured in unconstrained conditions in the presence of sparse non-Gaussian errors of large magnitude (i.e., sparse gross errors or outliers) and contain missing data. In this paper, we investigate the use of a method recently introduced in statistics, the so-called Joint and Individual Variance Explained (JIVE) method, for the robust recovery of joint and individual components in visual facial data consisting of an arbitrary number of views. Since the JIVE is not robust to sparse gross errors, we propose alternatives, which are (1) robust to sparse gross, non-Gaussian noise, (2) able to automatically find the individual components rank, and (3) can handle missing data. We demonstrate the effectiveness of the proposed methods to several computer vision applications, namely facial expression synthesis and 2D and 3D face age progression `in-the-wild'.},
author = {Sagonas, C and Ververas, E and Panagakis, Y and Zafeiriou, S},
doi = {10.1109/TPAMI.2017.2784421},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {computer vision;face recognition;Gaussian noise;fa,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
number = {11},
pages = {2668--2681},
title = {{Recovering Joint and Individual Components in Facial Data}},
volume = {40},
year = {2018}
}
@conference{Pala201895,
abstract = {In the typical approach, person re-identification is performed using appearance in 2D still images or videos, thus invalidating any application in which a person may change dress across subsequent acquisitions. For example, this is a relevant scenario for home patient monitoring. Depth cameras enable person re-identification exploiting 3D information that captures biometric cues such as face and characteristic dimensions of the body. Unfortunately, face and skeleton quality is not always enough to grant a correct recognition from depth data. Both features are affected by the pose of the subject and the distance from the camera. In this paper, we propose a model to incorporate a robust skeleton representation with a highly discriminative face feature, weighting samples by their quality. Our method combining face and skeleton data improves rank-1 accuracy compared to individual cues especially on short realistic sequences. {\textcopyright} 2018 The Eurographics Association.},
annote = {cited By 1},
author = {Pala, P and Seidenari, L and Berretti, S and {Del Bimbo}, A},
booktitle = {Eurographics Workshop on 3D Object Retrieval, EG 3DOR},
doi = {10.2312/3dor.20181058},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {95--101},
title = {{Person re-identification from depth cameras using skeleton and 3D face data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061202945{\&}doi=10.2312{\%}2F3dor.20181058{\&}partnerID=40{\&}md5=e3983b8db3fafe0ac7edc126398a0093},
volume = {2018-April},
year = {2018}
}
@inproceedings{7988861,
abstract = {This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.},
author = {Angonese, A T and {Ferreira Rosa}, P F},
booktitle = {2017 International Conference on Military Technologies (ICMT)},
doi = {10.1109/MILTECHS.2017.7988861},
keywords = {mobile robots;neural nets;object detection;object,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {779--786},
title = {{Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform}},
year = {2017}
}
@inproceedings{7049881,
abstract = {Conversational agents are receiving significant attention from multi-agent and human computer interaction research societies. Many techniques have been developed to enable these agents to behave in a human-like manner. In order to do so, they are simulated with similar communicative channels as humans. Moreover, they are also simulated with emotion and personality. In this work, we focus on issue of expressing emotions for embodied-agents. We present a three dimensional face with ability to speak emotional Vietnamese speech and naturally express emotions while talking. Our face can represent lip movements during emotionally pronouncing Vietnamese words, and at the same time it can show emotional facial expressions while speaking. The face's architecture consists of three parts: Vietnamese Emotional Speech Synthesis module, Emotions to Facial Expressions module, and Combination module which creates lip movements when pronouncing Vietnamese emotional speech and combines these movements with emotional facial expressions. We have tested the face in the football supporter domain in order to confirm its naturalness. The face is simulated as the face of a football supporter agent which experiences emotions and expresses emotional expressions in his voice as well as on his face.},
author = {Ngo, T D and Bui, T D},
booktitle = {The 2015 IEEE RIVF International Conference on Computing Communication Technologies - Research, Innovation, and Vision for Future (RIVF)},
doi = {10.1109/RIVF.2015.7049881},
keywords = {emotion recognition;human computer interaction;mul,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {jan},
pages = {94--99},
title = {{A Vietnamese 3D taking face for embodied conversational agents}},
year = {2015}
}
@inproceedings{8347107,
abstract = {Video data from surveillance cameras are nowadays an important instrument for investigating crimes and identifying the identity of an offender. The analysis of the mass data acquired from numerous cameras poses enormous challenges to police investigation authorities. Supporting softwares and video management tools currently on the market focus either on elaborate visualization and editing of video data, specific image processing or video content analysis tasks. As a result, such a scattered system landscape further exacerbates the complexity and difficulty of a timely analysis of the available data. This work presents our unified framework ivisX, which is an integrated suite to simplify the entire workflow of video data investigation. The algorithmic backbone of ivisX is built upon an effective content-based search algorithm using region covariance for low-resolution (LR) data and a novel 3D face super-resolution (FSR) approach, which can generate high-resolution (HR) 3D face models to render high-quality facial composites with a single blurred and pixelated face image of the LR domain. Moreover, ivisX has a modular design, which allows for flexible incorporation of various extensions ranging from processing and display of video data from multiple cameras to analysis and documentation of the results into a powerful integrated toolkit to assist forensic investigation.},
author = {Qu, C and Metzler, J and Monari, E},
booktitle = {2018 IEEE Winter Applications of Computer Vision Workshops (WACVW)},
doi = {10.1109/WACVW.2018.00007},
keywords = {cameras;face recognition;image resolution;solid mo,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {9--17},
title = {{ivisX: An Integrated Video Investigation Suite for Forensic Applications}},
year = {2018}
}
@conference{Liew2018105,
abstract = {3D tree database provides essential information of tree species abundance, spatial distribution and tree height for forest mapping, sustainable urban planning and 3D city modelling. Fusion of passive optical satellite imagery and active Lidar data can potentially be exploited for operational forest inventory. However, such fusion requires very high geometric accuracy for both data sets. This paper proposes an approach for 3D tree information extracted from passive and active data integrating into existing tree database by effectively using geometric information of satellite camera model and laser scanner scanning geometry. The paper also presents the individual methods for tree crown identification and delineation from satellite images and lidar point cloud data respectively, the geometric correction of tree position from tree top to tree base. The ground truth accuracy assessment for the tree extracted is also present. {\textcopyright} Authors 2018. CC BY 4.0 License.},
annote = {cited By 0},
author = {Liew, S C and Huang, X and Lin, E S and Shi, C and Yee, A T K and Tandon, A},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-4-W10-105-2018},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {4/W10},
pages = {105--111},
title = {{Integration of tree database derived from satellite imagery and LiDAR point cloud data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056170398{\&}doi=10.5194{\%}2Fisprs-archives-XLII-4-W10-105-2018{\&}partnerID=40{\&}md5=1e8199f1042e01555d0bd86203b981ab},
volume = {42},
year = {2018}
}
@inproceedings{8661454,
abstract = {In Text-Independent speaker identification, the individual that produced some captured speech signal has to be identified without his collaboration, he might not even know that he is being the subject of an identification process. The system could not ask the individual to utter some specific word or phrase, which is precisely what is done in Text-Dependent speaker recognition. Text-Independent speaker identification is far more complicated since we cannot simply measure the similarity of an utterance of a word or phrase to another utterance made by the same speaker of the same word or phrase in which case we could use the dynamics of the speech signal. In this paper we search in the speech signal looking for voiced speech segments and estimate its first three formants, so we end up with a three-dimensional point cloud for each speaker of the collection of known speakers. To identify a speaker we have to measure the similarity of a point-cloud from an unknown speaker to the point-clouds that belong to known speakers, we do that by searching for local structures in the cloud in a way that is highly scalable and robust. We performed tests with both a collection of our own in Spanish and with the English Language Speech Database for Speaker Recognition (ELSDSR) from the Technical University of Denmark achieving results that improve recent published work with ELSDSR.},
author = {Camarena-Ibarrola, A and Castro-Coria, M and Figueroa, K},
booktitle = {2018 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)},
doi = {10.1109/ROPEC.2018.8661454},
issn = {2573-0770},
keywords = {revisao{\_}ieeexplore,speaker recognition;speech processing;cloud point},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
pages = {1--6},
title = {{Cloud Point Matching for Text-Independent Speaker Identification}},
year = {2018}
}
@conference{Wang20191826,
abstract = {The recently developed event cameras can directly sense the motion by generating an asynchronous sequence of events, i.e., an event stream, where each individual event (x, y, t) corresponds to the space-time location when a pixel sensor captures an intensity change. Compared with RGB cameras, event cameras are frameless but can capture much faster motion, therefore have great potential for recognizing gestures of fast motions. To deal with the unique output of event cameras, previous methods often treat event streams as time sequences, thus do not fully explore the space-time sparsity and structure of the event stream data. In this work, we treat the event stream as a set of 3D points in space-time, i.e., space-time event clouds. To analyze event clouds and recognize gestures, we propose to leverage PointNet, a neural network architecture originally designed for matching and recognizing 3D point clouds. We adapt PointNet to cater to event clouds for real-time gesture recognition. On the benchmark dataset of event camera based gesture recognition, i.e., IBM DVS128 Gesture dataset, our proposed method achieves a high accuracy of 97.08{\%} and performs the best among existing methods. {\textcopyright} 2019 IEEE.},
annote = {cited By 0},
author = {Wang, Q and Zhang, Y and Yuan, J and Lu, Y},
booktitle = {Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019},
doi = {10.1109/WACV.2019.00199},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {1826--1835},
title = {{Space-time event clouds for gesture recognition: From RGB cameras to event cameras}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063575027{\&}doi=10.1109{\%}2FWACV.2019.00199{\&}partnerID=40{\&}md5=d8c48628e34a61213d411eca9d1c6850},
year = {2019}
}
@conference{Le20182555,
abstract = {Face datasets are a fundamental tool to analyze the performance of face recognition algorithms. However, the accuracy achieved on current benchmark datasets is saturated. Although multiple face datasets have been published recently, they only focus on the number of samples and lack diversity on facial appearance factors, such as pose and illumination. In addition, while 3D data have been demonstrated improved face recognition accuracy by a significant margin, only a few 3D face datasets provide high quality 2D and 3D data. In this paper, we introduce a new and challenging dataset, called UHDB31, which not only allows direct measurement of the influence of pose, illumination, and resolution on face recognition but also facilitates different experimental configurations with both 2D and 3D data. We conduct a series of experiments with various face recognition algorithms and point out how far they are from solving the face recognition problem under pose, illumination, and resolution variation. The dataset is publicly available and free for research use1. {\textcopyright} 2017 IEEE.},
annote = {cited By 6},
author = {Le, H A and Kakadiaris, I A},
booktitle = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
doi = {10.1109/ICCVW.2017.300},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {2555--2563},
title = {{UHDB31: A dataset for better understanding face recognition across pose and illumination variation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045561999{\&}doi=10.1109{\%}2FICCVW.2017.300{\&}partnerID=40{\&}md5=4a88cc6c019a2d8f3cb21196b1ab0a70},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{7495382,
abstract = {In this work, we exploit 3D Constrained Local Model (CLM) for facial landmark detection. Our approach integrates the geometric information of 3D face scans. The fast increase demand of 3D data invite to develop 3D image processing methods for many applications and especially for automatic landmark detection. The new step in this paper is the introduction of mesh histogram of gradients (meshHOG) as local descriptors around every landmark location. The proposed work is evaluated on the publicly available Bosphorus database. A comparison with the other descriptors mesh LBP and mesh SIFT are also depicted.},
author = {Rai, M C E and Tortorici, C and Al-Muhairi, H and Safar, H A and Werghi, N},
booktitle = {2016 18th Mediterranean Electrotechnical Conference (MELECON)},
doi = {10.1109/MELCON.2016.7495382},
issn = {2158-8481},
keywords = {computational geometry;face recognition;mesh gener,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1--5},
title = {{Landmarks detection on 3D face scans using local histogram descriptors}},
year = {2016}
}
@article{Yang2017180,
abstract = {In recent years, updating the inventory of road infrastructures based on field work is labor intensive, time consuming, and costly. Fortunately, vehicle-based mobile laser scanning (MLS) systems provide an efficient solution to rapidly capture three-dimensional (3D) point clouds of road environments with high flexibility and precision. However, robust recognition of road facilities from huge volumes of 3D point clouds is still a challenging issue because of complicated and incomplete structures, occlusions and varied point densities. Most existing methods utilize point or object based features to recognize object candidates, and can only extract limited types of objects with a relatively low recognition rate, especially for incomplete and small objects. To overcome these drawbacks, this paper proposes a semantic labeling framework by combing multiple aggregation levels (point-segment-object) of features and contextual features to recognize road facilities, such as road surfaces, road boundaries, buildings, guardrails, street lamps, traffic signs, roadside-trees, power lines, and cars, for highway infrastructure inventory. The proposed method first identifies ground and non-ground points, and extracts road surfaces facilities from ground points. Non-ground points are segmented into individual candidate objects based on the proposed multi-rule region growing method. Then, the multiple aggregation levels of features and the contextual features (relative positions, relative directions, and spatial patterns) associated with each candidate object are calculated and fed into a SVM classifier to label the corresponding candidate object. The recognition performance of combining multiple aggregation levels and contextual features was compared with single level (point, segment, or object) based features using large-scale highway scene point clouds. Comparative studies demonstrated that the proposed semantic labeling framework significantly improves road facilities recognition precision (90.6{\%}) and recall (91.2{\%}), particularly for incomplete and small objects. {\textcopyright} 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
annote = {cited By 31},
author = {Yang, B and Dong, Z and Liu, Y and Liang, F and Wang, Y},
doi = {10.1016/j.isprsjprs.2017.02.014},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {180--194},
title = {{Computing multiple aggregation levels and contextual features for road facilities recognition using mobile laser scanning data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014280392{\&}doi=10.1016{\%}2Fj.isprsjprs.2017.02.014{\&}partnerID=40{\&}md5=e55b1a964646c3a7ef9a46f5371ec8d1},
volume = {126},
year = {2017}
}
@inproceedings{Vlasic:2006:FTM:1185657.1185864,
address = {New York, NY, USA},
author = {Vlasic, Daniel and Brand, Matthew and Pfister, Hanspeter and Popovic, Jovan},
booktitle = {ACM SIGGRAPH 2006 Courses},
doi = {10.1145/1185657.1185864},
isbn = {1-59593-364-6},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
publisher = {ACM},
series = {SIGGRAPH '06},
title = {{Face Transfer with Multilinear Models}},
url = {http://doi.acm.org/10.1145/1185657.1185864},
year = {2006}
}
@inproceedings{8099733,
abstract = {We present a data-driven inference method that can synthesize a photorealistic texture map of a complete 3D face model given a partial 2D view of a person in the wild. After an initial estimation of shape and low-frequency albedo, we compute a high-frequency partial texture map, without the shading component, of the visible face area. To extract the fine appearance details from this incomplete input, we introduce a multi-scale detail analysis technique based on mid-layer feature correlations extracted from a deep convolutional neural network. We demonstrate that fitting a convex combination of feature correlations from a high-resolution face database can yield a semantically plausible facial detail description of the entire face. A complete and photorealistic texture map can then be synthesized by iteratively optimizing for the reconstructed feature correlations. Using these high-resolution textures and a commercial rendering framework, we can produce high-fidelity 3D renderings that are visually comparable to those obtained with state-of-the-art multi-view face capture systems. We demonstrate successful face reconstructions from a wide range of low resolution input images, including those of historical figures. In addition to extensive evaluations, we validate the realism of our results using a crowdsourced user study.},
author = {Saito, S and Wei, L and Hu, L and Nagano, K and Li, H},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.250},
issn = {1063-6919},
keywords = {face recognition;feature extraction;image reconstr,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {2326--2335},
title = {{Photorealistic Facial Texture Inference Using Deep Neural Networks}},
year = {2017}
}
@conference{Gedat2017627,
abstract = {Computer visual recognition of multiple human poses infers technological benefit in a variety of systems, including security surveillance, medical therapeutics, sports analytics and many more. For this goal the set of detected body parts on color or depth images must be aligned to reconstruct the skeletons of the humans. Here, an algorithm is introduced that models the body part point clouds using principal component analysis to obtain anatomically correct positions of joints, and that assembles the redundant and/or incomplete number of candidate joints with graph theoretical methods using Suurballe's k-shortest disjoint paths algorithm to build the skeletons. The computations were applied to MOCAP database motions rendered in Blender to produce idealized classified point clouds, and to real human depth images classified with decision forests similar to Shotton et al. For MOCAP data, in 68 images showing 3 persons all 204 skeletons were correctly aligned using 4.285 of 4.682 joints with no false assignment. For 33 real human images each showing 3 people, 71 skeletons were correctly detected with 1 false detection and 17 misses, which is promising with respect to non-perfect body part classification in real world. {\textcopyright} 2016 IEEE.},
annote = {cited By 2},
author = {Gedat, E and Fechner, P and Fiebelkorn, R and Vandenhouten, R},
booktitle = {2016 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2016 - Conference Proceedings},
doi = {10.1109/SMC.2016.7844310},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {627--631},
title = {{Multiple human skeleton recognition in RGB and depth images with graph theory, anatomic refinement of point clouds and machine learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015757362{\&}doi=10.1109{\%}2FSMC.2016.7844310{\&}partnerID=40{\&}md5=457718e5de7befb4e3049d644bad9242},
year = {2017}
}
@inproceedings{8571963,
abstract = {Cleft lip is a kind of congenital facial morphological abnormality. In the clinical field of cleft lip, it is necessary to analyze symmetric shape. However, there is no method to analyze the cleft lip technique based on symmetrical viewpoints. On the other hand, in our previous method to find a symmetric axis using a 2D image, since the middle line is extracted only from the front view of the face moire image. There was a problem that low accuracy was obtained by slight rotation of the face and it was not possible to consider 3D information. In this paper, we propose a method to extract the median plane of the face by analyzing based on bilateral symmetry by using 3D point cloud on the face of front. By extracting the median plane, we believe that not only surgical assistance of doctor be possible but also become a clue to development of simulation software which is the end goal.},
author = {Yamada, S and Lu, H and Tan, J K and Kim, H and Kimura, N and Okawachi, T and Nozoe, E and Nakamura, N},
booktitle = {2018 18th International Conference on Control, Automation and Systems (ICCAS)},
keywords = {biomechanics;face recognition;feature extraction;i,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1347--1350},
title = {{Extraction of Median Plane from Facial 3D Point Cloud Based on Symmetry Analysis Using ICP Algorithm}},
year = {2018}
}
@inproceedings{Gong:2009:AFE:1631272.1631358,
address = {New York, NY, USA},
author = {Gong, Boqing and Wang, Yueming and Liu, Jianzhuang and Tang, Xiaoou},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
doi = {10.1145/1631272.1631358},
isbn = {978-1-60558-608-3},
keywords = {3D facial expression recognition,basic facial shape component,expressional shape component,revisao{\_}acm,shape deformation},
mendeley-tags = {revisao{\_}acm},
pages = {569--572},
publisher = {ACM},
series = {MM '09},
title = {{Automatic Facial Expression Recognition on a Single 3D Face by Exploring Shape Deformation}},
url = {http://doi.acm.org/10.1145/1631272.1631358},
year = {2009}
}
@conference{Häufel2017,
abstract = {The concept of remote sensing is to provide information about a wide-range area without making physical contact with this area. If, additionally to satellite imagery, images and videos taken by drones provide a more up-to-date data at a higher resolution, or accurate vector data is downloadable from the Internet, one speaks of sensor data fusion. The concept of sensor data fusion is relevant for many applications, such as virtual tourism, automatic navigation, hazard assessment, etc. In this work, we describe sensor data fusion aiming to create a semantic 3D model of an extremely interesting yet challenging dataset: An alpine region in Southern Germany. A particular challenge of this work is that rock faces including overhangs are present in the input airborne laser point cloud. The proposed procedure for identification and reconstruction of overhangs from point clouds comprises four steps: Point cloud preparation, filtering out vegetation, mesh generation and texturing. Further object types are extracted in several interesting subsections of the dataset: Building models with textures from UAV (Unmanned Aerial Vehicle) videos, hills reconstructed as generic surfaces and textured by the orthophoto, individual trees detected by the watershed algorithm, as well as the vector data for roads retrieved from openly available shapefiles and GPS-device tracks. We pursue geo-specific reconstruction by assigning texture and width to roads of several pre-determined types and modeling isolated trees and rocks using commercial software. For visualization and simulation of the area, we have chosen the simulation system Virtual Battlespace 3 (VBS3). It becomes clear that the proposed concept of sensor data fusion allows a coarse reconstruction of a large scene and, at the same time, an accurate and up-to-date representation of its relevant subsections, in which simulation can take place. {\textcopyright} 2017 SPIE.},
annote = {cited By 0},
author = {H{\"{a}}ufel, G and Bulatov, D and Solbrig, P},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2278237},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Sensor data fusion for textured reconstruction and virtual representation of alpine scenes}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040194238{\&}doi=10.1117{\%}2F12.2278237{\&}partnerID=40{\&}md5=d493987a5f33890f50234d791a77f43c},
volume = {10428},
year = {2017}
}
@conference{Zhang20151991,
abstract = {Activity recognition of multi-individuals (ARMI) within a group, which is essential to practical human-centered robotics applications such as childhood education, is a particularly challenging and previously not well studied problem. We present a novel adaptive human-centered (AdHuC) representation based on local spatio-temporal features (LST) to address ARMI in a sequence of 3D point clouds. Our human-centered detector constructs affiliation regions to associate LST features with humans by mining depth data and using a cascade of rejectors to localize humans in 3D space. Then, features are detected within each affiliation region, which avoids extracting irrelevant features from dynamic background clutter and addresses moving cameras on mobile robots. Our feature descriptor is able to adapt its support region to linear perspective view variations and encode multi-channel information (i.e., color and depth) to construct the final representation. Empirical studies validate that the AdHuC representation obtains promising performance on ARMI using a Meka humanoid robot to play multi-people Simon Says games. Experiments on benchmark datasets further demonstrate that our adaptive human-centered representation outperforms previous approaches for activity recognition from color-depth data. {\textcopyright} 2015 IEEE.},
annote = {cited By 8},
author = {Zhang, H and Reardon, C and Zhang, C and Parker, L E},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2015.7139459},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {June},
pages = {1991--1998},
title = {{Adaptive human-centered representation for activity recognition of multiple individuals from 3D point cloud sequences}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938240196{\&}doi=10.1109{\%}2FICRA.2015.7139459{\&}partnerID=40{\&}md5=1487a4e32626d513e665613bc533caf3},
volume = {2015-June},
year = {2015}
}
@inproceedings{8578512,
abstract = {Existing single view, 3D face reconstruction methods can produce beautifully detailed 3D results, but typically only for near frontal, unobstructed viewpoints. We describe a system designed to provide detailed 3D reconstructions of faces viewed under extreme conditions, out of plane rotations, and occlusions. Motivated by the concept of bump mapping, we propose a layered approach which decouples estimation of a global shape from its mid-level details (e.g., wrinkles). We estimate a coarse 3D face shape which acts as a foundation and then separately layer this foundation with details represented by a bump map. We show how a deep convolutional encoder-decoder can be used to estimate such bump maps. We further show how this approach naturally extends to generate plausible details for occluded facial regions. We test our approach and its components extensively, quantitatively demonstrating the invariance of our estimated facial details. We further provide numerous qualitative examples showing that our method produces detailed 3D face shapes in viewing conditions where existing state of the art often break down.},
author = {Tran, A T and Hassner, T and Masi, I and Paz, E and Nirkin, Y and Medioni, G},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00414},
issn = {2575-7075},
keywords = {face recognition;image coding;image reconstruction,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {3935--3944},
title = {{Extreme 3D Face Reconstruction: Seeing Through Occlusions}},
year = {2018}
}
@inproceedings{8237379,
abstract = {3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Testing code will be made available online, along with pre-trained models http://aaronsplace.co.uk/papers/jackson2017recon},
annote = {27/04 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
27/04 Exclu{\'{i}}do (etapa 1)
27/08/2018 Revisado
27/08/2018 Exclu{\'{i}}do (etapa 1)},
archivePrefix = {arXiv},
arxivId = {1703.07834},
author = {Jackson, Aaron S. and Bulat, Adrian and Argyriou, Vasileios and Tzimiropoulos, Georgios},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.117},
eprint = {1703.07834},
isbn = {9781538610329},
issn = {15505499},
keywords = {computer vision,etaa1,face recognition,id338,ieeexplore,image reconstruct,poly,revisao{\_}ieeexplore},
mendeley-tags = {etaa1,id338,ieeexplore,poly,revisao{\_}ieeexplore},
pages = {1031--1039},
title = {{Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression}},
volume = {2017-Octob},
year = {2017}
}
@conference{Hammer2017,
abstract = {Today it is easily possible to generate dense point clouds of the sensor environment using 360° LiDAR (Light Detection and Ranging) sensors which are available since a number of years. The interpretation of these data is much more challenging. For the automated data evaluation the detection and classification of objects is a fundamental task. Especially in urban scenarios moving objects like persons or vehicles are of particular interest, for instance in automatic collision avoidance, for mobile sensor platforms or surveillance tasks. In literature there are several approaches for automated person detection in point clouds. While most techniques show acceptable results in object detection, the computation time is often crucial. The runtime can be problematic, especially due to the amount of data in the panoramic 360° point clouds. On the other hand, for most applications an object detection and classification in real time is needed. The paper presents a proposal for a fast, real-time capable algorithm for person detection, classification and tracking in panoramic point clouds. {\textcopyright} 2017 SPIE.},
annote = {cited By 1},
author = {Hammer, M and Hebel, M and Arens, M},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2278215},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Person detection and tracking with a 360° lidar system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446686{\&}doi=10.1117{\%}2F12.2278215{\&}partnerID=40{\&}md5=5843073ab0f3baf3e95aefb00d8747ea},
volume = {10434},
year = {2017}
}
@article{Zhang2019164,
abstract = {Face biometrics have achieved remarkable performance over the past decades, but unexpected spoofing of the static faces poses a threat to information security. There is an increasing demand for stable and discriminative biological modalities which are hard to be mimicked and deceived. Speech-driven 3D facial motion is a distinctive and measurable behavior-signature that is promising for biometrics. In this paper, we propose a novel 3D behaviometrics framework based on a “3D visual passcode” derived from speech-driven 3D facial dynamics. The 3D facial dynamics are jointly represented by 3D-keypoint-based measurements and 3D shape patch features, extracted from both static and speech-driven dynamic regions. An ensemble of subject-specific classifiers are then trained over selected discriminative features, which allows for a discriminant speech-driven 3D facial dynamics representation. We construct the first publicly available Speech-driven 3D Facial Motion dataset (S3DFM) that includes 2D-3D face video plus audio samples from 77 participants. The experimental results on the S3DFM show that the proposed pipeline achieves a face identification rate of 96.1{\%}. Detailed discussions are presented, concerning anti-spoofing, head pose variation, video frame rate, and applicability cases. We also give comparison with other baselines on “deep” and “shallow” 2D face features. {\textcopyright} 2019 Elsevier B.V.},
annote = {cited By 0},
author = {Zhang, J and Fisher, R B},
doi = {10.1016/j.sigpro.2019.02.025},
journal = {Signal Processing},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {164--177},
title = {{3D Visual passcode: Speech-driven 3D facial dynamics for behaviometrics}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062151576{\&}doi=10.1016{\%}2Fj.sigpro.2019.02.025{\&}partnerID=40{\&}md5=fd48929e8e498db458e84db847056125},
volume = {160},
year = {2019}
}
@inproceedings{8125644,
abstract = {3D reconstruction of human faces is of high importance in applications like forensics, facial features based human tracking and realistic reconstruction of human faces in virtual reality applications. The published algorithms so far require either a large number of views of a human face or a 3D facial reference model. This paper proposes a technique for 3D human face reconstruction using only five views without any reference model unlike most of the contemporary facial reconstruction techniques. Face localization is done followed by facial feature point extraction in the facial images. The features are tracked in the other images using the Kanade-Lucas-Tomasi (KLT) object tracking algorithm. 3D location of each feature is calculated in all the images using triangulation and a 3D point cloud is formed. The Point cloud is processed to remove outliers and holes. The 3D face is then formed by interpolating and meshing the point cloud. This computationally efficient and low cost technique outperforms commercial and online available software and published algorithms.},
author = {Kumar, A V and Prasad, V V R and Bhurchandi, K M and Satpute, V R and Pious, L and Kar, S},
booktitle = {2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)},
doi = {10.1109/CoDIT.2017.8125644},
keywords = {face recognition;feature extraction;image reconstr,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1185--1190},
title = {{Dense reconstruction of 3D human face using 5 images and no reference model}},
year = {2017}
}
@conference{Belghini2015317,
abstract = {In this paper, we propose a fuzzy similarity based classification approach for 3D face recognition. In the feature extraction method, we exploit curve concept to represent the 3D facial data, two types of curves was considered: depth-level and depth-radial curves. As the dimension of the obtained features is high, the problem 'curse of dimensionality' appears. To solve this problem, the Random Projection (RP) method was used. The proposed classifier performs Fuzzification operation using triangular membership functions for input data and ordered weighted averaging operators to measure similarity. Experiment was conducted using vrml files from 3D Database considering only one training sample per person. The obtained results are very promising for depth-level and depth-radial curves, besides the recognition rates are higher than 98{\%}. {\textcopyright} 2014 IEEE.},
annote = {cited By 1},
author = {Belghini, N and Ezghari, S and Zahi, A},
booktitle = {Colloquium in Information Science and Technology, CIST},
doi = {10.1109/CIST.2014.7016639},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {January},
pages = {317--322},
title = {{3D face recognition using facial curves, sparse random projection and fuzzy similarity measure}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938075627{\&}doi=10.1109{\%}2FCIST.2014.7016639{\&}partnerID=40{\&}md5=bca04cb8b40640821dbbc277a0cf6e71},
volume = {2015-Janua},
year = {2015}
}
@conference{Varney2015,
abstract = {LiDAR data is a set of geo-spatially located points which contain (X, Y, Z) location and intensity data. This paper presents the extraction of a novel set of volume and texture-based features from segmented point clouds. First, the data is segmented into individual object regions using an automatic seeded region growing technique. Then, these object regions are normalized to a N × N × N voxel space, where each voxel contains information about the location and density of points within that voxel. A set of volumetric features are extracted to represent the object region; these features include: 3D form factor, rotation invariant local binary pattern (RILBP), fill, stretch, corrugation, contour, plainness and relative variance. The form factor, fill, and stretch provide a series of meaningful relationships between the volume, surface area, and shape of the object. RILBP provides a textural description from the height variation of the LiDAR data. The corrugation, contour, and plainness are extracted by 3D Eigen analysis of the object volume to describe the details of the object's surface. Relative variance provides an illustration of the distribution of points throughout the object. The new feature set is robust, and scale and rotation invariant for object region classification. The performance of the proposed feature extraction technique has been evaluated on a set of segmented and voxelized point cloud objects in a subset of the aerial LiDAR data from Surrey, British Columbia, which was available through the Open Data Program. The volumetric features, when used as an input to an SVM classifier, correctly classified the object regions with an accuracy of 97.5 {\%}, with a focus on identifying five classes: ground, vegetation, buildings, vehicles, and barriers. {\textcopyright} 2014 IEEE.},
annote = {cited By 1},
author = {Varney, N M and Asari, V K},
booktitle = {Proceedings - Applied Imagery Pattern Recognition Workshop},
doi = {10.1109/AIPR.2014.7041941},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {February},
title = {{Volumetrie features for object region classification in 3D LiDAR point clouds}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937960702{\&}doi=10.1109{\%}2FAIPR.2014.7041941{\&}partnerID=40{\&}md5=ea1da5516de97b2a378ad09755e02a9a},
volume = {2015-Febru},
year = {2015}
}
@inproceedings{7910452,
abstract = {An animated character has its own characteristics and behaviour. The animator needs to be skilled enough to make a complex animation, especially on making face expression. a Well defined facial expression can represent the emotional condition and make the animation more expressing the mood. But most facial animation is done by manually, frame-by-frame. It is very time-consuming. This research proposed a combination methods for handling the facial animation based on marker location and face surface from the 3D character. The motion data captured based on the location and movement of the marker then implemented on 3D face model to generate the motion guidance. As a guidance, this marker data role as a centroid of a vertex cluster. The cluster provided by implementing segmentation fp-NN Clustering method based on surface and can visualize the deformation using linear blend skinning methods. The result from this research shows that this system can automatically generate facial animations based on the marker data and the surface segmentation. The visualization of deformation arranges accordingly to the motion captured data and organized sequentially.},
annote = {24/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
24/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Gunanto, Samuel Gandang and Hariadi, Mochamad and Yuniarno, Eko Mulyanto},
booktitle = {2016 2nd International Conference of Industrial, Mechanical, Electrical, and Chemical Engineering (ICIMECE)},
doi = {10.1109/ICIMECE.2016.7910452},
isbn = {978-1-5090-4161-9},
keywords = {computer animation,data visualisation,estela,etapa1,face recogni,id207,ieeexplore,revisao{\_}ieeexplore},
mendeley-tags = {estela,etapa1,id207,ieeexplore,revisao{\_}ieeexplore},
pages = {260--263},
publisher = {IEEE},
title = {{Computer facial animation with synthesize marker on 3D faces surface}},
url = {http://ieeexplore.ieee.org/document/7910452/},
year = {2016}
}
@article{7312454,
abstract = {In this paper, we present a novel approach to automatic 3D facial landmarking using 2D Gabor wavelets. Our algorithm considers the face to be a surface and uses map projections to derive 2D features from raw data. Extracted features include texture, relief map, and transformations thereof. We extend an established 2D landmarking method for simultaneous evaluation of these data. The method is validated by performing landmarking experiments on two data sets using 21 landmarks and compared with an active shape model implementation. On average, landmarking error for our method was 1.9 mm, whereas the active shape model resulted in an average landmarking error of 2.3 mm. A second study investigating facial shape heritability in related individuals concludes that automatic landmarking is on par with manual landmarking for some landmarks. Our algorithm can be trained in 30 min to automatically landmark 3D facial data sets of any size, and allows for fast and robust landmarking of 3D faces.},
author = {de Jong, M A and Wollstein, A and Ruff, C and Dunaway, D and Hysi, P and Spector, T and Liu, F and Niessen, W and Koudstaal, M J and Kayser, M and Wolvius, E B and B{\"{o}}hringer, S},
doi = {10.1109/TIP.2015.2496183},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Automated;Wavelet Analysis,Three-Dimensional;Pattern Recognition,face recognition;feature extraction;Gabor filters;,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
number = {2},
pages = {580--588},
title = {{An Automatic 3D Facial Landmarking Algorithm Using 2D Gabor Wavelets}},
volume = {25},
year = {2016}
}
@inproceedings{Vezzetti201624,
abstract = {A 3D automatic facial expression recognition procedure is presented in this work. The method is based on point-by-point mapping of seventeen Differential Geometry descriptors onto the probe facial depth map, which is then partitioned into seventy-nine regions. Then, features such as mean, median, mode, volumes, histograms are computed for each region and for each descriptor, to reach a varied large set of parameters representing the query face. Each set of parameters, given by a geometrical descriptor, a region, and a feature, form a trio, whose featuring numerical values are compared with appropriate thresholds, set via experimentation in a previous phase by processing a limited portion of the public facial Bosphorus database. This allows the identification of the emotion-based expression of the query 3D face among the six basic ones (anger, disgust, fear, joy, sadness, surprise). The algorithm was tested on the Bosphorus database and is suitable for applications in security, marketing, medical. The three-dimensional context has been preferred due to its invariance to different lightening/make-up/camouflage conditions.},
address = {Calgary,AB,Canada},
annote = {cited By 0
15/05/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
15/05/2018 Inclu{\'{i}}do (etapa 1)
12/07/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 2)
12/07/2018 Exclu{\'{i}}do (etapa 2)},
author = {Vezzetti, Enrico and Tornincasa, Stefano and Moos, Sandro and Marcolin, Federica and Violante, Maria Grazia and Speranza, Domenico and Buisan, David and Padula, Francesco},
booktitle = {Biomedical Engineering},
doi = {10.2316/P.2016.832-067},
isbn = {978-0-88986-981-3},
keywords = {estela,etapa1,etapa2,id478,revisao{\_}scopus,scopus},
mendeley-tags = {estela,etapa1,etapa2,id478,revisao{\_}scopus,scopus},
pages = {24--30},
publisher = {ACTAPRESS},
title = {{3D Human Face Analysis: Automatic Expression Recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015421390{\&}doi=10.2316{\%}2FP.2016.832-067{\&}partnerID=40{\&}md5=77c4f4a64cac89b59c02d9f7544acc5e http://www.actapress.com/PaperInfo.aspx?paperId=456184},
year = {2016}
}
@inproceedings{8538385,
abstract = {This study derived from a research focusing on 3D face recognition using ToF camera. But the system can't be used outdoors, because of a backlight. To solve this problem, a commercial digital single-lens reflex (DSLR) camera will be used. It can be approached y solving the stereo-view reconstruction problem for each pair of consecutive images. To reconstruct an object, projection matrix estimation from 2D point correspondences will be needed. The accuracy of 3D reconstruction is highly dependent on the corresponding points of 2D data projections from images to other images. In this research, The detectors are Harris-Stephens, SURF, FAST, Minimum Eigenvalue, and BRISK have been tested and analyzed through black box test. To evaluate feature detectors performance, the repeatability score for a given pair of images is computed. To do that it can use recall and precision. The best detector is the Harris Stephens detector because it has the best F-measure values of 0.46.},
author = {Kusnadi, A and {and R. Winantyo} and Pane, I Z},
booktitle = {2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)},
doi = {10.1109/ICSCEE.2018.8538385},
keywords = {F-measure,cameras;computer vision;eigenvalues and eigenfunct,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1--4},
title = {{Evaluation of Feature Detectors on Repeatability Quality of Facial Keypoints In Triangulation Method}},
year = {2018}
}
@inproceedings{Kempfle:2018:RRE:3266157.3266208,
address = {New York, NY, USA},
author = {Kempfle, Jochen and {Van Laerhoven}, Kristof},
booktitle = {Proceedings of the 5th International Workshop on Sensor-based Activity Recognition and Interaction},
doi = {10.1145/3266157.3266208},
isbn = {978-1-4503-6487-4},
keywords = {Kinect v2,ToF sensing,non-contact measurement,respiration measurement,respiratory rate,revisao{\_}acm,revisao{\_}scopus},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus},
pages = {4:1----4:10},
publisher = {ACM},
series = {iWOAR '18},
title = {{Respiration Rate Estimation with Depth Cameras: An Evaluation of Parameters}},
url = {http://doi.acm.org/10.1145/3266157.3266208},
year = {2018}
}
@conference{Lane2018,
abstract = {Multi-modal data fusion for situational awareness is of interest because fusion of data can provide more information than the individual modalities alone. However, many questions remain, including what data is beneficial, what algorithms work the best or are fastest, and where in the processing pipeline should data be fused? In this paper, we explore some of these questions through a processing pipeline designed for multi-modal data fusion in an autonomous UAV landing scenario. In this paper, we assess landing zone identification methods using two data modalities: hyperspectral imagery and LIDAR point clouds. Using hyperspectral image and LIDAR data from two datasets of Maui and a university campus, we assess the accuracies of different landing zone identification methods, compare rule-based and machine learning based classifications, and show that depending on the dataset, fusion does not always increase performance. However, we show that machine learning methods can be used to ascertain the usefulness of individual modalities and their resulting attributes when used to perform classification. {\textcopyright} COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
annote = {cited By 0},
author = {Lane, S and Kira, Z and James, R and Carr, D and Tuell, G},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2305136},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Landing zone identification for autonomous UAV applications using fused hyperspectral imagery and LIDAR point clouds}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050863160{\&}doi=10.1117{\%}2F12.2305136{\&}partnerID=40{\&}md5=c903d04c71626afc6ebbb9d747c8a92b},
volume = {10644},
year = {2018}
}
@conference{Sibbing20159,
abstract = {We introduce a new markerless 3D face tracking approach for 2D video streams captured by a single consumer grade camera. Our approach is based on tracking 2D features in the video and matching them with the projection of the corresponding feature points of a deformable 3D model. By this we estimate the initial shape and pose of the face. To make the tracking and reconstruction more robust we add a smoothness prior for pose changes as well as for deformations of the faces. Our major contribution lies in the formulation of the smooth deformation prior which we derive from a large database of previously captured facial animations showing different (dynamic) facial expressions of a fairly large number of subjects. We split these animation sequences into snippets of fixed length which we use to predict the facial motion based on previous frames. In order to keep the deformation model compact and independent from the individual physiognomy, we represent it by deformation gradients (instead of vertex positions) and apply a principal component analysis in deformation gradient space to extract the major modes of facial deformation. Since the facial deformation is optimized during tracking, it is particularly easy to apply them to other physiognomies and thereby re-target the facial expressions. We demonstrate the effectiveness of our technique on a number of examples. {\textcopyright} 2015 The Eurographics Association.},
annote = {cited By 0},
author = {Sibbing, D and Kobbelt, L},
booktitle = {VMV 2015 - Vision, Modeling and Visualization},
doi = {10.2312/vmv.2015125},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {9--16},
title = {{Data driven 3D face tracking based on a facial deformation model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018329328{\&}doi=10.2312{\%}2Fvmv.2015125{\&}partnerID=40{\&}md5=7f879795487cd2b31b323608dc0a286a},
year = {2015}
}
@inproceedings{8634685,
abstract = {Mesh upsampling and morphing is a challenging problem due to the irregularity and sparseness of the 3D data. Unlike 2D grid of pixels, 3D points do not have any regular structure and spatial order. In this paper, we present an efficient mesh upsampling and morphing technique. The proposed technique does not not require training and does not rely on any particular upsampling model. The key idea is to select and process each mesh triangle based on a heuristic criteria to define the 3D coordinate of a new point. An interactive mesh morphing technique is also introduced to test the effectiveness of the mesh upsampling algorithm. We perform quantitative and qualitative analysis to evaluate the performance of our proposed technique. Our empirical results show that our upsampled points have better uniformity and are located closer to the underlying surfaces. The computational time analysis demonstrates that the proposed technique is very efficient. The average mesh upsampling time is only 2.11sec which makes the proposed technique suitable for real time applications. To further demonstrate the effectiveness of our proposed technique, we evaluate it for a novel task of facial rejuvenation prediction and report our preliminary results in this paper.},
author = {{Ali Shah}, S A and Bennamoun, M and Molton, M},
booktitle = {2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)},
doi = {10.1109/IVCNZ.2018.8634685},
issn = {2151-2205},
keywords = {face recognition;image morphing;interactive system,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
pages = {1--6},
title = {{A Training-Free Mesh Upsampling and Morphing Technique for 3D Face Rejuvenation}},
year = {2018}
}
@conference{Macher2016667,
abstract = {Laser scanners are widely used for the modelling of existing buildings and particularly in the creation process of as-built BIM (Building Information Modelling). However, the generation of as-built BIM from point clouds involves mainly manual steps and it is consequently time consuming and error-prone. Along the path to automation, a three steps segmentation approach has been developed. This approach is composed of two phases: A segmentation into sub-spaces namely floors and rooms and a plane segmentation combined with the identification of building elements. In order to assess and validate the developed approach, different case studies are considered. Indeed, it is essential to apply algorithms to several datasets and not to develop algorithms with a unique dataset which could influence the development with its particularities. Indoor point clouds of different types of buildings will be used as input for the developed algorithms, going from an individual house of almost one hundred square meters to larger buildings of several thousand square meters. Datasets provide various space configurations and present numerous different occluding objects as for example desks, computer equipments, home furnishings and even wine barrels. For each dataset, the results will be illustrated. The analysis of the results will provide an insight into the transferability of the developed approach for the indoor modelling of several types of buildings.},
annote = {cited By 3},
author = {Macher, H and Landes, T and Grussenmeyer, P},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprsarchives-XLI-B5-667-2016},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {667--674},
title = {{Validation of point clouds segmentation algorithms through their application to several case studies for indoor building modelling}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979211453{\&}doi=10.5194{\%}2Fisprsarchives-XLI-B5-667-2016{\&}partnerID=40{\&}md5=b571d5617308ef12d724c3da427f8bfc},
volume = {41},
year = {2016}
}
@conference{AlQahtani201828,
abstract = {Head-pose estimation is a crucial component for analysing human behaviour through various 2D and 3D applications. However, the usage of the strategies based on 2D technologies is not very effective, as the sources of data are limited. In contrast, the usage of the strategies based on 3D technologies is a promising area. The 3D HPE methods are also imperfect, especially when they are applied in situations of inconsistent illumination or occlusion. An analysis of related works helps to establish an appropriate framework for the current research, and this paper extends previous work by creating an algorithm that further improves the framework. The method relies upon a stereo camera arrangement and utilises a method to detect and track key landmark points of the human face to evaluate improvement in 3D-head-pose estimation. {\textcopyright} 2018 ACM.},
annote = {cited By 0},
author = {AlQahtani, F and Banks, J and Chandran, V and Zhang, J},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3220511.3220522},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {28--35},
title = {{Three-dimensional head pose estimation using a stereo camera arrangement}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053684284{\&}doi=10.1145{\%}2F3220511.3220522{\&}partnerID=40{\&}md5=b0d1c7a7d932c95512b13e71cede2a15},
volume = {Part F1377},
year = {2018}
}
@conference{Chang20181599,
abstract = {We show how a simple convolutional neural network (CNN) can be trained to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose, directly from image intensities. We further explain how this FacePoseNet (FPN) can be used to align faces in 2D and 3D as an alternative to explicit facial landmark detection for these tasks. We claim that in many cases the standard means of measuring landmark detector accuracy can be misleading when comparing different face alignments. Instead, we compare our FPN with existing methods by evaluating how they affect face recognition accuracy on the IJB-A and IJB-B benchmarks: using the same recognition pipeline, but varying the face alignment method. Our results show that (a) better landmark detection accuracy measured on the 300W benchmark does not necessarily imply better face recognition accuracy. (b) Our FPN provides superior 2D and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces at a small fraction of the computational cost of comparably accurate landmark detectors. For many purposes, FPN is thus a far faster and far more accurate face alignment method than using facial landmark detectors. {\textcopyright} 2017 IEEE.},
annote = {cited By 11},
author = {Chang, F.-J. and Tran, A T and Hassner, T and Masi, I and Nevatia, R and Medioni, G},
booktitle = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
doi = {10.1109/ICCVW.2017.188},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {1599--1608},
title = {{FacePoseNet: Making a Case for Landmark-Free Face Alignment}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046252824{\&}doi=10.1109{\%}2FICCVW.2017.188{\&}partnerID=40{\&}md5=c5a847d100291fd1aa9c96b8a6885c46},
volume = {2018-Janua},
year = {2018}
}
@article{Dai2018939,
abstract = {3D face reconstruction from multi-view video sequences has become a hotspot in computer vision for the last decades. Structure from Motion (SfM) methods, which have been widely used for multi-view 3D face reconstruction, have two main limitations. First, self-occlusion causes certain facial feature points (FFPs) to be invisible in the images, which will lead to missing data. The existing SfM methods could recover the missing data through iterative calculation, however, with high computational costs and long processing time. Second, the SfM methods cannot reconstruct the accurate 3D facial shapes of cheeks because there are no FFPs in this area. This paper proposes a novel “coarse-to-fine” multi-view 3D face reconstruction method by taking the advantage of the complementarity between FFPs and occluding contours, i.e., the boundary lines depicted between the facial region and the background. In this method, a block SfM algorithm is firstly proposed to reconstruct a “coarse” 3D facial shape by utilizing sparse FFPs. The block SfM algorithm does not estimate the true locations of the self-occluded FFPs iteratively. Thus, the computational cost is significantly reduced. Then, a kernel partial least squares (KPLS) algorithm is introduced to refine the “coarse” 3D facial shape. The KPLS method applies occluding contours to remedy the limitation of sparse FFPs correspondence-based SfM method. The proposed method is evaluated on the synthetic sequences generated from the BJUT-3D face database and the real-world multi-view video sequences obtained in a controlled indoor environment. The results show improvements in both accuracy and efficiency. {\textcopyright} 2017, Springer Science+Business Media New York.},
annote = {cited By 1},
author = {Dai, P and Wang, X and Zhang, W},
doi = {10.1007/s11042-016-4325-y},
journal = {Multimedia Tools and Applications},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {1},
pages = {939--966},
title = {{Coarse-to-fine multiview 3d face reconstruction using multiple geometrical features}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008517643{\&}doi=10.1007{\%}2Fs11042-016-4325-y{\&}partnerID=40{\&}md5=dd454616b801c79f2d39e615445e75ca},
volume = {77},
year = {2018}
}
@article{7045610,
abstract = {This paper presents estimation of head pose angles from a single 2D face image using a 3D face model morphed from a reference face model. A reference model refers to a 3D face of a person of the same ethnicity and gender as the query subject. The proposed scheme minimizes the disparity between the two sets of prominent facial features on the query face image and the corresponding points on the 3D face model to estimate the head pose angles. The 3D face model used is morphed from a reference model to be more specific to the query face in terms of the depth error at the feature points. The morphing process produces a 3D face model more specific to the query image when multiple 2D face images of the query subject are available for training. The proposed morphing process is computationally efficient since the depth of a 3D face model is adjusted by a scalar depth parameter at feature points. Optimal depth parameters are found by minimizing the disparity between the 2D features of the query face image and the corresponding features on the morphed 3D model projected onto 2D space. The proposed head pose estimation technique was evaluated on two benchmarking databases: 1) the USF Human-ID database for depth estimation and 2) the Pointing'04 database for head pose estimation. Experiment results demonstrate that head pose estimation errors in nodding and shaking angles are as low as 7.93° and 4.65° on average for a single 2D input face image.},
author = {Kong, S G and Mbouna, R O},
doi = {10.1109/TIP.2015.2405483},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {estimation theory;face recognition;image morphing;,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
number = {6},
pages = {1801--1808},
title = {{Head Pose Estimation From a 2D Face Image Using 3D Face Morphing With Depth Parameters}},
volume = {24},
year = {2015}
}
@inproceedings{7961793,
abstract = {We propose a method to transfer both head poseand facial expression of a source person in a video to the faceof a target person in an output video. Our method models theentire 2D frame instead of the 3D face, and it generates outputresults using a status score, which includes the relative facialstatus about the head pose and expression in a frame. From thetarget video, the learning process obtains frame features neededfor moving to each frame from the neutral frame for all frames,and generates the basis of these features via principal componentanalysis (PCA). Then, it learns to generate these features from agiven status score sequentially. In the transfer process, it obtainsa status score from a source frame of the video and generatesthe features from the given status score. Then, it generates theoutput frame using the reconstructed features. An output videois generated by repeating these steps for each source frame.Our method generates output results on the trajectory of thetarget video by using the advantage of PCA. Therefore, in theoutput results generated by our methods, both head pose andexpression are transferred correctly while the non-face regionsof the frames are supported. Finally, we experimentally comparethe effectiveness of our method and conventional methods.},
author = {Hosoi, T},
booktitle = {2017 12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)},
doi = {10.1109/FG.2017.142},
keywords = {face recognition;learning (artificial intelligence,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {573--580},
title = {{Head Pose and Expression Transfer Using Facial Status Score}},
year = {2017}
}
@inproceedings{Zhou:2008:FRA:2381112.2381125,
address = {Aire-la-Ville, Switzerland, Switzerland},
author = {Zhou, Xuebing and Seibert, Helmut and Busch, Christoph and Funk, Wolfgang},
booktitle = {Proceedings of the 1st Eurographics Conference on 3D Object Retrieval},
doi = {10.2312/3DOR/3DOR08/065-071},
isbn = {978-3-905674-05-7},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {65--71},
publisher = {Eurographics Association},
series = {3DOR '08},
title = {{A 3D Face Recognition Algorithm Using Histogram-based Features}},
url = {http://dx.doi.org/10.2312/3DOR/3DOR08/065-071},
year = {2008}
}
@inproceedings{8500537,
abstract = {Facial landmark localization is a crucial initial step Driver Inattention Monitoring. The aim of this paper is to localize driver facial landmarks across large rotation, say [-90°, +90°] in yaw rotation, to cope with real driving conditions. The paper proposes a flexible pipe-line for creating automatically labeled face image to supply wanted dataset. The benefits of CG (Computer Graphics) techniques such as 3D face modelling and morphing, photorealistic rendering and ground truth generation are utilized. To the best of our knowledge this is the first time to combine CG rendering and automatic ground truth labelling techniques with face landmark localization algorithms. The effectiveness of the CG rendered data is proved by cross validation with Multi-PIE dataset. Landmark localization across large rotation is obtained by a system simply integrating the off the-shelves algorithms and trained with the CG rendered data. The experiments of the implemented system on Multi-PIE and real persons show that it could localize facial landmarks across large rotation accurately and in real time.},
author = {Shi, L and Yue, J and Dong, Y and Lin, M and Wang, S and Shen, R and Chang, Z},
booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2018.8500537},
issn = {1931-0587},
keywords = {driver information systems;face recognition;render,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1995--2002},
title = {{CG Benefited Driver Facial Landmark Localization Across Large Rotation}},
year = {2018}
}
@inproceedings{Xie:2016:IFR:3028842.3028853,
address = {New York, NY, USA},
author = {Xie, Lanchi and Xu, Lei and Zhang, Ning and Guo, Jingjing and Yan, Yuwen and Li, Zhihui and Li, Zhigang and Xu, Xiaojing},
booktitle = {Proceedings of the 2016 International Conference on Intelligent Information Processing},
doi = {10.1145/3028842.3028853},
isbn = {978-1-4503-4799-0},
keywords = {face recognition,reranking,revisao{\_}acm,revisao{\_}scopus,shape contexts,shape matching,similarity calculation},
mendeley-tags = {revisao{\_}acm,revisao{\_}scopus},
pages = {11:1----11:6},
publisher = {ACM},
series = {ICIIP '16},
title = {{Improved Face Recognition Result Reranking Based on Shape Contexts}},
url = {http://doi.acm.org/10.1145/3028842.3028853},
year = {2016}
}
@conference{Mayr2018691,
abstract = {To date multi-temporal 3D point clouds from close-range sensing are used for landslide and erosion monitoring in an operational manner. Morphological changes are typically derived by calculating distances between points from different acquisition epochs. The identification of the underlying processes resulting in surface changes, however, is often challenging, for example due to the complex surface structures and influences from seasonal vegetation dynamics. We present an approach for object-based 3D landslide monitoring based on topographic LiDAR point cloud time series separating specific surface change types automatically. The workflow removes vegetation and relates surface changes derived from a point cloud time series directly to (i) geomorphological object classes (landslide scarp, eroded area, deposit) and (ii) to individual, spatially contiguous objects (such as parts of the landslide scarp and clods of material moving in the landslide). We apply this approach to a time series of nine point cloud epochs from a slope affected by two shallow landslides. A parameter test addresses the influence of the registration error and the associated level of detection on the magnitude of derived object changes. The results of our case study are in accordance with field observations at the test site as well as conceptual landslide models, where retrogressive erosion of the scarp and downslope movement of the sliding mass are major principles of secondary landslide development. We conclude that the presented methods are well suited to extract information on geomorphological process dynamics from the complex point clouds and aggregate it at different levels of abstraction to assist landslide and erosion assessment. {\textcopyright} Authors 2018. CC BY 4.0 License.},
annote = {cited By 0},
author = {Mayr, A and Rutzinger, M and Geitner, C},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-2-691-2018},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {2},
pages = {691--697},
title = {{Multitemporal analysis of objects in 3D point clouds for landslide monitoring}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048356667{\&}doi=10.5194{\%}2Fisprs-archives-XLII-2-691-2018{\&}partnerID=40{\&}md5=54120583b761740bb74e66a716ea4370},
volume = {42},
year = {2018}
}
@inproceedings{8451742,
abstract = {This paper presents a method for reconstructing 3D face expressions from monocular video sequences. Unlike previous approaches we don't require any prior face models, nor a large collection of images with diverse variation of poses and illuminations. Instead, we leverage a monocular video sequence without any restrictions. We formulate the 3D face reconstruction as an energy minimization problem integrated with dense optical flow variation, as rigid as possible(ARAP) constraint, spatial and temporal constraints. This paper offers the first dense optical flow variational approach to the problem of 3D reconstruction of non-rigid face expressions from a monocular video. Dense optical flow variation cost substitutes for photo consistency cost to enhance the reconstruction of exaggerated expressions. A generic 3D face template mesh and a simple 3D warping algorithm allow us to reconstruct a true 3D face mesh, relax the constraints of diverse views or illuminations and also avoid the dependency of the quality of prior face models, such as the facial expressions or face races varieties limitation. Finally, we use a per-pixel shape-from-shading(SFS) algorithm to estimate the fine-scale geometry details such as wrinkles to further improve the reconstruction fidelity. Given unconstrained monocular RGB videos, our method reconstructs wrinkle-level 3D face model, without the need for any prior models or diverse capture conditions.},
author = {Wang, S and Shen, X and Liu, J},
booktitle = {2018 25th IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2018.8451742},
issn = {2381-8549},
keywords = {emotion recognition;face recognition;image reconst,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {2665--2669},
title = {{Dense Optical Flow Variation Based 3D Face Reconstruction from Monocular Video}},
year = {2018}
}
@conference{Srinivasan2015,
abstract = {Face recognition system has emerged as an important field in case of surveillance systems. Since three-dimensional imaging systems have reached a notable growth, we consider the 3D image for face recognition. Occlusion (extraneous objects that hinder face recognition, e.g., scarf, glass, beard etc.,) is one of the greatest challenges in face recognition systems. Other issues are illumination, pose, scale etc., an innovative three dimensional occlusion detection and restoration strategy for the recognition of three dimensional faces partially occluded by unforeseen objects is presented. Normalization provides orientation of the image to frontal view since we require frontal position for face recognition. An efficient method is used for detection of occlusions, which specifies the missing information in the occluded face. A restoration method then eliminates occlusion and renders a restored facial image. It exploits the information provided by the non-occluded part of the face to recover the original face. Restored faces are then applied to a suitable face recognition system. The proposed system will provide better accuracy to eliminate the occlusion and restored facial information method is independent of the face recognition method. {\textcopyright} 2014 IEEE.},
annote = {cited By 4},
author = {Srinivasan, A and Balamurugan, V},
booktitle = {IEEE Region 10 Annual International Conference, Proceedings/TENCON},
doi = {10.1109/TENCON.2014.7022477},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Occlusion detection and image restoration in 3D face image}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940493685{\&}doi=10.1109{\%}2FTENCON.2014.7022477{\&}partnerID=40{\&}md5=59b85a1c61b18d769cde95762474d1c0},
volume = {2015-Janua},
year = {2015}
}
@inproceedings{8100063,
abstract = {3D Morphable Models (3DMMs) are powerful statistical models of 3D facial shape and texture, and among the state-of-the-art methods for reconstructing facial shape from single images. With the advent of new 3D sensors, many 3D facial datasets have been collected containing both neutral as well as expressive faces. However, all datasets are captured under controlled conditions. Thus, even though powerful 3D facial shape models can be learnt from such data, it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions (in-the-wild). In this paper, we propose the first, to the best of our knowledge, in-the-wild 3DMM by combining a powerful statistical model of facial shape, which describes both identity and expression, with an in-the-wild texture model. We show that the employment of such an in-the-wild texture model greatly simplifies the fitting procedure, because there is no need to optimise with regards to the illumination parameters. Furthermore, we propose a new fast algorithm for fitting the 3DMM in arbitrary images. Finally, we have captured the first 3D facial database with relatively unconstrained conditions and report quantitative evaluations with state-of-the-art performance. Complementary qualitative reconstruction results are demonstrated on standard in-the-wild facial databases.},
author = {Booth, J and Antonakos, E and Ploumpis, S and Trigeorgis, G and Panagakis, Y and Zafeiriou, S},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.580},
issn = {1063-6919},
keywords = {emotion recognition;face recognition;image morphin,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {5464--5473},
title = {{3D Face Morphable Models "In-the-Wild"}},
year = {2017}
}
@inproceedings{Berretti:2011:PMF:2381170.2381192,
address = {Aire-la-Ville, Switzerland, Switzerland},
author = {Berretti, Stefano and {Del Bimbo}, Alberto and Pala, Pietro},
booktitle = {Proceedings of the 4th Eurographics Conference on 3D Object Retrieval},
doi = {10.2312/3DOR/3DOR11/117-120},
isbn = {978-3-905674-31-6},
keywords = {revisao{\_}acm},
mendeley-tags = {revisao{\_}acm},
pages = {117--120},
publisher = {Eurographics Association},
series = {3DOR '11},
title = {{Partial Match of 3D Faces Using Facial Curves Between SIFT Keypoints}},
url = {http://dx.doi.org/10.2312/3DOR/3DOR11/117-120},
year = {2011}
}
@inproceedings{8272746,
abstract = {In this paper we introduce the concept of correlating genetic variations in an individual's specific genetic code (DNA) and facial morphology. This is the first step in the research effort to estimate facial appearance from DNA samples, which is gaining momentum within intelligence, law enforcement and national security communities. The dataset for the study consisting of genetic data and 3D facial scans (phenotype) data was obtained through the FaceBase Consortium. The proposed approach has three main steps: phenotype feature extraction from 3D face images, genotype feature extraction from a DNA sample, and genome-wide association analysis to determine genetic variations that contribute to facial structure and appearance. Results indicate that there exist significant correlations between genetic information and facial structure. We have identified 30 single nucleotide polymorphisms (SNPs), i.e. genetic variations, that significantly contribute to facial structure and appearance. We conclude with a preliminary attempt at facial reconstruction from the genetic data and emphasize on the complexity of the problem and the challenges encountered.},
author = {Srinivas, N and Tokola, R and Mikkilineni, A and Nookaew, I and Leuze, M and Boehnen, C},
booktitle = {2017 IEEE International Joint Conference on Biometrics (IJCB)},
doi = {10.1109/BTAS.2017.8272746},
issn = {2474-9699},
keywords = {biology computing;DNA;face recognition;feature ext,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {590--598},
title = {{DNA2FACE: An approach to correlating 3D facial structure and DNA}},
year = {2017}
}
@inproceedings{Drira:2010:ERC:1877808.1877824,
address = {New York, NY, USA},
author = {Drira, Hassen and Amor, Boulbaba Ben and Daoudi, Mohamed and Srivastava, Anuj},
booktitle = {Proceedings of the ACM Workshop on 3D Object Retrieval},
doi = {10.1145/1877808.1877824},
isbn = {978-1-4503-0160-2},
keywords = {3D face recognition,revisao{\_}acm,riemannian geometry,shape analysis},
mendeley-tags = {revisao{\_}acm},
pages = {75--80},
publisher = {ACM},
series = {3DOR '10},
title = {{Elastic Radial Curves to Model 3D Facial Deformations}},
url = {http://doi.acm.org/10.1145/1877808.1877824},
year = {2010}
}
@conference{Kopinski2017,
abstract = {We present a publicly available benchmark database for the problem of hand posture recognition from noisy depth data and fused RGB-D data obtained from low-cost time-of-flight (ToF) sensors. The database is the most extensive database of this kind containing over a million data samples (point clouds) recorded from 35 different individuals for ten different static hand postures. This captures a great amount of variance, due to person-related factors, but also scaling, translation and rotation are explicitly represented. Benchmark results achieved with a standard classification algorithm are computed by cross-validation both over samples and persons, the latter implying training on all persons but one and testing on the remaining one. An important result using this database is that cross-validation performance over samples (which is the standard procedure in machine learning) is systematically higher than cross-validation performance over persons, which is to our mind the true application-relevant measure of generalization performance. {\textcopyright} 2016 IEEE.},
annote = {cited By 5},
author = {Kopinski, T and Gepperth, A and Handmann, U},
booktitle = {2016 14th International Conference on Control, Automation, Robotics and Vision, ICARCV 2016},
doi = {10.1109/ICARCV.2016.7838613},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{A time-of-flight-based hand posture database for human-machine interaction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010079572{\&}doi=10.1109{\%}2FICARCV.2016.7838613{\&}partnerID=40{\&}md5=f33679e9100a5d979fbe8e4d38009fa2},
year = {2017}
}
@inproceedings{7984642,
abstract = {Super-resolution face image acquisition system is indispensable in people's life. Under the condition of low illumination, the illumination environment difference is too big or the light is insufficient, which leads to the traditional image acquisition system can not collect the high quality face image, and the limitation is poor. Based on open source computer vision library (OpenCV) in the C++ environment configuration, the use of Three Dimension (3D) face recognition technology algorithm, design a set of low illumination conditions of the super resolution face image acquisition system. Experiments show that the design scheme with real-time focusing speed), fast (single acquisition 0.05 seconds), accurate (facial recognition rate of 99.3{\%}) etc. characteristics, be able to fully meet the needs of low illumination conditions for super-resolution of face image acquisition.},
author = {And and And},
booktitle = {2017 2nd International Conference on Image, Vision and Computing (ICIVC)},
doi = {10.1109/ICIVC.2017.7984642},
keywords = {computer vision;face recognition;image capture;ima,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {680--683},
title = {{Design and implementation of high resolution face image acquisition system under low illumination based on the open source computer vision library}},
year = {2017}
}
@conference{Yao20171001,
abstract = {The application of LiDAR data in forestry initially focused on mapping forest community, particularly and primarily intended for large-scale forest management and planning. Then with the smaller footprint and higher sampling density LiDAR data available, detecting individual tree overstory, estimating crowns parameters and identifying tree species are demonstrated practicable. This paper proposes a section-based protocol of tree species identification taking palm tree as an example. Section-based method is to detect objects through certain profile among different direction, basically along X-axis or Y-axis. And this method improve the utilization of spatial information to generate accurate results. Firstly, separate the tree points from manmade-object points by decision-tree-based rules, and create Crown Height Mode (CHM) by subtracting the Digital Terrain Model (DTM) from the digital surface model (DSM). Then calculate and extract key points to locate individual trees, thus estimate specific tree parameters related to species information, such as crown height, crown radius, and cross point etc. Finally, with parameters we are able to identify certain tree species. Comparing to species information measured on ground, the portion correctly identified trees on all plots could reach up to 90.65{\%}. The identification result in this research demonstrate the ability to distinguish palm tree using LiDAR point cloud. Furthermore, with more prior knowledge, section-based method enable the process to classify trees into different classes. {\textcopyright} Authors 2017. CC BY 4.0 License.},
annote = {cited By 0},
author = {Yao, C and Zhang, X and Liu, H},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-2-W7-1001-2017},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {2W7},
pages = {1001--1007},
title = {{Section-based tree species identification using airborne LiDAR point cloud}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030988804{\&}doi=10.5194{\%}2Fisprs-archives-XLII-2-W7-1001-2017{\&}partnerID=40{\&}md5=1f95514ee02f264362120c9dbae444d6},
volume = {42},
year = {2017}
}
@conference{Echeagaray-Patrón2015,
abstract = {Face recognition is an important task in pattern recognition and computer vision. In this work a method for 3D face recognition in the presence of facial expression and poses variations is proposed. The method uses 3D shape data without color or texture information. A new matching algorithm based on conformal mapping of original facial surfaces onto a Riemannian manifold followed by comparison of conformal and isometric invariants computed in the manifold is suggested. Experimental results are presented using common 3D face databases that contain significant amount of expression and pose variations. {\textcopyright} 2015 SPIE.},
annote = {cited By 14},
author = {Echeagaray-Patr{\'{o}}n, B A and Kober, V},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2186695},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{3D face recognition based on matching of facial surfaces}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951325808{\&}doi=10.1117{\%}2F12.2186695{\&}partnerID=40{\&}md5=cb5a757e08c6a66683fc0706f09faf95},
volume = {9598},
year = {2015}
}
@inproceedings{Galbally:2016:BSI:2982636.2982657,
abstract = {Biometric systems typically suffer a significant loss of performance when the acquisition sensor is changed between enrolment and authentication. Such a problem, commonly known as sensor interoperability, poses a serious challenge to the accuracy of matching algorithms. The present work addresses for the first time the sensor interoperability issue in 3D face recognition systems, analysing the performance of two popular and well known techniques for 3D facial authentication. For this purpose, a new gender-balanced database comprising 3D data of 26 subjects has been acquired using two devices belonging to the new generation of low-cost 3D sensors. The results show the high sensor-dependency of the tested systems and the need to develop matching algorithms robust to the variation in the sensor resolution.},
address = {Portugal},
annote = {13/04/2018 Em processo de sele{\c{c}}{\~{a}}o (etapa 1)
13/04/2018 Exclu{\'{i}}do (etapa 1)},
author = {Galbally, Javier and Satta, Riccardo},
booktitle = {Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods},
doi = {10.5220/0005682501990204},
isbn = {978-989-758-173-1},
keywords = {3D Face Database.,3D Face Recognition,Interoperability,acm,etapa1,gil,id51,revisao{\_}scopus},
mendeley-tags = {acm,etapa1,gil,id51,revisao{\_}scopus},
pages = {199--204},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
series = {ICPRAM 2016},
title = {{Biometric Sensor Interoperability: A Case Study in 3D Face Recognition}},
url = {http://dx.doi.org/10.5220/0005682501990204},
year = {2016}
}
@inproceedings{8095315,
abstract = {It is not only interesting to predict how an individual of a relatively young age will look in the future but also to reconstruct the facial appearance in the past during childhood. It can be even more desirable when different circumstances, behavior and lifestyle and their impacts on the facial shape appearance as a consequence are taken into account. Such may be applicable for many practical reasons in healthcare, forensics psychology, missing people and children, etc. This paper presents the 3D Face Time Machine Matrix (FT2M), a 3D Dynamic Shape Model which is a fusion of two models of ageing and rejuvenation with facial shape variations due to lifestyle and behavioral factors. This dynamic model is learned from a database of three dimensional facial images which is built by ten individual age groups between 3 to 75 years old. 3D facial aging modeling is a complex process since it affects both the shape and texture of the face. We propose a Dynamic face model to transform the given input face to his youthful or adulthood appearance by taking into account his lifestyle and behavioral traits and the probable changes may occur in perceptible appearance by altering its shape and texture simultaneously.},
author = {Heravi, F M Z and Nait-Ali, A},
booktitle = {2017 2nd International Conference on Bio-engineering for Smart Technologies (BioSMART)},
doi = {10.1109/BIOSMART.2017.8095315},
keywords = {face recognition;image texture;shape recognition;3,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--5},
title = {{A 3D dynamic shape model to simulate rejuvenation ageing trajectory of 3D face images}},
year = {2017}
}
@inproceedings{8267812,
abstract = {The human face has a unique shape and size, as well as a 3D character face model. During this process of animated facial expression of 3D virtual characters are mostly still done manually by moving the rig in each frame. The more characters used, the more production costs incurred. The absence of a cheap facial motion transfer system is also one of the reasons why not many studios are using motion capture technology in Indonesia. This study will evaluate the implementation of radial basis function (RBF) as a method of marker-transfer used as a reference to rig movement in the facial animation system. Testing is done by performing variations of radial function, namely: Gaussian, Inverse Quadratic, Inverse Multiquadric, and Multiquadric. The value of epsilon used is 0.01. The experimental results show that the range of feature point shifts generated by RBF Gaussian, Inverse Multiquadric, Inverse Quadratic, and Multiquadric have the same pattern with the difference in the distance of the marker rigging point shift on the target 3D model. The farthest range of differences is generated by RBF Gaussian. The resulting maximum range difference can reach 92.37{\%} and a minimum of 2.47{\%} compared to other methods. This appears on the Gaussian chart to have a sharper pattern. As with the concept of exaggeration, the Gaussian method which has the maximum range is the right method to apply the exaggeration principle in 3D face-expression animation.},
author = {Sulistiyono, A and Atmani, A K P and Gunanto, S G},
booktitle = {2017 International Conference on Smart Cities, Automation Intelligent Computing Systems (ICON-SONICS)},
doi = {10.1109/ICON-SONICS.2017.8267812},
keywords = {computer animation;face recognition;Gaussian proce,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
pages = {1--5},
title = {{Toward to an exaggeration engine for facial animation: Evaluating the difference of RBF implementation in expression-marker transfer}},
year = {2017}
}
@article{Minga2016117,
abstract = {In this paper, we design a unified 3D face authentication system for practical use. First, we propose afacial depth recovery method to construct a facial depth map from stereoscopic videos. It effectivelyutilize prior facial information and incorporate the visibility term to classify static and dynamic pixels forrobust depth estimation. Secondly, in order to make 3D face authentication more accurate and consistent,we present an intrinsic scale feature detection for interesting points on 3D facial mesh regions.Then, a novel feature descriptor is proposed, called Local Mesh Scale-Invariant Feature Transform(LMSIFT) to reflect the different face recognition abilities in different facial regions. Finally, the sparseoptimization problem of visual codebook is used to 3D face learning. We evaluate our approach onpublicly available 3D face databases and self-collected realistic scene databases. We also develop aninteractive education system to investigate its performance in practice, which demonstrates the highperformance of the proposed approach for accurate 3D face authentication. Compared with previouspopular approaches, our system has consistently better performance in terms of effectiveness, robustnessand universality. {\textcopyright} 2015 Elsevier B.V..},
annote = {cited By 5},
author = {Minga, Y and Hong, X},
doi = {10.1016/j.neucom.2015.07.127},
journal = {Neurocomputing},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {117--130},
title = {{A unified 3D face authentication framework based on robust localmesh SIFT feature}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975516142{\&}doi=10.1016{\%}2Fj.neucom.2015.07.127{\&}partnerID=40{\&}md5=8ff10332acc80f01c907d3c49c1da439},
volume = {184},
year = {2016}
}
@inproceedings{7163142,
abstract = {To enable real-time, person-independent 3D registration from 2D video, we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60 degrees. From a single 2D image of a person's face, a dense 3D shape is registered in real time for each frame. The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression. The algorithm first estimates the location of a dense set of markers and their visibility, then reconstructs face shapes by fitting a part-based 3D model. Because no assumptions are required about illumination or surface properties, the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video. The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction. Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video. The software is available online at http://zface.org.},
author = {Jeni, L A and Cohn, J F and Kanade, T},
booktitle = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
doi = {10.1109/FG.2015.7163142},
keywords = {face recognition;image reconstruction;image regist,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {1--8},
title = {{Dense 3D face alignment from 2D videos in real-time}},
volume = {1},
year = {2015}
}
@article{Woodward201761,
abstract = {This work presents a robust, and low-cost framework for real-time marker based 3-D human expression modeling using off-the-shelf stereo web-cameras and inexpensive adhesive markers applied to the face. The system has low computational requirements, runs on standard hardware, and is portable with minimal set-up time and no training. It does not require a controlled lab environment (lighting or set-up) and is robust under varying conditions, i.e. illumination, facial hair, or skin tone variation. Stereo web-cameras perform 3-D marker tracking to obtain head rigid motion and the non-rigid motion of expressions. Tracked markers are then mapped onto a 3-D face model with a virtual muscle animation system. Muscle inverse kinematics update muscle contraction parameters based on marker motion in order to create a virtual character's expression performance. The parametrization of the muscle-based animation encodes a face performance with little bandwidth. Additionally, a radial basis function mapping approach was used to easily remap motion capture data to any face model. In this way the automated creation of a personalized 3-D face model and animation system from 3-D data is elucidated. The expressive power of the system and its ability to recognize new expressions was evaluated on a group of test subjects with respect to the six universally recognized facial expressions. Results show that the use of abstract muscle definition reduces the effect of potential noise in the motion capture data and allows the seamless animation of any virtual anthropomorphic face model with data acquired through human face performance. {\textcopyright} 2017 Universidad Nacional Aut{\'{o}}noma de M{\'{e}}xico, Centro de Ciencias Aplicadas y Desarrollo Tecnol{\'{o}}gico},
annote = {cited By 0},
author = {Woodward, A and Chan, Y H and Gong, R and Nguyen, M and Gee, T and Delmas, P and Gimel'farb, G and {Marquez Flores}, J A},
doi = {10.1016/j.jart.2017.01.002},
journal = {Journal of Applied Research and Technology},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {1},
pages = {61--77},
title = {{A low cost framework for real-time marker based 3-D human expression modeling}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013659529{\&}doi=10.1016{\%}2Fj.jart.2017.01.002{\&}partnerID=40{\&}md5=21fa52776df8f0831d95326ccfec57eb},
volume = {15},
year = {2017}
}
@conference{Cai201540,
abstract = {A Generalized Beam Theory (GBT) approach is derived that performs automated, quantitative modal decomposition of thin-walled members with an open cross-section. The technique extracts modal amplitudes and modal participation factors from any 3D displacement field, for example from finite element analysis or point clouds measured in the lab during a test to collapse. Thin-walled members exhibit deformation that can be represented as combinations of cross-sectional and global buckling modes. It is useful to quantitatively decompose these modes for strength prediction and design code development. Conventionally, buckling mode participation has been determined by visual inspection. This process is subjective and tedious since the person conducting the inspection is often dealing with many models or experiments. Taking advantage of GBT kinematics, the proposed method distinguishes itself by using only the GBT cross-section deformation modes instead of member-wise basis functions. The method is by nature applicable to different boundary and loading conditions without recalculation of basis functions. The mechanics are formulated to show that the method is supported by GBT kinematic assumptions, which ensures its general applicability. The approach is implemented in a Graphical User Interface (GUI) that accepts a thin-walled member 3D displacement field as input and then calculates modal participation factors, i.e., for member local, distortional, and global (Euler) buckling. Copyright {\textcopyright} 2015 by the Structural Stability Research Council.},
annote = {cited By 4},
author = {Cai, J and Moen, C D},
booktitle = {Structural Stability Research Council Annual Stability Conference 2015, SSRC 2015},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {40--57},
title = {{Automated buckling mode identification of thin-walled structures from 3D finite element mode shapes or point clouds}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942412894{\&}partnerID=40{\&}md5=2ac8f762df454deb4113e4bd28ca94cd},
year = {2015}
}
@conference{Pang2017585,
abstract = {Efficient detection of three dimensional (3D) objects in point clouds is a challenging problem. Performing 3D descriptor matching or 3D scanning-window search with detector are both time-consuming due to the 3-dimensional complexity. One solution is to project 3D point cloud into 2D images and thus transform the 3D detection problem into 2D space, but projection at multiple viewpoints and rotations produce a large amount of 2D detection tasks, which limit the performance and complexity of the 2D detection algorithm choice. We propose to use convolutional neural network (CNN) for the 2D detection task, because it can handle all viewpoints and rotations for the same class of object together, as well as predicting multiple classes of objects with the same network, without the need for individual detector for each object class. We further improve the detection efficiency by concatenating two extra levels of early rejection networks with binary outputs before the multi-class detection network. Experiments show that our method has competitive overall performance with at least one-order of magnitude speed-up comparing with latest 3D point cloud detection methods. {\textcopyright} 2016 IEEE.},
annote = {cited By 3},
author = {Pang, G and Neumann, U},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7899697},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {585--590},
title = {{3D point cloud object detection with multi-view convolutional neural network}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019119289{\&}doi=10.1109{\%}2FICPR.2016.7899697{\&}partnerID=40{\&}md5=f7ff83a586a0107885e80fea79ee13b2},
year = {2017}
}
@conference{Kadamen2016617,
abstract = {Three dimensional models obtained from imagery have an arbitrary scale and therefore have to be scaled. Automatically scaling these models requires the detection of objects in these models which can be computationally intensive. Real-Time object detection may pose problems for applications such as indoor navigation. This investigation poses the idea that relational cues, specifically height ratios, within indoor environments may offer an easier means to obtain scales for models created using imagery. The investigation aimed to show two things, (a) that the size of objects, especially the height off ground is consistent within an environment, and (b) that based on this consistency, objects can be identified and their general size used to scale a model. To test the idea a hypothesis is first tested on a terrestrial lidar scan of an indoor environment. Later as a proof of concept the same test is applied to a model created using imagery. The most notable finding was that the detection of objects can be more readily done by studying the ratio between the dimensions of objects that have their dimensions defined by human physiology. For example the dimensions of desks and chairs are related to the height of an average person. In the test, the difference between generalised and actual dimensions of objects were assessed. A maximum difference of 3:96{\%} (2:93cm) was observed from automated scaling. By analysing the ratio between the heights (distance from the floor) of the tops of objects in a room, identification was also achieved.},
annote = {cited By 0},
author = {Kadamen, J and Sithole, G},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprsarchives-XLI-B3-617-2016},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
pages = {617--624},
title = {{Automatically determining scale within unstructured point clouds}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978036086{\&}doi=10.5194{\%}2Fisprsarchives-XLI-B3-617-2016{\&}partnerID=40{\&}md5=e45f3c738a9989d2d6d38853dcf4e37d},
volume = {41},
year = {2016}
}
@inproceedings{8265506,
abstract = {An efficient, fully automatic method for 3D face shape and pose estimation in unconstrained 2D imagery is presented. The proposed method jointly estimates a dense set of 3D landmarks and facial geometry using a single pass of a modified version of the popular "U-Net" neural network architecture. Additionally, we propose a method for directly estimating a set of 3D Morphable Model (3DMM) parameters, using the estimated 3D landmarks and geometry as constraints in a simple linear system. Qualitative modeling results are presented, as well as quantitative evaluation of predicted 3D face landmarks in unconstrained video sequences.},
author = {Crispell, D and Bazik, M},
booktitle = {2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
doi = {10.1109/ICCVW.2017.295},
issn = {2473-9944},
keywords = {face recognition;image sequences;neural net archit,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {2512--2518},
title = {{Pix2Face: Direct 3D Face Model Estimation}},
year = {2017}
}
@inproceedings{7552934,
abstract = {Estimating the 3D shape information of a face from a single image is a challenging task, especially when the input image is captured under unconstrained scenarios (e.g., variations of pose, illumination, expression, or even disguise). Previous approaches to this problem typically require careful initialization, registration, or segmentation of the face image regions. With the objective to match the detected landmarks of the input image with those of a set of reference 3D models, we propose a non-negative least squares (NNLS) based algorithm for joint pose and shape estimation. With the additional imposed pose regularization, our method is able to perform person-specific shape estimation, while the camera pose can be simultaneously recovered. We show that our method is robust, effective, and computationally feasible. Moreover, it would perform favorably against existing approaches to 3D shape estimation from a single unconstrained image.},
author = {Wei, C and Wang, Y F},
booktitle = {2016 IEEE International Conference on Multimedia and Expo (ICME)},
doi = {10.1109/ICME.2016.7552934},
issn = {1945-788X},
keywords = {face recognition;image matching;image registration,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--6},
title = {{With one look: 3D face shape estimation from a single snapshot}},
year = {2016}
}
@conference{Gálai2015,
abstract = {In this paper, we present a performance analysis of various descriptors suited to human gait analysis in Rotating Multi-Beam (RMB) Lidar measurement sequences. The gait descriptors for training and recognition are observed and extracted in realistic outdoor surveillance scenarios, where multiple pedestrians walk concurrently in the field of interest, their trajectories often intersect, while occlusions or background noise may affects the observation. For the Lidar scenes, we compared the modifications of five approaches proposed originally for optical cameras or Kinect measurements. Our results confirmed that efficient person re-identification can be achieved using a single Lidar sensor, even if it produces sparse point clouds. {\textcopyright} 2015 IEEE.},
annote = {cited By 5},
author = {G{\'{a}}lai, B and Benedek, C},
booktitle = {2015 International Workshop on Computational Intelligence for Multimedia Understanding, IWCIM 2015},
doi = {10.1109/IWCIM.2015.7347076},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
title = {{Feature selection for Lidar-based gait recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962873425{\&}doi=10.1109{\%}2FIWCIM.2015.7347076{\&}partnerID=40{\&}md5=39be328f36f8e80a79539086d1ad61e5},
year = {2015}
}
@conference{Xie2017417,
abstract = {Indoor reconstruction from point clouds is a hot topic in photogrammetry, computer vision and computer graphics. Reconstructing indoor scene from point clouds is challenging due to complex room floorplan and line-of-sight occlusions. Most of existing methods deal with stationary terrestrial laser scanning point clouds or RGB-D point clouds. In this paper, we propose an automatic method for reconstructing indoor 3D building models from mobile laser scanning point clouds. The method includes 2D floorplan generation, 3D building modeling, door detection and room segmentation. The main idea behind our approach is to separate wall structure into two different types as the inner wall and the outer wall based on the observation of point distribution. Then we utilize a graph cut based optimization method to solve the labeling problem and generate the 2D floorplan based on the optimization result. Subsequently, we leverage an $\alpha$-shape based method to detect the doors on the 2D projected point clouds and utilize the floorplan to segment the individual room. The experiments show that this door detection method can achieve a recognition rate at 97{\%} and the room segmentation method can attain the correct segmentation results. We also evaluate the reconstruction accuracy on the synthetic data, which indicates the accuracy of our method is comparable to the state-of-the art.},
annote = {cited By 2},
author = {Xie, L and Wang, R},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-2-W7-417-2017},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {2W7},
pages = {417--422},
title = {{Automatic indoor building reconstruction from mobile laser scanning data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030987630{\&}doi=10.5194{\%}2Fisprs-archives-XLII-2-W7-417-2017{\&}partnerID=40{\&}md5=ae7f9b3c16f3693b3f672d12939d4e30},
volume = {42},
year = {2017}
}
@inproceedings{7550083,
abstract = {The use of non-negative matrix factorisation (NMF) on 2D face images has been shown to result in sparse feature vectors that encode for local patches on the face, and thus provides a statistically justified approach to learning parts from wholes. However successful on 2D images, the method has so far not been extended to 3D images. The main reason for this is that 3D space is a continuum and so it is not apparent how to represent 3D coordinates in a non-negative fashion. This work compares different non-negative representations for spatial coordinates, and demonstrates that not all non-negative representations are suitable. We analyse the representational properties that make NMF a successful method to learn sparse 3D facial features. Using our proposed representation, the factorisation results in sparse and interpretable facial features.},
author = {Koppen, W P and Christmas, W J and Crouch, D J M and Bodmer, W F and Kittler, J V},
booktitle = {2016 International Conference on Biometrics (ICB)},
doi = {10.1109/ICB.2016.7550083},
keywords = {face recognition;feature extraction;image registra,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
pages = {1--8},
title = {{Extending non-negative matrix factorisation to 3D registered data}},
year = {2016}
}
@inproceedings{8373915,
abstract = {In this paper, we propose a novel method for reconstructing 3D faces from 2D images. The method is characterized in three aspects. (i) It utilizes only geometric cues in the input images, i.e., 2D facial landmarks. (ii) It works for an arbitrary number of unconstrained images, both single and multiple images. (iii) It can effectively exploit complementary information in multiple images of varying poses and expressions. The method is implemented based on cascaded regression in shape space. We have evaluated the method on three databases and observed from the experimental results that (i) the reconstruction error is reduced as more images of different poses are used, (ii) the proposed method can obtain comparable reconstruction results by using state-of-the-art automated methods to detect the 2D landmarks, and (iii) the proposed method is robust to variations in facial expressions and image qualities.},
author = {Tian, W and Liu, F and Zhao, Q},
booktitle = {2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)},
doi = {10.1109/FG.2018.00122},
keywords = {face recognition;image reconstruction;regression a,revisao{\_}ieeexplore,revisao{\_}scopus},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus},
pages = {774--779},
title = {{Landmark-Based 3D Face Reconstruction from an Arbitrary Number of Unconstrained Images}},
year = {2018}
}
@inproceedings{7877955,
abstract = {3D face reconstruction has long been a Research Focus and application hotsopt, in the field of 3D reconstruction from 2D face image sequences, the reconstruction result is impeded by the problems of the occlusion and illumination or pose variations frequently. Different from 3D morphable model fitting algorithm, the reconstruction method based on face feature points puts forward higher requirements to the robustness and accuracy. This paper propose a reconstruction method of fusing deep convolutional network for facial feature points extraction and factorization for SFM(Structure from Motion), we investigated the way of solving accurate sparse face structure matrix by importing a modified matrix, and the possibility of improving the realistic of 3D face reconstruction result by registering the sparse face structure and the general 3D face model. To reconstruct final dense face model, we use thin plate spline interpolation. The reconstruction result based on our method proved to be efficient and robust.},
author = {Liang, G and Shu, Z and Jianguo, J},
booktitle = {2016 IEEE 13th International Conference on Signal Processing (ICSP)},
doi = {10.1109/ICSP.2016.7877955},
issn = {2164-5221},
keywords = {face recognition;image fusion;image reconstruction,revisao{\_}ieeexplore},
mendeley-tags = {revisao{\_}ieeexplore},
month = {nov},
pages = {873--878},
title = {{Fusing deep convolutional network with SFM for 3D face reconstruction}},
year = {2016}
}
@article{Pushparani201533938,
abstract = {In networked society the people perform many e-commerce activities in daily life. In such applications personal identification is critically important. Biometric identifiers are replacing traditional identifiers, as it is difficult to steal, replace, forget or transfer them. It is possible that, a 2D-image based facial recognition system can be easily spoofed with simple tricks and some poorly-designed systems have even been shown to be fooled by the imposters. Spoofing with photograph or video is one of the most common manners to circumvent a face recognition system. It becomes easier to spoof in these biometric systems with the aid of fake biometric; it further reduces the reliability and security of biometric system. In this paper, face spoof attack many biometric, applying skull identification. We will be exploring the techniques that are more secure and reliable. In skull identification, nearly all of the methods depend on accurate extraction and representation of the relationship between the skull and face. However, it is very difficult to extract this complex relationship. Because this work aims to identify human face is from skull. This paper proposes a skull identification method that matches a skull with enrolled faces, in which the mapping between the skull and face is obtained using enhance canonical correlation coefficient analysis with scale invariant feature transform (SIFT). Here a statistic method is adopted to estimate outlook from subclass of skull-face database using Principle component analysis and Linear discriminant analysis (LDA). In order to improve the accuracy of the result, we select the suitable organ (eyes, nose and mouth) for the statistic result based on anatomy principle from the database and achieve the organ and face integration to build the final outlook, a method to build a joint statistical 3D model of the skull and face is presented. The Second approach for enhancing the matching performance of AAM is to AAM itself, by proposing a novel fitting algorithm or enhancing the existing fitting algorithms. In this proposed a fast AAM using enhance canonical correlation coefficient analysis (ECCCA), which has modeled the relation between differences of the image and the model parameter for improving the convergence speed of fitting algorithm. We propose to identify an skull through using a correlation measure between the 3D skull and 3D face in terms of the morphology, and measure the correlation using Enhance canonical correlation coefficient analysis (ECCCA).We use the 3D skull data as the probe and 3D face geometric data as the gallery, and match the skull with enrolled 3D faces by the correlation measure between the probe and the gallery. The Third approach for scale invariant feature transform (SIFT) bundles a feature detector and a feature descriptor. The detector extracts from an image a number of frames (attributed regions) in a way which is consistent with (some) variations of the illumination, viewpoint and other viewing conditions. {\textcopyright} Research India Publications.},
annote = {cited By 1},
author = {Pushparani, M and Indumathi, T},
journal = {International Journal of Applied Engineering Research},
keywords = {revisao{\_}scopus},
mendeley-tags = {revisao{\_}scopus},
number = {14},
pages = {33938--33948},
title = {{Human authentication by matching 3d skull with face image using Scca}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940062487{\&}partnerID=40{\&}md5=94160469b47077f0f6b1aa95ce98f35e},
volume = {10},
year = {2015}
}
@article{Siqueira2018,
abstract = {This work presents a multiple slicing model for 3D images of human face, using the Frontal, Sagittal and Transverse orthogonal planes. The definition of the segments depends on just one key point, the nose tip, which makes it simple and independent of the detection of several key points. For facial recognition, attributes based on adapted 2D spatial moments of Hu and 3D spatial Invariant Rotation Moments are extracted from each segment. Tests with the proposed model using the Bosphorus Database for neutral vs non-neutral ROC I experiment, applying Linear Discriminant Analysis as classifier and more than one sample for training, achieved 98.7{\%} of verification rate at 0.1{\%} of false acceptance rate. By using the Support Vector Machine as classifier the rank1 experiment recognition rates of 99{\%} and 95.4{\%} have been achieved for a neutral vs neutral and for a neutral vs non-neutral, respectively. These results approach the state-of-the-art using Bosphorus Database and even surpasses it when Anger and Disgust expressions are evaluated. In addition, we also evaluate the generalization of our method using the FRGC v2.0 database and achieve competitive results, making the technique promising, especially for its simplicity.},
author = {Siqueira, Robson S. and Alexandre, Gilderlane R. and Soares, Jose M. and The, George A. P.},
doi = {10.1109/LRA.2018.2854295},
file = {:home/artur/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siqueira et al. - 2018 - Triaxial Slicing for 3-D Face Recognition From Adapted Rotational Invariants Spatial Moments and Minimal Keypoi.pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
keywords = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
mendeley-tags = {revisao{\_}ieeexplore,revisao{\_}scopus,revisao{\_}webofscience},
number = {4},
pages = {3513--3520},
title = {{Triaxial Slicing for 3-D Face Recognition From Adapted Rotational Invariants Spatial Moments and Minimal Keypoints Dependence}},
url = {https://ieeexplore.ieee.org/document/8408720/},
volume = {3},
year = {2018}
}
